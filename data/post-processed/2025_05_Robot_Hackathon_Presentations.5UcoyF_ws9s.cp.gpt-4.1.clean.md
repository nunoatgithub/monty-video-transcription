So basically, the idea is that we go through three projects. Each project gives a short presentation with a couple of slides showing what they did. Our independent judge, Terry, picks an object that each team needs to evaluate. Each team demonstrates how their setup performs on this object, and we see who succeeds. If there's a tie, we'll pick another object until we have a clear winner.

That's half of the points. The other half are rated by our independent judge, Terry. There are several categories. Terry, if you're uncomfortable rating any of these, you can skip them. You just put an X in the column for the team you think did best. If you think there's a tie, you can put two Xs, or leave it blank if none of us did well.

The categories are: best in show/most fun presentation, easy on the eyes (most beautiful setup), impressiveness of the solution, most ambitious project, most creative solution when encountering issues, team players (where everyone contributed uniquely and collaborated well), best demonstration of Monty's capabilities, and the team that made you most excited about Monty's future. The last item is rated by all of us. Each person puts one X in a column, and we count them up. You can't vote for your own team.

Does that all make sense?

So it's not going to be blind? No. Are you worried people will get mad at you because of your judgment? No, I'm okay with it. Can you tell me, I know "It's Not a Tumor" is the international team. "Everything Is Awesome" is whose team? That's Rami, that's the Lego team. Okay, and the other ones? The only two categories I'm uncomfortable with are the ones about Monty, because I don't know enough about Monty. For those, just pick the one that shows the most promise or seems really cool for the future, since these are all demos. If you don't feel comfortable after the presentations, you can leave the row blank. I'll send you the spreadsheet link again on Slack. So I have to take notes—I haven't taken notes in years.

It's not that high pressure.

Are all of you ready? Do we have a volunteer to present first?

Does Terry have to pick someone? Okay, so much pressure. With the presentations, try to keep it under 10 minutes if you can. Iris is our team name, which Scott came up with. I think it's the best name, but I'm biased. Iris is a robotic unmanned scout, by me, Scott, and Jeremy. This is the only slide I want to present, and I think everyone has included the origin of their team names.

Iris is a character from Greek mythology, from Crete. There's a bit of a tie with the team there. The story is that he and his father, the old man in the picture, built a labyrinth in Crete for King Minos to imprison the Minotaur. They were then put in the labyrinth jail because Minos was angry that Theseus was able to defeat the Minotaur and become a hero. So they were imprisoned, but Daedalus was a smart scientist and engineer, which we tried to emulate this week.

He came up with a plan to glue feathers together with wax so they could fly away. Unfortunately, Iris's son got too arrogant, flew too close to the sun, the wax melted, and he fell into the sea and died. But he auto-landed.

Between the two men here, unfortunately our drone was more like Iris, randomly dying from time to time. That was one of the technical problems. We would turn on the drone, connect it, but then it would just decide on its own that it was done, even though the battery wasn't dead—it would just shut itself down. I guess the name is somewhat fitting in that sense.

The project goal for us, despite the finicky drone, was to put Monty inside. There are some unique challenges for the drone team. The starting point for a drone team is the camera attached to the drone. We are implementing a distant agent. In Monty, that means the agent or the drone itself is not on the surface of the object.

With this setup, we get an RGB normal picture, from which we had to do novel things like depth estimation, object segmentation, and projecting that to 3D space. The ideal goal was to learn all the real-world YCB objects that I think mostly Meals ordered from Amazon. This included the spam can you see here, some hot sauces, a new cup, mustard bottle, etc., and to perform inference on them.

Our general plan was split into two areas: the input area, which is the data loader, and the output area, which is the motor system. In the data loader section, the general step is to take a picture with the drone. That picture is really large and has the whole view of the object. For Monty, we need to take little patches of it—so that's what patch five means. For each of those patches, we need to estimate the depth and get the semantic label. In the picture, wherever there is an object in the pixel, we put a one, and wherever it's background, we put a zero. That's what semantic label means here. Then, using the depth information and semantic label, we project to 3D coordinates.

There were other challenges we had to address to actually implement this. One was keeping the drone alive. We had to calibrate depth, work on segmentation, and something called very good detection. I'm not going to go into each detail because it can get quickly technical.

Do you want to comment on the motor systems, Scott?

Sure. Knowing the location of the drone and the depth of the objects it's looking at are obviously crucial pieces of information for Monty. These were real challenges that we kept trying to pare back. The rotating spam here—at one point, we realized we weren't going to be able to control the drone in real time and do what we wanted. So this is just one of our simulated setups that you're looking at. Fairly early on, we abandoned real-time drone control. We'll show you the real-time drone control in our demo, because I think that's the only thing we can show. You can see how the drone motor system works, but here, this animation is basically simulating what kind of image the drone would potentially take if it was flying in a circle around the object.

That image is just because we produced this ground truth dataset, and I wanted to make sure the coordinate systems were as expected based on the geometry and that everything was aligned. That's all that is.

I have a request for Will: to make this gif into an official TP thing. I want to speed it up so that whenever I'm thinking, instead of my brain wheels turning, I'm going to say my spam wheel is turning. Keep that in mind. I don't know if we can make this a Slack sticker or one of those emojis. I'll remake it if we want to go official.

I want my spam wheels turning for brainstorming. That's my thing.

Depth estimation presented similar challenges to the Monty Meets World project from previous years. In that case, they used a true depth camera, such as an iPad or another device, to estimate depth. Our drone, however, is not as advanced as the Microsoft Kinect, which can provide depth data via lidar; it only captures RGB images. We had to implement our own depth estimation. We used a deep learning model called Depth B2 from Nvidia to generate a depth map. Here are some example outputs: the RGB image and the depth map. Lower values indicate objects are closer. In the image, the camera points slightly downward, which we noticed is a fixed feature of the drone. If the drone is flat, the camera is still tilted downward, and this cannot be changed.

Anything in yellow in the depth map is far in the background. We also created a makeshift box for calibration. The depth values obtained are random; lower values are closer, higher values are farther, but they do not correspond to actual meters. We performed a calibration by placing a box at a known distance and selecting a point to measure how far away it was. The box was slanted away, with several known points, so we fit known locations against estimated locations to remap the values into actual meters. This worked well for a single box, but when we applied the fit to other objects, such as a spam can, the results were inconsistent. There is no single set of parameters to invert the depth map. It seems the depth map provides relative differences between objects in the scene, not absolute measurements.

Segmentation was another challenge. We referenced the Iman Meets World project to see how they handled object segmentation. In our current Monty system, the simulated world is idealized, with objects floating in space and nothing around them—gravity is turned off, and we can view objects from below. In real life, objects rest on tables or floors, so the depth map is not flat. If we try to segment, sometimes the floor is included, and not all of the object is captured.

We implemented an alternative segmentation method using another deep learning model called Segment Anything Model (SAM). This allowed us to more cleanly extract objects from the scene, though it is not perfect and sometimes misses parts. By segmenting the object, we can remove the background and table, placing the object into the idealized Monty world where it floats in space.

To choose which object to segment, we assumed the object was at the center of the image. In other cases, we used bounding boxes, as seen in Slack. There are computer vision techniques to extract real-world coordinates relative to a landmark, such as a QR code. The center of the QR code is set as (0, 0, 0), meaning X equals zero, Y equals zero, and Z equals zero. Using this, we can theoretically determine the camera's position and the ground truth object's position and orientation. When flying the drone, it did not know its position in space, so by referencing the QR code, we could determine the drone's position and orientation relative to the code. The values obtained were sensible, with X and Y close to zero and Z around 20 centimeters, matching the drone's placement. The system accurately estimated position and rotation, including tilt and yaw.

Will came up with this idea, so thank you, Will. I was not aware of this approach before the hackathon.

With all the depth and segmentation information, we could generate 3D points. The images appear as pictures, but they are actually densely sampled points—hundreds of thousands per object. This is most evident in images showing stray points. We took spam can pictures from 12 different locations around the object and projected each image into 3D points. This is the result of our depth-to-3D process.

Individual pictures turned out okay, but the system is sensitive to parameters and can produce distorted results even if the image looks fine. Combining all three coordinates into the same space was the most challenging part.

Scott, do you want to explain this? Sure. On the left, after we calibrated the depth, we had a mapping from the values Anthony gave us to real-world coordinates based on the calibration. On the left is what the point cloud looked like after calibration. It kept interpreting everything as very close on the side of the screen. I thought maybe if we adjusted the distribution for each point cloud to move its center near 20 centimeters, which is what we knew was accurate, we could push everything back and see if that helped. We tried many different approaches to get these into a reasonable 3D space. The one on the right is from the non-calibrated, less precise estimation, which turned out to be better.

You can see individual images that would have been taken straight on or at a slight angle. We're modeling inside out, and ideally, you'd form a circle instead of this irregular shape. This is as far as we got in a week.

There were many lessons learned, and I didn't list them all here. I think there's a lot to consider on the drone project side. Starting with something that already has better control and reliable sensorimotor data is probably a better starting point for a one-week project. We were close to getting Monty to push data through, and we wrote the data loader and everything—it's ready to go. The challenge was getting the data into the same space as the pre-trained model; at a minimum, the units and scale are completely different.

This is definitely doable, just maybe not in a week. Monty could definitely run on a drone. With a better drone, we could get real-time feedback, which would be helpful. One issue was that to connect to the drone, we had to give up the house wifi. We'd take a picture and realize the drone was tilted, too far, or too close, so we had to do a lot of physical manipulation to get good pictures. The sub-animation here is cool because Scott and I measured exact distances and angles to get it as precise as possible. Thankfully, we have some wet lab experience, so this was okay. Overall, this was very informative. I learned a lot about depth to 3D. I didn't get to work on the motor side, but at least on the data loader and sensory module, it was very informative—a very cool, fun project. I wish we could have jumpstarted getting the data into 3D space a little quicker.

Jeremy worked more on the motor side to keep the drone alive and moving in circles, but we don't have outputs for those because we never got the data to Monty to emit a motor signal. If it had said "move up," Jeremy could have controlled the drone that way. If we don't have to return the drones, maybe Scott and I will take one each back and work on it a bit more. I think we are very close. It's really cool, and it sounds like there are a lot of pieces that are just super close. If the hackathon had been a few days longer, I think you definitely had a challenging project with all the different components. That was also what we imagined people spent most of their time on during the hackathon—the robotic side. Assembling the robot and tracking locations is definitely the hardest part. It's good we're not trying to do this every day, but it's useful to dip into it and appreciate the challenges and the assumptions we rely on when working with simulations.

Thank you. Were those all your slides? Yes. I'll yield the floor to the next team.

Wait a minute, I have a couple of questions. Could you explain how this demonstrates Monty's capabilities and how this project shows Monty's future in your words?

It's hard to say. We gained a lot of experience with Monty while preparing for this, and I learned a lot about how we might make the process easier for starting a new project. Of all the teams, we're not the ones to give a definitive answer on Monty's future because we didn't get far enough for Monty to be a major part of our project. At 2:00 AM, within a week, we did what we could. In terms of real-world application, among the three teams, the drone is probably the most accessible for the public. If we can put Monty into the drone, we just haven't gotten there in a week, but that means if someone had a flying agent, we could readily apply Monty for object detection and pose estimation in the wild. That's about Monty's future. Demonstrating Monty's capabilities, we haven't extended any; we're just using the existing graph learning modules. In terms of Monty's future, we could all get together afterwards and discuss what kinds of architecture we might add to make preprocessing more pluggable. We've gained a lot of user experience, and if someone wants to adopt Monty into their project, we know what challenges they might face and how to make that easier for others. We've definitely gained a lot of insights.

With the existing technology, how would Monty improve things? There are drones out there that can do location-based tasks, but can they recognize what they're hitting? Is there any visualization, or is it all based on location? Once we have the images, Monty will be able to recognize different objects in the scene. Right now, it's just a spam can, but theoretically, a drone could fly around and put boxes around objects—like "I see a spam can, I see a mustard bottle," and so on. In that sense, it's similar to some drones using deep learning for object tracking. We're doing it in a sensorimotor way instead of a deep learning way. The main point is that Monty is a sensorimotor system. For this drone project, the drone can intelligently move around the object. It's not just a static image for recognition; it uses learned models to move in the world intelligently. If it tries to find or recognize an object, it moves to collect the information needed to determine object ID and pose. It's not a static image recognition system.

Right now, do drones only do static recognition, or is there more?

Generally, it's a mixture. It can be static or use single images coming in. Some drones are designed to build a 3D model, like surveying a building. Right now, you wouldn't use Monty for that. The idea is that Monty builds representations that become more complex and abstract. With a full Monty system, it could understand the environment in a much more intelligent way than any current drone. There's no AI system that can understand the environment like a human does. Monty can continuously learn on the fly, go into a disaster zone, see objects and configurations it's never seen before, learn quickly, and then recognize or report what it's seeing, like where trees are blocking a road. Okay, I think we've taken up too much time, so let's move on. That's okay. I'll stop sharing.

I'll plug Hojae for creative solutions—the ArUco idea was really elegant. Figuring out the drone's position in space using those codes was a great idea. I'll toss a compliment to Will because it was his idea first. Actually, let me quickly ask about that. Could you calibrate the depth with that? If you knew it was 20 centimeters away in the depth image, could you say any pixel at that point, or is there an issue? Yes, that would be ideal. We did the slanted book thing to get different depths. You can do multiple codes. I printed out four and put them on four sides of a cube, or even on different planes. Technically, I could put them on all six sides. The more, the better, because each gives an estimate, and you can average them for a more precise pose. We're not getting perfect numbers because I'm just estimating from one marker seen. You can detect multiple markers, which would be more ideal, and aggregate information from there. Maybe we can have follow-up meetings to go into details, but let's keep it high-level for now. Otherwise, the demos will take too long.

Lego Team, do you want to go next? Try to keep it under 10 minutes. Can I share my screen? One second.

Do you have music in this presentation? No. We should have added a background track. Okay, can everybody see this? Hi, we're team Everything is Awesome—Rami and myself.

There we go. Explain the team's origin, please. In the beginning, we got some Legos, bought a bunch of parts, and started assembling them into a platform. This is the platform actually running—our robot assembly, showing one of our motors and part of our motor system. We calibrated it scientifically to see how fast it could go. The other part of the robot is a sensorimotor module. This is the elevator that goes up and down. In the background, you can see the sensorimotor with the RGB camera and the depth camera. That's what the robot looks like combined.

We did a lot of debugging. This is an example of debugging the depth sensorimotor. You can see different depth resolution versus RGB resolution. Every dark point is an RGB point, and the depth sensors were more pixelated, with worse resolution. Some lessons learned: early on, we reminded each other not to eat the dataset, since some of it was edible and in the kitchen. We finally separated it from the rest of the food we planned to eat. Another lesson learned was that building something with Monty end to end was really valuable. We had to write and understand every part of the Monty system. Rami and I are now much more familiar with how the whole system works. There was a resource that really helped with intuition for some of the Monty framework concepts. There are some ideas for action refactor I want to pursue after we're done, like noticing too many resets happening before an episode. In simulation, you never notice it, but with a robot, you wonder why it's resetting so many times before starting. That was a neat finding. We ran into the same problem, spent too much time on it, and decided we'd always have to press the button twice before starting the experiment. We also had to take two pictures because the first picture would always be black. I removed the extra reset; our coding approach was to fork the entire Monty repository and hack whatever we needed, which was convenient for the hackathon.

Picking coordinates that make the problem easy helped. Our robot has a central platform that rotates and a part that goes up and down. Our units are the robot radius, and we are on the unit circle, which simplified things immediately.

Next, debugging and visualizations. Early on, I realized that having good visualizations and debugging tools goes a long way in projects like this. We started working on visualizations that let us see the sensor positions live as the robot moves, as well as what Monty thinks is happening, the rotations, and all that. We had trouble with coordinate systems. For the get state of the agent, we used a different coordinate system than Monty, thinking it would work fine, but it didn't plug well with depth to 3D. We ended up changing all our computations for get state of the agent and started looking into cos too.

We tried model scaling, using a different scale than what was in the provided dataset, but that didn't work well. We added code to the grid object model to allow scaling the mesh and recalculating KT3, but that still didn't help much. We trained our models, but they were very sparse and may not work well. The idea is that a dense model would work better, but we still saw reasonable performance.

Early on, we had a lot of networking issues because there were multiple routers, and clients connected to different routers couldn't talk to each other. We needed a router but couldn't connect it to the internet except through a hacky solution: plugging it into a WiFi extender, which gave internet to the router, allowing us to connect.

The cool thing about what we built is that we could decouple the simulator from Monty itself. We tried running two different Monty instances on the same hardware. Tristan and I ran experiments at the same time, fighting for hardware resources. The sensors would move—I'd give a command to move somewhere, Tristan would move it somewhere else, and we'd argue about resources. The depth camera gave us a lot of trouble, with too much troubleshooting and debugging. Debugging setup was crucial.

With troubles changing the scale of the existing dataset, we ended up training on the dataset ourselves. If you don't recognize it, that's a cup. This is training sped up 40 times. On the left, you see the visualization being built; on the right, the robot uses a training policy to orbit around the object from the bottom up and scan it. That's the idea behind it.

This is what the final scan for the cup looks like. It does end up looking like a cup, but the handle is somewhat detached. We had some trouble with the depth information and possibly the focal point and field of view. The scan forms a perfect circle, but the handle is a bit separated from the cup. Our evaluation works fine with the trained models, but scaling the meshes didn't work well because they're different. During inference, we can see this. This is a different kind of visualization where I'm plotting the current MLH in real time. If you can figure out the rotation, this is essentially a sensorimotor comparison between where the system thinks the sensorimotor is and where the mug actually is. It was able to determine that the cup was right side up, but could not figure out the rotation on the y-axis or the handle's position.

It could separate the cup from all other objects in the dataset, which is good. We used a base policy with random actions. We had to rewrite it slightly to use our actions, but as shown in the video, those actions are purely random—moving up, down, and rotating around the object. The sensor is always guaranteed to be on the object in these cases. If you're off the object, the system knows based on the depth; if the depth is too far, you're not on the object. We build semantic labels based on the depth.

We can discuss more—should we do a live demo now or after all the presentations? Maybe we wait until after and pick one object to demo. Sounds good. Great job, it looks really cool. Can I ask those same two questions again? How did your project demonstrate Monty's capabilities, and how do you see the application for the future? Demonstrating Monty's capabilities, I think this is probably the first system with actual movements in the world, rather than just moving a patch over an image. The patch itself moves through the world, which is very different from what we had before. It's showing a different side of Monty's capabilities with an actual motor. In previous presentations, you saw a full image being taken, but Monty only sees those dots—there's no image of the cup. Monty learns just from those dots and recognizes objects based on them. There's no full picture given to Monty at any point.

For future capabilities, this is a first step. Like with the drone team, there are opportunities to explore boating, which is important. We're building this with Lego, so it's easy to add a different sensorimotor from a different angle. We can move it independently or together as part of one agent, like two sensorimotor patches on a finger or different hands looking at different parts of the mug. It's very extendable the way we've set it up. This is the first time Monty is working with a motor in the real world, which is a huge milestone. Now we have a test bed to try Monty in the real world. Previously, anything complex involving movement had to be tested in simulation. Now, we can use any object, not just simulated ones—pick something from the kitchen and see what Monty does.

It scans very nicely and very fast, but it's still sparse compared to what I expected. The measures you gave us are very dense, but this is sparse. The fact that it works even with sparse models is impressive. Does it work at any speed? If you slow down or speed up the motor, does it still have the same results? Slowing down is easier; speeding up, we're running at a high speed, but we're limited by the time it takes to transfer images wirelessly from the sensors to the computer and process them. This is actually the bottleneck. One challenge was sending bigger images to stitch the depth camera with the RGB, sending both from the Raspberry Pi and Monty to the computer, then stitching them. We found that sending just the patches and computing which patch we want, which means sending smaller images, is much faster. Monty is actually much faster than the bottleneck of sending images. So it works fine at high speeds. Monty doesn't care how fast you move or in what pattern you move over the object; it doesn't need to have seen that pattern before during training. It can generalize to any speed of movement as long as the transfer is fast enough.

You're muted. Muted. You can speed up inference by having Monty use multiple sensors. If I have a thousand sensors looking at something, you can recognize it instantaneously rather than having to move, or you can move it once. Once we put this in hardware, it could go very fast. Another future capability is that we have zero precise movement. When the elevator goes up, we ask it to go a certain distance, but it goes whatever distance it manages. There's minimal distance when it rotates; if we ask it to rotate by seven degrees, it does whatever it wants. As a result, we are able to work on proprioception and ensure Monty is still processing where the object actually is. What we're demonstrating is that you don't need precise movement as long as you know where you end up. The policies still work and can infer and train objects, even with imprecise motor control, which is a new demonstration outside of simulation. There were times when the rotation robot, the one that rotates the platform, was not even moving. It tries to move, but it looks like it doesn't have enough power or is just doing small steps. Because we're using proprioception, we're getting the position of the agent, so it doesn't matter. It would just add points at that same location, or depending on how the object is built, it doesn't need to add these points. The lesson learned is that getting an accurate position of the agent is more important than moving the agent somewhere accurately. It doesn't matter as long as you can get the position accurately.

That was helpful.

Awesome. Looks great.

That's how they got their name. That was Tristan's idea.

Then I guess it's our turn. All right.

How do I start the presentation? Sure.

I came up with the team name. This is where the team name came from, the movie Kindergarten Cop.

What's the matter? I have a headache. It might be a tumor. It's not a tumor. Not a tumor at all.

We are the ultrasound team. I'm Will, Viviane, and Niels. We are scanning things inside a bag that are not medically related, so that's why we call this the "not a tumor" plan.

This is the ultrasound we've been using. We got this on loan from Butterfly Ultrasound. This is it in my hand for scale, and on the right is an actual ultrasound at the top. You can see the probe, and down here you can see something that's definitely not a tumor. This is the cross section of a French's mustard bottle. That's your primer for what ultrasounds look like. It's pretty obvious.

What's exciting about ultrasound is that it has a huge number of applications, medical being one of the main ones, and it's inherently a very sensorimotor way for clinicians to understand what they're seeing. It's very challenging to use AI for this because in the medical field you generally don't have large data sets. There are many reasons Monty would be perfect for this kind of use case, but ultrasound is not a very intuitive modality, especially if you're first dealing with it. For example, the Thousand Brains Project mug—this is an image of it, a 2D plane going through the side of the mug and the handle. What you're seeing here is the top of the cross section of the handle. If you imagine a very sharp saw going straight through, this is what the top would look like. This is the top of the side of the mug, but you don't necessarily see everything you expect because you get acoustic shadows, where the sound cannot continue to propagate, so it's black behind this edge. You also get lots of artifacts, which can be caused by various reasons—if the sound wave gets delayed, reflected multiple times, all this kind of stuff. The image can hallucinate; as far as the probe is concerned, it thinks there are objects out there. These were some of the things we had to deal with when interpreting these images.

There were quite a lot of moving parts to setting this up. I'll talk you through the architecture diagram quickly. Over on the left, we have the ultrasound device. We strapped an HTC Vive location controller to it with Velcro. When you click the button on the probe, it captures what the probe is currently looking at and sends it to Monty with metadata about the depth and scale of the image. At the same time, the HTC Vive controller streams its location to two sensors on the floor, which then send them to the HTC Vive service, where it stores the current orientation and three-dimensional location of the probe. As Monty processes it, it gets the image and asks the Vive service for the data, then visualizes it so we can see what we're looking at. It will also suggest where you should move next. Monty can't move us, but it can tell us where the next best view for inference would be. It also visualizes how Monty is learning as we go. You'll see all of this in the demo. We wanted to give you a quick overview.

One thing to give you a sense of what the whole setup looks like: we had what was called a phantom. A phantom is used when testing ultrasound or learning how to use ultrasound, and is an artificial structure that can be scanned. You might have an obstetric phantom, but since we were dealing with objects that Monty has learned in simulation, like the YCB objects, we created a setup with a plastic bag filled with water. Imagine it as something like an amniotic sac with objects suspended inside, all set up with tripods so we could move around it. We placed the probe on the surface of the bag with ultrasound gel to ensure consistent signal propagation. The task was to determine what object was inside the bag. As Will mentioned, we tracked the position of the probe using base stations designed for virtual reality gaming. These trackers look static, but they detect invisible lasers from the two base stations on either side, which provides position and orientation in the world with good sensitivity. This was very useful for our task.

After obtaining the ultrasound images and tracking data of the probe's position, we needed to extract more information from the image. The processing pipeline starts with the environment class from Onet, which we customized to synchronize both types of information. In the data loader, we look at the full image and try to find a small patch on the surface of the object. We start in the middle and move downwards until we detect a significant edge, then extract a patch around that. From the patch, we detect point normal and curvature, and combine that information with the distance from the top to the patch and the tracker position to create a CMP message for the learning module.

In the demo, you'll see an iPhone/iPad app to orient you. We have the image, the depth we're targeting, and the gain, which controls how white the image appears. The app also shows the number of images captured so far, starting at minus two since the first two don't count. There's a manual capture button, and we are set to the default musculoskeletal type, which was best for identifying the hard objects in the demo.

A bit more detail on extracting the patch: there were several difficulties, such as acoustic shadows and artifacts. Sometimes the highest peak in edge detection is not the object, but an artifact, so we had to adjust settings and algorithms to get the correct patch on the surface. Next, we identify points on the surface and fit a circle through them, which tells us about curvature. We also estimate a point normal at the center, which informs the pose of the patch. This required further adjustments due to artifacts causing blue dots to appear in incorrect locations. Sometimes we had to limit the range for fitting the circle because surfaces like the brain are very bent. Eventually, it works reasonably well.

We then combine depth, sensorimotor data, and agent location. Looking only at the agent location from the HTC Vive, we get the shape of the bag. Adding the depth extracted from the patch and the offset between the probe and tracker, we get a shape resembling a mug. If you've seen it many times, you recognize the mug, but for a first-time viewer, it might not be obvious. It's not a tumor, but it could be many things besides a mug. Visualizing it in 3D allows rotation, and while it's hard to get a good screenshot, you can see the body of the mug and the handle.

It does look like a mug. We added more live visualization that we can run during an experiment to see what's happening. We have the input image and the extracted patch. You can see how we fitted a circle through that patch and a point normal that we extracted. This gets combined into locations and orientations in a common reference frame, relative to the world. Over time, as we move the probe, each blue dot represents Neil moving the probe to a new location and taking another picture. Over time, you have a collection of points that update our hypothesis space. Eventually, you have a most likely hypothesis and a few other possible objects, and hopefully, you recognize the object. In this case, we even recognized some symmetry in the pot meat.

Another feature we wanted was to show the user, the person operating this, where the probe is relative to the bag suspending the objects. The reason for this is that we also want to be able to direct the user where to move the probe. In the future, if Monty is used, you can imagine someone without much training in sonography being able to move the probe and receive direction about where it should go, enabling them to acquire high-quality images with guidance from Monty. The first step is showing where the probe is right now. Here, you see a visualization of the bag as a white rectangle and the probe as a blue rectangle. As the probe moves in space, its position in the visualization updates. Occasionally, Monty reaches a state where it thinks there might be a goal state that, if reached, could reduce uncertainty about what it's seen. This is called a goal state. The visualization shows a new blue arrow, telling the user to move the probe to that location. The blue arrow remains until the user moves the probe there. It's up to the user to decide whether to acquire an image there; Monty doesn't force them, but it provides guidance about where an interesting view of the object might be to disambiguate what's been observed.

Issues encountered, as Neil already showed, include the difficulty of working with ultrasound data. When Will and I first got the images, we were not optimistic about the week going well, but with Neil's expertise, we got something working. Coordinate transforms were, as usual, a huge pain to figure out. Tristan and Rami mentioned that good visualizations help, but we still spent a lot of time and frustration on that. Data streaming was tedious; in the Airbnb, IP addresses would randomly change, internet would drop out, and devices would go to sleep. The trackers kept falling asleep or batteries died. Communication was shaky between the four different components. We tried some AI-assisted coding, which sometimes helped with hacky solutions but sometimes did not. It tried to import a module called Monty Python and hallucinated some funny things. We couldn't resist the temptation to eat. Some of the dataset tried to make a case for why it doesn't matter if the hot sauce is filled or not; it was open, but that doesn't affect the ultrasound image.

We also experienced a magnitude 6.4, 6.2 earthquake at 6:00 AM that woke us up. For a couple of hours, we debated whether to go into the mountains because there might be a tsunami. Luckily, there wasn't.

There were also family emergencies, like kids falling down the stairs. One has a black eye now.

There are many interesting things to explore in the future, including the challenge of getting reliable features extracted from images. As Viviane alluded to, if the system picks up on an artifact and thinks it's the edge of the object, the representation is totally off. There are various ways to handle that better. Currently, we're just looking at ultrasound as images or edges in 3D space, which is important for how humans understand ultrasound images, but it's only part of it. We also look at the actual texture of what we're seeing.

Whether it's a lung, gallbladder, or other objects, they all look different, and that's true of the objects we scanned, but it's not accounted for. Curvature was something we didn't fully integrate. We measure it, but not in a way that allows comparison to simulated or learned models. We had ideas for ensuring we get the maximum curvature dimension, which works in principle but needs to be more robust. Some ongoing research, like Rami's work on resetting recurring hypotheses, would help with system robustness. We've collected many images along with positioning information and saved them, which could start a new dataset. As Monty improves, we can rerun it on this real-world dataset and see how it improves.

There was a tuna can in the dataset, one of the objects we practiced on. The most likely hypothesis is the tuna can, which is what's in the bag. On the iPad screen, you see the depth, the ultrasound image, and Niels holding the probe against the bag. The data gets streamed to the Windows laptop and the Mac, and Monty runs here. The setup includes a bucket to collect water potentially leaking from what's actually a urinary catheter bag.

Monty is actually running on the Mac machine. The Windows machine is only used for OpenVR, which tracks the marker. OpenVR doesn't support Mac and doesn't really support Linux, especially on a headless Raspberry Pi without a GPU.

Monty's capabilities are demonstrated by its shape-based approach to recognizing objects. It doesn't just pick up low-level features, which wouldn't be useful in this ultrasound data. It's about moving a sensorimotor in space and getting locations relative to each other, which Monty is uniquely designed for. There's also the gold state generator Neil showed, with a point that tells the user where to move the probe. Monty's Intelligent Action Policy uses internal models to suggest which view would best help recognize the object. It's not a random policy or a methodical scan like current ultrasound AI solutions; it's methodical about acquiring the information it needs.

In terms of long-term future applications, while it's not ready to be used now, improvements to Monty could make it a huge help for bringing ultrasound to parts of the world where medical imaging is hard to access. Ultrasound is amazing because it's handheld, can connect to a smart device, and is generally safe with no radiation. The real hurdle is that it requires someone with years of training to use and interpret it. Current deep learning systems require huge amounts of data and lack the sensorimotor spatial understanding needed to suggest data acquisition as we've shown. This approach could bring the benefits of ultrasound to many parts of the world that currently don't have it.

The cost of an ultrasound device is about a thousand dollars, while training a doctor costs around $200,000 per year. The device will get cheaper and is easy to distribute where needed. Medical data, especially abnormal data, is rare and hard to obtain. For example, scans of a heart with a tumor are rare, but you need a system that can identify a healthy heart and spot abnormalities without additional training data. While we didn't reach that goal in the hackathon, it was part of our stretch goals and would be enormously beneficial.

Should we start and go backwards in the water? Sure. Since we're already on mute, maybe we can play the Benny Hill music. I'll show the setup while Will and Neil get everything ready. Here's our Windows laptop, which communicates with the Vive trackers. The trackers are set up down here; there's one visible, and another over there. This wraps the tracker to the ultrasound device.

This indicates it's on and connected to the iPad. The setup was complex enough that we created a surgical checklist to ensure everything runs correctly. The Windows machine runs the server providing location information.

While we're setting up, Terry, do you have an object you'd like us to test on? What are my choices? We have the TBP mug, tuna can, mustard bottle, spam, a package of ramen noodles, hot sauce, a heart, and the little menta brain stress ball.

I think those are the objects. These are all in your datasets? Yes, these are the ones you can use. It would help if you picked an object that doesn't float. The heart and brain probably float. Does the mustard float? A bit, but we can try it. What doesn't float? The spam can, the mug, the tuna.

We also have some Issa sauce.

Let's go with the tomato soup can. We've never tested on that. The tomato soup can doesn't have a distinguishing feature and looks like the cup, so let's pick something else. The cup has a handle, which helps. Does the hot sauce float? No, the hot sauce might be a good choice because it has a different shape.

How empty is the hot sauce? You may need to hold it, but I think that's a good one. Let me fill up some water. Neil said he wanted to travel without hot sauce and ended up eating our dataset.

Not much control there, Neil. I was on holiday in America for two weeks, so it was a long time. America has hot sauce everywhere. When he said it doesn't matter for ultrasound, it was like an addict trying to convince us otherwise.

As things are shaping up, all three demo projects rely heavily on networking. That's an inherent robotics problem. It limits the ability to go to different places if you don't have good network connectivity because of all the data coming in.

If you were to design a full-fledged product solution, you could use cables to connect everything. This is more of a quick, hacky version.

We're going to start inference. Are you sharing your screen? Yes, I am. Did you calibrate the track? Yes. We've been through the text machine. Right now, you can see the probe. Sometimes it gets a bit laggy on Zoom because there are too many devices running. The probe is moving relative, and if you switch to the other view, you see the full ultrasound image on the top left, next to it the extracted patch and the features we extracted. To the side, you see the relative locations in the world that it estimated.

On the bottom row, you can see Monty's current hypothesis of what it thinks it is sensing. Almost all objects are still possible, but the most likely hypothesis right now is Monty's heart. What did we actually put in to change that? The hot sauce.

In our experiment setup, we have 30 steps to try and correctly reach the gold state. Gold state up here? Not yet. The ultrasound needs a lot of jelly and fluid to work properly; any air and it doesn't see anything. We're not completely sure that all the coordinate transforms are correct. There might be room for improvement.

Can you use Vaseline? Never mind.

What top hypothesis do we have at the moment? That's surprising.

It probably is the case that the coordinate transforms are correct because this should be doable. We just need a few more observations. Did we show the goal state? If you go back—no. It seems like the tracker probe is not calibrated, right? That's just for the human; it doesn't affect data collection. There is a suggested pose up here. If you rotate, that was before calibrating, so maybe it's really off. You can see the principles operating as well as the coordinate transforms that still need debugging. How many steps are we on now? We'll try to be a bit quicker.

On the ultrasound image at the top, you see zebra stripes. That's one of the artifacts that comes in. Are those reflections or something? It's bouncing back and forth and creating a false image at a certain depth. What did it categorize?

Monty heart. That's a shame. Next, we can try multiple times, or another object. Maybe you first demo yours.

If you also get it wrong, we can have a tiebreaker object. Can I head downstairs?

Maybe the last parameter changes didn't help because the way it was plotted looked off, but we did recognize the objects in the dataset we collected. This is the demo effect. The dimensions are quite different for the hot sauce bottle, so it should be doable, even with noisy point normals. For the next one, maybe we remove that last change to see if it broke something. Back up 50.

Is this your robot's camera?

No, it's not a robot's camera. Can you see it? His laptop is blurring things. He wants to blur the background. I'm going to do another thing here.

It'll keep seeing that unless it's turned off. It's definitely blurring. I'm joining with my phone and I'll do the same thing. Recording in progress. If it's on Zoom, it's under the video, the up arrow on the bottom left corner. There's a setting called Blur My Background.

It's not Blur. This would be blurred. Or Apple is applying its own thing. Then that would be the green camera in the top menu bar if you click on that.

I also did my phone view down here. My phone died.

The demo gods are not nice.

There you go. That looks better. Good. Maybe it's focusing on me.

Could you put the phone on the tracker so we get a first-person experience with it?

We need an object.

It's the same object: hot sauce.

Starting, what you're observing now is it's resetting into the starting position.

Already hot sauce is the most likely hypothesis, but Monty's heart is also present. It's neck and neck. Now hot sauce is in the lead, but Monty's heart was there for a few steps. Hot sauce detected. Good job. That's awesome. Let me share my entire screen; that will be better.

Pick another one. Can you do the mug? They already did the mug. Let's do something different. Spam. You can do the spam. Ours are pretty quick. Let me just turn off the share because I wanted to also see. There we go. It's still here we go again. It's resetting.

Terry, by the way, the spam can here is called potted meat can. Full name in the dataset. No product placement.

You can see the models in the upper right corner of the visualization. That's all that Monty knows. Interesting. That's sparse.

Between the two visualizers, it keeps saying global matching Step three. We didn't debug those logs. The camera is above the object, so it wasn't getting any sensorimotor input and was bypassing the learning module. Nice. There you go. Boom. What else do you want to identify? How about the Numenta brain? This one is a little different setup. You have the platform. I don't think this one works very well. Do you have the platform?

It is the top platform, right? I don't remember. Just put it on the top platform. We have an extra platform for this one because our sensorimotor didn't go far enough to see it on the platform we built.

and we also presume the platform is part of the model. Although we're not sure if this is the platform we used or if we used different ones. We'll find out.

Testing robustness. Here you go. This subject is very small, but the patch is sometimes just off the object, and for that reason it doesn't work. Hopefully it will not fling the brain off the platform either. It's pretty light, and we don't have many tolerances built in here—a flying brain.

I don't see Monty's brain in hypothesis space. This one might not work. We'll see. Right now the sensorimotor is above, so when you're seeing things like this, it's just not getting any data because our policy is purely random movements.

The time-of-flight sensorimotor works best with shiny objects. Objects that are not metallic don't give good data.

Visualization.

I think it's a tuna soup can. This is why it's tuna soup can—wait, tomato soup or tuna can, tomato soup. Sorry, tomato soup probably. You have an example of a failure mode. That's not bad for three tries. Awesome. Do we get a second try on the path? What if we get the brain? No, it's actually pretty good about the brain. We just put it on the bottom, right? Just normally. The heart—sorry, the heart. It's pretty good at recognizing Monty's heart, and you can see the image of its model of the heart. That's pretty cool. Do you want to try the spam can again? Should we try one more? You changed—hopefully have us. The checklist. I would say the prize for our highest definitely goes to you guys, but I want to have a little bit of success. Sounds good. I'll stop sharing as soon as this completes. In Monty, when you rotate, is it moving the agent around the circle? Yes, we are faking it—orbiting an object.

Monty's heart. So you're saying this setup would be something you will use a lot in the future for testing? That'll be a real-world test, and it's not made out of Lego. We can order one for each of us, and each one will have one at home. You can probably point on the open space there. We're done with this demo now. We'll try and do the po. If you have better left, anything timed out, get the track. You can see the meat can in the bag. Niels can actually, most of the time, not see it because it's not see-through from the other side. Calibration still looks good. The screen is also being shared, so you can see how the tracker's moving in real time, and then the iPad screen, seeing the corner of the can right now.

Maybe if we go to the hypothesis-based screen, make sure we're getting images.

You can see it does a decent job at extracting the patches, curvature, point normal, and all that.

Just a few things left to debug with adding it all together. What's in the back? Spam can. Can you tell from the images?

There you go.

Cool. The edge here, I think. What's the mlh mented mug? Are you kidding me?

I think you overfitted on the menta mug. So it's the patch right in the center. The patches are being extracted. It's not always in the center; it's when it first finds the edge. We lost the tracker. One second. We need to re—better off not having it, because otherwise it's going to change the location. Where did you use for the 3D visualizer? My put.

Getting that quick. The live one is B Python.

The sensorimotor location—I think that's a vibe thing. Part of the setup. Here we go. Here's a good goal state. Right now the probe is over here and the goal state is on the other side. The user actually moves it. But the problem is we lost one of the trackers, unfortunately. If we move it to a similar position, the tracker can try that better. Maybe it's back. That's pretty close. Maybe if you rotate it, I can see.

Once it goes into the actual point, the arrow disappears. So you see that once the arrow disappears, if you keep doing inference and get another goal state, it will do the same thing and create a new arrow. That's really cool.

How many steps are there?

We'll just collect a few more data points, see if we can—there's a can inside.

I think we learned that you over-interpret a lot of stuff if you know what it is, but a lot of it is just artifacts. What did start—oh man. It worked much better earlier today. We made the fatal error of trying some last-minute adjustments, and that may have thrown the system off, but that's how it is. If only we'd picked Monty's heart. Clearly we've gotten a hundred percent accuracy. Steps higher.

This is the number of points we collected just then, and the point normal pointing out. At least some of the learned models, like the potted meat can and the menta mug, we scanned with 200 points, and we've plotted those learned models and they look decent. Right now, curvature information isn't used at all. We're basically just matching the locations and getting a little help from the point normals.

Now we go to the scoring phase.

Stop sharing. Is Terry sharing her screen now? We'll give a few minutes to think. Harry, since you saw everything and you're pretty independent, if you'd like to score, I can send you a link. Let me find it again. Can you see the link filled out of mine? Hang on.

Yes, I can. Perfect.

You're going to put it in the chat to me? Yes. Do you want me to explain mine? Harry, use zeros for your choices.

Terry, go ahead.

Now, Harry might be a better judge than I am, but I'm going from my viewpoint, so it's pretty subjective. I understood more of what everything a is awesome team did. I thought they had their visuals match the image and the different perspective, the different angles. You could really see how it was going around. I understood their explanation a little bit better.

I thought their setup was a lot more condensed and easier to understand, which made it easier on the eye. I believe that "It's Not a Tumor" was a very impressive solution regarding the sonogram and the challenges of sonograms, as well as the future potential for remote medical diagnosis. That was impressive and the most ambitious. All three teams were very ambitious, but I think Vicarious had the most ambitious project because they lacked the equipment needed to advance further. If they had a more sophisticated drone, their outcomes would have been better. They spent most of their time trying to get the setup working, which took away from the rest of their project. Their project was more ambitious because they didn't start with sophisticated enough equipment. All of you encountered several problems and hurdles, and all three teams did extremely well overcoming them. You came up with creative solutions, and all three teams did a great job. The teamwork was also notable; everyone worked well together, and each person was a full part of their team and project.

I think Everything Awesome demonstrated Monty's capability because they were able to detect and figure out what the item was in almost every case except for the brain, which was exciting. All three projects make me excited about Monty's future because they have different applications. The Awesome project is something the whole group can use right now, so the benefits are short-term. I would award Everything Awesome the prize.

I think everyone will understand this decision. Thanks a lot, Terry, for the great ranking. I agree with all of that. Since Everything is Awesome is far ahead in points, we don't need to fill out this column because we can't catch up to them.

It's really impressive how much you accomplished. My brain hurts, and I don't even understand 90% of it. The part I do understand exhausts me. You did a wonderful job, and I really enjoy watching your team work so well together and develop these projects in such a short period of time. It's wonderful.

Thanks, Terry. Harry, do you have anything to say?

I'll make a few comments. I did not take notes, so I can't fill out things at the level of granularity that you did, Terry, but I concur with the general sense that Everything is Awesome takes first place. To me, what's most interesting is "It's Not a Tuna," because it shows how Monty could be used in very different real-world applications, as opposed to the other two demos, which are more classic—take pictures and see if you can recognize an object. Everyone's working on that, and that's fine. But the idea of dealing with ultrasound and its complexities is amazing; you got results even when looking at a sample image and its artifacts. Dealing with that represents fantastic promise, and I hope that's pursued. In addition to the classic ones, I have a question or observation about how these systems collect data and try to resolve it into a hypothesis. We do a good job understanding the world around us with one eye, but depth information from these devices is pretty crude, and that's a long-term feature compared to the resolution of the RGB camera. I wonder if including depth information actually makes the problem harder. Working with just a 2D snapshot, but moving it around from different perspectives, gives you all the information you need, I think. I'd like your thoughts on the pros and cons of using depth data given the technology deployed in these examples.

In humans, we have different heuristics to estimate depth besides binocular depth estimates. It's important in human vision to have depth estimates, and we do it early in the visual system, but it's not as accurate as lidar or maybe even ultrasound. It's a mix; you need some kind of estimate to tell how far things are from you, otherwise you can't know how a movement of your eye translates to movement in the world. An object far away appears much smaller than the same object when it's closer. Being able to deal with rough estimates is important, and right now we're decent at it, but not great.

Still necessary to a degree, but not to millimeter precision. In the "everything is awesome" case, the robot only moves up and down, and the depth changes because the object has shape, but the average depth is kept constant. For the ultrasound, it's kept constant with respect to the bag that it is in. There's not a huge variation in the depth value, even if you get it, so I don't think depth in those senses is very useful. If Monty were to admit a motor policy, and if we're far away, then it should have a policy to get closer. In that case, having accurate depth information is crucial so it can get closer to the object of interest and avoid colliding with it. Here, things are artificially set up so there's not going to be a collision between the agent and the object we're trying to identify.

You don't necessarily need an entire depth map. For example, in our ultrasound example, we are not using the whole depth image; we're just using one depth value at the center of the patch and then moving it. This approach is about asking, "How far away am I from what I'm currently looking at?" and then moving around that to build up the model, without requiring an entire depth map or integrating all that information.

If you have the capability and design to move the sensorimotor and orient it in any direction in space, then parallax information is as good as depth information gathered from the image. Given the resolution of those two different images—a depth map with today's technology versus a 2D snapshot—you may be better off using the 2D snapshot and the physics of parallax to figure out what's going on. Depth information can introduce noise and make things more difficult, but that's just a hypothesis. I tried SFM by taking 12 overlapping pictures and attempted 3D reconstruction from a bunch of 2D photos, which uses triangulation. The problem was that the image had a lot of backgrounds, and the features didn't really match—features from the floor tried to match, but didn't.

One of the nice things is that at one point we reviewed different theories for how depth information first appears in the primate visual system. It probably relies on parallax, both with binocular vision and motion. Monty is a sensorimotor system, and primates are sensorimotor, focusing on one area. In that setting, it solves many challenges that parallax faces in computer vision, where you're trying to match thousands of points in massive images. It's computationally expensive and ambiguous. The way our eyes are set up solves a lot of that. We don't have that in Monty at the moment, but it's a great point that it would be interesting to bring in, maybe at the next robotics hackathon. Intuitively, one always thinks more information is better, but given the specific technology, I wonder if that's true in this case. In the long run, if you had very high-resolution depth data, that would surely be helpful.

I have a funny story. When I first moved to California, I worked for a company called Biometric Imaging, run by a well-known guy in the valley who did spectro physics and a lot of lasers, mostly medical diagnostics. About six months ago, he contacted me because he saw my connection with Numenta. He's from India, his name is Bala Manion. As a child, he was in an accident and lost one of his eyes, so he's always worn a patch. He always wondered how he could drive a car with only one eye. He told me that the Thousand Brains Theory and another book, "How Do I Invent," were the two books he kept on his desk all the time, periodically picking them up and reading them again. He feels that Thousand Brains Theory explains why he can drive with one eye and thinks it's a great theory. When Harry mentioned one eye, I thought of Bala. It would be interesting to see if there are research papers about how people with one eye navigate and what deficits they have or don't have. It's probably been studied. Bala is a very interesting guy, and if you're ever around, he'd love to talk about it. He's very excited about it, does a lot of medical devices, and is very in tune with the gut-brain connection and integrating that information. I thought it was interesting that he had the same question as Harry. He's a smart guy, quite a character, and very entertaining.

I see Tristan, you're awarding yourselves the trophies. Everybody who hasn't seen these coffees, that's one as a result of the hackathon. You go around. Congrats. Thoughts on congratulations. Awesome project. Isn't there a spare spam can? I was thinking the team could open one and enjoy it as a treat. You can hand this to Hojae. There aren't too many trophies you can actually consume.

I'm not sure if there's actually still spam in there. I hope not; maybe it's filled with something else. I don't think there is. There are holes in them. If it is, I don't think potted meat is much of a trophy.