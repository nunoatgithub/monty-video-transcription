Okay. Over the Thanksgiving holidays, I was doing nothing and thinking about these topics on the plane. I had a couple of ideas—several things we've talked about before. They're not super new, but I thought they were relevant to our recent discussions. I wanted to put everything in one place. Most slides are about framing the problem rather than providing a concrete solution. Toward the end, I have some speculative ideas about how the brain might implement this, and we can brainstorm or use a shared whiteboard.

Just to remind you, I think I wrote about this in Thousand Brains: if you completely describe the problem, the answer becomes obvious. For me, accurately describing the problem is the most important thing. Hopefully, these slides will help spark interesting discussions and ideas. Why talk about object behaviors when we've recently focused on policies, decomposing policies, and goal states? All these policies aim to change an object or the world from one state to another. How an object changes from one state to another is modeled as an object behavior. To understand policies, we first need to understand how object behaviors are modeled. If we understand that, it will be much easier to talk concretely about policies and what learning modules might communicate to achieve goals.

I'll start by defining what an object behavior is, or how we might represent it. This is just my attempt: I would describe it as a sequence or space of states that can be navigated through actions or time. For example, a stapler can be in several different states. It can be open, closed, or in intermediate states, and we have a model of how it can move between these states. It can't just jump from open to closed; it has to go through intermediate states. We also have a model of how actions can move the stapler through this state space. There are other cases where we're not changing the object's location but its features, like turning a lamp on or off. Some states have a more continuous space, like a joystick, where you might need path integration properties to model navigation through that state space. There are also behaviors not affected by your actions, or that just change over time.

There are very complex behaviors of compositional objects, like a human running. All of them can be described as a set of states and rules for navigating from one state to another, whether by the passage of time, your actions, or someone else's actions.

A couple more requirements: behaviors should be transferable to objects with similar morphology. If I learn how one stapler works, I should be able to apply that behavior model to other staplers with different shapes, colors, or even slightly different concepts. I'll go further and say these behaviors are transferable even to objects that don't have similar morphology, or at least not much of it. Anything with a hinge, for example, might have a very different morphology, but if it has some similar part, the behavior applies to that part. For example, the behavior of running can be applied to a totally different object, like a banana. You have to have parts that move to define that behavior, but as long as there's some correspondence in morphology—some subset of the morphology graphs that can map onto one another—it works. A hinge could be present in many objects, and as long as the hinge-like part is there, the behavior can apply.

I don't mean you can only apply a behavior to objects of the same class, like staplers. You could also apply it to totally different objects. You could imagine a banana stapler, or recognize new combinations. You may have never seen a running banana, but if you see it, you can easily identify it as a running banana. That's one: behavior should be transferable to objects. Also, one object can have multiple behaviors. A human or a banana can be dancing, walking, or running. It always remains that object—it’s always a banana—but it can have all these different behaviors.

These two requirements lead me to say that behavior models are separate from feature and morphology models. Similar to how objects with the same morphology can have different features—like a coffee mug with various patterns or logos—we could have behavior models that can be mixed and matched with different feature and morphology models. This implies that different morphological models need to be mappable to the same behavior space, or at least a range of them. For example, if an object has a unique location space, or locations unique to that object, the behavior model needs to map onto all these different location spaces of different objects.

I'm just throwing this out there—similar to how we said morphology is more important than features, I might claim that behavior is even more important than morphology. Often, we define objects by their behavior. Chairs don't necessarily share a lot of morphology, but the way you interact with them is what matters. If you use slightly different language, you might ask which of these has precedence in a processing or learning model. It's hard to say what's more important, but if you recognize a potential behavior in an object, that might take precedence in processing or attention over something that just looks similar to something else.

One example is animating a usually inanimate object, like the lamp in the Pixar logo. It suddenly becomes something different—you give it a personality and can predict what it's going to do next. Lamps can have very different shapes, but what makes them lamps is that you can press a button and the light turns on. That's why I was considering this idea. It's just something I was thinking about. In terms of what we care about as agents interacting in the world, it's generally the behavior of things more than the morphology, because that impacts their affordances—how we can interact with them and change the state of the world. We're not just a classification system, so morphology doesn't matter as much.

A few weeks ago, I was talking about these topics and made a proposal that's a slight variation from what you proposed. I suggested that the way to think about this is you have models of location and features—features at locations. You could describe everything by saying any of these things can change: the features at a location can change, or the location of a feature can change, and so the entire model can change by just mixing those two combinations. For example, a red light turns to a green light, or a part of a stapler moves to a new location relative to the stapler. More generally, there are really only two things: features and locations, and any combination of those can change, suddenly or smoothly. That describes the entire space of objects of a particular object. Instead of saying morphology is less important than behavior, I think models can—as long as we agree it's the same model, the same object—anything can change in that basic idea: any combination of the model can change. I can give examples where features move, features change, or the orientation of features changes. So I can get rid of the morphology idea—it's just locations and features, and any combination of those can change. We can describe all the different things we're talking about that way. It's a slightly different way of saying the same thing, but it feels a bit more general purpose.

That's a good way of putting it. If you have one behavior of an object, all the features and all the locations could be changing while going through those sequences of states. Or maybe it's not even a sequence—maybe it's just sudden. The object could be, for example, the light could be on or off, so there may not be a sequence in between, or it can suddenly pop into a different shape. I'm trying to move away from the concept of morphology as a separate thing. It's really just locations and features, and any of those things can change in any way. Features have poses, and features oppose locations. It would be simpler than what you proposed—it would be a superset.

When I'm talking about morphology, I basically mean just the model of locations and orientations relative to each other. I know that, but I'm saying you could just replace the word morphology with features and say we have models, and by defining what a model is, anything that changes that model—any of the attributes—could be considered behavior or states of the object. It's all the same. I would agree with that. I was always bothered by the idea of a morphology model; it just didn't seem right to me. I think we can generalize it and say morphology is a word we use sometimes, but it's not actually necessary.

I'll keep using the word in these slides, but I understand the point. Last slide to recap the outline and requirements. The key ideas I'm proposing or repeating here are that there are three types of models: feature, morphology, and behavior. Feature and morphology can be in different states. State conditions determine which features will be observed at what locations. Depending on the state the object is in, you might see completely different features, and they might be in different locations than if the object were in another state.

The behavior model represents how to transition between different states of the feature and morphology models. I included this idea, though I'm not certain about it yet, that it would be beneficial if the behavior model had path integration properties as well. I agree with that. In fact, when discussing how the behavioral model might be in upper layers, I felt similarly. When you want to generate a behavior, you want to move an object from one state to another, just like moving your body from one location to another. You want to be able to do that along a path you've never actually traversed before. You want to be able to figure out how to get from point A to point B, even if you've never done it before. That seems like a property you'd want to have in behaviors as well.

The reason I'm uncertain about this is that I had a hard time coming up with an example of a behavior where you would have never observed a certain transition. It seems like there are only a limited number of behaviors, and we usually have a lot of exposure to them, even in the joystick example. If we can't find a mechanism that can do this with path integration properties, you could theoretically learn a detailed transition model of every possible transition of the joystick. Then, when you see an object with similar properties, like your arm—limbs are similar to a joystick—you can just apply the same behavior model to it. I couldn't think of an example where there's a totally new behavior that I haven't seen on a different object and couldn't just apply it.

With path integration, every edge needs to be observed, but a particular path does not need to have been observed. That's the difference between just memorizing a sequence. If you've observed every edge, you can follow a novel path using those edges. If you've never observed an edge, you don't know it exists and can't use it. I see your point, and I'll have to spend some time thinking to see if I can come up with a counterexample, but right now I can't. The reason I propose that path integration might work is more of an elegance argument, which could be wrong. The elegance argument is that it looks like there are motion-sensitive cells in the upper layers and in the lower layers. The ones in the upper layer seem to be for object movement, and the lower layer for moving through the world, sensors to the world. So, assuming the same mechanisms are at play, if we have grid cell-like mechanisms in the lower layers, we could have grid cell-like mechanisms in the upper layers. Path integration might come for free; it might be part of what grid cells do. My argument was more about elegance and simplicity of design, but those are often wrong. Your point about having trouble coming up with an example is valid, so maybe I'll try to think of some counterexamples, but I'll accept your point for now.

A general example might be if you have two states, like an intermediary state and a final state—say, filling coffee in the machine and then powering it on. You might have learned or experienced multiple edges into the first one and then multiple edges between that one and the next. This is where a sequence of two edges may never have been traversed, but you've experienced every edge and can combine them in novel ways. The point is, it's different from just memorizing the whole sequence, because as the sequence becomes longer, there's a huge space to explore. If you had to visit the whole sequence, you'd be more constrained in your behavior.

I can think of examples of solving problems in a very novel way, perhaps in a way I've never done before. Imagine I want to pour coffee into a coffee cup, but the top of the coffee pot is blocked. I notice a drain hole at the bottom of the pot that I can open, so I fill my coffee cup by opening the drain hole and letting it come out the bottom. I've never done that with a coffee pot, but I've seen other things where water comes out of a hole in the bottom. I'm not sure that's a counterexample, but it is an example of solving a problem in a novel way for coffee.

That example sounds like applying a different behavior to a novel object, like using the drain hole on the coffee pot. I had to figure that out; it's not something I've done before. Another example with the coffee is if, for some reason, I need to put the fluid into one vessel before putting it into the second vessel. Normally, I use the sink to fill it up, but in a situation where there's no water in the house, I use another source. That's following a different edge—I've filled up vessels with other things before, not just sinks. Although I haven't followed that sequence before, because I understand the edge of filling the pot with another source, I can follow a novel path.Let me talk about how we think about path integration in terms of grid cells and navigating an environment. The animal moves around and needs to get home, so it decides to go straight there, even if it has never gone directly from A to B before. This requires properties of path integration to know where you are after moving. In this case, the animal is still executing a behavior it has done before—moving in all directions, like compass directions. It has a repertoire of moving in different directions and applies one of those behaviors in a new situation, which we still call path integration. You might say it has traversed all those edges before, so it's not path integration, but we do call it that or something related, so I'm not sure why in other examples, when the same behavior has been applied elsewhere, we wouldn't call it path integration anymore.

When I wrote about path integration, I was thinking of the example where you move from A to B to C, but now you know how to move from C back to A, even though moving from C to A might be slightly different. I think that needs to be learned, though, because that is an edge. In an arbitrary space, you don't actually know if it's bidirectional; some systems only allow movement in one direction. For an object behavior model, you need to have learned or have all the edges emerge. It's not like a 2D or 3D space where you know how the space works—it's a more abstract state space. It seems like you have observed those edges. I'm not giving up on this because I think it's still a good idea.

What's important is that sometimes path integration is about achieving a goal. The animal wants to get home, to a safe place, or back to food, and it traverses a path through the environment that it's never traversed before. It can calculate how to get from its current location to the desired location. The equivalent in object state space or behavioral space is that the object is in some behavior, and you want to get it to a different behavioral state, even if you've never transitioned directly from state A to state B. Maybe you've reached state B from other directions, but now you're trying to go from a new location in behavioral state. For example, maybe the stapler is completely open, and you've never made a staple when it's completely open. What do you have to do to get the staple to come out? The point is, you could make the analogy: you're trying to go from behavioral state A to behavioral state B, and you've never made that specific transition before. You don't have to retrace your steps to get there; you should be able to go directly from A to B. That would be the goal. That's what I'm thinking about here—how to calculate how to get from one behavioral state to another directly, even if you've never gone there before. I just want to clarify that this is a little different from what you were talking about.

That makes sense. I'm not saying we don't need path integration; I just couldn't come up with a concrete example where we need it, and it seems difficult to imagine any kind of path integration in behavioral or state space, but maybe it's possible and maybe we can use the same mechanism we use elsewhere. I think it deserves more thought.

If we agree that path integration is about being able to make links that don't follow a single memorized path, then wouldn't you agree we need that? We can't just learn a single sequence and always use that. I agree—we wouldn't want to have to memorize sequences. We want to be able to move from edge to edge in novel combinations. What I meant with path integration is that I can't infer an edge I've never seen before, because I don't know how these abstract states relate to each other.

Why do the balloons come up? What happened? It's some kind of gesture recognition. Oh my gosh. That's a really good observation—you couldn't think of an example, and that's important. But again, I was starting with the idea of how the brain processes motions of the object, which I've argued are the motion-sensitive cells in the upper layers with restricted receptive fields. They would represent motions of the object. You have minicolumns of these motion-sensitive cells with the same directional properties, similar to what we see in the lower layers but with smaller receptive fields. This is the stuff grid cells require to build grid cell representations. I jumped to the conclusion that they're probably doing grid cell-like mechanisms up there, and if so, path integration comes for free. But that could be wrong. Maybe there are no grid cell-like mechanisms above, but then I'd have to understand how the upper layers process object motion, which I still don't understand.I think we can leave it as an open question whether there is a grid cell-like mechanism in the upper layer, in which case path integration would come for free, or maybe there isn't a grid cell-like mechanism in the upper layer. It just looks like there might be, so we need another hypothesis about how those upper layer motion cells are being processed and interpreted. Even with grid cells, what they're doing is allowing you to navigate over 2D space, realizing that these basically form edges with their neighbors, and when you're in a novel space, you're tiling that space with that behavioral model. At some point, you have to learn locally that there are these edges. There's not necessarily any setting where you can path integrate without having learned the behavior of edges, and that's true with grid cells. With grid cells, you must know you can go in different directions from your current position.

I don't throw away the idea of path integration just because I'm doing something I've done many times. At a high level, it feels like I have a problem to solve. I look at my situation and know how I want to resolve it, what state I want to be in, but maybe I never started in the current state and have to figure out how to do it. You can even argue this in the table setting example—maybe the plates and utensils are in a position you've never seen before, and somehow you're able to rearrange things to get it right. I'm not 100 percent believing these points; I'm just arguing for the sake of discussion.

That would be interesting to talk more about: how would we use a grid cell mechanism to represent state space, which is more abstract, with more abstract or constrained ways to move through that space? We talked before about how grid cells might be very general and able to learn such novel spaces that aren't 2D or 3D. They don't really know what they're representing; they just say, "Here's some movement, I'm going to represent it, turn it into a space," and whatever it means, I have no idea. It just starts with movement factors and says, "Okay, that's my space."

That brings me to the next part of how this might be implemented in the cortical column. I took all these requirements and constraints and have this big overview today. Over the past years, I've drawn all the anatomical connections that have been found. If you zoom in, there are quotes and figures from papers for each of these connections and references. I went through it and figured out which of these could work for behavior models, and also looked at some of our previous brainstorming notes. I'm going to show a very simplified version, but if we want to brainstorm more, I can pull up the full view and show the detailed connections.

To recap, we have the main feature input to layer 4, and then a movement or displacement input to somewhere between layer 5 and 6, and then an orientation input, both relative to the object, with orientation going right at the border of layer 3 and 4, roughly. We have associations that we learn between features and locations, and after successive movements, the activations here can be pooled into an object ID, which would be the main output of the column out of layer 2/3. The second output is the motor output out of layer 5A, at least in primates. We have these long-range connections, the voting connections, in layer 2/3, which usually vote on object ID, and also long-range connections found in layer 5E. What I'm suggesting is that those might be voting on the behavior model, since layer 5 seems to be involved with motor, behavior, and movement. It's layer 5B, not 5A—5A is the actual motor output to subcortical regions. Layer 5B has these long-range connections to other models. I have another side note on how this might be what we refer to as affordances.

If we go with this three-model idea, we talked before about layer 2 being a bit more sparse than layer 3, and maybe layer 3 is the general model ID and layer 2 is the specific object instance. One could also frame it as the feature model in layer 2, the morphology model in layer 3, and the behavior model down here. The three models add on to these three long-range connections. This is speculative.

Another possibility: a few weeks or months ago, I argued about what an object is. If all the features and their orientations can change, what's the commonality? The commonality is the location space. An object, not in terms of Monty, but in terms of grid cells or cortex, is where you anchor all your grid cells, and that defines a unique space you can move around in. That unique space defines anything within it as the same object. Within that, everything can change—the location and orientation of features, the features themselves, including changes in morphology, because when you move features, it's a new morphology.So in that case, one could argue that the layer 5b cells are voting on the space. It's like saying, in some ways, it's a class of objects—this is the space I'm working in, you should be working in this, we should all agree we're working on the same space, which defines a particular object. That was just an alternate behavior. I'm not saying it's right, just that it's another way of looking at 5b.

The problem I see with that is we can apply the same behavior to different objects. If the behavior models how locations can change, then it would be difficult to have—well, then I was saying that in the patient spaces. That's why I thought maybe the behavioral model is in the upper layers, because that's where we see those restricted receptive field movement cells in layers two and three. The behavioral model in the upper layer exists independent of the specific space. It's just, here's a behavioral model I can apply to cups, bicycles, coffee makers, whatever. In the upper layers, I have this behavioral model, and then I project to the lower layers to ask, given my current behavioral model, how would it impact the current object, which is in the lower layers.

But couldn't we have the location space in layer six, and then layer 5b is the general behavior model that can be applied to any location space of any object? When we remember, objects have to vote, and learning modules have to vote, and we've always said the learning modules are voting in layer two and layer three. This would be different; this would be saying they're voting on the object, meaning they're voting on what base object we're talking about. That would be layer 5b—they're saying, okay, we're all agreeing we're talking about coffee cups. We don't know what the state of the coffee cup is yet. Maybe the upper layers are voting on the particular state of the object. That would be an alternate. You'd still have all the same things here, just relocated. I'm not saying it's right or better than what you're proposing, I just wanted to point out that I was thinking about it slightly differently, and I don't know what's right.

I think that's a good point. I think it's actually quite similar to what—maybe, if I'm understanding it right—what you're describing. What I was thinking was being voted on here, and one thing I'm missing in this picture is where they would be voting on the state of the object. I'll talk about that a bit more. It would be like saying, we agree we're looking at—my hypothesis is I'm looking at a stapler because the local feature I'm seeing looks like a local feature in a stapler. But the different learning models don't know as a whole yet that it's a stapler or what state the stapler is in—open or closed. Each learning module says, I'm looking at a part of a stapler, but what is its state? They could vote on 5b, saying, all right, I'm looking at a stapler-like thing, and then you could be voting on the state of the stapler up above. It's all very speculative, so you're right, there are good arguments for both cases here. I don't think I can settle it.

One other thing we've talked about that would fit nicely with behavior being modeled in L5 is the amount of reciprocal connections with L6. It feels like it needs to have a huge amount of influence on the location representations that are emerging. I can make the other argument: with grid cells, you have grid cells that respond no matter what orientation you're in and how you're moving—they're just location specific. Then you have to have motion cells, which lead to grid cells, and they're highly interconnected. You have to have motion vector cells or velocity-controlled oscillators. One could argue—I'm coming back to counterarguments—there are good arguments for both cases.

They don't necessarily need to be mutually incompatible. Behavior could be modeled lower down, and state as a single thing could be higher up, which would make it easier to send the state up the hierarchy. If you want to influence compositional representations, that's useful to have in the superficial layers. The next thing you set up the hierarchy has to be the state of the object; it can't be just "stapler."

I drew two more connections on here that I have in my scientific neuroscience evidence diagram. There were tons of connections, but these might be relevant. There also seems to be a reciprocal connection here, which could work well if we say the morphological model ID is actually the state of the object—what kind of shape the object has right now. That has reciprocal connections to layer 5b. There's also connections between layer 6b and—I'm not sure if it's 5a or 5b, the paper just said layer five. You could learn how different states influence the locations that should be active in that object's reference frame. I put this in gray; maybe this is info about object state.

One caution is that when people talk about these reciprocal connections...Sometimes people report them, sometimes they don't, and one reason is the method they're using. A connection from one layer to another can be onto inhibitory cells or excitatory cells. If someone is doing anatomical tracing, they might say it ends up in layer five, but if they're measuring the effect of activating a cell in layer six and checking if a cell in layer five is active, they may not see it and conclude there are no connections. So just be careful about that.

The quotes I have here say that layer 6 cells appear to provide significant excitatory input to layer 5 pyramidal cells. So that's one direction. They say it's bidirectional—do we know if it goes the other way as well? Dendrites of many layer 6B neurons also reach through layer six into layer five. That's the anatomy. That could be onto inhibitory cells, you never know. That doesn't invalidate those results; it just adds more things to consider.

All those grey parts are from layer 6 to layer 5, the first one you mentioned, right? In the Thompson paper, she writes that the majority of intracortical excitatory input to layer 6 comes from the deep layers, like layer five.

So that's the other direction, then. It is reciprocal, at least according to those two pieces.

Just to throw out one more idea, and then we can brainstorm more: one side note on affordances. If we say there are behaviors folded into these lateral connections, affordances could be exactly the same—these lateral voting connections. Behaviors and visual models associated with motor behaviors are what I think of as affordances. For example, if a vision column has learned the pinch movement to zoom out on your phone, and we have a visual model of that behavior, that would be voting with a behavior model in the motor cortex of your fingers pinching. If I'm seeing that behavior, the vote would invoke it here, or if I'm invoking the behavior here, I would expect to see this behavioral change on the phone. That's what I was thinking—maybe that's how affordances are implemented.

Would this be for the vision model to almost enact that behavior in the touch model? That feels more like the kind of goal state you put in a project with L6. It wouldn't be the goal state. The vision model wouldn't have learned—pinching is maybe a bit of an abstract example—but the vision model can't know how the hand works or how it grabs a cup, but it knows what the behavior looks like. It knows how the hand looks when it grabs and can recognize that behavior. Let me give you an alternate example. Imagine the behavior is a door, like a doorknob, but instead of a knob, it's a lever, like a bar, and you rotate it down. The affordances—right now I'm looking at the door in the conference room, and it's got one of those levers. I can do that with anything: my knee, my shoulder, my elbow. There are lots of ways I can move that down. If that would be an affordance, it's not like you look at it and just think of moving it with your knee. My point is, I know that it has to move, and it's independent of what I move it with. It's not associated with a particular movement; it's a movement of the handle itself as opposed to how I'm going to move it. On the screen or display, you have to use your skin or that doesn't work. I picked something where it's completely independent of what physical thing I use to move it.

I would think of it as the doorknob and its behavior of twisting. That would be modeled in your vision column—you model how the doorknob can transition through the states of being twisted and turned, and how that changes the state of the door. When you see a doorknob, you have this affordance to grab it and open it with your hand, but that's just a voting bias toward that. It doesn't mean that's the only way you can do it. You still have the model that the doorknob can twist, and that's independent of how you actually twist it.

I could probably come up with examples where there isn't a preferred way of doing something, and then you don't have an affordance. I think that's just different, at least how I understand it.

Maybe I'm using the word affordance wrong. To me, the word affordance is something physical about an object that suggests how it behaves and how it could be manipulated. It doesn't suggest how I might actually manipulate it; it just says, for example, the stapler—I see a pin and two bars, and I say, oh, that's a hinge, maybe that's a hinge, and that feels independent of any way I might actually move the stapler. The affordance is just part of the property of the object itself as opposed to how I'm going to actually do it. Maybe that's different than the way you're using affordances.

In that case, that would just be in the model of the behavior itself that you're observing—how it can move based on some features you detect. That's how I was defining it. You may be defining it differently.

I think I was thinking of it a bit differently. Is there a technical definition of affordance? If I looked it up in the dictionary, what would it say? I don't want to use it in a way that other people are not using. Affordance is what the environment offers the individual.

That's pretty generic. I would say, like you're saying, Jeff, it's kind of a property of the object that's fairly independent of the specifics of how you interact with it. An affordance is like being able to hold fluid or something like that. In that case, the features on that model inform what behavior that model has—like, which behavior you would detect here would be informed by that.

Some learning module sees this feature of the hinge-looking thing, the pin, and it's like, oh, this could be a moving hinge point. This is more an example of how sensorimotor coordination works well. I was reacting to your first line, which says a forwarding curve encoded as lateral connection. I was using this term too narrowly. If that's the case, then the inferences can be encoded within one column of features. It can be encoded within the states of an object, observed state changes of objects.

The nice thing about this is we're getting closer to having the right terms, words, and components of the problem. I don't think we're there yet; we're still disagreeing about some things or don't know about them yet, but we're getting closer. That was another reason I wanted to put this all together—because we had a long Slack thread about it, and lots of terms were coming up. I thought feature, morphology, behavior, and states are key terms that are good to go over again. I'm going to push again to think about the argument earlier, which is that objects are defined by their space, and an object in theory could have any set of features or poses within that space. Using that as a generic idea—these are all the different things this object can be, and then how do we transition between them—is just a more generic way of phrasing it. I'm trying to get rid of that morphology term again.

I thought it was good progress to get rid of morphology. I wrote down a few open questions. One we already talked about: do behaviors need to be path integratable? The second one I was briefly thinking through, but I think not. Could rotation just be a specific type of state? Could we use the same mechanism—the rotation of the entire object? Like object rotation? I don't think so. What it could be is, if I have a compositional object—in some sense, all things are compositional objects—then the orientation of a feature is part of the state of the parent object, but the child object on its own would be independent of its current presentation to you.

I would think so.

I was thinking this is more of an engineering question as well. With rotations, we test different possible rotations, and depending on which rotation hypothesis we have, we expect the features and locations to be in different places relative to the body. A similar thing applies to state: if the object is in a different state, you expect features and locations to be different. You have to test different state hypotheses of the object. That goes into how we would implement recognizing the same object in different states. That goes back to the earlier point: we have to be voting on that somehow. It's insufficient to say, "these are all staplers." We have to say, "no, this is a stapler in this position." In a flash inference vision, I would see the state of the object, so we have to reach that by voting. Even a single learning module can be rotation invariant, and the way we do that now is by testing multiple hypotheses.

I think what you're saying, Viviane, is that we have to test multiple states. Does that mean there's some kind of symmetry between rotation and state? The way I think about this, we're relying on the thalamus to overcome the rotation problem. I don't want to have different models of the object in every orientation. A coffee cup in different orientations to my body shouldn't be a different state of the object. I don't want to encode that. I just have this model, and then it's a temporary problem of how to convert my senses and movement behaviors to the current orientation. That is a real-time compensation handled by the thalamus, whereas the state of the object is really a property of the object.

I would say it's more a property of the behavior. With the running banana, we don't have a model of the banana in every possible running position. We have a model of how this behavior changes the locations on any object. Be careful—when we talk about the running banana, we're mixing a behavior learned in one place with an object learned in another, which is fine. It's not an inherent property of bananas that they run. We'd have to see people running, learn the model that way, and then try to apply it. It's a great example because it reminds us that we have to be able to learn models of behaviors that can be applied independently to an object.

When I see the banana running, I don't think, "oh, there's a person running." No, it's a banana. It's something running, and it's a banana. I can't even imagine a non-describable object that's running. You could just look at the motion capture with the little white dots—the ping pong balls—and see a bunch of white dots moving and say, "oh, that thing is running." There's no object there, it's just running. You might imagine an object there, but still.

I think I'm agreeing with you, or maybe not. I think the first one might be yes, the second one I would agree with you, I don't think so. That relates to the big question I haven't really answered: what mechanism could the brain use to model general object behaviors that can be applied to many objects? If you had a space of states, that could be applied to any object because it's independent of the object. It's about how various pieces of the object are moving relative to each other, and it's independent of the actual object.

I'm still unclear: if every object has its own location space, how do you have a model that tells you how locations of features change and then apply it to different location spaces? I don't know yet, but it seems doable. It must be doable. There must be an answer to that question. Just because we don't know the answer doesn't mean we give up on it. It seems to be happening somehow.

That was everything I had. I don't know if you want to do more whiteboard brainstorming or just sleep on it for a bit and go into the minicolumns idea. One thing I wanted to comment on with the presentation: I thought it would be interesting to touch again on the L5 voting thing you were talking about. What you initially called affordances, but as you said, it's more about coordination. We might send goal states but also have some bias to a particular way of solving it. That's what you're showing: it wouldn't enforce a way of doing something because it's a lateral connection. It's not going to instantiate a particular representation on its own, but it could be that when this behavior is happening with my eyes, this behavior is happening with my head. That's interesting.

I'm talking about more complications here, and I might have alluded to this recently. When I think about what's actually going on in the brain, although it's possible for a single region to learn behaviors and learn objects, what typically seems to happen is that when we learn a new object, we learn it at multiple levels, actually higher up in the hierarchy. I don't want to dedicate V1 or S1 to some new thing. We build a model higher up in the hierarchy, maybe even in the hippocampal complex. It's temporary: here's a new object, I'm looking at it. When I want to actually interact with the object, I have to move back down the hierarchy and pick a specific way of interacting.It's, and then, as we discussed with typing and playing the piano, actions repeated many times can be learned at a low level in the hierarchy, so I don't have to move up and down the hierarchy. I can just type without thinking about it. But in the beginning, I have to think about it and choose which fingers to use, involving multiple hierarchical levels. I mention this because it's another complication when considering how to type or move a lever. In the beginning, I may not have a preferred way; I just decide the lever has to move or the button has to be pressed, and then it goes down the hierarchy to determine whether it's easier to use my finger, my hand, or something else, as opposed to a learned action. Only with extensive practice do you assign a particular method—like wanting to type a word and going directly to S1 and M1 to type it. It's just more complications in this process. In some sense, separating how I manipulate a new object from how I manipulate a highly learned object would involve different levels of hierarchical composition. This makes things much more complicated to think about for me.

I was thinking about these lateral voting connections, like voting on behaviors, because when we talk about goal policies, it seems difficult to have the same model that recognizes the behavior also output the actions to transition to a new state. For example, the vision column that actually sees the key going down or the light switching on doesn't project subcortically to whatever controls the finger; it moves the eye. It would be nice to have the spatial pooler behavior model of the lamp switching on in the vision column to recognize what's happening, but then have it associated with a behavior model in the motor cortex.

This gets back to the point I was just making: there's hierarchy involved in many of these behaviors, and they're practiced in different ways. S1 or V1 wouldn't be informing S1 directly. For example, if I walk into a room and see a lamp, unless it's a lamp I use every night and know exactly how to operate, I have to find the switch and reach for it. This involves a fair amount of the cortical hierarchy. You have to think about it; it's not mindless. You have to consider where it is and how to reach it. Some of these thoughts get complicated or yield different answers depending on whether it's a highly repetitive learning procedure or if it involves hierarchy.

Anything that goes within a primary region and between primary regions would be super highly learned objects or behaviors. Most examples like the lamp would fall into that category. Even the stapler probably wouldn't. Different staplers, you pick them up in different ways.

You can have a very learned model of the lamp turning on or the stapler—just how it looks—but then invoke different behavioral models for how you might apply pressure. I would argue that's always going to involve a hierarchy. It can't be done primary region to primary region, because at some point you have to decide that something has to move or press, and you have choices on how to do it. If there are choices, it has to go down a hierarchy to take different routes. Vision V1 can't connect to all the different fingers that might push the button. It's more like, "A button has to be pushed." In a novel situation, a higher level says, "What do I have to push the button with?" I can use different parts of my body and just pick one and go down the hierarchy. That might be more of a temporary environment model of where your body parts are right now. That's why I mentioned something extremely highly learned, like a lamp on your nightstand—you might just reach and do it without thinking, because you always do it the same way, with the same finger, and it always behaves the same way. You've done this a thousand times, so you might be able to do it automatically. It's similar to reading or playing piano. A pianist can recognize long sequences of notes as one thing, and their fingers just do it without thinking. The same goes for athletes. But for most people, we have to think about which finger to move and how to do it.

With the time we have left, I feel like Viviane, Jeff, and I have had a lot of chances to talk, but hopefully we haven't been dominating the conversation. I don't know if anyone else has anything they want to add.

There's a lot to take in. There's a lot of detail here on the particulars of the model, so I'm still processing and absorbing it. Is there anything that's unclear or that would be useful to get more details on? I don't know that there's any more details at this point. I've been thinking about behaviors as being closely linked to the underlying object model and the idea of abstracting the behavior away from the morphology of the model to make it more general purpose. That's a difficult thing to picture exactly how it would happen.

Agreed. It's hard to picture and think about.

We have to think about it. Next Wednesday, maybe Hojae will talk a bit about NeurIPS. The two Wednesdays after that, there are holidays, but maybe the week of January 1st we could do a research meeting on another day. I was thinking one thing that could be useful is to revisit concrete ways of doing behaviors. We talked about message passing in graphs, where nodes send messages to one another versus learning sequences. I could try to summarize some of that discussion, which might help us rethink possible concrete proposals. I thought that was interesting, especially the idea of learning general-purpose edges and nodes that can be reused in many different objects. For example, you might learn how a hinge works and then apply that hinge behavior to edges in all kinds of graphs. I think that was an interesting idea.

Go ahead. It's funny because at one point, Viviane said, when we get together in February—what are we calling it? It's not by the—maybe peer retreat. Retreat, yes, seems like an easy word to remember. Maybe we can make some progress on this whole object behavior and object state topic. In my mind, what we're doing right now is prepping ourselves by thinking about these problems repeatedly, which historically is what it takes. You have to keep working on the same topics, and after enough time, the answers emerge. I view this as preparatory to figuring it all out, maybe sometime in February. That's my goal. That would be great. But to do that, we have to sit here and puzzle over these things repeatedly. I've always found that somehow this works—you just rethink the problem over and over. There are so many ideas, and then somehow, in the end, it settles down. I think these meetings are very important.

We're making good progress, even though it may not seem like it. This is what we need to do to move towards a solution, so that's my goal—February, beginning of February. Maybe it would be nice to figure this out by then. At some point, I keep saying I'm going to give an overview of the connectivity and cortical columns. I could also just share my big paper at some point because today, when I was making these slides, it felt like a puzzle where I would go into that sheet, look at all the evidence and connections, and try to piece things together—figuring out which parts could be doing what, what properties the neurons have in the different layers, and so on. I think having that context in one place could be useful.

I agree. On the other hand, looking at that graph will never tell you the answer. Like all projects, you need empirical evidence and theory to inform each other. There are so many unknowns about the anatomy and physiology of the cells, and so many details that are relevant, but we don't know which ones matter. Looking at it alone is not sufficient. When we come back in, like today, with theoretical constraints and say, "It has to do this and that," then you can look at those diagrams and say, "Oh, yes, I remember there was a connection for that. That makes sense, and this explains that physiology."

In some ways, it's a highly unconstrained space—so many things are connected to so many others that you could come up with all kinds of theories. But it's nice to start with, "Okay, feature inputs are coming here, movement information is coming here," and that grounds it. As you say, you slowly build it up. It's this iterative process between theory and empirical data, which is the hallmark of all scientific progress, and we have to do both. We don't do the empirical research, but we have to read and interpret it. Over the years, many neuroscientists have said to me, "It's impossible to figure this out, you can't figure out how this works, it's just too complicated," because most of them don't think about the theoretical constraints the way we do. They think about it very narrowly. That's our unique value add here.

If we have a second, I'll give you a teaser of what I wanted to talk about with the minicolumns. Can I make one comment about the whole behavior model? What's been on my mind is how we can generalize behaviors to different kinds of objects. For example, pouring behavior to all kinds of cups, or sitting behavior to all kinds of chairs or flat surfaces. I think babies try out a lot of different things, and somehow they're implicitly learning the physics of how things work so they can predict what will happen, even when something is new. For an AI system, maybe behavior is something we can encode, or maybe we can keep an observation-action record: it did some kind of action, here is the resulting observation. The difficulty is to generalize that—if I was able to pour water into this cup, we want to know that it can also pour water into another cup.

That observation and action—somehow, we're not just connecting it between this specific cup and that specific action, but we're extracting that within this cup, there's a concave feature or a wall feature that makes it pourable. We need to associate that observation and feature not to specific instances, but to their general form at the feature level. I don't know how to do that, but that's how I'm thinking of actions. It seems similar, and Scott mentioned something similar: how do you separate out behaviors from objects themselves? We want behaviors to exist independent of the object. Of course, when you're thinking about pouring into cups, I'm thinking, "I could put fluid into a shoe," which is unusual, but both have a concave surface. That feature must be shared across cups and shoes. If it's an open-toe shoe, I know it wouldn't work. It's almost like a mental simulation where you play out what would happen, especially with a novel object. Eventually, you could learn by rote—things you could pour into—but you almost need to mentally simulate the fluid going in and how it will disperse.

It's more than just, "Oh, it's a container." It's, "Oh, it's a container that's waterproof or has no holes." A rubber shoe would be better than fine leather. All these things play out in your head as you're about to pour water into the shoe. Or a paper bag that's going to collapse—you can pour liquid into it, but it won't hold. This is the key: how do you separate behaviors from the objects in which you observe them? Stating that problem is a long way toward solving it, just knowing you have to do that. Assuming that happens on a column-by-column basis tells you a lot and puts constraints on the problem.

One interesting experimental thing that comes to mind is that each learning module is learning physics to some degree. That seems complicated, but I remember once doing a rotation with Kevin Smith, who was in Joshua Tenenbaum's lab. Tenenbaum's lab does a lot of work on intuitive physics in humans—how humans have this kind of notionBut what's interesting, I remember asking him if there is a region in the brain that, if lesioned, would result in people not having an understanding of physics. He said there wasn't a known one. People often talk about a face area or other specialized regions, but it really seems like physics is a very distributed function, which fits well with the idea that any model subject to physics is learned where it's needed. That's nice support for our general approach, where it's not that we have a totally separate part for learning physics and another for learning objects. Physics is an emergent property for most people, based on observation. To really understand physics, you have to take university classes for quite a while. You might know how something works, but it takes someone like Newton to figure out why it works that way.

It's almost like language: you speak your native language and use the grammar perfectly, but if someone asks about the past participle, you might not know what that means. The rules are not explicitly encoded; we just encode observation. Based on the hinge comment, composition might be a good way to address the abstraction problem—how a behavior could be applied to many different objects. If you identify a hinge, then through composition and identifying the objects that enable behaviors, you can abstract to many other kinds of compound objects. It seems almost necessary that to see an affordance or a possible behavior, there must be some physical attribute of the object that suggests that behavior. If I see a sphere, I won't assume there's a hinge and that I can open it. Maybe if I saw a seam around the sphere or something that looked like a hinge on the side. There are associations between behaviors and the physical properties of an object, but it can be done locally. You can observe a local feature that invokes a behavior model, but there has to be some connection—something about the physicalness. Hojae was talking about the shoe: vertical edges, an opening, or something else. There has to be some physical feature that connects the two, even if it's only a subset of the object, that suggests a behavioral aspect.

I think by what you're saying, Scott, it's true that it will simplify a lot—we will need far fewer specific behavioral models if we can compose behaviors into each other. For example, a running person is a collection of limbs moving in certain ways, instead of having to model each part individually. That reminds me of another point, as you said, Jeff: sometimes you talk about something, forget it, and then remember it again. I just thought of it again—way back, another By the Bay brainstorming session, we talked about how we can't really recognize behaviors in one column at a time. We actually need some flash inference to recognize the state of the person, because if we have a running person and only one sensor, by the time we move the sensor, that part of the person is in a different place. Especially for modeling states and behavior, you need a lot of voting to see the change in states.

If I'm looking through a straw, it might be hard for me to learn running. All I could see is one little piece at once. It might be hard to learn, but I'm glad you brought up that example again, Viviane. I also remember that we kind of agreed—you can't really learn this with the straw world. Maybe very simple ones, like if you have a tiny button and if you press it, it changes color—you can learn that behavior. But more complex and compositional ones, like if a stapler is opening and closing and you just keep going over it with the straw, you'll eventually be able to mentally map it, but it's a lot harder and would take many observations. Niels, you remember the specific dances—that's pretty good.

Scott, you were going to say something too. I was thinking about the composition of behaviors and that there might be multiple solutions for different kinds of problems. The hinge solution might not be the same as knowing that something holds water. For continuous deformable objects, it's less clear how to turn that into a composition. I was making the argument about thinking very conceptually about behaviors: is this any change in a model, as long as they're in the same location space? I'm trying to get around the issue, Scott, where you say it's hard to imagine this as being the same as that—containing water seems very different than a hinge. The answer is likely that the column doesn't make a distinction; it just observes a few basic things and says all these things are part of the same object because I'm in the same space. I haven't moved on to a different space. Somehow we can learn any of these things. That would be the nicest solution—to say we can come up with a hundred different variations of this theme, but they all fit into the idea that there's a space for the object, and different states represent different compositions of the object at that moment in time. That encompasses all the things we just talked about.

In the pouring water example, it seems more like it's a behavioral model of the liquid than of the cup itself. As Niels pointed out, we're actually simulating the behavior of the water and applying it to the cup, instead of the cup having the model of being pourable. If I see a bunch of cups in a cabinet, I know they're all suitable for putting tea in them. You eventually learn that, but I think you can only learn it if you've observed it in some object. Pourable water has to be observed in something before you can say it's that. You can't just simulate the water; it's the interaction between the water and the physical object. Initially, you see the behavior attached to a particular object—that's how you learn it. Maybe then you see another object and it invokes the same behavior, even though it's a different object, and you say, "I can apply this to this object," and eventually you can say any object with a set of properties might be pourable.

We're repeating ourselves. One quick thing I wanted to say, Scott, related to what you were saying: maybe some behaviors are almost like model-free versus model-based. Some behaviors are learned as an arbitrary sequence, and that sequence can be anything, learned by rote. Other behaviors are more like a general computation, where you're thinking about how it evolves. In some ways, it would be nice if it's just a smooth continuum, but maybe there's also an actual separation—if you're seeing a totally new object, you just learn this idiosyncratic behavior, but then there are certain things you can disentangle and generalize across different things. I have a lot of ideas, but we're out of time here. I didn't get to say my minicolumn idea.

Should I leave it for next time? It's up to you. I'm brief for a few minutes. I don't know if everyone else is. I'd be curious. It started with an interesting observation. When we move our limbs in different ways, we have to use different muscles. To extend my forearm is one set of muscles, and to retract it is a completely different set. It's not like I can just send a command to the muscles to move in a direction; I have to send "muscle set A contract" or "muscle set B contract." 

It occurred to me that minicolumns—one thing that always puzzled me about them—is that it seems like they represent movements in a single direction, and a different minicolumn represents movement in a different direction. When we think about grid cells, we want them to be independent of the actual movement that led to them, so there's this tension between wanting representations that are independent of movement direction and ones that are not. The thought that occurred to me, simply stated, is if we think about a minicolumn—it's very skinny, about 120 cells vertically—what does a layer 5 cell in a minicolumn represent? Initially in evolution, it could represent a contraction of a specific muscle. To affect a movement, you contract this muscle, and another minicolumn contracts a different muscle for movement in the other direction. That's not how it works today, but it could have started that way in evolution, helping to define a language of layer 5 cells.

When they invoke a movement vector, I used to think they encode some sort of movement, like moving in a compass direction, but maybe they're just individually saying "contract the muscle and move in this direction," or "if I want to move in another direction, I invoke another minicolumn." That's the idea—it was an interesting thought that evolutionarily it might have started that way. Maybe when we think about the motor output of the cortex, we should think about encoding movement output in this manner, with individual cells representing movements in individual directions, which makes sense.

So this would be movement relative to the body? It doesn't really matter at this point; it's just in a column, whatever the column is representing. That's not relative to the object reference frame. Well, it could be. If it was a learning module representing an object, it would be relative to the object reference frame, and then that gets converted to the body's reference frame.

But it would be initially generated relative to the object. I've argued that layer 5 cells might be doing the same orientation transpose that the thalamus is doing, so when it starts out in the cortex, it says, "I need to move this relative to the object," and then it gets converted—right before the layer 5 cells themselves convert it—to "move it towards this thing relative to the sensor or to the thing I'm moving." When I think about layers representing movement vectors being transferred cortically, maybe the right way is to think about individual cells representing movements in different, diametrically opposed directions. 

Imagine moving my finger in a circle; it would involve a whole series of minicolumns invoking in sequence. Each muscle has to be involved at different points in the circuit. It's a simple idea.

That's interesting. Do you know if there's—if they stick a probe in perpendicular to the surface of the cortex, they can see a very distinct response profile for the sensory ones. Is there anything similar where they've tested this? I don't know about it. I'm just wondering, because I can imagine if it was present, if you stimulated throughout the minicolumn of L5, you would get the exact same response, but as soon as you move over, you'd get a different contraction. 

If those movement cells are all representing an observed movement of the sensor in some direction, then the layer 5 cells would initially fire the same way, saying, "I'm part of that; I'm going to fire when we're observing movement in this direction," and then that layer 5 cell would associatively link to the muscle that actually made it move in that direction. If a layer 5 cell learns to control a muscle, that actually led to the movement observed in one direction. Later, without the input, you could say, "We want to move in that direction; I now know what muscle to contract." I don't know what cell fires. I lost your question.

It was just about the experimental evidence. There are a lot of things that are hard to decode, but this might actually be tractable. When they measure these cells, almost all experiments were done using sinusoidal gratings, so the animal's not really observing a real object. You see the layer 5 cells fire the same way as the layer 3 or layer 6 cells. They're all tuned to the same thing. If it's a motor output, it would be interesting if they actually stimulated layer 5—what would happen if they stimulated a particular minicolumn, and whether you see that same invariance within the minicolumn and then changes across minicolumns.

What would be really cool is if the animal is awake and observing something it knows, you would see the layer 5 cell fire before the observed change in the other cells. The layer 5 would generate the movement that then gets observed. That would be a great experiment. I'm not aware of anyone doing these things, because they don't really have theories about why you should do those experiments, so they generally wouldn't pick up on them, but maybe someone has. I'm not aware of it.

Anyway, that was the idea. I'm working on minicolumns, really trying to get to the core. It's another piece of the diagram that you didn't have there, Viviane. It's another piece of the puzzle to think about minicolumns as these independent units and how to interpret them as part of a greater whole. If you read Maumcastle's work, he actually argued that the minicolumn was the unit of replication of the cortex. He didn't argue the cortical column was; he said minicolumns are the computational unit. You take a bunch of minicolumns and now you have a cortical column, and it does something.

He had no speculation at all about what minicolumns did, as far as I know, but he said they're observable, they're physical, they're there, they have these common properties, and the cells in a minicolumn have certain common properties. Therefore, he said this is the unit of replication, and if you take a bunch of them together, you've got a cortical column and it does something bigger. I'm working on that right now. I think I have some interesting ideas about it, so hopefully before February I'll get to present those ideas, or maybe I'll just write them up at some point.

Anyway, that's it. It's just an interesting idea that these 35 cells could have started by actually activating individual muscles at some point in the past.