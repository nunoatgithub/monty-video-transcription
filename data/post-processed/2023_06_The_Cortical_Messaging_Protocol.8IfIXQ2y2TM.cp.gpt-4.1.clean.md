This presentation is about the common cortical communication protocol. It doesn't sound like much, but it's the glue within the Monty system.

So what actually is it? If you remember November 2021, we had a meeting where Jeff wrote on the whiteboard. We were talking about the cortical communication protocol, also called the AI bus. Everyone hated that name, so we didn't use it. We discussed how to build out an AI bus, which is a common protocol for how the components in Monty communicate with each other—how learning modules communicate, how sensor modules communicate, and so on. It was described as a platform, like when vendors sell sensor modules, robotic modules, and so on.

In the same meeting, there was a product roadmap, potentially looking into reference frames and TBT next, and options for what to do next, with AI bus research listed. I drew two circles around this, which turned into the Monty project. The mega project was also listed. The idea of this communication protocol was present at the beginning of Monty, and it's still there, even though we don't talk about it often.

Where do we actually need this protocol? Monty has three major components: sensor modules, learning modules, and motor systems. Communication within Monty includes sensor module to learning module, learning module to learning module hierarchically, and learning module to learning module through lateral voting. Hierarchical communication can also go top down, from the top to the lower-level learning module.

Finally, there is learning module to motor system communication. Communication between Monty and the world includes world to sensor module and motor system to world. We want all communication within Monty to use a common communication protocol—a common format for what is always communicated. These two are the interface between Monty and the real world. The main job of the sensor module is to convert raw data into the communication protocol. The motor system converts the CCP into motor commands, like muscle movements or moving actuators in the real world.

Why do we need the CCP?

This figure that Jeff made nicely illustrates how we want Monty to be a flexible system, where we can plug and play individual components as needed. We want to have as many learning modules from different modalities interacting with each other. We want to be able to stack them to create a deeper hierarchy. Parallelism should be possible, and that's only feasible if all components have the same interface, allowing us to combine them into arbitrary architectures depending on the problem.

How do we define the CCP? In the code, all Monty components expect instances of the state class as input and are expected to output them. This enforces that all messages passed within Monty contain certain attributes or information.

State always has the same attributes and is interpreted differently in different contexts. For example, a state output from the sensor module is interpreted as an observed state. The learning module output is a hypothesized state—identifying which object is present and its pose. Learning module votes, or lateral outputs, are all possible states—a list of state instances. The motor output is a target state. For example, a hypothesized state is the output of the learning module, an observed state is the output of the sensor module, and a vote is a list of possible states. They all have the same attributes.

What are these attributes? Each state communicates a location and orientation, which is the pose in a common reference frame, for example, relative to the body, and features at this pose. Features are optional and can be modality specific.

Additional information used for routing and weighing includes a confidence value, indicating confidence in the location, orientation, and detected features; a sender ID; sender type (learning module or sensor module), as they are treated differently in some cases; and a Boolean indicating whether to use this state.

What are the attributes of state in detail in the code? Location is explicitly expressed as XYZ coordinates relative to the body. Morphological features are a dictionary that must contain pose vectors—three orthonormal unit vectors encoding orientation. Together, these represent the pose. "Pose fully defined" indicates whether the orientation is fully defined or if there are symmetries.

Non-morphological features are also a dictionary, optional, variable in length and type, and can be modality specific. These features don't change when the object pose changes—for example, color, curvature, or temperature. Confidence is between 0 and 1. "Use state" is a Boolean flag. Sender ID is a unique string for each building block of Monty, and sender type is either sensor module or learning module.

There is no information as to whether it's visual, auditory, or tactile. It's completely independent of that. Non-morphological features might be features detectable only by vision, like color, but it's always the same format and attribute types, independent of modality.

Even the features, to the receiver, don't really matter. It might be color, but in the brain, it's just patterns. You don't know exactly what it represents, and it doesn't matter. We were talking about this morning—one of the most amazing things about brains is that neurons don't know anything. They don't know what they're looking at or what the data represents; they're just cells, and the system has to work with nothing knowing the nature of the data. For brains, each neuron just has dendrites, connections, and learning.

It sounds simple, but making a system work this way and understanding it is actually very challenging. I still struggle with it myself. Since November 2021, we've been implementing all the Monty components to figure out what we actually need to communicate between them and to fine-tune this information. It was clear from the beginning that we wanted to communicate features and poses, but whether this all works and whether we can actually use this format for all the different types of communication within Monty really came together nicely in the past months. This is really nice. I didn't know you were doing all this.

I can go to the last slide with what we can do now, but go ahead if you want to say something first.

Given this protocol, we can now flexibly combine Monty components and generate arbitrarily large and complex architectures.

We can use the inputs to learning modules to model sensorimotor input using reference frames. This isn't directly related to the CCP, but we needed to make sure we had all the information in the CCP to do this. Given the features and poses, we can perform fast learning of structured models and fast, robust inference on object ID and pose based on arbitrary movement over the object.

We can vote between models of different modalities. We can combine multisensory models of object components into compositional objects and scenes—that's the hierarchy work we've been doing lately—and use learned models to guide movement and interact with the world efficiently. That's work that Niels has been doing. That's it.

Any questions? I didn't know all this was going on. Maybe you worked on this in the context of the patents or something, like they just got today. They implemented this a while ago and have to show all the details. If anything, we see it like this, and it's really quite amazing to me. One way to think about Monty and the brain is that we always think about AI systems or vision systems processing visual, auditory, or tactile data. But what's exposed here, and one of the strange lessons of Thousand Brains Theory, is that the brain is really a processor of space. It processes space—the data type of space, pose, and orientation. The vast majority of what's going on in your brain is processing reference frames, spaces, and distances. It's just a process. What we typically think of as sensory input is, obviously, important, but it's a feeder into the space processor. We use this to figure out what's going on in all these positions of things in the world and how you know where everything is in relation to how things move. It's a very different way from traditionally thinking about both computing and AI, and I think it's really cool.

It's interesting to think about transformers because, with language, it's just one need, but they had to put in a notion of position overlaid on the feature to get it to work. The more recent stuff uses a cyclical location, not an absolute location. Going back to what Viviane presented, the nice thing is that we're laying a foundation for building a sensorimotor backbone for communication. Starting on that premise, instead of slowly discovering it through various AI projects, is a much more fundamental approach to the problem. I think that's something unique, and it's really cool. The idea that 3D position and orientation could be elaborated over time serves exploring sensorimotor space, but I think it could generalize. It could be in any space, even an abstract one. In fact, if our communication protocol defines three-dimensional space, that would be a limit—brains don't exist with that restriction. We define that it has to contain a location and orientation, but it doesn't have to be three-dimensional. I thought you said three-dimensional, but I think brains discover space. Neurons don't know about space; other parts of the brain might, but the neocortex neurons just have to discover what the world looks like. But it really looks like that's what they're doing.

The fundamental thing is that they encode independent dimensions.

You need to be able to path integrate through that space. It works the same in, for example, 2D Euclidean space, as long as you can do location and rotation transforms.

Anything you feel is missing in the CCP right now?

We have some exciting developments in the pipeline. Viviane briefly touched on the motor goal state. We're hoping this will unlock hierarchical action policies, where different learning modules can pass goal states to each other and enable subgoals. Currently, the policy is still quite simple, and much of this is still in the conceptual stage. The CCP will remain unchanged, but we're considering additional connections in the network, such as top-down connections. As Niels mentioned, the motor command connections are not really used at the moment.

Another topic discussed in the research meeting is that, as part of the features communicated, we might also include object states and object behaviors, but these can still be framed within the CCP. The goal state is interesting to consider—what is the difference between a goal state and an object? A goal is something you can reason about, manipulate, and decide which aspect you want. It's not that different from an abstract concept. The main idea is to unify this so that goal states are in the same form, essentially equivalent to the states a learning module can be in. A learning module can express what it sees or receive a goal state describing what state it should be in. Based on that goal state, it can output other goal states to other learning modules or basic motor actions to achieve it. This allows for hierarchical action policies, where a larger goal state can be decomposed into subgoal states and passed down the hierarchy.

A goal state could be, for example, "I want the sensor to be in this location and orientation" to gather information to recognize the object. In the future, it could be "I want this object to be in that state," to manipulate the world, not just sense it. Here's an analogy: the internet is based on a communication protocol—IP addressing, packets, and so on. It was designed robustly enough to enable everything we do on the internet today. It's gone through a few revisions but has mostly held up intact. I think the work you're doing is as fundamental as that. In the future, if it's not this protocol, something very much like it will play a similar role and be just as important. It may sound bold to say that now, but it seems inevitable to me.