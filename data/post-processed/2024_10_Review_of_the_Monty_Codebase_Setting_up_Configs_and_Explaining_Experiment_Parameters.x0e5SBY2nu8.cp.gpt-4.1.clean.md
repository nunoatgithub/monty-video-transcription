All right, let me share my screen.

I'm hoping this will be interactive, so interrupt me if something is unclear or you have a question. Some of this might already be familiar, so let me know if I'm going into too much detail. I'll start with a brief overview of the codebase and what's included, then we can cover different topics. This is the current version of master; I updated it earlier today. There are a couple of folders that start with a dot, which you can ignore for now.

We have the docs folder, which is currently not used. HTML coverage is a nice way to view test coverage for our codebase. I use it to see which parts of the code are not covered by unit tests.

There are README docs, a folder for RFCs, unit tests in the test folder, and a tools folder. I don't think I have HTMLConf, which is fine. It seems grayed out, so it may be ignored by Git. You might need to run a command to generate these. You have to add the coverage flag when running PyTest, so it's pytest --cov or something similar. If you look in CircleCI, you'll see the job uses PyTest with coverage tools. The command is pytest --report=html, and running that will generate the report. Let me open the coverage report quickly; it's a useful tool.

You can view each file, and it shows a coverage percentage. For example, if you look at object_model.py, it shows in green what's covered and in red what's not covered by unit tests. If you add new code, this is useful because sometimes you forget an else statement isn't covered, and this will point it out.

The two important folders right now are projects and source. Projects is a folder that tomorrow we'll move to the Monty lab repository.

In the projects folder, there are random files for different projects and ideas we've tried. The files here are not covered by unit tests and aren't actively maintained, so many scripts might not work anymore. When someone makes a change, they're only obligated to keep everything in the source folder working. There are a bunch of Jupyter notebooks and other files that aren't always updated.

None of the notebooks will work because I haven't updated them. After the big action refactor, there's a lot of broken code here, and I stopped updating the projects folder except for Monty runs. This is more of an archive at the moment—code we used to run and where we looked at specific features, like speeding up the code or object behaviors. The important folder that's maintained is the monty_runs folder. In there, we have the run script, run_parallel script, make_followup_experiment script, and the experiments folder, which contains all the different experiment configs.

The important ones are the benchmark experiments and the pretraining experiments. Pretraining experiments generate the object models and are run only when something changes in how graphs are learned. The last time we ran this was about a year ago, and the models are on our infrastructure for download. Benchmark experiments evaluate these models.

Tomorrow, we can go into this more, but the projects folder will move, and we'll probably keep the monty_runs folder. We won't have it nested in a projects folder anymore, but we'll keep the benchmark and pretraining experiments here. This way, we can easily rerun benchmarks when something in the code changes. The idea of benchmark experiments is that any time we make a functional change to the code, like changing the learning or sensor module, we rerun the benchmarks to ensure performance isn't degraded or there are no unexpected effects. We might also run a few for sanity checks when something changes. In our documentation, we have tables with current benchmark results, and if a functional change affects these numbers, we update the table.

The main two are the 10-object experiments, which are faster but only run on 10 objects, and experiments on all 77 YCB objects, which take longer.

We also have benchmarks on real-world datasets and unsupervised learning experiments.

As the documentation is now in GitHub, you could update the scripts to automatically update the tables, making them part of your pull request. We've always had this table in the README on GitHub, and there's a to-do item to write a script to automate this process.

One thing to figure out is that we currently run these experiments on our Numenta compute cluster, but external contributors don't have access and may have different hardware, so it's unclear how to keep runtimes comparable.

We might want regression tests that run at regular intervals to double-check this, but that's still to be determined.

Let me briefly go over an experiment config. Scott wrote some helpful tutorials on this, so I recommend reading those, but I'll give a brief overview here as well.

In the beginning, we define a couple of variables that use some utilities, such as sampling all possible rotations in 3D. For example, one interval samples all combinations of 90-degree rotations: 0, 0, 0; 0, 0, 90; 0, 0, 180, and so on. We specify where the models are stored and which pre-trained models to use; currently, we're using version eight. The comments outline changes between versions, which is important when we modify how graphs are built and learned, requiring us to rerun pre-training. This is not a common change, so we track it with version numbers.

Regarding the input channel, this was introduced when we implemented hierarchy, allowing a learning module to provide input to a higher-level learning module. Now, a learning module can have multiple inputs, receiving data from a sensor module or other learning modules. The input channel indicates the source of these features, whether from another learning module or a sensor module, so they can be matched appropriately in the graph. For example, when debugging for 1LM, 1SM, an input channel might be "patch" or "viewfinder," corresponding to sources from the sensor module.

Usually, the viewfinder is not connected to the learning module, so it doesn't send information to it. In these graphs, there's no viewfinder information stored.

Jeff recently mentioned that learning modules don't really know where their information is coming from—whether it's another learning module or a sensor module. He noted that it's useful to know the source, but in the future, we'll aim for a system where the learning module doesn't distinguish between sensor and learning module inputs. Essentially, a cortical column receives input from different sources without technically knowing the origin, but it has connections that differ depending on the source. If something comes from a sensor or another column, it forms physically different synapses. In the code, we're representing these synapses with dictionaries and mapping them with keywords, but the information is handled in the same way regardless of the source.

We also define parameters like minimum evaluation steps, specifying that we want at least 20 steps before making a classification to avoid early false positives. The comment suggests this may not be necessary anymore, especially if better policies are implemented.

Next, we set tolerances. Tolerance means that if the difference between the sensed feature and the feature stored in the model is within the tolerance, we add evidence for the hypothesis. If it's outside the tolerance, we don't add evidence, or in the case of location, we subtract evidence.

We can also weigh features differently. For example, with HSV, we weigh the saturation value by 0.5 because it can be influenced by lighting conditions, and we want robustness. This may not matter now since we're not testing under different lighting conditions, but it's a feature that allows us to adjust reliance on color, curvature, or object morphology.

For tolerance values, HSV values are all between 0 and 1. Hue represents the actual color, so a tenth of the circle is the tolerance. Principal curvatures in log scale are different; they can get very large and encode noise, so we take the logarithm, making lower values more interesting. A negative tolerance of one was found reasonable based on past experiments. I have figures and plots on this that I can share later. For example, a histogram of the first and second principal curvature shows distributions that, when logged, fall into the -7 to +7 range, which is more reasonable. The scan may look odd due to riffles, but the tolerance of one makes sense in that scale.

Good questions.

Okay. So this is the config for the learning module, and by default, we're using the evidence graph learning module. If you go to the class, we list out all the parameters with some text around them. I'm not going to go through all of them now because that would take a long time, but if you want to understand it in more depth, you can read through all these parameters.

We define the tolerances again, including the tolerance for distance, which is about one centimeter, some threshold, and how many neighbors we want to look at. There are more parameters for the policy as well, specifically the hypothesis-driven policy, where we jump to locations based on the models we have in memory and the hypotheses we currently have.

For this one, I had a question about what X percent threshold meant.

That basically means, if we go to evidence update threshold, an object whose highest evidence is greater than the most likely object's evidence minus X percent of the most likely object's evidence is considered a possible match. Every object and every location and orientation on that object has an evidence count. We look at the highest evidence count across all objects, which is the most likely hypothesis. There might still be multiple possible hypotheses, so we take the highest evidence count, subtract, say, 10 percent, and everything within that highest evidence minus 10 percent range is considered possible. For example, the terminal condition is that there's only one possible object and one possible pose for that object, and that's what that range is used for. Hypothetically, if I set 100, then all the objects will always be possible, unless they have negative evidence.

The related question is about evidence update threshold, which is a little above. This determines how to decide which hypothesis should be updated. It can take either integer, float, or these strings. For example, if we set X percent threshold, we only update the hypotheses that are still possible. This saves us time. If we do "all," we update the evidence for every possible hypothesis at every step, which gives the most accurate results but takes the longest because we have to do all these matrix multiplications at every step. If we do X percent threshold, we only take the hypotheses that are still considered under this threshold. If we do "mean," we update all the hypotheses where the evidence is higher than the mean evidence. If we do "median," we update all the ones where it's higher than the median evidence. If you ask for intent float, that would be an absolute evidence value. For example, if your evidence is bound between minus one and one, you could pass in 0.8 or zero, and it would only update the hypotheses that have positive evidence.

Is there a bound for evidence? It can go negative. I thought it was zero to a hundred, but that's not correct. The upper and lower bounds can go infinitely higher. At any one step, the evidence that can be added or subtracted for a specific hypothesis is between minus one and two. This is because we have evidence for morphology and evidence for features. Morphology can add and subtract evidence, and that is between minus one and one, and features like color can only add evidence.

For example, if I've already learned a model of a red cup, I can still recognize a blue cup based on morphology. The blue is not going to subtract evidence all the time, but if I already know about a red cup and I see red again, that biases me towards the red cup model. That's why it's minus one and two, because the features can add another up to one evidence to that. We have these past and present weight parameters, and by default, they're set to one. That means we take the evidence that has accumulated so far and add the one from the step, which can be between minus one and two. The evidence can grow infinitely if the past weight plus the present weight add up to one. If we set this, for example, to 0.8 and the other to 0.2, then the evidence is bound to minus one and two.

We can do that to bind the evidence into a range, and that works, but it's not as good as having them both set to one, because then we have infinite memory. If we set them to add up to one, we forget about things we've seen a long time ago. In the future, it would be nice to have them add up to one and have the evidence bound, but for that, we need more efficient policies so we don't need to take 500 steps to get to the other side of the mug. That only makes sense with more efficient policies.

This is clarifying a lot of things, so thank you. I noticed I didn't update Eh in those two places, so I just made a pull request to fix it. It's just in the docstring; it's fine everywhere else.

What's your color scheme? I really like that the names are popping up in red. Mine is all yellow. Where do I see that? I set that a long time ago and thought it looked cool. Color theme: Monocai.

We define a couple more things in the configs, some specific to the surface or a distant agent. For the five learning module config, we essentially copy the config for one learning module five times.

and then put it all into one dictionary, learning module zero through four. Then we define some things for the sensor module, specifying which features to extract from the raw observations. Each sensor only gets a small patch at any time. From this patch, we extract a couple of points in three-dimensional space, like a point cloud, since we also have a depth image. From this patch, we also extract a pose, defined by the location of the center point, the point normal, the two principal curvature directions, plus features independent of rotation and location, such as color or the amount of curvature.

The features extracted in these experiments are pose vectors, whether the pose is fully defined, whether we are actually on the object or not, HSV, and the principal curvatures, logged. For the surface agent, we extract a few more features for the policy to work, including minimum depth, mean depth across the patch, and object coverage.

For some experiments, we add noise to test robustness. This is not required for the algorithm to work, but we define how much noise to add to different parts, such as locations, color, and vectors. The location value, like 0.002, is in centimeters. Max graph size could be 0.3, which is 30 centimeters. 0.002 would be about two millimeters. In the benchmark experiments, we see how the noise looks. The distance units are specified here, but the algorithm does not need actual units to work. The distances are coupled to habitat, mainly as a reference. When performing actions, all distances are unitless, and units are only relevant when relating to ground truth in habitat or for a real object. We do not need distance units for this to work. In abstract 3D spaces, we still have distance, but not necessarily units. It does not matter if one is a centimeter instead of a meter; the system works the same. Tolerances for matching can be updated based on model scale. If the tolerance for location is 0.01, there cannot be much noise in the sensed locations; it must be exact to the millimeter. Tolerances can be scaled by the size of the models in memory.

All of this is coupled to a scale in the environment, but the distance itself is not.

The tolerances are defined in the configuration. We could determine them automatically. Distances are closely related to tolerances, which specify how much distance is allowed between an observed point and a point in the model.

There are coupled arguments, such as distance and evidence threshold. If we set the weights to sum to one, using an evidence threshold with X percent means anything within 20, but if the range is minus one to one, everything would be considered. If the threshold is set to two, it does not make sense. If feature weights are set above one, such as weighing Q by five, the evidence is not bound between minus one and one anymore, but between minus one and five.

It takes time to get a feel for these parameters and how they relate. Automating parameter settings could help, such as setting tolerance based on graph size or sensor noise. Fewer hyperparameters reduce experiment space and prevent running ineffective experiments. Most values in the benchmark experiments are optimal for now. If you adjust parameters, read the doc strings to understand their function and interactions. Misconfiguration is possible if settings do not correspond to sensor module capabilities.

For example, specifying extraction of a feature the sensor cannot detect, or setting tolerances that exceed the sensor module's resolution, can cause issues. If the sensor cannot resolve the tolerance set, it will not work as intended.

If you specify an additional tolerance that isn't being sensed, it doesn't matter because it won't be checked. For example, with the distance parameters, like location noise, we're saying not to add noise to the location. We define the noisy sensor module and pass in these noise parameters as a dictionary. We also define a feature change as M, which means we only pass a sensation from the sensor module to the learning module if the sensor module detects a significant change in features or locations. If nothing is changing, such as moving a small distance on a flat surface, we don't send that observation to the learning module, since it won't provide much new information. We wait until reaching an edge, where the curvature changes, and then send that observation to the learning module, which can then use it. This makes the process more efficient. For that, we have delta thresholds; for example, we send a new observation once we've moved one centimeter.

We have the same setup for the surface agent, with similar config and noise parameters, except surface agent SM is set to true.

Should we also define what constitutes a significant change in color? We have a threshold by distance, such as moving at least one centimeter, but for hue, if the color changes from red to blue, that could be significant. We could specify HSV and set a small value for hue, ignoring saturation and value if desired.

I have a question about the correspondence of feature change in the sensor module to anatomy. Considering phenomena like saccades, where the image remains stationary and perception fades, does desensitization happen inside the sensor module or in a cortical column? It seems the sensor module implements desensitization, but does this occur anatomically? Habituation can happen in primitive animals without a neocortex or cortical columns, so it can be implemented early and doesn't need to happen in cortical columns. I'm not certain it always happens at the sensor level, but it's feasible to do so early.

I'm not sure if habituation works on such a short timescale, like moving on the surface of a cup. I don't think habituation happens that quickly, but other mechanisms may trigger attention only when something actually changes.

When sending an observation after moving one centimeter, does a significant color change, such as at an edge, trigger a new observation even if the movement is less than a centimeter? Or does it always require at least one centimeter of movement regardless of other changes? I was trying to parse the if-then-else logic; does any color change get pushed through regardless of distance, or does it also require a one-centimeter change? In this case, it wouldn't care about the color change.

It would be useful to test if including color change as a trigger improves results. The function checks feature change by going through the features in the dictionary, so if HSV isn't included, it won't check for color change. It looks like an "or" condition—if any feature changes, it returns true. If not, it returns false, so even a significant color change won't be detected unless specified. We could experiment by adding HSV to the check. Scott had an issue with spoon and banana confusion due to similar morphologies, but banana is clearly yellow, so including color might help resolve that. The color will still be sent to the learning module once the movement threshold is met, so that information should be updated.

Now, we get to the actual config. We combine all these elements into one configuration, currently defined as dictionaries with entries for class and arguments. The experiment class is a Monty object recognition experiment, and the experiment argument includes a data class that defines defaults. Eval experiment arguments have "do train" set to false and "do one eval epoch," with the option to overwrite these defaults.

For example, the number of evaluation epochs here corresponds to the number of rotations we want to test. One epoch is one rotation tested for all objects. We also specify the model path, which is where it loads the pre-trained models for this experiment. We specify the logging configuration, including an output directory and a set of handlers. There are Monty handlers and Weights and Biases handlers. The most commonly used Monty handlers are the CSV stats handler, which saves a CSV file with all the statistics, and the reproduce episode handler, which saves the sequence of actions taken so that we can reproduce an episode. This is useful for debugging, since we mostly use random actions dependent on a random seed. If there is an episode where something unusual happens and you want to review it, simply rerunning the episode will result in different random actions due to the seed. Saving the sequence allows us to reproduce that specific episode. We can also save a JSON file with more detailed statistics using the JSON handler. All these handlers are saved in the loggers folder. For Weights and Biases, there are additional options.

We can specify where to log the Weights and Biases run. The Monty log level controls how much information is saved, and the Python log level controls how much information is printed to the terminal.

This setting needs to be true if running a parallel experiment, as extra workarounds are required to get it working with Weights and Biases.

The Monty config is where we specify all the details for learning modules and sensor modules. Here, we pass in the learning module config defined earlier and specify Monty arguments, such as minimum evaluation steps, minimum training steps, maximum total steps, and number of exploratory steps.

We start with a default and overwrite a few values. The default contains the main details. For example, in noise versus non-noise experiments, we use the feature change SM and specify delta thresholds for various features. We monitor changes in color, curvature, pose vectors, and distance, as well as steps taken. If 20 steps are taken without any change, another observation is sent to the learning module.

In a basic Monty config, the Monty class is set for graph matching. Learning module configs are provided as a list, one for each learning module, each with its class and arguments. Sensor module configs are also a list, one for each sensor module, with their respective classes and arguments.

The motor system config specifies the class of the motor system. Mapping dictionaries define which sensor modules map to which agent. Currently, all sensors map to the same agent, but in the future, independent movement (e.g., two fingers as two agents) may be supported, with each agent having multiple sensors. The sensor module to learning module matrix defines connections, such as the zeroth sensor module connecting to the zeroth learning module. For example, two sensor modules—patch and viewfinder—are specified. The viewfinder, not listed, does not send information to any learning module and is used for visualizations.

The learning module to learning module matrix is used only in hierarchical experiments, where one learning module receives output from another. If not specified, it is unused. The LM to LM voting matrix is also unused with a single learning module, but with multiple modules, it defines their connections. For example, in a two-LM Monty config, the voting matrix specifies connections, and in a hierarchical stacked LM config, the LM to LM matrix is defined. There are five learning module configs and five sensor module configs for patches, plus the viewfinder. All sensors are attached to the same agent, with a one-to-one mapping from sensors to learning modules. The viewfinder does not map anywhere.

There is no hierarchy in this experiment, and the voting matrix represents all-to-all connectivity.

That's the most complex config here. There are also configs for the environment and data loaders. The dataset class specifies the first dataset to use. The terminology is a bit confusing because we work with environments, not static datasets. Monty is designed for interactive environments, but the torch dataset naming convention was inherited from previous classes. There is a plan to update this terminology. Dataset arguments include an initialization function, which in this case is the habitat environment, along with its arguments.

Here, I can also jump to that real quick. This specifies our agent and its sensors. The agent is named agent ID zero, which we saw earlier with the sensor connection. We have sensor IDs, the height of the agent (set to zero for simplicity), the agent's starting position, and the resolution for the sensors—64 by 64 pixel patches. There are also positions and rotations for each sensor.

Are these agents for Habitat or is this agent like Monty, where Monty is initialized in the environment? This is for Habitat. We use this to tell Habitat where the camera and sensors are located. The zoom factor is set to 10, so the patch is zoomed in by 10, and the viewfinder is a normal camera.

Finally, we have the data loaders for training and evaluation. For training, it doesn't matter right now because we're only evaluating, but we still have to specify something. For evaluation, we specify object names using a helper function, but we could also pass an array of strings with the object names we want. We specify 10 objects and use an initializer to set the test rotations. The initializer formats these for Habitat, and then we have the 10 distinct objects we're testing on.

That's one config. It took me an hour to explain it. For the dataset, how dependent are we on Habitat, or can we use any third party as long as it returns what's defined in environment dataset? We have different datasets already. In the frameworks code under datasets, we have Habitat and other datasets. For example, 2D data includes the Omniglot dataset, which has handwritten digits with few examples. The sensor moves over the strokes for the digits. ModelNet is another dataset. For the Monty demo in a real-world environment, we used Cicada on image environment, where an iPad takes a picture and a patch moves on that 2D image. This doesn't require Habitat; it just needs to be an instance of the embodied environment where you can move, send an action, get an observation, and repeat. It just needs to follow this schema, regardless of the dataset.

Every following config deep copies from the first config and updates whatever changes we want. For example, the base config for the distant agent is adapted for the surface agent by updating the Monty configs, passing the same learning module config, a different motor system for the surface policy, some minimum eval steps, and initializing the dataset for surface viewfinder. For the noisy agent, we use a different noisy sensor module with specific noise values. Each experiment customizes a part of the config. In the end, all configs go into the configs dictionary, and we run the experiment by running Python with the run.py script in this folder, using dash E and the experiment name. You can also call run parallel with dash E and the experiment name to run it.

That took longer than expected, but I hope it's useful. I think this will be a video I revisit. So, what was next on my plan?

Terminal logs. Right now, there isn't much output, but let me show an example of how to see more.

Which experiment are we running? This one here. To update the logger, we can change the Python log level to debug for more information. Running this again will provide detailed output, which can be useful for debugging. If you don't want to be overwhelmed, set it to info.

Does this also go to the log.txt file?

Yes. Sometimes mine are empty, likely because of the log level set during pre-training. If set to error, nothing prints, but info is useful. I used to have info as default, so I still get dopamine boosts when Monty detects a match. The Python log level follows standard logging levels: debug, info, warning, error, and critical.

For example, in the code, you'll see logger.debug, which only prints if set to debug. logger.info prints if set to info, logger.warning prints if set to warning. When writing new code, use logging with the appropriate log level instead of print statements. Above error is critical, and below debug is not set, but debug includes everything above: debug, info, warn, error, and critical.

Now that the experiment is running, let me show you what it's outputting. We can get to the logs.

We have this results folder, Monty, and then projects, which I think we're logging to. Do you remember what the experiment was called? We're just running this one today. That should be it. Here we have the logs. If I run a new experiment and there's already an evalstats file in this folder, it will rename it to underscore old and start a new one. That's just so we don't accidentally lose something. We rarely need it, but we keep it just in case. The log.txt, as you mentioned, will save all the terminal outputs. Now I can go here and have a closer look. We see the target was the mug with this random rotation.

We're starting the simulation. We're skipping a step on the learning module because we're not getting input. Then we have the first matching step, sending input from patch to learning module zero. The current most likely hypothesis is bowl with evidence 0.96. The bowl is also red, same as the cup, so they usually both start with high evidence. We haven't moved yet; we just saw red, and possible matches are still all of the objects.

Step one, we're skipping step one on learning module zero, probably because features didn't change. Step two, sending input from patch to learning module zero, testing this many hypotheses out of so many for spoon, which are all the hypotheses where evidence is above 0.19—that's the X percent threshold—and we do that for all these objects. The current most likely hypothesis is bowl with evidence 2.08. This continues until, at matching step 34, all possible locations are in a radius of 10 centimeters. Symmetry evidence is added, symmetry detected for four hypotheses, setting terminal state, detected mug at this location, orientation, and scale, and then we move on to the next episode.

How are the number of hypotheses generated? For spoon, there were about 3000 hypotheses. That might be related to generators and other factors. The hypothesis space, basically, at the first sensation or before, could be any location on any object.

That's the initialization. For the pose, there are usually only two poses for each location, since we have the curvature directions. If I'm sensing this coffee mug, I get a point normal and the curvature directions. If I'm sensing this curvature, the cup can only be rotated like this or upside down; otherwise, the curvature directions wouldn't work. For most locations, there are two possible rotations, except on a flat surface, where the principal curvature directions don't work. It could be flipped, ambiguous, or rotated in any way along that surface. We still have the point normal, so we sample eight different rotations along that dimension. That's how we initialize it. Objects with a lot of flat surfaces or a round ball, where the curvature is the same in all directions, usually have more hypotheses initialized in the beginning, but they narrow down quickly and aren't all tested at every step.

Do you think symmetric objects, flat objects, or objects without distinct features generally take more steps to reach a conclusion? The hypothesis space goes down rapidly from 3000, but do you think those are more difficult for Monty, or does it not matter?

Not because they have more hypotheses in the beginning, but more because of symmetry—it takes longer to resolve symmetry. We have a symmetry detection mechanism, so we can detect symmetry and have an early timeout, but it might still take a few more steps to actually detect symmetry and say we can't resolve the pose along all three dimensions for this object. The symmetry computation is local, not object-wide. It's simplistic: if we have the same set of hypotheses that remain valid for X steps, these must be symmetric because all observations are consistent with all these hypotheses. That's the rationale, but we're not doing graph computations or anything like that. It's just how the hypotheses develop over time.

It's not perfect. As you saw with the log for the mug, it detected symmetry, but with a mug, you could resolve it by going to the handle.

With the unsupervised experiments, the mug often merges graphs with the handle in different places. It detects how to put the mug upright, but just gets the rotation; it doesn't go to the handle and figure it out.

It's interesting that if I'm on a mug and moving along the surface, and this is the handle, I could be at any point except the handle. During continual learning, say on the second or third epoch, it must eventually settle somewhere, even if it's not correct. All these hypotheses are about equally good, so it picks a spot. When it runs into the handle in an unexpected location, it just adds it. It goes into the symmetry condition, saying all these hypotheses have been valid for the past hundred steps, so it must be symmetric, and then the episode is over. It starts a symmetric object and takes the first one for how to add the new observations into the graph.

That's something we could do for the unsupervised learning condition: set the threshold for symmetry detection much higher, so it tries longer to resolve it.

We only have 10 minutes left. Maybe I can quickly explain the CSV stats.

For a five learning module experiment, this might be more interesting to explain. We have the primary performance, which is the most important metric: did Monty recognize the object correctly or not? There are different options: correct, correct most likely hypothesis (meaning it didn't converge or say "I'm done," but the most likely hypothesis was correct at that time). In this example, we have five learning modules, LM0 through LM4. At the end of the episode, three of the learning modules detected the object and said "we're done." Our condition was that we need three out of five learning modules to be done, so once three are done, the episode ends. The remaining modules were not done yet, but they still had the most likely hypothesis.

Other options include "patch of object," which occurs if one of the patches is not on the object, and "confused most likely hypothesis," which happens when, for example, three modules are done, one has the correct most likely hypothesis, and another has a different one. You can see the hypotheses they had when the episode finished. Both had "Spoon" in their hypotheses, but one had a different most likely hypothesis. This module only got two observations, compared to 36 or 37 for the others, so it probably didn't get enough information to say "I'm done" or have the correct most likely hypothesis.

The reason learning modules in the same episode, receiving information from the same type of sensor module, have different numbers of observations is that they are different patches in different locations. They are nearby and move together, but still occupy different locations. They also use the future change as an SM, so they might not all receive observations at the same time. If you imagine these modules on a distant object, the agent is saccading, and if four are on the left side and we're on the side of the cup, the ones on the side are not actually on the cup and won't get observations, while the ones on the cup do.

That might have happened here: learning module two was not on the cup and didn't get any observations.

Next, we have the number of steps. If you look through the data, you'll see columns for number of steps, Monty steps, and Monty matching steps. The first column, number of matching steps, is the number of matching steps a specific learning module performed. These can differ for each module, as each might have received a different number of observations and performed a different number of matching steps. Monty steps are global steps—the number of observations sent to the Monty model. This includes observations not interesting enough to be sent to a learning module, such as off-object observations, and includes both matching and exploratory steps. Monty steps are the same for all modules and represent the number of cycles completed, even if sensors didn't detect a feature change. It's a Monty step because the system went through the outer loop of taking a step in the environment, meaning the sensors made a movement, even if nothing happened in the learning modules.

Monty matching step is when at least one learning module performed a matching step.

There are also exploratory steps, which do not update possible matches and only store an observation in the buffer; these are not counted here. Matching steps are the same for all modules, but only count steps where at least one learning module performed an update and received an observation.

This occurs when the feature change was true for at least one module.

We also have the rotation error, measured in radians. In this case, it's quite high—between zero and pi, which is almost the maximum. This could be due to a mirror symmetry, where the system recognized the exact opposite.

The result column is marked, but two entries are in a list because those two timed out. This is just "correct most likely hypothesis." Why would this not be marked as "done" if it only has one object in the possible objects? For a learning module to say it's done, it also needs to resolve the pose. These modules already determined "mug" is the only possible object, but weren't sure of their position on the mug or its rotation.

Most likely objects are "mug" for all modules. The primary target object is the object we start on, and the stepwise target object is the object we were on when the episode ended. This distinction is useful in environments with multiple objects.

Highest evidence refers to the highest evidence for each module. Time indicates how long each learning module took overall; these values differ because some modules took steps when others didn't. I'd need to check the code to confirm why these are different.

Symmetry evidence is the number of consecutive steps where symmetry was detected. If a step occurs where the hypotheses are inconsistent, this resets to zero.

Monty step and matching steps have been explained. Individual terminal state performance shows which modules finished and had a correct classification, and which timed out because the terminal condition was reached with three modules. Individual terminal state reached at step indicates at which step each module was done; you can see which finished first, second, and third, and which never finished. We also have target position, rotation, most likely rotation, number of possible matches, detect location, detect rotation, detect scale, and individual rotation error.

mean objects per graph and mean graphs per object are relevant for the unsupervised learning experiments. When we don't provide labels during training, two objects may be merged into one graph, or one object may be represented with two graphs, and that's tracked here. True positives, false positives, and related metrics are tracked in this column, which is also useful for unsupervised training. For example, a model learned from the red mug and the green mug may result in both being merged into one model.

In this case, we can only have target impossible matches or target not matched.

This section is for the policy: how many jumps were attempted, how many were successful, and the learning module ID.