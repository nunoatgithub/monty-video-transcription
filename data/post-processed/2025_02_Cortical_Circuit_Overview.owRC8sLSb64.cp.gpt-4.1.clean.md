The brain has complicated anatomy that has been documented for over 100 years. It's really messy, with different terms used for the same things and conflicting empirical results. Even when studies address the same topic, differences in techniques and animal models mean there's often no overlap. To understand anything about the brain, you have to read at least seven papers before you start to get a sense of what's really happening. Any individual paper can be misleading—not wrong, but using different language or approaches, leading to different answers.

The anatomy itself is not like a circuit diagram you can look at and deduce what's going on. There are too many unknowns and too much complexity. It's not possible to just look at it and figure everything out. While there are a few simple biological neural circuits where this is possible, it's not the case for the cortex. We're still discovering how the neocortex represents information, so you can't just deduce everything from anatomy alone.

The value of studying anatomy is that it's a classic science problem: you have top-down theoretical constraints, and you check if the anatomy can support what the theory says must happen. Sometimes there's only one way for it to work, sometimes multiple ways, but it's an interaction between theory and anatomy. You might wish you knew more about the anatomy to prove a theory, so you dig deeper, and sometimes you find support or contradiction. On its own, cortical anatomy and physiology can't tell you everything; you have to match it with deductive, top-down theoretical constraints. That's the process we've gone through.

There's one axiom we follow, which is Mountcastle's proposal—the cortical column hypothesis. The cortex, this big sheet of neural tissue, is divided into columns that are all basically doing the same thing. There's a lot of evidence for this, and it's the one thing I don't question as we do our work. I never go back and wonder if Mountcastle was wrong about this, because it's foundational to everything we do. Hopefully it's right, because if not, we're in trouble, but so far the evidence supports it.

Regarding the extent of a column, Mountcastle's proposal addresses this. The neocortex is about three millimeters thick and forms a large sheet, with different parts doing different things. When first observed under a microscope, the predominant architectural feature was layers. You may have seen Cajal's drawings showing different cell densities and types, forming layers that extend across the cortex. They identified six layers, but that's not strictly correct. There are different cell types and densities, and the number six was somewhat arbitrary. For example, today, layers 2 and 3 are often grouped together as layer 2/3, and some people don't distinguish between them. Some papers talk about layer 2A and 2B, or layer 3A and 3B. Who's right depends on how you judge it—sometimes the cell types between two and three are a bit smaller or make different projections, so you might consider them separate, even if others don't.

Layer five often has two types of cells, and now there's a third type that connects to the striatum. Layer six has at least half a dozen different cell types. These layers are visually apparent, and you might count six, but they're really just placeholders to help locate cell types relative to each other. We often say things like "layer 6A connects to here," but even within layer 6A, there is more than one cell type, so we have to keep that in mind.

The next thing is that the general rule is, if you look at information coming into the cortex, information flows up and down vertically. You see many more vertical connections than horizontal connections. There's clearly a lot of information going back and forth vertically, and then there are connections in some layers that go small or long distances. Sometimes they exit and go out and back in again. So, there is very heavy vertically oriented connectivity, and then a sparser, but still significant, connectivity elsewhere. This is the first suggestion that there might be columns, because it seems like information is primarily processed in a vertical fashion.

Mountcastle argued that if you look at the cortex and consider what you might call a column, there is no visual demarcation between columns—you can't see a line between them; it just looks like layers. He argued that a column is defined by where it's getting input from. For example, in a famous picture of a monkey's hand, the skin of the monkey is mapped so that these columns are getting input from a patch of skin, but it's not continuous. All the cells within a column are getting input from this patch, but if you move to the next column, even just a little further, they're all getting input from a different patch. In the cortex, you can go from one region to another or one modality—this might be a somatosensory column, this might be a visual column—and they're getting input from completely different sensory sources.

Are these sensory patches overlapping, or do they not overlap? The sensory patches do overlap a bit, but not completely. The most important thing is not the overlap, but the fact that all these cells from here to here are all driven by the same area, and as soon as you move a little bit, they're all driven by a different set of inputs. It's not a continuous change; it's a sudden jump. He presented this as proof that these are separate processing units: this unit processes all the information from this patch, and so on. The same thing happens in vision, touch, and audition, and he gave evidence for that in all those modalities.

This is his proposal about columns. Since they all look pretty much identical, they're all doing the same thing, just processing a different part of the world. You could have a vision column right next to a somatosensory column, or a V1 column bordering another region. They're not intermixed, but there are points in the cortex where there are boundaries between these regions. If you move just a little bit, you can cross into a different modality. In fact, it was proposed that synesthesia is caused by cells incorrectly making connections across modalities. There are hard boundaries sometimes, but even within a single modality, there are boundaries.

A column is just looking at a patch, getting input from that patch, and processing it. There is another confusing element called minicolumns. If you're not familiar, a minicolumn is, again, something you can't see in the cortex. The only exception is the rat barrel cortex, where rats have a column for each whisker, and you can actually see those. Generally, you can't see columns, but there is something you can see called the minicolumn.

The minicolumn is much smaller. Where a regular column can be between a third of a millimeter and a millimeter in dimension, minicolumns are much smaller—maybe 30 to 60 microns. Each minicolumn is like a little stack of neurons that go across all the layers. The general idea is that all the different types of cells that exist in a column also exist in a minicolumn. A typical number you'll see is about 100 to 120 cells per minicolumn.

They form during development: when you're in utero, the cortex develops as a sheet of neural tissue, just a single layer. The cells start replicating and move vertically. Essentially, a single progenitor cell ends up creating about 120 cells, which grow up vertically. That's how they come about. The minicolumn is a real thing because that's how the cortex develops. You can often see them under a microscope—there are pictures showing these skinny little stacks of cells, then a gap, then another stack. They're not always visible; in some animals, like rats and mice, it's hard to see them, or they don't appear to exist. Some people say they were there when the brain grew but aren't there now. Others say that even if they are there, they're not functional. What do they do? Maybe it doesn't matter; it's just the way they grow.

Malkin argued that the minicolumn is the most important replicable unit of computation, but he had no hypothesis for its function. He said the minicolumn is the unit of replication of the brain, and if you take several hundred minicolumns and put them together, they form a column and work together. These are the two levels of organization you see in the cortex.

I have a theory about minicolumns that I've never fully shared. We can get into that later. I'm developing a partial understanding of what a minicolumn does. As an independent unit, it can't do too much—it's just 120 cells—so they have to work together as a group. But can you understand what they do? I think you can.

So that tells you, if you said, "Oh, there are six layers of cells," and you might say, "There's 20 cells per layer," or something like that, we've already seen that's not really true. In terms of the lateral extent, sometimes when I've read about a column, it's suggested it's around 10,000 neurons. My understanding is they're much bigger than that—more like 100,000 at least. I thought that was arrived at, and you mentioned, through some other logic. If you look at the hypercolumns that Hubel and Wiesel talked about in cats, those are about a millimeter square in area. Basically, just do the math: in a square millimeter, you'll have about 100,000 neurons. That's arguably the hypercolumn in a cat. I haven't really gotten definitive answers on whether that's a column or multiple columns—it's debatable. At the high end, you might have a column that's a square millimeter in area, about 100,000 cells. If your column was smaller, say 300 microns by 300 microns, that would be about a tenth of that, maybe 10,000 cells. The numbers generally work out well when you cross-correlate them, but they don't always fit properly.

There's also evidence—people argue that some of the cells in the cortex do not start minicolumns. Some of the inhibitory cells generate through a separate process, so there are more cells than in minicolumns. It's complicated. The question is whether minicolumns make up all the neurons in a column. The answer is yes, but maybe no. No one really knows. People argue different things, but the general answer is yes. There are other cells that are not. If you look at the cortex, about 75 percent of the cells are excitatory cells. These are cells that, when you input, have a positive effect on other cells—they make other cells fire. Roughly 20 to 25 percent are inhibitory cells, which make other cells less likely to fire. All the excitatory cells, I believe, are in minicolumns. Some of the inhibitory cells are unusual—there are ones in layer one, which hardly anyone talks about, that are really odd and don't fit one per minicolumn. So who knows? One of the rules I have about neuroscience is that for every rule, there are exceptions—every single one. I could say, "Oh, the cells are there," and someone would say, "No, they're not." I could say, "These are all excitatory cells," and someone would say, "No, here's one that's not." It's never-ending.

That's the general layout of this common algorithm: divided into columns, each one getting a patch, an input from someplace, and the next column processing input from someplace else, then that being divided into minicolumns, which our theories generally don't talk about. But if you look at the temporal memory algorithm, it's really relying on minicolumns. We do take advantage of it, but in our Monty system, I don't think we do anything with minicolumns—we just take the concept of it.

Any questions about this now, Aaron?

So the proximity of the sensory patches—is that mapped into the proximity between columns? They're generally mapped, except when you get to borders. If you switch from one modality or one region to another, there's a discontinuity.

Cortical computational unit, minicolumns as a computational unit—how, in the standard architecture, do different areas, different Brodmann areas, have different looking layers? No, the same layers exist. The differences would be that you mentioned Brodmann. Brodmann tried to define regions by looking at the visual differences between parts of the cortex. The typical variation is the number of cells in each layer can vary. One region might have more layer three cells, another might have more layer five cells. Cell types may vary in size. For example, what people classically call the motor cortex, which has neurons that project to your spine, those cells are really big because they have to project a long way. The same cells exist in other regions, but they're smaller because they don't project as far—maybe only to another part of the brain. They're all motor output cells, all layer 5A cells, but someone will look at the motor cortex—it's really the somatosensory cortex—and say, "Oh, those cells are different, they're bigger, so we can define the motor region because it has these big cells." But functionally, it's not really any different than regions with smaller cells in the same place.

That was going to be my question: does that distribution mean anything? Maybe for the motor system, they have to optimize for different things. No, it doesn't—it could be, and it almost certainly is. There are classic big exceptions to this common algorithm. One is the motor cortex, with these big cells, which people at the time didn't realize are the same as the little cells in other regions. The other classic difference is the primary visual cortex, V1, which is the first region that gets input from the retina and has some extra layers. In fact, a classic V1 in a primate has about twice as many cells per minicolumn.

and, but it's only in what they call extra layer four cells. They have a few extra layers there. That is an optimization for vision that works better, but many mammals don't have it. Dogs and cats do not have what they call a striate layer 4; they don't have those extra layers. Presumably, this is something primates have evolved for better or different vision than dogs and cats, but dogs and cats still see. So how do we handle that? From a theorist's point of view, I say these extra layers are important, but they're not essential for the basic algorithm of the cortex. They don't exist in touch, hearing, or anywhere else in the cortex—only in V1. Not all animals with eyes that see have a striated cortex, so it's important, but it doesn't overthrow Mount and R axiom.

The general idea is that we can focus on the core commonality across all modalities, because there seems to be the same basic arrangement of cell types in all modalities. The variations we see, whether it's extra layers, bigger cells, or different proportions of cell types, are all tweaks. We don't want to focus on those; we want to focus on the core algorithms first—what's common to all of these things. Later, we might say, even in our work with a learning module, "Hey, we're going to tweak the learning module for LiDAR. Here's how we could do it. We can add a little extra twist here and put some more of these things here." We can do that, but let's not focus on it now. So don't get hung up on that. 

A good question. Any other questions before I go on?

Some of this may be review for some of you, and new for others. This is a classic example: if you were to go to a neuroscience 101 class at any university, they would say, "Here's the cortex, here's a column," and ask, "What's going on here?" They would explain that layer four is the input layer, receiving signals from the outside—I'll draw it as if it's coming from the outside. Layer four is the input layer because this is where the eye projects to, or the skin, or hearing. Touch and hearing get pre-processed a bit, but vision goes directly from the eye to the visual cortex. When you touch or hear, the signals are pre-processed, but they all end up here. 

Layer three is considered the output layer, and layer four projects to layer three. Then, layer three is the output layer, and the next column over, or the next hierarchical level—say, from V1 to V2—receives input into its own layer four. This process repeats as you go up the hierarchy: information enters layer four, gets processed, goes to layer three, becomes input to the next layer four, and so on. This is the classic hierarchy. Of course, this ignores a lot, like what everything else is doing, but this is the classic view. We think we understand this pretty well right now.

I want to point out that all inference is a matter of mapping many inputs to fewer outputs. For example, if I'm going to label an image, there are millions of images I might call "cat" and millions I might call "dog." The same is true for different patterns—identifying a song, for instance. Inference is always a many-to-one mapping. If that's not obvious, I can expand on it, but inference is always many-to-one. Mountcastle said that every column in the neocortex is doing the same thing, and the corollary is that every column does what the neocortex does as a whole. If something happens in the neocortex, it happens in every column. This doesn't mean every column processes vision, but whatever processing elements are happening, they're happening everywhere. That's the background. So, anything that happens in the cortex happens in every column. Inference happens in every column, and mapping from many inputs to one output happens in every column.

This tells me there is a many-to-one mapping between layer four and layer three. The term we use for that is "temporal pooling." It means that different patterns over time get mapped into a single output. I can state that even before understanding sensorimotor inference; it's just a fact. If you're going to group patterns and call them one thing, it has to occur, and if this is the way up the hierarchy, it has to occur from here to here. Our current theory is that temporal pooling is the term for that. As your eyes or fingers move, the input changes, but the output becomes more stable. There's nowhere else for this to occur.

We then developed a theory for how the brain could learn sequences and make predictions of higher-order sequences, like melodies. It's a challenging problem, but we realized we could do it even in a single layer, like layer four, using many mini-columns. I won't go through that algorithm now unless someone wants me to, but this is the temporal memory algorithm. For example, if I were listening to a melody, I could have a series of patterns in these mini-columns, which then map to layer three, representing the name of the melody. That's the temporal memory algorithm, and it's very important. There are a lot of important ideas in how it works, and it's challenging for people to grasp in person.

That was great, but it didn't address sensorimotor inference. I avoided talking about it because I knew most changes in the brain occur because we move our bodies—our eyes, fingers, or head. Most changes are due to movement, not just passive input like a melody. Think about the flood of information coming in as I touch things and look around. I didn't understand how this could happen from a sensorimotor point of view for a long time, so I ignored it, figuring we'd solve it later. We did figure it out. The answer is that there's another layer of cells representing location, which we call layer 6A. There is a bidirectional connection between layer 6A and layer four—a major connection. This connection keeps track of where the sensor is on objects in the world. If I know the location, I can predict what I'm going to sense, and if I know what I sense, it helps narrow down the location. This pairing of sensations or features and location is the foundation of the basic model.

The idea that neurons in a column could keep track of the sensor's location seemed crazy, but it had to happen. That's the only way your finger can predict what it's going to sense. If the input comes from the tip of your finger and you can predict what you'll sense, it needs to know both what it's sensing and where it is on the object. There's no other way around it.And then we realized this seemed crazy at first, but then we found these things. We know about these other cells, part of the so-called grid cells, which act like a location signal. We speculated about grid cells here. Grid cells exist in the entorhinal cortex, and they serve as a reference frame. They tell you where you are on the object. Of course, you need something else—how does this system work? We know a lot about grid cells because it's a field of study. The basic idea is that there's a second input coming into each column: a movement input, as opposed to a feature input or sensory input.

It can be a feature; it can be more than that. It doesn't have to come from a sensory organ—it comes from the cells. Now we have two inputs to our column. One is updating where the location is: I know I'm moving in this direction, so I can calculate where I will be over time. Grid cells do that; we know this, and there's incredible data on grid cells. We speculated it happens in the cortex, but it's known to exist in the entorhinal cortex. You have movement information, which updates the location, and now you sense something at a new location. It doesn't matter what kind of sensation—I'm going to learn how something feels. It doesn't matter what order I go around it or the pattern; as long as I cover different locations, it will keep track of where I am, and then I can build this model.

You need to develop this image in your head of a model of a three-dimensional structure where, at each location, you have some knowledge about what is there and what was observed there. That's what Monty does. That is the basis of how sensory-motor models are built in the cortex. We're only guessing what these layers mean, but you can see how you're forced to say layer 3 must be doing something like that, and layer 6, because of these bidirectional connections, must be doing location. You can deduce these things. We might still get it wrong, but I think the actual layers are not as important as the functional role that must be done. It's a pretty good guess what the layers are at this point in time.

Is there any physiological evidence of layer 3 being more stable as we look at the subject? No, I'm not aware of it. But they haven't looked for it because they don't want to look for it. This is a great question. Let's talk about how a layer of cells represents something. If you look at layer 3, maybe it has 5,000 cells in it, maybe 10,000. I don't know, something like that. It's very sparse; we know that. There are very few neurons active at any point in time. Let's say 2% of cells are active—maybe you have 100 cells that are active. What is stable is that pattern of 100 cells. Any particular cell is only stable if the animal is observing an object that it knows. The animal has to be awake, observing an object it knows, continually observing that same object, and alert. Then you would expect to see stability of this hundred-cell pattern. Any particular cell will become active one or two percent of the time—so one out of fifty presentations.

If you look at the experimental paradigms people typically use, most of those conditions are not met. The animal is anesthetized, they're showing sinusoidal gratings, doing all kinds of things where the animal is not awake, alert, or looking at something. They'll say, "The cell fired on this image, but it didn't fire on that image," and get back to the work that you and Niels are doing—are these SDRs, these sparse distributed representations, do they have semantic overlap? If they have no semantic overlap—meaning one SDR is randomly chosen for another—then you won't be able to see a cell assign any meaning to it. One moment it might be a cat, the next it might be a bicycle. It's just meaningless. It's the step that matters. It's very difficult to know what to look for. Now they have techniques that might be able to do this, but again, these are very difficult experiments. The animal has learned an object, recognizes the object, is fixated, moving on that object, is alert, and they're looking at a whole bunch of neurons at the same time. Hard to do.

But there's evidence that it's more sparse in that layer and has electrical connectivity, which, as I said before, fits with it. You've got these 10,000 cells, and what you really want is for the SDR, the sparse distributed representation—the 100 active cells—to self-reinforce, and then another pattern of 100 to self-reinforce, and so on. The mathematics of sparse distributed representations is really interesting. We're not using that at all in Monty today. It's not clear if we have to, but brains do it.

All right, other questions. I'm going to fill in a few more pieces here. One thing we didn't understand at first is that there needs to be a concept of orientation. It's insufficient just to know where the patch of the eye is on the object or where the fingertip is on the object. You have to know the orientation of the patch. If I tilt my head left or right, the whole image is rotated. Or if I take my finger and rotate it, my perception isn't changing. I perceive the same object, but very different patterns are coming into my finger. The orientation has to be compensated for in many ways, so there has to be another signal representing orientation. This is equivalent to head direction cells in the hippocampal complex or elsewhere.

We have an orientation signal, a location, and both of those will be updated by movement. If the cortex is being told my finger or hand is moving, it could change the orientation or its location. Both of these can be updated by movement. Now we know the orientation relative to the object and the location on the object.

This is something we figured out fairly recently. How is orientation taken advantage of here? What we want to do is remove the input's dependence on orientation. Imagine if I'm touching an object with my finger—I don't want the orientation to matter. I want to get the same result regardless. If the object is rotated, or I tilt my head, it should be the same thing. It can't change it.

Here's something we figured out fairly recently. If we look at the cortex, we've got columns. There's another part of the brain that is absolutely essential to how the cortex works. It's intimately tied to everything in the cortex: the thalamus. The thalamus looks like a little bird egg. There are two, one on each side of your head in the center of the brain, and every part of the cortex connects to the thalamus, and the thalamus connects back to every part of the cortex. It's intimately connected. In fact, any input to the cortex—eye, finger, whatever—first goes through the thalamus and then up to layer four, where it stops at relay cells. Then the cortex projects back to the thalamus in a major way. There are lots of details known about these series of connections, but all inputs to the cortex go through relay cells in the thalamus.

If you look at a lot of the literature, they don't always talk about this. They'll say it projects to the thalamus, to a thing called LGN, which is the lateral geniculate nucleus. Then it goes to V1, then V1 projects to V2, and V2 projects to V4 in the cortex. This is just how you get into the cortex, and the information flows from region to region, which is true. But it's also known that every region of the cortex gets input from the thalamus, every region and every column. This is where the heterarchy paper comes in. There's clearly a hierarchy of projections in V4, but it's also in parallel.

Sometimes these regions work in parallel, and they're also working hierarchically. What we now believe, as described in the heterarchy paper, is that each region is learning objects. Each column in each region is modeling objects, and each one must have a complete sensorimotor interface to the world in some sense. These connections are for hierarchical composition—objects composed of objects. The classic view is that V1 just detects little features and you have to build up to get objects, but we think everything is recognizing objects. People didn't know it before because they didn't know how to observe it. These connections indicate that the object in V2 has a feature which is an object in V1, like the classic coffee mug with the logo: the logo is here, the coffee mug is here, so the logo is part of the coffee mug. This is recognizing the mug, and this is representing the logo. I'm virtually certain that's what's happening.

Does higher-level input also go to layer four? Higher layer—what do you mean? What's going to be two and three? This goes back to layer three projecting to layer four. There are more details I didn't mention. People talk about the main input to a cortical column going to layer four, but that's not completely true. There's another input that goes to lower layers, like layer three, and another input that goes to the layer five/layer six border. This could arguably be the layer three/layer four border, depending on how you look at it. So there are actually three different inputs to a column. People talk about layer four as the main input, but I think the motor input is the layer five plus layer three/layer six one. It's basically saying, "I'm going to drive these cells to path integrate and update the location." I think what's going on up here is related to object behaviors and movement—movement related to the object, as opposed to movement related to the sensor. The object is moving, something is moving on the object. We can come back to that when we talk about object behaviors.

Did I answer your question? Yes, it's the same for V1, V2, and V4. They all get the same composition. As far as we know, they all look like this. As Murray Sherman has told me several times, everywhere they've looked, they've seen this, but they haven't looked everywhere yet. These inputs from the thalamus lead to nuclei, and the thalamus is divided into many different nuclei that get different inputs. Each region has its own nuclei. Not only does the sensory input go to it, but the motor input goes to the thalamus too. There are two pathways to the thalamus. They don't call it motor, but we know it's motor. There are parallel pathways to the thalamus, and from the eye, they're related to the magnocellular and parvocellular pathways. One represents movement, and the other represents features. The movement pathway goes to the thalamus, and the sensory input goes to the thalamus.

One of the major things the thalamus is doing, as part of our theories, is relaying signals. They call these relay cells because it looks like one spike comes in and one spike comes out. What's that for? Not very useful, is it? Some suggest it might be a delay, but these cells are really complicated. They have 6,000 synapses and very complex architecture. There's a huge amount of complexity here. If it's a relay cell, what is it doing? That doesn't seem useful. No one really knows, but we're pretty certain now. They're remapping. The spike output can vary; these cells get more than one input and have more synapses in different parts of the retina. They're like multiplexers—they can take an input, remap it, and send it to the cortex. That's the basic idea. There's a lot of uncertainty about how it works, but many suggest that's what they're doing. Theoretically, it makes sense. One of the major projections going down there is layer 5A, which we think is orientation, and that projects down here. It says, "Given my current orientation of the sensor to the object"—remember, this is the orientation of the sensor, like a sensor patch or your eye—if I tilt my head, the orientation changes. If I rotate my finger, the orientation changes. That orientation comes down here and tells this cell how to compensate for it. It says, "Rotate the input," and it has to do that for motor as well. If I'm reading text and I rotate the page, which you do all the time, your eyes have to move diagonally to read the text. Your motor behavior changes based on the orientation, and what you sense changes based on the orientation. This compensates for it.

Now we have the complete system: movement coming in, features coming in, location, objects modeled by pairing locations with features, and orientation used to ensure that movements and features are rotated properly. We do temporal pooling, and now this is the object coming out. The reason it goes into layer four in this region is that this feature on this column is the entire object of this column—this is the logo, this is the column.

I'm going to stop soon.

You have one column to one column, but just for completeness, will the V2 column receive layer four input from a whole bunch of other columns? It's not clear. I didn't talk about columns to columns. The way I've always viewed this is that we need to understand a single column first, as per Mountcastle's axiom, and then understand how single columns work together hierarchically—one column here, one column there—without getting distracted by all the other columns. Now the question is, is there convergence onto V2 or something like that?

I don't actually know the answer to that question. There's evidence that there is, but that convergence can be interpreted incorrectly.

Isn't it often physiological? If you measure the response in V2, it appears to see a lot, but that's not necessarily due to convergence of input. Imagine you have a visual column and you're looking through a straw—one column sees a cat, and you can only recognize the cat by moving the straw around. If this is the case, the output will be "cat," and this column will receive input for "cat." That looks like a broader area of the cortex because it's everything this column could see as it moves around. Physiologically, as Neil said, this broader area would respond to anywhere in the cat, while a specific layer would only respond to part of the cat. If they knew how to look at this layer, they'd see it was responding to "cat," but they don't really know how to look at this layer because it's sparse and complex. These are testable hypotheses, but it has to be like this in some sense—it's deduction. People get annoyed when I say that, but that's how I approach it. The surface area gets smaller as you go up; either the columns are getting smaller in V2 and so on, or you're getting convergent input. Not necessarily either one of those has to be true.

First of all, going from V1 to V2, V2 is actually a little larger than V1. You typically wouldn't see the same with S1 and S2—the first two regions are really large, then they start getting smaller quickly. I interpret that as most vision actually occurs in V1 and V2, and you only need two layers to learn a compositional structure. There's a huge amount of memory in V1 and V2; they must be learning a lot about the world. Then these regions get narrower very quickly and become more multimodal. That's the main point—it's not strict convergence going up, and there are multiple ways a region can get smaller, with or without convergence. I want to avoid the word "convergence" because that suggests a column can only understand something if it's looking at a larger part of the world. It can be physiological. I don't think that's the right way to look at it. The system can be built with a single module, a single column on top of another, and so on. It wouldn't be the most efficient system, but it would work. You don't have to have convergence of columns for this to work. We can build our first compositional object with two learning modules if we want. It's not an issue if there is convergence—it can be accommodated, but it's not necessary. Don't fool yourself into thinking we have to have it. We don't. It can exist, but it's not required.

The theory of compositionality is as follows: imagine this is V2, with a bunch of columns, and here's V1, roughly the same size. The columns can be a little different, but they're roughly the same size in area. Maybe V2 is a little larger.

The theory we worked out—this was pieced together when Vivian was here—is that positionality works on a location-by-location basis. Imagine this is getting input from some part of the retina. The retina is back here; imagine we're getting input from this little patch. This column is getting input from that patch, and another column is also getting input from a patch co-located with it. They're essentially pointing at the same point in space.

V1 looks at the retina and gets input from it; V2 also gets input from the retina, and these columns are both centered on the same part of the retina. When we learn compositional structure, as described in the heterarchy paper, this co-alignment is essential. If I have a coffee cup with a logo, it's saying that this point on the logo is also a point on the cup. At that point in physical space, I can assign this point on the logo to this point on the cup. It's on a location-by-location basis. As you move around, you learn to connect—there's a sort of bidirectional connection here—at this point of the cup, I'm at this point of the logo, and so on. This allows the logo to be morphed, changed, rotated, and still be learned.

When we build compositional structures, we can do this with just two columns. It's sufficient, not necessarily efficient. That’s what the theory tells us. The location-by-location mapping can also store orientation, which is important.

It's worth checking that this makes sense because it will be important for the location-by-location mapping for behavior. The system has to know the identity of the object—this is the logo on the cup. It has to know the orientation of the object, like if it's rotated, and the scale, because it could be bigger or smaller. We learn these things quickly and for everything. The surprising thing is it has to be done on a location-by-location basis, but if you do that, the logo can have distortions, be twisted, and still be learned. These are the three things we have to learn as part of the model: the feature coming into this column is rotated at a certain scale at this location. We're going through the circuitry for how this works in the paper, but you can see how it functions. That informs inference. If you've learned a particular mug with a funny logo, the information about the pose at a particular location of the logo can tell you, "Oh, that's the mug with the weird logo." It can also assist prediction—if you see the mug, you can predict at a location a particular orientation and scale of the logo.

The location-by-location basis is important for both feedforward and feedback, which relates to the anatomy. I'm emphasizing this because when we discuss behaviors, it's helpful to understand that. When we talk about it, we should project the images from the paper, as they're not quite right in these papers.

Are we saying that input to V2 is more receptive field, or is it the same receptive field as the input to V1? Typically, it's a larger receptive field—more ganglion cells in the retina converge onto V2—but that's not important. It gets filtered out; it may be efficient and useful, but it's not theoretically essential.

This fits with the idea of one region learning a coarser model and another learning a more fine-grained model. There's only so much information any cortical column can store. If it's getting small or fine-detail inputs, it will realistically learn a fairly small but detailed model. If it's getting input over a larger surface area, it can't distinguish details as much, but it can build a coarser model. I've always thought this part is just in the hierarchy, and not—I've made a good type of pool in the hierarchy to take care of that part. How big can this receptive field go? In V1, they're really small. Before, does it cover the whole object? It's not clear. V2 definitely gets input from the retina; I don't know about V4. It cannot be so big that it covers larger objects, but it would be bigger than V1. There's evidence that convergence from the retina onto a cell in V1 versus a cell in V2 is greater for V2, making for a broader area.

That's also a property in somatosensory cortex. It's an empirical observation; we accept it. Neil mentioned a couple of advantages: V1 is the highest acuity you can get—there's nothing more refined. If I'm reading the smallest print I can possibly recognize, it's almost certainly happening in V1. V2 would be too blurry. But if I have a very big letter, V1 is too small because you have to integrate over a huge area, which isn't useful, but V2 could do it more reasonably. That makes sense from a scale perspective. When you have composite objects, it's hard to find an example where a child is bigger than the parent; it's almost always the child object is smaller, so that also makes sense. If it were bigger, you'd probably flip it and say one of those is the parent. From a theory point of view, it's not essential—they just have to be two different objects, and they have to be able to have the same point on them. They're looking at the same point, but one is focused on the logo, and one is attending to the object as a whole. When you see a new object—say, I pick one up, it's got water in it, maybe not a great example, but it doesn't have much of a logo, just "Kirkland" on it.

When you see a new object, you recognize it's a bottle, then focus on the details: "Oh, it's Kirkland, it says this and that, it has a funny bottom to the cap." While doing that, a higher region is still saying, "This is the bottle," and you're attending to the components in series, building up the model of the bottle. Now I see the Kirkland logo, then this thing, then that thing, and I can look on the back and see something else. Now I have a model of the object because I knew it was a bottle, and I'm looking at the intended features one at a time. Part of our mechanism for compositional structure is to keep—one region or column maintains, "I'm looking at this bottle," while another is saying, "I'm looking at the features individually," and recognizing them individually. How does the higher level know it's a bottle without the lower level? The higher level also has its own feature detections that come straight from the retina, in a sense—the boundaries of the bottle can be learned as grosser features. Only when you attend to a subset does the bottle stay the same. I don't know how else it could work. It seems like this absolutely has to happen. Once you understand it, there's no other way: you have to be able to assign individual components to a larger object, and you do that by moving around them, looking at them one at a time.