There are a lot of questions that come out of this for me. From my perspective, I want to understand that there are multiple ways I could go. One is to understand the hierarchy better; that would be one thing to work on. Another is understanding motor patterns.

Back to Viviane's question: how do I move? How do I decide what to move, and for what purposes? These modules can move their sensors—they have the ability to control some movement. Why do they do that? Who directs that? What kind of information goes back and forth to do that? How would that motor information transfer? That's another question to consider.

Marcus and I were talking yesterday about how sometimes you can recognize a very specific set of features, and other times you can recognize something with a fuzzy set of features. The fuzzy set would be all coffee cups, but a specific coffee cup has a particular shape and size. Or a face—like Lucas's face. I recognize Lucas's face as a particular face, but I also recognize faces in general. We came up with the idea of the stretchy graph, which is basically: how would you recognize things when you allow these edges to move? It's the same features, but they're stretched out a bit. You have to handle cases where you don't always have the same features. For example, Lucas has a beard and a mustache; I have to handle that too, so it's not just stretching.

Another related topic is states, logics, and behaviors. I would define a state as the current position of all an object's features. If you think about a stapler, it's the current position of all its features—the current poses relative to each other. Even something as messy as a t-shirt: t-shirts have collars, shoulders, arms, and certain features relative to each other, but the t-shirt could be on the table or folded up. It's still a t-shirt. I still have the concept that it has those features, but I don't necessarily know where they all are relative to each other. When I pick up a t-shirt out of my drawer in the middle of the night, I find the collar based on its texture, run my finger along it, orient the collar relative to my body, and then hang it up, knowing the rest of the features will fall into place. All those features are there, but there's a lot of uncertainty about their poses at any point in time. The knowledge of the links is confusing, but the local links should always be preserved. I know that this part of the collar is related to another part, and I can get there by moving along it. Objects have local features that are near each other and are more likely to have links, and you're stretching those links, but remote features can be far apart. If I think of an object as not having all the connections between all the poses and features, but rather local features, then you can stretch this graph and maintain the topology of an object, even in a very deformed state. We have to be able to do that. Sometimes those deformations have patterns, like a stapler opening and closing—that would be a behavior. If I look at your body, you're a set of features like a t-shirt, in some position relative to each other, but the state of your body depends on whether you're moving or sitting down. I still recognize you. The graph has all the components—your body parts are all there, but they're stretching out in different ways at any point in time, and somehow the model is able to deal with that. That's another thing to look at: states. Objects have states; your body has states.

Another question is about models in the where columns.

That goes back to this point: I couldn't develop a graph in the where column. This would be the relative poses of your features. The question is, is there an equivalent model here, and what would it be useful for? If we think all cortical columns are the same—what columns and where columns—then you might have a model here and ask, how would I use that? 

One example is when I drive my car. I get in, and I don't have to figure out where the steering wheel is because it's always in the same position relative to my body. I don't have to infer where the wheel is; I have this model that, when I'm in the car, the steering wheel is there, and I can reach it without even looking. It doesn't really move much from that position. That suggests I have some sort of model of the features in the car, and when I'm sitting, I've learned these features relative to that position. I don't have to infer them or rely on the idea that cars are composed of steering wheels and cabs and wonder where I am relative to the cab. I just know, and I might want to model that. 

A lot of our behaviors might be built on this kind of model, where once I orient myself, I know what I've got. These are some topics to consider. Finally, the big question is, when we understand this—even if we can build it, or even a more complex or a sub-complex system, or just the first one I talked about, and we do these models and this learning system—what would we do with it? What would we want to demonstrate? What could we do with it that's useful? I didn't address that. I wasn't even thinking about it; I was just focused on the mechanisms and how to talk about this. 

To me, this is real progress. I never had this clear conception of all these things before. It's much clearer to me now. It's a much clearer description. Is it for you too? Yeah? Good. 

One issue that I think is going to be important for any real implementation is the issue of what's a feature and how do you locate it. I think that's the number one thing that tripped up standard computer vision. A lot of this can be described in terms of old-style computer vision, where you can make decent demos, but to really get things working well, you have to deal with these fuzzy aspects of what's a feature and how to deal with ambiguity. It's not clear yet how that fits in. That's a great question. I think we have enough information to start honing in on it.

One aspect has to do with the fuzziness of representational space. When I detect a feature, the sensor tries to give a unique location for that feature, even though there might be some structure in that space. If it was an edge, I could say, "I know the orientation of that edge," and that's pretty easy. But even then, imagine you have a sensor that's detecting some volume of space, and as I'm getting input, I'm just trying to characterize it. I put it through a spatial core, not really trying to understand what it is—just getting some inputs, some bits, and trying to classify it.

What if there was an edge, but at one point the edge is here, then it's there, and then it's somewhere else? What am I reporting back? Maybe this is the question you're getting at. Is this an edge? Or is this an edge? Or is this one out of view? This one's more in the center, this one's barely touching it. What do I report back? 

One way to answer this is in a binary way: is there anything in that space? Anything physical in that space? This would just say there's a border, some edge of this thing, and I don't care to make further distinctions. You could start with that premise and work from there: how would this work under that condition? That would allow you to define the morphology—the boundaries of an object. It wouldn't provide additional information, just morphology. 

I'm thinking out loud about how to solve that problem. I agree it's a big problem. I'll just say, I agree it's a big problem.

Feature details. What would be a good word to represent this problem? That's it.

It's almost like the granularity of a feature—a discrete feature. Once you get to this upper-level module, it's a lot easier, because these are all going to result in some answer. It's sometimes easier at this level because there's no fuzziness; they're going to vote and pick something—it's either the face or the box, this or that. I may be wrong, but I'm going to tell you it's one thing. These will be very discrete properties, whereas down here is where you get into the fuzziness. 

If I have some edge of an object and I'm trying to identify a feature, where exactly is it, and is that the same feature? It gets weird at this level. It's pretty clear at the higher level because I've already bucketized it into one of N buckets. Here, I guess we want the sensor to do the same thing: discretize the feature in a way that doesn't cause a problem.

So from that point of view, we'd say it wants to pick one thing or the other and not allow subtlety to turn up. It's a good question. If I were to guess, I would say that in the beginning, features are just the raw sensory input. Over time, you learn to discretize it, to group the sensory input into useful chunks of information. At birth, children can distinguish all different sounds from all languages, but over the first months of development, they lose this ability and can only distinguish sounds from their own language, but they do this much more accurately. That's a good point. Let me rephrase that in a different way, Viviane. In my language, if a feature is just some quality at some pose, it's something that has a pose. Now I have a sensor that's trying— I should have drawn that in green to be consistent.

Green is a feature, and black is a sensor.

Now I have a module that's processing that. The first thing I need to know is the pose of the feature. Generally, the location is easiest to determine. We can debate it, but in some sense, it's easier. It's about where it is in the world—it's out here, at the tip of my finger, where my finger is, or my eye. There's one column in my retina, a cone in my patch of skin or retina that's looking in this direction, and I detect something. At some distance, I might have some ambiguity about the distance. That's not really weird. The weird part might be figuring out the orientation of the feature, and I've argued there are features which do not have orientation. This is some sense of a color on top of that. The most important thing is that something may or may not have a pose. Some features actually don't have a pose; you can't distinguish—I'm hearing a sound over there and the sound of a certain frequency. I can't necessarily say the orientation of the thing making that sound. Maybe I can, but generally I can't. The system doesn't work with ambiguity on the orientation. Then there are additional qualities, sensory-specific things that only occur in vision, hearing, touch, or something like that. What I'm pointing out is that the system can start working with just location, and then we can start adding pose and these additional qualities.

If they're understandable, if I've learned them, or in your example, Viviane, maybe I've been exposed to these enough that I've settled down on a few of these that I care about. Imagine I'm taking these bits and running them through a spatial pooler, and the spatial pooler is just trying to bucketize these things, saying, "That's it, that's what I'm going to know." Then I have to figure out the orientation, which is sometimes possible, sometimes not.

It's a great general problem, but I think it's solvable. The way Viviane phrased it, it seems like you could figure out some sort of learning-based solution, which she said was basically clustering or pooling. The thing that gives me hope is that I don't actually need to know anything other than location to get started.

The other things would be great if I could figure them out, but they're not absolutely essential. There's a mix of orientation knowledge and quality knowledge that seem to be additional help. In graph terminology, those additional qualities become attributes of each node that can help but are not really representing the graph in its edges. A color is not part of the graph in the sense that it's a part of a node in the graph, but a letter is. For example, a letter D followed by letter O, followed by letter G—you have "dog." If you mix them together, if you don't know if it's D, O, or G—those are in the graph. Those are just three letters. The D is in positions, the O is in positions, and the G, right? But if you don't know what's the feature in there, you just have—once I get to D, it's easy. Once I've detected a D, I'm already past the first level in the hierarchy, and I've detected a D, it's easy. D's are D's. There's no question about it, and I know where it is, and we can go with them. The problem we're dealing with is the first level of sensation, where you have a sensor reaching out to the world and detecting some weird property that you're getting back, which might be in shadow or shade or something else. There are a whole bunch of extra things that the brain or body does to help you figure out features, like how the retina compensates for changes in light and how you're able to do divergence and whitening type of things. There's a whole bunch of additional stuff that helps you figure out the quality of the feature, but the distance of the feature—where it's positioned relative to the body—is the most important thing. Again, when I get to the D, it's not a problem. The real problem is trying to recognize a D from just little patches that look for little edges or lines. That D could be black and white, white and black, written in different ways, with noise, or 3D with a shadow effect. The general problem with sensing a feature is that if you're looking at this thing and saying, "If I'm looking at an edge of a—" When you look at the wall that's coming in, it can be difficult to determine what's on that feature. The location can be known, but deciding the actual orientation and quality of the feature can be challenging. I don't think the dog is a good example. The dog is—yeah, I know what's a D, O, and G. It's more about how to determine the D based on sensing little parts. Here's another way of phrasing it: you have a D, and now I have a sensory patch that's looking at it like this, or like this.

There's almost a continuum of things here. That's the problem.

I think the ultimate goal is to binarize or quantize this. That's what we have to do, though I don't know the answer—it's a difficult problem. Once you know it's a D, it's easy. I've already classified all these little weird things as D's, and that's it.

This relates to the stretchy graph I made, because these different shapes are some relative set of features that are fuzzily stretching morphology too.

For demos and what we build, the obvious one is: you learn an object in one pose relative to the body, then go to a different location and recognize that same object again. That's hard for deep learning systems. Would that involve just one module? That could be one module, or you could use multiple modules. You'd probably start with one module, like in our columns paper—do one thing, then add more for speed. You could start with one sensor and one module, and that sensor moves around to build a model. Then you could extend it to multiple sensors in one module, but then you can only use one at a time, since that's required to learn and infer. You could still do that, then add multiple modules, and now you can do grasping with a finger.

For a demo, multiple modules would be best. Thinking about vision, you have an object, learn what it is, move to another location, and see it again. I can do that with one module here and one there, since the module can move. It's slower, but on a computer, we can do this really fast. If I have a camera, I don't have to move it physically—I can just pretend, attend to different points in the image, so this could be done with a single sensor. I can subsample parts of the camera image, so I don't have to move the camera, just process the image differently to focus on different areas, like covert attention.

If I had a camera doing this quickly, it would work. Even with a single camera, it would work if I moved it, and it should work as I move it. After moving, it should still work. That would be impressive. Or, once you get it after moving, you should be able to get it to work as you move it—move the camera around the object. That's easier, just tracking. From an outside perspective, that seems more impressive, but if it's easier, then fine, since that's a solved problem. Once you lock in on something and move around it, and keep saying, "it's still a dog," that's easy from the external point of view. That's not what I'm mentioning. It's more like, I don't know what you're saying, but that's hard because you have to update all these locations as you go. But I see your point, so let's put it aside.

The simplest system to start with would be to have this plus this and one of these, if I could move that. Second would be this plus this plus multiple of these. Then I could look at this from different positions, with different sensors helping.

For example, imagine you had cameras, but one was obscured and another wasn't, so you could use input from the one that's not obscured. Are you thinking each module is a separate camera or a separate image patch? In that case, it would be a separate camera—a separate, movable sensor. That gives you the ability to recognize an object even if some cameras are obscured. If I learned an object, and now some cameras are obscured but others at different positions are not, I can still figure out the object as long as some subset can see it. It's the same thing.

I'm not talking about obscuring, like losing the camera. What might be better is that no camera on its own has enough information to recognize it. Together, all four of them can. It's interesting—you're trying to recognize something, and various sensors may or may not participate at any point in time. As long as you have a sufficient subset working, even if they weren't used to train the object, they should work. If that's not impressive, fine. I'm not trying to decide. You can imagine each camera has its own deep learning network, but they don't have the exact same input. They're trained on images, so if you can recognize a shifted object, you could apply that same system from a different angle. It's the same thing.

To me, it's more interesting and impressive from a biological perspective because you know the internal mechanisms. The version that would be interesting is if no camera on its own can sufficiently disambiguate the object, but multiple ones together can recognize these things and all the poses. I think that would be pretty good. I'm thinking vision, but what's really interesting is that it could be anything. You should be able to do this with ultrasound sensors or anything that can detect some feature at some location. For example, imagine a robotic hand—our hands can't really detect anything until they touch it. There's always uncertainty about reaching into darkness because you don't want to hurt yourself. If you have vision, you can see where you are and that nothing is in the way. But imagine if your robotic fingers, in addition to touch sensors, had little ultrasound sensors to detect how far away things are from each part of your skin. This system would automatically be able to say, "I know what I'm about to touch," before actually touching it. I'd even know the orientation, so if I was going to grab something, it would be like looking at it, but using sensors in my fingers. They would tell you, "I know this thing, I know its orientation, I know how to grab it." 

We don't have to restrict ourselves to vision. You can build an object recognition system that works with other types of sensors, like ultrasound, radar, or even sound. The system should work in all situations. It can be better in some than others, but my point is, we can think about vision, but we shouldn't restrict ourselves to just vision. Maybe that's what people will respect more, but to me, it's cool that you can do it with any kind of sensor. It should work.

Building this plus this, plus one of these as a starter, then multiple of these, and then adding these components would be additional steps. Are you thinking there's a one-to-one correspondence between what and where modules? That's an interesting question. The way I've drawn it is that way. It's interesting how the what columns and the where columns are separated in the cortex—they are not next to each other like I've shown here. One obvious potential reason is that the what columns have to communicate with each other locally, and that information is directly shareable, whereas the information that goes between a what column and a where column has to go through a pose transformation. That may be centralized, maybe another part of the brain, like the striatum or claustrum, or parts of the basal ganglia that look like they're doing reference frame transforms.

Now the question becomes, is there a one-to-one correspondence here? My first guess would be yes, but I don't know. It's certainly simpler to think about as a one-to-one correspondence. I can figure out what would work as a one-to-one correspondence, so maybe that's what we should do. This could explain why the where columns and what columns are grouped together, including in the hierarchy, which we haven't talked about in the where column space. The short answer to your question is we don't know. The long answer is I would assume there is a one-to-one correspondence until we know the reason why there isn't. Why would we be better off not having one?

Is there any case in vision where pose relative to the body is tricky to figure out? The pose of the feature? Yes, in 3D. I think we should take a very abstract view and say, first determine the location, then take these bits and run them through some algorithm that does classification, like a spatial pooler or something else, to quantize the input. Perhaps even at the first level, the orientation is unknown. Sometimes we do know these orientation columns, but what if at the first level in the hierarchy, I just couldn't tell the orientation? All you do is quantize these inputs into different categories. I think the system would still work. Once I get up to here, the orientation is clearly known, but maybe not down here. I don't know. I don't want that to be a stumbling block. We can start with just taking these sensory bits, getting some location, and trying to quantify or classify the input into discrete buckets and orientations. We can leave that as a separate problem—how do we do that? But I think the whole system should work even if I can't solve that right now. I'm not sure, but I think so.

This black line, formerly known as the bus, is not a bus. I'm trying to understand its role. We talked about this in the meeting earlier this afternoon. There is no bus. These lines just show who is sending information to whom under different conditions. It does not imply mechanisms or exactly how information is sent. I'm just routing information about poses and objects, and no more detail is specified here about how a node would send a load. Is there a common communication structure or a messaging system? It's just more of a block diagram, and there is no bus. That's how we get rid of the bus.

The reason I added the word "bus" is I had this idea that you can take a module like this and just stick it in the system, and as long as it knows how to communicate with everybody else, it should work. In fact, yesterday, Marcus brought up an interesting idea. He wondered if you could have pre-learned modules up here, like face recognition modules. Modules that are not pre-learned, but come with some graphs already built in, or a bias to certain types of graphs. Maybe evolution said, for these face recognition areas, we're going to structure the graph for you, and assume it knows what faces look like generally. So not only are you doing a stretch on the graph, but you're going to assume everything's a face when you look at it.

That's an interesting idea. Marcus is thinking you could build a face recognition system with a built-in genetic bias to recognize faces because the graph is so pre-structured, and plug that in. They don't have all the same data everyone else is looking at, but it tries to think everything's a face and distinguish between faces. It seems like we have some innate ability to recognize and distinguish faces and predict. There was the Halle Berry neuron—what's that? It's a Halle Berry neuron. I don't think that's built in. There are so many different versions of that I've heard over the years. That's right, it's in Bill Clinton. I think that's another interesting idea: once you've got this infrastructure working, you could build modules up here that are more specific to different types of problems. Maybe cortical columns are not identical; maybe some come with a genetically predisposed graph structure.

If you have this prior that faces exist in the world, that in turn helps train your V1, expecting to sometimes see a face, expecting to sometimes see eyes and a nose and a mouth in a certain orientation. That allows you to train your V1. Your V1 has something to latch on to now. I would put that in this understanding hierarchy, because I didn't talk at all about it. I said, oh, it's interesting, this stuff goes in this direction, but of course we know these things send things back down to layer one in this direction. I was assuming you could treat them all as voting connections. I don't know if they're voting connections. The output of the models above also have green outputs. Going up or going down? Either. They're different, I think. The output of this green output could be fed to here. It's like the expectations from above. Yes, it could. That would only make sense if these models and these models both had a model of the same thing. That was the example of a large A and a small A. Or more generally, it could be many-to-one or one-to-many, just like between any two modules. There are a lot of variations on the theme here.

I don't think I understand here. I was thinking specifically about these layer six connections that go back to layer one, primarily in the lower groups. These are massive connections, so they're telling this thing something. I don't know what that is yet. But there are also skip connections. What Luiz is pointing out is that this module here can get information from here. That's a direct layer, like a layer three to a layer four here. This becomes the layer four input here. The output of this might be layer three. That's a classic connection that Felleman and Van Essen would talk about. But if I have a sensor down here, this guy also seems to get input from a set of sensors here—a broader set. It's basically getting sensory input from a larger sensory patch. It could try to detect some bucketized feature over this large area, or get this pre-processed green one. If there is a real consensus here, like there is an answer in green, like we know this is the letter B, why would I even want to look at the sensory input?

There might be a precedent here, because these both go to the same layer. They both go to the input layer here. They have different places they terminate, but they're both going there. It's okay, they're both inputs. One possibility is that if I have a green message that says, yes, we know this is the letter B, then I don't want to look at the low-level sensory details. I could, but maybe I would just take precedence of that. But if these guys don't know what it is—maybe it's a big B, and the low-level V1 column only looks at small pieces like this, and it's too much to build up a model—this guy is looking at big pieces. He says, I don't know what a B looks like in a big picture. So this guy can say, if these guys don't know anything, if this is a question mark, then I'll look at this. But if these guys do know something, then I'll just ignore that. It could be like that. That's just one speculative idea.

It seems reasonable to me because I can see how large things, much bigger than the receptive volume of this feature detector or sensor, would be very difficult to model down here. It's just too big a space to occupy. Do we also have a gating mechanism between those parts here? Between this and this? No, between the lower layer and the upper layer, like the thalamus. Ah, thalamus. Oh, God. Okay. I was hoping to avoid the thalamus. Sorry.

We can avoid it, or I can speculate, which you probably want. We should avoid it. I think it's fine. Another question: we've been talking a lot about object detection and optimization and not about predictions. Is that going to come in later at some point? You have a model, right? And with a model, you can make predictions. So we have the data to make as many predictions as we want. I guess the question is, under what conditions should we be making predictions? Locally here, let's see. If I know what model, what object I'm viewing—so let's say my green is determined, and we say we're all looking at the letter B—and then I somehow know the location of my sensor relative to the B, I should be able to run this model backwards and predict what my sensor should see. The information is there locally; as my sensor moves, I should be able to predict what feature I'm going to see. So it'd be a local thing here. Clearly, if you violate your prediction, that's useful for attentional mechanisms, which we didn't talk about at all. But there is the infrastructure here for making predictions. So we need to find out why we want to make predictions. One example: if you look at this situation, the table is covering the chair, so I can't see everything in the chair, but because I have a model of the chair, I can predict the way the chair is. If I know I left my cell phone on there, I know how to reach in and grab it without even seeing the chair. I can visualize what it looks like. So being able to understand the structure of things you cannot directly sense is pretty powerful. That's a type of prediction. I think what might happen here is we start building some subset of these things that we can test and build, and it's a joint exercise to figure out what that is. I won't say I know what that is. As we do that, we'll think of ways of doing clever testing on it. We'll think of problems that we haven't solved that we want to solve. It will start being a scaffolding in which we can build more and more detail.

That's what is exciting to me about this whole idea. I see this as a scaffolding in which you can start building the whole thing piece by piece, figuring it out as you go, where before I had no idea.

Does the output of a "where" module give the pose of the feature relative to the body? Yes, that's the feature relative to the body. It only does that, by the way, if the green answer isn't known. For example, if I see there's a letter B, I want to send down to this module down here: Oh, I'm seeing an edge at this location.

It's the same sort of priority. If I know there's a B, I want to send the B down here, because this becomes the input, just as this became the input to that module. Then I want to build a model of the B's relative to each other, relative to A and C, but if I can't recognize it's a B, I'll send down the low-level features and learn what a B is. This module locally has to determine the feature relative to the body. It has to do that for any of this to work. It only sends it down here if, for some reason, I need to attend to that feature—either because I decided to attend to it or because I don't recognize the object yet. If I can't recognize the object, I just have to look at features.

Why can't I always send it and let that module decide what to listen to? Sometimes it has to look, but it depends on how you want to consider the messaging. The same question applies here. I can give you the details or the big picture. If I know it's a B, I don't want to look at the details. Now I'm trying to build a model to let it work. You could send it here. Don't think of these as strict; it's just information being available. But I wouldn't always want it. My goal is to recognize the highest-level object I can, to build the biggest, highest-level graph possible. When I look around and build a graph of this table—Marcus is over there, Luiz is over here, I have my coffee cup, my own pens, and my laptop—I just built this model quickly. I don't really care about the exact orientation of the pen or the edge of the pen relative to the cap. These modules are looking at that, but I don't need to know that at the higher level. If I didn't understand the scene at all, I would just be looking at little features.

Generally, you want to prioritize higher-level green object features. This is just a normal type of feature. You want to prioritize that down here, assuming I've already learned the components. Once I've learned the components, there's no point in relearning them here. But I could attend to it if I wanted to. If today I really care about exactly where that pen is, I can look at it carefully—the cap is this way, not that way.

If we only have one module, we don't need this extra structure. You could start with that and add more, but then you need those little ones. No, you don't need little ones because this allows you to build your model with different senses. I can learn this object with my finger, another finger, or my eyes. It's still one module you're learning. I found it interesting that I could touch the pen with any part of my sensory skin and it would work; it just doesn't matter. From a machine learning point of view, maybe that's not interesting, but from a brain point of view, it's impressive. You have tens of thousands of these modules, and any one will work. That's pretty cool, but you don't need to do that. You can just pick one and go to this module, have a module with one sensor.

The other beauty is that whatever you use to design this combined module, it works down here too—it's pretty much the same thing. There's no advantage to having this here until I have multiple modules. Maybe that's the way to think about it. You could argue this is really too simple. A sensor, maybe the part of sensing a feature, is not so simple, but if I have a feature of orientation translated into body location, that's easy. Then I have to build these two things. I argue we know enough to get started on this. I'm looking at Marcus—do you think so?

I'm still figuring out what's going on. This is like what we did in the columns papers. We can make a module that does this, and we can easily do the reference frame transformation between them. These module models won't have everything we want—maybe they don't have stretchy graphs or certain features—but they can work. We can build and rebuild them. Over time, we can improve this. You could almost take the code base you used for this and plug it in down here with a different learning rate. Then you could go fast, or have fast learning down here. You can make interesting variations. In the brain, it needs to be very slow learning because these modules rely on that. Even with one module, that's the advantage—because that would be slow, and the one on top is for really fast one-shot learning. But I could make these tasks too, as long as they didn't have a hierarchy. Then I would be able to stabilize on the green. It doesn't matter because I'm not using it. It all needs to be for voting.

These are variations on a theme. I don't think it really matters.

I do the one-shot learning. I look at the table—the example with the food on the table—and you're looking at a bit of potatoes, the green beans. You build this one-shot module here. If I wanted to recognize it again, I’d have to walk into the room from a different direction and still look for the green beans and the potatoes from a different orientation. I wouldn’t be able to do this one-shot recognition of everything, but I could if I held these guys on really rapidly. It's an obscure point. There are various dials you can turn here to make these things into different things. Defining this as one module, we've identified two big problems. One is how to discretize features—that's the big problem. The reference frame calculations are easy. The other problem is being precise about how these graphs work and how these things work. I think we know enough to do that. I feel I know enough. We'll find out, but that would be another big project.

Given a feature at some location in space, converted into a spatial location of the object, there has to be a quantity, which is the pose of the body of the object. Now we build this graph with the idea that we’ll be able to infer which object we’re viewing based on a series of observations. I think we can do that. I think I could identify the steps that have to occur. I don't know how to code it, but I can identify the steps. I would just start with something like the columns floating, columns paper. That would be like this part here, with temporal pooling, so you have a stable representation of it. These are temporal pooling here, but these are not just features—they’re poses of features.

That's the difference. In the Columns paper, we just said there's a feature at some location, which is like no pose. It would work and—no reference. I would almost do their edges of the graph. The edges are relative positions of two features, well, in the columns paper. But we didn’t have that. I think that's what an object is. I think the object, this model in here, might actually have both. I'm not sure yet. It might have both. You might have what we did in the columns paper, which is remembering features at pose, feature at pose, feature at pose. In addition to that, you can build the graph. I think there are some advantages to doing both. If you have a very unique feature, you don’t need the graph. What's the difference then from the Columns paper? Same thing—in the Columns paper we had that too, a very unique feature you could recognize right away.

I think the difference is, is it a reference? The algorithm could be the same; it’s just what are you pooling? For example, the way we did layer four—remember, layer four was something like your feature at some location. Let's say it was this feature at some pose relative to the object, so we just extend it a little bit. You could dynamically build up this graph as needed with this data. I wouldn't have to store this up front. I could, but I could also dynamically—here’s an instance, which is advantageous. In some sense, this is an efficient storage mechanism. This is somewhat inefficient—this is an n squared type problem. It wouldn't be, but potentially an n squared problem. This is just a linear list of features at poses. I think you could calculate, given any two of these things, an edge of it. Therefore, all the knowledge you need is contained here. It brings up the idea that you might be able to dynamically do the equivalent of the graph with this information. I'll have to think about that. You can. Coding can. As a neural system, yes. Also, remember, we can run things much faster than neurons. It might take hundreds of milliseconds or tens of milliseconds in the brain, but the brain doesn't want to do that. We might be able to do it super fast and just run through all these combinations really fast and just—bingo. I don't know, it's an interesting question.

Just imagine diving into details. What am I getting here? I don't want to start just getting out of it. I have to leave something for my talk or doctors. Okay, that's it for today.

As an outlook for a cool demo on a larger scale, I would say that any behavioral task would be easier to solve with this as a backbone to understand and conceptualize the world, as opposed to using high-dimensional sensory input. It would be cool in a future step to test this on a behavioral task or something. When you say a behavioral task, what do you mean? Anything you need to do in the world—like Subutai had the example of picking up the phone from the chair—would be so much easier if you have all these models to understand the world, as opposed to having the raw sensory input from your retina. I agree.

So here's my problem, Viviane—that's my number two up there, right? Number two is motor, right? Work on the motor stuff. My problem is, you seem like you're right. All the information we need to solve really interesting motor problems is sort of here in the system. But I don't know enough about what people have done in this space. I don't even know enough about what they've done in the neuroscience literature. I don't know anything about what they've done in the robotics world. To me, that's a big open—oh crap, there's a lot of stuff to learn there. To me personally, maybe you could figure out, I don't know. So to me, yeah, I want to work on that.

It's a bigger, somewhat daunting problem because I don't even understand how to phrase the problems. I don't know what specific task or problem I would want to solve. If we could articulate very precisely the things we know the brain can do that people don't know how it does, that would help me think about it—maybe others too. I agree, it's just an unknown space for me. 

Motor commands in general—what is a difficult motor task for a robot hand to do? I don't even know that. Sometimes I'll say something and Subutai will say, "That's easy," and everyone else isn't sure. Other times, people say something is easy, but it's actually really important. I don't know what the equivalent touch points are in robotics. There's a lot of room for improvement in the field. Having such a model in the background would make things much more efficient because it adds so many inductive biases for the kind of 3D physical world we live in. It would make it much more efficient to learn and understand. But I need specific problems that people say are hard to solve. That's what I need.

I don't know what those are, to be honest.

Christian.

Sorry, I just didn't make the connection between Chris and Christian. Let's start talking about that. Maybe you know better than anyone here what the difficult challenges in the field are. Maybe you can propose something, give a short presentation, and get the ball rolling. 

Yeah, sure. I can make a short presentation on some of the challenges in the field currently. That would be helpful. Another thing that scares me is that anytime people start dealing with physical movement, you either need a physical robot or a simulation, and people always say simulations don't really capture the problem. I don't want to delve into building physical robots here. There's that aspect too—what makes a sufficient demonstration that you can do something? Is the simulation good enough or not? It's a lot of work to do simulations, so just think about that. We don't want to start a project that takes a year just to get the mechanical work done. Luckily, simulations are a lot easier now, but physical robots are still very hard. I don't think we have to deal with that, but simulations are still quite hard. How real is this kind of thing? I've heard that simulations are discounted because they don't really deal with real problems. I don't think that's true anymore. It used to be more true in the past. But recent environments—sorry, I cut you there.

Reasoning inside virtual reality might be an area to build. If you don't want to build it in real life, there are simulation environments trying to be realistic for testing robots, but I don't think that's what we want to build because those are slow. No, they're not slow, that's my point. The ones we're discussing are very realistic and fast. Maybe five years ago they were slow. I was looking for a 2021 paper—don't compare me—I'm seeing about four times real time. What's slow? Doing what is slow? We have to run multiple experiments, right?

Maybe. That's the question. If you're just trying to simulate in real time, yes, but real time is too slow. If you're trying to do machine learning, you need a million samples, but that shouldn't be the point. This should be fast—super fast. That should be the advantage of having this model: we can dramatically increase sample efficiency.

This really should do one-shot learning in the sense that sensing one feature at a time is sufficient. I can't learn a new composition or object in one presentation, but I can do it by doing a series of presentations, one at each location, and then build a model.

It's close to one-shot, but you shouldn't have to do many presentations unless you're trying to learn one of the lower modules that are learning statistical regularities in the world. You'd have to do multiple training sessions, but this module should be clear pretty quickly.

Those are interesting implementation details. Viviane, it would be great if you could present, as Lucas suggested, some of the key problems that people would think are amazing if you could solve.

Yeah, sure, I can do that.

Just to chime in, I think we would want something a lot faster than real time to train these models.

For example, the habitat we're discussing is running at a thousand frames per second—something like that. That's crazy fast. It has to be much, much faster than real time. But machine learning is still super hard on those, as long as we're not doing machine learning. If I were to prioritize these, I would say we can build a system like we talked about earlier that recognizes something from different positions. That's impressive. We should do that. The motor and behavioral parts are going to be harder and will take more discovery. We don't know how to do that yet. It may become obvious soon; that's my hope, because now I have a framework to think about it. But it is an unknown, and there are a lot of unknowns. I wouldn't take that as the only thing to do. We can start working on it, but I wouldn't wait until we figure it out. I don't know how long it will take to figure out those components. I wouldn't say start on this tomorrow, but I think this is where we could show the most impressive results in the long run, because in the end, our brain is trying to solve behavior. I agree. In this world, AI and robotics are going to merge into one. It's all the same mechanism. We should be able to solve really interesting robotic problems.

All right, you can make a presentation on what would be an interesting problem.