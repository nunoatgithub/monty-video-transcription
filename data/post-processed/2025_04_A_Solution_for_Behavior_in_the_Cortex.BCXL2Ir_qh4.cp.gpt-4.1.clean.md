Okay, cool. I hope I didn't overpromise too much.

I made a long, handwritten list with all the requirements and wrote solutions for each, checking them off in my head. Then I made those slides yesterday and today, and I noticed a couple of holes. So, it's not a solution to everything—there are still a couple of open questions with what I'm proposing. Also, not everything I'm presenting here is something we've already agreed on. I'll start with what we've mostly agreed on, and then clearly separate where I'm putting together ideas we've discussed but haven't settled on yet. I think if we pull in those previously discussed ideas, they could solve almost all the issues or problems we're trying to address. The main open questions are about where this would happen in neuroanatomy and some mechanistic questions around matrix cells and other aspects. That sounds promising, because in some sense, that's a less important issue. Hopefully, it would address, on the conceptual level, most or all of the problems we've discussed.

This is a dense presentation. I spent a lot of time trying to make it digestible.

I tried to put everything in reference frames and make it visually useful for people to understand my point. Still, some of these ideas are complex. Feel free to interrupt and ask questions whenever. We can discuss everything. Chapter one: there are four chapters and then an epilogue. Chapter one is the Problem and Constraints. I'll go through the problem space, which isn't trivial. I feel like half the time we spend is just talking about the problems we're trying to solve and all their different aspects. I tried to put together a complete list so we can check those off in the next chapter with the solutions, to make sure we're actually solving everything. You may not remember, but in my book I said that the way to solve problems is by defining the problem. If you define the problem sufficiently, the answer presents itself. I agree—if you really get the problem down, the answer is obvious. I was just talking to Will yesterday about how, in the beginning when I started at Memento, it was frustrating. You'd think, "Oh, this would work," and then you'd come in and say, "No, brains can't do this. Maybe you can do it in a computer." In the beginning, that was frustrating, but now it's great—we have one more constraint to find the optimized solution. Each constraint helps us narrow down the solution. Hopefully, this is helpful.

Number one: we want to learn a model of object behavior. That's the task we set out to do. There are many objects that move, and we want to be able to learn those behaviors. We want to include any arbitrary change in features and morphology over time. It could be something moving, something changing features, color, or brightness, or something moving in multiple directions or degrees of freedom. It could be multiple objects moving, dependent on each other, or a compositional object moving its parts in a coordinated way. There are many versions of behaviors, and we want to model all of them with one mechanism. We also need to be able to observe all the changes so the model can be learned in a column—either by following the movement, circling around, or having many sensorimotor patches covering the object.

We've talked about all of those before. I won't go into too much depth on this one, since it might be a combined solution of intelligent policies and voting.

Second, after we learn the model of a behavior, we want to use that model to recognize object behaviors. First, we want to recognize the behavior on the same object we learned it on. For example, we learned the stapler moving up and down, and we want to recognize it again when we see it. But we also want to recognize the same object behavior on a different object. For example, we learned the stapler, and now we can recognize that behavior on the hole puncher. The extreme example we discussed is the walking or dancing banana—something you've never seen before, but you can still recognize that the banana is running, and you can imagine a dancing banana without ever having seen one. Brains can clearly mix and match these object and behavior models.

We also want to be able to apply a behavior to an object at any location, orientation, and scale. For example, we might have learned a hinge behavior and then apply it to a door, a laptop, or any object with a hinge that opens and closes, like a book.

We also want to recognize where we are in the behavioral sequence. If we see a closed stapler, an open stapler, or a wide-open stapler, we want to know where in the sequence we are. We want to recognize the behavior at different speeds. We might have learned one speed of stapler, but we want to recognize it faster or slower, and we want to do this with single or multiple columns. That's a bit questionable—how well we can do this with a single column.

For very simple behaviors, like pushing down a button and recognizing that with touch, it should be possible with one column. But anything more complex and fast-moving might require multiple columns.

Next big topic. Those are the first ones we talked about a lot and have a solution for. Now I'm going into additional things that have come up lately. Can I just comment on the multiple column idea? In my mind, a single column should, in theory, be able to do everything, but practically it may not be able to. I don't think we should assume multiple columns are necessary for distributing the knowledge of movement or the object. That's why I'm saying a simple behavior, like pushing a button and recognizing that from just touch, needs to be learned in one column, since we are only getting touch input to one column. I don't want anyone to feel like this violates the columns hypothesis that columns all do the same thing. It's just that, practically, you might need many columns to do this.

We could almost phrase it as a magnitude issue: in terms of learning efficiency, there comes a point where a behavior is so complex that, in theory, a single learning module can still learn it, but it would take so long to revisit all the parts that it would be practically pointless. It's the exact same mechanism; it's just faster with more columns. Here, I'm even just talking about inference. Recognizing something again is hard if you have more global movement, like a person walking. If you just have one column and the movement is fast, it's hard to see enough changes relative to each other to recognize it again. That's maybe something to try with extra treatments, like straw vision and random dot movement things—just given enough time, do we recognize it?

Okay, so next bigger topic: learning associations between objects and behaviors. If I'm seeing a static object, like a stapler, and I've learned the behavior of a stapler before, I know I can push that stapler down. I have an association that this object can move in that way. I learned that, and how is that represented? Also, seeing the behavior can bias what object we infer. That's the example we talked about before, where we don't have any features of the object itself—we just see movement. In fact, if you show a frozen frame, you don't know what it is at all, but if you see it moving, you can infer that it's probably a person. One object can have multiple behaviors. The stapler can be reloaded, you can change the deflection plates, and apparently there are staplers where you can rotate the top part to staple at an angle. Lots of objects have many behaviors.

A behavior can be in an arbitrary location, orientation, and scale on an object. We already had the opening and closing behavior that can be at a different location, rotation, or scale on a book or a door. A mailbox can also open and close, and the lever can go up and down at a different location on that object. We want to be able to assign behaviors at any location, orientation, and scale on an object.

Another larger topic is making predictions about an object at an arbitrary point in the behavior sequence. We have a model of the object and a model of the behavior. Using those two models, we want to predict: if I move my sensorimotor from here to here, what am I going to sense? That's using the behavior model to make predictions about the morphology at any point in the sequence. We want to do that for object and behavior combinations we know, and also for a behavior applied to a known object as a new combination. For example, we learned the stapler and its behavior, and now we want to apply that behavior to a book for which we've also learned a morphology model, but we've never seen a book open and close. Again, we want to predict: if I have my patch here and then move up here, what am I going to sense? When either one of those two models is not learned yet, no predictions should be possible. If I've never seen the hinge behavior, I shouldn't be able to predict anything about the book opening. If I've never seen a book before, I need to learn a model of the book before I can make predictions about how it will look when it opens and closes.

Another constraint is that we don't want to be learning key frames or snapshots of the object's morphology, where we have to store how the object looks at different points in time. We already know the object. It's not that once the stapler opens a little more, I have to explore the whole stapler again and learn a morphology object again. Once it opens a bit more, I don't need to relearn it in every orientation.

If part of the behavioral sequence is occluded, we are still able to make predictions about what we're going to see once it comes out of that occlusion.

Finally, this one we haven't really talked about yet, but I think it's relevant and what I'm proposing also solves it: we want to be able to use the behavior model to compensate for object movement when we are trying to recognize morphology. If we have a moving car, it's moving, but we can still recognize the car, so we can compensate for the movement of the object when recognizing it. The same applies to subcomponents, like the stapler opening and closing—we can still recognize the top of the stapler and use the behavior model of how the stapler top is moving to compensate.

Could you try again one more time? Basically, being able to deal with moving objects when we are trying to recognize the whole object or its components. How does it differ from behavior applied to a known object? This is for inference of the object—now this is about the morphology of the object. We're trying to recognize the car or the stapler, but the object itself is moving because it has some kind of behavior. To me, the car driving is a funny behavior because the morphology of the car is not changing at all; it's just changing its location in the world.

So, this is more like the car is moving in the scene of the world, and here the stapler top is moving relative to the staple. But has the staple example already been addressed earlier, or did you already talk about the need to get rid of that?

If we can predict what we will see at a location given the behavior, then that directly enables us to infer the morphology, if that's already known. When it's moving, this straightforwardly comes out of this part, so it's not a huge additional problem. I don't see it as something different; that's my question.

Maybe it is, and I just don't understand it. I thought of it as different because when we talk about the larger capabilities of Monty, we discuss modeling object behaviors and dealing with a dynamic world. Dealing with a dynamic world where objects might be moving doesn't necessarily require modeling object behaviors; you just need to compensate for the object movement. That's interesting. Take a car, or just something on my desk—I can move my coffee cup to different positions, but I don't consider that any kind of behavior. However, a car going through an intersection is a model of an intersection with components called cars, and they follow certain paths and behaviors. I don't expect a car to jump over the median. In that case, it's a behavior of the intersection: cars are a component of the intersection and exhibit certain behaviors. But just physically moving something to different positions doesn't seem to be a behavior of anything. Does that make sense?

I agree with that. I think it's maybe just a linguistic issue. Maybe, if I'm understanding what you're saying, it's a special case of behavior where we can have an object in a behavioral state. For example, if someone says the stapler is open, we can predict what we'll see as a function of the behavioral state, but while the object is moving, there's an additional compensation that's necessary. Is that what you're saying? It's not just that there's a behavior; it's the fact that there's movement going on that presents an additional challenge. Maybe the slight difference is that in this scenario, we haven't actually recognized the object yet—we just recognize the behavior. So it's more about the initial inference.

We're still trying to recognize the stapler, but it feels like it's the same mechanism, just over the hypothesis space. Isn't that like the motion capture dots—capturing parts of the behavior, which tells you what the object is?

In the motion capture dots, it's not possible to really recognize the morphology because there are no clues about morphology. You infer the morphology and then look for it, like there should be a human or a person there. Everything up to now made complete sense to me; this last point is a little confusing. So the question is, is it worth spending more time on, or should we just move on? Maybe it'll be clear later when you talk about it. It's not that big of a deal; it'll be a short last comment in the end. Let's leave it at that. If it's not critical to the argument, let's keep going.

Lastly, just to note that in this presentation, I'm not going to talk about how actions relate to behavior models—how we use this behavior model to interact with objects or to know how we can interact with objects. That's not something I'm going to solve here. Actions will be next. I'm joking. These slides are awesome, by the way. Visualization looks great. This is a great presentation for conversation. That's just chapter one.

Does that all make sense? Did I forget about any problem or constraint? I can't think of any, but it seems pretty good. We can bring them up later if needed. If someone's watching this video on YouTube later and you haven't seen the solution yet, you can pause the video here and try to solve the puzzle yourself. Good luck with that.

All right, so chapter two: solutions we have so far.

My plan is to go through each of these points and check them off. As a prerequisite, I want to revisit our current theory—our confident, stable theory—about modeling and recognizing objects, like the morphology model, as a baseline. Many ideas for object behaviors are based on that and are just slight twists on it, so it's good to recap briefly.

I'm also trying a new visualization look. I spent all day yesterday trying to get all the information in a way that doesn't look super confusing.

All right. We have the input from the sensorimotor system that goes into the first oral nuclei of the thalamus. There, it consists of some movement information, which goes into the border between layer five and six and updates the location in the object's reference frame. We also have feature inputs, such as feature orientations like oriented bars in V1, and non-orientation-dependent features like color or temperature, which go into layer four. We learn associative connections between these features at locations in the object reference frame, and that's where the model of the object is stored. The model is essentially features at locations relative to each other, and over a series of movements—sensing and moving—we can narrow down what we're sensing and form a representation of an object ID. This can then be communicated laterally to vote and narrow down which object we're sensing faster. The object ID can also be sent forward through the classical feedforward pathway from layer three to layer four of the next higher cortical region.

Another point to highlight is that this location is unique to the object. Any location SDR here only exists on this specific object; it's not an X, Y, Z coordinate frame used for all objects. It's unique to each object, so looking at this location could already tell you which object you're sensing. The movement input and how you move through the object's reference frame is universal—movement to any object's reference frame updates it in the same structured way.

Does that make sense? If there are questions, it's important to resolve them. Sometimes it feels like you've taken years of work and made it seem simple in a minute.

It's something that needs to sink in and isn't easy to arrive at, but now it looks elegant and simple. I hope the future slides will look as elegant and simple as well. The goal is always that, in hindsight, the theory should seem obvious—how could it not be that way? But figuring out the hard part is the challenge.

Lastly, we also infer the orientation of the object, which is represented in layer 6B. This orientation can use modulatory backward projection to the thalamus to transform the movement and feature orientations that go as input into the column, so we are transforming movement and sensed features from the sensor's reference frame into the object's reference frame. That way, we can recognize any object at an arbitrary location or orientation in the world.

There's an existing question about the nature of the reference frames and the 2D/3D MD, which isn't being addressed here, but it doesn't seem essential at this moment.

We may not have discussed this before, but do we agree that scale might also be represented in the column and then sent back to the thalamus to scale the movements and features? That's the idea. Scale is something we haven't really decided on where exactly it happens, but the general mechanism should be similar to orientation: we can scale the incoming movement vector by whatever scale we're detecting the object at, and thereby recognize it at varying scales. We have a working hypothesis about the mechanism for that, which is changing the background frequency. We don't need to go into it here, but it's not like we have no idea. That would occur in the thalamus; the theta cycle, whatever it is, comes from the thalamus. For context, there would be additional scale invariance from columns across the hierarchy, where columns at higher levels tend to have larger receptive fields and are biased toward recognizing larger objects. That's less flexible, so within a column, given sufficient time, it can, like rotation, arbitrarily scale the inputs to recognize it. There's an assumption that different columns get different inputs and different motor inputs, but within vision or touch, they'd have inherently different scales of movement and feature sizes. That's inherent—it's adjustable within a range, but built in.

For a complicated sensorimotor system like vision, the information that's flowing—the location and the features—would be a cluster of features and a cluster of locations.

I don't know what you mean by that, Michael. If you're looking at it with your eye, you'll see multiple edges and regions of color. This is a single column, so one column sees just one small patch of the retina and models what it can from the input from that small patch as it moves. Obviously, many columns would be observing different parts of the object at the same time, all doing the same thing but at different locations and with different features. In the retina, the movements tend to be coordinated because the retina moves as a whole, but with touch, for example, your fingers and different skin parts can move independently. The more generic solution is that each column is oblivious to what the other columns are doing. Each column sees a little patch, moves around, and that's all it knows. It builds models, and maybe neighboring columns are doing the same thing, but it doesn't know that. All it knows is that they can vote on what they think they're agreeing to. A single column is a single feature and a single movement vector.

This input is just a small patch, not a full image like in computer vision.

All of this happens through a loop of sensing and moving, sensing and moving.

I'll move on to object behaviors now for this recap. This part is about learning to model a behavior and then recognizing the behavior. That's the idea we had during the last retreat, so you should all be familiar with it. To show it in this new reference frame: we get input from the sensorimotor system. First, we have the movement of the sensorimotor, which is the same movement that goes into a parallel modeling part of the column. The shaded light blue represents the modeling stream for object morphology, and light purple represents object behaviors. You'll see many parallels between these two.

For the object morphology model, we get a movement vector input showing how the sensorimotor moves, which updates the location in the behaviors reference frame. It's the same mechanism but a separate reference frame from the object's reference frame. We also get feature input. In the object model, we get static features as input. Here, we only get input when a feature change is detected. When the sensorimotor patch detects movement or a feature changing or rotating—anything that's actually changing, not static—the input goes into this behavior model. Just like with the object model, we learn associations between locations in the reference frame and changes.

Another difference is the temporal component to object behaviors. The working hypothesis is that matrix cells in the thalamus could broadcast an ATI global timing signal to this column, which can be picked up by the apical dendrites of pyramidal cells in layer three. The model of the behavior is changes at locations over time. At each time point in the sequence, we're storing changes in that behavior reference frame. For example, with a stapler opening or a hinge behavior, the change detected is the movement of an edge and its rotation over a temporal sequence. This can be pooled into a behavior ID representation.

To clarify, with a stapler, any given location is associated with one change, but you could have behaviors where a particular location is associated with different changes at different points in time. In theory, this would be covered by the apical input to the L3 cell being different, so you would predict a different change. The stapler could be closing, and it would be the same, or something else. I don't know if we've checked that the numbers make sense, but in theory, it seems like it would work in terms of how many potential predictions you need. I'm confident the numbers will work out, but it's good to think about it.

Quick question: this pooling for behavior ID is over space and time. We're pooling movements at little patches in space and also pooling over time. Even the morphology model pools over time—it's a temporal pool. You can only sense one piece at a time, so you're pooling multiple sensations. Here, you're pooling multiple changes over time. In that sense, they're both temporal pooled IDs.

It's temporal from the point of view of the neurons. In the behavioral model, time is actually a component of the model; it only flows in one direction, and the changes have to occur in that direction. With the morphology model, you could touch the object in a different order, so the order doesn't matter—it's independent of the order.

One key idea that solves the problems of mixing and matching objects and behaviors is that these are two separate reference frames and completely separate models. The behavior model knows nothing about staplers, even though it has been learned by observing the stapler opening. The stapler model knows nothing about the hinge behavior. The behavior model just models movement and changes over time, while the stapler model models static features at locations—like the color of the stapler, the sense point normal, or the curvature at that location. These are two independent models, and we can now apply or recognize the same kind of movement sequence of changes on any other object that moves in that regular path. If you open a book or something else, you would still recognize this behavior even if you've never seen it on that particular object.

Would it be reasonable to say that the behavior model doesn't know about the stapler, but it knows about an edge moving because these movements are arranged in a specific way that mimics an edge? I think it doesn't know—there's no representation of "edge." It's a set of points that you and I could say is an edge. Just like the morphology model: in the morphology model, there's a set of point normals and features, but no separate representation for "edge." It's just a set of features that we would look at and say, "That's an edge." The behavioral model is just these independent points and how they're moving. On the stapler, it looks like an edge, but on another object, it could be a changing circle or something else. There is no independent representation of "edge." That's the key point.

If that were to emerge, it would be through hierarchy and composition, where the input is represented as a separate object and can be represented as an entity. If the world had edges as independent objects, then it would be an object. It's nothing about "edge" itself—it's just an object. We've labeled these features as an edge, but you could have another set of features that's a slight arc or a curly cue. All are equivalent.

In that case, given that the features on an object are restricted to a certain subset of possible features, it could help narrow things down. It might help with quicker association with the behavior itself, because if the behavior is also dependent on the input features that generated it, that gives us a way to build the associative mapping. Would that be accurate? Maybe I'll show the proposal for that associative mapping and you can see if it makes sense, because it doesn't require any dependency between these two. As said before, to recognize this behavior, you need to be observing an object that has possible changes, like being able to move in a certain way, but it's not storing anything about the object itself or its features.

Go ahead, Michael.

You've now moved to a set of features from a single feature. No, this is the behavior of a single feature. It's like the morphology model—there are many locations in the model that would have changing features associated with them. In theory, no one can observe all those at once. At this location, we observe a movement; at another location, we observe a movement; and so on. These are in a reference frame—relative changes to each other. That's what I tried to visualize with these arrows.

Does the single sensorimotor observe those multiple changes at multiple locations at the same time? No, only one at a time. This is one of the problems we had, Michael. In the morphology model, you could take your time to move over the object, discover the features, and do one at a time. In the behavior model, the thing is changing. In theory, a single column could sample all the points quickly and say, "At this point it's moving this way, at that point it's moving that way," and so on, then go to the next step in time. The model itself contains all that information, but only a single point in time can be sensed by a column at once. You risk something like shutter lag, where you take an image and miss changes. I don't know if Vivian's going to solve this problem, but it's a real issue: you can't really learn it like this. In theory, the column could, but not practically.

At the retreat, we talked about how voting can help to some degree, because then you can get a quicker sense of the state of the object as a whole. Also, children, when learning behaviors, repeat actions over and over—pressing a button, opening a stapler, and so on. But there has to be some sort of shared learning. If every column has a model like this, they can't all learn independently. Somehow, they have to share their knowledge: "I learned this part, you learned that part, let's combine them." That's a hypothesis; I don't know if it's true, but it suggests maybe retaining state within the column, though I'm not sure how that fits in the overall picture.

Maybe Vivian's going to solve this problem, so I don't want to get ahead. I think what you're saying, Michael, is more like voting, whereas what Jeff's talking about is almost like weight sharing in CNNs, where a column would transfer knowledge to another so that collectively you could sample multiple points.

You can't expect every column to observe every feature at every location at every point in time. It's hard to do, but I'm not going to get into that too deeply in this presentation. In the open questions and potential solutions section, I assume that for learning behavior, we can't really use voting because voting just tells us about the behavior ID. That just helps for quick inference when we see an object behavior, but for learning, I think—I'm seeing Link all the time repeating a behavior for an hour. Today he spent just opening a teapot and closing it, taking the strainer out and putting it back in, really for an hour, just doing it over and over again and figuring out every part of the behavior of the teapot. I feel like that might just be how it has to be done. There are not that many behaviors learned later in life, but we'll see.

It would be interesting if there's any quantification in child development about how long children spend looking at static objects versus objects with behaviors. I wouldn't be surprised if it's an order of magnitude difference in how long it can hold their attention. Maybe I'll move on a bit because there are things we haven't discussed yet.

Another thing we want to be able to do is recognize the behavior at an arbitrary location, orientation, and scale. The proposed solution for that is to use exactly the same mechanism we're using for recognizing an object at an arbitrary orientation, location, and scale, which is to transmit this post hypothesis down to the thalamus and use that to transform the incoming movement vector and feature inputs into the object's reference frame.

So a separate orientation for the behavioral model, then? Yes, pathology model. I know we haven't completely agreed on that before, but in support of that idea, first of all, it takes very few neurons to represent orientation, unlike location, so it is a small population of cells—that's not an issue. We do have the different layers in the thalamus, which would suggest perhaps there are these paired layers. Perhaps they're paired in this regard, so we can accept that for now. It seems like it should be possible to send another projection down and transform those on those relay. It may look like the same projection; the thalamus may not see them as separate, they're just two cell populations that are co-located. We don't have to expect to see physically separate projections.

Generally, just looking at this, it looks beautiful because it's an exact mirroring of the mechanism we have for object models, now for object behavior. It gives us all these nice properties that we had for recognizing objects—being able to recognize them in any location or orientation in the world. Now we can do the same thing with object behaviors, independent of each other. It just looks really nice in my mind. It fits really nicely with so many things—the movement coming in at the edge between L5 and L6, and also L5 being the motor output. It makes a lot of sense that would be where behaviors are represented.

Another requirement we had was to recognize where in the sequence we are and recognize them at different speeds. This is one we haven't totally settled on, but I think the general mechanism I'm going to propose here makes sense. Basically, from the matrix cells, we get this timing signal. The matrix cells themselves don't know anything about the behavior model; they don't know which behavior we're recognizing or what we're sensing. They're just giving a timing signal, like a metronome. Here we learn at what click in the metronome we should recognize which features at what locations or which changes at what locations. When we actually observe the sequence faster than we have learned it in our model, we are predicting the correct changes at locations, but they appear earlier than expected. We need some way to speed up that metronome. I made that error red because I'm not sure where that signal would come from and how it would go to the matrix cells. We see correct changes at locations as predicted by the model, but earlier than expected, so somehow we need to tell the matrix cells to speed up that metronome. Once that's sped up, everything works again. I don't remember where they're coming from, but that projection back to the matrix cells exists. I talked to a bunch of scientists about it in the past, maybe 15 or 20 years ago.

So those connections exist. I don't remember the source layer. I can double check. I think it came out when I was looking at the colostrum stuff. I think it's from L5, which would fit, potentially. In general, conceptually, it's basically the same as scale and rotation. Here, we're scaling time based on an inferred value, and we might have a bias toward expecting it to happen at a certain pace, but we can recognize it at an unusual pace by scaling it.

Basically, the pyramidal cells could detect that mismatch—detecting a pattern on the basal dendrites, which depolarizes the cell and indicates that the next feature should be arriving at this location now. But then, apical dendrites don't get that matrix cell timing input at the right time, either too early or not at the expected time. Jeff, in a comment on the documents, you suggested that potentially they could send a back action potential up the dendrite, apical dendrite, on that expected timing and recognize the difference. There should definitely be some mechanism here to recognize an expected timing difference. There has to be a mechanism. The general idea of matrix cells providing the time signal and adjusting is very likely correct. Somebody has to do it, and the actual mechanisms are not as important at this point in time. Exactly how the neurons detect the difference and transmit that difference down—those are things that maybe no one's even discovered yet. We don't want to spend too much time thinking about it because there may be some unusual property of apical dendrites that no one's ever measured. We know it has to exist, so we can just say, great, it happens.

The conceptual mechanism is pretty simple, but we're not sure how exactly it happens in the anatomy. For the complement, if the sequence is observed slower, we again see the correct changes at locations, but later than we expected them. Then we need to tell the matrix cells to slow down the metronome, and everything fits again.

It's also interesting because—sorry, go ahead, Max.

It's a slight tangent, so you go ahead.

Just a clarifying question: the matrix cells and the metronome are a local clock to this particular behavior, and another behavior column has its own local clock?

No, the way the matrix cells work, first of all, they're not specific to any particular behavior. As Viviane mentioned earlier, they're a global clock and project broadly over multiple regions within a particular modality. So, there would be a set of matrix cells that project broadly over all visual regions, and another set that projects broadly over all somatosensory regions. That makes sense because time is going to be the same—you can't have one part of the object running at one time and another part running at another time; that would be a different behavior. You want this sort of global signal that tells everybody, "Hey, we're speeding up, we're slowing down," for all regions observing the same objects.

In electronics, a global clock is a very difficult thing because there are propagation delays as the clock signal moves across distance. You could take relative measurements, but you couldn't get an absolute time at two different locations and be assured it was the same time. But you have to remember, there are propagation delays in neurons, but it doesn't really matter. The pyramidal cell that shows up in layer three doesn't know about propagation delays. It just says, "I became active, this is what the clock said at this point in time." Maybe the clock started a little earlier; it doesn't make any difference to that cell. The cell just is—this is what it is, and it doesn't care how long it took to get there. This is what it is now.

As long as you're not looking to extract the exact global time, then you're fine. What's important is, in general, the fuzziness of neurons and such—it's not going to be perfect, but it's more about giving a general signal. The delays are there, but no one knows about them, no one's keeping track, and nobody is saying, "This is time zero." It's just that time has advanced approximately half a second, and I'm taking note of it. If it took a millisecond to get here, so what? It doesn't matter.

I was just going to comment on what came up in this conversation, which is that it's interesting how here it's kind of global because, as you said, Jeff, in general, an object follows a single sequence of time in terms of behavior. Otherwise, it becomes some other type of behavior. With rotation and scale, we can have compositional objects where different parts of the child object have different scales and rotations. It makes more sense to have that on an individual column basis. If we had multiple behaviors on an object, those behaviors could have different time scales, and we'd have to shift between them intentionally.

When I first thought of the matrix cells idea, I was thinking of melodies. Within a melody, you speed up or slow down all the notes together. If the drums suddenly started going really fast and everything else slowed down, it's all about relative positions and timing. It has to be a global signal so all the different columns participating know they're all being sped up or slowed down together. I think this hypothesis is almost certainly right to the level you described.

Talking about global signals, we also want to use multiple columns. The last part missing in this picture is voting. Similar to how the morphology model can vote on object ID with other columns, the behavior model can vote with other columns on the behavior ID. I put state in the sequence here, but at first I thought we could also vote on the state of the sequence. Then I thought this information is probably already contained in the matrix cell signal. I'm not sure if that's something we could already get from that timing signal. I don't think so. We should spend time on that later, but I don't think so. Maybe we would want to vote on the state in the sequence as well, or on which point in the sequence we're in.

That's up to the point we've discussed so far: a solution for modeling and recognizing object behaviors in a very flexible way. Does that make sense to everyone so far? Now I'm going to go into the more suggested solution territory.

It's pretty cool to see. It looks like you made a lot of progress.

So, solutions for the remaining issues. I'm going to do a quick revisit of existing theory. As a prerequisite for understanding these solutions, it's useful to revisit how we think the brain models and recognizes compositional objects.

We have one column that recognizes an object. In this example, it has learned the TBP logo mark as features at different locations relative to each other. Now we introduce a second column, the next higher region in the hierarchy.

This region will get the object ID recognized in the lower column as a feature input into layer four. It can also get direct sensory input through the thalamus, which gives it information about sensorimotor movement. It could receive direct sensory input from a larger receptive field than the lower column. Together, it can then learn a model of a compositional object. For instance, a cup. Using just this pink and solid blue line, it could already learn a model of a cup, but it would have to pay close attention to these details. Now, using this connection, it can simply assign the logo to a location on the cup. All the details of how the logo looks are stored in this model down here.

Is that clear? Although, one small technical thing: it's not assigned to a location. The logo is assigned to multiple locations on the cup, which was required to solve a whole bunch of problems. We don't have to go into it, but for posterity, it's not assigned to a particular location. Every location on the cup where the logo exists, in this model, will store a logo. It will keep getting this input and store it there. So it's not just one point for the logo in this model, but multiple points.

Additionally, to storing that the logo exists at these locations on the cup, it will also need to know the orientation of the logo relative to the cup. The proposed mechanism is to take the orientation in which the logo was detected and the orientation of the cup, and calculate the difference. That difference will be the feature orientation input to the higher-level column. This delta orientation is basically this orientation minus that orientation. It's the orientation of the logo relative to the cup. That way, if the cup rotates, it automatically compensates for which orientation we would expect the logo to have, and we can recognize the cup in any orientation and the logo.

We also want to be able to make predictions. If we recognize the cup, we want to make predictions about the logo down here. We can have classical backward projections from layer 6A down, and then all the way back up to layer one. There can be synapses in layer 6A, which can tell the lower-level model: if I'm here in the cup's reference frame, expect to be at this location in the logo's reference frame. It might tell it, "You're probably on the center dot right now," which will then tell the lower level which features to expect. Also, up here, these apical DDRs from pyramidal cells in layer three can get some information about the high-level object being detected.

What would that information on layer one be used for? I'm not clear about that.

So the idea is that this can inform the object ID detected below, since that's what's in layer three. The layer one projection goes beyond a single column and spans many columns. For example, the cup is signaling that multiple columns should be looking at the logo, biasing attention to the logo, but only the columns that are co-aligned. The two columns shown here are co-aligned, so the geo synapses in layer 6A—the purple in layer 6A—indicate that at this point in the cup, you're at this point in the logo. However, at this point in the cup, it's not possible to determine where other columns are in the logo. You can bias other columns to look at the logo, but only in one column can you specify the exact location on the logo.

Is it really informing what the object ID should be, or inhibiting unlikely object IDs, since these are inhibitory neurons? These aren't layer one neurons; this is an axon from the layer 6A cell in the second column. Those are axons from excitatory cells. If they form synapses on the apical dendrites as shown, those would be excitatory synapses.

Those would excite layer three cells, biasing them. There are some cells in layer one that are inhibitory, but we're not discussing those here.

The layer six in region two is not exciting or making the inhibitory neurons in layer one fire. We're not making any claims about inhibitory neurons in layer one; we have no theory about them. The well-known connection is between layer 6A axons and layer three apical dendrites, and those are all excitatory.

We don't have a theory about what the inhibitory neurons are doing up there. Since these locations are unique to their respective objects—this location is unique to the cup, and this location is unique to the logo—if there's a connection between those two, the cup will directly inform the logo model about where it will be. That makes sense.

Any other questions? We're still in review. I'll move on. Lastly, we also want to inform the lower-level column about the orientation it should expect to see the logo. The proposal is to take the orientation stored in the model above—the orientation of the logo relative to the cup—and add it to the detected rotation of the cup, then send it over. This could be done by layer six cortical connections, communicating the expected orientation for the logo by combining the delta orientation stored in the model with the detected orientation below.

That's the proposal. It's more speculative, but it has to be done, so we should feel confident that it's happening. This could be how it works. I think it's a general mechanism; these calculations seem like the best way to do it. The speculation is about which cells are actually doing it and where, specifically the layer six cortical connections. There are many unusual ones down there that seem to fit because they go where we want, but there's not much more evidence. It's a good placeholder; it doesn't impact our theories. It has to be done, so there will be a connection for it. It's a good placeholder.

It almost feels like you could further transform the thalamic input to the previous column, bypassing it rather than sharing it. I'm not sure if that causes issues. We would need to know if there are connections from V2 to first-order thalamic nuclei.

I don't think we should spend more time on it. The function has to occur. We have to do the same for scale. Scale will be relative; it's going to happen, so let's move on.

Now, on the topic of learning associations between objects and behavior: we want to be able to see a behavior and bias which object we detect. One object can have multiple behaviors, and behaviors can be at arbitrary locations, orientations, and scales on an object. Does that sound familiar? Yes, it sounds like learning compositional objects, where one object is made up of multiple child objects, and child objects can be at arbitrary locations, orientations, and scales on the parent object. So why not use the same mechanism to solve this problem?

The proposal is to have a model of the object—such as a stapler—at the higher-level column, and a model of the behavior at the lower-level column. The detected behavior ID essentially becomes a feature on the higher-level object at a location and orientation on that object. Just as we assign the logo to the cup using this feedforward connection, we could assign the hinge behavior to the stapler. To indicate the orientation of the behavior on the object, we do the same thing: calculate the relative orientation of the behavior to the object model using these two projections, then send that up as the feature orientation input.

Does that make sense? The first question that comes to mind is the projection of layer four in region two. Is there a case where you're projecting both a child morphology object and a child behavioral object at the same time? Does that recur? You could, for instance, be on the top of the stapler and on the bottom of the stapler as the projection from the lower-level column, if we have segmented the stapler into top and bottom. That would be the input here for the object. I'm thinking, what if the logo had some behavior, like the logo can rotate a few degrees back and forth or something? I think it works. I'm just thinking it through. So far, when we've thought through this, generalizing makes more sense when the morphology model is the child and the behavior is in the high-level column. But we started with the idea that within a column, you could assign the behavior to a different object.

No, I'm talking about two different columns. Then we came across the idea that, as Viviane just said, there could be a child—these are child objects—because you have multiple behaviors per parent object, you could have child objects. Where I was left struggling, and maybe Viviane will get to it, is that we had two proposals. One is that behaviors could be applied to an object within a particular column or hierarchically as shown here. I'm proposing doing it hierarchically, as behaviors on the object, because, as we also discussed before, the behavior is usually smaller than the object itself or applied to part of the object. So it makes sense that this is on the object. At one point, Viviane, you were saying you love the idea of doing it all in one column, and we're a little hesitant to give up on that. Have you given up on that? Have you said, okay, we don't need to do that? For this issue of assigning behaviors to objects, I think it's best to do it hierarchically because it's exactly the same problem as learning compositional objects. Are you going to abandon the idea that we'd be within a column? I'm okay with that. I'm just trying to understand where you're going. In this proposal, there's no right association within the column. My gut says that's probably the right way to think about it.

We also want to be able to see a static object and infer behavior. For example, we see the stapler standing on the table and infer that it can move up and down. For that, we have the same setup: the stapler model here, the behavior model here. We could simply use the feedback connections. If we have recognized the stapler down here, we can inform the lower-level column to sense the hinge behavior and where to expect to be in that behavior's reference frame. It doesn't tell me where I should be in the sequence of the behavior, but it tells me that this point on the stapler has a potential behavior. Usually, if the object is static, the sequence hasn't started yet. It's more like you can sense this is a potential behavior of the object.

This is probably something that would also happen within a column, right? For a familiar object and familiar behavior. If you go back to the constraints, that's the question I just asked a second ago. I think there are multiple things we could argue about: which things can happen within one column, which things can happen across multiple columns, and which things might be in both. Specifically, biasing the recognition of the behavior based on morphology seems natural to also happen within a column. You could do it within a column, but I didn't see a need for it here because it's already solved with the feedback connections. But what about the instance where the behavior and the state, morphology, are learned within the same column?

Here you're showing it across multiple columns because you're generalizing—this assumes we haven't learned the behavior for the stapler yet. Let me take a stab at that, Neil. It's a bit like compositional objects. In compositional objects, we assume that the two columns—meaning the two hierarchically arranged columns—both necessarily learn both sets of objects. The upper column can learn a mug, and the lower column can learn a mug; the upper column can learn a logo, and the lower column can learn a logo. Therefore, I can observe a logo on the mug, and I can observe a mug in a logo. The same thing would apply here. If we took that idea, these two columns might both learn the morphology of the stapler, and they might both learn the behavior of a stapler. Therefore, we could say, "This is a stapler, which is a hinge." That's a subset of it.

The main argument I forgot about is that if we do it within a column, we can't do relative reference frame transforms there. We can't say this behavior should be in this relative orientation to this object like we can in the compositional setup. It's like doing compositional objects within one column, where the presence of the logo biases the presence of the TVP mug, but we can't specify which locations it should exist at or its orientation. In this hierarchical setup, we can inform all that. I agree with all that, and it makes sense to have it across multiple columns. I'm just saying, have you totally gotten rid of having—because as you just said, Jeff, you'll learn potentially both in both columns, but there's not a connection between the two. Within a column, not within a column. Why wouldn't there be?

We went through the same transition with morphology objects. At first, I resisted the idea of using hierarchy. I said, "Look, columns should be able to do everything on their own." That's one of my things. But then we realized, no, it can't. You have to rely on hierarchy for doing more, for doing composition. It doesn't violate the columnar hypothesis. Every column is learning compositional objects; it just doesn't know what the child objects are. That's the answer to that one. In the same way here, we started off thinking we could learn this all within a single column—they're right there, right on top of one another. Isn't that wonderful? We could apply the behavior learned in this column to a new object in the column. But then we realized the hierarchical solution is more powerful for the reasons Viviane just mentioned. So the question is, why would I introduce it back into a single column? Just like we started trying to do morphology in a single column and abandoned it, we're not trying to do it anymore. I think the same argument applies here. It's a little counterintuitive initially, but it's more impactful.

I think it's worth moving on. The point I'm trying to make isn't particularly important. It's just this question of biasing the recognition of behavior—I think to a certain degree that could still happen within a column, but I don't think it's a big point. Viviane's point is that this solution solves all those problems. You don't need—I'm not saying it's necessary. It might improve the performance system, but then you're going to introduce problems like it doesn't work in this situation or that situation. I had this issue. I think it's a valid point, Neil, to say maybe there's one way to look at this: neurons form synapses with anything that makes Hebbian sense. They're "stupid"—they don't really know what these axons are. So it may turn out that within a column, if these axons are passing other neurons, they'll make connections we don't really need, but they might, and they don't hurt. That may actually happen, but we should follow the logic of how it can be done hierarchically, then come back later and ask if anything was missed or if there's anything we need that wasn't done yet.

I think that's what Viviane recommends. Yeah, that sounds good.

Let me move on to the last points, just for curious viewers who saw this and were confused why that's there too. Which dot? Oh, that one right here—the feedback projection. It's just for completeness because this is still the morphology model. This feedback connection, as we've shown before, also biases locations in the child object's reference frame. Like you touched on earlier, Jeff, this column might also get feedforward input of the object ID of the child object, like the top of the stapler. The same feedback connection that informs the behavior at this location can also inform the child object access at this location.

Can I make an observation? It's really a question. Yes. I was doing this chart of all the possible ways this could work, and one of the things you're suggesting is that there is an asymmetry here in the sense that the feedback connection is only coming from the full morphology model. There's no feedback from the behavioral model. I think that's right, but I don't know if you agree with that. The higher morphology model can predict behaviors and components, but there is no hierarchical behavioral model. Is that correct?

Yes, I think so. Let me come back to that in a bit. I thought about it yesterday and I think I came up with a purpose for a feedback connection from the higher-order behavior model, but that would be like a projection from layer 5B back—like the purple. I think I will show it in a future slide. I couldn't come up with a reason for that. I couldn't come up with a reason why a behavior model had to project backwards. It'll actually come up next, I think. I'll get back to it in a moment.

Also, we can use the same kind of cortical layer 6CC connection to inform the orientation at which the behavior should be observed.

Quick question. Does this solution help us break up the objects? The way I see it, in region one, we've learned the movements or the behavior. I'll also get to that in a moment. All to come.

This was the chapter about learning associations between objects and behavior. The proposed solution is to use hierarchy for that and use the same mechanism we use for composition models. Now, the next topic is this big one that we struggled with in the past weeks: how do we make predictions about an object at an arbitrary point in the behavior sequence, without learning key frames? I'll first look at it for an object and behavior combination that we know. For example, we learned the stapler opening and closing.

Now, here you notice I'm showing the different combination. In the last slide, I showed the morphology model up here and the behavior model down here. Now we're actually recognizing the morphology model of the subcomponent here, the stapler top, and the behavior of the stapler opening over here.

This is just as an FYI: this high-level model would then have the model of the complete stapler, where the top is a feature on the stapler.

Let me think through this again. Did I explain it correctly?

Ah, yes. So what would be a behavior at this higher level? One thing that can happen is we have the orientation of the child object and the orientation of the parent object—the stapler top and the stapler. Normally that's a fixed thing, and that relative orientation becomes the input to layer four minicolumns. But if we have a moving object, that can actually be changing. Suddenly, this relative orientation is changing. Now this will become the input to the behavior model. What becomes the input to the behavior model? Is it the change in pose of the child object relative to the parent? Is it a change, or is it just the current orientation? No, it would be the change, because the behavior model only gets changes as input. It doesn't get input if there's nothing changing. If we just have a stapler sitting there, we calculate the relative orientation of the top to the stapler, and that becomes the input to layer four to the morphology model. We assign the stapler top to the full stapler model here. But if the stapler top now changes its orientation because the stapler is opening, whatever we are calculating here is suddenly changing, and that change in pose of the child object will then be sent to the behavior model.

This pulls in my thinking about it. There's orientation represented by minicolumns. Would these be minicolumns, or would these be features of the behavioral model? I think it's basically the same mechanism as we use at the lower level for the behavior, where we detect changes by temporal offset of on-off cells in the thalamus being activated. Here, we have temporal offset of relative orientations changing, and that encodes an orientation change that then activates minicolumns here.

So, same as here, we would detect a rotating bar or moving bar; here we would detect this relative orientation changing.

I'll have to think about it.

So, for me, it answered a couple of questions. First, what is even the input to behavior at a higher level? Because it only receives changes, not features. The behavior ID is a static feature at this point, so that needs to go into the morphology model. It can't go into the behavior model. This answers that if the pose of the child object is changing, that suddenly becomes an input to the behavior model. But that's only the orientation of the features changing, right? Yes. But other things could happen—it's not just the orientation. It could be appearing or disappearing or just changing in any way. That would be other kinds of changes. For example, I don't think I show it in this graph, but if the object ID changes or the behavior ID changes, that would also become input to the behavior model. If I have a red light that changes to green, or if you have an app on your phone and you press it and the icon changes, then it's a new icon ID that has changed when you pressed it. That change would be an input. Here, the goal of this exercise is to predict what the morphology of the object will be based on its current behavioral state. Is that right? Yes. This is a preamble to that. I don't think we've solved that yet. No, I didn't. This is just defining what is stored in this behavior model. The idea is that the behavior model stores changes of the child object's location and orientation. In this behavior model, at the higher level, we store that the top of the stapler is changing its orientation over time. For one, we can use a back projection, which tells where we expect to be in the child's reference frame—so which features to expect. Then we can have this layer six CC projection, which updates the expected orientation of the child object. Up here, we store the changes in the pose of the child object, which is the behavior model now. We can project these changes over here and, through the sequence of behaviors, update the expected orientation of the stapler top. All the predictions about morphology, about which features to expect, are made down here using this learned model of the stapler top. We don't have to do anything; we just adjust at which orientation we expect it to be, using this feedback projection. That transforms the incoming movement vector and the incoming feature vector and lets us make predictions.

I think that makes a lot of sense. That fits with intuition of how this would work. I hadn't assumed that somehow this would work, but I think you're putting some meat on it. I think it's promising. There's one big issue with it that I haven't mentioned yet.

The main big issue is that this only works for orientation changes of the child object. It doesn't work for location changes. The fundamental issue is that nowhere do we ever communicate the location of the object relative to the sensorimotor or the body. Hang on, what do you mean by location change? Let me think of an example.

It just seems like we can have object behaviors where subcomponents are changing their location and not their orientation.

It's interesting.

Would that be more like a location-by-location basis? Do you have to break it up, and then those become different associations?

It would help if we had a specific example. If you think of a part as a compositional object and the lid goes up and down—it's not a perfect example, because you can take the stapler and, instead of having a hinge, let's say the whole thing at the top just extends up and then extends down. It's still connected somehow. It's like your pot, but instead of hinging, it's moving. It's not clear to me right up front why that's a problem.

The movement or behavior in the high-level column tells us that after this behavioral state, we'll be at a new location. As long as there's a way that location is common in the higher-level column, it's still going to predict the right location in the lower-level model and therefore predict the right morphology. The tricky thing is understanding how behaviors in the higher-level column actually represent changes in location.

We can bias the location in the object's reference frame that we want to expect, but we can't really communicate that the object is in a completely new location relative to the sensorimotor again. It's too much for me to absorb right now, so it's not clear to me that it doesn't work. We can leave it at this and say that maybe we can make it work for location as well. I think it's a nice solution for making it work for orientation changes, and I think it's a nice solution to make predictions about object morphology—basically making all the morphological predictions using this morphology model, and the behavior model just rotates those expectations. I'm not sure why it just has to be rotating. You're saying it's not clear to me that it won't work for locations as well.

Maybe I'm not thinking hard enough about it, but maybe with a brainstorming session we could figure out exactly how it fails. It's not obvious to me. I can't absorb everything right now. What you've presented today is great—it's fleshing out details of one of the ideas we've had and putting a lot of substance on it. That feels great. I love this, but I haven't internalized what the implications are yet. It's hard for me to internalize all that in one 20-minute presentation of this particular idea. I'm not able to say if it works or doesn't work, or if I see the problem or not. I have to spend some time working through this to understand it better. Maybe I'll just wrap up the last slides since we're already a bit over time, and then I can share the presentation with the slides on what I think are open issues. We can talk about those more in the next meeting after it's settled a bit. That sounds good.

Basically, I put a grayish check mark here because I think it solves it, except for location changes, but maybe it can do that too and I'm just not thinking of it. The next problem to solve would be to make predictions for a behavior applied to a known object but a new combination—so we've never seen that behavior on this particular object before.

For instance, we have a book down here and we've never seen the book opening. We are detecting this kind of opening behavior and want to make predictions of how the book will look when it's opening.

We can use the same mechanism of sending down and changing the expected orientation of the book, but we would have to apply this temporary mask that we talked about before. Since we haven't learned a compositional object of the book yet, we haven't recognized that this part of the book can move independently of the other part. We want to make predictions about the book changing orientation, but only for part of the book. Where this attention mask is applied could be informed by which parts are moving. We only want to make predictions about the parts of the book that are moving, and that could be the heuristic for that. I'm not sure how that would work on a neural level or what kind of connection would be needed.

This has to happen for the stapler too, right? Before we have learned the sub-component. The idea is how do we determine that the top of the stapler is a separate object? Exactly. The assumption has to be that the lower column has already learned the complete stapler or the complete book, and now we're just going to mask off part of it while it's moving. Over time, this mask could be used to learn a compositional object—learn that this is a subcomponent on the higher-level book object. It seems like we learn this very quickly, so this isn't a slow learning process. Yes, this could happen in one or two exposures and you understand it.

This would be the proposal for applying behavior to a new object that we haven't segmented yet. I think it's a nice mechanism, but there are still a few things that need to be worked through. It would at least solve the general problem.

It definitely feels like it's along the right track. I think the location representation in the higher column, in the behavioral reference frame, is key because by definition that's changing. If we're going through behavioral space, how do we then maintain an association with the morphological model in the lower reference frame? You mean this location is changing? Moving the sensorimotor, but then wouldn't this one move as well? Yes, exactly.

I think it would. So, regarding the same, we moved to a new location—let's say we saccade—so we expect, like with the open thing, that now it's going to be somewhere else. We haven't made an association. That's why I feel there's almost an association between the behavioral model and the morphological model in the higher column, and then between the two morphological columns, because that's at least fixed. I think that's why you need the morphologies to be similar. You can't really apply, for example, a clam-like morphology and apply the hinge behavior, but you can't apply it to a box unless there's suddenly a seam in it. Maybe I'm missing something you're saying. 

Something to also point out is that here, I haven't drawn the feedback connections in this example. There's no association between this location—no reference frame—to this location in the reference frame, because we can't know about this at this point in this scenario, since we've never seen the book with the hinge behavior. So, we can't really have this kind of location association learned, either between two columns or within the column, because it's a novel combination. I don't know if that—maybe when we learn it with—because, and I guess that's the other thing, it maybe fits better with the anatomy that it would be an association from the morphology model in the higher-level column. When you learn a composition object, like you see a logo for the first time on a new object, you look at it, you attend to it, you can learn that connection immediately. That enables you to generalize to when the model causes the object to deform.

That's at least how I was thinking of it, that it's a learned association, but it would be in the L6 to L6 connection between the two columns.

So you say there would be L6 between the two morphological models. And then, because the behavior is familiar in the higher-level column, you can map from a behavioral point of space to a point of morphology space where it's familiar, and that tells you the corresponding point in morphology space in the lower-level one. Then you're predicting what you'd see after moving in that space. 

What problem would that solve? This is for applying the behavior to a novel object. But why wouldn't this mechanism be sufficient?

I guess it's just in terms of how you actually predict. Maybe you're right—maybe for orientation this is sufficient, but for predicting other kinds of changes that can happen in the lower-level one, other than just rotating, I'm not sure.

If it's a solution to it changing location, then yes, definitely. I'm not sure I understand completely yet. Maybe we can draw it out sometime. I don't think it makes total sense in my head; it's definitely just a rough feeling, but it would be worth working through properly. But I think you're right—maybe with orientation it's sufficient just to have this transformation. That's what I like about this mechanism so much: it's really simple. It just takes the stored orientation change here, sends it over here, and applies it to the orientation of the object's reference frame, and that takes care of all the predictions we want to make about morphology. 

It reminds me of something you talked about at the retreat, which was, what if the behavior is predicting a transformation, the change, then almost subtracting that in the thalamus or something like that. But here it's more column to column.

Let me get to the last—sorry, I'm going to see open issues. All right. Just briefly, if a part of a behavior sequence is occluded, that's no issue because we have learned the behavior model, and that sequence is just getting played through the matrix cells. We're not pausing it as we go through the occluded part. We keep making predictions even when the book is partially occluded, and we are just at the correct point in the sequence once it comes out of occlusion again. We make the correct predictions; basically, that metronome keeps on ticking even if there's a black bar here.

Lastly, compensating for object movement when recognizing morphology is also no issue because, as we talked about at the beginning, it's a sub-issue of the previous problem.

Because this behavior rotates the expected orientation of the sub-object, that automatically takes care of properly transforming the movement vector and incoming features, so we can still recognize it even if it's moving.

So that's that. Chapter four: remaining issues. You all still have a few minutes to go through them? Yes, I do.

This one is the main one that I see, or it's two sub-bullets. Host the child object, location change, communicated to region two. We have a way of communicating that the relative orientation of the child to the parent is changing. We use this kind of relative orientation calculation and communicate that change up here. But what if the relative location of the child to the parent object is changing? I don't have a good answer for this right now. One random idea I had is I could use the layer 6A projection. I still don't understand this problem well enough. Even with the staple top, it doesn't seem—the staple top doesn't, in a compositional idea, it's not at one location there either. The parts of the staple top are moving relative to the other parts. The orientation, remember, we apply this at a point-by-point location. So I guess I'm still—I'm not even—I'm not sure if this is a separate problem or maybe you haven't solved it in the earlier one. I'm confused by this. Basically, I'm thinking of the scenario where we have a child object that moves relative to the parent object.

Have a pen and we press the back of the pen and the tip comes out. That movement of the child object relative to the parent—I don't know where this information would come from. We can detect relative movement, or the staple top is the same thing. The staple top is not just an orientation change; it's the location of parts of the staple top moving. It's not just orientation. If it were literally just orientation, the top would pivot on a point in the middle. I'm not solving the stapler completely either. Maybe just to clarify, so everyone is talking about the same thing, that's just one way of conceptualizing it. I think it's even more complex than that, Neil. It's not that it would rotate in the middle; we only define rotation at a point-by-point location. Remember the logo on the cup that bent—that's how it works.

I don't think this solves it. I don't think it's solved for the stapler top either. I'm just stating the problem: I don't know where changes in location of the child object relative to the parent would be detected and where it would be communicated. It's a good opening. I think there's more to fill in here. Two possible options I considered were layer 6A projection, because there's a reference frame there. It's not ideal because I don't think it projects to the higher level of the thalamic region. Another option could be layer 5A, because that one is already in the global reference frame, relative to the body, so it would be easier to calculate the relative location change. But it's also not ideal because here we want to share the goal state, not the current location of the object. Those are just the two I could think of. I'm not a fan of either one, but it's an open issue. 

Related to that, how can region two communicate the expected child object location back to region one? Isn't that the same problem in some ways? It's basically the same problem, just the backwards connection. If I know the behavior, how do I tell it to expect the child object at this location? It could be a similar way of projecting the change that's stored in the model down here. Then we can communicate the expected location of the child object over here. Again, this is a vague idea and I don't really know. This is where it feels like the L6 normal feedback connection could be natural. If L5 can tell that reference frame—like L5 in column two—if that reference frame can tell the morphological reference frame where it should be, then that could work. The issue with the feedback connection is that it's in the object's reference frame, which is specific to the object. If we've never seen that object-behavior combination, we can't have those connections in the backward projection. Here, I drew this in black because I meant that this is relative to the body, so in a common reference frame, and then we can transform it into the object's reference frame. That way, we could tell the location of an arbitrary child object relative to the body.

We're assuming that we know the morphology and the behavior; we just haven't seen them together. I think it could be done with quick binding of those things, but that's just a thought. I want to think more about this and discuss it further. 

Another issue is how the attention mask would be applied to region one before the object is segmented. The idea of the attention mask is nice, but I'm not sure what neural mechanism could be used for that.

I can use the heuristic of applying it to any part of the object that is moving, but I have to think through if that actually works for any example we can think of.

It seems there's a general problem: what if I have an object that's moving relative to me and I can still recognize the object? For example, the stapler as the car is moving, or the stapler sliding across my desk.

If you understand how we still infer the morphology of the stapler even as it changes location, then I think you'll solve the problem of how to break out the moving top. It seems to be the same problem. I'm just thinking out loud about how you could solve that.

I feel like the first one is more an instance of the last problem I stated—still recognizing object morphology even though it's moving. There wouldn't need to be a mask applied to it necessarily. It's not a mask; it's more like an object is moving, and I have to make sure my reference frame and everything is anchored to that moving object. Its position and orientation relative to my body are changing, but I want to maintain the same model. In this case, the model only applies to part of the object, because only part of the object is moving relative to my body.

Maybe one relevant thing—this was a comment from you at the retreat, Jeff—is that there's almost a question of how much a mask there really is, because you still see it as the object. If the top of the stapler is moving, you can talk about the top of the stapler, but in some sense, you just say the stapler is moving. It feels like the opposite: once the top of the stapler starts moving, that's the only thing I'm focused on. I could come back and say, "Is the rest of the stapler there?" Yes, there it is. But the point is, it's still a stapler, so maybe we don't even need to mask that. The masking is useful conceptually, but it doesn't actually need to happen for the system to work well. At this point, we're saying it's a separate part of the stapler.

I want to make sure we get through all of your points. Those are the big remaining issues I had that need more thinking and could break some of these ideas. I have a couple of remaining uncertainties that are more about going back into the literature to read and point out what I'm not so sure about. One is this kind of child object orientation change and whether it can actually be communicated by these layer six cortical connections, similar to how we propose it for other compositional objects. In both instances, this is the least certain part, even in the compositional object idea. I'm not a hundred percent sure if this is the best way to communicate this in the layers.

Whether changes are actually represented in L3—at least from a lot of papers, it seems like direction-sensitive cells in V1 are usually found in layer 4C alpha, which is the main target of the magnocellular pathway.

Maybe a brief comment on that: maybe a simple solution is that that's the direct input in the column, but then it's processed into L3, and that's why you see movement-sensitive cells with narrow receptor fields in L3, even though the magnocellular input is in, say, L4C alpha. Just to bear in mind, there are people who have argued that the labeling of these extra layers is incorrect, that the different layer fours are actually different layer threes. It's an arbitrary decision to call them layer four. I'm just finding out that this exact layer assignment, at least, I'm not a hundred percent sure on, and whether layer three is the best one. I think whether there are direction-sensitive cells earlier in LGN or even retinal ganglion cells is dependent on species as well. There are direction-sensitive cells in the retina, but they're not ganglion cells; they're internal cells, and it's believed they do other functions. We assume that information is not transmitted.

On a general note, I think the mechanisms I presented here are pretty nice and general, especially for the behavior model, but the exact assignment to layers—at least I'm not so sure about yet, with the behavior model part.

Another one I didn't talk about is how we recognize change if the sensorimotor system is following the object—so, smooth pursuit, where we have movement input but no feature change. That's something you also brought up before, Jeff. How does this change information get into layer three or get compensated in the thalamus, and how would that mechanism work?

This question we touched on earlier: how can columns share knowledge so they don't all have to observe every change in the behavior to learn it? I don't have a great answer yet, besides that you do have to observe every change and repeat it many times.

Another one is whether, for inference of behaviors, voting on behavior ID is sufficient for past behavior inference, or if we need a stronger mechanism—actually communicating movements between neighboring columns that are being detected. Behaviors can happen quite quickly, and if you're just voting on their ID, you're not sharing a lot of information between columns to detect the behavior.

But maybe it is sufficient.

Do we vote on the point that we are in the sequence—where in the sequence we are—or can this be communicated by resetting matrix cells and telling them we're at the start of the sequence now, and from there the clock starts?

Generally, how is timing implemented by matrix cells, and how are they modulated for speed adjustments? How do we indicate the start of a sequence or recognize it in the middle?

Those are all things to ponder. Yesterday I spent all day putting all the connections into one diagram so I could build the slides and just remove the connections that were not relevant for the point I'm making. This is how it looked. I guess the solution is not as simple as it might seem. That's pretty funny.

That was a great presentation. Can I try to make the crudest summary I can? I loved it, especially the diagrams—they were very helpful. The first thing is, there are really just a couple of things you're proposing. One is that we should focus on behaviors in a hierarchy and not worry about what's happening in a particular column between the behavior models and the morphology model. Just focus on the idea that there are parent-child relationships. That's a great clarification. Thank you for doing that.

The second point, which was surprising to me, is that the behavioral model ID has become a feature of the morphology model that is projected to layer four. That was a big insight. I don't think we discussed that before. It may be obvious in hindsight, but it was really cool. The third thing is, given those two assumptions, if you assume the same mechanisms are in place for both the morphology model and the behavioral model, and then walk through all the issues, most of your presentation was just that—walking through the issues that arise from those assumptions. I got lost towards the end and couldn't follow everything, but eliminating the need to think about what's within a column helps me focus on hierarchy. Assuming the same mechanisms everywhere, and explicitly stating that, ties together the two types of models. Then, tying them together—like the morphology model projects to layer four, and the behavioral model projects to layer four of the morphology model—shows how you can connect them. Given all that, this is a great start with a lot of valuable ideas. Now it's just a matter of working out the details, which can't be done in one session, at least not for me. But it's a huge clarification, simplifying the problem and eliminating unnecessary complexity.

I liked how you added the two preliminary sections about the existing theory. For the first set of problems, we could use the existing theory about object models, and for the second set—learning associations and making predictions about morphology—we could use the theory about compositional objects. My favorite part of this proposed solution is how well it fits with what we already have. If the past is a prediction of the future, it will take several rounds of discussion to work out all the details, but I think it's the right path. The details are just a matter of working through them, thinking about the problems more concretely, like the issue of moving the staple top versus changing the orientation, which confused me.

I bet we'll be able to solve those issues with this clarification. Those were my observations. I don't want to shut down the conversation. If you're on board with the general idea of using hierarchy for some of these issues, then our remaining problems become much smaller and more concrete. I'm not trying to take anything away from it; we just hadn't fully embraced the idea yet. It's going to be hierarchy like this. We already knew that you needed multiple behavior models per morphology model, so it has to be hierarchical in that regard. Stating it and putting it in a picture makes it clear—of course, it has to be that way.

Why was I struggling with this? For me, another interesting takeaway was the idea of relative orientation and what we can do with that. It was mentioned briefly, but as you said, Jeff, putting it in a figure and thinking it through is helpful. Sometimes, just stating the possibilities and walking through the consequences is half the solution. Just focus on this one thing. We can keep going back to our existing solutions for hierarchy and see how they work here too. I'm still confused, but in general, it's satisfying that what once felt insurmountable can be chipped away at through this process. If you think about where we were six months ago, it's definitely huge progress. That's why I commented earlier about reviewing the morphology model—it was ten years of my life, and it was so hard to figure out. Now, this seems easier, like we're just doing variations on a theme. It's still hard, but it feels easier in some ways. I'm going to think about this more. If anyone else wants to talk about it, thanks, Viviane. Sorry for going on so long. The diagrams are great, and all the animations are really cool. It took me an hour to write the notes, but two days to put them into slides because it's so hard to communicate these ideas. It's all in my head, but putting it in a way that's clear to others is challenging.

Those are nice diagrams—a nice new format.

It worked for me. I'm going to go back to the hierarchy paper.