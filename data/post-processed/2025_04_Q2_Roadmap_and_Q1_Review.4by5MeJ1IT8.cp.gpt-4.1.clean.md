It's the beginning of Q2, so the next three months. I asked Will, Tristan, and Niels for input on our priorities for this period, and here's what we came up with. First, Tristan made a nice analogy of how these align with our mission, so I'll start with that. Our mission is to create core sensorimotor algorithms guided by neuroscience, create an open-source platform for sensorimotor products, and increase awareness of sensorimotor technologies for AI and robotics. Our Q2 priorities align well with these.

The first priority is creating the algorithms. For the next three months, our focus is to pave the way for Monty to model compositional objects. I'll talk more concretely about what we want to do for that in a moment. Second, we want to make Monty easy to use in a variety of applications, currently focusing on the TBP team since we are the biggest user of Monty at the moment and don't have a platform yet. We are the best judges right now of how easy it is to use, but this focus is just for the next three months. For increasing awareness, we want to increase engagement, particularly academic engagement with the project. We also have a fourth priority that doesn't map directly onto the others but is still very important: figuring out how to model object behaviors both in the brain and in Monty. This is core research, understanding the theory behind it.

Does that make sense for everyone as priorities? The idea is to have these concise priorities, and whatever we work on in the next three months should contribute to one of these four things. Concretely, these are the results we want to see at the end of the three months. For number one, paving the way to model compositional objects, we want the accuracy on the unsupervised inference benchmark that Ramy set up to dramatically improve, probably using the hypothesis resampling that Ramy is planning. We also want the speed of inference for the distant agent to dramatically improve via better policies, even in the absence of the hypothesis testing policy. That's a Cade policy we've been discussing. We want a test bed set up for compositional objects—logos on cups is what we're currently thinking—and have that test bed available, with the performance of baseline Monty with two hierarchically stacked learning modules quantified there. This is just the test bed, so we're not expecting it to solve it perfectly yet.

Lastly, the MORTAR system should support swapping out policies, which will be important for the policy work. For making Monty easy to use in a variety of applications, we want the motor system to be disentangled from other classes, which we discussed in previous coding sessions. Monty should not be constrained by Python 3.8. We also want to publish tutorials on Monty and novel applications in robotics, plus several example projects as outcomes of the hackathon.

For increasing academic engagement, the plan is to contact at least 20 universities and create five pieces of content targeting university PhD and master's students. All of us have booked to attend a conference in 2025 for face-to-face engagement. We aim to add 500 academic contacts on Twitter and Blue Sky, release 36 hours of video on YouTube, and publish two articles in peer-reviewed journals—the DMC paper and the hierarchy paper.

On the object behaviors theory side, we want a concrete proposal for generalizing the stapler behavior to another object, applying a behavior from one morphology to another, and predicting those features. We also want a concrete proposal for how object behavior models could be implemented in Monty, and to file a provisional patent application or, alternatively, a public disclosure writeup.

All of these are documented, and I can share the link with you in Slack later. They're in our shared Google Drive. There are also subteam objectives in here that I won't go through in detail now. The subteams—research, engineering, and communications—set three priorities for the next month that each align with these companywide priorities, with measurable outcomes. If you're wondering what your subteam will focus on, this is a good place to look. Generally, this is meant to be a north star for your daily tasks, to make sure that whatever you're working on contributes to our main goals for the next three months and aligns with these priorities. Does that make sense?

Any questions?

Cool. I made this first half of 2025 timeline in February, talked about it during the retreat, and this is how it looked back then. I just made a couple of adjustments to it, so now we are here. We extended the work on the two papers a little longer, hoping to finalize that by the end of April. I made a few minor adjustments, like extending the test bed work a little longer and starting the learning module work a little earlier. There's been some overlap between the two, and policy work is starting a bit later. We split up the benchmark automation into part one and part two; part two is currently not scheduled here. We also have a new task that wasn't on there before: making the DMC papers' experiments reproducible. This involves turning Monty into a package, putting it out on Conda, making template repositories, and moving code over. This is still ongoing. We also have the task of making policy swappable. Previously, we had this one first and then the second, but now we've switched the order. We have a first trial of doing a Python and Habitat update, trying to get it to work with Monty and have Habitat work with a newer Python version. It's unsure if that will be possible in that time-boxed week, and that will determine the next steps—whether we decouple more from Habitat. Those boxes are dotted because it depends on how difficult that will be.

As for how these map onto the priorities: we have the "pick the composition objects" priority, which includes the green milestones—the unsupervised learning prerequisites part, hopefully completed by then, and also the test bed part of composition objects and making the policy swappable on the engineering side. We have making Monty easy to use in a variety of applications, which includes the hackathon work, the refactor, the data loader, and the Habitat update. Increasing academic engagement includes both a lot of the outreach work Willis is doing and the two papers we're working on. Lastly, figuring out object behaviors includes the brainstorming work down here.

Does that make sense?

I think the one thing I missed was making policy swappable earlier now, rather than after the refactor. I think that makes sense in terms of priorities, but maybe it's worth discussing whether it might be hard to do that without the refactor. It might actually be easier; all the complexity will still be there, but you'll just be able to swap the policies out while everything is still connected. After planning and looking at it for a while, we might actually be able to just do this.