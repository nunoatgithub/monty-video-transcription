Thanks, Ko. I'll be talking about action policies and what we're currently doing with them, as well as what we hope they can help with, particularly focusing on multiple objects at a time. One of the goals for improving our action policies is to make recognition more efficient. If you have these objects in your memory and you're experiencing a new object, ideally we do something better than just a random walk over the surface to identify it. For example, making a quick movement to a specific point, which might be either a bottom-up or a more top-down approach. In my mind, bottom-up is anything the sensory module and motor system have access to, while top-down relies on a learning module—an explicit model of objects based on previous learning.

Philip had already implemented a kind of momentum in the touch policy, which can circumnavigate an object by moving along its surface. There was a momentum parameter, but I don't think there was enough time to really explore its potential to make recognition more efficient. The default value was 0.5, which gives a rambling, slow, slightly directional random walk. I was interested in whether we could improve on that with different momentum values for more efficient recognition.

I also hoped to improve further with something more intelligent—still bottom-up, but driven by the features the sensory module and motor system are experiencing. This policy has the agent follow the principal curvatures it encounters, to more quickly get an overview of the object's structure. Principal curvatures are the minimal and maximal curvature; for example, on the side of a cylinder, the maximal is the curved direction and the minimal is the flat one. The policy does both, and that's what the white and black indicate here. When it's blue, it's just following momentum and rambling along. When it's white, it's following the minimal curvature. When it's black, it's following the maximal curvature.

It alternates between those. In addition, it tries to avoid revisiting previous locations. The green shows the policy starting to move in one direction, then realizing that continuing would just yield the same information, so it tries a different direction. All of this doesn't assume any knowledge about the actual object—it's just based on information from the sensory module and some knowledge of where you've been, in an egocentric or environmental reference frame.

Maybe in another presentation I can go into more technical detail, but briefly, here are some results. These are from the first 50 YCB objects in two very different rotations, looking at performance up to a maximum of 30 steps. The idea is that this is a bottom-up policy, ideally just a first step. You might do something like this, and by this point you'd expect to have a reasonably good hypothesis about the object. There's a good chance you won't have recognized it because many objects are similar, but if you have a good hypothesis at this stage, that will feed into the next work, which is about top-down policy. Then you can test specific hypotheses and so forth. For this analysis, I'm allowing 30 steps and looking at performance at that point. What is performance here? There are two outcomes shown, both of which are good. Blue means it's correct—in those 30 steps, it managed to find the solution and identify the object. Red means it's a correct hypothesis—the hypothesis with the most evidence is the correct object, but there are other objects with sufficiently high evidence, so the evidence hasn't reached the threshold for a decision. Once an object has taken a few steps and has a maximal hypothesis about a particular object, it could do a top-down policy-executed action to test that hypothesis.

I'm still confused. Let's walk through one of these examples. You show these bars in blue and red. I'm trying to understand the difference between blue and red. The red means it hasn't converged, but the one it thinks is most likely—if you forced it to decide—would be the correct one.

A correct hypothesis means it has converged and is confident it's the subject. There is a threshold for actually identifying an object: it needs to be confident above a certain level, and the evidence must be higher than all other objects by a certain threshold. If you look at how classification is normally done, you just pick the top hypothesis as your classification—that would be the red one. Blue means, in addition, it's actually very confident that it's the right one. Is it confident because it eliminated all possibilities, or is it just above some arbitrary threshold?

It's a relatively arbitrary threshold, as it's user-set.

It depends on where you set your threshold. I think it's still useful to consider that in some cases you want to be more confident. You have to have a threshold at some point if you want the system to output one label, but the labels are continuously being updated. It's okay to say the system is always guessing; it's never a hundred percent certain about anything. Just like in science, your hypotheses are always hypotheses, but if this is feeding into something else, you have to decide how much confidence is needed before acting on a hypothesis. We want to first find out what it is and then try to pick it up or something similar. We need some sense of confidence about the object before executing an action. Jeff said you need a threshold, but that threshold is just a parameter. It's more of a functional thing because ideally, we would have continuous evidence that updates every step, but we have to measure it at some point. We have to decide if it's correct or not, and for that, we need a threshold. We also need to end the episode at some point, so there's nothing magical about a particular number.

What does the outcome proportion number represent? It's a percentage of the total number of objects experienced. For example, on about 12 objects, it got the correct solution in 30 steps. So it's just classification—higher is better. Twelve of the objects were correctly recognized.

That means even with all the different tests here, there are a lot of objects not correctly recognized. Is that bothersome? The steps are very small, so 30 steps is not a huge distance.

It would definitely be better if you did it for longer, but this is why I think ultimately we don't want to be just at inference time dragging our fingers slowly across an object. You could do it much faster. If you go on the surface, the touch policy is constrained to follow the surface, at least at the start, while it doesn't have a hypothesis about the object. I think that's a good thing because it relates to what would happen if we eventually have a robotic digit. It would have an accurate depth reading but would have to follow the surface and not knock the object over, which is very different from using your eyes.

When I think about how I touch something, it's pretty obvious how I do it, and it's not quite like this. If I touch an object, either I'm learning it or trying to do inference. I typically find an edge—like this cup in my hand, I find a sharp edge and follow it around. That's what I was trying to capture with this principle curvature idea, laying it onto minimal and maximal curvature. For example, when I'm on the lip of this cup, to go to the maximum curvature, I go down the side, but I don't really want to do that. I want to follow the edge, which is in some sense the least curvature, but it's still following the edge. It's not clear; you don't really want to go on the flat surface very much. You quickly try to find an edge and then follow it around, which is hard to map exactly onto this approach.

You could set the number of steps it's going to follow a particular curvature before switching to another, to avoid spending ages just going around the rim. In some sense, you go around the rim until you get back to where you started, then go someplace else. You get to a point where there's a bifurcation on the edge, and then you follow that one. It's very related to what you did here, but not quite the same.

With touch, you do it continuously. You don't stop and go "sense, move, sense, move." It's odd because that's not how we've talked about it in the past. You continually follow the edge; it's not a saltatory experience, it's a continuous one. There's a distinction there. I've talked with Viviane about this—it is possible that neural systems can learn continuously like this. They don't have to be done in discrete steps. You don't always have to do "move, sense." 

If I want to move further, I don't want to just move my finger in a small area. If I have to jump, then I'm losing information. But if I can just follow quickly all the way around, it's not like I'm storing a thousand points. I think I can go through the basis of how this would work. You don't have to form new synapses all the time as you move around; it just adds new ones if needed. That's not how we have it structured right now—we have it as point and movement, point and movement. I see the problem there: if you just jump further, you're losing the continuity between points and don't know what happened in between.

In general, something useful would be using movements themselves to estimate principal curvatures rather than having point estimates. How a finger moves along the surface, when constrained to follow it, tells us information about the principal curvature of an object. That could be less noisy than a depth camera reading. 

A fundamental issue here is the number of points that have to be sampled. It's easier to sample close points, but then you have accuracy issues because you've only sampled a small part of the object. If you want to sample the whole object, you either have too many points or you might miss key features.

If I just touch four sides of this circle, I can't tell if it's a circle or a square. For example, four certain edges at 90-degree angles to one another. I'm just pointing out a problem here: the sample density is an issue. That's why you're only getting 30. I'm not too concerned because this is just a starting point. At this stage, you would start testing likely hypotheses, and then you can make larger jumps. But it's hard to do that if you're using one finger. You touch one point on the object, and now you want to touch somewhere else, but you have no idea what the structure is, other than there's a surface in a particular direction relative to you. You want to follow the surface, even if you're just learning it, but you don't want to store it in points. The continuous versus discrete distinction is a more fundamental issue. You can view this as 30 steps, or as viewing one or two centimeters of the surface. That's the continuous surface, but 30 centimeters is not what we do with our body, and it has restrictions. I think what you're saying is, "I can sample a small part of this object and get enough of a hypothesis of what the object is. Now I can use a top-down action policy." That may be true, although it's not going to work extremely well because only 30 of the object is being recognized, so that's insufficient. It's also not sufficient if we're going to have an action policy for learning the object, in which case I have to sample much more of the object.

I still think these results are promising because 30 actually had the most likely hypothesis. Saying it's the most likely hypothesis means there were several other possible ones, maybe objects with similar morphology or symmetric versions of that object—different poses that are symmetry equivalent. Even if you use the most likely hypothesis and it's the wrong object, it will most likely still have a similar morphology. Using that with a top-down policy should, in most cases, work. Don't take my comments incorrectly; I didn't mean to be critical. I think these are good results. I agree with that. I'm just anticipating problems and trying to understand how this is different from what the brain does. That may or may not be important, but that's what naturally comes to mind. I think this is close to what the brain does, but I'm just bringing that up. I think it's still very well resolved. So, it would be simple to change this, for example, so that you follow most of this, and rather than having the action policy follow the whole thing, you only feed five or ten observations from across the whole rim to the learning module. There are simple approaches to make it more like what you're describing. The distinction between discrete and continuous is a much more fundamental change that would need to be implemented at some point. That gets to how the learning module represents things, and that would be a fun side discussion to have—the discrete versus continuous issue. I don't understand it completely, but I have ideas about it, and it would be an interesting challenge. I wonder if we could add continuous learning to the models you're already building, which are fundamentally not continuous. That would be an interesting mental exercise to go through someday. Yes, I agree, that would be really good. You guys set up proposals for different directions to go next.

Maybe just very quickly, in case it wasn't clear, this plot: on the X axis we have random walk, and the momentum at 0.5, which was the default value used before. This was exploring a smaller momentum value, so this is just carrying forward more. Then this is the principle curvature-guided policy. Given the number of samples I have, it's not clear if there's a huge difference between these two, but it seems there's a trend towards improvement over random walk, and potentially over previous might walk. Could you remind me how to interpret the momentum value? I thought a larger number means you'd go further, but here it looks like it's the opposite. So, a momentum of one—maybe momentum here is one over, which is a poor choice of words. It's a parameter called alpha, so maybe momentum is the inverse. Yes, it's one over momentum. That's a good point. So, one is a random walk. Got it. I'll change that for next time. If I present the opposite of the word momentum, you might say something like—

Dixie, just take one minus. I know, but the word momentum sticks in your head. The bigger, the further I go before I turn, it's the opposite of this. This number represents how crazy you are. I have a question. Is the word you're looking for "mean free path," like when you're talking about atoms bumping into each other? You're technically right, Eric, but that doesn't speak to the mass's notion. If I asked my wife what's the opposite of momentum, she wouldn't say mean free path.

I have a question, Niels. You're making a jump from these principal curvatures to object identification. If I'm thinking about the criticism that you have to do all these micro steps and maybe store them, is it that they're contributing to a hypothesis of some kind of contour? If you have something relatively smooth and you think there's a continuous curvature, so there's a contour, and you keep moving and it's being satisfied, you only need to store that kind of rough curvature, if you wish, rather than all the intermediate steps. As an intermediate representation before you leap to understanding that the set of contours represents a particular object.

No, I think that's a great point. Exactly, I think something like that will be important. When there is something like that, the advantage of the PC-guided approach over just the standard momentum will be even more stark. At the moment, this policy means we sample this principal curvature going in this direction more often, but it's not in itself really giving us information—the fact that we're following an edge about the principal curvature. But as you say, that is added information, which could itself be an intermediate representation.

I have a bias against trying to parametrize the objects. We've been through this before, where we say cups are made of certain curvatures plus edges and so on. If you're going to do that, you have to do it with no real assumptions about what the structures are. You don't want to get down to a model of a cup as some predetermined structures combined together, like blue arms or something like that.

I'd prefer to keep going in the direction you're going and solve the problems as they arise. I don't want to overstate. There are a variety of ways, as Kevin described and as I mentioned earlier, about capturing what movement can represent. I agree, we don't want it to just be a list of three variables that parametrize cones or whatever. As long as we agree on this way of thinking about it, then I'm okay with that.

Just quickly, the other result was that if you go back to these red ones where you have the correct object as your most likely hypothesis, but you have a bunch of other objects you haven't eliminated yet, they're cluttering your comparison. This is what we're looking at here: the number of potential hypotheses against different conditions. It seems these policies do a better job of ensuring we have a more constrained list, not something with 50 or 20 objects, particularly with the PC-guided approach.

When the number of possible hypotheses is down to these low numbers, are you checking that the correct object is actually one of these possible hypotheses? Yes, it's not only one of them, it's the leading one. The leading one would give you that 30% accuracy, but it could be the second leading one and still be a decent hypothesis. That's not what I'm looking at here. This is for the ones that were correctly identified. Exactly.

As well as what's already been discussed, one thing I want to explore is that this policy is interesting, but it's biased to sample high principal curvature areas. I think there's a risk that some of these areas are, by their nature, more poorly sampled or more liable to noise in the model. In particular, this is all using a model learned from vision, where it did a scanning policy, and now we're evaluating it with touch. A natural approach would be to augment the vision model, go back in, and also learn with this kind of policy, visiting a lot of these interesting parts of the object, which I think would help boost performance.

I didn't understand that. If everyone else understood, you can go on, but I was confused. You're talking about using two different modalities. If you think about the points on this graph that are represented in memory, there may be areas where there aren't many. It is a lot of points, I agree, but regardless of whether we're representing with all these points or not, we want a good representation of interesting points. The analogy I'm trying to get to is that the principal curvature areas on these objects, when you're feeling them with your finger, are like when you scan with your eye over someone's face and look at high-contrast areas like eyes and a mouth. You're more likely to visit those points both at learning and inference, which will speed up recognition, but you also want to learn the whole object, not just those things.

The vision policy is really good at getting a broad overview of everything because it's systematic, unlike the finger policy, since it does a spiral scan. However, it may not be great at making sure we've visited and paid attention to these interesting areas, like the edges of a cup. What I'm suggesting is you can do both: first learn with vision, then go in and add points, learning through this kind of policy. Is there really something fundamentally different between vision and touch? It seems not. You could implement the spiral policy with the finger, for example. There's no reason you can't do that, but you're right, because you're basically doing the same thing in vision and touch—sampling one point at a time. With vision, you're doing a more comprehensive search, and with touch, you're matching and following a trajectory. In theory, there's really no difference between them; they're both the same.

Maybe this is related to what Jeff was envisioning: in vision, we have pre-cortical and subcortical areas that help determine where you might want to go next. You see a lot of the areas of the object, but only a few specific points are actually sent for recognition. I'm wondering if there's something analogous we could do in the touch realm, where you're moving smoothly and fairly fast, but only every once in a while you actually send the data for recognition. You could use some very low-level technique to decide when to send a data point up. I'm redefining the steps in some sense. It's a bit like what Kevin suggested earlier: you might say, "I'm following this curve, it's pretty boring, I know what curves are like, and now I get something interesting—now pay attention to that one." This is something we've discussed, and I think you could definitely do that.

We actually already implemented this technique—not with the touch sensor, but it could easily be applied to the touch sensor, where we just send an observation to a learning module if the features have changed significantly. Whatever that strategy is—feature significance or some other approach—there's some low-level strategy that decides when something should go through the whole recognition process. I think that's still different from what Kevin was describing, or at least how I interpreted it, which was more about using the actual movement to represent a curvature. Still, we could definitely do that, especially by redefining steps. You could cut it down, and in 30 steps, you would have seen a huge chunk of the object.

As you're moving over the object, the number of bits of useful information changes. If I'm on a flat surface, there's very little new information coming in. If it's a curved surface, maybe a little more, but not much. When I get to an unrelated surface, there's more; if I get to a sharp corner where the handle comes out of the cup, that's much more. You want the information stored to vary based on how much is actually useful. Saving a whole bunch of points on a flat surface is a waste. I was thinking the same thing, and I can imagine how this might happen in neurons. I have to spend some time thinking about it.

I don't think it's optimal, though, in the sense that if you compare it to what our brain is doing, it's not like we're completely throwing out the information of what's happening in between, which is what you would be doing if you implemented this. The best solution is probably something in between. I didn't say throw it away; I said store less when you can, but at the moment, storing less would mean a more discrete sampling of the space or a subsample, which is not what's happening here.It's like saying if I'm moving my finger on the lip of the cup, I don't need a lot of information because it's a nice continuous circle. But at one point, something unusual happens, and at that point, I have to store much more information. It's just an efficiency of the coding scheme. Anyway, I don't think we need to worry about it now, but I do think there's potential biological evidence suggesting how it works.

There is something in information theory about the value of taking additional measurements when you already know what the result will be. You don't need to take temperature measurements every microsecond because the results won't change. Having a lot of information densely packed in one region leads to redundant information, which is a low entropy environment with little variation. The extra bits used to store that information are not useful. Shannon's information theory says certain bits are more valuable because they provide novel information. Temporal memory is very good for this because anomaly detection allows you to scan along a surface quickly as long as the curvature is constant. The moment the curvature changes, the anomaly score increases, signaling the need to slow down and take more measurements. The question is how to store this information in reference frames in a way that allows variable-length encoding—storing more bits when needed and fewer when not.

There are different things going on. One is deciding what your action is—how fast to move and in what direction. The other is that when you reach an interesting place, you need to collect more information, which goes into a hierarchy of features detected at that location. I implemented some things so you can use how much the principal curvature or point normals are changing to determine step size. Unfortunately, we still have to take fairly small steps along the surface because of how this works and the limited concept of surface, and we don't want to fall off the object. In general, I found it isn't improving performance in terms of step size and convergence, likely because the principal curvature and point normal are still noisy. The core implementation is there, so at some point, we could return to it and take quicker steps across a flat surface and slow down in more complex areas. If taking small steps with the touch policy is a problem, we can combine it with the feature change policy. The motor policy only sends sensory information to a learning module every few steps when features actually change. You still take small steps, but only send potentially informative data to the cortical system.

A while back, Viv and I discussed the big difference between representing things with SDRs versus numerical values. There's always been a question in my mind: can you build systems like this without relying on SDRs, just using numbers? That's still unknown. I think the brain's solution relies on not sending an SDR for every movement through space; the SDR changes slowly as you move, with bits changing gradually if the movement is smooth, or more rapidly if it's not. The brain likely relies on this principle, which doesn't directly apply to the numerical way we're doing it now, but it would be interesting to try to make it work that way. Right now, we decide whether to save a number, but I don't think neurons do that. Neurons don't save a new set of synapses for every point; they just add more when necessary. There may be a fundamental dichotomy between how the brain handles information and how we're doing it here. I'm not saying we have to abandon our approach, but we should look at the brain's method to see if we can solve this.

So that's all of the bottom-up material. The next plan is to look at top-down driven actions to speed this up, especially for the vision sensor. What about using multiple columns to help drive what's next? In some sense, having multiple columns is like having multiple sensory inputs over time, just in parallel, which can help drive the process. One limitation is that multiple columns are currently a bit slow. I'm not experimenting with that at the moment, but I think it would be interesting and important. Is that slowness due to our implementation or something fundamental? It's a mixture—finite resources have gone into the implementation so far, and there are also issues with the Lambda node that people are trying to figure out. The top-down approach could work really well. I was thinking about the picture you started with—the set of dining utensils. They all have the same handle, and now you say, "Okay, I imagine there are a lot of other objects, not just these three." So you say, "Oh, my utensils—what are you going to do?" You know right away that you have to go down the end and can predict either the fork tines, the spoon bowl, or the edge of the knife. That's a great illustration: you don't have to spend more time moving your finger along the handle. You can immediately jump forward and figure out which one it is through a top-down action policy. It's a good illustration.

I promise this isn't my room. Did you get a picture of Sophia's bedroom? Is that what you used?

Some of this is odd—there's a lot of old stuff here, like old photographs, old projectors, old boxes, and some old cord phone. All that with policy was primarily focused on efficiency, but now I'd like to talk about more speculative ideas, particularly how action policies can be leveraged for dealing with multiple objects in cluttered scenes.

When dealing with multiple objects, you need a visual understanding of the scene. In computer vision, this often takes the form of segmentation, but we want something richer. There are various cues you could use—like depth, lighting, and texture—to segment the image, and deep learning methods can help with this. But ideally, we want to avoid that approach and instead leverage Monty's knowledge of objects in a principled way.

Before talking about policy, it's worth discussing how we want to represent multiple objects, because I don't think this has been discussed much. Imagine you're sensing a knife and have high evidence for it, then you leave the knife and feel a coffee mug, becoming confident there's a mug. None of the evidence in that area is evidence against the knife. If all of this is in an allocentric or egocentric reference frame, then both objects exist with some relative arrangement. As currently implemented, it's not clear what a single cortical column would do to represent all this. It makes sense that it would represent one thing, but the natural question is whether there might be a level in the hierarchy representing the actual objects and their relations.

This is something that will be implemented, and it ties into questions of hierarchy. It's going to require a hierarchy: one region of cortex has a reference frame larger in space than the one below it. Whether these are objects in a room or features of a larger object, that's likely the solution. You recognize different components, and they get located as features in the larger object, which is a higher-level reference frame. So you'll need two reference frames of different scale for certain, and that seems to be the answer. Often, we might not need to instantiate new graphs for new scenes, but some scenes have underlying structures that repeat, like dinnerware on a table or messy rooms. Even if none of the features are identical, we recognize it as a messy room. This relates to the issue of classifications and similarity between objects made of different components.

That's a bit of a mystery still. I think we could start by saying, when we're dealing with an issue like that, we establish a reference frame, even if it's temporary, where we can place objects in relative positions to each other. If we do this every time, we create a new scene and try to recognize components and place them in that scene. The next time we come back, we start again. That wouldn't be a bad place to start.

We all recognize a cluttered room immediately, even if it's full of old objects. Another element is that when returning to a location in space, it would be useful if the scene-level representation helps us quickly recognize objects we saw there previously—essentially a form of object permanence. If you return somewhere, you don't have to study the teddy bear again to recognize it; you expect it to be there. As long as there's some evidence for that, it confirms that kind of representation. In neuroscience, this is handled by feedforward and feedback projections between regions, which would solve that problem. If I saw it here and come back to that location, I immediately invoke the expectation of the object I saw before. We should do something similar. Even in the original Monty architecture, we had a short-term memory module learning in the same way, representing the structure of the current scene. It's not a permanent representation, just a temporary one of the scene structure.

The idea is that each learning module has a hyperparameter for how fast it can build new models. If it builds them up quickly, like short-term memory, it's not very permanent. If it's built slowly, it's more stable and general. You wouldn't always want to classify it as temporary. If I saw something, I would recognize it, but I wouldn't say, "Here's another cluttered room." If you showed it to me three days from now, I might have no memory of it. There could be a continuum: build something up, and the memory fades slowly if not reinforced. That's what we do with most of our memories.

In the short term, I propose we don't worry too much about getting a scene-level understanding at its most basic. What we first want is, given a series of overlapping objects, to be able to recognize everything that's there and, when looking at something, correctly recognize what it is. This work can operate in parallel to work on hierarchy, which is important for true scene representation. You need to be able to recognize the objects first, which is necessary for the next step anyway.

With that discussion aside, I wanted to return to action policies and how they could relate to multiple objects. A simple, useful policy is to stay on an object, or at least try to, until there's a reasonable amount of confidence in its ID. Sometimes this will be harder, especially in scenes that are difficult to parse and visually understand. There are various heuristics and top-down cues that could help, based on the model you have—for example, if you expect to see a car, you have some understanding of where it would be. You can combine this with multiple modules representing objects to guide sampling, especially in clustered regions, but potentially in a more intelligent way. You might try sampling elsewhere to see if it's consistent with being the same object or not.

Similar to staying on an object until you're confident, generally focusing on nearby foreground objects with less occlusion or near in depth should make it easier to resolve a heavily occluded scene. Ultimately, there may be genuine ambiguity or uncertainty in a scene. You can move your sensors, or eventually interact with the objects, to reduce uncertainty. Some scenes may be impossible to disambiguate passively, so the sensorimotor element is important. For example, if you look at a picture of a car, and instead of the car being black and white, it's green, and the bush has little lines of green in it too, it might be much harder to distinguish. There are all kinds of clues that come into play—color is a big one, as is the general texture of components. There aren't as many lines in the tree as in the car. It's a multifaceted problem.

We just have to be careful not to make it too hard for ourselves. I also think with some of these things, what you're describing can be hard to codify and write down explicitly. I wonder if this is one area where we might want to use a bit of deeper learning, just in extracting features that tell us how similar parts of an object are in that feature space. I don't think it would be too crazy if the brain is doing some bottom-up, non-linear processing.

Actually, I was thinking the opposite. If I'm looking at the car on the left side, I don't get fooled by the green things; I see if they make sense with the car. It's not like I'm jumping to the right. It's clear I'm looking at the back of a car—I recognize that as a VW Beetle. I'm not confused by the green bush because it's an easy distinction. In terms of action policy, it's an easy action policy. If I occlude everything to the right off the left edge of the tree, I can still recognize the car. I don't actually need the other end to recognize the car, but once I recognize it, knowing the morphology of cars, I can predict there's going to be a front wheel there.

It's really not about bottom-up cues. The bottom-up cue might be important if the car was green and the textures weren't as different, then it could be hard to stay in this area and know what to do. To me, recognizing this as a car is a bottom-up action policy. I can just look at the left side, follow the black lines, and quickly say, "Oh yeah, that's a car." Multiple columns could be voting on that at once. That's a bottom-up thing. Once I see it's a car, I could test some things, like check for a taillight. In standard computer vision, it's traditional to use low-level cues like color, T-junctions, and overlaps, but I don't think that's necessary here.

If you don't see what it is right away, you would narrow your attention and maybe the action policy is to go to the high-frequency components of the image, like the black lines, and move your attention around those. In our vision system, multiple columns would still be moving around over that rear end, and I'd say, "Oh yeah, that's a car."

Can you say that again in terms of why you don't feel like these kinds of bottom-up cues are necessary? Are you saying the bottom-up cues aren't necessary? No, not in the traditional computer vision way, where they deal with occlusions using specific types of edges and junctions. There are rules of continuation, edge continuation, color differences, and other low-level cues for when occlusions occur. I don't think we need any of that. It's much more powerful to recognize the object and know from your understanding that it should be there, even if the bush isn't there.

One question I have is, when you perceive that the tree is in front of the car, that has to come from some higher level. That would be in the scene representation, independent of the specific features. Even if you exclude the left and right sides of the car and just have that one little blind contour in the notch of the tree, you would still have the sense that the green object is in front of and occluding whatever is behind it.

Border ownership is important. I think to get something that works and is able to recognize objects in a scene, there are starting points that are required. I had to figure that out before I could recognize the car, or is it something I do top-down later? There are things that must be in front of others. I don't know which comes first, but there is a percept of how to interpret the scene based on the sense of one thing occluding another. I think that's a top-down process. If you just look at the left-hand side, you can say there's a car that looks like the rear end of a VW bug or something like that. These processes, whether they happen sequentially or not, inform each other and become part of the composite percept of what you're looking at.

It's 11:20. We've been going for over an hour. Do we have additional material, or is it just this presentation? Are we close to the end here, Niels? Yes, last slide. This is about multiple columns. You could have islands of consensus, but it may be necessary to use something like attention to narrow the fields of columns that are communicating to reach consensus. For example, in a setting like this, you might focus high-sensitivity sensors, like the fovea in humans, while getting signals of something similar, like a hubcap, in the periphery. Whether through direct lateral connections or via a level of hierarchy, there could be a way for learning modules to communicate and decide to move high-density sensors to a specific location.

If learning modules are uncertain about a particular area of the scene, they could ask high-density or other modality sensory modules to help identify what's there.

Many of these ideas are straightforward and basic. The main thing is how to implement this, and that's the next step.

Thanks very much.

One of the bottom-up parts of vision that will probably be necessary, especially with occlusion, is the blob and interblob cells. One tells you this is a contiguous region, and the other tells you where that region ends and another begins. Being able to segment space is important for figuring out that this part of the vision space is one object and this is something else. Is it possible to put that into a sensor in your current framework? Are you talking about blobs in V1? Yes, the blob—edges versus gradation, continuous regions.

I need to review that because I'm not sure. Can we table that for now? I don't think everyone understands the blobs concept. It's the low entropy, smooth regions versus the high entropy, edge regions. Could you send a paper on that, Eric? I'll find it. I don't remember that being associated with blobs, so I must not be up to speed. I'll see if I can find it. Thank you.

This presentation is a bit more high-level and vague than Niels's. As you saw in the timeline, there are two branches: action policies and hierarchy. Action policies have already started—Niels has been working on them—but hierarchy has only been discussed in brainstorming meetings. I've tried to condense some of this here. Sorry if I didn't capture all the ideas we discussed.

What are problems we would like to solve with hierarchy? One is the classical problem: being able to quickly associate learned models with each other without completely relearning them. For example, if we already have a model of a cup and a model of the NOA logo, and then see a cup with the NOA logo for the first time, we can quickly associate the two without building a completely new model. That's how I'm going to phrase hierarchy in this presentation: hierarchy serves to associate models with each other.

That ties into the second goal: representing higher-level object-part relationships and more complex object structures without requiring huge amounts of memory. I'll make a detour to visualize this problem. I've tested our Monty system on the Omni Guard dataset, which is 2D data. 2D is very nice to visualize this problem. The GL dataset is a collection of different alphabets, and each letter is drawn in many variations—20 versions of each letter in each alphabet. For example, in Korean, we have a character that looks like an H, but the H can take many variations. The two bars can be further apart or closer together, longer or shorter, and the horizontal bar can be tilted. There are many variations. The Monty system, if it has learned the first variation of the H, can recognize it again with small local distortions. But with more global distortions, like longer or shorter lines or a larger distance between the lines, it can't really handle that.

Basically, to illustrate that, here are the different versions of this character in the dataset. If we overlay all of them, it gets messy. To represent this with one model, we would have to give the model a lot of slack, meaning anything in this purple area would count as this H character. I'll just call it H for now. If we have a model that allows for a lot of slack in this spatial field, it would also allow the letter to look like an X or something very different, which is not what we want. We want to preserve the subcomponents of the letter, like the two longer parallel vertical bars and the smaller horizontal bar, but allow their relative arrangement to vary. I think this requires hierarchy.

If we use a hierarchy as I imagine it, we could have lower-level models. I know this is in 2D, but I'll show a 3D example in a moment. In general, all our models will inherently support or be in 3D. We would have models of lines of different lengths, and one level up in the hierarchy, these models or object IDs would become features with poses. The longer line would be a feature present here and here with the same pose and orientation, and the shorter line would be at this location with a 90-degree rotation.

Quick question on the lower-level models: when representing these lines, it's not just an edge. It's actually representing it as an object that you recognize under different variations and poses. It's not just an edge; that's a big distinction. It's a complete object. Even though you're writing this as one letter, it could be like three letters in a word, like CAT for cat. This thing is a vertical, horizontal, vertical.

Like you said, this is definitely not just an edge; it's a full object model. Even though I'm showing this as a 2D image, it is represented as a three-dimensional graph in theory, or at least supports it. All of these are full object models. When we explain this externally, we have to be careful because that's a common misunderstanding. In neuroscience, you get these edge detectors and miss the fact that it might be modeling a complete object that happens to show up as an edge.

I hope this will be clearer with a 3D example, but I thought this would be easier to follow at first to see the problem. Now we can constrain the degrees of freedom. We need to have solid, straight lines without breaks for this feature or object ID to be detected. The pose of these features can vary slightly; for example, we can rotate the bars a little or adjust their relative positions, but that significantly reduces the degrees of freedom.

The output of the higher-level model is the same as the input: a feature ID and pose. For example, the feature ID would be for this character, and the pose would be the pose of this character if the object is rotated. It works the same as the lower-level models, just outputting the same feature ID with a different pose. It assumes all these features will have their poses rotated relative to each other, working exactly the same as the lower-level learning module—same input, same output, same inner workings. That's the principle behind it.

Because it constrains the degrees of freedom, if we take these other variations, they would not be recognized as the letter H anymore. For example, a curved line is not one of the features in the H model, so it would not be recognized. It might recognize the longer line as a feature, but the relative pose would be wrong. Some variations would not be recognized at all. The lower left one is a lowercase H, but that would have to be represented independently. This is a Korean character, so it doesn't apply, but if it were English, lowercase H would be a separate model.

Vivian, just to be devil's advocate, if your original H was composed of dashed lines, how would you handle that?

That's a good question.

That's just aligned with occlusions. It goes back to representing these lines as complete objects. If the occlusions became too much, you wouldn't see it as a line. If the dots were close together and there were many of them, you wouldn't have a problem; you'd still see those as a critical line. It's fascinating that the brain seems to have a prior that sometimes what you're seeing is a known object, so you interpret a dashed line as one continuous thing and assume the occluder is in front, just like a 3D prior.

I wonder if some sort of top-down feedback might be important or helpful. If the Korean letter was overlapping another Korean letter, and you were trying to parse the low level, which line segments do we care about? Information from the higher level can potentially help with that. That kind of iterative process is what a lot of different systems try to do for this kind of thing.

I'm not really thinking of this as a pure hierarchical process where we first get the lowest-level features and then build up, but more as an association of models with each other. We also have this higher-level model of the H, which has a certain contour shape.

I don't know if that makes sense, but that's the idea.

I think you also have to consider that when you have the H, you can handle occlusion and easily remove my dashed line with that. However, if you can see an H and the only difference is texture, the notion of what constitutes a feature starts to overlap with what is a connected region of a similar type. This touches on what we discussed earlier: there are two forms of the model—a morphology model and a feature-based model. The way I've been thinking about it, crudely, is that the morphology model is much more forgiving, with lower resolution and less high frequency. Fuzziness, dotted lines, and similar aspects are more likely to appear in the specific feature component of a model. That's why you can recognize the morphology of multiple letters that have different colors or textures; none of that matters for the morphology model, but they matter for the other details.

In the same way, a coffee cup is still a coffee cup regardless of a logo on it—that's a specific feature on that particular cup, but it doesn't change the fact that it's a coffee cup. Some of these issues rely on the idea of these two model components.

Let me show the 3D example real quick. Now, moving to more complex examples than simple characters—here, for example, are a bunch of different airplanes. If you wanted to represent all of these in one model, you would have to allow for a lot of slack in various directions. They vary along many dimensions. Even with hierarchy, I think this is difficult to do; the bomber plane is quite different from the other planes, and the X-Wing is also not an airplane. I just wanted to point out that the X-Wing is not an airplane; that stood out to me as one that doesn't belong. A space plane is not an airplane, but all the other ones are. The standard airplanes have a pretty common structure. As an example, they could be represented the same way as I showed with the letter: lower-level modules would have models of cylinders, tube shapes, and wing shapes, and then in the higher-level model, we would store the relative arrangement of these 3D models as features relative to each other. That would make up a generic airplane model, but the relative arrangement and tilt of these features can vary, allowing for different versions of airplanes.

What's interesting and puzzling about this is that we can do what you just described—we can move these parts around, give them different shapes, and so on—yet at the same time, I can identify many of those planes individually. This goes back to the classification issue. It's puzzling that we can have models that let us recognize a new plane, but we can also recognize many different specific ones.

This brings up the generalization and classification problem again. We recognize both at the same time: a specific airplane and that it belongs to the general category of airplanes. To recognize a specific one, I have to attend to specific features, like the one on the bottom center—I said, "That's one of those transport planes because the front looks funny," or the one at the bottom, "That's got propellers on it, so it's an over-the-wing prop jet." There are numerous interesting problems that come up in this analysis, like a functional analysis. All the airplanes have wings that can support themselves in air, apart from the X-Wing, which wouldn't, but I have to know that. To know what an airplane is, I don't have to know what wings do—a child could play with a toy airplane and not know that they fly.

You can recognize a category without realizing why it's a category. Toys you look at may not mean anything, but you recognize them. This classification versus specific general category issue keeps coming up and makes you feel like we have to deal with it sooner rather than later.

I'm not disagreeing with any of this analysis. I'm just thinking about how we could deal with it. For example, the general category model of an airplane might allow for a lot of slack in the relative feature arrangements and for additional features to be present, like propellers. A specific model would associate additional specific features or constrain the pose of some features. In terms of the work you're doing, if you wanted to avoid this problem for now, it wouldn't bother me. You might say, "We're just trying to differentiate specific airplanes," and not worry about the general classification problem. We don't have to tackle everything at once.

Even without categorization, this approach gives us the advantage of being able to store very complex objects like airplanes more efficiently by arranging subparts relative to each other. Coming back to the problems we want to solve, at a higher level, the key capabilities we want from hierarchies are being able to deal with compositional object categories and structurally complex objects efficiently.

And I have a few slides on how this may be happening. I'll go real quick. Basically, we already have the bottom part of this diagram. Now we have to figure out the communication protocol between learning modules and different levels of the hierarchy. Ideally, we wouldn't want to touch the inside of the learning module or the general input-output format. The design should allow the hierarchy to associate models with other models as features. The idea for communication is to keep the protocol the same, no matter where a learning module is in the hierarchy. The input is the same as the output: features, a number of features plus a pose, and features at higher levels can be object IDs. They always have just one pose, possibly relative to the body. 

We could have lateral voting between modules that share models of objects. This is unrelated to hierarchy, and those can vote on multiple possible poses, so that can be a list. That communication is independent of what is going on inside a module and doesn't pass on information about the internal model itself, only about public ID and pose. That seems right because the lateral voting point isn't about hierarchy; it's something that exists independently. I included it because I was talking about communication in general, but it's good to clarify that these are two different communication pathways.

What would be the difference between lateral voting and top-down voting? Evidence from above could impose restrictions on what inferences you make lower down. I haven't really thought about top-down feedback yet; that's something we still need to figure out. It could be thought of as just another voting pathway, but they would have to know about the same object IDs. If the lower-level module has more of the part component IDs, then the higher-level module would need to know that if it votes on the whole object ID, this is associated with certain parts. The same is true for lateral voting, because if you have voting between touch sensors and vision modules, they may not have exact correspondence of object IDs. It could be a many-to-many relationship.

Before, with lateral voting, you're always voting between columns that are trying to infer the same object. They have models of the same objects and are trying to reach a consensus that this is the same object. In hierarchical voting, that's not the case. In hierarchical voting, you're just associating the location on one object, the higher-level object. At this location, a feature can be an entire object from the region below, but I may not know what that feature is or what it means. I may or may not even have a model of that thing. I just know I was given some bits that said, "Guess what's at this location." I can give feedback saying I'm at that location again, and you shouldn't have broken the same thing you sent me before. 

So the cup could be saying, "On this location, you showed me some bit patterns. I didn't know what it meant. It turned out to be the logo. I'm not going to tell you that same ID—I don't know what it means, but you're supposed to invoke it." Now you have to vote for a reference frame for the logo and instantiate the logo. In one case, you're voting on the same objects; in the other, you're associating different objects at some location. Does that address what you're thinking about?

I understand that distinction. I'm just thinking that lateral voting, in some situations, may have the same properties. Maybe we can think of it as the same. An example would be multiple sensory modalities—maybe you hear something versus you see something. It's not necessarily a one-to-one relationship between them. If you see something, it constrains the types of sounds you might hear. The same thing applies to the object up top: the higher level will constrain the objects that are possible down below.

There's a biological difference too. From what I recall, the lateral connections are layer 2/3 to layer 2/3, whereas the top-down connections are layer 4 back down to layer 2/3, or at least the connections go that way. There is anatomical layering: layer 6 to layer 1, and then going backwards and forwards, it's primarily layer 2/3 to layer 4. The lateral connections are more than just layer 2/3 to layer 2/3; it's also to half of layer 5. This is how it was done in the paper on connections in hierarchy. They discussed how to decide if two regions from the same regions are connected—the same cells in the same layer connecting to each other. So layer 5 connects to layer 5, or layer 2/3 to layer 2/3; those are considered lateral. Those are the same level in the hierarchy. Anything that's different cellular layers, like layer 3 and layer 4, or layer 6 to layer 1, are hierarchical connections. That's how I continue to think about it.

From a mechanistic point of view, maybe there's a similar kind of mechanism going on. I'm not sure. I'd have to think about that.

Whether it acts more as a gating mechanism as opposed to a voting mechanism—in other words, is it possible for it to say, despite the hypothesis you're coming up with down there, I have this overall gating thing that negates all that? In other words, it's a conditional rather than a strict override. I don't think you want to throw out the thing at the bottom because that would be total confusion. It would be like saying, I think I'm on the coffee cup and the bottom says, I'm seeing a car. That doesn't make sense; something's wrong.

What the top-down feedback does is bias the lower-level region. It says you may not be certain what you're looking at, or maybe you're not even seeing anything yet, but I'm telling you what you should be seeing. You can use that to both predict what you're going to see or to refine and narrow down your confusion.

I'm just trying to think about whether the lateral is voting based on various pieces of evidence, and whether the top-down has some primacy or might be able to override. In the dendrites project, the dendrites are basically conditional on what you're going to see at other levels, so maybe there's a different mechanism, but I don't think there's a primacy to this. Different parts of the cortex are saying what they see, and if they're incompatible, the brain wants to resolve that incompatibility. It's not going to say, just ignore what you're saying, I'm in charge. I don't think it's a primacy.

Let me give you an example. If I'm searching for a particular book that I know has a red spine, I will exclude a lot of other things that I'm looking at. That filtering mechanism says, at the lowest level, you're seeing coffee cups and all these other things, but I can tune my vision so that I'm only looking for something, and that's the only thing I'm allowing into my perception. That is often true. In that case, you'd be saying, you should be biased to look for this book, and then the lower region will try to fit its inputs into a book hypothesis. If the lower region's not seeing a book and it's seeing a coffee cup, it's not going to say, no, it's too bad. It's going to say, no, I'm not right, keep going.

At a mechanistic level, the way we implemented it, gating and voting are basically done with almost identical mechanisms. Apical dendrites do have slightly different behavior than lateral or basal dendrites, so there may be something there, but I'm not sure.

Another comment is that you have things represented as discrete object IDs, and this is something in the comms paper as well, but we've often thought about that as not being so discrete, but more of an SDR, where there's some overlap. This gets back to your question: can you do all this stuff without SDRs? How far can we go before we have to move to a new version where everything uses them? It's 42 versus object ID 103, as opposed to feature vectors or overlapping SDRs. I think the communication protocol should be relatively unaffected regarding how we actually represent the features and the posts; we could represent them as SDRs as well. It's a question of similarity—object IDs imply just categorical things with no inherent similarity, as opposed to distributed representations that have some notion of similarity.

We've had this discussion over and over again about whether we have to use SDRs, and if we do, how far can we go before we have to use them. I'm fine continuing without them, but it keeps coming up. That would definitely be a good reason to use them—to be able to express more similarity between objects.

It's interesting to consider whether you could do a hybrid system, with very high-level thinking where you can inject SDRs where you need them, but go back to numerical representations when you don't, or represent categories too.

Can you have a system where part of it runs on IDs and numbers and the other part runs on SDRs? Today, everything is numeric, but we wanted to do both. I would just say, use SDRs. It's a significant change. Incrementally, for example, we could keep everything in the learning module as it is now and just represent the object IDs as SDRs that express similarity. At the higher level, the object ID basically becomes a feature, and the learning module automatically takes into account the similarity between the SDRs. That would be a hybrid system. I'm not sure you can actually do that, but if you can, that would be great. It would be really nice not to have to throw out everything and start over. If that could be done, that would be great. It's not clear how you would do that, but we have our company meeting in less than five minutes.

One more slide—maybe I'll just go over that real quick if we have another minute. We have five minutes. How would this look in the learning module? The previous slide was about communication. Each learning module learns three models, as we already have, no matter where in the hierarchy. A higher-level learning module can use lower-level learning module outputs—object, idea, and pose—as features. You could also combine this with direct sensory module output. The lowest-level learning module could, in theory, learn complex object models like an airplane if needed, but it would require much more memory and training and would allow for less category generalization.

How would learning work? It's not clear to me. You don't have to explain it, but just to note, I don't know why it would allow for less categorization. I'm not questioning it; it just didn't immediately make sense to me. Basically, like the example with the H, it allows for small local distortions, but if global parts of the object are distorted—like the bars of the H are further apart or closer together—then it can't really deal with that. I think there are other ways we could potentially solve that problem, so let's leave it at that.

One big question that remains is how learning would actually work in the system—how do we decide what is being learned and where? That's a pretty big question mark for me.

This is my favorite slide. It doesn't seem as mysterious to me, so maybe we can talk about that if you want.