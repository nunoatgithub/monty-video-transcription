Q2. It was fun putting these slides together because you forget about all the things you did, and then you see them all again. This is what we set out to do. Three months ago, we wanted to pave the way for Monty to model compositional objects, make Monty easy to use in a variety of applications, increase our academic engagement, and figure out how to model object behaviors. I'll go through the sub-goals we set for each of those.

For paving the way for Monty to model compositional objects, one sub-goal was that the accuracy on the unsupervised inference benchmark is dramatically improved via hypothesis resampling. That's definitely checked off—we went from 12% accuracy to 96%, and from 6% to 94% accuracy. That's a dramatic improvement, achieved through hypothesis resampling. Great work, Rami.

We aimed to dramatically improve the speed of inference for distant age via better policies, but this was deprioritized, which is why I marked it gray. Other priorities came up, like finalizing the DMC paper and wrapping up the hackathon, which took a lot of time. We deprioritized this, but toward the end of last month, Scott spent a lot of time outlining how we want to approach this, and we now have a concrete plan for next quarter.

We also aimed to put together a testbed for compositional objects. Logos on Cups is available, and that's done, thanks to Scott's work last week. The performance baseline quantified is not done yet; that will be a bigger project, figuring out how we want to measure performance, how we want to do inference, and so on.

The motor system now supports swapping out policies, thanks to Tristan.

For making Monty easy to use, we had the goal of disentangling the motor system from other classes. Tristan did a lot of work on this, but it's a huge project and still in progress, though there's been significant progress. As you can see from all these merged PRs, Monty is not constrained to Python 3.0. This is also in progress. There's a draft PR that Jeremy put together, and he put a lot of work into making it unconstrained to Python, but we haven't merged it yet. Jeremy is currently exploring a different path because the newest Habitat SIM versions, which are needed for a new Python version, require GPUs. We would have to add GPUs to all our infrastructure just to run the simulator. Right now, Jeremy is adding a different simulator instead of relying on Habitat.

We published tutorials on using Monty in novel applications and in robotics. I put up two tutorials and published several example projects from the hackathons: the Lego, Asbury Pi project and its documentation, the ultrasound project and its documentation, and the drone project. There are also hints about what to watch out for when building a robotics application, plus the website once it's public.

For increasing academic engagement, university outreach and creating content targeting universities didn't get as much attention as planned this quarter, but Will created some content pieces, like a short two-page PDF. We'll also have two papers to use once they're published. Everyone looked into conferences to attend; not all are booked yet, but many are planned and booked. Some plans are made for this year. We aimed to add 500 academic contacts on Twitter and Blue Sky. On Blue Sky, we have more than 500 that Will added; on Twitter, not as many, but Blue Sky works better now as more academics are there. This is still in progress.

Will released 36 hours of video. For some reason, YouTube doesn't give you this statistic; it just tells you how many hours people have spent watching your videos, which is 1,362 hours. I used a calculator and added them up, and it actually totaled 36 hours. I don't know if you planned this, Will, or if it was just a lucky coincidence. There were many research meetings that Will processed, transcribed, and uploaded. The most exciting thing is that we're almost caught up with the backlog of videos. That's very exciting because we have over 200 videos in that backlog.

The item that got the most attention this quarter, especially from the research team, was publishing or finalizing these two articles, especially the DMC paper. Niels uploaded them last night to arXiv. For the peer-reviewed journal, it will take a couple more months to get the reviews and address them.

For figuring out how to model object behaviors, one goal was to have a concrete proposal for generalizing the stapler behavior to another object, including being able to predict observed features. We have a concrete proposal for that: sending the kind of movement stored in the behavior model down to the child object's column and moving through the child object's reference frame. We also have a concrete proposal for solving many questions around modeling object behaviors in Monty. There are sections on that in the disclosure document that Jeff and I wrote and published, but it's not super concrete yet, so I marked that as in progress. For filing a provisional patent application or public disclosure, we decided to go with public disclosure and disclosed this PDF plus a bunch of videos of us discussing these new ideas.

Although we didn't complete all of the sub-tasks we set out for each of those, I decided to mark all of these as green because we spent a significant amount of time on each of these broad items and made a lot of progress on each. Overall, almost everything we did fell into one of these four categories and tasks, which I thought was really great. There was a lot of good focus on those.

There were a couple of things I didn't mention yet, but we still spent a good amount of time on, so I wanted to highlight those as well. One is interacting with the community, which is an ongoing effort but still takes a lot of time, especially when there are more detailed replies and questions. Thanks to everyone who interacted on Discourse, posted replies to people's questions, and helped them out. Also, thanks to people like Tristan, who reviewed PRs on our GitHub repository from external contributors.

We also presented the project at several venues. Tristan presented the project, I was on a podcast, and Scott presented our project at the Chen Institute. Both of those involved new slide decks and new ways of presenting Monty, which are really good to have in the repertoire.

We got a better idea of potential applications of Monty. Tristan visited the expo and other potential places in the robotics space. I put together a presentation of applications and how they tie into Monty's capabilities. Niels and I spent time talking to ultrasound vendors, people who worked with AI on ultrasound, and people who worked with computer vision and implemented ultrasound projects, including those with access to clinics, to figure out what's out there in the space. We considered whether this could be a first application for Monty and whether someone would be interested in building this.

That also got a fair amount of attention. The hackathon, although it was just one week, demanded significant preparation, including ordering parts, pre-training models, and getting swag and prizes. There was also a significant amount of post-work, like documenting everything. Tristan collected feedback from everyone on Monty's usability and issues they encountered, started addressing some of that feedback, and did some debugging work.

There was also a good amount of brainstorming, especially during the brainstorming focus week. I already mentioned in the main Q2 priorities that we have a proposal for object behaviors applied on a stapler, but we also talked a lot about additional problems like object distortions, feature or surface models, general, generic, and specific models, circles and ovals, interpolation invariance and equivariance, learning in lower and higher regions, and other topics. There were a lot of interesting ideas and thoughts. There were also code, infrastructure, and style improvements throughout the month, continually improving things in small ways.

Overall, really great job. Everyone did a really nice job with all the things we accomplished.

Now I'll hand it over to Niels for a quick recap on everything that happened to produce the DMC paper. I know there's been a lot of recap already, so I'll make it quick, but I thought it would be nice to revisit this since it was a huge effort to get this out. It's definitely been a big milestone and ended up being a big project that we worked on for a long time. I think everyone's really proud of what we've put together, and rightly so, because our ambitions grew as this unfolded. The original motivation was to establish Monty's capabilities, show that to the community, and communicate the current state of the art of a Thousand Brains system. There are a variety of reasons to do that, but one of the main aims of the TBP is to get people excited about and using Thousand Brains systems and this approach. Showing what it's actually capable of is a big part of that, especially for the academic community.

Along the way, it was surprising how many new things came out—new metrics, new understandings of what Monty is actually capable of, things we hadn't really conceptualized when we first decided to write this. There were also lots of fixes and general improvements to Monty's implementation, many of which were definitely not trivial.

It's been a huge team effort from everyone at The Thousand Brains Project. As I said on Wednesday, a lot of the groundwork for this even started at Menta. Although we've been working on the DMC paper for maybe eight months, this really started more than three years ago with the initial work on Monty. Special shout out to Scott and Hojae for the huge amount of effort they've put into this. When you joined, the idea was this would be a two- or maybe three-month project, a nice way to get familiar with Monty's experiments and how to run those. Thank you for all the effort you've put in, despite it actually being a much bigger project than we expected.

To rewind back to when we started on this, what we had at the time was Monty, a similar version to what we have now, but with some bugs and issues that we would later discover. We had some benchmarks gauging things like accuracy in the context of certain amounts of noise. We also had some visualizations, but nothing particularly glamorous. When we were doing the initial scoping work, it started fairly simple but eventually grew quite a bit in ambition. I won't spend a long time on it, but to put into context the amount of work that went into some of these figures, we had these alii draw boards—this is just version three—where you can see a huge amount of variations, comments, and other things going on. This was version four, and I think it went all the way up to version five. What we ended up with were these six really beautiful results figures that do a great job of showing how diverse Monty's capabilities are. Any one of these would be an impressive result, but the fact that all of these capabilities emerge almost automatically from developing sensorimotor AI informed by the cortex is exciting. It feels like a strong signal that what we're doing is on the right track.

Along the way, this resulted in two new repositories providing comprehensive support for anyone who wants to replicate these experiments. That's really important from the perspective of open science, and it should also build trust in what we've done and increase the likelihood that people build off of this. This was a huge amount of effort—30,000 lines of code and experiment configurations for the main paper repository, and then the epic repository that Hojae put together, TPP Floppy, which for the first time enabled us to quantify the amount of flops, the amount of compute that Monty consumes, and benchmark that in NBTS. But this doesn't capture all the other work, as I mentioned before. Even just replicating the experiments, not to mention running them in the first place and running them many times—every time we realized a change we should make to a figure or a bug was identified, it led to a cascade of running everything again. It was a huge amount of effort.

A quick summary of some things in terms of new evaluations and metrics: we added better visualizations of movement sequences, properly quantified symmetry for the first time, and found a way of verifying that. We looked at adversarial color robustness, the stepwise effects of the model-based policy, scaling of voting for the first time, figured out tie breaking, and quantified learning efficiency. We also compared Monty to VIT; before, we had no comparisons at all to deep learning architectures. That was a huge effort, particularly on Hojae's part. We also compared flops and quantified that versus the VIT, looked at continual learning, and compared that against the VIT. Outside of that, we got updated professional diagrams, did a literature review on related approaches, and made many improvements to the configs. Simple things like specifying a voting system in Monty, where you have multiple learning modules in a grid, was not trivial before. Scott made many improvements to that. We now have a comprehensive mathematical description of how Monty works, which we didn't have before, and notation for that. We also fixed a lot of bugs. When we started, voting only worked with learning modules and sensorimotor modules aligned in a one-dimensional grid. We couldn't have other arrangements. We fixed the X percent threshold, patch off object, which was a huge effort on Scott's part, and semantic sensorimotor, which was leaking into our experiments and we didn't want that. I'm sure I've missed some things—it was a lot. All these metrics and evaluations and the pipeline for visualizing them are work we can reuse when Monty's capabilities grow and we want to communicate that again. It's definitely not a one-off use case. Now we have this 32-page paper that we can share with everyone and talk about when they ask what Monty can actually do. Thanks, everyone, for their hard work and patience through this whole process.

Since you highlighted yourself, I want to highlight that it would not have been this high-quality, awesome paper with all these great results if it wasn't for you. You put a huge amount of effort into it, especially over the past months, pulling everything together, coordinating, making sure all the experiments are up to high standards, and writing really nice text that's fun to read. Up to last night, you were figuring out the last issues and checking the final numbers. It would not have been this great, high-quality paper if it wasn't for you. Great work, Niels. Thank you, everybody. I didn't know what this paper was going to be like when we first started, but it turned out much better than I imagined. The results are really impressive. Thank you. Thanks to everyone.

Q3 has already started as of last week. Neil, Tristan, Will, and I put together a couple of priorities. I forgot to change this header—it should say three here. I liked how they aligned last time with our TBP mission, so I tried to align them again this time. Some of them are similar to or extensions of last quarter's priorities. The first one is an extension of the first one from last month. Last month, we were lining things up for Monty to be able to model compositional objects. This quarter, we want to have Monty actually model compositional objects, assess its accuracy, and demonstrate that with hierarchy, we can model compositional objects in Monty accurately and efficiently.

Then, we want to make Monty easy to use, improve, and contribute to, both for the TVP team and for external people. I'll break down each of those into actionable items we're planning to do. Third, we want to increase external interest and awareness of our approach. Fourth, we want to figure out how to model object behaviors completely.

What do those concretely mean? Enabling Monty to model compositional objects accurately and efficiently. First, we want to set up a dataset, training and inference protocol, and evaluation measures to assess Monty's composition modeling abilities. This is not trivial. We have a dataset that Scott put together, but now we need to figure out how to actually train Monty on it, how to provide a supervisory signal for the child object and the parent output, and how to evaluate the correct classification, since different learning modules recognize different objects. We'll need to set up new evaluation measures and think through how training and evaluation should look in this compositional object setting.

We want to improve hypothesis resampling based on different signals. Last quarter, Rami implemented a way to resample hypotheses, and now we have several signals to inform that, such as prediction error, out-of-reference-frame movement, salient features, and confidence. This should result in improved or comparable accuracy on the composition dataset with reduced computational overhead, making it faster and more accurate because we don't have to test as many hypotheses.

We also want to update policies to improve accuracy and enable sparse-sum models. Currently, we use a random walk on the object, but this ties into model-free policies to focus on salient features or ensure we stay on an object until it is recognized. Lastly, we want to add and test the idea of a 2D surface sensorimotor module that extracts 2D movements and features to learn things like a logo—a 2D representation that can wrap around 3D objects.

For making Monty easy to use, improve, and contribute to, we are continuing the motor system refactor to disentangle the motor system from other classes.

Monty will no longer be constrained to Python 3.8, which also ties into support for the Muco simulator.

We are adding an interactive and up-to-date overview of where people can contribute, so the number of external contributors can increase because it's easier to see where to help. We are also adding more documentation on how to contribute and how to customize Monty for specific applications, making it as easy as possible for people who want to contribute or use Monty.

To increase external interest and awareness, we are publishing material around the hierarchy and DMC paper. Will already has a lot lined up, and there is more to put together: videos, presentations, plain language explainers, posts, and so on. We are presenting Monty and BRS theory at several conferences and speaker series, reaching out to universities, and presenting at their symposium speaker series. We are also organizing an online workshop or symposium about Monty, similar to what we did in December.

This might not happen next quarter, but we want to at least organize and plan it.

For modeling object behaviors, we've made a lot of progress in the past half year. We hope that in the next three months, we can have a theoretical proposal for the open questions, especially around object deformations and how actions play into that—how actions change the state of objects. We want a concrete writeup or even a prototype of how object behaviors could be modeled in Monty. Ideally, a code prototype, but that depends on the complexity of the solution we develop.

That's the overview. There's a Google doc in our drive that I can share in the team channel later with more details. These were just the high-level company priorities, but there is also a more detailed breakdown, especially of the research objectives, that Niels can go over later in the research meeting.

The engineering and community objectives are also broken down in more detail there.