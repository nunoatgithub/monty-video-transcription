The first part is a recap of the basic proposal I made a couple of weeks ago: behavior and morphology models might be in separate columns. One might also call it "going full Monty," since no one referenced that before. That's the code name I'm using now, just following Mountcastle's proposal to the end—that whatever each column models depends on the input it receives.

Here's our current model. We have movement and feature input from the sensorimotor system going through the thalamus. Movement input goes into upper layer six, feature input goes into layer four. We discussed that orientation input might go in at the border and activate the minicolumns, and features might cause very specific activations within the minicolumns. There are associative connections between layer four and layer six that learn which features are at what locations. Layer six represents the location on the object, and "a" is location on the object, "b" is orientation of the object. The orientation can be used to rotate both the incoming movement vector and the incoming orientation feature. Over time, those activations get pooled into something like an object ID, representing which objects we are sensing overall. That remains stable as long as we're on the same object, even as the sensations in layer four and layer six change at every step. We can also have feedback input from a higher-level column that tells this column which location it should expect to be at, and a broader context signal to multiple columns in the lower level of the hierarchy, in layer one, which can be connected to layer two, three, and five A. I'm not a hundred percent sure exactly what each of these gets out of this context signal. So far, nothing new here.

When we introduced behavior models, we considered a parallel system: changes and directions as input to upper layer three and layer three minicolumns, a movement vector as input to layer five B. We have two reference frames—one for the behavior model, one for the morphology model. That one also has an orientation, which can be used to rotate the input. There are associative connections, maybe between layer three and layer five B. I thought it didn't require doubling up on these things, that it didn't require a separate movement vector and a separate orientation vector, that those were shared.

They need to be separate if we want to recognize behaviors and morphologies independently of each other in independent orientations.

Then they get pooled into a behavior ID. An additional aspect is that there's some kind of time signal in layer one, where neurons in layer three can synapse and have some temporal conditioning on what changes to expect at which locations. 

In this picture, there are doubles of everything, and there's no interaction between these two systems. Blue and green don't really interact at all within this column. We talked about how they would, but all the interactions so far were always between two hierarchically arranged regions. We tried to do it originally within a column, but that became difficult, so we said it's too hierarchically arranged. We've discussed different interactions between behavior and morphology models, and there need to be some, but the solutions we've come up with so far were always two hierarchically arranged regions. The only one I can think of is that a behavior can bias an ID recognition and vice versa. For example, recognizing the banana morphology will bias the banana peeling behavior and vice versa, but that's easy to do with voting across columns. That would work the same way with voting across columns.

A general issue is that sometimes behaviors are instantly associated with objects and are not generally shared. In fact, they start off being associated with objects. Sharing behaviors generalized to other objects is the challenge. We also said that you can have a behavior at the lower-level column and then associate the behavior ID with a location on an object, so at this location, that object has a certain behavior, but again, that's hierarchical—the behavior is in the lower column.

The general proposal is to separate out the blue and the green, which would allow us to move everything back into the original layer positioning, so we don't have to shift things around to make space for more functionality. We could also generalize it and say that the layer one input in the morphology object could be something like state. For example, if an object can have multiple morphological states, that could be a conditioning for that, or it could be whatever is passed down in the feedback connections. It seems to me that time is appropriate for both of these. Maybe not, but you don't have to separate it out; you just say time is always there. What I was getting at is that this could generally be considered as context or state, and context can be time in a sequence, a behavioral state, or an action condition state. There are different cell populations to project to layer one, so it's not just one thing. Time is a kind of state. Maybe it would help to show that we almost need to add a separate connection, which is the matrix cells. I like that idea because it fits with the concept that, instead of behavior, in the morphological models they're learning the different key frames—when an object stops moving, those would have different state variables that recover them.

So, in a nutshell, this is the proposal. Instead of requiring each learning module to have two reference frames and all the associated machinery duplicated, we would have different learning modules or columns that receive different types of input. For example, they might receive features at locations or orientations at locations for the morphology model, and changes at locations for the behavior model. The reason I first thought of this was because I was considering the feature model, or surface model—a kind of 2D map that can wrap around the surface of an object as a general feature map, which could receive features like colors and 2D movement vectors along the object's surface. There might be other types as well. 

Today's topic is about taking our jack-of-all-trades column and breaking it into two parts. Now, I'm suggesting breaking it into three parts. This is mostly for context, as I was thinking about these surface models. In the past, we discussed three types of models: morphology, behavior, and feature models. The feature model seems to be more like a two-dimensional thing that can wrap around an object. We started talking about this because of object deformation. I think we also wanted to learn a general class of morphologies, which I believe Jeff presented about a month ago. We wanted to separate out classes, separating the morphology portion from the features associated with the morphology, which is where this feature stream idea came from. I remember posting this idea on Slack, and Jeff, you mentioned it would be a lot to expect a column to have three reference frames to present those three models. So, I thought it wouldn't actually have to be in the same column. Then I wondered why the behavior model even needs to be in the same column. I hadn't fully accepted the idea that surface models and morphology models are separate things; I was still assuming they were in the same column. Now, I'm going a step further. This raises the question: where are these different modules, and how do they relate?

Last week, I did a deeper dive into the literature, particularly on where motion is processed in the visual stream. The first figure I found showed that with just one vertical electrode penetration—within the same column—they found both direction-selective cells and orientation-selective cells of different bar sizes, as well as non-orientation-selective cells. I realized that if this proposal were correct, I wouldn't expect to find motion-selective and orientation-selective cells in the same column.

Is that the same column? It doesn't look like a perpendicular projection. Yes, it is. As I looked deeper, there are all these maps of response properties in V1. I have a bunch of miscellaneous slides at the end for questions. This was the first thing I saw, but then I read further in the same paper and found something interesting: the feedforward projections to MT, which is classically thought of as an area responsible for motion processing, actually come from layer 4B specifically. They come from layer 4B and layer 6, but not from layer 2/3, which is where classical feedforward projections usually come from. The layer 3 outputs from V1 go to V2. 

Here's another visualization: retrograde labeling of neurons in V1, where dye was injected in MT, and these are the neurons in V1 that project to MT. They're not in layer 3; they're mostly in layer 4B and some in upper layer 6, with about ten times more in layer 4B than in layer 6. Most of them, about 80%, target excitatory cells in MT, independent of the blobs in V1.

Here are two diagrams from different papers showing the same anatomy: projections from spiny stellate cells in V1 going directly to MT, and in layer 4, the same—layer 4B going directly to MT. Layer 4B also projects to V2, but that's a different cell population, mostly pyramidal cells.

To get to my main point: V1 could act as a relay, directly sending motion information to MT to be modeled there, because it doesn't seem to be processed much more in V1. It doesn't get pooled into a representation in layer 2/3 and then sent forward. Instead, it enters from the magnocellular pathway into layer 4B alpha, goes up to 4B, and then projects directly to MT. There's another pathway to MT as well, from a different population of pyramidal cells that projects to the thick stripes in V2.

and on the other picture, it would be this arrow here. From there, this process actually exits in layer 2/3 and then projects to MT. Viviane, do you have a guess or thought as to why there are these extra layers in V1? Couldn't the magnocellular cells project directly to MT? Why do they have to go through this? Why don't they just go right to MT?

That's what I was trying to get at. With the second pathway through V2, physiological studies show that this input integrates more three-dimensional motion information, specifically the disparity between the two eyes. It takes disparity into account at the point in V2. Maybe V1 also does some processing to take depth or more global information into account, to get motion information in world space instead of retinal space. That was always my hypothesis. I suggested that several times in the past, that it's somehow dealing with getting to a 3D representation. Still, it doesn't explain why that processing couldn't be done by having two separate pathways coming off the thalamus—one projecting to MT and one to V1. I don't think it's important, I just wondered if you had a thought about it.

I don't have a clear answer on why this happens, or what would be happening between those two. Why does it go through layer 4 of V1 and not directly from the magnocellular cells to MT? My guess is that there's some kind of local integration of information for getting from retinal space to world space of movement. It seems like that's happening in V2. It is a total mystery to me, so if you have any insights, that would be great.

It could also be just an accident of evolution. There are mammals that don't have this dry eight V1, and they're missing something. Everything I'm showing here is primates. Evolution might just change a few cells in existing places rather than add a whole new structure. It could be just an accident of evolution, so it can't be ruled out. I'll keep going.

I saw this paper that meshed well with what I was thinking. I'll read out the marked parts for context. The paper says some neurons in extra-striate area MT are capable of signaling the global motion of complex patterns. Neurons randomly sampled from V1, on the other hand, respond only to the motion of individual oriented components. The neurons that project from V1 to MT were directionally selective and, like other V1 neurons, responded only to the motion of components of complex patterns. Maybe that's another hint of what is happening in V1—that it ensures the motion is associated with one object or something like that. The projection neurons were predominantly special, complex, and responsive to a broad range of spatial and temporal frequencies. The interpretation is that V1 acts as a clearinghouse of basic visual measurements, distributing information appropriately to higher cortical areas for specialized analysis. Potentially, V1 is a special region in that sense, in that it sorts this information. It might still do modeling there as well, but at least layer 4B might be sorting out some of the change information and just feed-forwarding it to a column in MT, where it then learns the behavior model.

This is my interpretation mapped onto this figure, plus adding some columns where purple is the behavior model pathway and blue is the morphology and feature model pathway. The parvocellular input goes into layer 4C beta, then gets pooled up into layer 2/3. Depending on whether it's blob or interblob, it goes to the thin or pale stripes in V2 and then from there to V4. For the behavior models, there's a magnocellular pathway input that goes to layer 4B and from there directly into MT.

Those cells are highly directionally selective. The idea is that MT has the behavioral model, is that right? I'll show that in a second. This pathway through V2 would extract more three-dimensional motion information that then also gets input to MT.

There's also feedback connectivity between MT and V1. Interestingly, those feedback connections mostly come from layer 6 and terminate in layer 6, but also in layer 4B, which is not usual for feedback connections. Some terminate in layer 1, but only at high eccentricities, which makes sense. If this is a behavior model, it wouldn't tell you much about an object ID in V1, I would assume.

On a high level, my rough idea is that the ventral stream could contain the feature and morphology models, starting in V1, then V2, V4, and the dorsal stream to MT could contain behavior models.

The pathway through V1 and V2 would be responsible for some motion processing, like incorporating disparity.

There's also some directionally sensitive information going to V2 and V4, but people have proposed that might be more for motion-based shape processing, like figure-ground segmentation or detecting motion borders of objects.

Lastly, this is a bit more vague. If we have feature and morphology models, it could be that morphology models are the interblobs, labeled "form" here, and the pale stripes, and feature models could be the blobs, which is why you detect the 2/3 and people assign color to that. That could be blobs and the thin stripes. But that's a bit more speculative. I have more random evidence and fun facts about visual processing. 

Would you say that the relationship between the three streams—morphology, features, behavior—is that morphology and features are horizontally at the same level while behavior is on top? I'm trying to understand how these three different kinds of columns could be a unit, or maybe we don't have to do that.

I think in Monty they would be basically the same. It's not like they would have different hierarchical relationships or anything. I was thinking that the brain separates everything in different ways, clustering different orientation preferences, direction preferences, ocular dominance columns, and so on. It separates all these different properties, so it could separate out the behavior models, the morphology models, and the feature models into blobs, but not necessarily making any of them more or less important or lower level.

Or the color blobs, down within one column—as in, we're separating out the feature and the morphology, with morphology being like the interblob area. But within a column, my understanding was that there's both, which kind of goes back. I like the separation idea, but I guess it's just a devil's advocate for it. I think blobs and interblobs happen within one column.

That is definitely an argument against it. I'm not sure it was the most reasonable thing that currently makes sense for me, because those are just in layer two and three. If you imagine having a model of the surface of an object and how color and other features are mapped onto that, I would expect those to change the activation in layer two and three in a consistent way. In implementation, we can make them separate, but somehow these are more intimately connected, in the sense that they must be laterally connected.

We can strictly follow the biology, where they must be in one, or represent that in a different way but with the same meaning. That would be a very important point right now.

That was really nice. Thanks for the nice overview. I definitely still think this is interesting. It's crazy how complex the connectivity of V1 actually is. I was curious if you had a chance to look much at how V4 and MT are connected, and how that might fit with the old ways of thinking about how behavior would map or change predictions of a morphology model—what connections could help with that? I didn't look at V4. I looked at V2 and V1 connectivity to MT. V2 has forward projections to MT, from layer three to layer three and four. The backward projections from MT to V2 are from layer three A and mostly layer six to layer one and six. So, as if MT is hierarchically above, which I think fits. I'd be curious about V4, because I think MT is often alongside, but I guess it's a hierarchy, so you can slice it many ways. I don't know.

In V1, it's a bit atypical. Feedforward comes from layer four B and layer six, not from layer two and three, and it goes into layer four and six in MT. Feedback connections come from layer three A and six, same as to V2, and then end in layer four B and six. That was the one you were talking about. That's really interesting. Could you go back a few slides where you had the flow chart of V1 going to MT or the 3D? I just don't know the anatomy very well, about how close MT is to V1, but the fact that the spiny stellates, which in my mind look basically like pyramidal cells but shorter—

So, like the pyramidal cells without the apical dendrite. I think they can still have long axons. So they can still project. I think the argument was they're exactly the same as pyramidal cells, minus the apical dendrite. That's the interpretation I adopted, which was proposed by someone. But they can still connect to MT, because I thought those were mostly for connections within a layer. No, they have long-range projections.

Another minor question: I know somebody did research into the third type of cells. I forget the main one that's K—the koniocellular. So I'm saying that right?

Did we figure out more about that? I just don't remember what we decided or found out about it. I can look it up in Google Docs. I don't think we reached any conclusions. The only extra image I have is from a review study that looked at over a hundred studies and shows this connectivity of koniocellular input to layers 1, 2, and 3. But I'm not sure I remember—wasn't there some kind of overlap with the matrix cells?

Maybe I'm misremembering, but that would match with this, I guess. One of my favorite things about this is the idea of simplifying each learning module. I think that is really nice. Oh, I forgot my main slide. It's not the main slide, but the benefits: it's a relatively simple change. The evolutionary argument is that environments usually don't have behaviors, so this might have been a capability that needed to evolve in the neocortex to model objects. It seems like the pre-processing before getting to MT columns may be a reasonable small addition that doesn't require a whole other set of connections for behavior models within a column, and would make it much easier to add that capability.

That's interesting. I think there are some mammals, maybe marsupials, that don't have motor cortex, and they don't have as fine-grained dexterity as primates. It fits with behavior in cortex being a special thing, rather than something that's represented everywhere. Everything has bodies, so surely everything needs to be able to model the behaviors of their own body, but actually a lot of animals don't, and that's why they have more simplistic behavioral models, or they're maybe all model-free.

That's interesting. I can look up again which animals that applies to. It's basically some animals that have cortex but don't have motor cortex, and it generally comes with more limited dexterity. Have you thought about how to connect motor models or behavioral models with objects?

So what I was thinking was that we could use the feedback connections, which go both to V1 and V2. I'm not sure—I would have to look up MT2. Thinking about it in terms of Monty, for example, let's say we are modeling a bunch of learning models that are learning behaviors.

Those behaviors have to be learned in the context of specific objects, and we want to associate them with those objects, but then apply them to other objects. Instead of a high-level block diagram, how would those connections be made? Is there topology associated there or not? I guess not.

I'm trying to imagine the mechanics of it; it might work out fine. I just didn't know if you had thought about it. I haven't had the time to think much more about how we would apply a behavior to a morphology. I think it would still be the same mechanism because nothing really changes in the requirements if we separate the behavior and morphology models—they're just physically maybe more apart from each other.

Since before, we assumed using backward feedback connections, basically from V2 to V1, for example. I would assume the same thing could still be happening from MT to V1.

And just some kind of colocalization in inputs, which fits with the idea that you would learn this behavior at the same time as this morphology if they were looking at the same thing, since they derive from the same map. So I suppose if I see a behavior in a new context, it might work out fine. And MT is also retinotopically organized.

If we're running out of time, I have a general proposal, and it may not be new—it's consistent with things I've said in the past. Do you want me to stop sharing? No, it doesn't matter. I can see you all; if you want to stop, that's fine, but it was useful to keep up. You might want to leave it up; doesn't matter.

Monty is not a brain simulator; it's an AI system. The challenge has been to figure out which parts of the brain we need to pay attention to and which we don't. In some ways, that's the most difficult exercise for anyone who wants to build a neural computer: to know which parts to focus on and which to ignore. I think we've been particularly good at that in the past. There are no spikes here; there's a lot of stuff we're going to ignore. Some of these details we're talking about right now are details we could be inspired by, but really don't have to emulate at all. Monty has already progressed enough that we're relying less and less on neuroscience and more on machine learning ideas. As long as we don't abandon sensorimotor learning and a repetitive, common algorithm, we can't go too wrong. So I think we shouldn't spend too much time on these details. You've proposed a major architectural change to Monty, which is to separate out learning modules specifically for behavioral learning. That's a great idea. At some point, we should think about how to implement that in Monty. I don't think looking at all these things going on in these layer fours and such makes a lot of sense.

Do you want me to stop? I can stop if you want, but I may tell you what some of my concerns are. I've always reserved MT as a pretty major thing in the brain. I always felt that it was modeling egocentric spaces, so we model both behaviors and arrangements in egocentric space. If you are going to make MT do something else, then I ask myself, do we still need to model egocentric spaces and arrangements of things in egocentric spaces? If not MT, then where? That's a concern of mine. It's a high-level thing—it's neuroscience, but it's also a big idea. That ties in with the voting question we discussed earlier: at the moment, basically all of the learning modules have access to egocentric location information.

We could continue to let all of them have that kind of information if we find it practical, and at some point it doesn't matter if it's the way the brain is doing it. When Niels and Viviane started this project, we talked about the essential elements that have to be in Monty—the core principles we cannot violate. Those include the distributed learning system and sensorimotor integration. As I said a moment ago, there's a lot of variation we could have around that, and I think you have already done an amazing job of that. Monty isn't a neural simulator; it does its own things and has its own ways of doing stuff, which is fine. In fact, it should be that way. I'm just pointing out that this is a major change you're proposing. We need to think through the implications for Monty and not worry too much about the implications elsewhere, although this may be what you're going to say, Viviane. It's actually maybe simpler. One of the reasons I like this proposal so much is that it changes behavior models from a major architectural change to a minor change. If we allow behavior models to be in separate learning modules from morphology models, then the main thing we need is just a different sensorimotor module that outputs changes and directions instead of orientations and features, and just plug that into the existing learning module. Otherwise, we would have to rewrite our learning modules to have two models at the same time and test both at the same time. So it actually makes the change simpler.

I agree. That's the beauty of it.

Just to highlight that it's not the only thing needed, but the biggest change necessary in either solution is to give learning modules stability to state-condition their models. Depending on the context, like time or state, they should expect different features at different locations, and we would need that capability either way. That's the biggest change, and it also feels like a nice feature to have for the morphology models. It gets towards some of this deformation, key frame kind of stuff. In concrete Monty terms, this would be like key frames or multiple graphs for each object that lead to the same object label. The state could also become part of the output of each learning module, something that infers and informs other learning modules on what state to expect—like a state in a behavior model informing which state in morphology to expect, and voting can happen on state to quickly inform. We talked about all this back in January. This is not really a change; it's more like an elaboration of what Monty needs. Monty does not have this right now, but before we even proposed separating out learning modules or behavioral models, we identified the need for this. It's not a new requirement; it's just a reminder that we still have to do this. No matter how we implement behaviors, we need this. If I were to implement behaviors today in Monty, these would be the three steps I would take: make a new sensorimotor module, give the learning module stability to state-condition its models, and give a learning module the ability for the behavior to apply movement to another learning module's movement input. The last part is still the most unclear in my head—how it would happen in the neocortex and in Monty—but it's about applying a behavior to a morphology, applying movement to the morphology model's reference frame.

I hope this makes the proposal more concrete for people thinking in terms of Monty. I was mostly going deeper into the literature because it was interesting, and I wanted to see if it's realistic to think about separating them physically. The thing I would look for is physiological studies of MT—what do they know that MT does? When we start getting to those TRI eight V1 connections, it's a huge morass of data and can be very confusing. Sometimes I worry about everyone on the call thinking, what are we doing? It's so complex. But we don't have to do that. Our reliance on neuroscience will go down over the years, and pretty rapidly. I looked at some MT reviews and studies. You anticipated every one of my questions here. I really had a lot of fun reading neuroscience papers again, so I did more of it than I was planning to. You had the slide right here. I read this MT review paper, and it talked about all the connectivity, but it didn't even mention feedback connections—it was all about feedforward processing. It's hard with the response properties in MT because the experiments are so simplistic; it's hard to find a paper that would actually detect that it's modeling an object behavior.

I could only find general things like orientation tuning, binocular disparity tuning, and directional selectivity. They say it's quite complex, though. There could be studies where they disable MT in an animal doing a high-level task and see what the deficit is. That's the classic example: deactivate MT, and now the person can recognize an object but can't reach for it. I always felt that's indicative of its egocentric space modeling.

I'm not sure which papers, but I think they didn't have nice figures to put on a slide.

This was the review paper: one of MT's main functions, above and beyond what is done in V1, concerns integration and segmentation. MT appears to have built-in mechanisms to deal with inappropriate merging of independently moving objects. For example, Poncy has a disparity constraint and possibly other constraints to limit integration to a particular depth.

Neurons tend to segment motion from its background. MT is involved in the computation of structure. Here, you apply MT responses and the perception of 3D cylinders—the remarkable integration of direction, speed, and disparity gradients all make a compelling case that MT is processing motion, but doing more than just computing the direction and speed of motion. That would be behaviors. I was just trying to pull up something from Joshua Tenenbaum's group—they do a lot about intuitive physics, and they definitely talk about the dorsal stream being more implicated in that. I was thinking this morning before our meeting, going back to the list of basic topics I presented a few weeks ago, and thinking, let's tackle something basic: recognize an object, go out and pick it up, and bring it into some pose. How do I do that? How do we take these ideas and implement that? That's the test in the end. We're modeling behaviors separately—okay, how do we do something basic with it?

What I think is cool is, as you're proposing, Viviane, if we were going to implement this in the grand scheme of things, it would be a pretty straightforward addition compared to some other changes you might imagine for behaviors. It might not be long before we can start testing it and seeing how it works. You can almost imagine that if we revert back at some point to behaviors meaning every column, the code for this separate one would probably be how you'd want to encapsulate it as a component in a learning module anyway. It feels unlikely that we would be wasting time by trying this and seeing how it works. It seems like a reasonable assumption to go forward on. Actually, I was just thinking we could start with one and two, while three isn't completely figured out yet. Just testing what we learn if we connect a sensorimotor module that sends changes to a learning module—does it learn a behavior? Can it recognize a behavior? Also, adding the capability for learning modules to model time or different states is generally a useful thing that we will need either way. Both of those should give us interesting results and capabilities on their own, even without being able to apply a behavior to a morphology yet.

Can I ask my question? From the very beginning of my interest in brains, I've always thought about music. Would this suggest that learning a melody—the actual melody itself—is in the behavioral model, and the instance of that melody, the key, the instrumentation, and specifics are in the morphology model, the feature model?

I guess you could think of the intervals as changes in frequency, and that would be a behavior model. The specific key it's in, those are features you're applying to it. One of the problems I had when we did the behavioral model back in January at our offsite was that I thought this could solve the melody problem—recognizing melody in different keys. But when it was in a single column, it couldn't work because a single column generally represents a certain set of frequencies or responds best to certain frequencies. I started reading the literature about the brain's representation of sound, but the problem was that by having them co-located in the same column, it was hard to apply the melody, which is a behavior, to a completely different space in the feature space, which is the actual key.

It just didn't work. I have to apply this behavior melody to some other part of the cortex, which is representing different frequencies. Now, by separating them out, I at least have the ability to apply a behavior model in the behavioral model space to any place else in the morphology model space. The mechanism for those to communicate should be flexible enough to accommodate that. I could take this melody and apply it to different keys, which are represented in different places in the auditory cortex. It's interesting to think about. We've talked a few times about the connection of displacement to invariance and song recognition. I agree it's clearer with them separate. With the melody, aren't you recognizing the intervals between the notes and not the notes themselves? That's the displacement. That's the problem, because it turns out you can readily—there are a lot of details to that. First of all, most people do know the key they learned their song in. They think they don't, but they do. When you hear Paul McCartney singing "Yesterday," the key—if you think you don't know it, you can hear that melody played in any key, and it sounds just as good as any other key. There's no preference in some sense. It is the interval. That's the trick. I think the behavioral model has the ability to capture the intervals.

To make that more concrete, it's basically like the same way a feature like diagonal bars or whatever moving is movement in the frequency space. I can't remember when we talked about it in January—it seemed like it was going to solve that problem. I don't remember how, but I think part of it was also the matrix cell stuff that you're bringing in. The matrix cell is just purely the timing, and I think I've had that all along. That's great. I think it's a pretty major addition to Monty, but not a lot of work.

It'll probably be more work than it looks like on the slide now, but I at least have a pretty concrete idea now of how to get started implementing a behavior in Monty. In summary, in January we came up with the general idea of how to model behaviors, but we assumed it was going to happen in the same learning modules as the morphology models. Now we've said it's really a separate one. This is completing the picture or going on to the next stage in the evolution of behaviors. Is that a good summary? If you're all on board with this, separating them out, then yes. Does anyone want to argue against it?

No, I think it's definitely something to pursue. Unless we have a really good reason to reverse it, which we don't, the story you've presented is pretty compelling. We needed reference frames, then this transformation, then this. Everything's been doubled. The thing that bothered me is it required a whole other set of grid cell equivalents driven by the same motion vectors but anchored differently. I thought, yes, we could do it.

That's a lot. I think this is much neater. That's a lot to ask for a column—a lot of modules to fit in. We have trouble fitting one set in there, let alone two.

Okay, cool. That's exciting. I'm happy.