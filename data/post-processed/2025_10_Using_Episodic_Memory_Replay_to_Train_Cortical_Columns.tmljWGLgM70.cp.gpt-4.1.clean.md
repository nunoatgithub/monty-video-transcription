Alright, there have been a bunch of things bothering me recently. I've been trying to put them together in some cohesive form so we can think about them. When you have a lot of things that seem crazy or don't make sense, you try to find the common theme and figure out how to approach it. That's what I've been doing, and it has to do with how multiple columns work together in the cortex. I'm focusing on the biology here, not necessarily Monty, but some of these themes will be important for Monty. This slide is just a review of what we do know. We've learned a lot about how columns work. A column can model static objects, and we've shown how the same basic column can model dynamic behaviors. Those little figures, the ones Pivine came up with for the modeling behaviors document, illustrate this. We've also learned how two columns that are co-located—meaning looking at the same point in space—can model compositional objects. The parent passes down the child ID, and at this location, you should be seeing this child object. That's all good. I assume there are no questions about that, but if there are, speak up.

Here's some things we don't understand yet. When it comes to a single column, we know how it can represent behavior models, but we haven't figured out how it can learn a behavior model. It doesn't have time to sample all the points, which we could do with a static object. It doesn't seem possible that a single column can learn behaviors, yet we know it could represent them. This suggests that multiple columns are active and participate in learning. You can't learn this with one column like you could a morphological object. That's an unresolved problem, unless someone knows a solution.

There's another problem that's been bothering me for a long time. The Thousand Brain Series says that all columns learn complete models, so a region has a bunch of models that are complete in some sense. But is it really reasonable to expect every column to train on every location, on every object? We can imagine doing that with a single column—your finger moving over the entire object as you build up a model, as we do today in Monty. But can we really expect that to happen for all columns in a region, and multiple regions? It's never felt reasonable to me, although technically it could be done. When we add learning behaviors, we know there's a problem. Even the idea that every column has to have the same model suggests something else is going on, that learning has to be shared among columns. It doesn't seem like we can expect every part of your skin to touch every part of every object. Something else is happening. There have also been observations about learning that suggest it involves the hippocampus.

For one, when we learn new models, we do it very quickly. If I stick my hand in a black box and feel an object, it doesn't take many repetitions—I can do that very quickly, so fast memories are involved. The place we know fast memory exists is the hippocampus. Another thing is that learning new models is always multimodal. If I stick my finger in the box and feel an object for the first time, I can visualize what it looks like while I'm touching it. In fact, it's almost automatic. It's hard not to do that. My visual system isn't clueless about what my hand is doing. I can learn an object by touching it and later recognize it by looking at it. How does that happen? Another, more subtle point is that when we learn new models, it's always associated with some sort of episodic memory.

If we were to do a test where everyone sticks their finger in the black box to learn some objects, you'd have a memory of that—at least an episodic memory, even if it doesn't last forever. You'd remember that you did this at a certain time of day, and you'd almost certainly remember, when learning to touch a new object, how you touched it, where you started, or where you were confused, at least for a short period. So you have episodic memories of learning events. All of this suggests the hippocampus plays an important role.

This suggests that learning new models at least involves the hippocampus, or at least starts there, or involves it if it's happening in multiple places. When you think about that, a lot of questions come up. How could this be, and so on.

Thousand Brains Project: Maybe with "starts," you mean it happens there.

JHawkins: That's true.

Thousand Brains Project: But not necessarily proceeds, or not necessarily has to proceed otherwise.

JHawkins: I don't know, I'm not sure about that. I've been trying to think of situations where I'm learning something new. It could be anything—a new word, a new cup, an arrangement of plates on the dinner table, cars at the intersection. It's hard to imagine that occurring without forming some sort of episodic memory.

Thousand Brains Project: Sure, but I guess I just mean that would happen in parallel, because...

JHawkins: For someone...

Thousand Brains Project: ...who just happens not to have a hippocampus, with enough repetition, he can still learn.

JHawkins: With enough repetition, he could learn some things, but not many. He had a very limited ability, and it took lots of repetition. It wasn't like he could learn new words or new things and just not remember that he learned them. He had trouble learning anything, and I believe they found that he could learn a few things, but it was pretty minimal, unless you have better information than I do.

Thousand Brains Project: No, I don't know about things like if they tried showing him a word every day, if that was...

JHawkins: He would...

Thousand Brains Project: If you...

JHawkins: He would meet the same people every day and wouldn't remember them from day to day.

Thousand Brains Project: Yeah. I think he met the same people for years and didn't remember them.

Every time they walked in the room, it would be like, "Who are you?" He had a really bad deficit. I think it was more like, I'd have to go back and look at the research, but it was more like, "Oh, look, we found something he did remember." But the vast majority, he just really was stuck. For many years, they thought he didn't learn anything, and then they found a few things he could learn.

I'm going to go with the assumption that, even at the most basic level, without the hippocampus, you really can't learn. There might be some exceptions, but I'm going with that.

Thousand Brains Project: Interesting, okay. But that is quite different from before, I feel.

JHawkins: In the paths of our thinking? Or the past?

Thousand Brains Project: Yeah, or how you've often described it.

JHawkins: Everything we've described, I don't think is wrong. Models have to be learned in other parts of the cortex, right? Models have to be learned in V1 and V2. All I'm saying is that it seems the hippocampus is involved. You can't— And by the way, I think all three of these problems on this page are related.

This is a set of observations—things that don't make sense to me.

It doesn't throw away anything about how we assume models are learned in a cortical column. It just says that's not the complete picture. There's other stuff going on.

That doesn't quite fit, right? But I'm glad you're—

Thousand Brains Project: Yeah.

JHawkins: —today.

Thousand Brains Project: Maybe it'd be interesting to discuss more after the next slide, but it feels like learning almost has to happen at the same time in the columns, or as in, it would—

JHawkins: I didn't say it didn't happen at the same time. I just said that.

Thousand Brains Project: Okay.

JHawkins: The hippocampus has to be involved. The hippocampus is participating in— and I say new models, not reinforcing existing models necessarily, but learning something new. A new thing you've never observed before. A new arrangement, right? I'm not saying you can't reinforce existing models without the hippocampus. I think you can. But if I give you a new word, a new mug, the first time you see a mug with a logo on it, it gets your attention. You look at it and go, "Oh, that's new." If I asked you later in the day, "Did you see a new mug today?" you'd say, "Oh yeah, I did. I was at Neil's desk, and I saw he had this interesting mug." So I'd have this episodic memory of it. It's not like I said, "Oh, yeah, I learned a new mug, but I have no idea where it was."

Episodic memory goes away at some point, and you might still have a model of the mug. I don't remember the first time I saw a mug with the Nimento logo initially. I don't remember that, but I do remember the mug, so the episodic memory fades, but the model persists.

Thousand Brains Project: I guess I just mean that, with Hebbian updates, you can imagine that each column, if we're attending to something, there are neurons becoming co-active together. There's a reasonable chance they're going to adjust themselves a bit. Over sufficiently long timescales, you could imagine— that's why I'm saying maybe after the next slide, because you were going to talk more about replay then.

JHawkins: Yeah, but I feel—

Thousand Brains Project: Even without sleep and replay, maybe that's helpful, but not necessarily entirely necessary for learning.

JHawkins: That's quite possible.

In fact, I just can't imagine a scenario where I learn something new for the first time and the hippocampus isn't involved. I tried to think of examples, but I couldn't think of one. Maybe that's because my hippocampus didn't remember it, but I'm just trying to imagine new things.

Thousand Brains Project: Or at least not structured knowledge. Maybe some motor reflex or something.

JHawkins: I'm—

Thousand Brains Project: I'll notice.

JHawkins: The kind of models we're talking about, like models of behaviors and models of morphology—those kinds of models.

Thousand Brains Project: No, I agree.

JHawkins: We're talking about cortical columns. Clearly, models have to be learned in all these cortical columns throughout the hierarchy. The learning is what we've talked about. There has to be repetition of exposure to features at locations, our standard methods here. But that's not the only thing going on. It doesn't seem to be sufficient. By my second point on this slide, we don't seem to be able to train all of the columns on all the models and all the locations just by experience. I don't have enough experience or time to do that. I can't expose every column to everything, and this multimodal learning—if I've only learned something in one modality but can recognize it in another, how is that?

Thousand Brains Project: And also how fast it is, that seems hard to imagine anywhere but in the—

JHawkins: So there are some holes here, not in the theory of the column necessarily, but in our understanding of how hierarchy works, where multiple columns interacting together work. These things have weighed heavily on me for a long time, and it got worse more recently when we started thinking about behavioral models, because it became clear that a single column can't own behavioral models. How could it possibly do that? It's just not able to look at everything, so what's going on? That was maybe the genesis of this thinking: it must involve multiple columns observing the object at once, and somehow knowledge of the behavioral learning has to be shared among columns. Not all columns can experience everything, but if they know everything, it has to be shared somehow. That's got me thinking about this, and many of these items are things I've thought about for years, but I tried taking it seriously and seeing if I could make progress. I don't have an answer. Go ahead.

Ramy Mounir: Naive question about what the circuitry looks like for the hippocampus, how it's connected. Is it just like the neocortex connected to the thalamus, and is it getting the same sensory input?

JHawkins: I don't think it does get projected from the thalamus.

I think, unless someone else knows otherwise, we've talked about cortical columns all getting sensory input. That's true for V1 and V2, but I don't think it's true for higher-up regions, and it's not true for the hippocampus. That's a good observation, Rami; we never really addressed that issue. What's the equivalent?

Ramy Mounir: The hippocampus is connected hierarchically, exactly as V2 is connected to V1.

JHawkins: It's connected hierarchically as shown in the Van Essen diagrams, Feldman-Van Essen diagrams, as I've shown here. The actual connections are not clean in the brain. The hippocampus has a different structure than the cortex. The hippocampal complex includes the entorhinal cortex, the subiculum, and the hippocampus proper. I didn't spell that out here. The architecture is reminiscent of cortical architecture, but it's different. One theory I like is that the hippocampus and the entorhinal cortex are separate organs, but they folded on top of one another, so the hippocampus is like a sheet. Imagine a sheet of cortex: when you get to the edge, it starts looking different. The entorhinal cortex is a three-layer structure, and after that is the hippocampus, which is another three-layer structure. The hippocampus is physically folded on top of the entorhinal cortex; the edge of the sheet is bent over on top of itself, and there are connections between the hippocampus and cortex. One theory was that the basic folding of a three-layer structure on top of another three-layer structure led to the evolution of the cortical column.

Ramy Mounir: Honestly.

JHawkins: It's not the same; it's very different, but it has some similarities to cortex. It's not clean or prototypical cortex at all—it's messy. But it is connected in a generic sense, like I've shown here. If you look at the Feldman-Van Essen diagrams, they show the top box in their hierarchy as the HC. It's just a block diagram, not showing the details.

Thousand Brains Project: Ian...

JHawkins: Excellent.

Thousand Brains Project: I could be wrong, but I think often the entorhinal cortex, as well as LEC, are intermediary areas for the rest of the cortex.

JHawkins: What do you mean by that?

Thousand Brains Project: I think a lot of the cortex doesn't connect directly to the hippocampus, but connects to the entorhinal cortex and LEC.

JHawkins: I think LEC in particular gets a lot of—

Thousand Brains Project: Sensory and association area inputs, so that's often considered the "what" of what's being encoded.

JHawkins: So you're saying there's a lot of convergence on the LEC from different regions?

Thousand Brains Project: I think so, yes. That's the least—at least with these diagrams.

JHawkins: There has to be convergence somewhere, because you have these separate sensory regions that all funnel down into one entorhinal cortex and one hippocampus. In some of the higher regions, there's a lot of convergence as well, as suggested in this toy diagram.

Thousand Brains Project: Exactly, so that would be the block just below the hippocampus.

JHawkins: So you could—

Thousand Brains Project: Rather than all-to-all.

JHawkins: It could have been entorhinal cortex there. There's a lot of weirdness. If you look at the actual Feldman-Van Essen diagrams, there are all kinds of regions converging and diverging, and it's messy.

But none of that scares us; we just need general principles for how they communicate. The exception is the entorhinal cortex and hippocampus, because they look different and perform differently. I have a theory of how the hippocampus is able to learn very rapidly using silent synapses. That's the one place known to have lots of silent synapses, and it's the one place that can learn very quickly. Synapses can be grown quickly—growth has been shown to occur in about a minute—but that's not enough for the kind of instantaneous learning we can do with the hippocampus. That may be rare anyway. Any more discussion on this picture before I jump into a bunch of text to resolve how this might work?

Thousand Brains Project: No, I just thought I'd mention that I think, in general, the consensus view is that the medial entorhinal cortex is where grid cells are and where the "where" is represented going into the hippocampus, and lateral entorhinal cortex is generally the "what." Sorry, I meant—I don't know if—

JHawkins: Everywhere.

Thousand Brains Project: Grid cell is where? And then lateral entorhinal cortex is what? Huh.

JHawkins: So, I think what you're saying is there's a general belief that there are two parallel "what" and "where" pathways in the different sensory modalities, and that seems to be preserved in the entorhinal cortex as well.

Ramy Mounir: Is that—

JHawkins: And—

Ramy Mounir: Analogous to layer 4 and layer 6 in the cortical column, the "what" and "where"?

JHawkins: Huh.

Ramy Mounir: Blend.

JHawkins: No, they're more like—

Thousand Brains Project: Or layer 4 and 6?

JHawkins: The "what" and "where" pathways are just two separate regions.

Thousand Brains Project: But you're saying within a column, Rami?

Ramy Mounir: Yes, within a column, so in the—

JHawkins: I know.

Ramy Mounir: Hippocampal complex.

Thousand Brains Project: I feel like the input from lateral entorhinal cortex is the input to L4. L6 is medial entorhinal cortex, the "where" for representation, and then L4 is the hippocampus, the actual place cells that are binding the two together.

Ramy Mounir: Okay.

Thousand Brains Project: Yes, maybe.

Ramy Mounir: I was just wondering about the associations between the "what" and "where" in the entorhinal cortex.

Thousand Brains Project: Yes.

Ramy Mounir: It would be similar.

Thousand Brains Project: Yes, I feel like the "what" and "where" here is more similar to the "what" and "where" within our columns, rather than the "what" and "where" in the ventral versus the dorsal streams.

JHawkins: Oh, I thought it was more like the ventral versus the dorsal stream.

Ramy Mounir: No, not at that.

Thousand Brains Project: I guess it's similar in that it's a macroscopic anatomical thing, but the "where" generally isn't about reference frames, I feel. It's more egocentric versus allocentric.

JHawkins: I always thought that the two divisions of the anterior cortex were more like dorsal and ventral streams. I think that's what most people think.

Thousand Brains Project: Maybe it is, but what was...

JHawkins: Where are the Layer 6 location and the Layer 4 feature? You can think of those as "what" and "where," but that's a different "what" and "where." That's just saying this feature is at some location, whereas, as Neil just said, the "what" and "where" pathways are more like egocentric or allocentric models.

Thousand Brains Project: Yeah, in the dorsal and ventral stream, but I feel like with the hippocampal complex, the hippocampus is learning a structured representation of what is the sensory experience at a location. That feels like what our columns are doing.

JHawkins: I see.

Ramy Mounir: The division.

JHawkins: Between medial and lateral, the lateral and the medial entorhinal cortex are more like dorsal and ventral. I don't think—because they both have grid cells, right? Don't grid cells exist in both the medial and...

Thousand Brains Project: I'm not sure, I thought they were.

JHawkins: They wouldn't. So here's an interesting thought. The classic grid cells representing locations in a rat's environment would only be on one side, but there would be equivalent grid cells. In the same way, you might have grid cells on both sides, but they wouldn't see them unless they knew what to look for, because they're not going to be modeling the same thing, just like the "what" and "where" pathways don't model the same thing. I think we can put this aside for the moment, perhaps. I'm not going to suggest anything as detailed as what we're talking about here. I'm going to propose some things to talk about that are more very large block-and-tackle ideas. What could be going on here? In the end, it's consistent with a lot of things people think about the hippocampus already. I just have it in a different direction, and it seems more clear to me what might be happening. What I'm about to present, I'm certainly not 100% certain about. But at least it's a way I've started thinking about this problem: how columns can learn when they're not exposed to stuff, and how the hippocampus comes into play.

Anyone want to discuss any more on this page before I do that?

Viviane Clay: Maybe to throw out one alternative view on the second problem. I'm not sure if it's as much supported with anatomical evidence, but an alternative to all columns needing to learn all the different models so we can recognize an object with any patch on the skin could be that the movement information of whatever patch on the skin is currently sensing is somehow broadcast or pooled in a common place that has that model. The movement information is universal, and wherever that model exists, even if it is in visual cortex, it should be able to recognize the object based on shape, just from the movement vectors.

JHawkins: You'd have to pair the movement vector with the sense feature at that location, right? So you'd have to broadcast—I'm agreeing.

Viviane Clay: Yeah, I guess movement and orientations, maybe.

JHawkins: You'd have to also broadcast the features that are detected there, right?

Viviane Clay: If you learn something with vision, and then you're inferring with touch, or the other way around, you can't really share the features. I'm just saying, the shape...

JHawkins: So it should be the morphology.

Viviane Clay: Yeah.

JHawkins: So the orientation, it would be the location and the orientation that have to be broadcast.

Viviane Clay: Yeah, I agree. Broadcast or learning associations between movement of a patch of skin, movement of the eye, or having a common place higher up in the hierarchy that pulls information off that and has a more multimodal or general morphology model.

JHawkins: These are all things that I thought about too, and they're consistent with what I'm about to present, so it's not really an alternate view. When I say learning is shared, it suggests that learning is shared somehow. That means that somehow things are broadcast. That's what I mean by that. I think it's the same thing you're saying, that a column...

Viviane Clay: Yeah, I guess what I'm saying is, if we have that, then we wouldn't need—I don't need a model of a cup in whatever patch my wrist connects to in the neocortex to recognize the cup with my wrist.

JHawkins: You'd have to have a model to recognize it there.

Viviane Clay: Not if the movement of that patch is broadcast to a different column that's learned in Europe.

JHawkins: That's clearly the case, for example, when we learn an object with touch, and then I can ask you to identify it for vision. I'm not proposing that the visual columns actually formed a model of the cup. I'm not saying V1, after I touch an object with my finger in the black box, has models of it, but somehow I have a visual imagination of it, and I'm able to imagine what it's going to look like.

Viviane Clay: Yeah.

JHawkins: I'm not disagreeing with anything you said there.

Viviane Clay: I think it must be part of the solution, but I wouldn't know which connections would actually work for that kind of sharing of information. Maybe you go ahead with your next slide first.

JHawkins: That's the problem. On the second point, for a while, I used to think about not all columns getting information. I've shown these blue arrows; those represent the columns that get information. The other columns are not getting anything, because if it's vision, this is V1, those columns, the center-surround receptive fields are quiet.

And so, one thing I thought was that even in V1, training could spread horizontally within V1. Take the blue arrow on the right: the columns next to it are not getting input, so could it be that if I'm learning something, my neighbors should learn too? We discussed a paper showing that activity in a column can spread to nearby columns that are not being driven. I considered the possibility of horizontal spread within a region, but it didn't seem sufficient. It may still be happening, but it's not enough to address all these issues.

There are multiple possible ways that columns can learn something without being directly exposed to it.

Viviane Clay: I wasn't even referring to learning; I was thinking of inference. We could do inference in a column that isn't actually getting sensory input, just based on motor information from a different sensor.

Thousand Brains Project: But you'd have to associate the input feature across modalities, which is where the hippocampus or some association area would be useful. It feels hard to do that in the primary cortex.

Viviane Clay: Higher up in the hierarchy, yes, but we would have to associate movement and orientation information.

JHawkins: But not all features, right? You can't learn—

Viviane Clay: Not the features.

JHawkins: I can see the mug with the logo, but I can't feel the logo with the mug.

Thousand Brains Project: You also need to somehow, when you feel something like a hard edge, that kind of awakens something visually in you. It feels like an association area, whether that's the hippocampus or others, would be the best place to have that learned.

JHawkins: These are the kinds of conversations I have in my own head. They're all fuzzy—yes, it has to do this, it has to do that, but how? What is going on? That's what bothered me. We need concrete mechanisms to solve these. Let me go on to the next slide. The next slide is just a bunch of text. By organizing them in numbered paragraphs, we can talk about one at a time. I may want to come back and look at this picture of the hierarchy as we're doing this, or just try to keep this little picture in the bottom left in your head. Again, this is not a concrete proposal, but it has some interesting ideas. This is all hypothetical. Let's start with number one. I've already stated this, but I'll state it again: the hippocampus is always learning. It never really stops. When you're awake, it's always getting information. It's going to play a role in learning all new compositional models. Anytime you're learning new structure in the world, the hippocampus is going to be part of that, at least in the beginning.

Then I start at number two.

I have this vision in my head. This paragraph tries to describe it. You can imagine the world as things composed of things. These models have varying levels of depth; some things are structured in many layers, and some are shallower. But there's always a limit to our knowledge, a boundary to what we've modeled. There is always more structure out there that we haven't learned to model yet.

There's this boundary to our knowledge: I've learned all these models of structured things, but perhaps there's new structure built on the previous things I've learned that I haven't learned yet. I'm studying and will learn those things. The brain is continually trying to extend its models of the world by forming new compositional structures. Some of these will last, and some won't. Some will be more permanent, and some less so. It's heavier on learning—if I see an arrangement of cars in the intersection, I might learn that, but if it doesn't happen again, I can't learn that particular arrangement. But if I see an arrangement of letters in a new word that repeats, I can learn that because it statistically repeats. The brain is trying to continually deepen its hierarchical structure of composition, always forming new compositions and seeing if they stick, if they repeat. It's constantly trying to learn. 

Now, moving on to point number three, and this is a question in number two. I think the hippocampus is a staging area for attempting to extend its models.

It needs to be able to build new models very quickly, because you may only be exposed to them briefly. It's at the top of the hierarchy because, presumably, everything that's coming in—let me just read it here—it's a staging area to extend models. It's where new compositional arrangements are first learned. This is important: it's not just for very complex things. It can be new arrangements of simple objects, such as learning a new word with new letters. If I see a new word with a different arrangement of letters, I will focus on and attend to that. Or a new shape. But it's also new arrangements of complex objects, such as the mug on the logo or cars at an intersection.

Anytime it sees a new arrangement of anything it's learned before, the hippocampus gets involved and tries to learn it. It's a staging ground because it tries to learn it, but it may not persist. Like the intersection, it may go away, or the object on the table may not happen again.

It's okay—I'm going to try to learn this and see if it sticks around for a while. To understand how we can deal with little objects like letters and big objects like cars, there's going to be an attentional mechanism, and I'll talk about attention in a moment. Essentially, the hippocampus is saying, here's a new arrangement of things I haven't seen before. Let's attend to these items one at a time and see if we can learn a new compositional structure.

The hippocampus, number 4, is fast, so it can learn a new arrangement in a single presentation. How does it learn a new arrangement? You might think it works like our current columns, like a reference frame with features. The entorhinal grid cells and place cells are reminiscent of that, but we know the hippocampus forms episodic memories, which are different. We don't assume that's happening in cortical columns. Episodic memories are memories of where and when things occurred, and the order in which they happened.

It records the order in which features were attended to, not just their location. If I were putting my finger in the black box and feeling a new shape, trying to learn it with my finger, after I do that, I would remember the order in which I touched the object. It's not that I have no memory of where my finger was, but I would have this episodic memory: first I felt this, then I felt that, then I realized it was rotated, and so on. The hippocampus can recall specific sequences of observations—the order in which they were experienced. In a cortical column, that doesn't happen. The order in which things are learned is not important, not saved, not remembered, and it doesn't matter. We use the reference frame to make that the case.

Ramy Mounir: Unless it's a behavior, like a sequence where the order of the sensory input was important to form that sequence.

JHawkins: That's correct, you're right. I wasn't thinking about behavior.

Ramy Mounir: It's only for objects; learning objects, it doesn't.

JHawkins: Absolutely right, that's a good point.

In the behavioral case, it's still a little different than in the hippocampus, because episodic memories are placed in time and space. It goes beyond just the sequence—it's about where I was, what time of day, and these kinds of things.

Thousand Brains Project: It's interesting to think about how time is represented differently in the hippocampus. Behaviors like a song or a sequence of movements feel different. It feels like time and...

JHawkins: It's...

Thousand Brains Project: The hippocampus feels much more biographical, like when did this happen relative to me, and all that. What we think would be learned in columns feels more like a pattern that plays out and repeats.

JHawkins: The sequence in a column is a high-order sequence; it is the structure. In the hippocampus, as you said, it's not really representing the structure of things, just the order in which you did them. There's nothing in the world that says I should touch an object in this order, but that's the way I did it, so that's what it remembers.

Ramy Mounir: So the hippocampus is more egocentric, and the cortical column is more object-centric, with respect to the actual structure of the sequence we're learning, but not really with respect to me. It's not an egocentric representation of what happened.

JHawkins: I'm not sure—I'm sorry I used that word, Rami. It's more like a melody is a structure in the world. It's something that exists in the world through time, but the order in which I touch an object is not part of the world. I'm not sure it's egocentric; it's just a history.

Thousand Brains Project: Yeah, it feels like there's a sense of when something happened. Of course, you can have an episodic memory of a sequence as well.

Ramy Mounir: But it feels very...

Thousand Brains Project: Different somehow. It's, "Oh yeah, that happened at that time. I remember that event." It's like a point in time, which is very different from just, "I'm at this point in the song," or whatever. Like you say, Jeff, that's just an object or structure that exists in the world, and you can be at some point in that structure.

JHawkins: Think about it this way. We all hear melodies and learn the same sequence of the melody. No one's going to learn it differently; we'll all have the exact same sequence because it's a real thing in the world. If each of us were to feel the object with our hand in the black box, we would move our fingers differently and remember how we moved them, but it doesn't really matter. Yours would be different than mine. It's not part of the world; it's just a memory of what I did. It's not any kind of structure in the world—just a history of my behavior.

Alright, going on to number 5. What was striking to me was how I'm able to learn a new word based on very low-level objects like letters, but at the same time, I can learn a new phrase based on mid-level objects like words. I could even learn a whole new speech, a sequence of words. I can append my models and add new models at different levels of composition in the world.

How is it that I'm learning a new model of an intersection between cars and bicycles, and a moment later, I'm learning a new shape, which is just a bunch of edges? If the hippocampus is going to be involved in all these things, there has to be a way of narrowing down and saying, "Let's all talk about this right now, because I'm only interested in this thing." This is what attention is. I'm going to define attention here, or at least a part of it.

I think attention is not just about focusing on a part of the retina or visual scene. Attention is about specifying a region of space we're interested in—an area in real space relative to the body. It's not simply restricting input from parts of the retina; it's about attending to a location in space, and any column receiving input from that space is relevant. It doesn't matter if the column is visual, touch, or auditory—anyone sensing in that space is included.

That's the definition of attention: only columns sensing within that region get to vote. Other columns may receive input, but they don't participate. This relates to your work, Rami, on identifying child objects in a compositional structure. I'm not suggesting changes to your work, but I think this is what's happening in the brain—there's always this idea of attention.

If you're sensing the world, I don't know who's in charge of specifying the region of interest. It could be the hippocampus or something else, but usually, it's triggered by something unexpected—an area where we didn't expect to see something. That's where attention is directed, as it's a region where learning can occur due to an unexpected arrangement or item. I don't know who specifies it, but someone has to define a region of interest.

That region is defined in space, and any column that can get information from that space is valuable. If I define a region to the left of my gaze, my eyes might move to attend to it, bringing more columns into that region of attention. If I feel something unusual on the back of my hand, I might rotate my hand to feel it with my finger or look at it. By attending to a space on the table, I move my eyes and fingers there, gathering all possible input from that region. What reaches the hippocampus is restricted to what's in that region; we don't infer anything outside it. That is what rises to the top of the hierarchy.

The input to the hippocampus is the largest object recognized within the attended region. If nothing in the region is recognized as a previously learned object, the structure is unknown, and voting doesn't reach consensus. The attended region then narrows until something is recognized. For example, if I don't recognize a word, I zoom in on the letters, and if I see a letter I've never seen before, I focus on its features. At an intersection, if a car is unusual, I attend to it, and if it doesn't match a known car, I zoom in further until I find something recognizable and start building structure again. 

To summarize, the hippocampus tries to learn new structure. When something doesn't fit a model, the cortex and hippocampus attend to that region to see what's there. If it's recognized, it's a new item in a location; if not, attention narrows further until something is recognized, and compositional structure can be built again.

Thousand Brains Project: I like the idea of attention as a heuristic.

JHawkins: I think that's what we do. I've observed myself doing this.

Tristan Slominski: I have a question. A counterexample to the phrase "region of space" is if I'm listening to you, closing my eyes and looking away. How does listening to a podcast while doing other things fit the idea of attending to a region of space?

JHawkins: You're right, Tristan. I don't know.

Thousand Brains Project: That's the cocktail party problem.

JHawkins: You could focus on a region of space where the input is coming from.

Thousand Brains Project: That's why it's difficult for people with hearing aids at parties or in crowds.

JHawkins: Oh, I see.

Thousand Brains Project: It's harder to localize sound.

JHawkins: And—

Thousand Brains Project: They can't attend to it in the same way. With headphones, you're probably just attending to your head, since there's no other sound.

Viviane Clay: This seems more about attending to something in conceptual space versus having trouble getting the auditory input.

Thousand Brains Project: There's no issue with getting auditory input, but if I'm multitasking and focused on something in front of me, whatever I'm hearing, I stop hearing at any significant level. It feels like you can't attend away from it.

I think there are two things we're discussing here, and maybe Vivian just touched on it. One is Neil's point—you made a great point. When we're listening, we attend to a specific point in space, and we're trying to restrict input from that point. That's correct. But there's another issue: often, when I'm listening to a podcast or something at a fast speed, I might not understand something someone said. So I back up, slow it down, and listen again. If it's still too fast, I slow it down further until I can hear what was said. I think that's a different type of issue—more of what Tristan or maybe Vivian was talking about. There was a series of patterns that didn't fit my models of temporal patterns. It's not a spatial pattern, it's a temporal one, but it didn't fit. It was a model that didn't work. It didn't sound like a word or phrase I knew, so I had to go back and attend to the components slowly to make sure I understood, or maybe I learned something new.

I feel like both of those are still about the sensory input and getting the right sensory input. At least what I was thinking of when Tristan brought this up was actually being in conceptual space. If I'm driving and listening to a podcast, I'm obviously paying enough attention to drive, but my brain isn't really registering what I'm seeing. My brain is having a bunch of mental imagery and moving through the conceptual space of what the person is talking about in the podcast. The words themselves elicit a space in my head that I'm moving through, imagining, and paying attention to.

I see what you're saying. That makes a lot of sense. The point is, we don't really have a good understanding of conceptual space yet. We talk about it a lot. It's going to be based on grid cells, but we don't have a deep understanding of what it is. If we accept that it exists, then I see your point—we could have a mismatch in conceptual space.

There's also the example of two people talking at the same time. It feels like I can filter and actively switch between one or the other, paying attention to one and completely disregarding everything the other is saying. There's no location there; it feels more like model-based attention. I want to listen to something and not attend to everything else happening at the same time.

Vivian brought up conceptual space, which we talk about a lot but don't understand deeply. We say it's going to be built on grid cells. My argument is that specifying a region of space could include conceptual space, which I think is what Vivian was suggesting. I can't think about it too clearly—it's much easier for me to think about physical space. That's how I'm thinking about it right now. But it should apply to conceptual spaces too. I think that's Vivian's point. Even if we don't understand it completely right now, it's okay; we can keep focusing on physical space, because in theory, whatever we do for physical space will work for conceptual space.

I'm trying to think about something you want to attend to that is itself moving around in space. For example, a dog walks into your apartment. You're going to continue attending to where that dog is in your physical space. Something is informing that—something is helping you update the location in space that you want to attend to. It's still spatially bound to begin with, because the animal is moving around in your house.

I don't think these are counterexamples. The dog walks into your house—maybe you weren't expecting that. That's like a car showing up at an intersection. You can attend to it, but you may not attend to it forever. Eventually, you give up, go back to work, and stop attending to the dog; you don't see where it goes. The dog walking into your house is very similar to a car at an intersection. It may or may not be repeating. Maybe the dog walks in every day using the same pattern, and you can learn that as a behavior. Otherwise, it might be random, and you can't learn it.

I'm just thinking about what initializes that location in space that we want to attend to. What defines that?

Like a model-free sort of stimulus.

It feels like a mixture of model-free and model-based. As soon as anything is moving in your apartment, you probably get model-free attention, but then you recognize it, and then you're like, oh, that's...

Not necessarily.

There might be something moving in my apartment all the time, and it's always there, and then I don't notice it anymore.

Yeah, but then that would probably be model-based. I'm just saying it's probably a mixture, but it almost always has to start with some amount of model-free.

I think when we say model-free...

I would say my model doesn't predict it. You could call that model-free, or you could say, I could learn my dog walks in the door every time I pick up my coffee cup and goes to sit in the corner. At some point, I'll stop noticing that because it becomes a learned behavior. I can give examples of things like that.

It's... is it model-free? It's basically outside of an existing model, but it could become part of a model. How about phrasing it that way? It's not purely—well, you could think of it in different ways. You could say something unexpected happened, but it's only unexpected because it wasn't incorporated in the model. Remember the example I sometimes use? The kid's in a classroom, the teacher says, "Focus on the board," and then someone opens the door on the side of the room, and everybody turns their head to it.

If that door opened all the time in a consistent, persistent way, people would stop turning their head to it. So it's not purely model-free. It's unexpected in my current model, but it could become part of a model if it was repeated consistently. It would suggest that it's not completely based outside of models. It can become part of a model, and then I won't notice it anymore.

Never.

Viviane Clay: Yeah, I think that makes sense. I think Niels is having headphone issues.

JHawkins: Who is he? Niels, can you hear us? I've got all my people turned off here.

Viviane Clay: Yeah, I guess in terms of conceptual space, coming back to that while we're waiting on Niels, it almost seems like it's also using the same kind of hippocampus mechanism of quickly laying down points. If I'm listening to someone talk about Monty, and sensor modules, and learning modules, and how they connect, it's like I'm forming a quick model of putting these things in a relative arrangement, connecting them with lines or something in my head. Then I can also focus my attention on different parts of that kind of conceptual space, even though learning modules and sensor modules are not really something that physically exists, or that I've ever sensed with my sensors.

JHawkins: Yeah.

Viviane Clay: But still, conceptually, I'm putting them in some kind of space, and then attending to different elements as you talk about them or describe them.

JHawkins: And you typically have an episodic memory of that sequence of thoughts.

Viviane Clay: Yeah.

JHawkins: Just like we'll say, "Didn't we just talk about this a minute ago? We were bringing up this, or Romney said something," whatever.

Viviane Clay: Actually, when you listen to stuff while driving or walking somewhere, sometimes you associate the information with the place you were at when you heard about it. When you go back to that place, you're like, "Oh, this is where we talked about that," or...

JHawkins: That is true. I see it all the time. I can't remember faces or names, but if I see the person in the right context, the name will come to me. If I see the image out of that context, I just don't get it.

Thousand Brains Project: I guess clearly the hippocampus has evolved to specialize in remembering environments and where we are in an environment, which is, I think, the whole concept of a memory palace. It's much easier to remember large amounts of information if you place them in a hypothetical physical space.

JHawkins: But it's interesting here, because I think about the hippocampus. Neural tissue doesn't know anything, right? It's just dealing with locations and regions, and it doesn't know if those are physical spaces or conceptual spaces. If it's just grid cells, it doesn't know that. So it seems like the mechanism should work universally. I'm just guessing.

Thousand Brains Project: If I had to guess, when you're learning something in conceptual space, the grid cells in your entorhinal cortex aren't moving that much, so there's more interference.

JHawkins: Yeah.

Thousand Brains Project: When you're moving through physical space, or even a virtual or mental physical space, like an environment or a memory palace, then the grid cell code is going to be more distinct when you're laying down information, and there's less interference.

JHawkins: Okay, maybe we should just try to keep going here. I think that's a good observation. I'm trying to paint this picture in your head. The hippocampus is basically somebody saying, "There are things here we don't anticipate. Let's attend to them." The hippocampus is laying down a memory of these observations. Those episodic memories we were just talking about are located in some place. I remember where I saw this thing, or what letter in the new word was the first letter, the second letter, the third letter, and so on. Point number six says, while we're awake, the hippocampus learns these episodic memories. Episodic memory, briefly, is a sequence of attended spatial regions and the objects that were observed in those spatial regions. That's what we were just describing in the previous point. You're just attending to regions in space, whether it's conceptual space or physical space—same idea. But imagine physical space; it's a little bit easier.

So you're imagining you attended to these things in space, the order in which you observed them, and what was observed at each location. That's what episodic memory is—it's a recall of what you did and what you observed. It's well-known in belief theory, but I don't know the details. It's certain that during sleep, memory consolidation occurs; if you don't get the right sleep, you don't remember things. There's a lot of research about how, during sleep, your memories are consolidated. There's also a lot of evidence that some episodic memories are replayed during sleep cycles, and sometimes they're replayed very quickly—not in real time, but very rapidly. You can see neurons recalling sequences of events from episodic memories, like in a rat, recalling what it did very rapidly.

During sleep, no columns are getting sensory input. That's the rule—they're not getting sensory input. As episodic memories are replayed, the hippocampus could enforce the region and object pairing from the sequences learned in episodic memory. It can enforce that over all cortical columns. I have a lot of questions and don't fully understand this, but the idea is: let's replay this thing I heard and broadcast it to everybody. Everyone pretends they're attending to this region and observed some object. For some cortical columns, this won't make sense, but for others, it will. You're telling everyone to do the same thing—to relearn what was done earlier. You're exposed to this. I don't know how often this occurs or how rapidly, but this could be a way for cortical columns to learn models they never directly experienced. You're not getting sensory input; you're told to pretend you're looking at this region and saw this, then moved to another region and saw something else. This is the first time I've had a sense of how we could form memories over broad areas of the cortex that never actually experienced the behavior or input directly.

It's not inconsistent with existing hippocampal literature, but it's more detailed and specific, and it makes more sense in this context.

Thousand Brains Project: I like this approach. Maybe you know, Jeff, there are computational theories about the role of dreaming and hippocampal replay, focusing on how neural networks and deep learning rely on independent and identically distributed samples. Dreaming might be a way of randomly sampling from an infinite buffer of experience to avoid catastrophic forgetting.

JHawkins: This is interesting, because I'm not aware of this.

Thousand Brains Project: Okay.

JHawkins: It seems to be the opposite of what I'm suggesting.

Thousand Brains Project: Yeah.

JHawkins: You don't want IID here; you want the exact detail of learning.

Thousand Brains Project: Or at least, but it doesn't actually work as a solution unless you literally sample from an infinite distribution. It also doesn't explain how online learning happens during the day. You don't have to sleep to learn something; you can learn during the day and remember it better after sleeping. This feels much closer to reality. The solution to continual learning isn't going to lie in hippocampal replay or stable learning; it's just a way for columns that take a while to learn, or didn't get the full sensory input, to be more exposed.

JHawkins: We don't have the problem biology has, where learning is slow in synapses. Monty can do quick learning, but it suggests a way that we could be training lots of columns that aren't getting input at this time. I've even asked: what if I'm awake?

Could this mechanism still be at play? When I'm awake, some columns get direct input from sensors or other columns, driven by sensory data, but others do not.

Could this same broadcast mechanism work while you're awake on columns that aren't being driven by anything? When I'm talking to this group or listening to someone else, I often close my eyes. I feel bad about it because it's rude, but it helps me think. Maybe it's a way of turning off input to parts of my cortex so they can be involved in this attentional mechanism. I could be training things in my visual cortex because I'm not looking at anything. My point is, I've described something that would work during sleep, but Monty doesn't need to sleep. It can run quickly. The basic sharing could occur during wake periods too, but not for the entire cortex—only for parts not receiving input at that time. I don't know about...

Viviane Clay: Or inference, then, to generalize inference?

JHawkins: No, it would be more about learning. Again, I put my finger in the black box and feel an object. In some sense, I visualize what I'm feeling. I have an image in my head of what it looks like as I'm touching it.

And so it could be that I'm actually training my visual cortex—imagine my visual cortex has no input coming in, or I'm not attending to it. I'm attending to a region in space, but potentially I could be training my visual cortex on that same thing at that moment, from a sort of top-down mechanism. Obviously, I can only train on the morphology, but all I'm pointing out is that this mechanism I propose, that somehow works during sleep, might actually be partially operating during awake periods too. That's all I'm saying—the training records.

Viviane Clay: Do you know what kind of connectivity people propose to be used for the replay?

JHawkins: Nope, I know nothing about it. All I know is that they look at either grid cells or place cells, and they see a sequence of patterns that they know was experienced by the animal earlier.

Thousand Brains Project: Yeah, and then, again, it could be wrong, but I think from what I was reminding myself of earlier, any reactivation of cortex would probably happen via lateral entorhinal cortex and medial entorhinal cortex.

JHawkins: Projecting back to the cortex.

Thousand Brains Project: Projecting back to cortex. That's like the way station.

JHawkins: This idea suggests that there needs to be a broadcast signal that represents a region of space that is the attended region, and this has to be in some sort of universal reference frame. Then individual columns can say, "I'm in that zone or not, I'm in that region or not." I don't know how that would work. I haven't tried to walk through that at all. I don't know.

Scott Knudstrup: Yeah, I don't know about the replay stuff, but a big part of sleep, at least during a slow wave, if I'm not mistaken—which is distinct from replay—that's going to have to be something that happens during REM, probably, because otherwise the brain is too synchronized.

His synaptic renormalization: most of your synapses strengthen throughout the day, just during waking life. At some point, you've got to bring those synapses back down in terms of strength. Otherwise, your brain will just be teasing all the time. The idea with sleep is that you take the small handful of synapses that have strengthened the most and you strengthen them, and everything below that threshold gets weakened. You've got this mechanism by which the most important things get committed, and everything below that, you basically make room for more memories and synapses to strengthen the next day.

JHawkins: Is this empirical results, or are these theoretical ideas?

Scott Knudstrup: I think it's both.

JHawkins: The renormalization doesn't make sense to me, because I think about synapses, for the most part, in cortical synapses—they have very little weight. They're binary more than anything else. Either they exist or they don't exist, but the idea that there's any degree of fidelity in the weight is really minimal or non-existent. There are a lot of theories that people speculate about neural networks, including deep learning networks, where they assign synaptic weights to some level of precision, which just doesn't exist in real brains. The idea that you're renormalizing weights seems odd to me. It doesn't fit my understanding of how synapses really work in the cortex.

So that's why I was asking, is this proven empirical results, or is this just people—

Scott Knudstrup: Yeah, that's a good question. I'll post about it later, because it's been a while. I'm seeing that there's some empirical stuff now, but obviously I can't—

JHawkins: It's just—

Scott Knudstrup: In the short term.

JHawkins: I think so much of what people believe about synapses is wrong.

Scott Knudstrup: And—

JHawkins: It's confusing, too, because if you look at other parts of the brain and other animals, then there really is the strengthening and weakening of synapses. You go to the Aplysia sea snail, right? They can see that. But in cortex, it doesn't look like that. In Aplysia, all the synapses are there; you just change their weights a bit. But in cortex, we know that synapses are forming and unforming all the time, and the whole theory about sparse representations suggests that a synaptic weight is really not an issue—it's not something we need to think about. They're more binary. When we look at literature on this stuff, you have to be very careful about what they talk about, what animals, what is the paradigm, what are the assumptions.

Just bear that in mind when you're reading through that stuff.

I should at least mention the last point here. I didn't think about this theory at all in terms of behavioral models.

I want to, but I didn't have time. So I don't really know, because this helps me see a path as a solution: all columns can't experience the same thing, but they all need to learn models. How does that happen? This gives me a path to do that, but it's not clear how it would work for behavior models. Maybe it's easy; I just haven't thought about it.

I just throw that out there—it's a total void in my thinking at the moment. But hopefully, if you think about it, maybe this basic mechanism would also suggest how a column can learn complete behavioral models, even though it doesn't experience everything. It doesn't jump right out at me, though.

That's something to think about.

Thousand Brains Project: Yeah, that's it. I think it'd be interesting to think through how the replay actually works.

JHawkins: I don't think—mining, we can do all kinds of fun stuff in mining that doesn't have to work like this at all. The idea that you might broadcast or train all columns that aren't currently involved in something, have them all learn the same thing in some sort of basic form, and use attentional regions, could be important. It could be important for the work that Rami's trying to do now, too.

As much as I understand it.

I just want to make sure—I don't think we have to go and have Monty sleep. Doesn't have to do anything like that.

Thousand Brains Project: Yeah, it can be a bit more like dolphins, maybe.

JHawkins: Switch off.

Thousand Brains Project: Half their brain at a time.

JHawkins: And—

Thousand Brains Project: That half-brain sleeps.

JHawkins: While the other half is awake.

Thousand Brains Project: And learns.

JHawkins: It's interesting. I remember reading it now.

That's pretty funny. Does that mean—I don't want that.

Thousand Brains Project: So Monty can asynchronously...

JHawkins: We can do it, we can do whatever we want.

Thousand Brains Project: We're just coffee muddles over.

JHawkins: Yeah, we might have copied Miles, and Monty could do this stuff in 100 milliseconds, then get back to the main event. I've never had a comfortable feeling about how we can train models without them experiencing something.

Thousand Brains Project: Yes, we can copy it, but I wanted to understand how brains do this.

JHawkins: That can help us think about it. Unless it works across modalities, as Vivian was saying earlier, obviously I can't learn complete models across modalities, but I can learn morphology models across modalities. Maybe that's something to think about, too.

Thousand Brains Project: For what it's worth, I was just looking at V1. At least from some quick Googling, both MEC and LEC have pretty minimal projections there. They exist, but only about 0.5% of the projections to V1 come from MEC and LEC.

JHawkins: Doesn't have any projections.

Thousand Brains Project: From Entorhinal Cortex would be one, but...

JHawkins: I'm surprised they have any.

Thousand Brains Project: I guess we want to project back in order to reawaken, but this suggests maybe that this could be a way to help learning, particularly in higher-level cortical regions. I think higher-level cortical regions get more projections from Entorhinal cortex, but something like V1, like you've often said in the past, Jeff, will learn models, but it just takes a long time.

JHawkins: Yeah.

Thousand Brains Project: Maybe that doesn't need hippocampus, but just needs a lot of time.

JHawkins: It might be hippocampus, but here's another...

Thousand Brains Project: My suggestion is, how would it even get help from the hippocampus based on that connectivity?

JHawkins: I can imagine—I didn't know if I needed that connectivity. I was thinking more like the cortex, the hippocampal complex, has to tell all cortex during sleep periods, "Here's a region in space, you translate that into your reference frame somehow, and here's the basic thing you're supposed to be observing there."

Thousand Brains Project: So is it projecting back to almost like LGN, or...

JHawkins: It could be something else. It could be the reticular formation, or there are a bunch of things that could be playing a role here. I don't know what the mechanism is. I don't think it would be a direct connection from entorhinal cortex to every region. I would think it'd be more global or go through some intermediary. There are a whole bunch of things that could come into play to make this happen, because the hippocampal complex is going to be broadcasting some sort of egocentric region space, and it has to be converted into models. I haven't even thought through this. By the way, I want to come back to one thing about V1 and V2.

I think of the models in the cortex as hierarchical compositions. When something unexpected happens, it can happen at any level in the cortical hierarchy. If an unusual car shows up at the intersection, something that doesn't look like a car, that's going to be pretty high up in the cortex. I don't think it's V1 that's going to say, "Hey, there's something wrong here."

But some things could actually show up in V1, depending on what it is. What I'm imagining is that when we're training during this sleep cycle, not everybody in the cortex is learning. It almost feels like it would be starting at the hippocampus and going down some levels, then stopping. Sometimes it goes a little further, maybe all the way to V1, but that's rare. Something down in V1 doesn't make sense—it's like some basic structure in the world is weird.

You can imagine this mass training event. The things they're actually learning start at the hippocampus replaying stuff, and then it dynamically goes down as much as needed to train these regions. Most of the learning will be of complex objects in complex environments, but if needed, it keeps going further down, tending to smaller and smaller regions of space. It might get all the way down to V1, say there's a new letter here that doesn't make sense, and maybe that letter was on the logo on the side of the car in the intersection. First you look at the car, then the side of the car, then the logo, then the word, then the letter, and you go, "What is that little thing there?"

Thousand Brains Project: Maybe the TikTok logo? I don't know if you've seen it, but it's those two colors, where when they're really close to each other, it looks 3D. I feel like that's something that maybe V1 would be troubled by.

JHawkins: You look at it in detail—what's going on there? It's red and green at the same time. Anyway, these are just fuzzy ideas, but I'm not countering what you said about V1. I'm just pointing out that even in the normal learning process, most learning wouldn't affect V1 because the representations of V1 are not incorrect. We only want to change models or extend models where the model isn't sufficient, and those will mostly be towards the top of the hierarchy.

We're learning new arrangements of complex things we haven't seen before, mostly. But not always—sometimes there are new, basic little structures that we might learn.

Scott Knudstrup: I don't know how to square these two things, but I've got these two things in my head. One is that the hippocampus during this really rapid learning memory—let's say you look at a table, plate, fork, mug, cup, things like that. Those are super coarse-grained. You're just getting category tags for each of those things, like plate; you're not really learning a super detailed special representation.

JHawkins: Why do you say that? These are known objects, right? I'm looking at the things on the table?

Scott Knudstrup: They're known objects, but when you quickly learn that arrangement, it seems like you're really just learning that's a cup. Unless you really focus and try to learn that cup in detail, this rapid learning seems to be about learning the coarsest, most general version of this kind of object.

Thousand Brains Project: It exists in that location. It feels like some sort of neural signature on a continuum. If someone swapped in a similar object, your chance of noticing the discrepancy would probably be proportional to how similar it is. If someone puts in something that's technically a cup, but it's a totally different cup, there's a reasonable chance you'd notice. If it's only slightly different, who knows? It feels almost like the distance in representational space.

JHawkins: Here's another way to look at the same thing. In my house, we set the table every night, and either my wife or I set the table. We have a set of dishes, a set of cups and things. If I walk in, I'll say, oh, did Janet put out the spoons, or this, whatever. If one of those cups wasn't one of the five glass styles we have, I would notice right away and say, hey, that doesn't belong here. If I walked into your house, Niels, and you had a glass, I wouldn't think twice about it. My point is, my prediction can vary. There's a scale between, oh, any cup would do, maybe Niels likes these kinds...

Thousand Brains Project: I'd be surprised if it was the exact same cup that you have.

JHawkins: I would notice that. You're right. I'd say, hey, you bought these at IKEA too. It goes back to your point, Scott, about how detailed it is. I don't understand it, I'm just making an observation that it can be quite detailed, and I'll notice something's wrong, or it could be not, if I'm at Niels' house. The general rule is, I don't know the answer. Sometimes it can be quite detailed, and the exact arrangement can change all the time. There's not a set arrangement every time we sit down at the table.

Ramy Mounir: No, but if you do the same arrangement every night for years, then...

JHawkins: There is this scale between things that are repeated accurately, things that are repeated, and things that aren't repeated.

This is similar to the idea that cups can look different, have different basic shapes, but they're all cups. Or my cup, which has a very specific shape. The same basic thing. Anyway, I'm not sure what your original point was, Scott.

Scott Knudstrup: I have sort of two...

JHawkins: By the way, before you...

Scott Knudstrup: Figure out how to...

JHawkins: I want to know what your background is, because it's very distracting, I keep stopping...

Scott Knudstrup: Sorry.

JHawkins: Looking at your offset.

Scott Knudstrup: I'll switch it. I checked the stocked backgrounds last time I was on Zoom. Thank you.

Thousand Brains Project: I'm wondering, is it meant to be a scene? The inside of a...

JHawkins: Cardboard buildings with shadows or something?

Scott Knudstrup: It reminds me of Inception, when they're bending the city up and stuff like that.

Thousand Brains Project: Less disorienting if it didn't have that thing at the bottom.

JHawkins: I'm getting distracted because I'm attending to your background, trying to figure out what it is, and then I don't hear you.

Thousand Brains Project: Or a cardboard forest.

Scott Knudstrup: Here we go.

Thousand Brains Project: Okay.

JHawkins: There we go, there's your home.

Scott Knudstrup: The idea of tagging locations with the most general object type, or really rapid learning, fits nicely with the compositional idea. We'll build a compositional objective as well as we can, which means tagging locations with IDs. I was thinking that's a pretty sparse way to represent a cup, if you can manage it, and there's nothing special to learn about it—you've just got an idea at this location for a cup, plate, knife, and fork. It's pretty sparse, which is nice. Then I was thinking about model sharing throughout the rest of the cortex. It's a lot easier to share model information if it's a really sparse, basic, general model.

It's a challenge to imagine how really detailed special information is going to get relayed between different columns.

JHawkins: Let me stop you for a moment. Imagine the hippocampus has a representation of location, meaning region of space. It's not a point, it's a region, it's different. It also has the ID of the object that was in that location. That ID could be very specific or generic. The location could be very specific or fuzzy. The difficulty of transmitting that to other people is the same. I don't think it's any harder if the SDR represents a unique object or a generic object. All we're doing is specifying objects at locations, and the object ID can unfold into a very detailed object. I'm not trying to pass all the details. If it's my cup on the table, I don't have to say, here's all the facets of the glass. It's just the ID for that glass, and that's it. I'm pushing back on the idea that transmitting or broadcasting a detailed model—meaning objects that are detailed versus objects that are fuzzy—really doesn't take any more effort.

Scott Knudstrup: Needs.

JHawkins: It's just location and ID.

Thousand Brains Project: Storing them seems like it would be similar effort.

JHawkins: But...

Thousand Brains Project: Maybe what you're saying, Scott, is more like, if the hippocampus knows it's learned about a new object today, and now I want to transfer that over sleep to other columns, presumably the more points there are in that model, the more time-consuming it would be to visit all those locations and share all that information. Is that what you're saying?

Scott Knudstrup: Yeah. It'll be a lot easier to share information if it's a cup or a mug or something that I'm trying to share between columns.

Be a lot easier to do that in a compositional setting, where I share a vague cylindrical-looking thing connected to a vague handle-looking thing.

As long as those columns also know about those,

Yes.

Stop objects, or whatever.

Yeah. I'm going to push back on this a little bit. As you said, Neil, there could be more points to transmit. If I've learned about the mug with the logo, and now the logo has a bend in it, that takes more information. I had to make more observations of the logo, had to attend to the point where it bent. Otherwise, I can extrapolate between.

There are a few more observations. The number of observations that has to be transmitted would change, but I think the quality of the observation doesn't change at all. It's still this location ID, same thing—it doesn't matter what the ID is, if it's a fuzzy object or a specific object. I think the difference between what you call a detailed model and an undetailed model might be the number of points you have to transmit, the number of items, or the number of observations that you made. That would be true.

But I think your point, Scott, is just that compositionality enables us to have sparser models, which is good for this. In your example, Jeff, of the logo with the mug—or the mug with the logo—if you didn't have a model for the logo, you would have to transfer a model where you had learned all the colors at specific locations, and that...

But we're not doing that.

Yeah, exactly. I think we're all in agreement. Scott, was that... It's nice that we get sparsity from compositionality, and maybe just a reminder to lean into that, because it's useful for transferring models.

Yeah, and the final point was sleep as a potential mechanism for helping with the sparsification process or building up of categories. So starting to take synthesized information throughout the day, in many kinds of cups. Perhaps that offline period is a period when you start building these sort of general models.

But that's not what I'm arguing. That may be true, Scott, but that's not what my point today was. My point was, you're not doing some sort of data management here. You're just replaying experiences. That's it. Whatever action you experience, it's going to be replayed, and the receiving columns that are getting this location and ID information, they can learn what they're going to learn from it. But I don't like the idea of—or maybe I just don't understand it—the idea that somehow we're using this to come up with conceptual things, or to massage the data, or build different types of models. The mechanism I propose here doesn't do that at all. It just says, I observed some ID at some location, I have a sequence of them, I can play them back, and everybody can be told to pretend you're seeing the same thing.

It's not doing more than that. Maybe something else is going on more than that, but I'm not proposing that.

Fair enough. I was just pointing with the idea that the detailed model information is learned to some degree in cortex, and the hippocampus is—the reason why it's a fast memory is because it's not storing that much information.

No, I just...

If it's just storing,

But it is going...

That are, with the detail information being lost.

But it might be—the ID that it's storing could be a very detailed object. It's still an SDR, there's no more extra memory involved, but it could be a detailed object. It's not like the hippocampus never has to transmit—all it has to ever do is transmit the IDs of the existing known objects that are children of a new composition. So it's only transmitting ideas of already learned things.

And, where it was absorbed in the world.

It could be very detailed, or it could be a super complex object—this is the space shuttle, that's the ID for the space shuttle, or it could be a letter A.

I'm just trying to get away from this idea that I keep hearing, that somehow we're trying to solve an information bottleneck problem or something like that. It's not like that at all. I don't think it's—it's just a way of getting all the columns to act as if they actually experienced this thing directly.

Meaning?

That's all it is. And the information sent is very minimal. It's just ID and location. These are just SDRs.

I'll—can I try to share something that might support Scott's idea?

I just started drawing this, I'm not sure if it's going to work out. Everybody see the Excalibur?

Yeah.

Okay, I was making this earlier, and then I gave up on it, but so let's say we have multiple columns, and I'm ignoring any of the intermediates. And these three are getting input, so let's say this is finger 1, finger 2, and finger 3, and we're learning—let's say finger 1 is on the handle, we haven't learned a mug, a handle of mug, and this F2 is the body, and this is the rim or something.

Are we learning the mug for the first time?

Yes.

Okay, so let me...

It's like putting three fingers into a black box. And we're touching around the mug.

Okay, so let me stop you right there, because I think you might be coming down a path I disagree with.

Okay.

When you're learning, you attend to one region at a time. I can't learn the mug by putting my hand in there and grasping all the... I can infer it this way, but I can't learn it this way. Imagine the mug is a...

Or maybe at a higher level column, you could. The column that's getting inputs from all of those fingers could say, "Oh, I'm sensing..." But then you're attending to a region, and there's one thing you're sensing at that region.

The output, during learning, has to be a single object at some location. And if the three fingers do not recognize a single object, because I haven't learned this mug before.

Dan...

I'm going to attend to some region, and maybe only finger 2 or finger 3 is getting the input in the attended region. We can't learn all these together; you have to attend to a region at a time. Learning always involves attending to a region where a single object can be identified. If finger 3 is attending to a region and recognizes a rim, that's great, but at that time, the other two aren't being used.

During inference, all can vote, but during learning, only one region can be attended to at a time. If the object can't be recognized within that region, the region gets smaller.

For example, you put your hand into a box, and someone's arranged a board game with some dice on it. You touch a die with your three fingers and instantly recognize it. Now you're learning that there's a die at that location, and then you move to a new location. But as you said, there's one object. If it's a totally unfamiliar thing, then you would—

Let's say you touch—

So you grab it with your three fingers, and it's like the old shoe in the Monopoly game, or something like that. If you're familiar with that, they used to have these really old ones from the 1800s, old leather or metal shoes. Anyway, it had a weird shape. If you touch it with your three fingers and you don't know what it is, unlike the die, you'd say, "I don't know what this is." Then you'd have to use one finger and move it slowly around the shoe, trying to figure out its shape. You can't learn the shape of that object, this totally new shape. The point is, you have to attend to a region and recognize something in that region that you already know. If you don't recognize anything in that region, it doesn't matter how many sensor modules are looking at it; if you can't recognize it, you have to narrow it down to a smaller region. Multiple learning modules can attend to the smaller region, but they all have to agree on something. If my three fingers are going to work together, they have to recognize an object that gets passed up to the top of the hierarchy. If they don't recognize the object, we don't learn it right then. We have to narrow it down to a smaller subset that we do recognize. At some point, you get down to the smallest subset—it's just an edge at some location, or something like that.

Is "rim" an object ID?

It could be, if it's something you recognized.

But if it's something you've learned—generically, all we can say is, when you're learning a new object, you're finding a child object that you can place within the parent. If the input to all these columns is not recognized as a known child object, you have to narrow down the area of attention until you get to a known child object. I can't say up front whether those three fingers are recognizing something or not, but if they're different parts of a mug and you haven't learned the mug yet, they're not going to work together.

Let's say they're close together, three sensors sensing, "Okay, I recognize this is a mug." The idea, or the way I understood today's conversation, is that that ID will get passed up eventually to the hippocampus. The hippocampus will somehow broadcast all of these back down to all the cortex, not just the fingers that have learned. Since the fingers only learned—let's say this is all visual—

No.

Let's say the rest are visual. The visual columns will still learn something because it's coming from the hippocampus, but they only have the morphology of the rim and not the color of the rim. Is that—

Yeah, these are details I didn't try to figure out.

But clearly, you can't learn the color of an object by touching it.

But I can learn the morphology of the object by touching it. When the hippocampus replays, I don't know what the mechanisms are or what it's actually replaying. For example, let's say you're learning the mug on the coffee cup, and you're learning it visually. There's a region in the visual space that says, "Oh, I recognize this whole logo." That's the attended region, the entire logo. The logo ID gets passed off. Now, at some location, the logo ID, orientation, and location are known by the hippocampus. The hippocampus plays that back.

There's no representation for the logo in the somatosensory columns, so they wouldn't even know how to interpret it. There's no model of logos in the somatosensory cortex, but we could say there's a logo at this orientation and location. They might not know what the logo is, but the orientation and location make sense, so they can just learn a model of the orientation. Of course, a logo wouldn't be built on part of this model, but the point is—

Would it ever work for features? Even if it's also a visual column that knows about logos, its SDR of the logo will be different.

How would you broadcast that?

Presumably, the SDR ID is something that can be broadcast, that is in some sense shared among columns when we vote. There is a pattern of activation that—

But yeah, because that's—

He's—

About that object. It does feel like the projection almost needs to be basic sensory input, like you were saying before, rather than projecting to V1.

No, what we want to project back is a recognized object at some location. But if a column doesn't know about that object yet—

There's no—

Whoa.

Meaningful SDR to act.

Some columns know about that object. At some point, the object was recognized. There was a stable pattern in layer 3 or layer 2 representing that object. That stable pattern can be re-invoked by the hippocampus, and all the columns that know how to vote on that object would, in theory, know what the object is. If a column never knew that object—say, a touch column that doesn't know what the logo is—it can't invoke the ID. It just can't do that. It says, "I don't recognize this. I've never seen this before." But if we're passing down three things—location, orientation, and ID—the location and orientation would still work. It would learn the morphology of the object but not know what the particular feature was. I'm not sure if I'm expressing this well.

Yeah, it feels like if we are going to get this to work, it makes more sense to try and reawaken almost the raw sensory and movement input, because that's the only thing that's universal.

A thing that's universal... The thing is...

In order to learn new models in columns that have—

We don't want to specify the movement. But I don't think that's what we know. We know the region or area in which the object was.

Or location, but I guess—

Okay, location. Location.

Model.

You could specify location, orientation, and feature ID. The location and orientation work for everybody; the feature ID only works for those who know what that feature is.

But if you do know what that feature is, then you're golden, right?

Wouldn't the feature have different representations in each column?

Yes and no.

A side note here: when we talk about voting, every column has its unique ID. They send their projections broadly, and people then associate with them. The projections can be very sparse. I don't have to associate the ID in column 1 with the SDR in column 2, also associated with the SDR in column 3, and so on. If I looked at all the activity going across all these neurons, and it's sparse, if I sample from this activity—just 20 or 30 neurons in any set of columns, it doesn't really matter. It could be one from every column. If I sample from that and get a new SDR from those 30 out of a million, that is sufficient to specify the ID for everybody. I know it doesn't sound like it, but I worked this out once. As long as you have enough connections to all of them, you're good. Thirty connections, and as long as you randomly distribute them.

But wouldn't you have to do it the other way around? If you project back, you'd just activate one in—

No, because that—

One might be used in different multiple records.

Because I believe the voting mechanism would work. All the columns are trying to reach a consensus, and if you specify individual bits distributed around this region, it'll force it to get to the right consensus. I'd have to work through it again.

As your point, Vivian, it's coming from one region, so you don't have a bunch of different places you're sampling from. You only have the one?

And so it almost needs to—

Bull's signature.

Imagine—

You would elicit a partial representation in each column and then use voting to resolve the rest of it?

That's how voting works anyway. I may know my ideas and broadcast them, but I could do very sparse broadcasting, and it'll force others to come to the same conclusion. It's a bit counterintuitive. So imagine this—

I had a separate question: devil's advocate, why are dreams so random, rather than just the sequence of the day?

I'm not sure dreams are necessarily—

Not random, but there's certainly a lot of strange things that happen.

I'm not sure dreaming is the recall we're talking about.

Okay, I thought there was some evidence that there's—

It might be.

4AM sleep is when the recall happens.

All I know is—

Or the replay.

First of all, these—

These replays occur very rapidly, remember? They occur much faster than real time.

I don't know if dreams are like that. I didn't make that assumption. I didn't assume that dreams are the recall we're talking about. I do know that recall of sequences occurs in the rat hippocampal complex, and it's very rapid and precise for the exact things the rat did. It can retrace what the rat did in the maze, or something like that. I don't know if that's the same as dreaming, because dreaming seems to be weirder. We should look that up.

During REM sleep, where dreams occur in humans, replay events also occur both during slow-wave sleep and rapid eye movement.

Suggesting a possible role for place cells in dreams. In those computational models I mentioned before, the randomness of dreams has a computational role: shuffling the sample data and making it more statistically independent. In your response to Vivian, about how to get back the representation with such sparse inputs, I was thinking that would go wrong a lot. You would basically hallucinate, if all you need is a bit of top-down input, and then voting will—

So maybe that's—

Often just triggering a random incorrect object, which is why in dreams, suddenly, something will appear out of nowhere that you weren't expecting.

I'm not sure you'd get accurate results; you'd have a lot of errors. Think of the hippocampus as just being one more column that's voting.

Or a bunch of columns—it's a big structure. I guess one column isn't normally enough to trigger everyone else to act.

It might be enough to force everyone to reach the same consensus. Let's assume that for the moment. I actually think it is, but let's assume that for now. I know it doesn't seem like it, but I think—I can't remember how I worked this out once. But imagine that's what's going on. I'm the hippocampal complex, and there's a bunch of activity in layer 3 or 2 that's being spread all over the cortex. I get to sample that. The only thing happening in that set of neurons is what's being attended to at that moment. We're attending to some region, and there's some activity on some of these neurons. What if I'm able to sample from that set of activities? I'm in the hippocampal complex, and I say, "Okay, I'm going to remember that's what the intended object was." I don't have to do anything else. I just have to form synapses, or turn on synapses, that capture a sufficient number of bits in the pattern that was on some sections of the cortex. Maybe I'm looking at all the cortex, so millions of these bits are coming in, but I'm only going to quickly say, "Okay, here are 30 of them that were active. I'm going to remember that as the ID." Later, I'll play that back—it's the same ID. My argument is that this is sufficient to force those other columns to reach the right conclusion. I can't prove it to you right now, but I worked it out to convince myself that it worked.

Yeah, okay.

Maybe Super Tai would remember how we did that. Maybe not. I could work on it again if you want. I had some good arguments for it. It was a surprising conclusion. Voting requires this: if I have millions of columns, I can't be sending connections to all of them. If I have a thousand columns that are active, I can't try to activate every one. All I can do is send out a broad broadcast of which of my neurons are active, and it will randomly associate with some other neurons that are active elsewhere. It's the collective effect that's important, not any major visual column. Collectively, they're going to settle.

We can just take that for the moment as a working assumption. I think it would work. All this is amazingly complex.

Yeah, but if it's the voting that settles to an object ID, are you saying that would be the input to the next column, and that's the one that's learning?

No.

Because if you just get the object ID re-invoked, that doesn't really help you learn more about that object.

Remember, we're also going to broadcast the location and the orientation, right? We were just focusing on the object ID because Scott brought up the issue that not everybody knows the object ID.

So is this the ID of the incoming feature at that location?

Whatever feature was determined at the attended-to location. We attended to a region. The neurons are restricted to anybody who is getting input from that region and gets to vote. They reach the consensus of what's in that region.

That consensus is then stored in the hippocampus. The hippocampus can play it back later, saying to everybody—not just the region, not just the people who observed it, but to all the columns—"We observed this object ID." Some columns will be able to recognize it, some won't, but it would be a lot more than the ones that actually sensed it to begin with. I hope this is not super confusing.

Yeah, I guess I'm getting a bit confused with sending back the object ID, which would go to layer 4, but then also resolving it through voting, which would be happening in...

No, I didn't say it goes to Layer 4.

No.

If we're using this...

Yeah.

...to learn models.

I guess you're saying it would be—let's work out the details. The feedback wouldn't go directly to the column. It would go to the regions that are able to recognize this object. Those regions would then project to Layer 4 in the next hierarchically higher regions, so that's where training would occur. Basically, I'm invoking the features in some columns, and then we're going to try to learn the arrangement of those features in the next region up, right?

Okay, yeah.

You wouldn't...

Okay, yeah, that makes sense.

There's the boundary of knowledge, right? The boundary of knowledge is that you're going from the bottom of the cortex, recognizing the compositional structure, and at some point, the compositional structure is no longer recognized. There are features that I recognize, but now the arrangement of them is not recognized. At that point, to train, I want to invoke the features at the child regions, and I want to invoke the locations at the parent region.

And I haven't thought about how to do that.

Yeah.

So you wouldn't be projecting to layer 4; you'd be projecting to some place, probably Layer 1, so you'd be invoking the correct model ID. You'd be booked into layer 1, then to layer 3 or 2, and then book the correct model ID. It all seems crazily complicated, but I can't reach any other conclusion. In some sense, it fits a lot of data that we know, so I'm reasonably confident of the basic ideas.

Now, just thinking through it here, I can see how the ID would work. I don't know how you would transmit a region of interest or a location. I don't know how to do that. I don't know how you transmit or broadcast the...

One way it could be done is if you reactivate the sensory input, which then gets filtered through whatever sensory processing exists and provides the orientation of the lower-level input. Then, if a movement occurs as it moves from one location to another stored in the hippocampus, that movement is transferred to the columns. As far as they're concerned, it's as if they are just awake.

That would at least fit with the phenomenological experience of dreaming, where you feel like you're moving around in a virtual world. It doesn't feel like you're just getting flashes of what you've seen when awake; you're actually moving and your sensorimotor system is still active.

I'm going to stake a point of contention here. I think we should separate dreaming from the mechanism I talked about, because I don't think this replay mechanism is dreaming. It is a precise recall of experiences of an animal, and it happens very quickly.

It happens very quickly, so let's not conflate dreaming with this, because maybe it has nothing to do with it. I don't like the idea of trying to fool the columns into thinking they're actually moving and bringing about movement data. That's problematic, because every column—if it somehow—that's a detail that can't be transmitted from the hippocampus. The hippocampus doesn't know how things moved; it just says, at some location, I observed this, and later, at another location, I observed that. It seems like we're trying to get away from representing the movement. I'd rather say, somehow you have to communicate a location in space.

It's weird, because normally we don't communicate location directly to columns unless there's a learned association, since it's always in its own reference frame.

Alright, so I don't know how it works. I'll just admit, I don't.

I don't think the solution is converting into movements.

And just, a virtual world.

Somehow it's going to be—yeah, I don't think so. Again, remember, we're not talking about recalling movements of your finger over an object, or recalling—

It's not necessarily dreamlike things. They could be.

At the intersection, I look at and see different cars and things, but a lot of what we learn—the logo on the mug is—anyway, that's the dream part, put that aside. It seems to me that what you want to transmit is a language that could be understood by every column. What the column needs to know is two things: to learn, for the cortex to learn, child columns need to invoke the idea of the child object. I think I have a good sense of how that happens. Then the location information has to somehow be invoked into the parent column's language, and I don't know how to do that. But I bet we could think of a way.

Yeah, I feel like that would be an issue.

Interesting. Follow-up exercises to try and think through some of these details.

In my notes, I said, lots of questions, and I didn't understand it,