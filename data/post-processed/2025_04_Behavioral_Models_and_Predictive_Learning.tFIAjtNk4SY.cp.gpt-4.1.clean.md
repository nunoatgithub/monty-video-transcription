So it should say subgoals, right? I say subgoals because we're trying to understand how the brain not only understands behaviors of objects, but also affects behaviors, makes changes in the world, and solves goals. What we're talking about is a subset of that: how do we learn models of behaviors? We make an assumption that we do learn behaviors. Viviane gave out some nice pictures of that last week. Then we want to ask, how do we predict? One of the things we've been focusing on is how to predict a changing morphology of objects based on a behavioral model. For example, how could we predict what the staple will look like when it's opening, or a novel object? We want to apply behaviors not just to objects that were learned, but to novel objects as well. This is the only thing I'm talking about today. This is a subset of a lot of things we have to do. 

We can think about it like this: I'm going to make some statements, some arguments, and then give you a bunch of examples that I walked through to help me think about this problem. Let me start with where we ended up. Let me start with a definition. A behavior is when a child object changes relative to a parent object. We've talked about this before: we're going to abandon the idea that this can be done in a single column. It requires two columns. It requires a parent and child relationship, much like we have with composition objects. Therefore, behaviors require a parent object and a child object—two learning modules. 

We've stated this last week as well: behavior models exist in the parent object and not the child. If I have two learning modules and there's a parent and child object, the behavior model exists in the parent object. We can't do this in a single learning module.

Just to clarify, at the lowest level, like the first definition—a child object change relative to a parent object—can this child object be a feature that comes from the sensorimotor directly, like a color change? That's a good question.

I've avoided that for now. One thing I've learned about thinking about hierarchy is that if you focus on the primary sensory regions, it really messes up your brain a bit. Typically, when you figure out the hierarchical region between two learning modules or two columns, then you can back off and ask, what's the acceptance that's happening in the primary one? I didn't do that in this case. That's a good question. I didn't focus on that. I think you might be able to predict changes of orientation, but you certainly wouldn't be able to predict an entire child object changing. My guess is it would apply, but with some kind of simpler behaviors that can be learned, or trialed objects. Are you willing to accept that, Viviane, or do you want to make some more points? 

Yeah, no, that makes sense. Again, this is a subset of the subset. I didn't deal with that particular point right now.

and then this is the main idea that Viviane and I came up with, which felt like the big insight. I'll explain this in multiple ways. The behavior model communicates with the child by sending movement information to the child object, which is different from what we've discussed in the past. In this case, it's sending movement information to the child object, but not in the way you might expect. I'll clarify what I mean by that.

Here are two pictures. On the left is our standard compositional model. I haven't shown all the details, but there are two objects: a parent object and a child object. The child object is part of the parent object; it's a subset in some sense. We have two morphology models, one for the child and one for the parent. Going back to the coffee cup example, think of the logo on the left and the cup on the right. Both are updated and have a point where the sensorimotor is focused at that moment. There's a location on the parent object and an equivalent location on the child object, and that location gets updated by the sensorimotor movement. Sensorimotor movement comes in, and through path integration, we can move the sensorimotor around on both objects. The objects are tied together by the blue arrow, which connects the location and ID on the parent object to the location and ID on the child object at that location. For example, at a certain point on the coffee cup, we're at a specific point on the logo. There are orientation and scale factors, but the basic idea is a one-to-one pairing between the child and parent objects. These are tied together through the blue arrow connections, and that relationship changes as we move around in space. At a different point on the parent, there's a different child object, and so on. That's our old compositional model.

On the right, I show the same model, but now we've added a behavioral model, shown in green on the parent. Viviane and I have discussed what that looks like, but I won't show it here—just note it's a model of behavior. In this case, the behavior model also sends a movement command to the child. We can't tie the behavioral model directly to a child object because we want the behavioral model to apply to different children and environments. So, we can't tie the behavioral model and the parent to the morphology model and the child, since the behavioral model will apply to different children. The only way to do this is by sending a movement command to the child. That movement command does not cause the child object to move; it just changes the expected location of the sensorimotor on the child object. It's doing the equivalent of the brown arrow. It's telling the child object, "It's as if the sensorimotor is moving," but it's actually not moving. The actual child object is moving, but the relationship between the sensorimotor and the child will change. Therefore, if we want to make a prediction, the behavioral model says, "I know the child should be moving right now, and if the child is moving, the location of the sensorimotor on the child will be changing." So, it tells the child object that the location of the sensorimotor is changing, and the child can make the correct prediction.

These are what we might call allocentric models. They don't really know where they are in the world. The left learning model doesn't know where the child object is; it just maintains a relationship between the sensorimotor and the object and changes that position. This is the basic idea of how a behavioral model could lead to predictions as a child object moves. For example, if I know how the stapler top is moving, I can tell the model that's modeling the stapler top that it's moving through the world. What's going to happen is the location of the sensorimotor will change. There are two sources of movement information: one is the actual sensorimotor moving relative to the parent, and the other is the child moving relative to the parent, which makes it look like the sensorimotor is moving relative to the child. That's the gist of the solutions Viviane and I came up with.

Before I go on, the next slide will have a series of examples to walk through this way of thinking about the problem, which will make the diagram more obvious. But before I do that, does anyone have questions about this?

Did anyone not understand it? Raise your hand. Don't be embarrassed. I'm a little unclear, but I'll screenshot this slide and refer to it while you go through the samples. That might help me. Scott, tell me what part is unclear. I'm looking at this part: "Does not cause child object to move. It changes the expected location of the sensorimotor to the child object."

I thought the child object itself was moving relative to its object. The child object is moving, but the diagram I've shown is only trying to predict the result of that movement. It's not making the movement happen. It moves you through the reference frame of the child object. In the bottom left column, the child column, we have a hypothesized location of where the sensorimotor is in that child object's reference frame. The movement command from the behavior model moves that hypothesized location in the reference frame of the child object.

It's a confusing concept, so I understand the confusion. Imagine I have an object with its own inherent behavior. No one causes it to do anything; it just acts on its own. I'm not making it happen. For example, the hand on a clock just moves. I don't have to do anything; it moves on its own. The cortex or learning module isn't making the child object move. It's moving, but we have a model of it, and we want to predict what it will look like after it has moved or while it's moving. The goal of the green arrow is not to make the thing move; it's to say, given I have a model of how it moves, what should it look like after a move?

Viviane O'Neill had a couple of images where we talked about motion through an obscured area. For example, if a stapler is opening and moving through an obscured part of the scene, we can still predict it will come out the other side at the right point in time. We're just trying to make that prediction. We're not trying to make the object move; we're trying to predict what's going on. If someone is swinging a bat at a ball, I expect the ball to move. I'm not making that happen, but I can predict what's going to happen given a behavioral model. That's all this is doing.

Changing the location of the sensorimotor to the object is what would happen if the object moved, and it's also what would happen if the sensorimotor moved. You can move one or the other. For example, if my eyes fixate at some point and I move the object, the location of what I see will change based on how the object moves. The green arrow is just indicating the object is moving. What should I predict? I have to know how the object is changing—its direction, speed, change in orientation—all the things the green arrow represents. I have to be able to say, the child object is moving, so the relationship between the sensorimotor and the child object will change. I want to predict where the new location of the sensorimotor will be, given the object is moving.

It was just the coupling between what the child object is and the sensorimotor resting on it that was unintuitive at first, but it makes sense now. The only thing you know about an object is what your sensorimotor tells you about it. If you can see my hand here, my finger is touching my hand with my fingertips. This is a sensorimotor, and this is the object. In our normal object models, we move the sensorimotor over the object and make predictions. Now, because of behavior, the object is moving.

From the model of this object, it doesn't know if it's moving or if the sensor is moving. It just knows there's a sensorimotor at some location and should predict accordingly. If it's in a new location, did the object move or did the sensor move? It can't tell the difference. In the case where the sensor is following the object, like smooth pursuit, both the sensorimotor movement and the movement from the behavior model cancel each other out. We keep predicting the same location on the child object, which is what we want if we are following the child. The green arrow and the orange arrow will cancel each other.

I'm not proposing any mechanisms here or showing this in layers of cells. I just wanted to take a step back and get the basic idea down.

Does anyone else want to ask a question before I go on to the next slide?

I have a quick question. This makes sense to me, and we're talking about movement right now. Are you going to cover cases where the color is changing? That's confusing for me right now.

I will, but not as much. I think that's the easier case.

I did think about it, but I didn't put it in my next slide. I didn't really prepare to talk about that today. It's like the traffic light changing from red to green. I think that's the easier problem.

I didn't develop it here. In today's presentation, all I developed was prediction for movement. Let's come back to that at the end. That would be a good idea. Absolutely. Just for completeness, relative changes in orientation—like if the child rotates relative to the parent—just to confirm, in your proposal, that wouldn't move you through the location space, but instead would adjust the rotation hypothesis. I didn't put that in this diagram. The location ID is shared between the parent and the child. Of course, we're actually sharing orientation as well and scale, but I didn't include those here. This is a conceptual overview, not covering all the necessary details. I'll show some of those in the next slide. I think it was a good idea to simplify it. It already felt promising and right when you were presenting it last week, Viviane. I think this helps show the core idea, and it definitely seems like this could be right. It's really just this basic idea: we have two ways for a parent object to communicate with a child object. One is through the mechanism we've identified before, which associates the exact parent with the exact child very precisely. The other way is to take the behavioral model, which we can't associate with an exact child, and we can't know everything about the locations and so on, so we have to approach it indirectly.

That's it. If that's all you remember from today, that would be a success. Let me show you some pictures. I apologize for using the coffee cup and the Mento logo again—it was just convenient. I didn't have a Thousand Brains Project logo with me at the moment. There are four little examples here. Imagine a magic cup. Don't ask how this happens, but you can imagine it. This is what I use in my mental imagery, so that's what I made pictures of. It may not be the best example, but it worked for me. On the left, you can see what I'm proposing: imagine a behavior where a logo on the cup changes position. Every three seconds, it would move from a lower position to an upper position, and then three seconds later, it would move back down. I'm not showing the movement here, just the two endpoints. You can imagine this logo going up and down. I have a Thousand Brains Project mug here. It shows the bent logo.

It goes up and down, and that's the behavior. We can learn that behavior, so we have to be able to make predictions about where the logo will be every three seconds as it moves, including the velocity and so on. The second example is changing the orientation of the logo. The logo just rotates—same idea. It rotates down, then up, then down, then up. We can also change the scale of the logo, going from small to large and back to small. Finally, you could alternate between different object ideas, switching from the Mento logo to a British flag and back. It gets more complicated. You can't just change the orientation. For example, when I change the second image below the first one, the Mento logo is changing its orientation, but it's rotating around a different point. In one case, it's rotating on the corner of the triangle; in the upper one, it's rotating around the center of the word "Mento" or around the "M." You can't just say, "I'm rotating the logo"—that's insufficient. We don't want to specify a rotation point, which is tempting, but we know that doesn't work. The answer is, we have to learn this on a location-by-location basis. This, in some sense, proves that. In the next example, the logo bends: imagine the logo is straight, then three seconds later it bends, and then three seconds later it unbends. You can just animate logos here.

These examples illustrate that this has to be done on a location-by-location basis. You can't treat the entire child object as rotating or moving. We made that mistake with composition objects and then figured out how to do it. The same thing happens here. It has to be done on a location basis, and the system can interpolate between points, so we don't have to learn every single point.

These are all very similar to what we've seen before. I thought of another example, which is a bit unusual and fooled me for a while. Imagine I had a magic logo, and any place that logo appeared, it would move. If I placed the logo on the front of my refrigerator, it would go up and down, and if I placed it on my laptop, it would also go up and down. That would imply that the model of the behavior is not on the parent object; it seems like it would then be part of the logo itself. It's the logo that moves up and down, and no matter where I put it, it would do so. That seems odd—a counterexample. In the end, I concluded that it's not really any different; it just expanded my idea of what an object model is, a morphology model.

I've shown a little dotted line here, and I think the way the brain would handle this is by having a reference frame for the logo with a bunch of features in that reference frame. You could argue it's one feature if it's the whole logo, but it doesn't matter. All the features are moving—the behavior of that larger reference frame, the dotted line. Essentially, all the features in that reference chamber are changing. In essence, there is still a parent object; it's just a concept. There's a reference frame in which all the elements in that object move up and down, and they have two positions they can exist in. I know that sounds a little cheap, but I don't think it is. Mentally, that's how you end up visualizing it.

That could definitely be happening at a lower level, but you could also have a third level in the hierarchy and basically have the mental logo as a child on this little dotted square, which might be like a magnet you can put in different places. That's what I'm suggesting. You wouldn't have to move the individual features in the logo; you've already learned the logo. You can just move the object.

Moving features is what behaviors are—it's just moving reference frames. Maybe you're saying it's just an invisible parent object, like an anchor point, an extra object that has no other purpose. As Viviane says, it's three levels: you have the cup, then this object represented by a child object (the dashed lines), and that child object has behaviors that move the features of the mental logo up and down.

Maybe I didn't phrase it well, but the point is that I puzzled over whether I could prove that behaviors always exist in a child-parent relationship. I wanted to be certain that the behaviors never apply to the object itself, that there's always a parent-child relationship. This seems at first like a counterexample, but mentally, what's going on is that you've created a new object that basically has no other instantiation than a behavior where the child components go up and down.

One way to think about this: imagine that rectangle was actually visible, like a sticker or a magnet you put on your refrigerator, and this magnet had a unique icon. Anything you put on that magnet would move up and down. If I saw that little magnet and then stuck the Memento logo on it, I would expect it to go up and down. It's the behavior of the magnet, not the logo.

This is a really interesting example because it gets at object physics, which we've often puzzled about. You can imagine gravity as a behavior like this, where you just put an object in that behavior and it starts behaving according to gravity. It's interesting—modeling physics as a behavioral model, like the refrigerator magnet.

and it fits because we can imagine it's the world. We can imagine an object that isn't subject to physics, or we can play a video game where there's unusual physics. It's nice that we can apply it arbitrarily; that's a good point. Niels, I want to remind you of what I said in the beginning. What I'm presenting is a subset of the ultimate problems we have to solve. I'm trying to argue that this subset is valid, and then we would build upon it. That is an example of building upon it—thinking about physics and how they would apply. I think that would be a nice way to solve physics too. Neil said in the brain, there isn't a specific area where physics are represented; it's more like everywhere. That would fit with behavior models of how objects fall and so on, and they could fall differently in different environments. The weight of the object changes things. At the Kitty Science Museum, they have a big air tube with air going up in it. You can put things in it and they can go up and down. Physics doesn't apply, but once you know that air tube, you don't expect physics to apply. Very quickly, things can go up and down; they don't just go down.

I don't have a picture of this, but I'll talk about it. I also thought about things like objects moving through space, like a bird flying, or in American baseball, a ball that's hit in a ballpark and goes into an arc. I'll share some thoughts I had about that. If a ball moves in an arc like that in the ballpark, that's not a behavior of the ball; it's actually a behavior of the ballpark. In ballparks, I expect balls to start at one point and move through the air in trajectories. I have a sense of where they will end up, and I've seen lots of examples of this. They might bounce on the ground or do various things, but there are behaviors that a ball could exist in within the ballpark. It's not a behavior of the ball itself; it's a behavior of the environment or the ballpark. I could substitute a different item that would perform the same—basically, physics moving through space. Then I thought about a bird. A bird can move across your scene, flapping its wings. Clearly, the bird flapping its wings is a model of the bird. I don't have to see the bird moving to see that; I could have a video of a bird just flapping its wings, and I can recognize that behavior, like the banana walking, even though the banana's not moving. But I can also imagine, what if the bird just flies across my scene? If there was no background information, it would be very difficult to say much about the bird. But if the bird was in my living room, flying around, then I could learn the behavior of the bird flying around the room. The bird would go to this corner, then that corner, bang against the window, whatever. I was thinking about examples where, like a ball or the bird, the ball does not have any behaviors on its own that I can think of, but it can move as part of a behavior of the ballpark, in the physics associated with that. The bird does have behaviors, but its movement through space is not part of its behavior; that's part of the space's behavior. Anyway, that's a little aside. All that was consistent.

Now, we started with this example of the stapler, and that really is confusing for quite a bit. We've already talked about the solution, but I wanted to bring it up again. In this case, the child and the parent object are actually the same objects. The two learning modules would be observing the same object, and when it starts to move, if we're going to make predictions about it, then we have to maintain the model in the child object. We basically decided that the way this is going to happen is that somehow the child object will be masked, so it's really only attending to part of the stapler—the part that's moving. Once you've done that, everything else works.

Over time, that could be used to segment the stapler into top and bottom as separate parts. It could be doing that.

Viv and I talked about this issue related to key frames. Do we remember all the positions of the stapler? That would be true of all these other examples—do we remember all the in-between positions of the logo changing, and so on?

I imagine the following: a learning module says, I'm always trying to learn the morphology of an object. I'm always attending to things and trying to make connections between the features and the locations on the object. If those features are changing or moving, I really don't have time to learn the morphology; it just goes by. I don't really have time to do it. But if the thing stops, then I can learn the morphology. I can continue on. Look at the example in the upper left-hand corner: the mental logo in the two positions on the cup—there's no preferred position. I drew this diagram specifically to make it look like there's no preferred position. There isn't a case where the organ is supposed to be in the center and then it moves up and down. No. What I think the learning module would do—the parent learning module—would be to learn the behavior of the logo of an item moving up and down. If it stopped for three seconds at the top and stopped for three seconds at the bottom, it would learn two different morphology models of that object there. It's not like there's a preference; it's not that the logo is always on the bottom and then it moves up and down.

If it stopped in between, I would probably learn that one too. It's fluid—the way I'm proposing, and this is true for all these things. If something is moving, you don't really have a chance to learn a morphology object. But if it stops, then you can start to learn a morphology object. You might call it a key frame, but it's not acting as a key frame.

I believe this is very efficient in the brain for learning these in-between positions. It's not a waste of resources; it's very quick, but it doesn't have to be learned. I can still predict where the logo will be in the middle if it stops, but I don't have to learn it. The system will always try to learn the different morphologies of the behaviors. If things stop for a while, it will try to learn them. In all these examples, I imagine them stopping every three seconds, so I would learn all these as valid morphologies of the cup. That would be true of the stapler as well. I hadn't thought about it that way until you mentioned it in the Slack thread. It's a nice way of thinking about it because it's just taking our initial premise and ruthlessly enforcing it. Whenever something is changing, we put it into the behavior model. Whenever static features are observed, we learn it in the morphology model. If the logo stops somewhere in the behavioral sequence, it goes into the morphology model again. The purpose of learning these different states of the stapler would be to efficiently recognize the stapler. Once it's open, if you see the stapler in its open position on the desk, it would be much easier to recognize if you've stored that in the morphology model. Otherwise, you would have to mentally simulate the behavioral sequence and see if it matches your stapler model.

If you take the two upper and lower mental logos, and imagine the cup with the logo going up and down every three seconds, you might imagine one of those two positions and maybe alternate between them, but you wouldn't imagine it in the middle because that's not a point where it stops. If I take the cup out of the counter, it could be moving, at the top, or at the bottom, but I wouldn't expect to see it in the middle because it never stops there. This helps you infer, but it's also just an observation that you wouldn't know which one to predict. I then asked myself, what if the logo went up and immediately came back down and never stopped at the top? It just went up and down, so the logo is supposed to be at the bottom, and the behavior moves it around, but it comes back to where it's supposed to be. I went through all these mental scenarios to make sure it would work, and I think they all do. I think I have one more thing on this page. No, that's it. That's all I was going to present today.

If Viviane wants to explain what I got wrong or the parts she was trying to explain—no, that's exactly what I was thinking too, so that's perfect. But you did mention the problem of transferring location. I still don't understand what that problem is. The thing that got me thinking about it was the feedforward version of how the behavior model is learned in the parent object in the first place. How do we learn the changes in location? The answer I came up with, and I think you agreed with, is that it also just gets the sensory input and basically learns location changes from the sensory input it gets. Can I counter that? I wouldn't use those words. I would say the behavior model doesn't learn anything about location. It's basically learning movements—movements occur here at this time. It's an observation in the parent; it doesn't come from the child. Maybe, Viviane, if you have a diagram from one of the ones you showed last time, that could help.

Let me see if I can pull one up real quick.

Let's see.

I'm not sure if I have the perfect one right now, but this is where I started off with the one you just walked through, but in a much more complex diagram with a lot more going on. I tried to build it from everything we had before, but it was more confusing. This red arrow is the arrow you had shown in green, and similar to how the sensorimotor movement can move you through the reference frame, this stored movement in the object's behavior can move you through the reference frame here. That all matches, I think. The second part I was thinking through was how this kind of local movement that we're sending down here gets learned in the first place and how it arrives in this column. There are two options. One is we detect that local movement in the sensorimotor patch and then store it there—these arrows that are moving here. The second option is that the lower region could output the location of the child object relative to the parent, or rather, it outputs the location of the child object relative to the body. If the location of the child object relative to the body changes, that change is encoded here. If the former solution solves the problem, it's much simpler. It's much simpler, but it relies on direct sensory input to that column. If we also output the location of the child object relative to the body, in a common reference frame independent of the child object, then higher-order columns that don't get direct sensory input can learn that.

Don't we need the latter for some types of behavior, like the ID change and things like that? It seems the more general thing is that we detect a change in the output of the previous column, like the Union Jack going back and forth to the logo.

The funny thing about the Union Jack is it's not clear that it's a behavior that can be generalized.

Maybe—remember I went from the logo to the flag. Maybe if I put a Greek flag there, it would expect the Greek flag to change back and forth to the local. But, in general, it's not like the other ones. With the other ones, I could essentially take a stapler that doesn't look like the one I learned on—it is very different. If I applied a behavior to it, it would predict the position and correctly handle whatever happens as it moves. I don't know how that would work on the ID change.

For ID change and orientation change, I basically assumed we get that information from the lower-level column. We have the detected ID from down here, and that works well. But can I stop for a second? Why does the orientation change have to come from the child column? Can't that just be imagined as coming from the sensorimotor? Isn't it sufficient to get it from the sensorimotor?

I thought it's the relative orientation of the child to the parent, not just some low-level sensory orientation, so we can make a direct prediction about the relative orientation of the child. Basically, we are already sending the relative orientation of the child to the parent up here, so that already exists. Then we are just saying: is it static or is it changing? If it's static, it goes into the parent's morphology model. If it's changing, it goes into the parent's behavior model.

The way I thought about it is, just take vision—the retina provides, going back to the example of someone playing a video game and you're watching them, the retina provides all the information you need to know about how the player is moving, including changes of orientation—forward, back, left, right, up, down, and also changes of orientation. That information is available; it must be coming from the retina. So I just said, great, all that information can be picked up from the sensorimotor. I don't need to rely on any other connections between the parent and the child object. That's simple, but I'm not sure that's right. Maybe there are some things that can't be done that way.

The two counterarguments are: what about something from which I'm not getting direct sensory input? That's one. The other might be something related to changing IDs, which I don't understand yet. Changing IDs—I agree with you that this is not so generalizable. I don't think we can just generalize which ID changes to what; it's a fixed change that we'd have to store.

For changing IDs, I was just doing the same mechanism we already have: sending the ID up to column two and learning that on a location-by-location basis. If the ID stays the same, it gets stored in the morphology model. If it changes, it gets stored in the behavior model.

I'm beginning to understand what you're saying. In fact, I would argue that it's always being stored in the morphology model, even if it's not changing. If the logo's not changing or the staple's top isn't changing, there's no harm—it's still saying, "I'm looking at the logo." The connections are there; you could learn them. But if the ID changes, then it would say, "Now I have a new ID at this location."

So I'm agreeing with you. I think the change of ID would involve the layer three to layer four connection because it's changed. But it's not a special case. The layer four connections are always trying to learn connections, and if it changes, we'll have to learn some new ones. If it's not changing, there's nothing to learn. Most of the time, it would be laid down in the morphology model, like one change from one flag to the other. Most of the time, there's no change happening, so we're actually learning the flag or the idea of the flag on the cup. But when it changes, that way we can know the timing of when in a temporal sequence the child ID changes.

It's a little bit like, imagine the logo that's moving up and down on the cup. What if it didn't move up and down? What if it just appeared at the top and then appeared at the bottom, with no movement in between?

We know if the logos are close enough together, you'll perceive movement. But if you separate them enough, you won't perceive that it's moving—you'll just perceive that it disappeared from one place and appeared somewhere else. In some sense, that's very much a changing ID, like the flag. "Oh, I had a child object here and now it's gone. Oh, by the way, there was no child object up here and now it's here." I put—the child object was the mug and it's changed to—wait, that confused me. The mug—I thought the child object was the logo. No, but when it's just a mug, both the child and the parent are the mug, so there's still a change of ID. It is a change. Exactly. That's ahead of me there.

That's an interesting point, because maybe that's why with a lot of UIs, you usually simulate some movement. Maybe it's easier to do this kind of movement change instead of actually having to learn a change in object ID at a location. The brain wants to detect movement. That's why when you show two objects and then turn off one and turn on the other, you often perceive it as moving. You'll say, "Yes, it moved, I saw it moving," even if it didn't move. Your brain wants to believe it's moving.

but if you separate them enough, it says, no, it didn't move. This just reappeared. You could also disrupt the perception by, for example, turning off the bottom logo and then half a second later turning on the upper logo, or having them overlap—bottom logo on, then for half a second the upper logo appears, then the bottom turns off. You can disrupt the whole concept of movement. It's really about changing; you have a model, and at any point, a new child object can appear and another can disappear, but they're not moving physically.

This could be a weak argument for communicating the child object's location relative to the body, because then you don't have to detect optic flow in the sensorimotor system; you would just detect a change in location relative to the body, which doesn't have to be continuous—just a change. It feels like both approaches could be useful. I went down a real rabbit hole thinking about whether we have to incorporate both egocentric and allocentric reference frames—the "what" and "where" pathways in the brain—because when manipulating things, you need to know where they are. I got overwhelmed with that, but then realized the problem I'm trying to solve is predicting what an object will look like after it executes some behavior, and that doesn't require knowing where it is in the world. It's all allocentric; it's just about the object itself, not its location. I'm attending to it, and introducing body-centric concepts is daunting, but maybe "shared coordinate system" is a better term.

At any given point or in any part of the cortex, it might generally use allocentric or body-centric frames, but it doesn't really matter. I think I can solve the subgoals I set out without using the other reference frame, the common reference frame. I would prefer to have a problem or task that absolutely requires a third-party reference framework, and then use that, because it's a difficult concept. Two points: first, it's straightforward information that would be available in every cortical column. We get sensory input, and depending on the system, in Monty and how the cortical messaging protocol is defined, we communicate in a shared coordinate system. Sensorimotor modules sense relative to a common reference frame, like the body, so every learning module knows where the sensorimotor module is relative to the body. If it wants to output where the object is, it's straightforward: take the location of the sensorimotor and add where you think you are on the object, then output that. That would be a perfect way to stay within the cortical messaging protocol and have the object location in the body-centric reference frame.

The issue with the brain is that if we only communicate movement and not locations relative to the body, the movement information doesn't contain this. Still, for the brain to interact with the world, it needs a representation relative to the body. Maybe that's just in the "where" pathway, but if we assume we get that information as input to the column, it's easy to output it. It's interesting that you already have it in the learning modules. I made a conscious choice this time not to show the layers of the cortex and the connections, focusing on the concepts, because those are important. This could be a great example where the concepts and implementation differ; maybe the brain has "what" and "where" pathways, but maybe we don't need that here. Maybe you've already addressed it by having learning modules with a shared reference frame, which may be more efficient. That would make my life easier if I never had to think about "where" pathways in the brain. We just focus on the problems we're trying to solve, come up with the right solution in the framework, and implement them in the most efficient way.

That would be great.

So maybe that's the approach I'll take from now on: don't think about "where" pathways.

I think for clarity, it's always helpful to return to the idea of a shared coordinate system, so it doesn't matter much whether it's body-centric or egocentric. I don't know what's happening in the brain, but there's reasonable evidence that both exist. To your point, how important is it that both exist? Is it a quirk of evolution? Monty today doesn't do everything exactly as brains do; there are many differences, and that's fine. The principles really matter, and we've established good principles so far. As long as we stick to those main principles, the actual implementation won't matter as much.

It sounds like we agree that it's reasonable for location information to be passed up, and if we find it's helpful or necessary, that change in location could inform the behavior models. You wouldn't have to rely solely on direct sensory inputs. I think our robots will need to quickly detect changes in the sensory periphery and attend to them immediately, just like brains do. If something unanticipated happens, we need to respond quickly because it could be dangerous, like falling or being hit. In a realistic lower sensory region scenario, the higher column would have an easier time detecting movement of the child object from the sensory input than the child column detecting its own movement. This approach becomes more useful higher up in regions where we lack direct sensory input and operate in abstract space. I agree with that, but we can't eliminate the sensory aspect. Ultimately, we'll need it, especially in fast-moving environments. If you get slower, danger increases—falling, collisions, accidents.

This is exciting and feels like real progress. I was wondering if it would be useful to talk through the example you gave—we could use the stapler or the logo that rotated on its endpoint.

I'm still struggling to visualize inverse movement. It's clear when something moves up and down, but if it's rotating and moving at the same time, visualizing that is challenging. I didn't discuss how you go from learning a behavior model to applying it. We know there are problems with learning behavior models quickly, even though we have a concept of what one looks like. How do you generate the correct movement command from a behavioral model at any moment? I didn't address that, and it may not be hard, but it's another complication. That's what you're asking about, Neil. There are many details we haven't walked through, including how the behavior model generates the correct movements at any point in time for any module.

Vivian might have the answer.

I was thinking that we have local movements stored in the behavior model—small errors or local movements of the child object. The behavior model specifies, at some point in time and location on the parent, that the child is moving in a certain direction and velocity. At that location in the behavior reference frame, we have a stored movement. These errors don't all move in the same direction; they can rotate and move differently. At one location, we have a local movement stored, which is the expected object movement—what we expect to sense as input or what we sensed when we learned it. We then have to invert that movement, take the opposite, and apply it to that reference frame to compensate for the movement in the child object's reference frame. It gets more complicated if there's scale, as we also have to compensate for that.

Here's where I get tripped up: I'm looking at one location and say the child object should be moving, but all the locations are moving—maybe the whole logo or stapler top is rotating. I'm only sending the movement command for the one point I'm looking at, which only tells the learning module in the child object where its sensorimotor should be. If I move to a different point on the object, the learning module hasn't received those movement commands. How does it know where it's supposed to be?

Imagine looking at the stapler as it moves slowly upward. If I look at the bottom, I'm not looking at the top anymore. When I return to the top, I expect it to have continued its movement, but my learning module wasn't sending movement commands during the time I was focused on the bottom. This relates to whether we path integrate through behavioral space—do we sum all the steps, and is that displacement applied in the top-down connection? At a single point in time, a single solution isn't sufficient. If you had many learning modules covering the entire parent and child objects, all running simultaneously, then all would be sending the correct movement commands at any point in time.

The problem really comes about because I was thinking of a single column or single learning module. I was thinking of it working similarly to the occlusion example. Even though we have partial occlusion of the optic input, the behavior model still keeps stepping through its sequence, sending movement commands and moving through the space, even if we're not getting the feature inputs at that moment. If this is a single learning module, it's only sending movement commands for one spot on the child, but all parts of the child are moving together. There's no concept of togetherness here. That leads to the fallacy of thinking, "Oh, this is an object that has a bar or a line," but really, you have to do it on a point-by-point basis unless we argue that if there are multiple parts moving independently, those are broken up into multiple child objects. That would be like a temporary attention mask in the beginning, and later, as Neil says, if they're moving independently, they'll be represented as independent children.

If your sensorimotor system moves between different objects, maybe the individual columns don't immediately switch their representation to the different objects. The column that was representing the stapler top keeps moving through the stapler top space and where it would be if we moved back there. Imagine I have a single column looking at a point on the stapler top. If I focus on that point, the behavioral model will tell me that as the stapler top moves through my fixation point, I should be moving relative to the stapler top. If I move my sensorimotor focus to the other end of the stapler top, that end hasn't been doing path integration because it hasn't been getting any movement commands—nobody was looking at it. That mixes up rotation and location. When we have the opening stapler, it doesn't matter if it's location or rotation; it's the same problem. This model ensures that as we are at the top of the stapler and the stapler is opening, we keep predicting the features at the top of the stapler, but only if the column is looking at that point. If no learning module is attending to that point, nothing is happening in that learning module. The only thing the learning module knows is where it's looking.

Maybe one way to phrase this is: imagine you come across a stapler that's already halfway through opening. The first thing you would do, because of the movement you've observed, is infer that you are at a point in the behavioral space—at one of these points where the dashed lines are. If it was moving, you would infer that you're in the middle of the behavior. If it was just stopped, you'd say, "It's stopped, better learn this morphology," and start learning that morphology. You'd probably be able to say something about being halfway through the behavior.

That might involve some sort of path integration. But isn't there a time signal, a global signal, telling you this? Even if you know the behavior started, the time signal is proceeding. If you never attended to the top of the stapler while it moved away, and you were staring at the bottom, when you go to the top, the state of the parent column has already changed because it's getting a time signal at the end of the sequence. That tells it where it should be in the sequence and tells the child object to look farther to see the top of the stapler. That's part of the solution, but not the whole solution, because if no column is looking at the moving point, the system relies on path integration. These movement commands have to be path integrated. If I'm not sending the movement command to some portion of the moving object, it can't path integrate. I don't think we're necessarily very good at that either. If you're not looking at something and a behavior is happening somewhere you're not looking, you have to actively attend and mentally simulate that behavior to predict where it will be when you look back. I don't think so. This would all work if I had a bunch of learning modules and attended to all the ones covering every part of the object. I would activate the learning modules currently over the stapler top, and the system would work. But if you only had one learning module and could only look at one part of the stapler top, I don't see how you would know where to be when you move to the other parts.

I feel like I need to see it simulated to convince myself that it works.

Like you say, Viviane, maybe in the worst case, in a single or few learning module situation, we can only do it if we're mentally stepping through the behavior.

Maybe we need a better term than "half" when we talk about this. If you're listening to a song and it suddenly stops for five seconds, and you want to predict the next note, if you hadn't turned off the volume, it's really difficult. You have to keep counting or singing the song in your head to know what it will be when you unmute.

I keep coming back to the idea: can you do this with a single column, or do you need many columns sampling the whole object at once?

Imagine I have a single column, and I've moved it over the object and inferred it's a stapler. Now I'm looking at a point on the top of the stapler, and it starts to move. That might be sufficient on its own to invoke the stapler top movement behavioral model. I can now infer the behavioral model because that's the only thing that fits a movement at that location in that direction.

Therefore, it must be that the stapler is opening. So I've inferred the behavioral model.

Now, what if that column just stays on that part of the top of the stapler and follows along? Or if it doesn't move—the point is, it doesn't look at different parts of the top. If it followed along, it would say, "Okay, now I've made this correct prediction. Now the stapler top is open." My point is, my location is sensing the part of the stapler top. Could I then infer the entire object from that? From that one observation of being at point X on the stapler top at this angle, could I infer the rest of the object? It could be voting, or—what do you mean, sorry, the last bit with "infer"? Because the opening description was that we had inferred the object. Do you mean we predicted where the features are? Or right now, we want to predict—the object is moving, and we want to predict what I'm going to see after it's moved. I think that makes things a little more complicated, perhaps. I'm looking at a novel stapler, and it may not even be the same size or shape as the original stapler, but I recognize the behavior and say, "Okay, yes." Now I'm trying to make predictions about this novel top as it moves into new positions, and I've never seen this novel top in these new positions. The state—yeah, go ahead.

At least, Todd, I was thinking that would be solved by the mechanism we have, because if I understand your scenario, we are still following the stapler top. That means we're integrating the movement from the behavior that's stored at that location. Only one point—remember, only one point. But it means there's nobody else integrating; it's just that one point. But it means we are still in the correct location in the child object's reference frame. We're still on the correct location there, and we're also updating the orientation of it. So we are in the correct orientation, rotating the incoming movement correctly. Because we are still in the correct location of the child object and have the correct orientation of the child object, that should allow us to make predictions about everything else again. That's what I'm saying. Then I would basically path integrate. If I moved the sensorimotor at that point in time, I would path integrate over the stapler top in its new position and orientation. You would use the sensorimotor movement again to move through the reference frame because you were in the correct location to start with. That should work again, especially if the orientation—as long as I know where I was initially.

And the incoming movements would transform by the new orientation. I think you're right. I might give up on this concern. I don't think you have enough information to know whether the stapler is opening or translating if you're looking at a single spot and observe a change. The stapler could just be flying off into space. You don't know if it's opening or if someone is pushing the entire stapler together. I thought we assumed we recognized the hinge behavior. If we're not recognizing the behavior, then, as Jeff said, if you're staring at one spot and it starts to move—okay, that's an interesting question, Tristan. If I'm just looking at a point on the stapler and that point starts moving, I can't know yet whether the stapler is opening or the whole stapler is moving. That's your argument. In that case, I think that's one of the main assumptions: to make predictions, we need to recognize the behavior. In order to use the behavior model to make predictions about morphology, I don't think it would work if we don't recognize the behavior. I don't think I would be able to make any predictions if I was just looking through the straw and didn't know what behavior I was observing. This might be an example where you actually need to have multiple columns.

But you would need them to recognize the behavior, not necessarily to—well, that might be one where it helps if you're passing the location or the change of location of the child object, because that's going to be more consistent across the learning modules, whereas each parent, if it's just looking at the sensory patch, is getting very different movement information coming in.

There's almost an assumption I didn't follow. That's fine. If you want to make it again, go ahead. I can mention it. There's this idea that the retina can detect motion. If you detect motion everywhere, consistently, then it says, "Oh, the eye's moving. The world is not spinning; the eye's moving." But if it detects motion locally, then you assume the object's moving relative to the world.

We've had an assumption in the background that we could tell the difference between an object moving versus the world moving, or the object moving versus the eye moving. Now we have to extend that idea to a parent object. In some sense, there's no way to determine if the whole stapler is moving or just the top if you're only looking at the stapler top. As you point out, we might have additional information that tells us whether the whole thing is moving or just part of it. I don't know the answer to that question yet, but there must be a solution. It doesn't bother me too much.

Going back to the shared representation of the location of the object, that's also useful for both recognizing behaviors and segmenting the object. Maybe all these LMS are saying, "Yes, the object is at this location," or "Yes, this object is moving in a particular direction." That's much more efficient if you're trying to recognize how it's moving and then communicate that up to the next level. Maybe it's also helpful for segmentation, because how does one LM know how to segment the object, like the top of the stapler, without some information about what the other LMS are seeing?

Let me try to summarize something. I was worried that we would need multiple sensorimotor modules, multiple running modules observing all parts of the moving child so they all get updated by the behavioral model. I think I've been convinced that's not necessary. We can track one location using the behavior model, and if the child object is known, we know where we are on the child and its orientation. If I move, I use standard path integration to make predictions because I have a model of the child object. It doesn't work if you don't have a model of the child object, but if you do, standard path integration will take you to the right place. As things are moving, wherever I am, I have to path integrate using the movement from the behavior model. If I move across the child object, I have to path integrate using the movement of the sensorimotor. You have to do both these things at the same time. As long as you do that, when the child object stops moving, you'll be able to path integrate the normal way and know what everything looks like. That problem is now, in my mind, sufficiently solved.

The second thing is, I'm still struggling with how we learn the behavioral model without having lots of columns. I haven't thought about that recently. We still have to address how to learn models efficiently using multiple columns. Maybe someone should come up with a battery of behaviors, and everyone has to try and learn them through looking through a straw, and then we can see if anyone can actually learn them or if it's impossible. It's almost certain that even if we're just learning morphology models, we don't sense every location. It feels like there's some sort of sharing going on. You already know that if you use the straw, it takes a lot longer. Learning seems to involve some kind of shared learning, just like we do voting for inference. Somehow we have to be doing shared learning. Then, in the inference and learning of behaviors, even learning morphology models seems to require shared learning. Maybe we have to have shared learning from behavioral models too. Maybe that's where hierarchy could come into play. It doesn't necessarily have to be shared laterally; it could be top-down sharing of a general model. Every time I start thinking about that, my head spins. 

It seems like when we learn a model initially, we don't learn in the lower regions; we learn in the higher regions. We don't learn a vision model; we learn a model that can apply to vision, hearing, and touch. It probably uses the hippocampus or upper regions of the cortex. Somehow, you learn the upper-level model quickly and then push it down. It's really confusing.

It won't be confusing in the end. It'll be very simple as we're coming towards the end of the meeting. I think we're making good progress on the problems, especially now. The last time I showed the slide, I had these gray check marks because I didn't have any idea how location changes would be incorporated into making predictions and applying behaviors to different objects. Now I feel like we have a pretty good idea about it. I would probably still leave those gray because we still want to play through the mechanism a bit more in our heads to make sure it actually works in all scenarios, but we're definitely a lot closer. I have one other new open question that I didn't bring up today yet, related to the key frames, but maybe that's something for another topic, and then also the sharing information and voting mechanism. There's another bigger one to talk through more, but I feel like we're making good progress. At some point, we have to go back to the idea of interacting with objects to create desired states in the world. I proposed the solution to that a couple of weeks ago: actions of objects are basically just object behaviors applied to scenes. So, the two or more objects that are interacting are just two features on a parent object, and those are moving. It's a behavior of how to make those things happen. How do actions come into this, or learning causal relations? For example, I pressed a switch that turned the light on. The discussion today was about behaviors that had nothing to do with my interaction with them—they just happened. I'm taking actions out of this entire topic right now because I feel like we have to do it sometime. If we solve all of this in the next two or three weeks, then we'll have a new topic for the brainstorming week.

So, we had the big question about how we do learning, shared learning—is it hierarchical, multicolumn voting, things like that. What was the other topic you mentioned? I didn't ask the question, but it's basically with key frames. If we lay down different points in the morphology model for the different states of the object, we need some way to condition the morphology model on what to predict at what point in time. Otherwise, we would make predictions about the open stapler and the closed stapler at the same time. We need some way to say, now I want to predict the points. 

For example, with the logo—the top and bottom logo—there's no preferred position for it. Somehow, you have to have a morphology model that has multiple possible learned states. Unless one happened 99% of the time and the other didn't, if they happen equally, you have to be able to infer any one of those states. Basically, you need a way to tell it when to predict which state. If I've seen the stapler open, now I want to predict that state. Maybe it's just inference, like how we infer rotation—we would have to try different states. At the moment, our morphology model doesn't have the concept of different morphologies at different points in time. We've talked about having different states. We would have to add a state dimension to our morphology models, where we try to infer which state. 

Here's a possible approach: we have a behavioral model, and the morphology would be associated with one point in the behavior model, and a different morphology would be associated with a different point in the behavioral model. This goes back to the very beginning, where I talked about states of objects. The states would, in some sense, be the point in time in the morphology model. You would be inferring not just the morphology, but also the state in the behavior model. We can learn an associative connection between points in the sequence in the behavior model and states in the morphology model. As long as we don't try to learn too many key frames, I think it's manageable. The key frame issue is less of a problem in brains because of the way they learn, but it's not required and wouldn't be a lot of association. The state is like a global variable—just a point in the sequence to a state in the morphology model. It would be a one-to-one connection, not every feature needing to be associated. When you're inferring an object, you're looking at features, their orientation, their IDs, and you also have to infer the point in the behavioral state, just like you infer orientation or location.

If you could do that, then it would work. So we can check off that question too. How are you going to do that?

It's the same way we're doing rotation. I don't deeply understand the way you're doing it in Monty today, but I'm happy for it. Great, add it to your list instead. Maybe I'll think about the shared learning problem.