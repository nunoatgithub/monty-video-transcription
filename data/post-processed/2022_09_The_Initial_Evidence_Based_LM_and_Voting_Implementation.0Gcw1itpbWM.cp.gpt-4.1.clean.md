Okay, so everything I'll present today is what I've been working on for the past two weeks. It's still pretty fresh, and there aren't complete final evaluation runs or long evaluation runs yet. It's more about what's new conceptually, what's happening in the code, and a few preliminary results, but there's still a lot of content. Many new things have been added, so I hope it's not too overwhelming. I tried to make some visualizations to explain everything. Let me know if you have any questions.

There are two topics today. One is the evidence-based learning module. It's similar to the previous learning module we've been using, but this one doesn't have a list of possible objects and poses that gets narrowed down over time. Instead, it has a list of hypotheses—objects and poses—and each hypothesis gets its evidence updated at every step. You can have more or less evidence for a hypothesis instead of just eliminating it completely.

The second topic is voting. I already shared the report from Weights and Biases, but I made a few more visualizations to explain it better. If anyone has more questions, let me know. Just a quick note on the Weights and Biases report: I think there's a way to make the report publicly available or have a public URL, since some of us don't have accounts. I wasn't sure how to do that, so I recorded it as a PDF and sent it in Slack, but the PDF version isn't as good. It inserted some blank pages. I think Louis can tell you how to get a URL you can send to specific people. Ask him about that for next time.

I'll get started on the evidence-based learning module. First, a bit about how I define an object at the moment and how it's represented in the learning module. This also applies to other learning modules based on graphs. Each point in the graph is defined by a 3D location and three pose vectors. The three pose vectors define the rotation at that point, and they are three orthonormal vectors—each orthogonal to the others and of unit length. The first is the point normal, which is orthogonal to the tangential plane where the sensor is. The other two are the two principal curvature directions, which are the maximum and minimum curvature at that point. These three vectors define the pose of that point or feature.

The normal vector is always normal, right? How does it add anything? Is that relative to the plane or a global coordinate system?

It's within the object model's reference frame and tells you the orientation of that point. For example, if you look at all the blue vectors here, those are the point normals, and they always point away from the surface.

To represent that, it's not just a scale value. The length is one, so it's a location on a unit circle, and the location indicates which direction the point normal points. It uses three numbers because it's in 3D—x, y, z on a unit circle. Maybe the question is which reference frame, and it can be attached to the object. I might be missing a basic idea here. I understand the concept, but it seems if the point normal is always tangential to the plane, I could represent it in three dimensions, pointing in any direction. The other two pose vectors are also in three dimensions. Are they relative to the point normal? Yes, they are orthogonal to the point normal.

So we have a unit sphere. All points on the sphere are distance one from the center, and it's a three-dimensional sphere. The point normal can be anywhere on the sphere, representing its direction. If we only have that one, it only specifies the direction. I'm trying to clarify what the actual representation is. It seems like the point normal requires two dimensions, and the other two are relative to the point normal. Is that right? Or should we mention how many numbers are stored for this pose vector? Each pose vector is three numbers—x, y, z—and we have three pose vectors, so it's nine numbers overall. The directions don't just constrain the orientation in the tangential plane; if you have only the point normal, it can still be rotated around itself in any direction. With the curvature directions, we constrain that. There are three independent numbers.

They’re all relative to the object's orientation pose, right? I'm confused now. Yes, they are all on a unit circle and associated with one location on the circle. What was confusing me is I thought you were saying three one-dimensional numbers, but it's not. It's nine numbers. The length of them is one, but they are in three on a three-dimensional unit circle, so they have XYZ coordinates. If it's on the unit circle, in principle, you only need two numbers; you don't need three. The third can be derived from the first two by making it orthonormal. That was the confusing part for me too. Can we think of this unit circle as being centered and fixed on the object somewhere? So it's in the object's reference frame, and each point on the mug will have a different point on this unit circle. This one would be associated with one point on this model. I think I got it. When I think about 3D location, I think of three numbers, and then you say three pose vectors. I was thinking those are unit numbers, six, three numbers too, but it's nine numbers. All right, I got it. You need different numbers because you could have different flatnesses of the curve. I got it. Sorry, just in terms of the reference frame, I appreciate that in the model it's represented in the object's reference frame, but the input from the sensory module is presumably in the environment's global reference frame, right?

Sensors from the sensor would be the curvature direction point normal relative to the sensors. So we'll have to convert that. I just wanted to know exactly what we're storing before we got too far. It's important.

Principal curvature one is always larger than principal curvature two, and they are orthonormal to each other, so all orthogonal and they span this reference frame. Here I visualized this on a mug. The color of the dots shows the principal curvature magnitude, and the vectors show the direction the curvature is pointing. The blue vectors are the point normals, always pointing away from the surface, and the red and yellow ones span the curvature directions, orthogonal to that. We store these three vectors for every point on the model, and we also store the location of all the points.

If principal curvature one and two have approximately the same magnitude, for example on a flat surface or a round ball, the curvature directions are meaningless and could point in any direction. In these cases, we disregard them and only use the point normal.

Their values are important, even if their directions are meaningless. The magnitude is used, but the directions are not used to specify the pose of these features.

Now, getting to the evidence-based model and how it works. Are there any more questions about how the morphology is defined?

We have the first step and then all the following steps. The first step is unique because it's used to initialize the hypothesis space. At the first step, we sense a point in space, and for every point stored in the model, we determine possible locations if we were at this location. For example, if I sense a point normal in space, I can say if I were at the top of this can, the can would be at rotation zero. If I were on the side, it would be rotated by 90 degrees or negative 90 degrees. If I were on the bottom, it would be rotated by 100 degrees. We do that for all possible objects. That's a large space, but it's smaller than if we tested all possible rotations for all points and locations in the model. It's still an informed hypothesis space. When you say possible rotations, is there a fixed number of rotations you're testing? If I have the point normal and the curvature directions, that's basically two rotations per location: zero and 180-degree flip, because the curvature directions could point either way. If I can't use the curvature direction, for example on a flat surface, I have to sample possible poses along the remaining axis. That's a hyperparameter where we can set how detailed we want to test. If an object was rotated at 12.3 degrees or something in between, it would snap to the nearest one we're checking.

If we have the curvature direction, we can initialize it very precisely.

Is there a simple way to specify a policy where you ignore input features that don't have a fully defined 3D pose and just continue? That's something I have as a to-do in the code. We'll have to handle the case where an object only has that—if we just have a ball, we can't write directions anywhere, but that's a special case. For all other objects, it makes sense to first move to a place with curvature directions and then start initializing your hypothesis space from there. Even though it might take more steps, the overall time might be quicker because you're focusing on informative features. That's definitely something you see now. If we start on a flat surface, like the top of the can, all following steps take longer because the hypothesis space is larger and we couldn't constrain it with the curvature direction.

Any other questions so far?

So now, the next step is testing hypotheses. At the first step, we initialize possible locations and rotations. Now we use the incoming sensed features and displacements to test these hypotheses. We loop over all possible hypotheses. First, we calculate a test location by taking the location of the hypothesis and applying the rotation of the hypothesis to the displacement we just sensed. We sense the displacement, rotate it by how we think the object might be rotated, and add that to the location we think we were at before the displacement. That's the search location. We also apply our hypothesized rotation to the sensed pose features, such as the point normal and curvature directions.

This is our query. Then we go to our model and find the nearest point stored in the model to the query location within a given radius. For instance, this radius is 0.1 or 0.01, which I am using in most experiments at the moment. I have a query location, and then I look within this radius to see which points are stored in the model. For these nearest points, I calculate the error and from that, evidence for the hypothesis. If I have any points in this radius—it can happen that I don't have any points if the hypothesis is just absolutely off—I calculate the angle between the pose features stored at these points and the sensed pose features. What's the angle between the point normals I sensed and the point normals stored in my model? That's the pose error, and I add this to my hypothesis evidence. That's the evidence for the object morphology. That error is not just the point normal vector; it's all three vectors, except if the curvature directions are not specified.

Optionally, in the current setup where features and morphology are separated a bit more, we can add additional evidence for features that match. I can look at all the points in this radius and check if they match with the features I'm sensing, like color or the magnitude of the curvature. Both of these are weighted by distance from the query location, so if a point further away matches, it weights less.

The curvature magnitude is just the feature itself, not included in the three vectors earlier, which were just the direction. So, for example, if I had a saddle, all saddles would be equivalent regardless of how severe they are. The severity of the saddle is considered more of a feature because it's independent of the rotation.

When describing the hypothesis, it might sound like you're rotating the object as opposed to moving the sensor. Actually, I'm rotating the displacement that is being sensed. It could be the finger moving or the object moving; it's not the same thing either way. At the moment, it's the sensor moving, so the sensor moved and that displacement is what I'm rotating. The language I used earlier may have been confusing.

The evidence is a scalar value, a float. The pose error can be up to pi in radians, and we have a tolerance value. The naming convention is because I'm keeping all the parameters from the previous learning module, but this tells us when the evidence is positive or negative. For instance, if the pose error is below 30 degrees, we add evidence; otherwise, we subtract evidence because the pose error is too high. The evidence being added here can be negative. There are a lot of hyperparameters in how you accumulate this evidence. Maybe "bias" would be a better term than "tolerance," since you're biasing the evidence a bit.

Here's everything that goes into the evidence updates. Regarding the previous slide, let me explain the feature match on the last line, the optional part. It calculates a distance-weighted error between the sensed features and the features stored at those points in the radius. At the moment, it's just a binary check: for every point in the radius, do the features match or not, using the same tolerance values as in the previous learning module, and then weighing it by distance. If there are a lot of points in this radius with similar features to what we're sensing, it's more likely that it's this object. If all the points in this radius have matching features, this will be one. If some points on the outskirts don't match, it subtracts a little from one. If the points at the center don't match, it will be a low number.

There might be different objects with different speeds of change, like adversarial objects with different colors at every point or extremely wiggly surfaces. This kind of evidence update might be object-dependent, but that's a nuance we can address later. That's definitely a complication to keep in mind.

Regarding the pose and the feature match, since there are different scales, how do you balance the two? At the moment, that's also a hyperparameter, and I'm not sure yet what the best values are. There is a weight parameter for the feature, determining how strongly you want to weight features into the evidence. Currently, it's set at 0.5 by default, so this value will be between 0 and 1, and then it will be multiplied by 0.5. The pose error can be between plus pi halves and minus pi halves. I've been experimenting with different parameters there, so those are all things I'm still testing and adjusting.

Jeff has a question. The one time we were talking about explicitly storing displacements, and it sounds like in this model right now, you're not doing that—you're just calculating on the fly. Is that correct? That's correct.

I'm a little surprised. I don't quite understand why the curvature is considered a feature. It seems to me we're trying to separate out the models into two things: morphology and the features that might be associated, like a specific sensory modality feature such as color. It seems like the curvature is part of the morphology.

Am I wrong about that? That's true. In this case, the categorization is looking at two different factors. Morphology includes everything related to morphology, which would include the magnitude of the curvature. Here, I was defining the pose features as features that depend on the orientation of the object. If the object rotates, these features will be different at the same location.

Is that an important distinction for you, or is that just something you did right now? In the code, it's important because I have to apply the rotation from the hypothesis to these features. I have to rotate the point normal and curvature directions, which I don't have to do with the curvature magnitude or color. I agree we should still make a distinction between the curvature magnitude and things like color or texture. If you have enough point normals—just the one vector—that defines a morphology too, right? You don't need the curvature component. Is that right?

Yes, this works completely without the last line. It still recommends objects. Is it fair to just have structural features and non-structural features? It's still features at points in space, but that's at least how I've been thinking about it. One way I've been thinking about this is that across modalities, like between touch and vision, and even within vision, there are different submodules—some get color, some don't. The morphology component is common.

I can learn the morphology of it through touch and recognize it through vision and vice versa, but the actual sense features may not transfer. I can't feel the color or see the texture of the temperature. The morphology is something we can vote on across modalities. I'm trying to make a clean separation between things that define morphology and things that are additional components. Here, you could recognize the object just using the point normals. You've decided to put the curvature as a feature, but it's not even necessary to do that, right? You said you could recognize the object without it.

Maybe in the future, I might want to rethink this, but I think I got my questions answered. The reason I made the separation is because everything I have in the features here are things that I do not have to rotate depending on what I think the object's rotation is. I agree it's not very nuanced yet, and it would be nice to add more options where we can weigh different features more or less. For example, we might want to focus on curvature and not on color, or not at all. There's an option to not look at color at all; one learning module may know about color, another may only know about curvature, and they can still vote and communicate.

What are the set of hypotheses in the beginning? It's locations and poses at those locations—the rotations at those locations. In the beginning, it's every possible hypothesis: all the locations stored in the model, and for each location, a set of possible rotations.

That's a pretty large outer loop in the beginning, then. Yes. There's even an extra loop over possible objects too, right? Yes, exactly. I'll get into more details in a bit, but since we have an evidence-based model here, we don't need to test every hypothesis at every step. There's a hyperparameter to say, "I only want to test the most likely hypotheses," or only hypotheses that have evidence above a certain threshold.

It seems like the evidence should narrow down the hypotheses, even after the first touch, right? As opposed to the old learning module, you can still come back to a hypothesis that you previously had. Even if I have a bunch of very likely hypotheses, if I then get a lot of negative evidence for these, I can still come back to a less likely hypothesis that I previously had, if that makes sense.

Would it help to have different features with different weights? For example, if the curvature is the same in both directions, that would have a very low weight, but if you had a very sharp edge with high curvature in one direction and low in the other, that would be a more unique feature you could assign a higher weight to. You could weight those points more heavily than the ones with less or more ambiguous features.

Yes, this is one of those nuts and bolts I'm still trying to figure out. There are so many moving pieces and parameters I'm trying to set, but that's definitely one of them.

I'll go on so I can cover everything. It's interesting to brainstorm. It would be interesting to look at the Hough transform approach for handling hypotheses. It changes the loop a bit; you never have the four agent hypotheses. Instead, the evidence indicates which hypotheses are likely, and at any point, you have a distribution of hypotheses.

Maybe I didn't understand that fully, but it sounds similar to what we were considering with displacements. It's basically what the columns paper did too. That's what capsules do as well. One way to improve this is by saving certain displacements that have been observed repeatedly. Those would be logical things to test immediately.

You wouldn't move randomly. There are two models: one of reference frames and a graph of where the features are, and another of stored displacements of components. You can't store all displacements because there are too many, but you do store some, and those are the ones you've seen consistently in the model. Those are the things to test right away. For example, if I see your eye, I don't just randomly look around; I'd consider three hypotheses—nose, eyes, and mouth—and choose among those directions. These are things we could discuss. That's a way to narrow down the problem, which is otherwise very large. Another way is to consider that we might be learning morphology at different levels in the hierarchy, with coarse and fine morphologies coexisting. Coarse morphology is another way to quickly narrow down hypotheses.

We have to come back to that. I'm trying to keep all those ideas in mind or on my to-do list.

To wrap up, things that influence the evidence value include the morphology error—the angle between curvature directions and point normals can add or subtract evidence. Pose-independent features can add evidence, and that's weighted by a factor, so we don't necessarily need to add those in, but they could help with faster recognition. There's also an evidence decay factor, which subtracts evidence at each time step so it doesn't accumulate forever. The most recent evidence from recent observations is what matters, and the evidence decay currently looks like this function: if the evidence value is negative, it's pushed toward zero. If one hypothesis has negative evidence, the decay makes it less negative until it reaches zero, then nothing happens. If it's positive, it goes down.

Is it subtracted or multiplied?

It's added in—just plus evidence.

This whole thing is called an evidence learning module because it's not a formal probabilistic framework. It's more like a history of past evidence accumulated for different hypotheses. The evidence values don't add up to one, but they give a good idea of the most likely hypothesis at each step.

It has other useful properties. What's not yet implemented is voting with evidence, which would add or subtract evidence. Also, as you mentioned, Jeff, if we recognize common displacements stored in the model, like moving from eye to eye or eye to nose, this could add evidence for certain locations.

I'll go through one example to visualize it. This is the first step: we're sensing pose features—three vectors. This is just a zoomed-out view for us; the learning module doesn't get this view. This is the sensor patch that the learning module sees.

At the first step, we initialize the hypotheses—the first possible poses. We haven't moved yet. Now we've moved; the red dot shows where we started, and the black line is the displacement we sensed. The other three vectors are new pose features that were sensed. On the actual picture of the cup, we started in the center and moved a little bit over. Actually, we moved down; that's the movement. That gives a sense of it.

We use this displacement and the new pose features—no other features like color—to update our evidence values for the hypotheses. Some points moved off the object; that's when the hypothesis was, for example, starting at the corner of the can and sensing this displacement. Now, I'd be out here, with no nearby points stored in my model, so I decrease the evidence value. This starting hypothesis now has negative evidence. 

How do you know the cup hasn't been rotated or is in a different orientation?

I'm rotating the displacement, which is why the points can move off the model of the object. The hypothesis could have been here, but since you don't know the orientation, you moved in that direction. I sense something, but I don't know where I am. I can say there are impossible points on this object, but I don't know the object, its scale, or its orientation. So I move and end up off the object, but I can't eliminate that object because it could be in a different orientation or scale. Each point shows all the possible hypotheses that remain. All the hypotheses include all the orientations. There are a lot of cans not being considered. That's why my high-level point is that for agent hypothesis, it's a very large loop.

I'm not rotating the whole can. For each hypothesis that I could start with—each of the gray points on the can—there is also a possible rotation. For example, if I would be on the top or bottom of the can, the rotation would be 180 degrees along the x-axis. If I test this bottom can hypothesis, I take the displacement, rotate it by 180 degrees, and move from that point in the model reference frame. Then I check in my model reference frame if I sense the correct features at this location. That might be completely off, which is where the blue points are. All those different blue dots also represent the different orientations of the can. 

If you do one touch, like you did, how many hypotheses are there for the can? In this example, it was around 3,000 points stored in the model and around six to seven thousand hypotheses being tested, except if we start on a flat surface, then we're testing more than that. It's a big number, but we have ways of reducing it. You can set it so you only test the most likely hypotheses. Here, I'm just updating all of them for visualization.

We move again. In this case, we actually moved back to the starting location, so all the hypotheses are back to where the model locations would be. Nothing is off the object anymore, and we couldn't really add any evidence. We didn't really learn anything from that. Now we move to the left and update hypotheses.

While we're moving at the bottom of the mug, we can't resolve much information. It matches with all of the models just on the bottom of the cup. Now something interesting happens: we move from here to here, and the cup model in the YCB dataset has a weird shape on the inside. It goes up and then up again. I've never seen a cup like that, but that's how it is. They actually took real objects and scanned them. We're going up to the first fold on the inside of the cup. This black displacement went out of that bottom plane and went up. Now, a lot of hypotheses are receiving negative evidence and many are going off the object. For almost all objects, this sequence of displacements doesn't match any possible locations and rotations. Only on the mug do we still see some red dots that still match up.

These illustrations are amazing. In the Jupyter notebooks, you can also rotate them. Maybe it's a simple tool, but I've never seen anything like it. It's very helpful.

I'm using it to debug everything. It's very helpful.

Every little dot you see here says if I had started with my initial hypothesis, I would be here now, given the sensed displacements.

Then it checks: if I'm here now, does this match my model of the object? All the blue dots mean no, this does not match my model of the object. That's why they have negative evidence here in the cup. In some cases, it matches—on the top of the rim, for example, if we started on the first fold and moved up, or on the inner fold of the cup if we started in the middle and moved up. If you move and predict that movement would be outside of the object, that would be considered positive evidence.

If you predicted you were outside the object—off the object—yes.

At the moment, we're not processing observations that are not on the object. The policy just moves back on the object and then continues. In the long run, we could use this information, saying, "Okay, I should be off the object now." That makes things harder because you could be collecting evidence all day without being on the object. That's true.

Especially the ceiling—that's good evidence.

Sometimes it might actually be a quick way to narrow down between two possibilities. Negative evidence is a quick way of proving something, but you don't want to make the space too big, or your search space becomes huge. 

How are you determining your next movement? Is it just random at this point? Yes, at this point it's random. Only if we move off the object do we move back on it; the rest is random. For example, if you're looking down at the cup, these are the remaining points that are still consistent with previous movements. Each one has a set of neighboring points they could move to, and you could command that movement to further narrow down which one would satisfy the criteria. That's one of the main advantages of this new learning module: we can easily use it to write much more efficient action policies that move to test hypotheses.

I think if you consider touch, and to some extent vision, we don't make random movements. We typically follow an edge or a surface and naturally do that. You would stay on the surface of the object, and all your displacements would be valid on the object. That's a simple action policy: just build a system that traces the edge. This is a really basic system, and the action policy is definitely not very good. Also, we can't move around the object, so many views are ambiguous, and we can't easily resolve symmetries in this example. That's in my next steps—to have more efficient action policies. Let me finish going through this example. We keep moving around, and there are still some possible hypotheses for the mug's pose. It's already saying the most likely hypothesis is marked with rotation zero, zero, and we have an evidence value for it. The nice thing in this learning module is that at every step, we have a most likely hypothesis. In the beginning, it may not be very accurate, but we still have some hypothesis at every step. The ones that are still possible are also symmetrical, which is something else being tested for. We're testing for symmetry, but it's not very robust, which is one of the main problems right now. In this case, we detected symmetry and that these poses are symmetric, reached the terminal condition, and classified this as a mug.

What does it mean that they're symmetric? It means that from this view, all the rotations of the mug look the same. The only way to resolve that ambiguity is to look for the handle. For many objects in the YCB dataset, there's no way to resolve it—like the bowl, which is inherently symmetric around one axis, and many objects have at least one axis of symmetry.

Overall, from initial tests, this learning module seems a lot more robust.

One problem is that it reaches timeout very often now, especially with symmetry or ambiguous views. For example, if we look at the mug from the side, it reaches timeout because it could be the mug, the can, or the side. This is just one run with different objects. Sorry for not having nicer results yet, but for almost a thousand steps in those two episodes, it said mug and Master Chef can are both consistent with my inputs. Wouldn't that have been solved if you moved further? You can't in this setup—the action policy only tilts the camera up and down; we can't move around the object. So you can't determine the difference between those objects at all. I guess if you move to the top of the mug, you could, but it didn't do that in the random policy. This is again due to the random policy. If you had a chance to connect it, it's like being in a field of grass, walking in a circle, and not going to the edge to see there's a road or a tree.

Have you had a chance to connect it to the finger? Doesn't that actually go around? I did. It performs better than the old learning module with the finger. I didn't get to run more than one experiment yet, so I don't have conclusive results, but one problem with the finger is I didn't get very good object coverage during learning. Some parts were not really explored, and I think that's where some errors came from. With the different views, you're densely sampling each view, so it's more systematic, while the finger is just trying everything. Matching is a lot more recognized with the finger, so it's more robust to sampling new points on the object. Before, it only worked well if we sampled the same points as during training, but now I have some results. Before, you would throw out a result if it wasn't a perfect match, but now you're claiming evidence with hysteresis and can blur over incorrect data points. Before, if we got one inconsistent observation, it was out and you could never go back to that hypothesis. Here, we can get inconsistent observations and still recognize the object if it's still the most likely hypothesis.

You don't necessarily mean robustness to noise. We haven't put in noise so far—just robustness to sampling. Is there an outside significance on the first sensation, in the sense that if a hypothesis wasn't initialized on the first sensation, you'll never consider that starting point? That's one problem. If you could uniformly resample, that would help. Why would it be that way? Wouldn't you, as part of your evidence, say, especially if your search space is still large, that you'd want to go back and reassess your evaluation? Like when Neil says you always start over again, or at least consider that possibility. I'm thinking about going back to initializing with uniformly sampled possible rotations and then assigning evidence to the ones that match the first observation. One reason I've done it this way so far is because we can really get the pose to half a degree if we want to, or if we have a good initialization. If we have to uniformly sample, even in 45-degree increments, that's already more than 200 possible poses to test. It's a combinatorial problem that makes it difficult.

If the first starting position was adversarial for some reason, it's a niche situation that can be dealt with later. I don't think it's a new situation because there are many things in the world where it takes time to start tracking at all. The extreme example is the Dalmatian dog in the park, but also when listening to a song, there are many notes before I begin to recognize something. Once I'm into it, I start catching something familiar. There are many situations where we could be confused for some period, and my initial observations may be useless.

The way I think cortical filters deal with this is to occasionally sample uniformly from the hypothesis space, and you can set how much you want to do that and bias that sampling. Two things I was considering: using the point normal to inform the poses, but for the curvature directions, which are much noisier, do more sampling; and using incoming votes to add more possible pose hypotheses, so if other learning modules suggest slightly different poses, we can consider them in the next steps.

That would be helpful, especially in cases like occlusion, where some parts of the retina have seen something and others haven't. Let me continue, unless there are more questions for now.

These experiments used just the morphology. The mug obviously has a very different color than the can, so if we use the features more, we can recognize those as well and avoid timeout conditions like the two timeout episodes just shown. That's a case where features like color can help.

Here, we are better able to deal with new sampling. This is the experiment described in the README, where we have new sampling by moving a different step size and learn that. We move in different increments over the object, and now we get 100 percent accuracy, whereas before it was about 75 percent. It's a small experiment, and I need to run more experiments to verify this. For noise, I'm not specifically testing that, but I assume it should be more robust to noise. That needs to be tested. Are you dealing with scale changes here at all? No, right now this is all at a fixed scale.

Other advantages are that at every step we have a most likely hypothesis; we don't have to wait until we narrow down the possible matches to one, but we know what's most likely at the moment. We have a rough idea of the likelihood of different hypotheses compared to each other. It's easier to use this to inform actions and to make more efficient action policies. It's a good representation for voting. I can talk more about that later. I think it could be used for generalization and categories, because even if my mug is slightly differently shaped, it should still have the highest evidence compared to other objects that are not mug-shaped. That still needs to be tested; that's speculation.

It opens up possibilities for using priors, like initializing the evidence so not everything is zero—some objects may have prior evidence based on context. It also allows for more customized mixing of features, morphology, and other evidence sources like displacements.

Those are the advantages. Next steps include adding voting with this evidence-based learning model. Can you explain why you think this is a good representation for voting? What are you thinking there?

I think it will be easiest to explain after I go through how the voting is working right now. Are you going to do that? Oh, I thought you were wrapping up. I'm wrapping up the first part. One thing I'd like to see is a noise analysis—noise in the pose estimates, movement displacements, and so on. It's not clear how well this would work. I mostly ran these experiments today and definitely want to test that. It's important to apply this to real-world sensor data and have at least some noise in the sensory input, movement, pose estimates, and curvature estimates. So far, I'm still working on getting all the hyperparameters and code right, but that's one of the next steps to evaluate.

Using voting and multiple learning modules should also help with noise.

Another major point is to have more efficient action policies, which should help with timeout conditions by allowing us to move to locations that resolve the most ambiguity and efficiently explore the object. Short-term code tasks include figuring out the right thresholds, terminal conditions, symmetry conditions, and so on. Can I add to your major list? We talked about these, but I want to make sure we don't forget them: using displacements to improve our action policy, and a hierarchical representation of morphology, which we haven't discussed but is important. Just don't forget to put those as placeholders. That's a good point. Right now, it's still doing a lot to represent very complex objects.

For the displacement aspect, that necessarily requires intelligent action policies. No displacement will be more likely until there's a reason for it from the bottom up. You could learn it by seeing which things are consistent.

There are various ways you could learn it, so I don't think it's random. Maybe I misunderstood your comment. I see what you're saying. I guess I just meant that for it to have a big effect, it would be because the action policy tends to sample in certain ways, and you can leverage that. Exactly. Whether that's bottom-up, going from saliency area to saliency area, it would mostly be that repeated, consistent patterns get stored, and inconsistent ones are set aside.

It's pretty easy to do in the neurons. It's good enough—just put them in there soon. I think those will be important to solve this problem, especially the hierarchical point. I'd really like to get into that more soon. I've only thought about it a little. The idea is you have a model at one level that's very coarse, and at each point in that coarse model, you could have a refinement below it. That would be an interesting topic for a brainstorm because, at least in my head, it's not clear how you practically create a coarse model that ties into what you're actually observing. Let's put it on a list for brainstorming. Schedule it.

Another issue at the moment is that it's still a lot slower than the other learning module. I already changed a lot of code to use matrix multiplications and got more than a 10x speedup, but it still takes about three seconds per step if we are testing all hypotheses. If we don't test all, it's faster. Adding more intelligent action policies and voting will definitely help, but perhaps there are also ways we can make the code run faster. I think we should focus on the former rather than just speeding things up, since that's not the real solution. It's just a little slow to run experiments. Are you able to use the meganode and run 200 in parallel? 200 cross, things in parallel.

As of yesterday, that should be working. That should be like a 200x speedup. We have 256 cores, and I set the limit to 128 right now. We can change that if needed. Another interesting thing would be flipping the loop so it's more like a Hough transform, rather than explicitly checking all hypotheses every time. I'm not exactly sure how to do it, but there might be some potential there. I'll have to look into that more. What do you mean by that, Subutai? I'm just trying to picture it. That's probably a longer discussion, but the Hough transform approach is that you have the space of hypotheses, and you don't explicitly go through each one. As you get evidence, you accumulate it at the points in hypothesis space that are consistent with that evidence. At any point, to make an inference, you pick the points with the maximum value.

That's what we did in the columns paper. It's similar to the columns paper, and capsules definitely use that intuition. I'll have to explain it to you—it's a longer discussion. I'm also not an expert with transforms, so I have to look into it. One thing that's holding me back from using more matrix multiplications for these hypothesis tests is that we have to rotate all the displacements by different poses and then find the nearest neighbors in the model. That can be difficult to do. I'm not sure how it would work with a transform, but it's just a different approach—it flips the loops.

Maybe it won't be a small thing, but it's another possibility.

Now, another point: we have one bad point here. The last point is adding a real-valued feature error instead of just binary, and balancing the different evidence sources, perhaps adding in displacements like recognizing common displacements, and figuring all that out.

Now, I would go to the voting, but I don't know how you feel—if you have more questions or if that was already too much information.

We can look at the votes. I go voting. You vote for voting? I vote for voting. Are you ready, or were you hoping we'd say no? I was hoping—are you ready for voting?

This is also implemented, but only for the old learning module, not yet for the evidence-based learning module. But it should be the same principle. I just don't have any results on the evidence-based one. It should be the same principle. The idea is, we have multiple sensor patches—they may be at different locations, different sizes, different resolutions, different features they're sensing—and each sensor patch feeds into a separate learning module. These learning modules float with each other. Each learning module has its own model of the object; the models may be different, storing different features, with more or fewer points stored in them. Each learning module has its own model of the object and gets its own patch of sensory input. The vote contains a vote on object ID and a vote on poses and locations. Make sense so far? Also, we would include scale, but we're not doing that yet. She has different scales of features, but those are different scales of the learning patches, not the objects. In some sense, when we vote, we do a flash inference: we know what the object is, where it is, its scale—basically how far away it is—and its orientation. Those are the things we seem to vote on.

So, we don't have to handle scale yet. We're not doing scale—just orientation and ID. There's a placeholder for scale, but since we're not detecting it right now, it's not being voted on. For now, voting is on object ID. This is just a first attempt and open to suggestions. Are you going to vote separately on object ID and orientation, or at the same time? Is there one representation or multiple?

They interact, but the mechanisms are slightly different. I'll go through it. For object ID, each module sends a plus or minus vote for each object model it knows about. Incoming votes to a learning module are added up, and if an object's minus votes outnumber its plus votes, it's removed from possible matches. One issue is that a learning module can't completely rule out an object on its own. It's not eliminated with just one minus vote; the minus votes have to outweigh the plus votes. This could be a problem if one module is really sure it can't be this object, but it can't eliminate it alone. Not all learning modules need to know about the same set of objects. For example, learning module four doesn't have a model of a cup, so it doesn't vote on the cup. They all have different ideas. In this example, the cup is the target object. Learning module zero got some inconsistent input from the cup, so it has a negative vote for the cup, while the others have positive votes, and module four has no vote.

Those are the outgoing votes for learning modules. I'm assuming an all-to-all connection between modules, but theoretically, they don't all have to connect. It just adds up the votes from whichever modules connect to you. The green and red indicate yes or no votes for the object—green is yes, red is no. This line is outgoing votes, and this line is incoming votes. Learning module zero receives three plus votes for the cup from modules one to three, and no minus votes for the cup. It receives one plus vote for the apple from LM1 and three minus votes for the apple from the others, and so on.

Philosophically, I'm against this type of coding, but maybe you can convince me. Why? Let's say you're sensing the apple but don't know if it's the apple or the ball—they're both curved—but one module definitively senses the leaf at the top. It can say it's impossible for it to be the ball, but this system doesn't allow that. Nobody here can dictate to everyone else. That's the problem. In the pilot paper, we definitely had that. The trick is, if you're not sure, you don't want to eliminate anything, but if you're really sure there's a leaf, you can be certain it's not a ball.

This is something I hope the evidence-based learning module can help with. In the old module, objects are just possible or impossible. In the evidence model, we can have evidence for different objects, allowing for more nuanced votes. We can say, "I'm very confident it's the apple." If you introduce noise, your votes can never be completely confident. I'm pushing back on the idea of being certain it's an apple or not a ball—maybe that's not possible. Maybe 100% certainty is never possible, but you can be pretty sure, even with noise, and still reach the right conclusion. Maybe this is more of a consensus-based thing, like majority rule, rather than strict voting. Not everyone has to agree. For example, the mug was negative in learning module zero, but overall, it was still identified as the mug. The main point is that not all learning modules need to know about all objects, and we can handle some noise if the majority still consider the object possible.

Overall, I was trying to strike a good balance between optimizing for robustness and speed. For example, we get a negative vote for the cup here, so we eliminate it, which narrows down the possibilities faster but makes the system less robust to noise. That's the balance I've been trying to achieve. It's a tricky balance, but if learning module zero is certain it's not the mug, we don't have certainties—only yes or no votes. Learning module zero could sense the object multiple times, so it's voting with itself over time, in addition to seeing everyone else's votes. You want to ensure that no one can only eliminate something and say, "My fingers skipped the row, there's something missing here." 

The counterargument is, suppose you have a mug without a handle and a mug with a handle. If one finger is touching the handle, three out of five fingers would need to get to the handle before you eliminate it. Maybe I grab something and there are two objects in my view, or I'm looking at something and there's a mug and something next to it. Different columns will be looking at different things, so maybe only one column sees the handle and the others don't. Maybe that handle is part of the mug, maybe it's not, maybe it's part of something else, so I can't be certain yet. This will require a majority of the columns to see enough distinguishing features. I think that's right, and it's a good idea, but it might be slow.

Subutai, would you agree that once we use the evidence-based model and don't have binary votes, that would address your problem? At that point, you could have a strength-of-vote parameter or something like that. This is binary—I forgot about that. There are definitely cases where you do a single sensation and know what object it is, and that's okay. Even if one of my fingers, when I grabbed the cup, felt something weird, I would say, "That's the cup, but there's something weird down here." If that one weird thing told you, "Oh, it's my mug because I know there's that logo sticking out," then maybe you have to attend to that component and see it. We want to allow a single sensation to tell you what the object is. Both this and the evidence-based approach would allow a single sensation to settle on the object, but there are many cases where it won't.

I guess it depends on how well distributed the sensors are on the object. If all the sensors are looking at the bottom of the coffee cup, it's not going to do very well. But if they're dispersed around the coffee cup, it'll do pretty well. How are you taking advantage of the relative positions of these sensors? That's the key thing here. That's the next step. You don't have that yet? No, that's not in here. Otherwise, it's just a bag of features, and we know that's not good enough.

Now that's voting on object pose.

Let me go through the example. In this example, we have two sensors—two fingers. One is touching the mug on the top, and one is touching the mug on the handle.

Each learning module sends out its current possible poses and the current pose of its sensory input. It has the possible locations and rotations of the sensor relative to the model of the object. The green dots here in learning module zero correspond to sensor module zero. Learning module one has, for example, two possible locations on the top of the handle.

Anticipating some confusion about what we said before, this process takes into account relative positions. What we're describing now is not a bag of features. To clarify, we have the sensor module location and rotation relative to the body. For rotation, I'm referring to the three vectors mentioned earlier: the point normal and the curvature directions. In the next step, I calculate the displacement between the two sensor poses—the sensor pose of the sending to the receiving learning module. For example, learning module one sends votes to learning module zero. I calculate the displacement—location and rotation—between how my sensor, like this hand on the handle, is rotated compared to the hand on the top of the mug. This involves both orientation and location.

The reason for this is to transform the votes using this displacement. If I think I would be on this handle, but your sensor is displaced by this location and rotation, you should be over here on the model. These transformed votes—the yellow dots—are sent to learning module zero, which might receive more votes from different learning modules. The receiving learning module checks for each of its possible poses if there are at least n incoming votes close by; otherwise, we remove the hypothesis from the possible matches. In this case, the incoming votes are only close to one green dot, so after voting, all the other ones are removed because they didn't match what the other learning modules were thinking, and only this pose is left. In the real model, before voting, all the green and red dots were possible locations for the mug. After voting, all the red locations are removed, the green ones remain possible, and the gray dots are other votes from other learning modules.

One way to think about this is that a single module does this sequentially in time and knows the displacement of the sensor as it moves. It calculates its displacement as it moves. Ultimately, we want multiple learning modules working together, using the same mechanism in parallel. The goal is to have the same mechanism working in time in one module and across time in multiple modules. There's a paper showing that when you activate a single column, information goes up and down in that column, and then spreads to the next columns over in the lower layers, like layer 5. It might be a sequential step: a single column gets its input, makes its hypothesis, and then, a moment later, gets another input and adds it to the hypothesis. That's an alternate way this might work. Cortex might do this differently.

Currently, the difference between the update within the learning module and the update from the votes is that the input to the learning module is a feature at a location. The features received can be specific to a sensory modality and are specific to the features learned in this learning module. This sensation leads to an update to the possible objects and poses. The votes are consistent across all learning modules—always possible objects and poses—and they add or subtract evidence for the poses each learning module represents. The models in the learning modules can be very different; they can contain more or fewer points, store different types of features, and be more or less detailed. They can also store completely different locations on the object.

In practice, we do this with complex 3D objects. We receive votes for each model in a learning module and perform this update for all learning modules. For instance, the learning module on the bottom has more features stored and may store different features at different resolutions. The possible locations on the objects within those models will be different because the sensors are in different locations. Learning module one will have different locations on the mug than learning module two because the sensors are displaced. That's why we apply the pose transformation to send the votes. We do this for N steps to narrow down possible votes, and at each step, we update using the sensor inputs and then update using the votes.

A quick result: using voting reduces the average number of steps needed to recognize an object. With voting, the process is more efficient.

One detail is that you're voting on both pose and object ID. Yes, both.

In terms of processing, do you take in the object ID vote first? It seems like that could save computation time, particularly with your evidence-based model. If you eliminate an object ID, you don't need to process further. The votes are all sent out at the same time, and then they're received. If a learning module never eliminated an object after receiving the votes, it has still already sent out its own votes before that happened. After receiving votes, if it first checks the IDs and, with the evidence-based model, there's large negative evidence that passes a threshold, you can decide not to iterate through all the points in this object.

If you had a single learning module getting five times as much data, that would be one learning module taking five steps. In the no voting case, it takes close to 30 sensations for each learning module to figure out what the object is on average. Five learning modules should be dramatically faster—about a fifth of that. With noise and other factors, it won't be exactly a fifth; it'll be a bit more. The purple bar seems too high in this example because the sensors are pretty overlapping and get very similar inputs. It's not five fingers all over the object; their input doesn't provide that much diversity. A single learning module going through 30 steps may cover a larger area of the object than these five modules in a single step. If all five modules are looking at the same thing, there's no advantage, but over time they move away. Maybe the question is whether this is an artifact of how the sampling is set up, as you're suggesting. It could be due to how the data is fed into these five learning modules versus something inherent. If I had done this experiment, I would have picked five disjoint points on the object, as opposed to five next to each other. Then the purple bar would be a lot lower. Even with more intelligent sampling, like a retinal patch, what's sampled at one point will be resampled by learning modules as they move. You can't necessarily avoid that, but you can recognize the object in one step if the different learning modules are looking at the full extent.

The action policies here are very simple: all patches move together. If I put them too far apart, it's difficult to keep them all on the object. Even now, many patches are often off the object while the one used for the movement policy is still on it. That's your policy: just don't have them go off the objects for starters, or use tactile sensors, which you can distribute more easily, like having five fingers. The problem with the touch sensor was that during learning, I couldn't get it to cover the object as evenly as with the vision sensor. Even with the vision sensor, you could say, "These objects are in non-cluttered backgrounds, so make all my vision sensors on the object somewhere."

They don't have to be together. I know they're together on the retina, but theoretically, they don't have to be. From a testing perspective, is there anything wrong if something is off the object? No, the step is just skipped for that learning module. That would still work if you had five patches that were separated. That actually happens a lot in these experiments—some are off the object, some are on, and they just don't update their model in those steps. Imagine looking at a pencil: the pencil will be across the retina, but most of the retina won't see the pencil, so most image patches won't be on it. Going back to the touch sensor, you don't get good coverage during training. Could you use the vision sensor for training? We do a morphology-based model, so you get the model from the vision sensor and use touch just for inference. I'm hoping to do this with the evidence-based model. The model I used was too sensitive to sampling, so it didn't work with new points from the touch sensor.

One thing is that the terminal condition needed to be adjusted. Some learning modules may have already recognized the object after a few steps, but others may not have narrowed down the pose yet.

If we want every learning module to have uniquely narrowed down the object and pose for the episode to finish, it takes longer than if we look at one learning module individually. Shouldn't it be the case that once the majority of learning modules have narrowed down the object, everyone should have narrowed it down? For the object, yes, but for the pose, no. 

With the pose, we don't use a majority vote; we use whether there are enough votes in agreement in the vicinity. In principle, since these are rigid objects, if the majority agree on the pose, the rest should also agree.

I'm not sure if that's always the case, especially if some models store, for example, the mug with fewer points. If three out of five say it's this pose relative to the body, it has to be the case for the other two by majority vote.

It's not that simple. Conceptually, it seems that should be there. We're going with majority vote, and even more interesting than that, at any point in time, there are a lot of columns—only a small subset will be sampling this object. I believe voting would occur across all the columns, even the ones that aren't getting any input. In some sense, an entire section of V1, or some area of the cortical tissue, comes to agreement, even the ones not getting input. They should know, because when those columns do get input, they should already be biased by the previous knowledge of all the other columns. It's like coming to an agreement that's shared among everybody, even those not voting right now. The whole system knows: if I do get input, I know what it should be.

If a module is not on the object, it doesn't use the sensor input to update the hypotheses, but it does use the incoming votes. I don't understand Subutai's comment about why all the columns wouldn't have the same understanding, but there's some reason. Like you said, all the voting wouldn't be equal, so I don't understand that.

A lot of it also has to do with hyperparameters. For example, if you look at the last frame here, all the possible poses are pretty close together. The question is, when do we say this is specific enough? Do we want it to be one specific location, or do we say as long as all the possible locations are within a radius, it's okay? That is influenced by the radius a vote can influence, and so on.

Those things play into it as well, because we're not voting on discrete sets of poses. We're voting in a continuous space of locations, since every model may store different locations—these points don't overlap.

One thing I forgot to point out with this picture: you can see voting is really useful, especially on the first step before voting. We have all these gray dots scattered around the mug—those are the incoming votes at the first step. Then all these votes are gone at the next step. At the first step, the relative sensor displacement eliminates a lot of impossible locations and poses.

That step is just a voting step, right?

That's a nice thing—not just a bag of features, but it takes into account the relative displacement of the sensors. With that relative displacement, a lot of locations on the objects are not possible, but you are calculating on a vote-by-vote basis, calculating the displacement of those two votes.

By five columns, if I'm one of those columns and getting four votes, I have to calculate the displacement for all the other four columns.

Certainly, but one at a time.

Every learning module sends out its votes and possible poses and its sensor pose, and then the receiving module takes the sensor pose to transform the votes and take them into account. Computationally, it's a serial operation. It doesn't seem like that's going to work in the long run, but it could work right now, so that's good. It's really cool to see this working—voting with poses and object IDs. It's got an initial version working. There are a lot of additional things we could do, but it's great to see something up and running. It's working so well that we get agitated and want to fix all the little problems. If we actually say that we terminate an episode once one learning module has recognized the object and pose, and we just say this learning module spreads that knowledge to all the others, then the number of steps gets cut down by a lot more. This bar here is five learning modules, no voting. The purple one is five learning modules with pose and ID voting, but all the learning modules need to narrow down to one object and one location and rotation. The pink and lighter pink are either three out of five need to come to a conclusion, or one out of five needs to come to a conclusion, and that's actually also a lot faster than just using one learning module.

That's it.

Wait, how can it be fast with just one learning module?

Because the more information it gets, the learning module also gets the voting input, which it can use to narrow down the possible objects and poses faster. I'm trying to decide between one learning module on its own versus five learning modules without any voting. How come there's such a big difference between them? All five learning modules need to come to a conclusion on their own, and some may recognize the object faster than others. Wouldn't the one on the right be confused half the time? It wouldn't know enough.

The one learning module—is that one step?

No, it's ten steps. It's still not obvious. If you have more learning modules, it always takes longer until everyone comes to a conclusion without the module. I can see that, but I'm not sure why. It's not clear why. On average, it would be the same, but there will be some spread. Every purple line is when each individual learning module recognized the object, and it may be much earlier than some other learning modules because they get different sensory inputs. Sometimes the sensory input gives you more clues, so one learning module may be faster than others, but it still has to wait until all the other learning modules figure out the object and pose. If you were to look at the max and min for the one learning module, the max time it might take would be close to 30.

From five seeds, kind of thing. Exactly.

Okay, very nice. Thanks. All right, Jason, we'll see you soon. I stopped the recording. We'll see you soon.