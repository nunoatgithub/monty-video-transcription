I thought it would be nice, given that this was a huge effort to get out, to revisit it because it's definitely been a big milestone and ended up being a major project we worked on for a long time. I think everyone's really proud of what we've put together, and rightly so, because our ambitions grew as this unfolded. The original motivation was to establish Monty's capabilities, show that to the community, and communicate the current state of the art of a thousand brain systems. There are various reasons to do that, but one of the main aims of the TBP is to get people excited about and using a thousand brain systems in this approach. Showing what it's actually capable of is a big part of that, especially for the academic community. Along the way, it was surprising how many new things came out—new metrics, new understandings of what Monty is actually capable of, things we hadn't really conceptualized when we first decided to write this, and also many fixes and general improvements to Monty's implementation. Many of those were definitely not trivial.

It's been a huge team effort from everyone at The Thousand Brains Project. As I mentioned on Wednesday, a lot of the groundwork for this even started at Menta. Although we've been working on the DMC paper for maybe eight months, this really started more than three years ago with the initial work on Monty. Special thanks to Scott and Hojae for the huge amount of effort they've put into this. When you joined, the idea was this would be a two, maybe three month project—a nice way to get familiar with Monty's experiments and how to run them. Thank you for all the effort you've put in, despite it turning into a much bigger project than we expected.

To rewind back to when we started, what we had at the time was Monty—a version similar to what we have now, but with bugs and issues we would later discover. We had some benchmarks gauging things like accuracy in the context of certain amounts of noise, and some visualizations, but nothing particularly glamorous. When we were doing initial scoping work, it started fairly simple but eventually grew quite a bit in ambition. I'm not going to spend a long time on it, but just to give some context for the amount of work that went into some of these figures: we had these alii draw boards, and this is just version three, where you can see a huge amount of variations, comments, and all this kind of stuff going on. This was version four, and I think it went all the way up to version five. What we ended up with were these six really beautiful results figures that do a great job of showing how diverse Monty's capabilities are. Any one of these would be an impressive result, but the fact that all of these capabilities emerge almost automatically from developing sensorimotor AI informed by the cortex is pretty exciting. It feels like a strong signal that what we're doing is on the right track.

Along the way, this resulted in two new repositories providing comprehensive support for anyone who wants to replicate these experiments. That's really important for open science and should help build trust in what we've done and increase the likelihood that people build on this. This was a huge amount of effort—30,000 lines of code and experiment configurations for the main paper repository, and then the epic repository that Hojae put together, TPP Floppy, which for the first time enabled us to quantify the amount of flops—the compute that Monty consumes—and benchmark that in BTS. But this doesn't capture all the other work, as was highlighted before. Even just replicating the experiments, not to mention running them in the first place and running them many times—every time we realized a change needed to be made to a figure or a bug was identified, it led to a cascade of running everything again. So, a huge amount of effort.

A quick summary of some new evaluations and metrics: we added better visualizations of movement sequences, properly quantified symmetry for the first time, and found a way to verify that. We looked at adversarial color robustness, examined the stepwise effects of the model-based policy, explored scaling of voting for the first time, figured out tie breaking, and quantified learning efficiency. We also compared Monty to VIT, which was a significant effort, especially for Ho J, as we previously had no comparisons to deep learning architectures. We compared FLOPs and quantified that versus VIT, looked at continual learning, and compared that as well.

Outside of those areas, we updated professional diagrams, conducted a literature review on related approaches, and made many improvements to the configs. For example, specifying a voting system in Monty with multiple learning modules in a grid was not trivial before, but Scott made significant improvements. We now have a comprehensive mathematical description of how Monty works, including new notation, and fixed many bugs. Previously, voting only worked with learning modules and sensorimotor modules aligned in a one-dimensional grid; we couldn't support other arrangements. We addressed issues like the X percent threshold, fixed patch-off object (which required a huge effort from Scott), and resolved semantic sensorimotor leakage into experiments.

I'm sure I've missed some things, but it was a lot of work. All these metrics, evaluations, and the pipeline for visualizing them are reusable as Monty's capabilities grow and we need to communicate them again. This is definitely not a one-off use case. Now we have a 32-page paper to share and discuss when people ask what Monty can actually do. Thanks to everyone for their hard work.