September has gone by quickly; it's already the end of Q3. We'll do the Q3 review at the end of this, but I'll start with a shout out to the community. It's been exciting to see more collaborations, significant contributions, and interesting exchanges on the forum. It's really fun to put together these slides.

First, some first-time posts and new people joining the forum: Ram from the University of Groningen in the Netherlands introduced himself and asked some questions, including about paper docs and time series pattern recognition. There are also a couple of other posts from him and even a PR that I'll show in a bit. Welcome to the forum and the project.

There were several interesting posts to highlight. A couple of papers were shared; for instance, DLA shared a paper discussing whether current models have an understanding of physics or just generate impressive visuals. Another paper focused on robotic manipulation and how it helps to provide or predict point cloud data, with some discussion around those topics. Another interesting thread was started by pep docs about how learning in Monty works algorithmically.

In the category of helping and interacting with others, the award this month goes to Agent Rev. He has been very helpful, answering questions, replying to people who share articles and research related to Monty, and giving feedback on the workflow for doing research with Monty.

Replying to different articles about researchers exploring alternative approaches to AI and TBP-based projects and plans, Zach posted a more concrete outline of a potential undergraduate project using Monty for robotics applications. Spencer posted about implementing a custom learning module using SDRs and some related proposals, mentioning that he started to implement an SDR-based learning module and will share visualizations once it's working. Sebastian asked for more details on previous investigations into integrating HTM into a learning module, acknowledging it's a monumental task but expressing continued interest.

We had a couple of community PRs this month, including documentation fixes and link path corrections. Paper Docs submitted a PR updating the environment to allow MKL dependency for Mac Intel. Agent Rev added a WSL tutorial for setting up the Windows Subsystem for Linux, previously shared on the course and now integrated into our documentation. Another PR from POCs fixed a path assumption. These smaller contributions were helpful, so thanks for those.

There were also contributions to our overall roadmap, which often involve substantial work. Each item is a significant chunk, and the icons show what peers on our team and in the community are working on. A notable example this month was Anna, who posted on the course about contributing in the intro thread. We scoped out options and created an action plan, with ongoing interaction and progress on combining data load and dataset into an environment interface. Anna submitted a pull request refactoring Monty to resemble an embodied system rather than an AI learning from a static dataset. This PR was merged last week, consolidating data load and dataset into one class, updating calls and configs, and adding unit tests. Thanks a lot, Anna, for this contribution.

Colin is working on another roadmap item: adding GPU support. We now have a GPU working group, and the first two videos of our new Community Edition series are meeting recordings of Colin and Tristan discussing GPU support for Monty, with meeting notes available on the course. Greg Robinson wrote more blog posts on theory of mind and humor, and shared previous posts referenced in last month's presentation. These are great reads.

Others have been sharing the Thousand Brains Project on social media, responding to discussions about AI and the path to real intelligence. Grok mentioned the Brains Project for the first time, with someone asking about Monty and the ex-AI agent summarizing our project. Another person shared our paper about Monty, and there was a post referencing Jeff's definition of intelligence from the Thousand Brains book. There was also a thread about LLMs, Monty, and learning through interaction with the world. Thanks to everyone sharing the project, posting on the forum, and contributing to our codebase. It's exciting to see, and I hope the trend continues. I'm looking forward to seeing everyone's projects and how they're using Monty.

The goals and priorities set for the past three months mirrored the TBP mission. The research priority was enabling Monty to model composition objects accurately and efficiently. On the engineering side, the aim was to make Monty easy to use, improve, and contribute to, both for our team and external contributors. Another goal was to increase external interest and awareness of our approach. On the theory side, we aimed to figure out how to model object behaviors in the brain and in Monty.

For modeling composition objects, one subgoal was to establish a dataset, training and inference protocol, and evaluation measures to assess Monty's composition modeling abilities. A significant portion of this was completed during the focus week, using Scott's dataset and implementing the training and inference protocol, supervision, evaluation measures, and evaluating Monty. Some cleanup remains before integration into TBP Monty, so it's marked light green, indicating follow-up work is needed before merging.

Improved hypothesis resampling was based on different signals, such as prediction error, reference frame movement, salient features, and confidence, resulting in improved or comparable accuracy on composition datasets with reduced computational overhead. Rami and Hojae scoped approaches in different RFCs, with Hojae's RFC splitting into three. Rami has been working extensively on a feature branch, and J has been testing auto reference frame movement, which IMARK completed, though we decided not to integrate it now. We have a detailed writeup of the results, but none of it has been integrated into TBP Monty yet, and there are no final results on improved accuracy for this effort.

SIC policies improve accuracy on composition datasets and enable sparse models. Scott and Jose worked extensively on this during the focus week. Scott implemented the Cade policy for moving to Gold States, added plumbing into Monty for sensorimotor modules, enabled emitting gold states, added the gold states selector, implemented inhibition of return, tested different saliency methods, evaluated them against each other, and worked on learning sparse models. This showed improved accuracy, but it's marked light green because it's not integrated into TBP Monty yet.

Adding and testing the surface sensorimotor module enables improved representation of 2D on 3D compositional objects. Jose started on this last week, and there is a feature branch with some initial results, but it's still early in progress.

Making Monty easier to use, improve, and contribute to was a goal. Motor systems were disentangled from other classes, but this was superseded by platform planning, so there wasn't much progress. Monty is no longer constrained to Python 3.8; Jeremy is working on this, and several PRs have been merged, with more changes in progress. The current approach is to integrate MUCO as a different simulator, which allows upgrading Python. A higher Python version was briefly used during the focus week, but that's not the main path. An interactive and up-to-date overview of where people can contribute has been implemented by Will and is under review, expected to be live soon. Documentation on how to contribute and customize Monty for specific applications has seen several PRs, including hackathon projects as examples and general improvements, with more cross-links and information on using Monty, contributing, and doing research. External interest and awareness have increased, with material published around hierarchy and DMC papers. Will recorded, edited, and produced shorts and full-length videos for the DMC and hierarchy papers, and two blog posts with plain language explainers are now on the website. The shorts are being gradually released and are in the pipeline.

TBP, TBT, and Monty were presented at several conferences and speaker series. This was marked as green, but presentations have been limited; several conferences were attended, and more presentations are lined up for the coming weeks and months. Organizing an online community event has not happened yet this quarter.

On the theory side, there was a proposal for open questions around object deformation and actions, but this hasn't been discussed much recently. A concrete writeup and prototype for modeling object behaviors in Monty were created, including a presentation and a Google doc outlining the steps needed for these capabilities. This is planned to be integrated into public documentation soon, and a concrete writeup exists.

Overall, decent progress was made on the priorities. Many items required significant work, but follow-up is needed, especially for integration into Monty. Most objectives saw progress.

Moving to Q4 priorities, Tristan, Niels, Will, and I put these together. The mission is on the left and priorities on the right, following a similar pattern as previous quarters. The first priority is a continuation from last month, focusing on improving Monty's ability to model compositional objects accurately and efficiently. Several research lines contribute to this, including improving baseline metrics and enhancing Monty's modeling of compositional objects. Making Monty easier to use, improve, and contribute to remains a focus, with subpoints to be discussed. Increasing external contributions is also a priority.

Theoretical progress on utilizing behavior models is another focus, building on previous work but shifting toward using these models for actions and predictions.

For modeling compositional objects, the model-free and model-based Cade policy Scott has been working on is important. Gold state generation, sensorimotor module saliency, gold state selector, and CIC policy are validated and integrated into TBP Monty, requiring several integration projects. Dynamic adjustment of the hypothesis space, which Rami has worked on, is validated and will also require integration.

The Composition Object Test Bed and Metrics are integrated into TBP Monty, with a PR opened last week. Additional ways to improve performance on the compositional test bed are being tested, including the 2D sensorimotor module Hojae is working on, potential masking or segmentation in the sensorimotor module, and saliency hypothesis. Re-anchoring and adding top-down connections are important, as is storing child relative to parent orientation, which requires additional plumbing in Monty. Hyperparameters will be tuned, and many experiments are expected in the coming month.

figuring out how to best model compositional objects, making Monty easier to use, improve, and contribute to. The MO motor system is currently disentangled from other classes. There is significant entanglement between the motor system and the data loader and dataset, which are now one class, but the entanglement remains. Tristan has been working on this. Monty is no longer constrained to Python 3.9, and Jeremy is working on the Muco integration. Monty is now integrated with simulator runs on Windows and distributed via methods other than Conda, which also ties into the MACO integration Jeremy is handling.

Increasing external contributions to the project is not very concrete and is partially outside our influence, but we have a few specific actions planned to help achieve this. One is to get the future work table live and update the research engineering and community items, along with the associated metadata, so everything is current. This will make it easy for people to find different projects that require various skills and interests in the interactive table, helping contributors find a place to get involved.

Fostering community engagement and collaborations is ongoing, including work with Penn State and IDPR community members. We are publishing more resources on Monty and the project, such as the DMC and hierarchy paper being published in a journal. We need to address reviewer comments and resubmit the DMC paper. Additional efforts include more video releases, documentation updates, and speaking opportunities.

Lowering the barrier to finding information and contributing involves understanding the paths people take, the information they seek, and making it easier for them to access it.

We are also organizing and hosting a community event.

Lastly, there is theoretical progress on how to utilize behavior models, including a proposal to use behavior models to make predictions about mythology, such as distortions, and a proposal to use behavior models to inform actions.