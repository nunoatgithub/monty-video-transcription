I'll start with a shout out to the community again, which was a pretty exciting month. This month is really getting more substantive, putting together these slides now, which is awesome.

First, some first-time posts to highlight. For example, Steven read one of our RFCs about doing research while building a stable platform and posted some thoughts, feedback, and questions on that. Then Spencer posted a pre-RFC request, a basic outline of an idea, running it by us to see if it makes sense—an interesting idea around a learning module with different phases. Anna also introduced herself in the general introductions channel and asked where she could help out, suggesting two things from our future work roadmap and asking whether they're still up to date and what would be most helpful. I'll have another slide later about where we ended up. We also got several interesting questions, such as how we define intelligence and how that compares to the definition in the ARC Challenge paper, which Jeff answered.

We had two posts about using Monty for audio processing, which is pretty interesting. Today, there was another long-form post about a potential way to convert audio into a spatial representation and train Monty on that. It's an interesting direction, and it's good to see people looking into these applications. There was also a comment on one of our research videos referring to another talk by a neuroscientist about the representation of space. This was on our research meeting about grid cells, path integration, and mechanisms of representing space.

We had a long-form post with questions about reference frames and Monty, and how that works in the brain as well. There were also thoughts on our research meeting posts, such as a meeting about modeling object behaviors. Adam proposed using the second-order derivative instead of velocity or acceleration, which might be interesting features to represent for object behaviors. It's great to see people thinking about these topics, watching our research meetings, and engaging.

Adam also asked a question about a topic we've discussed in our research meetings: how you learn something with one sensorimotor in one learning module and then further it with a different sensorimotor—how that could work. We also got feedback and suggestions, for instance, on an RFC that Tristan drafted about the cortical messaging protocol and on design patterns in Monty. There was feedback on video formats and chapters, and that sometimes the audio isn't as good. Agent Rev put together an AI summary of the subtitles from our videos.

It was also great to see people helping others. Someone posted about installation problems with Monty, and several people jumped in to help figure out how to set it up. Agent Ref posted a full tutorial a couple of days later on how to set up Monty, a step-by-step guide to installing it on Windows using Windows Subsystem for Linux, and replied to follow-up questions.

This literally saved my life after a week of trying to install it with Claude, so thanks a lot for writing this. We also had several PRs from external contributors this month. For instance, Anna submitted a PR refactoring the gatherers to properties using the @property decorator. Lucas saw an old to-do I added in the code about three years ago, saying there must be an easier way to do this, and he picked it up and implemented a simpler solution, changing several nested for loops into a single vector operation. It's great to see someone pick up a to-do in the code, fix it, and submit a PR. I think it was approved today to be merged and passed all the tests.

We also have a new contributor on our roadmap, Anna, who started work on combining data load and dataset into an environment interface class. When we first started Monty, we used traditional machine learning conventions from PyTorch, naming things data load and dataset and using indexing as if we were working with a dataset. But Monty is a sensorimotor system that works in an environment, so it's been confusing for people that our classes are called data load and dataset. This refactor will be really useful, and Anna has opened a draft PR that's in progress. She posted follow-up questions and updates on how it's going. Thanks a lot, Anna, for working on this.

We have our first community work group. Colin posted a detailed analysis at the beginning of the month about whether it's worth accelerating learning modules using GPU operations on CUDA. He implemented several main operations in CUDA and compared their speed to CPUs with different batch sizes. There can be significant speedups. We had some back and forth on the results and whether to parallelize within a learning module or across modules. Tristan created a second topic, going into more detail about how this can be turned into a prototype and then a community-maintained repository for a GPU backend for Monty. There were a lot of detailed plans and discussions, and he shared a well-organized code repository. Here is the first video recording of a meeting between Tristan and Colin about all this, which will hopefully be our first community edition video of a work group with someone from the community working on an important feature of the Thousand Brains Project. This is really exciting, and I'm very happy to have Colin contribute in such a great way.

Several people have been sharing Thousand Brains Project posts. It all hinges on the Thousand Brains Project—if they can't find the way, no one can. Someone else posts something, and here someone commented on Monty scoring similar wins. A recent example is sharing the ultrasound demo. Greg posted two more really nice blog posts that mentioned the Thousand Brains Project: one about catastrophic forgetting and one about reference frames, using them for navigation and learning in general. Here, the Thousand Brains Project is mentioned as one of the promising bio-inspired machine learning approaches.

Thanks to everyone who shares, likes, comments about, or mentions the project. Recently, we reached 400 stars and 200 forks, which is a great 50% ratio. Thanks to everyone who has starred or forked our repository—it's super exciting. I'm really curious to see what people are doing with the code when they fork it. If anyone is watching this and wants to share on our discourse forum, please do.