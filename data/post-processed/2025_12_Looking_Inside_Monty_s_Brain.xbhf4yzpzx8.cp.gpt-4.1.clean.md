Ramy Mounir: My name is Ramy Mounir, and I'm one of the researchers at the Thousand Brains Project. Today, I'm going to talk about the learning modules, which we believe are at the core of how Monty learns and infers objects.

Scott mentioned them briefly in his presentation. I'll go into more detail and show visualizations to help you understand what Monty is doing behind the scenes. Let's start.

I've set up an experiment with three episodes. Each episode features a single object at a specific orientation. Monty, acting as the agent, will move around the object. Let me run this animation.

Monty moves around the object, observing and moving its sensory patch. This is one learning module in action, moving its sensory patch around the object. The goal is to determine three things: object identity (for example, is it a spam can or a spoon), the object's orientation, and the sensor location on the object.

Since this is a sensorimotor experiment, the learning system should also know its position on the object. Just as when a rat is dropped into an unfamiliar environment and the grid cells re-anchor, it knows both the environment and its location within it, using this information for path integration.

A notable aspect of this experiment is that Monty doesn't know where one object ends and another begins. For example, it might switch from a potted meat can to a die or a mug, and it must figure out when that transition occurs. Detecting temporal or spatial object boundaries is useful for learning compositional models. If you're looking at a car and focus on the wheel, then move to the door, you need to identify spatial boundaries and detect when you've moved from one object to another.

Let's open up Monty's brain and take a look inside. This is one of the visualizations I'll show. I have another visualization to switch between during the presentation. We've seen this part before, where the agent is represented by a violet or purple sphere. We can step through the episode, observing the agent and its sensory patch moving on the object. We can also change the episode to another object. Monty doesn't know when the episode or object changes; we don't tell it.

Another feature is activating a tracer for the agent and the sensor patch. Running this animation shows the parts of the object we've seen. This is the agent's path, and this is the sensory patch's path.

Let's move on to the second part.

How does Monty determine the actual object? It wants to infer the three things mentioned earlier. Monty creates a hypothesis space—a collection of hypotheses. Now, let's build an intuitive understanding of what a hypothesis is in Monty. I'll reveal another part of the visualization, laying out all the hypotheses Monty has about this object. Let's move to step six. Ignore the X and Y axes for now; I'll discuss them later. Think of the scatter points as randomly laid out. Each point represents a hypothesis. We'll investigate what representation is captured in these hypotheses. I can reveal a third part of the visualization and select hypotheses to see their details. For example, clicking on a hypothesis shows the object identity (potted meat can), its rotation or orientation, and the sensor location. In this case, only the object identity is correct; the orientation and sensor location are not. Early in the episode, Monty generates and samples hypotheses intelligently, then tests them. Some hypotheses are very good; for example, this one has the correct orientation, identity, and almost the correct sensor location.

Some hypotheses prove to be good when tested, others do not.

We can also sample hypotheses not on the same object. There are many views with various hypotheses—dice, mustard bottle, and so on. Focusing on those with the correct identity, to emphasize the point about hypotheses, we can activate the sensor path. The hypothesis integrates movement, transforming it to fit the object's allocentric reference frame, then path integrates and updates the sensor location. This is the trajectory it believes it moved. We started here and moved all the way over. This matches the simulator's movement: started here and moved this way. The movements are the same, though the location or orientation may differ. Looking at hypotheses at the bottom with low pose error, they all match this one.

You will see that they look the same, but they are just translated. They are translated because it thinks the pose is correct, but the location of the sensor is different. If I move to something with a high pose error, you'll see the whole trajectory is rotated, as it tries to fit into the allocentric reference frame.

Let's replay the animation and see what happens to the hypothesis space as a whole. We have all these hypotheses and want to observe what happens. The hypothesis space starts with many hypotheses, and then we begin pruning them, depending on the evidence. As Monty moves, it sends movements and features to the reference frame and compares them to what the hypothesis predicts. The movements are also applied in the hypothesis, which then makes a prediction about what it will see and compares that to the actual observation from the simulator. If it matches, it adds evidence; if not, it subtracts evidence. The evidence added is proportional to what it sees. A typical good hypothesis shows evidence growing steadily. We also track the evidence change, which is the slope averaged over the last few steps, and use that to delete or add new hypotheses.

Running it to the end, we see just a few hypotheses left, about ten, which is very efficient. Now we look at which one Monty thinks is best. Setting this to one shows the top hypothesis according to the accumulated evidence score, not necessarily the slope. We can examine this hypothesis.

This one is quite good, and the evidence has been climbing since the episode started. It only began decreasing after switching to the next object, and then we deleted that hypothesis, which is the desired behavior. Looking at this hypothesis, it has the right orientation and object, and importantly, it knows its position on the object. That's a crucial piece of information. The second most likely hypothesis is structurally the same, but with a very high pose error at 180 degrees rotation. It's structurally the same because Monty focuses more on the shape of the object rather than texture or colors. This is an important aspect built into Monty, though it can be changed.

The third most likely hypothesis is again the same object, just rotated 180 degrees, maintaining the same overall shape.

I'll skip over the die example due to time, but it's getting the correct information. These are the same orientation but different faces, just swapping some of them, again focusing on the overall shape. An interesting example is the mug. Running this to the end, the green hypothesis is the most likely according to evidence. It's an interesting hypothesis: the object is in the right orientation but appears rotated around one axis, the Y-axis, which would be the axis of symmetry if the handle were not present. Looking at a few more, they all follow the same trend, rotating around that axis of symmetry, assuming the handle isn't there. We can conclude it hasn't seen the handle yet, which we can explore by looking at the sensory patch. Removing the cup shows it's just been moved around the object, exploring the bottom but not seeing the handle. It makes sense that it doesn't know where the handle is and considers all these as good hypotheses, accumulating evidence.

Another view is this figure at the bottom, which we call the hypothesis space. Looking at the area under the curve, the highlighted area represents the hypothesis space size. At the beginning, it grows as it samples new hypotheses, since it starts with none. Then it prunes them.

It prunes those hypotheses down to a few, about ten, as seen in the previous visualization. After some time, we switch the object, and Monty automatically figures this out by looking at the evidence and slopes. It realizes it needs to sample new hypotheses because the existing ones aren't good enough, triggering a sampling burst over a few steps, increasing the number of hypotheses again, and then pruning them. This happens again when switching to a mug. This process also works when moving from one object to another sharing the same space.

I want to show you what this visualization looks like for the cup because it's interesting. I'll run this again and turn on the hypotheses. Here, I'm not showing the hypothesis orientations; I'm showing where each hypothesis thinks its sensor is. There are many hypotheses spread across the object, but once the animation starts, we'll prune them, leaving only a few. We've pruned the hypotheses, and now it thinks there are just a few remaining. The hypotheses are distributed around the object because it hasn't seen the handle, so it assumes they're all around. If I turn on the evidence scores, you'll see it considers there are good hypotheses everywhere, with very high scores. It interprets this as a symmetric object, so all hypotheses are considered equally valid.

To finish, I'll run this again. As the sensor moves up and down, the hypotheses move with it because we're path integrating; each movement shifts the sensor locations accordingly. This is the expected behavior. If you like these visualizations, there's a repository: tbp.plot, with a small gallery you can contribute to or use for your experiments. There are written tutorials on creating new visualizations and a video tutorial on how to use them. I'll now hand it off to Will, who will guide you on contributing to various aspects of this project.