I have some slides on dynamic and constraint models, model building, and related topics, but I also reflected on our meeting yesterday and the cortical column structure and how it maps onto the algorithm we're implementing. I'm a bit hesitant to talk about it because I might have misunderstood something, but I've learned to push through that fear. I'll briefly share my thoughts, which also relate to the drawing I'll show next about object models and location representations.

The idea is that we have a movement signal coming in, which gets applied to different possible locations in the object's reference frame. Neurons representing those locations project upwards to other neurons, which receive this strong location input. This is a strong abstraction—imagine a cup outline, with each point represented by a neuron. If a location in the cup's reference frame is activated, that neuron is strongly activated. Dendrites on these neurons receive feature input, and if the features match, the neuron responds faster. When one of these neurons responds strongly, it projects upward and represents the object ID and location.

This is my interpretation of the hierarchy aspect. This column could be at the second level in the hierarchy, like LM3, but it could also be LM0, LM1, or LM2. The raw sensory input comes in as features, with LM0 at the center of the receptive field. The larger receptive field attaches to the dendrite closer to the soma, while LM1 and LM2, further out, attach further along the dendritic branch. Input from LM0 is enough to prime the cell to fire, but if there's no input from LM0, LM1 or LM2 might still have an effect.

This matches what we discussed previously. There are places in the brain where segregation along the dendrite exists, such as the thalamus, so it's not a crazy idea, though there's no specific evidence for layer 4 cells. This reflects my speculation, not a confirmed fact.

This also solves the problem that LM0, LM1, and LM2 don't need to represent the object ID with the same pattern. The SDR representing the logo pattern doesn't have to be the same for all; the dendrite just needs to detect the pattern for a logo at each location. LM0, LM1, and LM2 are separate neurons in these columns, so it's meaningless to say they have the same pattern. There's no one looking for the same pattern unless their axons join together, so they don't need the same SDR, which is good.

From a machine learning perspective, you might say this is the same representation, but from a neuron perspective, these are separate populations that don't know what the others are doing. That's good. There's a big addition to this diagram I should mention, but I'm not sure if you're done with it yet. I have a few more additions—are you referring to the left or right diagram? The left one. I would add top-down input and voting, but there's another big aspect. We've separated morphology from features, which is necessary.

In V1, specifically layer four, the predominant cell property reported is edge orientation. Minicolumns in layer four contain cells that all respond to the same edge orientation, with adjacent minicolumns tuned to different orientations. The speculation is that two things happen in layer four: a minicolumn detects the point normal or edge of the object, and vertically aligned cells represent different point normals. When features come in at a specific object location, one of those cells in the minicolumn is selected. You get a representation of the object's edge or point normal, defining its morphology.

To represent this uniquely for a particular object, if only the edge is detected, all cells in the minicolumn are activated. With more information, such as location or additional features, you can narrow it down. The theory is that layer four is driven by both edge detection and input from the movement grid in the reference frame, plus features. This helps determine which cells in the minicolumn to activate for a specific object's edge, based on features and location.

We want to recognize objects by morphology and also narrow them down by features or specific locations. If you know your location on an object, you always activate a specific cell in the minicolumn. In this diagram, you can recognize the object from activated locations in the reference frame, but also from morphological features like point normal and curvature. Morphological features are the base input, primarily detected by the retina, and additional information helps narrow down the identity. It's a temporal memory algorithm: minicolumns represent the current point normal at a location, and we want to pick a unique cell in each active minicolumn to identify, for example, the horizontal edge on a coffee cup versus a stapler.

I could draw a diagram, but I can't do it here. I hope that's helpful.

I think it makes sense. I wouldn't know exactly how to modify this diagram to incorporate that, so if you can draw something and send it, that would be useful. I can try to do that—just send me a reminder so I don't forget. To add more detail, it appears there are two sets of cells in the brain receiving input from the retina: the layer four cells shown here, and another set that seems to drive a minicolumn. These are bipolar cells, which activate all the cells in a minicolumn together. There aren't many of them, and they detect the point normal, inhibiting each other to form a distribution of edge angles—maybe 10 or 12 different angles.

These bipolar cells drive all the cells in a minicolumn as a group, with each group representing a specific edge angle. We then want to pick a unique cell in each active minicolumn to form a unique representation of that point normal for a specific object. We concluded that we need two representations: one for the object's morphology and another for additional features, which could include location, color, or projections from other modules. The morphology is also represented in this layer, and we move through the same location reference frame.

Imagine two cell populations: one of bipolar cells representing the point normal, and another set used to represent the point normal uniquely. In previous diagrams, we separated feature and morphology layers, with two different output IDs in the top layer—one for morphology and one for features. There are three locations in a column that get input from the sensor: layer four cells, the border at the bottom of layer three, and the bottom of layer five. It's possible the input at the bottom of layer three drives the minicolumns via the bipolar cells, representing morphology, while the others connect directly to layer four cells.

The input at the bottom of layer three might detect object movement and shift object location, which was a previous speculation. There are clearly these bipolar cells, sometimes called "horsetails" because their axons and dendrites are fine bundles narrowly constrained, defining the minicolumn and spanning up and down.

Those bipolar cells get direct input from the retina and have the same basic edge response properties as reported in layer 4 cells. They also fire earlier than layer 4 cells, possibly due to input from the magnocellular pathway or simply because they respond faster. The input comes in, detects the edge and orientation, and then a specific representation is selected. It's possible the bipolar cells are temporally pooled, representing a class object—like a cylinder or coffee cup—without encoding details like a logo or color. The bipolar cells could be pooled to create a morphology model, though there are challenges with this idea.

From previous discussions, there were two possibilities: dendritic branches on L4 cells could receive parvocellular input for non-morphological features like color, while somatic inputs via intermediary bipolar cells provide morphological features. Thus, different parts of the cell process different types of input. Alternatively, non-morphological features might be represented by stellate cells in L4, while pyramidal cells encode morphological features, both bound to locations via layer 6. Pooling over this combined L4 representation could yield a stable representation in L2/3.

Traditional neuroscientists detect edges in layer 4 cells, often in anesthetized animals, where pyramidal and stellate cells appear as edge detectors. In alert animals viewing known objects, these cells become much sparser and unique to specific objects and locations—these are border ownership cells. This sparsification has been observed and supports the idea that edge detectors become object- and location-specific in context.

One addition related to our code is the use of rotation hypotheses to transform movement in body-centric coordinates into the object's reference frame, possibly occurring in the thalamus. The displacement of the sensor is rotated according to the hypothesized object orientation, then used to activate the next location in the object's reference frame and the corresponding neurons.

Another point is tracking possible locations. Connections from layer four help narrow down possible locations as movement and features are integrated, keeping only those consistent with observed data. This is likely handled by layer six, which tracks orientation and location. We speculate this is done through unions that get narrowed down, though this may not be the complete solution.

An open question is how the system keeps track of possible rotations—how rotation hypotheses are narrowed and associated with different paths in the reference frame. In code, this is straightforward, but the neural implementation is less clear. Perhaps multiple neurons represent each location, each associated with a different rotation, and the active ones signal the thalamus to rotate the displacement. This remains uncertain.

The blue arrow representing features also needs to be rotated, just like the morphological features such as point normals. The top-down object ID and pose from a higher-level learning module would project onto the dendritic branches of neurons that recognize or represent the object ID, and these neurons would send out lateral voting connections, potentially influencing the location neurons as well.

If a higher-level module recognizes an object, location, and pose, it can reinforce specific location neurons. For example, if you're on a coffee cup at a specific location with a logo, the top-down signal could re-invoke that specific location. This is why the lower arrow becomes a cross there.

I've argued that there are two types of layer 6 cells, A and B, with parallel organizations—one representing location (like grid cells) and one representing orientation (like head direction cells). Both need to be updated by path integration: some movements change orientation, others change location. There might be two populations—one for location, one for orientation—both updated by movement, with the orientation representation sent back to the thalamus to help with rotation, and the location representation sent up to layer 4 cells. This idea is still being worked out, but the distinction between location and orientation populations seems promising.

If you have conceptual progress on this, let me know, as the representation of rotation hypotheses is still an open question for me. A system dedicated to orientation would use a movement vector to update orientation via path integration, using sensory feedback to correct for noise, analogous to grid cells for location. Having two similar representations—one for orientation, one for location—makes sense.

Regarding top-down feedback, if it influences the apical branches, it impacts L2, L3, and L5. Layer 4 generally does not have apical dendrites reaching layer 1, and stellate cells have none. L5 is impacted, and these are likely motor cells, but their exact function is unclear. L2 and L3 are easier to understand—they receive feedback to invoke the correct object. The lower projection into layer 6 helps lock in the correct location and orientation for prediction. The feedback to L5 cells remains a mystery, possibly related to hierarchical action policy.

It's notable that these are the two locations where top-down signals are needed, matching our requirements. I previously doubted the lower projection's role in location, but now see its importance.

I'll try to make a more elaborate version of this diagram. I hope this wasn't all old information; I just needed to frame it for myself. The diagram is helpful for everyone to think about and discuss.

One quick thing on your previous slide, Jeff—when we talked about backpropagating action potentials, I was trying to remember their relevance. Do they suppress more distal inputs? No, they don't suppress them. The basic theory is that when a cell spikes, the spike travels back up the dendrite as well as down the axon, but the backpropagating potential is weaker and doesn't reach the end of the dendrite. If an NMDA spike is generated on the dendrite and travels to the soma, the backpropagating potential will travel back to where the NMDA spike was generated. This means synapses that were predictive of the cell spiking—those that generated an NMDA spike just before the action potential—will be reinforced. Inputs along the dendrite that didn't generate an NMDA spike won't be reinforced.

If multiple points generate NMDA spikes, the outcome may depend on their order, but generally, any NMDA spike that contributed would be reinforced. The details could be more complex.

Should I move on to the next topic, or is there more to discuss here?

No, that sounds good. It only took a week to get to this point. I originally made three slides about connectivity, thinking it would be quick, but it turned into three meetings. Still, I have a much better understanding now, so it wasn't wasted time.

Okay, regarding the models, I started by considering what we want from them because my plan was to rewrite and redesign the object model code to add more desired properties and make it more useful for hierarchy. The model should be fast to update and search, with fixed storage capacity. Large, detailed models are decomposed into smaller, detailed part models; we can't store infinite detail, so we need hierarchy and composition.

By "limited," I mean fixed—not small. There's a lot of storage, but it can't expand indefinitely. A module can't learn everything, but it can learn a fair amount. Lower-level modules learn more models of small objects, while higher up, you can learn lower-resolution models of larger objects. This isn't a limitation but a difference. The system should work regardless of the capacity of any module, and differences in capacity are just hyperparameters that optimize performance.

We want the models to learn statistical regularities in the world, not just store every random variation, but focus on what's consistent. The models should efficiently cover the whole object, possibly representing the concept of surface—meaning you move along a surface, not inside or off it. This is an action policy stored in or associated with the model.

We want to associate features from different input channels, such as learning or sensor modules, and allow feature and morphology maps to be associated so a mug can have different textures or logos. In the future, feature and morphology maps might be state-conditioned to represent object states and behaviors.

Representing object states and behaviors is still a mystery. If a model is a set of unique locations, forming a sparse representation, it's unclear how to handle objects with behaviors that change morphology, like a stapler. It could be a new point in location space, a remapped object, or treating moving parts as separate objects with learned relative displacements. This remains confusing.

For now, I'm focusing on adding fixed parameters to these models to enforce learning statistical subparts of objects.

I'm applying three constraints to the models. The first is maximum model size, which refers to the physical size of objects a learning module can model—for example, up to 10 cubic centimeters. The second is the number of cells per dimension; for instance, a 4x4 grid where each cell covers 2 cubic centimeters, constraining spatial resolution. The third is the maximum nodes per graph, where the most statistically regular cells (those crossing a permanence threshold) are used for matching, and the number of top K winners defines model complexity.

Currently, our graphs are unconstrained, allowing arbitrarily large and detailed objects. The new approach constrains models: a larger cell can learn an entire mug, while a smaller one learns only part of it. More cells per dimension increase spatial resolution, and a smaller K means fewer winner cells, focusing on the most frequently visited points. The cells represent granularity, while K is the number of nodes or points in the graph.

To avoid confusion with the overloaded term "cell," I suggest using "voxel" to refer to these subcubes of space. In 2D, for visualization, observations fall into voxels, and the K voxels with the most observations become permanent nodes for object recognition. Biologically, a set of grid cell modules can uniquely encode these locations, distinguishing between voxels. When moving in the world, associations are learned between uniquely encoded locations and observed features, with the strongest associations made permanent.

If a feature, like a bent handle, is seen more often, new connections form to reflect this regularity. Regarding grid cell wraparound, I decided not to implement it, as a separate module can detect whether we're in the field. In neural representation, a set of grid cell modules can represent locations over large distances without repeating, especially with enough modules to form a sparse distributed representation. Repetition is more likely across multiple objects than within a single one. If each movement vector cell has, say, twenty positions and there are twenty such vectors, the representational space becomes extremely large.

I have to think about that. The outer limits are confusing—it's a huge space, possibly billions of points. Practically, we don't need to worry about space being limited. If the distance between uniquely represented points is a centimeter, a point cloud could have a billion locations, which is vast. The real problem is that as objects get larger, the resolution becomes noise; for example, representing a building at centimeter resolution is meaningless due to path integration noise. It's not a representational limit of neurons, but a statistical noise issue. In the brain, you can't reliably path integrate over large distances at high resolution.

I'm not suggesting you change your approach; the analogy is useful. The main idea is that we have a set of uniquely encoded locations, spatially and resolution-limited, but not by a hard edge. In practice, you wouldn't want a hard cutoff, but rather a soft limit—after adding a certain number of points, the model can't grow further unless it sparsifies elsewhere.

Another point: in V1, the number of ganglion cells converging on a layer 4 cell depends on environmental statistics. If the world only had vertical and horizontal lines, a cell could take input from many ganglion cells, as there are few patterns to represent. If the retina is detailed, the receptive field contracts to match what can be represented. This dynamic adjustment means a V1 column learns smaller models with detailed input and larger models with simpler input.

You could consider incorporating this property, making the size of the space and the receptive field dependent on data statistics rather than fixed physical constraints. The k-winner mechanism acts as a spatial pooler, narrowing or expanding the viewpoint based on pattern complexity. This approach is statistically limiting rather than physically limiting.

Currently, we have two learning modules: one with a larger, low-resolution model and one with a small, high-resolution model. If both receive input as we move along the cup, learning module 1 is cut off when we go off the handle, while learning module 0 can learn the entire cup. Observations are collected as breakpoints, each falling into a voxel. We count how many observations fall into each voxel, color them accordingly, and select the k winners to form the matching models. The first observation anchors the reference frame, and each model has scale and offset parameters to map new locations into its grid, giving each object model its own reference frame.

We average the locations in each cell. The grid represents locations in space, but a point on the retina can look at any location in space by moving the eyes. The observations shown were collected by moving over the handle, but the module isn't restricted to the handle; it can follow the handle to the cup and rim. Other parts of the cup are learned in different models, as this module only learns small, detailed models, resulting in a hard cutoff.

This area is confusing. Both learning modules can move over the entire cup, with module zero seeing a larger, fuzzier subset and module one seeing a smaller, more detailed subset. Both could potentially learn models of the entire cup, but representing a large object at high resolution is impractical. It's unclear how a module decides when to break a model or treat something as a new model; it likely isn't a binary decision but a statistical one, based on prediction accuracy. The scope of what a module can learn isn't strictly defined, and locally it may perform well, but not globally.

I don't know how breaking down into submodules or subcomponents would be implemented; it's not as simple as a hard cutoff. Currently, when we move more than a set distance from the first observation, the model starts a new segment. After moving over the entire cup, learning module one would have several models for different parts, each starting anew when the cutoff is reached. Over time, it would see many cups and only store repeating elements, like the handle, while rarely seen subparts would be forgotten.

A better illustration would show both learning modules covering the entire cup, with module one having finer resolution. Both modules observe the whole cup, but module one can't learn the entire cup at high resolution due to statistical limits, not spatial ones. Each module has a grid—one fine, one coarse—and the sensor moves through the grid. In the fine grid, only a subset of points can be learned due to statistical capacity. If there were only one, unchanging cup, module one could learn the whole thing, but real-world variation prevents this.

Points are added based on spatial distance and feature change, naturally capturing statistical regularities. The current figure may be misleading, as it suggests module one only sees the handle, but it actually sees the whole cup and learns multiple models for different parts. The structured grid is used for computational efficiency, but conceptually, the model builds out organically based on data. The graphs on the right represent this process, even if the implementation uses a 3D grid for speed.

Maybe it's a good stopping point. If you could, please make a diagram showing LM1 looking at the entire cup, with the same outer rectangle as LM0 but at higher resolution. Then highlight that LM1 can't learn the entire model due to statistical limits, so it only learns a subset, like the handle or rim. That would clarify the concept for me. It's not a change to your method, just a different presentation.

Before wrapping up, I have one more figure. This shows, from the code's perspective, how we track observations: gray points are observations, and we keep three representations—a count of observations per voxel (for k-winner selection), a grid of average locations per voxel (to preserve continuous location), and average features per voxel (like color or curvature). For matching, we use the k voxels with the most observations, storing their average locations and features in the graph.

This is a real example of a 2LM heterarchy: the higher-level module learns a larger, lower-resolution model, and the lower-level module learns a smaller, higher-resolution model. Features can also store object IDs passed from the lower model.

This approach may work well, but it's not necessarily an improvement over storing unlimited information. We're now limiting models, which makes hierarchy and compositional models more meaningful. Previously, one learning module could learn everything, so hierarchy wasn't needed.

When we first tested columns with locations, features, and temporal pooling, I was concerned about capacity. With fixed resources, we still managed hundreds of models of modest complexity, each with hundreds of locations. The capacity was surprisingly large, though not infinite. This semi-empirical result suggests the system can handle a fair number of location-feature pairs.

I made this change because, without it, hierarchy isn't meaningful—detailed modules would just learn detailed models of large objects, with nothing forcing subcomponent learning.

If I had a very detailed model and moved my eyes from the handle to the other side of the cup, we would want to predict what we'll see next. If the prediction fails, the model is likely wrong. With very detailed models, moving a long distance reduces location accuracy, so predictions become unreliable, and the system must re-infer its position. For small movements, predictions are more accurate. In practice, there's a limit to how large a model can be while still making reliable predictions during movement, due to the imprecision of real-world movements. Even with unlimited model capacity, these practical issues would arise. This is the general approach I'm taking, and I'm continuing to refine the details and improve efficiency.