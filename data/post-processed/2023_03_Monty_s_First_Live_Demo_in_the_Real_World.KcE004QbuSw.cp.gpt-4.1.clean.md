Welcome to the Thousand Brains Project YouTube channel. This video is one of my favorites. It's short, but it's the first demo of Monty on real world data. In March 2023, Numenta organized an internal hackathon where everyone at the company had a week to accomplish a moonshot project. At the end of the week, we would demo our results. We formed small teams to tackle these projects. The Thousand Brains Project team, which then consisted of four people, was considering what we could do. We had been thinking about doing a real world demo of Monty, and decided this week would be a great time. We tried to figure out a demo we could accomplish within a week without needing expensive robotics hardware. Half of our team was remote—Niels was in England and I was in Greece—while the other half was in the Redwood City office. We needed something we could all work on and share. We decided to use the iPad or iPhone camera and write a small iOS app that takes a camera picture, then manually move a small patch over that picture. It's a bit artificial, but a good starting point and something we could accomplish quickly.

I won't give away too much; you'll see everything in the video. I hope this project inspires you to think about other ways Monty could be used and other projects you could try with it. Have fun.

This was an exciting week for Monty. It was the first time Monty saw real world data.

This was something we've wanted to do for a long time, and the hackathon was a great opportunity.

You can go to the next slide. The goal for this week was to set up an environment for Monty to be tested in the real world. We decided to use the iPad or iPhone TrueDepth camera as a sensor because we all had access to it. We didn't need to buy any equipment, and it gives reasonable depth estimates. I didn't realize the iPhone had a depth camera; it's for Face ID. It has LIDAR as well. LIDAR is on the other side and is more for long range depth detection, while the front-facing one is for accurate short range depth, up to five meters. We tested both, but chose to go with this one. The main goal was to see how well Monty can deal with real world data and to have a testbed so we don't always work in simulation and forget about real world problems. A secondary goal, which we weren't sure would work, was to have a first demo of Monty showing it can do something with real sensors.

To evaluate this in the real world, as Viviane said, we are going to perform inference using these depth images taken from a phone or iPad. One of the current limitations is that we don't have information about how the iPad or iPhone is moving through space, which is essential for Monty as a fully sensorimotor system. To enable learning, we had to get real world objects into a simulation environment where Monty could learn, and then, based on learning in simulation, perform inference in the real world. We took nine real world objects that many people have around, like the classic Numenta mug, the Numenta brain, or a Halloween model of a human heart. We used photogrammetry to stitch together a bunch of 2D images and create 3D models that we could put in Habitat and learn on.

This is how the data pipeline works. We have a native iOS app concerned with image acquisition. It takes RGB and depth images, then streams both to a local server hosted on my laptop. This is the inference phase. The server picks up all the streamed data and saves it into a local folder. Monty runs in parallel, constantly looking at the folder for new data, and loads it into the inference pipeline in real time. The images are used to initialize the Monty environment, and the action policy is used to extract patches at different locations within the complete image.

Eventually, Monty runs inference, and the app outputs the most likely object and pose that is recognized.

Monty uses sensor patches, as Jad mentioned. The full camera image is never given to Monty at once; instead, Monty has a small patch it can move over the image. It uses internal models learned in simulation, updating its evidence for different hypotheses of where and on which object it is by moving through the image.

Although simulated models are one way to get around the bottleneck of missing motor information, it makes things more challenging because we are transferring from simulation to real data. This is something deep learning systems classically struggle with if trained only on simulated data.

The input sensors for the app are currently the front-facing camera on the phone and the iPad, which provide a more precise 3D depth image than the Urizon, as it works over 5 meters. Once the app collects both the depth image and the RGB image, it sends them to Monty, just like the simulator. In the simulator, we grab the same information and pass it via the app, so we capture two separate things: a depth map and a color image, or RGB and depth map. You'll probably see what they look like during the demo. The system projects an infrared grid on your face, and by observing the distortion of that grid, it computes the depth. We tested this in the dark, and it still works if you turn the lights off. The infrared projection allows the phone to recognize faces in the dark by projecting a grid of dots. You can use this imaging in the dark, and I was able to get some images from the ceiling as well.

Dimming the lights helps by removing reflections and setting the demo mood. Jad, you should share the screen. All right, this is the server. You can start the server. Here’s the image we’ve got. Once Viviane and Niels talk, we’ll no longer see this demo when it’s recording, so they should mute themselves. Usually, the recording captures the person who’s talking. Who’s taking this picture? It’s this iPad here. But how do I see the front of the iPad? There are many cameras involved.

That’s just the room camera we’re looking at. This view is not on the shared view, so I’m not sure if it’s recording. Viviane should probably mute themselves to ensure this is being recorded from where the sound is coming. Whose screen is that? That’s mine. I started running the iPad camera in the same window as the shared screen, so it may be recorded. I see both screens at the same time, but the recording might not. Everyone is here, so we can do this again another time if needed.

I just ran Monty, and now it’s waiting for new data from the iPad. We have a button labeled "save," and I have to press it to take a picture. It’s scanning and updating the hypothesis. How many objects are there? Nine objects at the moment, but there’s a fair amount of ambiguity. It’s a difficult dataset. Monty works not just on the phone but also elsewhere. How’s the accuracy? That’s probably the wrong question.

In ImageNet, you have top five accuracy, and that has a thousand categories. Our top one accuracy is around 40 percent, but top two is 80 to 90 percent. That’s impressive for a simple real transfer. Training and inference in simulation gives around 90 percent with noise, and nearly 100 percent.

What is Kashmiri Chili? It’s a pack of chili powder, Monty’s heart. Would it recognize it if we rotated it, like seeing the other side? You can try. We tested different orientations; some work better than others. If you just see the cylinder, it often confuses it with two other cylindrical, wide objects. If unique features like the handle aren’t visible, it sometimes gets it wrong. Do you need to flip the image because the writing is backwards? I’m not sure. We checked, and that’s because it’s cut.

They got it pretty early. That’s very cool. Excellent. It was quite a team effort. We had an advantage being in two time zones. Niels and I would work during the day, then have a handoff meeting and pass the baton to Jad and Luis, who worked while we slept. It was like 24 hours around the clock. 

How did you use ChatGPT? The only thing I used it for was to save the RGB image; most of the work was done manually. That’s the first real-time sensorimotor reference frame. Has anyone else done this in the world this way? People have worked on similar objects for years, but not with a convolutional neural network and a sensorimotor system. There are active vision systems out there. It’ll be exciting when we can get motor information from the iPad and actually move around objects. In a future hackathon, we want to send back the detected object and pose to the iPad and AR render it onto the camera image, so you can move it around and see the detection rendered in real time. That would be like in spy movies. If you stop sharing for a second—

Now, if we speak, that one should be recording this scene right now, at least on the video. It only picks one camera image to record, so at least a little bit of this video will definitely have this.

Viviane took a baby steps video of the demo earlier. Very impressive. That's really amazing.

Thanks to Subutai for lending us the iPad and the stand. My iPad was there first. You're on Monty? And the stand, and everything else. You need to engrave it, frame it. I don't know if others have had this experience, but a few times in my career, there have been moments when things first start working the way we wanted. The board comes up with Palm Pilot for the first time after months of debugging, and this felt like that, except this was a week of debugging. At the beginning of the week, we really had no idea if it would work at all or if it would just fail completely. On Wednesday, Niels, you sent me the first results and it got it correct. It was almost too good to be true. This was definitely a very unrealistic goal to attempt. Congratulations.