There are a few things we mentioned in the group that we could discuss, but much of it revolved around the example of a balloon. I also had some thoughts on that, which I'd be happy to share at some point, but I don't know if you wanted to start, Jeff.

Last week, we talked about how every column has to learn every model, and it became clear that this is a difficult problem, especially when working with behavior models. I think I have a general direction to solve that, so when we get a chance, I can talk about it, but I can do that after we talk about balloons if you want.

Maybe I could talk about it briefly, since we might all disagree, and it could be a nice segue into what you're going to discuss. My topic is about balloons, but it's also about model sharing. The suggestion is that maybe we're making it more complicated than it needs to be, and what we have is, in some sense, sufficient, at least for this context.

When we're talking about the stapler opening, there's the issue of observing the behavior at a specific location and learning that, then later seeing it at a different location and needing to generalize. That's where the question of sharing models across different learned locations comes in. The balloon presents a similar issue: how do you describe the complex distortion of the balloon as it increases in size, not just uniformly, but in many different ways? For example, if a logo is printed on the balloon, it would be distorted in complicated ways. The balloon problem is that all the features of the balloon are moving at once, but not together in a consistent way. With the logo, they're all moving in the same direction, but with the balloon, they move in different directions and by different amounts.

With the stapler top, we've discussed how, if you're modeling it as a child object, you could vote on the state of this child object as a whole—its position in space, orientation, and so on. If that's the information you're passing up, it forms the basis of the behavioral model in the higher-level learning module. Later, when you're looking at a different part of the stapler showing the same behavior, as long as you can still infer the state of this child, that representation will generalize. It doesn't matter that you first learned it at one location and now you're at another, because you're describing it at the child level.

What felt good last week was that the lower learning module is inferring the child, and as long as it's inferred, it doesn't matter where it's looking. In that case, you don't need to share models, because the description of the child is generally sufficient. If it's a complex behavior, like someone dancing with multiple children, you probably do have to attend to all the locations to learn the behavior. Even initially, you may not know that—there could be two stapler tops, or several. One is where the whole staple top moves at once, another is where it divides in two and moves as it opens, or rotates as it opens. All those situations would require multiple observations.

It's hard to imagine how, with a single column—like looking through a straw—you could possibly learn those. If we assume there's a single child, and we know that's the child, that's one thing, but we may not know there is only one child; maybe there are ten, or maybe the stapler bottom is rotating. It's more complicated, and we can't just assume the stapler is one thing.

How we segment and represent that is a separate and unsolved problem. But if we have that kind of segmentation and we're representing the state of the child and can vote on it, then many concerns about sharing models between learning modules disappear. The question started as, how does a single module learn a behavior, and how could it observe all these things at once? It can't. If you parse the stapler into two parts and one part is moving, and there are two columns that know which is the parent, that would work. But if there are multiple pieces moving at once, it gets harder.

If you imagine looking at a stapler and using the logo pattern as a template for multiple columns, each column sees a part of the stapler. If a part starts moving, all the columns that are getting information about a particular state of the stapler might agree that, at the object level, the stapler seems to be moving or rotating in a consistent direction.

So that it feels like that could help all of these learning modules talk about the same thing. If the top opens and this one shifts back and forth, this is the child level we're looking at here. The child level at this point is seeing all parts of the stapler. We've pulled out the parts that are moving versus the parts that are not moving. The first level of the representation of the child objects is still just a stapler, but we have to attend to or segment part of it. The straightforward part is that all these modules are talking about the same stapler because everything they're seeing is moving in a unified way. The ones talking about a different part of the stapler are experiencing a different stapler—it's almost like two different objects, because the other properties associated with it are different. This part of the stapler isn't moving, and so on. There are two issues here: one is figuring out how to segment this so we know the child's object is the moving part, and to pay no attention to the others.

That's part of the problem. This is how one learning module could be confident there is a stapler that is moving, even though it only sees part of it. This is just inference. This assumes we have a model of the inference, but we're now learning the behavior. We have a model of a static stapler that would be learned in the higher-level column. Are you suggesting we would use the votes to learn where to lay down points in the behavior reference frame?

No, just that the votes help a single learning module recognize a state of the entire child object. It's changing in some ways, and that is what's passed to the higher level. The problem is, before we've done anything, we don't know what parts of the stapler are going to move and which parts don't. We know that, but the system doesn't. When something starts moving, one column has no idea what is moving. It just says, the thing I'm looking at is moving, but that could be the back half, the front half, or the logo on the stapler. We don't know what's the top half, because we don't know about top halves yet. As far as it's concerned, the whole stapler is moving, but it doesn't know. I think what Niels is suggesting is that if we assume the lower-level column has learned a model of the stapler, then having many lower-level columns associated with all these patches means we can do flash inference of the stapler and its orientation. We can quickly know the orientation and ID of the child object and pass that up to the parent that learns the behavior. But I see a problem with that. Imagine many different parts of the stapler might be moving now, and that single column can't say—looking at one place it would come to one conclusion, and another conclusion looking somewhere else. Exactly. It's basically going to be saying we have two models of the stapler as far as low-level columns are concerned.

The learning module here, what the arrow's pointing to, is voting with a bunch of other learning modules that are seeing the same thing. They're doing flash inference and saying, the stapler is at this orientation. A learning module here is voting with others and saying, no, I see a stapler at this orientation. Those learning modules are going to think they're seeing two different objects. It would be as if there were two objects; they don't agree, so they're talking about different objects. All this learning module is going to pass to the next level is a stapler at that orientation. If we look down here, it's going to pass, there's a stapler at this orientation.

They would still have conflicting votes. I'm not sure if they would actually agree. They would disagree. They might agree on ID, but it seems the only way to get them to agree is to mask off the other parts.

That seems—but why is it any different?

I think masking becomes important when we want to make predictions and similar tasks. Even if I'm a learning module and I see something locally moving while everyone else sees the stapler, my vote will be overshadowed. It would be like everyone is voting and saying, "No, this is a stapler, you just have something odd going on." Unless you pay particular attention to that, it becomes a blind spot—you probably wouldn't even notice it. But if you attend to it, that learning module will say, "I'm definitely not seeing a stapler. I'm seeing a small piece that's doing something else." Attending includes masking; it's saying, "This is the part that's important, I pay no attention to the other parts." That's where we came up with the idea that the parent, or someone, has to say, "Attend to this section, ignore everything else." That's essentially masking out everyone else—these are the only columns you care about. I don't think we can get rid of the idea of masking. I'm arguing for that, and I don't want to either. I agree.

So, I guess I'm just arguing that I don't think we need to share models. I think that's what I'm arguing. I think we have a problem, and we can talk about how to address it. The problem, as I see it, is that as an object's behavior unfolds, there are many things that might be moving in different directions. I don't know how many child things are changing, and I can't attend to them all at once. I can't attend to all these things at the same time. That's the trust issue I was struggling with.

Imagine if the top of the stapler didn't just raise but bent as it went up, forming an arc, and then flattened. There are a lot of changes happening. The behavior can't be described as just part of the stapler moving—there are many changes. That leads to my other point, which relates to the balloon. It's interesting to think about, but before we start talking about the balloon, we should make sure we all know what we're talking about. I made a couple of animations for it.

If it's a quick point, Niels, you can make it. Maybe I didn't understand your point. I thought we came to some agreement about this last week: especially during inference of a behavior, we can make correct predictions if we know the child object is moving. We only have to observe one part. If we know how that part is moving, we can predict what the other parts of the child object are doing. So if I'm only looking at the middle of the top, I can predict what the end of the top is doing because I know the whole stapler has moved. I thought that was the conclusion last week, or at least that's how we left it.

In my mind, it's not clear yet how that would work when an object is actually deforming, if the features are moving in different directions, such as with a balloon. Exactly, like the balloon. I'm not sure if I missed something, Niels, or if there was an additional point you were trying to make. Maybe I misunderstood your concerns, but I felt that when you argued for sharing models across columns, it was about the issue of generalizing from seeing only a small part. I think we came up with a reasonable solution for that, and you just reiterated it, maybe in more detail. But I still think there's an issue with how a single column can learn behavioral models when you don't know which parts are going to be moving or how. 

If it's learning through the straw, which we've argued about several times—is that even possible? Can we really learn behaviors looking through a straw? No. I've always felt that we can learn a morphology model looking through a straw, but it's pretty slow. I don't think every column has to view every part of an object. We can train many columns so they don't all have to experience every location on an object. But it's not a hard requirement, because a single column can do it by moving slowly. We've done that—that's what Monty does.

Now with the behavioral model, it gets really hard because we don't have the time or luxury to go around and look at everything and see who's moving which way. It's all going by really quickly, so the problem is forced upon us. That's how I felt. There was always a problem, but it wasn't really big. We hit it, but now it's in our face, so it became a learning problem. I think we did solve, as you just talked about, how once I've learned it, I can make predictions. We can now predict—even a single column can predict—but how did that column learn correctly?

What I was arguing, and I'm not saying this is new, is that even during learning, voting is still relevant. That's where the flash inference for the state of the whole object can help inform learning. That assumes we know what parts of the object are moving. In my head, I think it's fine that we just get these islands of agreement. It's the same as looking at multiple objects, but now we take the balloon, and the bending staple. I can't do that—I can't vote staple—so then the balloon.

There were two ways I felt we might be making life harder than it needs to be. First, I think flash inference helps a lot with training and learning. The second, in terms of things like the balloon, as Viviane wrote, is that it could just be like scale, but that's too simple because distortion is more complex than scale. I was just thinking, do we actually model it at that level, or do we just model it at a coarse level, like scale? When we look at a balloon, we're not expecting to see a particular contour at a particular angle. We're just expecting, as it inflates, to see something consistent with our internal predictions. Maybe this applies to a lot of distortions—our behavioral predictions, unless we've spent a lot of time and become experts with a particular behavior, aren't necessarily as good as we sometimes project in our thought experiments. The balloon would be that case. The t-shirt would be that case. I purposefully didn't talk about the t-shirt because it's so complex, but the balloon is simple. We all know how a balloon inflates and how it changes scale differently at different locations, and how it would distort if something is printed on the balloon. But are we really able to predict that? Or is it more like, when the logo is distorted and we look at it, we just see it's a distorted logo? Now we're learning a new distortion. It's not that we're thinking, "Wait, this distortion was inconsistent." I can imagine in my head how it would distort, but I don't have to be recognizing it.

Maybe your imagination is better than mine, but if I imagine a logo distorting on a balloon, it's a pretty fuzzy mental picture. It's not like I have a crystal clear sense of how it's deformed. If a checkerboard pattern is printed on the balloon, I don't know exactly which checkerboards would get larger and which would not get much bigger.

Now we're getting back to the idea that I have to mentally attend to the top and bottom, and then I can roughly say, at the top the scale is increasing more, so I expect them to be bigger. But again, I don't have a clear picture of acute angles becoming two. With compositional objects, like a logo wrapping around a cup—that's a distortion just like the balloon. That was solved by not distorting the logo, but by assigning it point by point on the morphology of the cup. The logo or image on the balloon could be just like that. We're not really distorting the child object; we're just creating a new parent morphology and mapping point by point.

I think that's a valid view. Can we start with Viviane? Do you want to describe the balloon problem? I just want to make sure I understand. The main reason I didn't go with that solution is that learning it on a point-by-point basis requires active learning and specific connectivity. We can't reapply a behavior to a new object out of the box; you have to learn the associations at each location, only if the balloon was really an expansion of the reference frame.

If it really were a scale change, then you wouldn't require any new learning. The points on the balloon are now further apart, but they still correspond to the correct points on the logo or whatever is on the balloon.

A hundred percent, but before I pull up the slides, do you want to go over what you prepared on Xra first? If I start the slides now, it will probably end up being a longer discussion. 

Sure. That might not be a long discussion, but I only have a few images. How do I do this? Do I have to share my screen? Here, I share my screen. I think that would be best. Then you can point at things. I only want to share the second screen so I can still see all your faces.

and it says I'm sharing a screen. Do you see that now? Yes. All right. I'm just trying to keep my setup organized. I'm assuming you're all seeing the same thing. You can see my cursor, or should I go to the lasso? Yes, we can see the cursor.

Here's a single column with a learning module. Let's assume it's a behavioral or morphology model, or both. We start by saying a single learning module must see every feature location. It doesn't have to look at every location on the object, just the ones where there are features. If it's a behavioral model, it has to look at every location, every active point in time. That's a lot of burden for a single column to learn.

I've often felt there has to be some sort of shared learning or learning transfer. In the past, I've talked about this second image. Imagine you have a bunch of columns in the same region. Only some of them are getting active input—those are the ones with the green squares. Others aren't getting any active input; they're looking at some space in the object, but there's nothing there. One way to handle this is to have the columns that are learning share what they're learning with their neighbors. That would speed things up. For example, this column is learning something, and another column isn't getting any input, so the first could say, "Why don't you learn what I'm learning? We'll share information, and you can learn the same thing." If you're not doing anything, be productive. I've often felt like this, but I never really liked it. After thinking about it yesterday, I think it's not right. I think something completely different is happening, and that's shown in these images.

Sometimes it just switches to another template or scene. How does it do that? I don't know—scrolling around causes this switch.

Now we're looking at a typical hierarchy. This is drawn like a cortical hierarchy, but it could be a hierarchy of learning modules. These are regions, so there are multiple learning modules in each of these layers. This is your cluster. There will be many learning modules here, though I'm not showing them. We'll make the following assumption: the things at the top of this hierarchy are very fast learning. The top is instantaneously learning, like the hippocampus. As you go down, they get slower and slower, or they're all slow. At the bottom, things are learned very slowly, so you won't learn quickly here.

There's a lot of evidence, and I've talked about this before, that when you first learn something, you learn it in a way that's modality independent. Even if I learn what a cup looks like with the tip of my finger, I can visualize it immediately and make visual inferences, even though I've only touched it. That's not consistent with learning in a single learning module, even a single low-level region.

The standard neuroscience view is that when you learn something, it starts at the fastest level. Information enters at the bottom but goes up through a series of higher levels. The first learning module you use is at the top, and this requires attention. Part of attention is routing where you want information to transfer. You don't pass this information up unless you want to start here and get it all to the top. You have to go through some number of learning levels to get to the top. The lower modules know something, but they're slow learning, so they won't learn right away. They already have previous knowledge, so by the time you get to the top, you're learning fast about existing compositional objects.

For example, I walk into the dining room and look at where the plates are on the table. I know what plates, glasses, forks, and tables are, so all I'm doing at the top is learning the arrangement of those high-level objects. It doesn't really matter what happens; you'll attend to something, information will flow up, and you'll learn this model at the top. Because these modules are slow learning, they may start learning something, but in the initial phase, they just don't have enough time.

Once you've learned this model at the top, it's independent of where the information came from. It doesn't matter if it came from here or there; it's independent of where it is on your skin or which modality you used. You've reached this higher-level model—it's modality and location independent. Now, I can infer from a different direction. For example, if I touch the object with my finger and then look at it, I can infer it through vision because I follow the same channel up and get to the top. I'm using the same model I learned earlier, which is modality and location independent.

An infant can use different modalities, but I still have to attend. For example, I attend to the visual system and can now recognize the object I learned by touch. As you do this, these modules are all trying to learn more sophisticated models as information flows up. They're just slow, so one exposure isn't enough. But if you practice repeatedly using a specific set of columns going up to the top, these other modules will learn models too.

If I spend all my time not just looking at coffee cups but touching them, I'll get really good at touching coffee cups and will learn models lower down that are touch-based. In the future, I won't need to go all the way to the top or attend to the cup because I've learned what it is by touch. After touching it many times, I've formed models in the touch regions that are sufficient. I don't need to attend to it. I can learn down to the lowest level or just the lowest two levels. That frees up my attention; I don't need to attend to the whole stack. I can drive a car using these low-level models while thinking about a concept or talking to you. My hippocampus is involved in our conversation, but my driving is handled by the lower levels. This involves no transfer learning. No models are transferred anywhere.But it does say that I have to learn multiple models at different levels. I don't want to learn multiple models at all levels. I don't even really want to learn at the ones I've already experienced a lot, so I don't need to learn what letters feel like—I only need to learn what letters look like. Once I've done that, I don't have to attend to it at all. But if I wanted to learn what letters feel like, for example, if I wanted to learn braille for the first time, I'd have to think really hard about it and use the whole stack again.

The basic idea is there's no transfer learning, but we do need a way for many models to learn slowly. Until they've learned slowly, I can rely on the higher-level models. I can immediately start doing what looks like transfer learning between touch and vision, but it's not really transfer learning. I'm just using a sort of modality-independent model at the top.

That's the basic idea. This is the direction of how I think about learning now, and I don't have to worry about whether every column has a chance to see everything. In the beginning, when information just passes from the lowest level to the highest level, as it passes through the intermediate levels, what are they doing? Are they just outputting the same thing they're getting as input? No, it depends on what they know. If a child knows nothing, then none of these things are learned at all. If you just came out of the womb, you'd be passing up the most basic information—I don't even know what you'd be passing up. It would be the most basic stuff.

As you learn more, as information moves up, imagine trying to learn a logo you've never seen before. You focus on the logo, and maybe you don't even know the word, so you attend to each letter individually. It's unclear where letters are learned—perhaps at this level, perhaps lower, especially if you read a lot. You might pass up letters, and then this could become a syllable, but you would be learning the word at the top for the first time. It's not like you're passing up raw data; raw data is quickly converted to previously learned objects. Ultimately, you're trying to learn a new configuration of child objects. Which child objects are passed to the top learning module depends on what you've learned before. For example, if you've learned the whole word "menta," you could pass up "menta" as a child object and learn the logo as a composite of "menta" plus an image. If you've never learned "menta," you might pass up syllables, or if you're a poor reader, you might pass up letters. If you're looking at a different language and don't recognize the characters, you might pass up individual strokes, trying to learn what the strokes look like. It all depends on what's been learned previously. Each layer tries to do compositional structure with what it's learned and passes up the new arrangement of these features as a composite object. How sophisticated this is depends on past learning.

The main uncertainty is whether each of these layers will try to learn compositional structure and pool features into a condensed representation for input into the next layer. It seems difficult to imagine that they would pass forward the same thing if they haven't learned about it before. For example, would a stroke be passed through multiple levels of hierarchy and remain unchanged? This might be a startup problem—how does the system get started? It seems the system would work well once some learning has occurred. Another question is about the hierarchy: to what degree are there direct connections from lower levels to the hippocampus? That's a great point. I didn't show that here, but I wrote about it, thinking that the green arrow has to go through all these things. It's possible that the voting layers are shared more globally, and if so, you could skip over some layers. At the top, you're asking who's voting on what, but it's unclear how that would work.

I like this way of thinking about it; using hierarchy seems to solve a lot of issues and enables fast learning. One thing I thought about differently in this framework is that, in the beginning, you would pass lower-level features directly to the fast learning modules, without them passing through several layers and trying to compose a model of objects. Then, learning lower down would be driven by top-down predictions from the higher-level compositional model, which could predict, for example, "I've stored this cup ID here, and whenever I send this cup..." Let me challenge your first point: I wouldn't want to send up low-level features if I already know what they represent. If I'm looking at a dinner table and know it's a plate, I shouldn't send up the edges of plates—just that it's a plate. This is more relevant earlier in learning, so maybe it's a startup issue, as you said.

We can stick with the plate example: we know the plate, but now we have to go through three more levels of hierarchy. What do you compose in those extra levels? I don't know. That's a good question. Is the hierarchy strict, or is there a pooled understanding of the current object that moves up? I don't know. It remains to be seen, and I didn't claim this was all worked out, but I feel this is the right direction.

I agree.

We could start out with just three levels of hierarchy—region one, region two, and hippocampus—and see how it works. That's probably how I'd begin thinking about it. In some ways, we've already started with the hippocampus, given how fast our models currently operate. We have one column in the hippocampus right now, which is fine. We can do all these variations that biology can't do. I've never had a problem with that. We can speed up or slow down learning as much as we want. We can have a single learning module that's not learning at times, and at other times, it's learning everything instantly. Those are all fine. At least we have hyperparameters to slow down learning with the grid object models.

I really like this because it explains a lot. It explains why you have to practice to get really good at something—because the memory at the lowest level of the hierarchy is very precious, and you can't learn everything everywhere. For example, when I'm typing, these are very low-level representations. If I try to type by shifting my hands on the keyboard one key over, even if I know where all the letters are, it's impossible. I've learned very precisely which finger moves in which direction for each letter, and that's it.

One more point: as I get older, I find I'm forgetting how to spell complex words. I can't remember if it's two L's or one L, but I can recognize the word and know if it's wrong. I have these low-level memories of reading the words and know exactly what they look like, but if you ask me to spell them out, that requires attention and a higher-level model. The spelling of a word isn't stored in V1; it's higher up in the cortex. I might have forgotten those because I haven't exercised them recently, or my memory is failing. If you stop exercising the components and just learn the whole thing, you can forget the components you originally learned with. It's just an interesting observation, and I can think of others like that. The point is, you can have really good memories lower down but forget the original ways you got there.

I like the point that there's still utility in these lower-level learning modules, even if it's hard to grasp. In terms of acting without as much attention, I think V1 is where I recognize small letters, so there's a lot of utility there. Anyway, I think I've said enough. I think you alluded to this, but just to clarify: for this to generalize zero-shot to a new modality, you need some amount of co-learning at the highest level during the startup phase of different modalities. That way, you understand that when you feel a contour, that's what a contour looks like. At the highest level, it understands those two are the same thing and can predict somewhere—I don't know where. It doesn't have to be in the hippocampus, but somewhere it's clear this happens. For example, when you're touching an object with a blindfold on, you can visualize it.

Somewhere up there, there's a convergence—this is classic neuroscience. There's a convergence of modalities; we don't even have to think about it. There's a convergence of learning modules. I didn't break these pictures up into different regions; it's more that there's a big collection of learning modules, and you're picking out some subset of them. Some subset could be part of the retina and another part of the retina, or it could be a part of the retina and part of your body skin, or something like that. Something low level, like how a surface or an edge feels and how it looks.

Couldn't you also just use voting connections between the lowest level? Yeah, but it is compositional recognition, right? Now you're looking at a new object, or you felt an entirely new object, so it's a new combination of contours. Then you look at it, you can recognize it, and I'm assuming that's because the input representation at the highest or a higher level is common between those. I'm getting contour, and then I can predict either contour in vision or in touch.

I think there are some studies where they took congenitally blind children in India with cataracts from birth, and early in childhood fixed the cataracts. They showed how much these children struggled to do this compared to how we would. They would feel a pyramid, which they could recognize easily, but then they would see a pyramid and really struggled to connect those two, any more than feeling a sphere versus seeing a pyramid. There's a whole book—I don't remember the name—by a guy who went through this. Some people who have their vision enabled later in life have regretted it because it becomes an impossible distraction. They can see things, but they can't internally process them. Some people have gone crazy; I think some have even killed themselves because it was so bad. This guy wrote a whole book about it, describing what it was like. If you do this later in life, not only is it initially bad, but it may never get good. It becomes a sensory input where you can see things happening, but you can't understand them or correlate them with other things.

Let's not spend too much time on that, but the point is, I think this general idea is probably right—almost certainly right. There is no transfer of learning; we have this scheme where we learn slowly at the bottom and fast at the top. Through practice, we relearn models lower down in the areas we've practiced a lot.

To tie it back to what originally motivated this: how would it solve learning your behavior quickly without observing all the locations on the behavior? Do you think there's some convergence of the input at the higher level? I don't know yet; I haven't gotten that far. All I'm going to say is that there's a general tool for thinking about learning in multiple modules. This tool explains why I can learn what a cup feels like using my right hand and later recognize it with my left hand without retraining all those columns. It's just a general philosophy for doing that. I haven't applied it to any specific problem yet, so I haven't thought about how it works with behavioral models. But that would be the next thing to do. At least now I have a tool to work on it. I didn't like the tool I had before; I didn't think it was right. I think we need a tool. I don't think we can solve this by just assuming that a single column is zipping all around the world all the time, learning everything. We can do that in Monty; that works fine. Iani, we can skip all this. This is why the brain doesn't have to do it this way—there could be many other ways of doing this, perhaps. But at least this is one other way of thinking about it. Okay, I'm done.

Time for balloons. What caused the balloons?

Can I just share my thought? I'm trying to imagine learning a behavior by only looking through a straw—how that might go. I'm not sure if you can. Hojae, I don't think it's possible if the behavior isn't in one go, even if it's complicated. Let's say the behavior involves multiple things, like a stapler opening with multiple parts moving in different directions, not just one piece. It would be extremely difficult to observe the behavior through a straw, then see it again from another angle, and know it's the same behavior or not a different one. That's the problem.

One way it can be simplified is if it's a behavior you can control. If you're the one opening and closing the stapler, you give yourself the supervisory signal of when you're changing the state, and you know where in the sequence you are. For something really simple, like just pushing down a button that you can sense at one location, I think you could learn it in one column. But if it's a larger behavior, being able to control it yourself might be key. If you know it's the same behavior because you're controlling it, you can go fast, slow, back up, or repeat actions. If the thing is moving on its own and you don't know if it's just a hand behavior or a different behavior, it becomes much harder. Imagine trying to learn behaviors like walking, running, or jumping jacks—five different behaviors. If those require looking at all the limbs at once, and the individual limbs may be shared across behaviors, discerning between them through a straw would be really hard. You'd see a leg moving one way, a foot another, but not know what's going on.

Maybe a learning module can learn just one movement. If the stapler top is opening or something is rotating, at least you can learn that one thing. I think Viviane is right: if you knew at every moment that this was the beginning of the sequence and you were going through it in the same order as before, and you could keep all the neurons tuned to that, then in theory you could learn a complex behavior through a straw, with one learning module. I just don't think that's always the case.

I'm not saying I disagree with this hierarchy. It makes a lot of sense. But in terms of learning behaviors, I'm trying to understand to what degree it can solve the issue of everything moving, without some amount of voting still being necessary. I didn't say there's no voting. I think there's voting at every level. It's more about sharing the models. It's two things: attending to some subset of the stack of learning modules, and by attending to them, each area is voting. So you'll be voting on, say, plate or logo or whatever. There will be voting, but on a restricted area of the input space. That's all I'll say about that—it's still voting.

I like the idea of top-down biasing the voting to force that island of voting columns to be smaller in some way, with the decision happening at a higher level. It seems the restriction will be on anything that's changing. It's easy to detect when things are changing; the sensory inputs will tell you. They'll indicate when something is changing. I'm not sure if that's at the parent or child level, but any region—a set of columns—can say, "Hey, my input's changing right now."

Let's look at only the parts that are changing. This isn't a worked-out theory for how we learn behavioral models; it's just the framework I'm thinking about.

So I think there's still voting, with some sort of restricted attention. I've expanded attention to not just a restricted area, but doing it through a stack of regions. You're routing information from the bottom up to the top, and voting is occurring at each level.

Thanks. I'm having some trouble understanding how we can learn higher-level objects before learning the components that compose them. We can't; you always have to learn components first. Maybe I misunderstood the hierarchy. Rami, at any point, unless you just came out of the womb, you'll have something learned in these regions.

So, Link or any child has some limit to the number of components they've learned, and their composite objects are limited to what they've already learned. If they haven't learned letters yet, they won't learn words.

At first, we still have to go through the slow learning of lower-level objects, and then send those to the higher level? No, I think you can start at the top. The very first things you imagine—let's say every column doesn't know anything, just edges or something. At the beginning, the only things you'd learn at the top are really simple, like basic shapes. You can't learn anything else because there's no supporting structure underneath.

The whole system slowly builds. This is one reason it takes so long to learn the world. It takes 18 or 20 years to become trained because we're gradually building up all these component objects that become more sophisticated. I don't know if I've addressed your question, Rami. I feel like I can look at a car and know that I've built a model of the car by first learning about its components. Maybe after that, I can forget some of the lower-level components. Once I've built a model of the car in my mind, I don't think you would necessarily forget them. You don't have to, unless they're not really used in other objects. If a model of the tire isn't used elsewhere, I might cluster it with the whole car. But typically, most components are used in other objects. The ones that aren't are probably forgotten or clustered with the model itself, because I don't really need to learn them. Just like with spelling, you first learn the letters, then compose higher-level models of whole words, and after a while, you can forget how they're spelled. In my mind, spelling is how you build the higher-level object, and then you forget how to spell it. I think you're basically forgetting the lower-level representations you used to build this. There's no reason you have to forget them, but you might if you never use them again. A tire on a car is always going to be its own object because it moves and rotates, so it's always part of a behavioral thing.

I agree. Let's take the Thousand Brains logo, the little image with the swoopy lines. The first time I looked at it and Viviane showed it to me, I paid close attention to all the lines, the curvature, whether they intersect, and the different colors. The day she showed it to me, when we were debating which ones I liked, I could recall all those details. But today, I can't recall any of them, though I still recognize the logo. That's an example where the details of the compositional object were once in my hippocampus, but now they're just somewhere in the visual cortex. That's exactly like the spelling problem you mentioned. Most of the world isn't like that, though. I don't forget what letters are just because I've learned the words, since the letters are used in other words. As long as something is a moving component or a child of another object, whether static or moving, you're not going to forget them because you reuse them all the time. If I see a new object with a tire, I recognize the tire again.

Now there's a three-wheel vehicle or something like that.

All right, I think that's enough of that. We should get onto Viviane's topic.

I have to stop sharing.

I didn't make a lot of slides, but I'm trying to get through the first ones quickly to reach the main point. I have a few building sites at the beginning to recap the solution we discussed last time and show the general way I'm visualizing it. To preface, I'm ignoring a lot of different cases here. Right now, I'm just visualizing the case where the sensorimotor patch is not moving, the object is moving, and there's just one sensorimotor patch. There are many cases I'm not showing now because they're not as relevant for this point. My problem is what I'm trying to highlight.

I enjoy this stuff, so it was fun to make this. I'll start with the example of the moving logo. We have the logo on a cup, which is a compositional object, and we have a behavior of that logo, which could be applied to other objects. For modeling the logo cup, this is basically what we're sensing: the logo with a cup in the world and a little sensorimotor patch. That sensorimotor patch sends information, which could include movement information. In this case, there's no movement, just features and changes. It can send that to the columns at the lower or higher level. At the lower level, we would recognize the Thousand Brains Project logo and locate where we are on that logo. We're assuming some voting is going on, so there's more than one patch, or we're just assuming it knows it's the logo. This is a very simple example with just one patch that has already learned the logo. Right now, it's just doing inference. In reality, that's not enough information to infer the logo from one patch, but we'll assume it is. In this example, let's say we've moved this for a couple of steps already. Like I said, I'm just showing a small part of the whole theory to get to the main point.

Let's say we've moved over this cup and logo a bit and inferred the logo, the location on the logo, and the orientation of the logo. We're passing up the logo as a feature to the higher-level column, which models the mug and recognizes the mug. It has stored the logo at this location and recognizes that it is at this location on the TPP mug. It also recognizes the orientation of the mug and can use that to rotate the incoming movement vectors. It can also calculate the relative orientation of the logo to the mug and use that to recognize the mug. We've stored the relative location of the logo at that point, since the logo could be changing. It's always on a point-by-point basis; you don't store the whole logo, just at some point. At all these different locations, it says, "Here, the logo exists." The language bothers me if we don't remind ourselves. It's good to clarify, because at first you might just assign the logo to the cup, but after years of figuring out how it works, I don't want to use that language anymore.

So, simple enough—nothing is moving yet, but we have a compositional object. Those are both morphology models. We can also have top-down projections, so knowing where we are on the mug, we can predict where we should be on the logo.

Now we have this actual moving logo on the mug, so we have a behavior. For simplicity, I'm going to remove all this rotation stuff from the image; that's still going on, but I don't want to have so many lines there. I also removed the pink line here because in this scenario we're not moving the sensorimotor patch anymore. We're just here, and the object is moving. Basically, we have this little sensorimotor patch. It senses how the logo is moving. Sometimes it is not sensing anything—in that case, it's sensing a static feature, and it goes into the morphology model. Sometimes it senses some change, some movement, when the logo passes through this patch. That movement is passed as an input to the behavior model. Using this sequence of sensed movements, we can infer the behavior and where we are on the behavior. The main solution we talked about last time is that we can take the inverse of the movement stored in the behavior model, send it back down, and apply it the same way as we would apply a movement command from the sensorimotor to move where we expect to be on the logo. As the logo is moving and we have this behavior that we recognized, we can use the stored movement to move us through the space in the logo and make predictions of when we should sense which feature on the logo. Does this make sense? This is a review, right? This is a re-review.

Makes sense to me, and it's very beautiful. I think it's lovely. The only thing I was looking at was, really, what I would do if I saw this logo like this—I would probably track the logo with my eyes. I wouldn't be fixated at one point. I'm not sure if it changes anything, but I would be pursuing the logo, so what's actually being sent would be a little different, but it's the same idea. Like I mentioned, I'm just showing one scenario here. For example, if we were actually moving the sensorimotor patch to follow it, then we would be getting a movement input here as well. That movement input would also influence where we think we are on the logo. In this case, if we're doing smooth pursuit, that would cause the two to cancel out, and we just keep thinking that we are on the logo mark.

That was all the review. Now I would get into the issue I see with it, but if anyone has questions here.

So, one—I'm going to present them in the opposite order of how I wrote it in the document, just in case you read it so you don't get confused. One problem I see here is we still have this compositional model of the mug, but we're not moving anything in it. What kind of predictions is this model going to make about the feature that's coming in?

That model doesn't really get any information right now from the behavior, so it will keep predicting to see the logo at the location where it was stored.

One potential solution could be to also apply this movement here and move the same way. That way, we would predict the logo at the correct time, but that only really works for predicting when to expect the child object. It doesn't work for predicting any of the other features that are not changing on the mug, because we're not actually moving on the mug. We are still at the same location on the mug; we're not moving our sensorimotor, so that doesn't seem like a valid solution. Another one is that we are actually getting input from the lower-level column. That gives us some information, but we still don't have a way to predict when we would get this input. There's no way for us to be surprised when we see the logo at a different location right now. 

The second issue, which I think is the same issue, at least in my head, is object deformation. The example of the balloon is the simplest one I could think of, where we have an object and all the features on the object are changing their location, but not in a coordinated way. With the logo, all the features are moving together, but here the features are changing in different directions.

This inflate/deflate animation should be on this one as well. We could still have the balloon model down here and apply the behavior movement down here to move us through the reference frame of the balloon. But that only really works for the location we're currently sensing, because if the balloon stops in the inflated position, we can't make accurate predictions about all the other locations anymore once we start moving our sensorimotor, because the morphology has changed completely. All the features are in different locations now. We could have the balloon model in the higher level and solve it the same way as we do with the stapler, basically decomposing it into child objects, or the same way as we solve it with the mug. That kind of works, but then we have to decompose the balloon into a lot of different tiny child objects, because all of the locations on the balloon change—the features on the balloon move in different ways. We would have to decompose it into a bunch of different child objects, which maybe we would have to do, but it seemed like something to point out.

Both of those led me to a similar conclusion: we need to have a way to update the locations at which a morphology model expects features within a column. We need some mechanism to change the expected locations without hierarchy, without having to decompose it, because there are examples where we don't have clear child objects, but it's the direct sensory input features that are changing.

The potential mechanism I was thinking of is that we have—and we talked about this before in a different context—like a common space for all objects. When we talked about it before, it was about what space we move through before we know which object we are sensing. We can't really move through a unique location space yet.

If we had a common space, and each location in that space had an associative connection to each location unique to the object, we could learn how to apply behaviors to that common space, distorting it. Because of these associative connections, we could apply this to any object we've learned about before and condition at which location we should expect the feature we've already associated. We would be moving this kind of orange bump somewhere with the behavior model, and because of these associative connections, a unique location on the object would fire at a different physical location in space, or we would expect this feature at a different physical location in space.

I know it's a little out there, but that's the best idea I could come up with over the past few days. Maybe you still disagree with the problem in general?

I see there might be two different problems.

For the one with the cup and the logo in different positions, as I wrote, there seems to be an obvious solution. It's almost like you have to do it.

I can describe that briefly. Imagine the logo moves quickly and then stops at the top for three seconds, then moves quickly and stops at the bottom for three or five seconds. Not a slow movement up and down. It's clear to me that you would learn two morphology objects for that composition object. No one has a preference; there's no reason to assume the logo should be in the top or bottom position. They're both equally valid positions, just two states of the object.

I would learn them and infer them. If I saw the cup with the logo at the top, I would recognize it. If I saw it at the bottom, I would recognize that as well and expect it to move. The problem is, you can't really make predictions about any of the in-between states of how the logos should move. The bigger issue is that this requires learning the specific behavior on the specific object. We can't just apply a behavior to a new object; you have to learn the different key states of the object first.

Let me come back to that in a second because I have to think about it, but it seems logical that I would definitely learn those two states and be able to infer them. I agree with that. Before I understand your objection, let me describe how I think that would work. The knowledge within the higher-level column that represents the mug must have some state indicating it could be in this or that state. The two positions of the logo can't coexist; they don't happen at the same time. I don't see the logo at the top and bottom simultaneously, or the left side of the logo at the top and the right side at the bottom. They're very separate entries. I have to know whether I'm in A or B, and the obvious place for that is the state of the behavioral model. If the behavioral model reflects the movement up and down, it will tell you where you are at any point in time. Therefore, the behavioral model could be a gating factor on the morphology model. It says, at this state in the behavioral model, the morphology should look like this, and at another state, it should look like that. There's a connection. Does that make sense so far?

Now there's a continuum. What if there were five different positions of the logo, and it stopped in each one for several seconds? That would still work. But if the logo moves quickly, I wouldn't expect to be able to make a prediction in between, because it doesn't stop. It's not obvious. The point is, how many morphology models we learn depends on how many morphology instances there actually are. It's a continuum: things that move quickly and don't stop, I wouldn't learn those; but things that stop for a long period, I would learn those. The system could try to learn morphologies whenever the behavior stops, but if it's moving through, I wouldn't be able to learn that as a new morphology, at least not easily, because it doesn't stop at that position. It seems to handle the issue nicely of pairing, within a single column, the behavioral model with the morphology model. The point is, that would work for the logo, but not for other objects. The morphology model could only predict locations and orientations, not the actual feature. If I relied on the actual feature, I wouldn't be able to apply this to something else.

The behavioral model interacts with the morphology model, which is really about locations and orientations, and that's what you can do in the upper learning module. The actual logo ID helps you infer the object, but it's not a requirement. If I'm on a mug and there's a different image, and the image starts moving and I recognize that behavior, then the upper learning module can predict the position and orientation of the child object at any point in time, but it can't predict what the actual child object is.

With the logo, it's easier to think about because if you apply it to a new object, it's just changing the object ID. Originally, I thought about this with the example of the stapler. It's harder to visualize or animate, but if you have the stapler opening and closing, the lower column has the top of the stapler that changes location and orientation. How does the high-level column, which has the model of the entire stapler, make correct predictions about where it should be sensing the stapler top at any point in time? Or if the hinge behavior is applied to a new object where we haven't learned these different states yet, that would be the main question.

Wouldn't the high-level learning module be telling the low-level learning module the orientation it should be seeing at that point in time? We can do it at the lower level and make correct predictions about the stapler top at the lower level. But would we just say this one doesn't make any predictions because it can't rely on the lower level? Which one can't make any predictions? The high-level one, like the composition. Even in our standard compositional objects, the higher level doesn't know what the lower level is, but it would still want to predict which features are coming in from the sensorimotor. The only feature it can predict is—so we haven't really worked through this completely, but we've stated it multiple times—a morphology object is really orientations at locations. That's what a morphology object is. We can use a feature ID to help us identify the morphology object. The parent object of the mug with the logo, even if the logo isn't moving, has no idea that there's a logo there.

It just says, what I know is that I've learned there are points on my morphology object that exist at these points and orientations. The child object that's currently there, I will link to it, but I don't know what it is. It can send information to me, and I don't know what it is, but it'll help me infer my morphology object. If I know there's a logo there, there was some ID that came in before. I don't know what it is, because under the parent, I don't know what the ID represents. It says, this ID is usually associated with this point on my object, and that helps me clue in to what object I'm looking at, but it's not a requirement. I can still recognize everything with just my morphology model.

I'm not talking about predicting which object ID we expect. It's more about predicting at what locations we should expect any kind of features, even just low-level edges that come in from the sensorimotor directly. I thought even low-level orientations and locations—didn't we already see that? Didn't Niels already talk about that? Once we've inferred the lower-level object, the individual prediction would be made by path integration at the lower-level object.

Maybe I can rephrase. I think I see the problem you're raising, Viviane. In this instance, when we're using the behavioral model with the lower level, it's almost like the morphological model at the higher level becomes less relevant. Why is that? It's almost like it's not making very good predictions about the direct low-level features coming in. It can't. Maybe it just depends—like it's suppressed or something. If the child object stops moving and I then learn a morphology, it does the standard composition object thing. Is a single column attending either to the morphology or the behavior? No, think of it this way: while the child object is moving and not stopping, I can't make accurate predictions about low-level features. Is that true? Let me think about that.

The parent object can't tell the child object what location it's supposed to be on, but I still don't get it. Explain what my logic here is. Imagine I have these two columns, and they've learned a set of morphologies where the behavior stops: the stapler is open, the stapler is closed, the logo is at the top, the logo is at the bottom. We've now learned two separate morphology objects, and on the parent object, we know which one we're looking at based on where we are in the behavioral sequence. I can infer either one, with the stapler open or the stapler down.

Now, when the child is actually moving, I haven't learned the composition pairing. I haven't learned a location-by-location pairing between the parent and the child. That's not there now, but while it's moving, I can do path integration because I'm sending motion commands to the child, and the child can know what to expect and predict on its own, but the parent can't tell me that. What is missing in that? I'm just not sure.

So, the answer would basically be that the compositional object wouldn't make any predictions unless it had learned the key frame morphologies and was being invoked by the behavior model. It couldn't make predictions otherwise. I'd have to think about it a bit more, but it wouldn't—yeah, it has to start with a key frame. I don't like the word "key frame" because I think that implies a specific purpose. As long as we don't all think it's the motion picture definition of a key frame.

Let's say we have the stapler. We've learned the behavior on the stapler and several states of the stapler. The behavior model up here can invoke a state of the morphology model within the same column, and we can make correct predictions. But if we observe something else with that behavior, like a hole puncher we've never seen before with that behavior, then we have to use the child object of the hole puncher—just the top of it. During static moments in time, we have a morphology model, and the link between the two is location to location. During the motion portions, the parent object can't tell the child object what to do, but the parent object can tell it to update itself through movement. The child object knows what it should be sensing, not because the parent told it, but because the parent told it how the sensorimotor is moving. Then, if it stops, I start learning again. If it stops moving someplace, I'll establish some links there between the two.

That is a sufficient solution for me for this problem: just inhibiting the predictions if we haven't seen that. There wouldn't be any predictions because during the movement parts, I've never established any projections of the location in the cup to the location on the child object.

It would basically know that once the stapler is opened and in the open position and stopped, it shouldn't make predictions. It should just learn a new morphology model about it. The child object will know its position and can make predictions, and the parent object knows where it is. It will try to make predictions, but then fail and start learning the open stapler position. Because it's stuck open for a period of time, it's open long enough that it could actually learn it. At this point, imagine we have a novel object that's moving and then stops, maybe even halfway. At that point, the child object knows where it is and its orientation to the sensorimotor. The parent object knows what's going on, so everyone can make correct predictions. At this point in time, we just haven't established a connection between the two.

If the parent object had gotten to that location on its own, it wouldn't be able to tell the child object what it is. But the child object at this point says, "I know who I am," and the parent object says, "I know where I am." There's no problem here. Let's start establishing connections so that in the future we can infer this position.

This would be a way of inferring this object in this new state by just looking at the child and the parent. I don't see a hole in it yet. We're assuming they don't vote on a behavioral state at this point. Who's not voting on a behavioral state, the higher level or the lower level? There's no transfer of behavioral state to a high level. There's no voting. The lower level module has no idea about the behavior. It has zero idea, just like the lower level module never knows anything about the parent. The logo doesn't know it's on a cup. It doesn't know that.

The behavior state on the lower level is not sent to the higher level.

Right now, in this object, we haven't defined a behavioral state for the lower level. There isn't one. The logo is not observing any changes to itself, so as far as it's concerned, it's just a thing. It knows that it's moving relative to the sensorimotor, but it's not behaving or changing anything about its model. It's simply at some point in the world, and its position is changing, but that's not a behavior. In this scenario, there's only one behavioral model, which is in the parent. The parent communicates to the child via movement directions, but the child does not know that it's moving or that it's part of something else. It just recognizes its relationship to the sensorimotor is changing for some reason—maybe the sensor is moving, or maybe it's a child, but it doesn't know. It just knows the location of the sensorimotor is moving.

This mug model up here still makes predictions about what the sensorimotor patch is sensing when there's static input. For example, when we have just the white patch and the logo is not passing through the sensorimotor patch, it still makes the correct predictions that match the input. But when the logo passes through the patch, it doesn't get that input anymore because now we're detecting change. Instead, the input goes into the behavior model. It would still predict sensing a white surface, but it doesn't get any input, so maybe there's no prediction error in this case.

I'm confused. I don't understand. First of all, I never think we predict white space, because I thought we're only predicting features that have some property, like the point normal or whatever. That was just a bit easier to think about. I don't even know. I don't know what we do if there's just blank space—maybe the edge of the cup or something.

The way I'm thinking about it is that the morphology model is always trying to learn morphology. It's always trying to learn what point on the child it is, or what point on the parent. But if it goes by quickly, you don't have time to learn anything. If there wasn't enough time, you don't learn it, but if it slows down and stops, then you can learn it. I imagine something similar is happening in the behavioral model. If things are moving, you can learn something, but if they slow down and stop, then you can't learn anything. I don't think there's a binary switch happening here. The neurons are just trying to learn, and if they have a chance, they will. If they can't, they need some time to learn something.

If we accept the constraint that if we apply a behavior to a new object, we can't instantly make predictions about a new state of it. If we have a different hinge that just opened, if a normal moved to the open part, we haven't really learned what's there yet. Or we could still use the child object. Even with a novel object, if I have a behavioral model, I'll be able to make predictions. So what am I missing?

If you have a different hinge that you apply the opening behavior to, and you have the child object here for the top of the hinge—what do you mean a different hinge? Are we talking about a stapler still? I've been thinking about a stapler, but it looks like a hippopotamus or something. But it has a different shape. Let's say you've learned the stapler, but you've never seen it open and close, but you have a behavior model for something opening and closing. So can we say, I learned the stapler and now I'm applying it to something else? Is that the equivalent? You learned the stapler, but now you're applying the behavior to the stapler, even though you've never seen the stapler do that. Isn't that equivalent to me saying, I've learned what a stapler does, and now I see a hippopotamus, and it starts to open? Sure, it's the same thing.

So the hippopotamus head model is up here. We know how it looks with the closed mouth, and then it opens its mouth. We wouldn't really be able to make predictions yet about where the new locations will be. Why not? I wouldn't be able to predict where the locations are, but I would be able to make sensory predictions. I would be able to predict what I would see when I get there, but you'd have to use the child object for that, right? What's wrong with that? I basically turned the head of the hippopotamus into the child object, and I've now changed its position and orientation relative to the sensorimotor, so now I can make predictions about it.

So then, we're saying that this compositional model of the hippopotamus head would not be making any predictions at this point. At that point, no. But as soon as it stops moving, I could start learning a compositional object. You'd have to learn it first. Once it stops moving, you couldn't make predictions about the morphology in this new state unless you learn that new state. Now, I'm still confused. As the head of the hippopotamus is moving and it stops, the lower column will make correct sensory inputs and predictions about what it's seeing. There's nothing unexpected.

but then the higher-level column, the compositional model, can only make correct predictions after it has learned the morphology of the whole head in that stage. Think of it this way: if I didn't see the head move on the hippopotamus, then the top-level model would not be able to predict what the object is going to look like. If it was open, I'd have to know there's a behavior, and once I start recognizing the behavior, then I could recognize it in the open position. I recognize the hippopotamus with the mouth closed, but if I see the mouth open, I would say that's not the same thing. Unless I know about the mouth, that's hard. There are two ways you can establish a relationship between a parent and a child. There's the static way, where we learn the connections from layer six to layer six. That's one way. And there's the dynamic way, which involves movement. Based on the movement, I can now predict where the child would be relative to the parent. Those are my two options. They both work well. If I ever want to infer the object in some position without any movement, I will have had to establish that child-parent relationship through the morphology connections. But generally, there are two ways. If there's a behavior, then I can make all the predictions I want. I don't see a problem here. Maybe that's a good stopping point. Or can we talk about the balloon for a second?

I don't mind talking for a bit longer. People can always drop out if they need to. It's recorded, so I don't know if this is the exact same problem or not. I think it might be a different problem. We couldn't apply the same solution to this. Unless we say that we decompose the balloon into a bunch of little child objects, then we can apply the same solution. On one hand, you're tempted to say, isn't it just the same shape at different scales? I tried to show with this model here that it's not. I drew this exactly from this animation: the way it expands and contracts, the arrows at the top are much larger, while at the bottom they're really tiny. That's just because you've anchored the reference frame to the bottom, which I'm not against, but that's why that's the case. If I just had a circle going from small to large, I wouldn't anchor it at the bottom of the circle. I could, but I would typically think about the center of the circle as the stationary point. Those arrows just show what a small sensorimotor patch would perceive as motion in that area, because the bottom of the balloon is stationary. It's not moving out, and that's why you see more motion at the top than at the bottom. There's nothing wrong with that; I'm just pointing out that if I had showed you a circle expanding, this is not a counterargument, just filling out other issues. If I showed you a circle getting larger and smaller, how would I perceive it if the bottom of the circle was sitting on top of a desk? I would perceive it like the balloon. If the circle was sitting out in space and it seemed to expand and contract on the center point, I would think of it as different and say the expansion is happening everywhere. That's purely a scale change. Here, you've got this not-exactly-circle, it's like a balloon shape and it's fixed at one point, so it's more complicated. You can think, as I wrote in the write-up, of lots of other cases where you can just roughly apply scale, like with the balloon. Once it's inflated, you can poke it in any place. I understand. The only reason I bring up scale is it can play a role sometimes. There are times we can look at two objects and say, that's just a bigger one, that's a smaller one. In fact, they could be mixed together. You could say, that's a bigger version with some sort of deformation. A mug could be on different scales on your retina.

Scale could play a role in some situations. It's not like everything has to be handled without scale. Clearly, we use scale at times.

A related problem, which is a little bit simpler and what I thought about a lot in the past, is imagining a circle that turns to an oval and then turns back to a circle. It just oscillates, just like you're doing here. It's the same problem, but a little bit cleaner because you don't have to think about a little tied end or an odd shape. It might get at the same basic issue: how do I learn that morphology? I was thinking first about visualizing a ball that bounces. I went through all these a while ago, years ago, but I ended up settling on the circle to oval because that was the simplest one I could think of that captured almost all the problems. But you could use any one. Anyway, that's a little bit more pure, because I can't say it's not inflating; it doesn't even have to be getting larger or smaller.

It's got some shape to it. That's maybe an even cleaner example. I only bring it up because it's cleaner and one I've thought about. I have a preference for it, but it's less easy to refer to. You can't just call it the balloon. It's getting at the abstract essence of this changing morphology. At every point, what's changing is the orientation of each piece and the location. It's continuous—that's the issue here. There's a continuous change in the morphology. There isn't a piece moving relative to another piece; there's a continuous flow of change at every point. You can imagine the circle becoming an oval one way, then switching and becoming an oval the other way, changing axes. There's no point that's not changing, but it's different than its neighbor. That's why I was thinking of distorting grid cell space by applying slight, targeted phase shifts. That would be a continuous shift that can be targeted to expect one place cell to fire in a slightly different location. But I don't know if I like it.

Take the oval and the circle. Imagine I have a circle that turns into an oval in the horizontal direction, goes back to a circle, then turns into an oval in the vertical direction, and stops at those three points. Now I have three behavioral models of that object—three morphology models. I could associate them with different states in the behavioral model. If I were to distort the grid, which would be the base and which would be the distorted one? Is the circle the base and the ovals distorted? Or what if I showed you the oval first, then it turns into a circle? When I start with the grid, it's rect on the oval, then it becomes distorted for the circle. I guess it doesn't matter; you could learn it both ways. I have a gut dislike of distorting the reference frame. I also don't like it in general, but the reason I thought it might work is because we could, in theory, do very targeted learning of how you distort the space. But that's not feasible if you want to apply a behavior to any object.

If we had a common space where we could do this targeted learning once and learn how that space can be distorted with different movements in the behavior, then through associations to the unique location space of each object, we could distort any object and even anchor them at different locations. We could start at different places on the object. It's not clear to me this is really better than learning multiple morphologies. In some sense, it would work to apply it to any object at any point in time. But think about it: the distorted balloon is pretty... I could come up with examples where the distortions are very local. Imagine I have one of those squeegee toys where, when you squeeze it, all the eyes pop out—the little antenna come out of its head. It looks like a coronavirus thing, and when you squeeze it, all these pop out. How am I going to learn that with a distorted reference frame? That's crazy.

That seems hard. The distorted reference frames make sense when you look at something simple like this balloon or circle, but the little squeegee head thing wouldn't work very well. I'm also not sure yet; I'd have to think a lot more about the mechanism to know if this would work in general for even larger distortions, but I couldn't think of any other way. You can't just apply it to a new object. Why can't we apply the behavioral model to any new object and achieve the same result? I understand we can do it if we have child and parent object relationships and use the solution we talked about before, but if we don't have child-parent relationships—if we don't want to decompose the balloon into a bunch of small pieces—then I don't see how it works. I think you apply it within a column.

Let's go back even simpler. I asked myself once, how do I recognize a circle? What's the model of a circle? It's got features at locations, and those features are continuously changing. The orientation of the features is continuously changing at these different locations. Where do I break it up? When do I say—it's like an integration problem. We've defined a model where we have unique points and unique orientations, unique features, but now we have one that's continuously changing. There's nothing unique anywhere. There's no point where you can say, this is where the feature changes, or this is where the orientation changes—it's continuous. This is a pure, simple morphology model of a circle. I think if we solve that, we'll solve this problem. I can convince myself that our morphology model works for circles. If your sensorimotor system senses a circle, it would still sense the curve, like an edge. But how many features do I store, and how do I interpolate between them?

Maybe distance is a related kind of thing. In this setting, a combination of attending to regions when it's particularly relevant—a logo being distorted on the balloon, for example—and simpler things like scale could be enough to predict as well as we actually do.

Are you talking about attending? I'm talking about the balloon, but I think it's similar to what you're saying: we need to figure out the granularity of representation, but it's not necessarily a fundamentally different way of representing it.

So here's another problem to think about. This one really bothered me. I was considering circles and how to represent a simple circle—just the morphology, nothing else. There's no color, no line thickness, just the essence of a circle, and it could be made of anything. Then I asked myself, what's an egg shape? Not an egg, but an egg shape. It's similar to a circle everywhere, but it's not a circle. At some point, my circle becomes a little distorted, and now it's not even an oval—it's like an egg shape. How do I recognize that? How do I know, at different locations, when the circle becomes an egg? That tells you something about the granularity of the problem.

You mean, on the continuum, you could have a morphology model for an egg and a morphology model for a circle, but what are the morphology models for the in-between? When does it become one versus the other? If I show you a circle and then slightly distort it, maybe you wouldn't notice. If I distort it again, at some point you start saying, "I think it's not a circle anymore." That gives you clues to the granularity of the representation. I would say you just have a model of a circle and a model of an egg, and then when you recognize it, you pick whichever fits better with the features.

Let's say maybe I don't even have the egg anymore, just the circle. When do I say that circle's not regular? It's when you start failing to predict what you're going to see. I'm not saying this is an impossible problem—maybe our models work for it—but it teases apart some of the issues. Is it the location of the feature that's important, or is it the change in curvature, or both? I don't know. I was just trying to figure out how we recognize a circle, an oval, and an egg—such simple shapes—but I don't want a super complex model to recognize them. I don't want a model with thousands of points on it. Somehow, that doesn't seem right.

I think that's a general issue we can discuss another time, but the way we represent surfaces right now is very dense. Maybe that's the lesson for today, and that relates to the balloon problem: how do we recognize surfaces without being dense? I feel like we've already solved that problem, at least partially. When we store points, we don't store thousands of points on a circle just because the features aren't changing. We have criteria for when to add a new point—for example, after the point normal has changed a certain amount, we add a new point. If the current point in the model has a normal pointing in one direction, and then another point is rotated by 20 degrees, we lay down a new point because the features are sufficiently different. If there's a sufficient distance between two points, we store both.

My question is, if I drew a circle and you wanted to learn it that way, and then you sort the points using your algorithm so it's not dense, could I show you a hundred-sided polygon and would it say that's the same object? It's like a circle, but with all straight edges. I'm testing the model: you've used this formula, there's no interpolation. I think it would work, but with lower evidence. It wouldn't say with the same confidence as an actual circle that this is definitely a circle. It would say, "I'm getting circle vibes, but maybe I'll learn this as a new object." Every point is correct, right? If every point's curved features don't match, like the curvature is different—oh, you're storing the curvature at that point, not just the tangent. Yes, both. You're right, sorry about that. I was thinking you were storing the tangent, but the normal is correct. The normal and the position, basically. The location and orientation would stay the same, but the feature would be different—the actual amount of curvature.

If you sample points on the hundred-sided one, and you sample points the way we calculate the curvature from points, if you sample points on the circle, we still calculate the same curvature. It's going to be the same. I didn't follow that, Ron. You're saying in Monty, we have points and get the curvature from there? Yes, but if we have a small patch, it would probably extract a straight line. If the patch is sufficiently small, if the patch covers half the thing, then it would extract a curve. If there are too many sides, even a small patch would still sample points that give you the right curvature, so even Monty wouldn't know. I guess the point Jeff is raising is that there's a resolution problem: if you make the circle have too many sides, even Monty will get confused.

To me, that sounds like a circle on a computer screen, made of pixels. Perceptually, it would be a perfect circle, even though it's made of straight edges. The morphology would match, so we would get confused if there are too many edges. If it's somewhere in between—lots of edges, but not quite a circle, like a shape with many sides—then I think Monty would get high evidence for a circle, but not the same as a prototypical circle. You could start learning it as a new model, but you could also say, if queried, "Yes, this is similar to a circle," which is what we'd want.

Anyway, I don't think this is the problem I was trying to get at. I'm getting hungry, so maybe we should pause. It's a good sign that we're moving from staplers to things like balloons and squishy objects, because that means we're really solving the hard problems now. We're solving more problems than we set out to. The latest problem always seems like the hard problem, so the really hard problems we've already solved don't seem hard anymore.