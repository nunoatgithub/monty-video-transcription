I'll start with a really basic review.

This is a really complex topic, but essentially, if you look at the retina on the back of your eye, there are a million fibers coming off the retina that project to the thalamus, and then the thalamus projects to the cortex. If the area in the cortex is V1, this basic arrangement also holds true for auditory and somatosensory input. Everything from your sensors goes to the thalamus before it gets to the cortex, and this also holds true for regions of the cortex projecting to other regions of the cortex. They also go through different regions of the thalamus, which has a bunch of little regions, each dedicated to certain things, but with a very common and simple structure that no one fully understands. We have a theory about it.

There's a green thing here relevant to today's discussion. If you look in the cortex at a column, this is now layers one through six in the sheet of cells. If this is in V1, there are two projections back: 6B and 6A, and they both project back to the thalamus. In theory, these are large projections. The cortex is telling the thalamus to do something about the information being sent to the cortex.

For almost all the literature on the thalamus, it's just not clear what it's doing. They'll say it's changing the receptive fields a little bit, timing a little bit, but it doesn't look like it's doing anything significant. They refer to the cells in here as relay cells because, for a long time, when they observed them, it looked like a one-to-one ratio coming in and out. You have about a million fibers coming in and about a million fibers coming out. You get a spike on one of these cells, and you get one single spike out. That's not always true, but it is in some situations, and they observed that. So they called them relay cells because they didn't seem to be doing anything—spike in, spike out. But this is critical to everything in the brain. If you don't have it, you're dead or in a vegetative state. Clearly, that didn't look right, but that's how people talked about it, and that's why I mention it.

Leaving out a lot of details, we have hypothesized that one of the major functions of the thalamus is to perform orientation transforms. For example, you have input coming from the retina, and then you rotate the eye relative to the world. All the inputs are rotated, and you have to map them back to the canonical form. The same thing is true with motion. If your eyes are moving—if you're reading text and your eyes move from line to line—your eyes move horizontally to read the text, but if the page is slightly tilted, you can still read, but your eyes are no longer moving purely horizontally. You have to compensate for that because the models in the cortex are not orientation-invariant; they have a preferred orientation, the one they were learned in. This is similar to what you do in computer graphics. If you have a model of a house in the computer and rotate it, everything is in one model in an orientation sense.

To complete this picture, one of the observations about these relay cells is that they have a cell body, a bunch of dendrites, and a lot of synapses—maybe 6,000 is typical. Of course, the cell has an output axon.

If it's a relay cell, why does it need 6,000 synapses? That doesn't make much sense. Worse, if you look at the axons coming from the eye, for a relay cell, you'd expect an axon to go into one cell and that cell to project to the cortex, but there are a lot more synapses here than axons coming in. These axons clearly connect to far more than one relay cell. An individual axon coming in is going onto multiple cells, which doesn't make sense for a relay, because it's supposed to relay something, not integrate. Our original hypothesis was that, since almost no synapses are on the cell body—they're always out on the dendrites—the cell forms synapses with different axons from the retina, then selects which one it is going to relay. It's not integrating them; it's picking one, acting as a multiplexer. The output now is from here, now from there. If all the relay cells are doing this, you could do any kind of transform you want in this situation. You can route an input to a different input to the same output.

We speculated this partly based on observed physiological evidence from researchers, but also largely theoretical. I knew the relay cells wouldn't be doing something as simple as relaying, but they looked like relays. The most obvious thing they're doing is rerouting, and there was some evidence for that. Now there's a lot of evidence for that.

Last week, we read a couple of papers about how, on a relay cell, they can activate individual dendrites separately. You can think of this as one routing mechanism. There's a lot we don't know about that, but this is almost certainly happening.

If you're ready, I can talk about my questions. If you have questions about this, you can ask them.

Would it be possible in the model we make to have the column just learn every rotation of the object instead of having a thousand? That would be very inefficient, and then you'd have to figure out which of the models is the right one. You'd have to have many models, and if you don't actually change anything, you'd need models for every orientation you've ever seen. You'd still have the problem of knowing which one is correct at any given moment. If you're learning one model, then it wouldn't learn the other models, so it wouldn't be very effective. You also might not have a good understanding of the commonality between the models, so you'd think of them almost as separate things. I highly doubt that's happening—I'm pretty sure it's not.

This brings up the question I wanted to ask: how is the transform learned? How does it know? My assumption is this transformation is learned. I can give you the arguments for it. How would these cells learn what to do, assuming they're not genetically predisposed to do this? They're genetically set up, but my assumption is almost everything in the brain is learned, so I assume this has to be learned. That's my first question, and I'll talk about that.

The second question is about the two different projections back to the thalamus. These are two separate cell populations, often referred to as 6A and 6B. In our model, 6A is a sort of location signal, and 6B is the orientation signal. The orientation signal makes sense—this cell is telling another what orientation the sensor is in relative to the model, so it says, "Right now, you're at one orientation, so make this change." But there are all these projections from 6A, which is location, and we actually no longer have an idea what it's doing here. We don't know why the thalamus needs another location on the object, so there's some question about that.

Let me first talk about how the transform is learned. I'll just lay out what I was thinking and see if you have any thoughts. Imagine you have the retina—this could apply to your skin too, but the retina is the most studied. You might think of a patch of retina as a bunch of ganglion cells. The neurons in the retina that project out are ganglion cells. The retina does a lot of processing, but you can ignore that for now and just say the output of the retina is a sort of topographic map of ganglion cells. I'm looking at a little patch here—there may be a million ganglion cells in your retina. So there's a little patch, and these outputs are coming in and projecting. They're going to the thalamus, and after the thalamus processes it, they're going to the cortex. The general idea is that there's a topographic representation in the cortex that's similar to the topographic representation of the retina. It's not exactly the same—you don't see cells here that fire exactly like this. By the time you get here, these cells have a more sophisticated response, but there's still a topography that's maintained. It's not all random.

This is your cortex, maybe V1. It learns a model of whatever you're looking at—a sensorimotor model. You're moving around, and it learns a model of it. If there were no orientation transform, you might just say this cell goes to this part, this cell goes over here, and this cell goes to here. In the thalamus, you have relay cells. I'm just going to draw it like this—it's not really like this, but you can imagine it this way: one-to-one connections. But if I want to rotate it, I do some orientation transform. This cell now has to activate a different one here. It's worth mentioning that the connections from layer 6 to the thalamus are modulatory, while the inputs from the retina to the thalamus are the driving inputs. The modulatory connections don't make the cell fire, but the evidence is that they do select which dendrite is active.

These inputs don't make the cell fire; the cell is relaying information. The goal is not to create new information, but to modulate—do something to the inputs. We think it's an orientation transform.

Imagine the whole thing is rotated. All these cells now have to respond to a different set. For example, this cell here might, instead of getting input from here, get input from another location as I rotate it. I don't think this is pre-programmed; I think it has to be learned. The cell doesn't really know much—it's not smart enough, and there's probably not enough genetic code to determine all this. We know that people learn new orientations and new transformations, so it has to be learned.

The question is, how would these cells learn to know which place to map over here? How do they know that? That's pretty hard to do. Here's my thinking so far: imagine you start with a child, and you learn an object first in a canonical form. For example, a letter is vertical. That's the easy way to learn it. You don't show a child letters all rotated; you show them letters in a standard orientation. So you've learned a model, and let's say, for example, there is a mapping from here to here, but you haven't learned any transformations yet. Now, you rotate it. When you rotate it, we can assume the cortex knows it's rotated and changes the signal back, but it doesn't know what to do with that signal. These cells just receive the input, and now the input has changed. How would these cells learn what to remap to? That's the question, and it's a challenging one.

The only way I can think of doing this—and maybe someone can think of something better—is... Where did I write this down?

Imagine you start by looking at a simple object you've learned, and then you rotate your head slightly, changing the inputs. If you assume this happens gradually, you can also assume the object you're observing hasn't changed. For example, as a child, you look at something and rotate your head, but the default assumption is that changing rotation doesn't change the object. The cells stay active, and you're essentially saying, "I'm getting input from here." If you gradually change the input, the retina is no longer driving this cell, but it still needs to be active because it corresponds to your model. The cell could then pick input from a nearby, adjacent ganglion cell—so as you rotate, a different ganglion cell becomes active. You could say that's the new input you should have, based on a gradual change in orientation. Under each orientation, you bring over the corresponding input. This is the only way I can think of doing this: if you don't know anything else, and neurons don't know anything else about the world, you take a canonical representation, change it slowly, and assume the model is the same. The same cells should remain active, like a hysteresis effect, maintaining activity because the object hasn't changed. You can then map yourself to the new input based on orientation.

That's where I am on this problem. We want the system to be specific or sensitive to the orientation being sent down and predicted, but we don't want the thalamic cell to activate the same set of neurons regardless of their position on the retina. If you look at a dendrite, you might see a large synapse from the retina and several synapses recognizing orientation. You'd form a synapse to the retinal ganglion cell and recognize the orientation pattern at the same time, associating that orientation with that retinal cell. These retinal cells form very large, unusual synapses—one synapse can make the cell fire, which is not typical for neurons. You don't need to recognize a pattern on the retina, just form a strong synapse with that particular ganglion cell. This approach is a bit sketchy because it assumes a lot, but I couldn't think of another way to do it.

We want some flexibility, but what's the argument against some of it being hardwired from the beginning? For orientation transforms, it doesn't seem too hardwired, but for a baby to learn this is difficult—they can't control their motor movements enough yet, so their head just moves around. If things are changing all the time, they'll be confused. This assumes you've fixated on a new object, and you don't have to do this when you're young. Once you've learned these orientation transforms, they remain useful for a long time.

There are arguments for why this has to be learned. We have the inverted glasses argument and the fact that very few things in the cortical system—individual synapses—are genetically determined. They're determined to go to a certain place, but evidence for deterministic synapses doesn't exist in the cortex, as far as I know. In simple organisms like snails, synapses are completely specified by the genome, but in the cortex, the inverted glasses argument could be explained by the cortex learning to send different orientation transforms to the thalamus.

That's a little hard because it does feel like if we could do it naturally, why can't we do it for such an extreme transform? What's special about that? In general, a lot of these studies we found, which were less extreme, showed that at something like 60 degrees as you turn your head, it becomes very noisy, whereas less than 60 degrees is quite good. It seems like it could be experience-dependent, although it could also be evolutionary—organisms don't really need to deal with being upside down that often. 

There's also all kinds of issues of plasticity throughout the cortex. You can reroute to that column; this is like the old ferret experiments where they take the optic nerve that's going to the cortex and, at birth, reroute the somatosensory cortex to V1, and it seems to work. Think about all the variations of things you might do with somatosensory touch—there have been orientations, people with limited movements, or just so much variability. Over the years, I've come to accept that there's so much variability in the cortical system that you just have to assume things are learned. That's a basic assumption because there are so many examples of plasticity. I bet if you did experiments teaching people to read vertically, they'd get better at it.

I'm trying to think of alternative explanations. Orientation transforms are very general, so you could have a general set of orientation transforms that you apply to any sensory modality. The part that's actually learned, with all the plasticity, is in the cortex. Why would the cortex have to learn anything if you hardwired it? It would just learn which orientation transforms to send to the thalamus. But that could be hard-coded too, right? You could just assume everything's hard-coded. I don't see why you have to learn anything. Maybe that's the way to do it for Monty and the Thousand Brains Project; that may be the best way for you. I'm not saying we have to do this at all, but I need some really strong evidence to suggest otherwise. There's a lot of evidence suggesting this is learned.

There's a limit to how much each cell can do. In this case, an individual relay cell may have a limited number of dendrite segments. There may be ten inputs from the retina that it can change, so the amount of remapping you can do is limited in a neuron. The interesting question is: should I learn the ones I observe most commonly, and the ones I don't observe very commonly, I just won't be able to do? I can't remap a thousand axons; individual cells can't do that—there just aren't enough dendrites.

One thing about this is it feels like each cortical column needs to learn a lot of transformations. We've talked before about each relay cell having to learn, but how each relay does that is a question. Sometimes we've talked about phase codes, and a phase code feels like something that could be more general, where you don't need to learn every single orientation, but can just shift it as necessary, and that's doing the transformation. The output of a relay cell doesn't have to remap; it just has to put a different phase on these different inputs. If it's one spike in, one spike out, maybe that's just changing the timing. I don't think the evidence shows that, but I'm trying to think.

It depends on what it's relevant to. When people try to figure out what relay cells are doing, they typically do this in a slice preparation because the thalamus is in the middle of the brain and they can't really get at it. They use a dead animal, slice the brain tissue, and get a little slice of cortex and thalamus to see what happens. Those studies tend to show small changes in the output of the thalamus. One of those changes is a temporal shift—under certain conditions, they can get the spikes to move a little bit, or the response becomes broader or narrower. That's an interesting idea: if there's a phase code, but there is still a lot of evidence that you get one spike in, one spike out. It's not that you get one spike in and a lot of cells send the same spike out at different phases. It doesn't look like that. These studies are so limited, it's hard to know what's really going on because the preparations are so artificial.

For what it's worth, both the one spike in, one spike out, and the power of the thalamus when it arrives at the cortex—the spikes—suggest a more temporal code. If there is a temporal code in the brain, it's much easier if individual spikes have a lot of weight. There's some other evidence suggesting that too.

Let's think about it a little bit. The cortex has to do a bunch of transformations. We've talked about orientation, but we've also talked about scale. The way we think scale is likely implemented is a change in the beta frequency—I can't remember beta frequency.

Maybe something like that. I don't want to spend too much time on this because, in the end, Monty doesn't have to do it this way. You can do it however you want, but if you're going to use neurons or SDRs, you have to think about this a bit. It's easy to do with numbers—you just put a bunch of fusion coordinates and rotate them. I guess the one relevant area for this would be if we learn to represent spaces other than 2D and 3D. Then you would have to learn transforms in those new spaces. The question is how relevant that is. That's another big topic we’ll probably avoid today: the 2D/3D org space issue, because it's really complicated.

Anyway, I just brought this up because we have a paper that will eventually get finished—the head paper—where we're talking about this concept, and we ought to be able to logically explain to someone how it might work. A sophisticated reviewer might ask, "How is this learned?" I was asking myself the same question: how would I know this stuff? But I'd rather leave that first slide for now and move on to the next question, which is even more relevant to the Thousand Brains Project: you have these two projections going back to 6A and 6B. Now I would draw the whole thalamus, like a blob, and say, we'll just call this part of the thalamus, the LGN, which is from the retina. So it's a whole bunch of these cells—this one here and then there. For those who have been around for a while, we know there's something called the thalamic reticular nucleus. If you have the cortex here and you've got 6A and 6B—by the way, there are a lot more cells in layer 6 than just 6A and 6B, so this is just a subset. We know that 6B, which we think is orientation, projects directly to the relay cell dendrites. That's just what I was talking about.

6A, which we think is location information, projects in a more complicated way. It projects both to the TRN and to individual cells. The TRN does some MR stuff, and I can't remember—do we know if it's the same cells or the same nucleus? That was the recent paper I read. I mentioned it: classically, if you read about it, the layer 6A cell projects here and the layer 6B cell projects right there. But then there was this recent paper that said that's not true. They were tracing this and said that's true, but what it projects to is a different relay cell. So, in the same column, it wouldn't be projecting to the same cell; the layer 6A cells were projecting to here and to here. They were suggesting—I'm forgetting the details—that there's a sort of inhibitory surround about this. In this scenario, if this column is active, this cell is active, but the surrounding ones are being inhibited—not just the adjacent cell, but some distance away. That was interesting because if you look at individual cells from the cortex down to here, they look just like I drew before, but you have to make sure you're tracing them from the same spot and seeing what effect they have. It's not the same cells; it's different ones. The idea from that recent paper—there was only one—was that maybe it's providing some sort of inhibitory surround. I thought that would be a way of dealing with capacity issues, because a single cortical column may not have enough capacity to do what we want. But if I could, at any moment in time, say, think about cortical columns in the cortex—here's one column and a bunch of columns around it—there's a long discussion that perhaps an active column will inhibit neighboring columns. In that case, you'd have much higher capacity. At any point in time, on a particular object, I'm using one set of columns, and at another time, another set. There are some suggestions about that. Again, in the Thousand Brains Project, we may not have the same capacity limits as the cortex. Cortex cells have limited ability—it's quite large, but still limited.

So, that was a complication: these layer 6A cells may not be the same ones that project to the same cell as the layer 6B cells. But the question I'm having is, why am I sending down a location signal at all to the thalamus? What theoretical reason could there be for telling the thalamus the location on the object you're recognizing? In our current thinking, this is a unique location—a unique location on a unique object. So there are a gazillion of these encodings, SDRs here, and if you're expecting the thalamus to align every location on every object, that's asking a lot of it.

It's possible, but it seems really hard.

Can I ask a question? What's the relative number of neurons in the thalamus compared to the cortex? What kind?

The number of relay cells is quite small compared to a column—it's very small. In fact, the whole thalamus is small; in humans, it's about the size of a small bird egg. Relay cells project up to layer four in a column, where there are many cells, but they form less than 10% of the synapses on those cells. Generally, there are 10 times more fibers coming back than going forward, especially from layer 6A, which sends 10 times more fibers back to the thalamus than go forward. This suggests that the code going up is not really a sparse or distributed code—it's more like a one-to-one relay code, just rerouting information. That explains the low numbers: if there are a million fibers from the retina, there might be a million relay cells, but V1 has many billions of cells, so the relay cells are a small number. The number coming back is much larger.

For orientation, the brain doesn't need many cells to represent it—it's not a sparse code, more like a compass direction, which doesn't require many cells. Location, however, uses sparse distributed representations and requires many cells, making it very sparse. You don't see many layer 6 cells projecting back for orientation, but you do see a lot of layer 6A cells. For a long time, people ignored the distinction because they saw all these cells from layer 6A and thought that was the main input to the thalamus, while the others were much fewer and overlooked. This makes sense for orientation and location: orientation signals, in theory, have more direct influence because there aren't many active cells, but for location, the code is sparse and the individual cells coming back, which seem to do very little, actually matter for the code.

We discussed that if you had a second column with a hierarchical relation to this one, the 6B cells would project to wherever their input is, ensuring the direct retinal input is oriented correctly. There's some kind of relative orientation learned when you have a compositional object between columns, like the output of one column influencing another. Maybe that's stored in an L6B connection. If this projection goes to a different neuron, it might fit with the idea that it's influencing what goes in, or that they're just segregated. Six B should go through six A, but only six A projects back to the retinal ganglion cell—the thalamic relay cells that project to that column. This projection only goes to the relay cells and projects back up, while 6B projects to both this thalamic nucleus and the next one. There could be a higher-order thalamic nucleus here, with 6A projecting only to this one, but 6B projecting to both. Vivian and I developed a hypothesis about how the brain calculates relative orientation, which is necessary because objects and their components can rotate.

Orientation projects to both the input thalamic region and the next higher thalamic region, while layer 6A only projects back to the input region. That's a clear distinction. For location, the projection only goes back to the input region, so it's as if this cell is telling the thalamic input, "here's where I am." It's complicated because we know 6A is involved in location, but we don't know if the exact same cells that represent location are the ones projecting down. There are many intermixed cells in layer 6, so while many 6A cells project back to the thalamus, it's unclear if they're the same ones we rely on for location, though that's been reported. Sherman and Guillery say that a layer 6A cell projecting back to the thalamus also projects up to layer four, which matches our model: location is associated with sensory input. If they're right, this would suggest the same cell projects both to layer four and the thalamus, making it the location signal. It's associated with whatever is in the thalamus, but it's not itself location information up here. This representation might not mean there's location there, but it needs to be somehow useful for location.

We've speculated that these cells might be very phase dependent. We've discussed sets of cells with the same movement orientation but different phases, where the phase determines the location. Maybe you ignore the phase when projecting down, so if you have those cells, they're all representing movement in the same direction but at different phases.

The cells that are in phase with the theta cycle would represent the location. If I ignore the phase, then all of them are active at the same time, and I'm just representing a movement direction.

Do we know what the TRN is doing as opposed to the LGN? No, it's inhibitory, meaning the TRN cells themselves project back to the relay cells and inhibit them.

There is a complex network here. Some of these neurons have what are called roro connections, meaning they are tied together through their dendrites, so they don't act like typical neurons. They act more like a mesh and are believed to be involved in rhythmic generation, producing different rhythmic signals. No one knows their exact function, but they are generally inhibitory to these cells.

They spread, are arrhythmic, and don't act like regular neurons. They don't project back to the cortical areas. These UHT neurons project to each other and back to this neuron. This is interesting in relation to what was mentioned earlier about connections to the LGN neurons. There are two different ways it could be, and I'm not sure which is correct. One shows the inhibitory connection: the classical view is that the layer six neuron sends an excitatory signal to the TRN neuron and to the thalamus relay neuron, and then the TRN sends an inhibitory connection to the same thalamic neuron. Alternatively, the layer six neuron sends excitatory connections to both the thalamus and TRN neurons, but to different populations. The ones that get input from the cortex then send inhibitory connections to the surrounding relay cells. This suggests column inhibition, where you select the orientation transform for one cell and inhibit or disinhibit other cells.

Regarding capacity, the thalamus is small, so only so much information can pass through. Attending to what is important means processing only certain information. When I was talking about capacity, I meant the capacity of a cortical column to learn—how many models, locations, and sensory pairs it can learn, such as layer six A to layer four connections, and how many classifications it can make. Early modeling suggested that an individual column with a reasonable number of neurons could learn, for example, 500 objects with 40 sensory pairs, but that's not enough. V1 can learn a lot, but not everything. Even though it can learn complete objects, it can't learn all complete objects, just some set. The question remains: do we need more capacity? I was thinking more about the capacity of the cortex itself. If every column learns the same thing, the capacity is not sufficient. If different sets of columns are active for different models, then models are distributed across columns. This introduces competition to ensure columns learn different objects. There's also competition at a moment in time, where only certain populations of relay cells are active, preventing a cacophony. I hadn't thought of relay cells needing to compete with each other.

If they're doing what we think, we're not asking too much of them. We're just asking them to remap the relay cell, to remap which input is driving it. That's all we're really asking. In general, the thalamus doesn't know anything about models; it's just transforming input, either sensory or motor, and knows nothing about the models in the cortex. Layer six A is model specific, so if we're right, it's telling something specific about a model. I wouldn't expect the thalamus to know anything about a specific model. The thalamus is just a routing mechanism; the model should be in the cortex, not in the thalamus. If the location signal is unique to objects and we're sending it down to the thalamus, the thalamus will associate something unique to a particular object, which doesn't make sense to me. Maybe, as you said, there are other neurons in layer A, possibly a different subpopulation in that area.

One figure that never made sense to me divides it into the parvocellular and magnocellular pathways. Maybe that's interesting to look at now. Upper layer six is the parvocellular pathway, and that separation exists in the LGN as well, with forward and backward mapping from layer six. I'm looking at this figure to see if it makes sense.

It doesn't quite make sense. The problem is that this is looking at primate V1, which has the striate layer four, which is unique. It has four sublayers: 4A, 4B, 4C alpha, and 4C beta. Most of the cortex doesn't have that; most just have layer four. Many mammals don't have a striate cortex, like dogs and cats. Some mammals don't have it at all. This is an additional capacity for vision only in certain higher mammals, and no one knows why those different layers exist. This is not the canonical form; this is a unique system.

In my view, the magnocellular cells represent movement, and the parvocellular cells represent features. We can discuss why that is. Looking at this, the magno cells should represent movement, but they're only shown projecting to 4C alpha, which is unclear—maybe that's the movement vector, but I can't make sense of it. I'd prefer to see the canonical column, where inputs go to the border of layers five and six, and the border of three and two. Those are the classic inputs, but this diagram doesn't show that. Regarding the magnocellular part, we don't usually think of it as going to layer four as much. Some people argue that this is a mislabel, that these aren't really different layer fours, but actually different layer fives and threes. It's a historical accident in labeling, so when we say it's going to layer four, maybe it's really part of layer five. Personally, I've tried to avoid thinking about these striate, meaning striping, layered V1 areas because of the confusing nomenclature.

It's worth noting that the relay cells—magno, konio, and parvo—are physically aligned to project to the exact same part in the cortex. The relay cells all project to the same place, maintaining topology. If you think of this as motion and sense features, you'd want to implement the same orientation transform on all three types. They're all transformed the same way: movement and sense features both change with rotation, so it makes sense to keep them separate because one is movement and one is sense features. There's some evidence that konio is also movement, but often the konio input is discussed as coming from the superior colliculus, not the retina. I'm not sure why it's shown as coming from magno here, or what the konio one exactly is—it's just another layer.

There are three basic types: two big ones, parvo and magno, and the konio, which is smaller. Also, parvo and magno are subdivided by left and right eye. In the classic view of the thalamic layers, if you look up images of thalamic layers, you'll see them. The LGN is V1, and you see these layers of cells, all with congruent receptive fields—each cell has the same point on the retina and the same point in the cortex. You can see left eye (contra), same eye (ipsi), so these are the parvocellular layers: one eye, the other eye, and the same for magnocellular. There are two pathways from the two retinas, processed in parallel. They don't even show the konio layer here, but it exists—it's a small one, a little higher up. In some figures, the konio layer is prominent, in others it's in between. I thought it was mostly at the top, but here it's also in between, so maybe this is different than I recalled. That suggests it might be doing something different, maybe modulating or acting on all these at once. Nobody really understands the konio layer.

The main ones are magnocellular (movement) and parvocellular (features), and why it's divided in two here is unknown—I've never heard an explanation. It is left eye and right eye. I didn't realize the konio layer was interspersed between these.

There are many things we don't know. This is an important question for the Thousand Brains Project. It's bothersome to me that there would be a location segment coming down here, because that would require this area to learn unique locations, and it almost can't do that. There's almost no way it can.

That bothers me. If you believe Sherman and Guillory that these are the exact same cells projecting to layer four, you should go back and check that. Are they certain it's the exact same cells? Did they stain a cell and see one up here and one down here? If it's the exact same cells, that's puzzling to me. That's where I came up with the hypothesis that maybe it is sending these signals down here. If there was phase encoding, it could be ignoring the phase, and it would just look like a directionally sensitive cell. The directionally sensitive cell could be something this area could use, perhaps, because I'm not asking it to learn specific locations.

To make associations between two relatively independently moving sensor patches, like two fingers, that has to happen in the follows, right? Why would I have an association between them? Let's say I anchor the grid cells on one of the columns and want to send that signal to the other. That's an assumption we haven't made yet. You start by thinking about how a single column works, and a single column has to be anchored on its own. If I understand that, I don't have to assume it's telling another column how to re-anchor. I don't want to start with that assumption. You could have two different columns—imagine two different fingers. This finger doesn't really have much to tell the other finger. The orientation is independent; this one is rotated independently of the other, and somehow we hope they all work out the same. I wouldn't make that assumption. I would assume that a single column, as many times as needed, has to be able to do this, and we need to work out the mechanisms for a single column.

Just throwing out another thought on the layer 6A connection: could we think that part of layer 6A uses a shared location space, and then, like we talked about last week, a subpopulation in that shared location space gets specified to form a unique location space? It only projects the shared location space down. That's a lot of assumptions. Then you're saying I'm also learning a 6A to 4 connection based on a shared location space—partially, yes. That would be like learning the cylinder shape and its rough features, like curvatures everywhere, in a shared location space. But then the specific cylinder pull objects, like the coffee cup, would be a specified version of that location space. It's also a little inconsistent with the number of fibers coming back from layer 6A to the thalamus. In V1, you might have a million relay cells coming in, but you have 10 million layer 6A cells projecting back. That's a lot of cells for a shared space. It could be, but I don't know.

What would the thalamus want to know? Why would I want to tell the thalamus something else? What other information could it use? We need to do two things: it needs to figure out the orientation transform, and we have a sort of complete solution for that. What other information? It also has to figure out scale, and we haven't talked about scale much. Scale is when you have a composite object, and you say the object could be further away or closer—that's different scales. It could also be relative scale, like the size of a logo on a cup—a smaller or bigger logo. It has to learn that. It also has to do scale, so it could be related to scale, and maybe scale could be related to specific locations on a specific object. For example, on the coffee cup, there's a scale that has to be here. I don't know if the anatomy is really consistent with this, but that's why I was trying to bring up the relative orientation between a parent object and a child object again. This is a nice solution for calculating the relative orientation, but just in terms of having a strong hypothesis or conviction that we're now on this parent object, we move to a location and want to predict the orientation of the child object and bias that orientation. That's going to be location-specific, and we need to store and communicate that somehow. That's what I was trying to think about.

Vivian said it was a direct 6A to 6A connection, which it could be. But it has found specific locations on the object, right? You have to say, for this child object on this location—like a coffee cup with a small logo and a big logo, and a rotated logo. I have to be able to learn both of those. I appreciate the location has a lot of information, but if they really do have such a rich dendritic structure and their main concern is orientation, maybe that's what they're storing. Maybe it helps that it's projecting to a different cell, but I don't know if it actually is. 

Another possibility, getting back to the idea of shared and non-shared location space: across layer 6A, there are a lot of neurons, and they could represent a highly sparse, unique location in space. But when projecting down to this relay cell, there are 10 million fibers coming back, but only a small area would project to a small area here. It's like there's a topological arrangement.

It might be that they're relying on topology here, that the layer 6A cells are converging in the same spot, or maybe it's more continuously spread out. 

This is just the proposal for learning the relative rotation of the child object to the parent object. We didn't talk at all about layer 6A here, but how do we associate the relative orientation with the specific location on the parent object? That was the trick—we want to say, on a particular location of the parent object...

We would send the relative orientation of the child to the parent up here, to the lower layer three or four, to activate these columns. Whatever feature goes in there would be associated with the location. That's done up there, right? That's right. It worked nicely. I can't remember. I'm so glad you made this picture.

I don't want to get hung up on this. I don't want this to impede Monty and its progression. It is bothersome to not understand this. Let me just send him a message.

Maybe one potential takeaway for Monty is that a lot of the transformation stuff we need to do, we're already doing. But the idea that the thalamus is creating a more competitive environment between the columns is something we could consider. If we find that all the columns are just learning the same objects, they would, unless we did something like this. It doesn't need to be exactly this, but just having something that addresses capacity issues. Some astute observers have asked about the capacity of these columns—how many things can they learn? It's going to be limited. They can't learn everything. We talk about V1 being able to learn complete objects, but it can't learn all complete objects; it's only going to learn ones that are small areas of the retina. If it's still named enough, we might have a way of expanding capacity. Maybe in Monty we may not have that issue, since we can build unlimited resources into anything we want. But that's a good takeaway. Ultimately, we'll have to deal with the scale issue and figure out how scale is represented and how it determines scale per location. It might be related to that. One reason to suggest that is, if this is location and the TRN is associated with various oscillations, you could be selecting a particular oscillatory scale here, telling it what scale to use based on the location. Maybe this is time orientation, and we're going to use this for determining scale somehow. I don't know.

One thing that's currently different in the Monty implementation compared to how we talk about it in the brain is that in Monty, we do the transformation of taking the current location, adding the movement, and then getting the next location outside of the cortical column, and then give the column the next location. Here, we're saying the column gets the movement vector and does the location transfer. Is there a way to think of that actually happening in the thalamus? Basically, this layer 6A cell tells the thalamus, "I'm at this location right now." The thalamus gets the movement vector and does the path integration. I know it's a bit of a crazy idea, but it doesn't seem like there are enough cells in the thalamus to do path integration. It's a simple structure. There are, though.

If you look in the thalamus, there are basically two types of cells: relay cells, which we've been talking about exclusively, and the TRN, which is separate. There's also something called matrix cells.

For many years, I've been working on the assumption that these are the cells that determine timing for sequences and movements. There's a lot of evidence accumulated over the years that supports this. I think they project to layer one and could provide a timing signal. If you're trying to learn the durations of elements in a sequence, like the duration of notes in a melody, these cells would be like a countdown clock that says, "Beginning now, you can count about a second, then you have to reset, another second," and so on. I mention that because there's more. There is another cell type in the thalamus. These are very different from relay cells. As far as I recall, they don't get any input from the sensors; they're more like they get input from the cortex. The way I think of it is they're told when to start. If you listen to a normal melody, notes always have an attack at the beginning.

There was some evidence that these cells become active at that attack and then play out a timing signal, which then gets sent up to the cortex so the cortex can learn the duration of individual elements in sequences.

Beyond those two, I'm not aware of any other cells in the thalamus. There might be some, but I'm not aware of them. So what cells would do path integration? They don't look like they'd be there. By the way, in one of these other papers—

Did I print that one out? It's on my computer. I think it's on my computer.

Yeah, it's on my computer. I could show it to you.

Where was I going with that? Why did I bring that up?

It could be some interaction with the TRN, with one inhibiting some of the cells in the LGN, but it's a totally different mechanism than the grid cells. My general rule of thumb is that it's generic—it doesn't know about specific objects. It can do orientation transforms, scaling, provide a timing signal, and modify that timing signal. Sometimes you want things to speed up or slow down, just like you want to be able to scale time. You can scale space, but this is scaling of time, so you could change that rapidly. When you want to play back a melody faster than you learned it, these cells would speed up their timing signal. These are things that apply to all models: a timing mechanism, a transfer mechanism, a transformation of movement, and scale. This would be scale for timing, and it might work for scale for other things too. It's generic—nothing specific about any particular modality or model. That's how I see it. I don't think there are other cells here that could do path integration; it doesn't seem like there are enough cells. It would have to be something in the TRN. The TRN is pretty weird. It doesn't look like it would be doing that. The TRN is often referred to as an attentional mechanism. If something occurs that you didn't anticipate, it essentially opens the gates to other cells. 

For example, in the thalamus, when inhibition is present, you can open that. You had it there—I had it on the other laptop. Not that one.

One of the things I did is just that on B, you're inhibiting these other relay cells nearby, but when you do, they're in this pre-bursting mode. If an unexpected input comes in, those outside green cells would burst, firing rapidly and resetting everything. It's like if I'm attending to something here and then there's a little flashlight off to the side, I lose my attention and move over to attend to that. The relay cells and TRN cells are associated with this resetting of attention signal—if something unexpected occurs, you have to attend to something new instead of what you were attending to before. There's a lot of literature about that. That's interesting. Could that be a clue as to what the layer 6A signal is, that the location signal somehow incurs a sudden prediction error in location space?

In fact, that's what many people have speculated in the predictive coding world: the cortex tells the thalamus what to expect, and if the thalamus doesn't get what it expects, the cells burst and attend to something else. My problem with that is, I can imagine the thalamus being told what feature to expect—here's a feature you should be expecting, like a line—but I have trouble explaining how it could be specific to an object. The object-specific part doesn't make sense to me.

I don't want to spend too much time on this. We've probably spent a lot of time on it already. I don't know if we have time for more.

This was an interesting paper. Going back a bit, layers two and three in the cortex are often just lumped together as layer two/three. People didn't really know why, and some talk about it. Want to go to a figure? Maybe in a second. Go to figure one. This is a very misleading figure, as I'll point out—very misleading.

In our models, we've looked at it like this: layer 6A and layer 4 are bidirectionally associated and connect to each other. This is a location, and this is a feature, and you're associating features with locations as a basic model. Then you need to form a representation. Sometimes you don't have to do this, but sometimes you want to put a label on the object—what is it? Then you'd have another layer, which we've always said is like layer three. We call this a temporal pool. It takes all these associative pairs—these representations represent a location and a feature—and since it's unique to the object, every one of these representations is part of this common object. This would be the object, and these would be the location-feature pairs.

An object is a bunch of location-feature pairs. This would be a stable representation while these are changing. For example, my finger is moving over the coffee cup, but it's still a coffee cup. My perception is that it's stable. Layer three cells project to the next region, but these cells don't. Basically, you're aware of the object—you're touching it, sensing it—but unless you attend to it, you're not really thinking about where your fingers are on the object or what they're feeling. You just pick it up and say, "I'm holding my coffee cup." I don't say, "My finger is on the little lip there." You can do that, but generally you don't. So, you have this object, and that's the basic model, and these operations have to occur. This is what inference is: figuring out a common representation for a series of changing sensorimotor inputs.

This becomes the input to layer four in the next higher region. In the paper, we wrote that this object is now a child feature of the next object up in the hierarchy. All that looks nice, so it makes me feel good about that.

But what about layers two and three? They're not one thing. In the paper, they were trying to measure the physiology of layers two and three—what the cells respond to. They weren't looking at the anatomy, just how layers two and three respond differently to inputs. What's misleading about this figure is that it's an anatomy figure, and it doesn't show layer three projecting to layer two.

In the paper, they point out that inputs to layer two are not well known and were not shown. They may come from layers three, four B, and five. If you look at this figure, you would think that layer three is not connecting to layer two, but here they say it might, though they're not really paying attention to that. Other people have clearly stated that layer three projects to layer two. If you start with this paper, you get a misleading representation of what's going on. This is classic neuroscientists ignoring other data, making a picture that would confuse you. We don't really want to look at this figure; it's really misleading, and it's the first figure in the paper.

Let me talk about what's going on here. The upper line is a layer two response property, and the bottom line is layer three cells. The monkey is sitting in a chair with its head fixed, staring at a dot on the screen. The monkey is trained to keep its eyes on the dot; if his eyes move, he doesn't get water. Off to the side, not where he is staring, there are cells responding to a part of the visual field. They put a sinusoidal grating there—lines moving in particular directions. The cells in layers two and three, the ones they're showing results for, are movement sensitive. The cells respond when there's an oriented line moving. Above, there's a long line and a short line, with little arrows showing direction. It's not just a line, but a sinusoidal grating moving through space, and they're showing how wide the line is. The cell prefers a certain width. Layer two prefers longer, broader things, and layer three prefers much shorter lines.

The big difference is that in layer two, the cells respond if the line is moving in either direction. On the top line, every half a second or every second, the direction changes, and the layer two cells respond in both directions. The little vertical lines are the cell spikes. Layer three cells respond in both directions, and a plot on the right shows this. The layer three cell responds to movement, but only in one direction. When the lines move one way, it responds; when they move the other way, it doesn't. This is one of the primary results of the paper. Also, layer two cells are much less picky—the line can be big or fuzzy, not very precise—while layer three cells are much more precise, needing a skinny line, exact orientation, and movement in a specific direction.

A couple of things come out of this that are problematic for us. If layer three cells are the inputs to the next region, and if they're actually measuring the same layer three cells projecting to layer four, those cells are only motion sensitive in one direction. That's not what I just said; I said these would be stable over multiple movements of the object. I have some thoughts about this. Sometimes there's evidence that layer two also projects here, so you have layer three and layer two and these different things going on. What is going on here? Why are these motion sensitive at all? We don't want them to be motion sensitive.

Before we get too concerned about that, there's another paper I read that discusses an issue we've talked about a lot: the difficulty in figuring out what cells you're looking at and why. They take the monkey and show moving sinusoidal gratings. They don't see what happens if there's an image; they only show movement, so they're only going to find cells that respond to movement. These cells are not easy to find, so when they find one that's responding, they focus on it, but the vast majority of cells aren't responding to these inputs.

So, remember that? They don't say it directly, but it's hidden in the data. This could be very misleading. There could be many different cells up here. We've talked about the possibility of path integration happening here—where the object is moving, not in space, but exhibiting its own behaviors, like a stapler opening and closing. That might be represented up here, and these might be related: path integration in object motion and object behaviors. It's very misleading, but I still thought it was interesting that they found cells where the distinction between layer two and three was that one cell responded in one direction, and another responded in the opposite direction.

This gets really obtuse because, in the path integration technology we've discussed, you want cells that are phase encoding and moving in both directions, not just one. This suggests that maybe there's path integration, and maybe layer two is doing path integration, really representing the location or movement of components of an object. They would encode the state of the object. It is encoding the state, like a predecessor to the state of the object. For example, the stapler is opening—not moving relative to me, but part of the stapler is moving relative to another part. That's different from the stapler moving through the world or me moving through the world. These cells might be doing the path integration of that component.

The gate of its cage—that is learned. The monkeys generally live in a cage. The monkey doesn't get to see that; it just looks at a screen and a dot all day long. They did this for about two hours until the monkey was satiated. They get the monkey really thirsty, and then it gets a little drop, one little drop, and it lasts for two hours. Once it's no longer thirsty, it stops doing the task.

I just thought it was interesting. We talked about layer two and layer three and didn't really have a clear distinction. We talk about classification, and there are clearly sets of cells up there that are most insensitive, and there's a differentiation between layer two and layer three. I thought that was interesting, and I'm filing it away in my head.

This suggests there might be path integration occurring in that region relative to object behaviors. I've speculated for a long time that this is where it should be—the lower input to lower layer three, upper layer four, perhaps motion-related. There's some evidence it's processing motion and likely doing something, but it doesn't make sense to send that lower line signal to layer four if it's supposed to represent the object. I'm pretty sure it does. There are probably different cells in layer three projecting to layer four than these other cells. When sending that signal, is movement itself the object state we might want to send at the object level? They might see it when doing sinusoidal gradients, but if this is more about encoding the movement of the whole object, this would be movement of the whole object, while the other would be movement of part of the object.

If it's movement of the whole object, that might be more useful to the next layer—moving the whole object through space, or meaning it's changing, like a behavioral state. This could be any kind of change to the object itself—closing, morphing, flapping its wings. The point is, the line down there on the B1 is restricted to a small dimension. If you showed the whole field of vision moving, these cells wouldn't respond. They don't want to respond to you moving in the world; they want to respond to some part of the world that's moving while the rest is not. That suggests a focus on the object itself, not the whole world. In the lower layers, the cells respond to very broad areas of vision, which is more like motion in the world. Path integration might not even be happening in layer two/three, but rather in the lower layers, with layer two/three just encoding the state of change.

But wouldn't you need to separate mechanisms? My location in space—my body's moving—has to be maintained separately from objects moving. I can't use the same mechanism for both, right? You could do path integration in the lower layers by having each state of the object as a different model, then learning associations between how to move through the different states. For example, with a stapler, the higher region could look at an object composed of two components in the lower region and their relative positions as they move. It might take a hierarchy to do that. The relative locations of those two parts could be encoded in the layer 6A to 4 connection, or it's more about learning the relative orientation of two components—two objects—and how that orientation changes. We don't really want grid cells in layer two/three as well.

But what would these cells do up there? It would make sense if they just encode the state of the object, and that state changes as the object moves. If it encodes the state of the stapler—neither completely open nor closed—it fires every time the stapler enters that state. Once it stops, the cells stop firing, so they wouldn't encode position, only the relative change of that position. These cells would stop firing if the stapler top stopped opening.

It might be part of the state of the object whether it is currently in motion, but I'm not sure. For the newcomers, we're bringing up all our uncertainties, but we do understand a lot. The research meetings aren't to go over what we understand, but the things we don't. I'm trying to figure out a way to explain this without needing grid cells and location representations in layer two/three. Since these are temporary, these particular cells wouldn't fire if the object stopped moving; they couldn't represent the state of the object, only the movement of a component. The monkey never sees the rod stopping; it only stops at transition points, reverses direction, and the cells respond. The event or state of the model is always in motion. I'm pretty sure—about 98 percent—that these cells would not respond to a static image. They would respond to motion only, though I can't be completely certain.

There are many cells in these regions, and the ones recorded from are chosen based on the experimental setup. Anyway, it was interesting data—something I hadn't seen before. What to make of it, I don't know yet.

These are things I'm filing away for the future.

For future discussion, especially for newcomers, there's this concept that we haven't really addressed yet. All we've done so far is recognize static objects and movement sensations, but we haven't dealt with objects that move themselves and have behaviors. For example, a bird with its wings out is still a bird with its wings in, and if it's moving them back and forth, that's typical bird behavior. You wouldn't see a bird moving just one wing back and forth; you've learned that. So, the model of a bird has to include behaviors, or the model of a stapler has to include behaviors you typically observe, like opening, closing, staples coming out, and so on. This is an important part of sensorimotor learning because so many objects have behaviors, and we push, touch, and manipulate them. We don't really have that worked out yet. We do know how to recognize objects and multisensory touching and seeing, but I don't think we have all the answers yet. We're getting close, but at the moment, it may not seem like it.

I have one other thing, unless someone wants to discuss this further. I just thought this paper was interesting.

There was this paper about concept cells. Is that the one I have here? Yes.

This paper is about humans, specifically in their hippocampal complex—the hippocampus and the entorhinal cortex, which together are called the medial temporal lobe (MTL) in humans. They talk about MTL, which is the same as the hippocampus and entorhinal cortex in rats. In these studies, humans who are awake have electrodes implanted in their heads because they have intractable epilepsy and are going to have part of their brain removed to stop the seizures. For five days, they have electrodes in their heads so doctors can figure out where the seizures are starting and know what tissue to remove. During those five days, the patients agree to be subjects in experiments.

They have human data, and they can measure from individual cells or groups of cells at once. Concept cells is a term for what some people referred to years ago. People found during these experiments—when the subject has electrodes in their head, in the temporal lobe, deep in the brain, in the entorhinal cortex and thalamus, the hippocampus—that there are cells that respond to concepts. The famous example is Jennifer Aniston. I guess she's an actress from "Friends." There's a cell that responds to Jennifer Aniston, and not just to a particular image—it responds to anything related to Jennifer Aniston. If you spell her name out, that cell responds, or if you show her name written in different positions, it still responds. They call these concept cells.

This paper was about that. It wasn't particularly interesting—they were just showing in humans where the hippocampus is and so on. What was interesting is the whole idea of a concept cell, like a grandmother neuron that just responds to Jennifer Aniston. That doesn't really make sense. You can't assume that one cell represents Jennifer Aniston; that's nonsense. No one believes that. How many different things can you encode? This gets to the nature of sparse distributed representations. We've argued that when you represent an object, you're using a very sparse code. Individual cells don't always respond in the same way for that object, and they'll also respond to lots of other objects. If it's a 2% sparsity, that cell should become active for every 50 things.

This paper got into that question, and I thought there was some interesting data.

It says that in a population of a billion neurons, less than a million are involved in the representation of a given concept, such as Jennifer Aniston or Halle Berry. Each of these MTL neurons may code up to a few dozen of the 10,000 to 30,000 things a person can recognize. They're saying maybe these cells respond to a dozen different objects, but that's not enough—the math doesn't work out. Then it says both of these estimates should be taken as upper limits; the true values may be a couple of orders of magnitude lower. That's a big caveat. And because it's difficult to select very selective neurons, let's go down to this box here.

They're discussing how, when examining a particular neuron to see what it responds to, the vast majority of neurons seem silent and don't respond to anything. Researchers often assume these neurons aren't doing anything, but in the context of sparse distributed representations, it may simply be that the right stimulus wasn't presented. In their study, they looked at three neurons—one blue, one red, one green. Two of them fired a lot of spikes (6,700 and 1,800 over some period), while the third fired only 218 spikes during a 30-minute recording. Over 30 minutes, showing all these images, only 218 spikes were recorded from that neuron, which is very little—almost like noise. Yet, they could still determine what that cell seemed to respond to: two different towers and some rabbits. These cells responded to 40 times more stimuli. The paper argues that the data typically obtained from these experiments is biased because researchers try to pick things the subject would know—TV characters, family, researchers—hoping to get a response. However, most neurons may still represent things in the world, but in an extremely sparse way, so you wouldn't see their activity unless you happened to present the right stimulus.

Neils and I have debated this for a while, and as a group we've discussed it, but the paper makes good arguments about why it's difficult to get a proper representation. 

In images where concepts are very familiar to the patients—such as pictures of themselves, family members, experimenters, and celebrities—the probability of getting a response increases. Personally relevant items elicit the largest number of responses. Still, most of the 30,000 things a person can recognize may not be represented in the entorhinal cortex at all, as these may not be salient enough to trigger a memory process. The authors also argue that these items might be represented in the medial temporal lobe, but you won't find them because they only spike every 200 or so times in 30 minutes. There is interesting data here about the problems of detection and the bias present in these experiments. When we talk about finding cells that respond to concepts, that's the exception. The vast majority of concepts wouldn't be found because they're not very familiar. They gave an example of one cell that fired 218 times over 30 minutes, and it was just luck that they found something that made it fire. Most cells, they said, you would eventually find something that would make them fire.

To clarify, they're not claiming it's a "grandmother cell" like the Jennifer Aniston cell. It's still a distributed code, but the only thing they found that this cell responded to happened to be Jennifer Aniston. There are likely many other things it would respond to. They also included a helpful figure.

This figure shows cortical regions: the pink one is V4, and the other is upper auditory cortex—areas where you expect objects to be recognized. The cortex already has representations of Jennifer Aniston or similar concepts, and the blue regions are the medial temporal lobe, including the entorhinal cortex and hippocampus. These are three steps in processing, so you're not feeding in raw sensory data but representations of Jennifer Aniston into this system. Our argument has been that the mini-columns themselves might represent something like a "Jennifer Aniston mini-column," given how highly represented she is in the data. The authors, however, suggest you only get Jennifer Aniston representations up in the hippocampus. 

Our theory is that these blue regions are predecessors to a cortical column, existing in older animals as just the entorhinal cortex and hippocampus. A cortical column is essentially a collection similar to this, with the entorhinal cortex representing location and the hippocampus representing object-feature cells. The cortex has its own model of Jennifer Aniston, which it then feeds into the blue region, which learns temporary memories and associations about Jennifer Aniston. 

The paper highlights why the data is biased and why typical representations might not be seen. It's a warning we've discussed before, and it was nicely articulated in this paper.

I'm not going to read all this behavior section.

As mentioned before, our concept cells do not act in isolation, but as part of cell assemblies. This is what Neil wants here. They're speculating, as you are, Neil, that I have representations for Luke Skywalker, Yoda, and Darth Vader, and some of the cells are common in those representations. When you see Luke Skywalker, you get some of the same representation as Yoda. These are all Star Wars examples. It acts like that, but they didn't walk through the numbers to see if it actually works, which is my problem with this. They're arguing this might be what's going on, which is what you've argued, Neil—that there are these overlapping bits. In my opinion, they didn't walk through it in detail; they just threw it out there.

To be fair, my understanding is that's also something you meant to use. I argued for this and gave talks on it. I think even now, does he still do it? SDRs should have this kind of property, but I don't know how it works. Maybe I used to believe it, but I don't anymore. Maybe I'm wrong. It's difficult. I can walk through the difficulties, but here, this is a picture of what you've argued might be interesting. There's a nice drawing of that, sort of.

This paper talks a lot about this, so I thought that was interesting. Thanks, that's it.

Again, lots of these are papers I thought had interesting numbers. I just mention them because it's worth keeping track of. It's an interesting curiosity. I don't think it's necessarily that significant, but there was a paper a couple of years ago that showed similar things in transformer networks from OpenAI and others, where they found a Spider-Man neuron or a Trump neuron that would activate to Spider-Man written down, the Spider-Man logo, Spider-Man images. Were those sparse representations? I think they argued something similar—that it's not like this neuron is only active for this, but they were relatively sparse. The problem I have with this coding is that when you get very sparse, there aren't enough neurons to do the overlap. I guess those are real-valued neurons, so maybe they have higher representation capacity. They're all real-valued neurons with very specific real-valued synapses. You're not really doing a sparse code. It seems like with real-valued neurons, you're not relying on sparse coding.

This was a regular clip.

They show a model, an image, and then the text of that image, like captions. Then they map those two representations together in dense space. They're real-valued, not really sparse coded, but they learn this mapping. Then they look at the individual neurons to see what output they activate at the end, or they go backwards from an image, look at the representation, then go backwards to see which neurons are responsible for activating that image. Then they find, oh, this neuron is responsible for all of the same concepts. We've talked about writing this down.

There are different ways to represent things, and this may or may not be useful. Is it?

I think it would be worth having an explicit session just talking through this idea of SDRs encoding meaning and coming to a conclusion if this is something we should keep pursuing. Good idea. There's a lot of that in HTM School as well, so we don't need to cover it all here. One of the big questions is how neurons represent information. The classic way, going back a long time, is you start off with firing rate, meaning the neuron's spiking and the faster it spikes, it's a graded signal. There are lots of examples of that, like a neuron integrating a muscle fiber—it's basically a rate-coded neuron. If it fires faster, the muscle pulls tighter. There are lots of neurons in the old part of the brain releasing chemicals and so on that are firing rate-based. But firing rate is hard. We don't believe that's happening everywhere. 

Another thing you could do is have a sparse distributed representation. You might have a million cells and 2 are active, or 10,000 cells active. That's a lot, but it could be 10,000 out of a million, or a hundred. The point is that the actual rate of firing doesn't matter; it's which cells are active at the same time. This has very high capacity, meaning you could represent a ton of things—an unlimited number of representations. It has other desirable attributes, which Sumati wrote about in a neuron paper: noise tolerance, robustness, and so on. It's incredible. We know that in the brain, or in the cortex specifically, there are lots of sparse representations, so we're pretty sure that's happening, at least in the cortex. 

We've also had another idea: there are mini-columns in the cortex. You might have 10 mini-columns representing different orientations, and three are active at once. You're representing a 360-degree orientation and moving it—it's like a three-of-eight or three-of-ten type of representation. It's not high capacity or unique, but it's simple, you can do path integration on it, and it suffices for many things. Our basic neuron theory combines this with sparse representations. You start with a non-sparse mini-column representation and make it sparse by picking individual cells, which gives it all these other properties. 

Another thing we've learned recently is a representation that was before.

This is the phase using phase. This idea appears in place cells and grid cells, where you have a representation that is between sparse and non-sparse. Grid cells and place cells are sparse, but not extremely so—perhaps semi-sparse. The same neurons, different cells, fire at different phases. If you look at grid cells in a single theta phase, say about a hundred milliseconds, you go through a series of representations in order, packing in several of them rapidly. You might have representation one, then two, then three, all occurring quickly because it's the same neurons, but which ones are in phase changes as the theta cycle progresses. So you have a series of representations in time, and this clearly happens in parts of the brain. The question is whether this could be happening everywhere in the brain. In addition to SDRs, could you also be doing phase versions of SDRs? That's essentially what this is describing. The decision of which cells are attached to which phase—whether that's hardwired or encoded genetically—is unclear.

I don't know if anyone knows the answer to that. This comes from studying grid cells and place cells, where phase precession is observed. For new people, when an animal moves in an environment, there's a representation of a location called the place cell. When the animal is there, that cell fires. Other cells fire at other locations. As the animal approaches, one cell first comes into phase with the base frequency before the animal even reaches that location. As the animal arrives, that cell becomes in phase and active, coinciding with the location. After passing it, another cell becomes active. It's coding a series of locations in order as the animal moves, firing rapidly. If you were decoding these, you might say, "I might be here, or here, or here." I've thought this could be a way of counting noise in path integration. The idea is you don't always know exactly where you are, so if your sensory input matches, you realize you're further along than you thought and adjust. For example, walking in the dark, expecting to feel a wall, and realizing it's further than expected, you re-anchor yourself. You can feel that adjustment happening, and that might be what's going on. I don't know if that answers the question. The question is whether these are hard coded. I think the mechanism for phase progression is hard coded, but there's a question about how you can convert from a firing rate code to a phase code. You can imagine, for example—

If this is membrane voltage over time, and you have two neurons receiving a lot of inhibition, their membrane potentials drop to a common value. Due to the way conductance works in synapses, there's a limit to how negative they can go before the sign of the voltage gate flips. They can rest at a very negative value. If these two neurons receive different amounts of firing rate input, one will rise quickly, the other more slowly. These are not spiking yet, but when they reach the spiking threshold, the one with higher input spikes earlier. This is proportional to the difference in their inputs. That makes sense. This relates more to rate encoding, or using rate coding as a basis to switch to phase encoding. Once you spike, the rate no longer matters—spikes are spikes, and there's no more rate.

There are some people who believe that information is encoded in the intervals between spikes. I think the evidence for this is really weak. They like it because you can encode a lot of information that way, but with rate encoding, the problem is that it's very slow. Between one spike and the next takes some time, and you don't have any rate until you've got the second spike. It's really slow.

This is interesting—it's an integration of input that then converts to a spike at a particular phase or time. There's another one, polychrony, which was written about by Izhikevich in 2006. With this phase code, it's a synchronous code: these neurons are synchronous at the point relative to the phase at which they fire. They work together because they're all at the same frequency. These neurons all fire with respect to a background oscillation, and they're encoding something together because they spike at the same time relative to that. It's synchronous only in the phase peak, relative to the base frequency. All those cells in those mini columns will be firing at the same rate, but which ones are maximized depends on the phase.

Polychronous code is where there's some offset in when the neurons are firing. Imagine you have three input neurons, B, C, and D, and output neurons A and E. If they all fire at the same time, time is along the X axis, and these are axonal delays. This is the axonal delay concept, which has issues, but it's interesting to mention while discussing neural codes. These are the different neuron IDs. If these fire at the same time, the arrows indicate the conduction delays associated with their spikes. The spike is an all-or-nothing event, and depending on the axons, there are relatively large conduction delays. Some are very fast. If these neurons all spike at the exact same time, the signals arrive at different times. Some neurons are very sensitive to the exact time at which inputs arrive.

If they fire with a particular pattern where the D neuron fires first, then C, then B, with precise delays, they all arrive at A simultaneously. If they fire with a different pattern, they all arrive at D simultaneously. Is there evidence of this actually happening? I think it's similar in strength to phase code outside of the internal, or the OCaml complex. It's definitely speculative, but in theory, neurons can do it. There's also what's called myelin plasticity, where myelin—the fatty sheath around neuron axons—can change over time, which can change the conduction delay of neurons. That could create some interesting effects. This is a bit like people who argue for spike timing-dependent plasticity (STDP) and the delays. It's very high capacity, and mathematically it's nice, but I don't know if it has anything to do with brains. I think it's potentially not very robust to noise, depending on how you encode. Maybe the myelination would change and you form memories by changing the myelination. It's possible, but that's a pretty slow learning process.

It was something I explored in my thesis, as a way of encoding that my professor was a big fan of. I am equally skeptical. Part of one of my chapters was looking at whether you could realistically encode something, and I was actually trying to avoid myelin plasticity. I wanted to see if STDP, given input synapses, could select the correct conduction delays. We know from certain experimental studies what the distribution of conduction delays looks like. It's a log-normal distribution—most connections are quite short and fast, but some, even in nearby cortex areas, can be up to 20 milliseconds if they're very fine. A neuron sees a bunch of input synapses, and then STDP, which is very time sensitive, might select the right ones. What I found was that there was a narrow space where all these different parameters—like the time constant of STDP, the distribution of axonal conduction delays, and the sparsity of connection—interact. There's a narrow region where it might just work.

What am I gaining from this? If I'm not learning, what am I doing? You're selecting some patterns that you become highly specific to, but the actual delays don't really matter. You learn a different set of patterns, but it matters in the sense that it's learning in the same way as an SDR. The difference is that you can reuse the same neurons. It's primarily a capacity thing: the exact same neurons could be active, but if they're active at different points in time, they encode a different pattern. It's like an SDR where you add an additional dimension of timing. On the receiving end, I don't know any of this; I just see if it matches. In the same way that a neuron sensitive to a particular SDR doesn't care which neurons, it just knows its dendritic segment is active and gets coincident inputs. This is coincident inputs, but if there were delays, those inputs are still coincidence. That's the pattern I'm going to learn. You only respond when they arrive in that narrow window, so you're sensitive to a more specific pattern. The learning rule and the dendrites we know are very sensitive to a small window when things have to arrive.

Imagine going between layer four and layer six A, with millions of fibers going back and forth, trying to learn associations between them. I can't really take advantage of it. The same neuron could represent different things at different times, but I can't take advantage of it. It feels like it is what it is. If a neuron is receiving an SDR, and we're concerned about the capacity of SDRs, we don't have as much of an issue.

This set of neurons encodes this SDR; they've already been used, but with what temporal delay relative to one another? That's a totally different code if they have a different temporal pattern. If SDRs have no capacity problems, then I guess it's not an issue. The second potential benefit is that if I'm the sending neuron and can encode different delays, I can encode different things. But if I'm just relying on the delays that exist in the neuropil, and they're just what they are, I can't control it, and the receiver can't control it. They're invisible; I can't really use them. It feels a little weird unless I have some control over it.

Another potential advantage is if your input signal is temporal, that can influence how you receive that information. I was trying to explore whether you could do something like capsule networks, where as you move a feature in a receptive field, that changes the timing. Whether you get some coincidence—anyway, let's move on. It's interesting. Vivian's back, and she suggested maybe in the future we talk about this, because encoding is really key to everything. By the way, for newcomers, we don't do any of the spiking stuff right now. We're just using real numbers, and there's been a debate about whether we need to.

Maybe it's all useless, but it's really nice to know what's going on in the brain, because I find it's generally not useless.

Even if you don't do it this way, it's helpful to know what is being represented and how different types of information can be encoded. There's a spectrum here: one cell firing rate is very limited. You might have the mini column hypothesis, which is essentially a limited number of things or mini columns, but because it's only a few of them, it's not a rate code as much as a place code. For example, which three of these are active at one point in time. That's more capacity than a single cell, but not a lot. You're not relying on one cell, which doesn't really work. Then we talked about really sparse representations, but maybe there are things in between—between non-sparse and super sparse. Maybe these change over time; that's an open question. The whole phase aspect is another layer on top of that.

Maybe there are intermediate representations. We see a lot of variety in density in the cortex. You can see very sparse representations where a cell fires 200 times in 30 minutes—that's not much, so you could easily miss it. That's super sparse, but there are also less sparse cases. There could be multiple types, less sparse, and a variety of sparsity. The ideas just mentioned could be another example. It's an interesting question to collect data on and develop further thoughts about.

One nice thing about our neuron paper is that we introduced a mechanism to go between two of these modes: the not-so-sparse mini column representation and a super sparse SDR. The SDR is always the same, but the less sparse representation has many specific sparse ones, all equivalent to one of these. In some sense, you can go between these; it's like having two representations simultaneously—the mini column representation and the MDR version. The same cells are involved, so it's like a sparse and less sparse version. When we say "too sparse," is that in the mini columns or the SDR version? That would be the SDR version. This is three out of eight or ten—not sparse at all. If I have a million cells and 10,000 are active, that's sparse. The whole population is sparser; if you count all the active cells, it's sparse. That was the point—the mini column itself is not sparse. In the neuron paper, you might have 20 cells in a mini column in a particular layer. That's not unreasonable—a good number. So you have 20. If I'm right, I might only have 10 different representations here, but for each of those 10, I can pick 20 to the third power. That's 8,000 versions—8,000 ways of representing each of these orientations.

You can get a much higher representation space by sparse volume. In this case, if there are 10 columns, you have 200 cells. These are low numbers; it would be bigger than this—200 neurons, of which only 10 are active. So it's 200 choose 10, but not quite.

That was one of the greatest things we came up with, because it bridges between these two modes of an input: not unique to an input, but the same input, and yet unique. The evidence space is very high; that's almost certainly correct in some form.

Today was a day of banging our heads against the wall, speculating, and confronting problems we don't know.

Did we have any new ideas today?

At least from my perspective, one takeaway was maybe the competition aspect. We definitely had lots of interesting biological ideas. I put these in a mental file to revisit when a problem comes up and you wonder how to solve it—then you remember, "Oh yeah, that could do it." It wasn't that we solved a problem today, and we didn't do that. Some days we do, but not today. It's good to go over all of this again and clarify the problems and questions. It's a settling process: we throw all these ideas around, and eventually it sorts out and makes sense.