I think what I was saying is we still want a unique representation in layer four, whether it's a cap or can, but the location representation in layer six would be very similar. What about region two or region one? In layer four, that would just be a feature. It was just an idea, more motivated by this, but I'm not sure. I haven't thought it through in the context of object similarity in the feature space. When we're thinking about the location in the lower region, maybe the shared cells and the grid cells—the ones that are reliable—are just representing some sort of cylinder, right? That kind of similarity needs to come from higher levels, or maybe not. I was thinking it's basically just encoded in the grid cells, and once turned, it can be learned through the connectivity. What I'm suggesting is maybe it's influenced by merging these two objects or making them overlap more in the higher region, and that affects the six A in the higher region. When it projects back to six A in the lower region, it has that effect because now they're closer in the higher region. We assume that depolarized cells stay depolarized for a while after—300 milliseconds or something like that. I think some can be longer.

When we first came up with the temporal memory algorithm, that was a key item. It has to stay on long enough. I think we convinced ourselves it goes long enough. There's some literature that said 300 milliseconds, and other literature that said longer.

There's enough ambiguity there that we felt it's going to work. It wouldn't be super long, but that's okay. I don't know. What do you think is necessary?

I have this idea, but I don't want to derail the conversation. No, it's all right. We're waiting, so it would probably be better if I—okay, whatever you want. It's up to you, Ron. No, I want to share it, but I don't want to write in the corner up there. You don't want to erase anything here? No.

We just assume that we have multiple columns, and these are all in region one. We have an object, these columns, minicolumns, the columns. We have an object, a feature, and a location. These are all connected together with associative links—something like this.

Although it isn't clear that layer three connects to layer four, so there's minimal evidence. Layer—oh, I see here. It's generally not viewed that way, although there are some connections. People generally say this. But anyway, for the moment, whatever you want. To recognize an object, all of these have to agree or reach some sort of—I'm going to use the term energy here, but I don't like it.

In an attractor sense. I know, but I find people use the energy concept and it just sticks too long. I just call it agreement between all of these layers. In that sense, I think the implication is Rami's doing a completely orthogonal idea while we were waiting for you to come back. Perhaps we could revisit this at the end, but I want to say right up front, as Viviane said a moment ago, it has to work without multiple columns. It's got to work with a single column. You can't rely on multiple columns—they're for speed and other things, but you have to be able to do it with one. That's what I was going forward with: the depolarization that stays on for a while. Remember? A neuron cannot transmit that signal if it doesn't spike, but it could act as a prior later on. If you start asking how long a cell should stay depolarized—you didn't explain why you were asking—he's trying to spend now. I said they're very clear for several hundred milliseconds, like 300 milliseconds, and there's some evidence it could be much longer, maybe a couple of seconds. You were about to explain why it's important. 

We have a union of predictions and features here, and we choose one of them. That union will still stay active, even after we choose; the others that did not get chosen will still stay depolarized. They have a high probability of getting chosen later on if, for example, in a different column or even here, we get some evidence for a feature. What I'm trying to say is, over time, this would be a way of reducing the possibilities, but it would depend on the depolarization of the cells. You don't really want depolarized cells to last for very long, because if they did, as you move around—say, with multiple sensations like saccades with your eyes—if they lasted for tens of saccades, you'd get a mishmash of depolarizations that aren't meaningful. My assumption has always been, and this could be wrong, that you have an anticipation before sensory input, then you get that sensory input. It matches or doesn't match the prediction, you pick a set of cells, and then you have strong inhibition, which shuts down all the other cells. 

But what if they keep getting more evidence? The depolarization evidence wouldn't be useful at that point. It's interesting—they talk about 300 milliseconds as typical, like a saccade duration. It lasts long enough for you to do the next sensation, but not enough for two sensations. I'm thinking of depolarization as the way Viviane is doing it with the evidences. We're basically accumulating evidence as depolarization, and when we try to sample some of those to maybe simulate one of the possibilities and go with it, if we sample based on the depolarization from the ones that are active, the highest ones are going to drive some projection to the object and the location, and then they're going to go across and try to do something here. But then maybe it doesn't match, and then it gets something here. Again, I'm going to force you to think about one column.

I think one thing to keep in mind is that you could depolarize and predict a cell, and maybe have more depolarization to bias it further. To benefit from that, you have to revisit that SDR, those active cells, but if that's unique for a given location and you keep moving along the object, that depolarization won't have any effect unless you revisit the location. You have to keep providing more evidence, or multiple depolarizations might bias a particular SDR so that when you return to it, you recognize the location because you've predicted that feature multiple times. However, this requires looping back, which isn't typical when recognizing objects—you're usually getting that feedback from the object, not the location. You get multiple features at some point, and occasionally you get a feature that corresponds to more, leading to an object prediction that responds to more of those features. You keep thinking, "Maybe it's this," but this is just an ortho thing.

Alright, let's go back to our first conversation and review everything we've discussed. I particularly wanted to recap the joint grid cells and graphs we were talking about before the break. The whole cylinder idea—at a high level, intuitively, we might imagine that when you put your finger on an unknown object and it feels like a cylinder, grid cells have been pooled in a way that a set of grid cells are active for cylinder objects. As you move through that space, assuming this has happened through learning, it predicts features associated with objects that have cylinders, like coffee mugs and cans. The grid cells are predicting that, representing cylinders, but they don't know which kind—they predict both.

This relates to the idea of transition structures we discussed two years ago, where grid cells can learn different transitions, and also in the context of UV maps, mapping different feature maps onto the same morphology or structure. This would help with representational capacity because, for this part of the object, you just need the location for cylinders. Then, if you move to the handle, experimentally that would be the re-anchoring point because you have a unique feature. Now you know you're on the coffee cup, so the grid cells change to the location representation for the coffee cup, but then we go back to the pixel and product. What happens then? I don't know.

Another issue is that we're thinking of it as different 3D object shapes, but all the experiments are with mice in navigation tasks, where it's always a 2D Cartesian plane. If it's different 3D objects, it wouldn't be necessary to have a different space representation for each—just the 2D navigation plane. In this idea, there's no evidence that when the rat stops looking at one signpost and moves to another, it goes back to a generic room. You could argue that, following the earlier description, if you move back to the cylinder or the body of the mug, but now you're in the unique coffee mug space, it doesn't seem efficient because you'd have to relearn the object. That seems to be what's happening, at least from the rat in the room. Once you remap for all points in the room, the evidence is that once the rat's in the room, it knows it's in the room. I don't go back to just representing "cylinder" because I know exactly when I'm supposed to predict it—peeling the handle, with no ambiguity. Back on a cylinder again? No, somehow we can do both. You don't want to have to relearn all cylinders.

Regarding grid cell neurons being dropped, maybe what fits with the literature is that when you're on the cylinder, you're in that generic space, then you move to the coffee cup and a neuron is dropped—the one corresponding to the Coke can. Now it's a single neuron, but the problem is whether this has enough representational capacity to be useful. Assuming dropping a couple of neurons is enough, it drops those associated with the Coke can, so now instead of predicting a union of features in L4, it's predicting just the features associated with the coffee mug. But then there's no re-anchoring, which doesn't fit with the fact that re-anchoring occurs.

Are there any studies of grid cells in relation to objects, like in monkeys? There probably are. I know there have been studies with bats flying in three-dimensional spaces, and grid cells in primates. With objects, not just navigating vision space—actually, yes. There's a study with macaques looking at an image, and they have grid cells associated with that. If you could share that one, I think it was from around 2012.Anyway, it would be a significant problem for us if it didn't work. It was still in the entorhinal cortex. There's also research involving the prefrontal cortex, but those are the two main areas. In the prefrontal cortex, there were findings related to abstract optics, even in humans. The study with birds used MRI. The primary study, I'm pretty sure, was single cell, but I can double check.

The fMRI is a crude measurement. You're not looking at individual cells, just collective cells and how they behave. If you imagine space tiled with grid cells, moving in one direction activates the same grid cells repeatedly. If you move at a slight angle, the change is slower, and this can be detected as less activity in the expected pattern as you move in different directions. It's strong evidence—not direct, but strong.

I wanted to revisit this because the idea of merging the grid cell space addresses concerns about representational capacity when learning many objects. It also relates to having multiple populations of active cells. Additionally, it fits with what we've been trying to do in Monty: start merging 3D models. Currently, we have unique 3D models for different mugs, my cup, and so on, and they're almost identical. We have the hypothesis space to orient them together and say, "This is essentially the same graph." We could do that, even if it's the same graph at a different scale or with different colors. You can have the same graph with a small modifier, like specifying you don't want a coffee cup.

This is the paper by Killian, 2012. The monkeys—Nathaniel Killian. Elizabeth Buffalo? No, Killian worked at RI. You can see a cell that's active at specific locations as the animal moves around. There's another project where we say, "We don't want to relearn all cylinders." That's wasteful and doesn't make sense. We have clear predictions: when I know what object I'm on, I expect something different than a cylinder, like a handle. If I'm only feeling part of the cup but not the handle, I still know it's there. I haven't lost any information.

One way to do this is to have two spaces. We talked about this earlier: there could be a generic space. I think there's evidence for this in rats. If you put a rat in a circular room, you get one map; in a rectangular room, another map; in a square room, another. These aren't unique until you add objects, then the rat can make unique maps. There might be two different representations: a base representation for a generic object, which isn't discarded when you get a specific object. Maybe both continue to exist. One way is to have the same set of cells represent different things in different phases. When rat researchers look at this, maybe they're biased, or maybe the second representation is very sparse compared to the first. The same set of cells could, in different phases or periods, represent both the cylinder and later the specific object. That's one possibility. You could have two sets of objects, two sets of grid cells, both updated independently.

I like how you proposed, as Neil said: there's the base object, the cylinder, and once you get to the handle, you drop off some neurons from the base object, making it specific. That was my hope with the tank paper, but I couldn't get it to work. When I tried to work through the details, there wasn't enough dropping out to make a unique code. That's important evidence: why would a particular grid cell stop activating at some point? It would be nice if it kept the information that it's a cylindrical object, but it doesn't keep enough about the specific version. If there are only so many cylindrical objects, I couldn't encode hundreds of unique ones. Another problem is that if I drop out a few cells, the representation is still mostly cylinder. I have to know that when these few cells are missing, it's no longer a cylinder—now it's a coffee cup.

I did not do that. It just wasn't clear where the information was. You might say, "Oh, I know this cell is missing and that cell is missing." To paraphrase, one approach is to re-anchor, but we also maintain the dropped-out generic one and use the two different phases. We might have the dropout one active in one phase and the unique new re-anchored one in another phase. When we revisit the base, are we relearning? I don't know. It doesn't really stop being a cylinder. The information that it is a cylindrical object is still very useful for making predictions about where the object is at all times. Only very specific things change once we know it is a cup, and maybe only those changes are conveyed by the neurons that are dropped. The neurons that are dropped wouldn't predict the other possible features anymore. 

At a given moment, you might have all six, and the majority of cells are generic. The majority of the grid cells behave as grid cells are supposed to. They are associated with both cylinder objects, and then you have a couple of grid cells that are associated with a coffee mug or a Coke can. Are they dropping out or are they active? At the moment, they're both active because they're regular grid cells. As far as we're concerned, they're regular grid cells because we haven't recognized the object. This kind of sparse representation doesn't fit with how the numbers normally work, but for the sake of argument, let's say this is enough to make predictions. In general, the features we're going to be predicting will also overlap at a lot of locations until we get to specific knowledge.

But then, once we sense the handle, this one drops out, or a different grid cell becomes active. The sparse handful of cells representing the Coke can drops out, but the one for the coffee cup remains active. At this location, it's predicting "handle," whereas the can predicts something else. We always need a distribution to predict anything; one axon doesn't suffice. That's the issue—from a numbers point of view, it doesn't work. But if you assume sufficient capacity, in Monty we could do this. You could, but in practice, you can't. That's the premise behind everything here: in Monty, we may decide to do whatever is efficient for coding, but the way we're doing it now is not very efficient, so we'd have to change things. It's helpful to get clues from neuroscience about how this might work. 

Earlier, I mentioned it's hard to imagine propagating two sets of grid cells simultaneously. That seems difficult. First one set, then another, but not both at the same time unless they're completely different sets. One possibility is that you have a denser base object, like a round or square room, or a cylinder, and then you sparsify it in phase. It goes through phases where many cells drop out quickly, becoming very sparse at one point, and then all become active again. When people look at grid cells, I don't know if they look at the phase, but imagine you have one set of grid cells being propagated, trying to do the dropout thing, but much sparser than what Tank showed. In the off phase, maybe for the sake of language, there's an on phase and an off phase. In the on phase, they're active; in the off phase, very sparse. In those situations, typical research doesn't even look for it, so they don't see it. It's so sparse it looks like a random spike and doesn't fit in their histograms, so it's easy to miss. The Tank example is an oddball—why is this cell never active? Maybe that's an artifact of something else going on. We don't know.

It could be an artifact, but for now, there's an on phase and an off phase. It looks like all these are real grid cells, and then in the off phase, it's really sparse. No one notices because we're talking about exact spike times. They have to look at multiple cells and see whether they're spiking together, which is difficult. It's possible that kind of thing could be missed. That feels like a better solution to me. I know I'm on the cylinder, and I also know I'm on the coffee cup. You just look at the on phase and the off phase, and those two things are there at the same time.

What might happen is that now I'm making a prediction. I have two predictions going up to layer four. One would be: if I look at the on phase, it's a generic prediction—it could be anything on the cylinder. If I look at the off phase, it's very specific, and I would know right away if it's right or wrong. The off phase could be prior to the on phase. We could start with a specific prediction and then move to the generic one; it doesn't have to be the other way around.

If I have a specific prediction, I project it up to layer four. If it matches, I'm done. If it doesn't match, I go to the on phase and have a more generic prediction. That's why, if I'm moving on the Coke can, I may not have a specific prediction at a particular point, whereas on the coffee cup, I would have a specific prediction that was either met or not.

Something like that fits the evidence better for me than anything else. I guess the only thing is, going back to the experimental literature, in that argument we're basically saying re-anchoring happens, but it's very sparse and precise in time, so it's not often observed. But then, re-anchoring is observed. You're right. What is the re-anchoring that is observed? Sorry. I like the idea that this is happening, but it's just hard to record. The whole idea is that re-anchoring is occurring. The evidence says that, but then we don't want to have to learn everything because it re-anchors.

When the rat is in the room and it re-anchors, it's funny because when the rat's in the room, imagine for the moment it's in the dark. You don't really have much information at all. You're not really making any predictions until you get to some point. It's not like moving your finger around the cylinder, where you have this constant prediction of what it's going to feel like. Obviously, that feels correct, but in other situations, if the rat is in the room and just feeling with its whiskers, there's a long period of time where it makes no prediction of anything, and then it says, "I should feel something."

It's not like learning every point. It's not like saying, "I thought in the room, learn what I'm feeling, nothing," so it wouldn't be a lot of learning. But again, we have the problem if we're trying to learn a cylinder or something like that, but then we feel like we have to learn the same thing. We have to learn all this learning about—there's a difference there. A rat around the room doesn't have to learn every point until it senses something, yet we're assuming we have to learn every point because we're sensing continuously.

Why is that? The rat doesn't even have this problem as much, but even though it re-anchors, it's not learning every point. That could be a useful piece to think about: we don't want to have a big union of predictions in the beginning when we are uncertain, because maybe in the beginning we just don't have any predictions. We just wait until we collect some sensory information, until we have enough information to make an educated guess about what shape of object we're on or how big the room is. You just walk in the dark room until you hit the wall, and then you use the distance you walked to make a prediction of what size room you're in. Maybe in the beginning, when we're touching something, it feels like we're getting information right away—oh, it's a curve.

It's not like with touch, on the object, it's hard—it's not like the rat. It seems like I'm getting information right away, immediately. But still, if you put your hand into a dark box and you're supposed to recognize an object, I feel like you don't make a prediction on first contact. You still use your whole hand, try to judge the size of the object. You would still go over it once to get the size of the object at least, or the rough shape, and then you start making more specific predictions. I feel the opposite. I would argue that if I touch an object and don't know what it is, I notice, "Oh, there's a curvy thing here," and then, "Oh, there's a little hole here. I know that hole. That hole's on my coffee cup. It's an oval hole. I know what it looks like." I didn't have to touch the whole thing, but as you touch the cup the first time, or even when you start moving, do you already make predictions like, "Oh, this could be a cup or a can or a spoon"? Somehow I'm able to infer rapidly without touching the whole thing.

Can I make a suggestion? I don't know if this has ever been done, but we get a box, and we each pick an item for someone else, put it in the box, and we're each going to put a hand in. I did that at the Exploratorium last year. It was actually not easy, because we use this example so often. I think you can't do it easily, but especially in the early stage, it's interesting to think about how your hypothesis space is developing. It would be fun to try. We can take an empty box. What did you learn? I think I've got a slightly bigger one. You have to be able to, or you can put a blindfold on. You don't have to use a box. Put a blindfold on.

You have to make sure you just use one finger. That would be fun to try. Should we do it right now, or would it take a long time? Or after lunch, then we can have a bit of time to find an object. Whenever, I would wait. How are you around today? I'm here all day. I know someone wanted to talk to me around 11:30. I have to see him. I don't know how long. What do you guys have planned for the rest of the day? We were scheduled until 11 today. I think Josh might bring by a link around 11. That would be fun, so you can meet her if you like. Cool. I have nothing else planned other than the research meeting right now. If you guys have to do something else, I can spend time thinking about this, see if I come up with any ideas. We still want to work a little bit on the hackathon project as well, but since you're here, it's great. Wonderful. I'll wait till later. Nothing conceptual.

I would prefer to keep going here. Personally, I want to keep thinking about these things right now. I think in the afternoon we can do something fun, but I feel like it would be interesting to roll out the implications a bit more. Let's assume we have the generic space, and then the more generic one for cylinder objects, and the more unique one once we're on the coffee cup. Maybe there are two phases, and they're both active. Maybe they're both very active, and that's what the re-anchoring that people observe is. But in terms of how catastrophic that is for learning efficiency, that's not ideal, because then you're predicting with two different ones. What does that mean for how we learn the object? Do we have to revisit every location twice? 

No, what we do is use the generic object that I learned, like cylinder or something. It has a set of predictions, but if I have a different input that comes in, then I'll pair it with the sparser version of the grid cells. If I have an input that doesn't match this—like I'm on the cylinder, but now I get input that doesn't match—then I wouldn't re-anchor right away. I would learn that new feature with the sparse version. If the sparse version has a prediction, I'll predict it, but if it doesn't, I'll just go with the generic one. The sparse version has priority. Earlier, you were saying once you feel the handle and go back on the mug, you're not like, "Oh, I'm back on the cylinder." At that point, you're like, "I am on a mug." It feels like the internal representation is different, but maybe that's just because that's the L3, that's the question. It could be L3. L3 says, "Oh, I'm this specific thing." Let's say that works, and now layer three is projecting to layer six. If I know the specific thing and there's a unique match for some location, I'll predict that unique match first.

But imagine you have the unique prediction first, and then it doesn't matter. Maybe it could be resolved by layer three.

So you would say that you go back to the cylinder representation once they're both active, but the unique one doesn't get used that much. I feel like it would make sense to stay with the unique one once you have it. Why would you go back once you know you're on a coffee cup? Because you don't want to have to relearn all the things for cylinder. If it's the same SDR, you just drop some of them; you don't have to relearn. But I can't see how to get that to work. Dropping a few doesn't allow me to learn something unique. It's like noise. SDRs are very robust to dropping some things; they still recognize the same pattern. 

I'm thinking there's a potential advantage, other than learning efficiency, to going back to the cylinder. Later, we might use these models for things like action planning. You can generalize much more because anything that applies to the body of the cylinder applies to any cylinder, regardless of which object it is. It's a different face of the same problem. Not only do we infer objects without having to learn the entire object, but we also want to be able to create action plans without having to perform a unique action plan for every object. I get your point that it's difficult to do this with SDRs, but if it's just turning off some of the neurons, then it would make sense not to turn them back on. Once you know you're on a coffee cup, you want to predict all the things you would see on a coffee mug, like the Mento logo or that it could be warm. You don't want to go back to all the predictions for any solution. 

Imagine you have the generic prediction and a specific prediction, and the specific prediction only comes into play if there's a specific prediction.

I'm reacting to something you just said. You don't have to go back and forth. The generic prediction works, but when I get to a location where I have a specific prediction, that one dominates. Layer three knows I'm on a unique object. Layer six is representing generic objects, and to the point where I have learned a specific feature on the generic object, the unique layer six activation becomes dominant. Otherwise, you are in the generic space. 

If you have an SDR and you drop some of the neurons, the analogy is that you could have been on a generic object, but now you know you're on a specific one, so you're just making fewer potential predictions. The ones you're still making from the cylinder could be the curvature you'll sense or where in space the object will be, but then you drop some of the potential predictions, like the Coca-Cola logo is not predicted anymore as a potential feature. The problem is you can't just drop a few; you have to drop almost all of them. You can't represent something unique by taking a representation and dropping a few bits. SDRs are robust to dropping a few bits; they'll still recognize the same thing. You have to go to a very sparse representation that's unique.

I'm considering whether these are more like grandmother cells with some form of lateral inhibition. We still need the SDR, but once you drop one out, it's no longer inhibiting a set of cells. My feeling is that dropping out a few cells is more of an artifact or a clue about what's actually happening, not the main event. If we could explain the dropout with two phases, for example, that would be interesting.

I can't see how you get the representational capacity by dropping out. I understand your point. The general idea is you want to have a whole bunch of grid cells active. Let's look at the Tank paper, which is key here. Go back to the Tank paper.

This figure shows a grid cell module. The point of the paper is to figure out the new morphology.

This is a grid cell module, and these are actually grid cell modules here. There are six of them. These cells are all tying in the space.

The colors represent where these cells become active in space. There are six of these, and these are all the grid cells at some scale. This is another grid cell module, with a set of grid cells at a different scale. Grid cells come in different scales, and these are all the ones at a particular scale. There are six redundant sets of grid cells to represent the tying of the space.

There are multiple cells that represent the same location in space. If I find the other red cells here, these all represent the same location. So, there are multiple grid cells active at that location because there are six of them here—six of these sets. Six is not a lot. Originally, we thought we had many grid cell modules. To be clear, what we normally consider a grid cell module is one of these six things here. In our first paper, we thought we had a lot of grid cell modules, which would allow for a very unique set of locations. But there seem to be only six. 

I asked Tank about the way they do this, possibly with the two-photon microscope. He was looking at a particular plane, and I asked if there could be a lot of cells vertically that aren't shown here. I believe he said yes, but I'm not certain. These only go down to a depth of about 10 or 15 microns, so there could be more cells deeper than that. I'm trying to get to a large number of grid cells, where you could drop a bunch out and still get a unique representation. Six didn't seem like enough, but maybe there are more cells than shown here because of the depth. That's a possibility. 

To form a very unique representation, you'd have to drop out a lot of these cells. You can't just drop out a small percentage, because if you do, the SDRs are not very sparse. This representation is not very sparse—grid cells come on every certain distance, so it's not a sparse representation in any situation here. It only becomes sparse if you could drop out a bunch. I even considered the idea that if there's a vertical stack of these, and he's not showing the depth, there might be many cells at that location in this module. In that depth, they might become more sparse. I'm trying to get to the sparsity aspect—somehow, we need a sparse representation of space.

Maybe some of these cells at one layer are the ones measured most often because they can be reliably identified as grid cells, but there might be many other cells further down that are very sparse and have grid-like properties, but they don't get captured because they're sparser and don't fire most of the time. Maybe the dropout they observed was right at that transition, where the cell just below starts to be sparse and is out of the border, so they found one cell dropping out. If you go deeper, maybe they all drop out, or there are many dropouts. I was trying to find ways to get sparsity out of this while preserving path integration. You want path integration to work for everything, but I was trying to get to a denser representation and then a sparse version. This should be consistent with anchoring on an environment and then sparsification for that environment or specific features. That would work.

I like the idea that the 3D space gives rise to the sparsity, but if this is the level at which experimentalists see a lot of activity, maybe it's because it's the generic or representative model. You would still see re-anchoring at this level. Do they talk about re-anchoring in this paper? I don't remember. It would be interesting if, under these experimental conditions, re-anchoring never happened or something like that. It's a puzzle—there are conflicting pieces, which is why I can never find the Tank paper. Tank is the last author, and I save it under "goo." He is the lead author; it's his lab.

This is a classic problem we deal with. There are pieces of data that seem somewhat conflicting, but not completely. We have theoretical constraints we're trying to achieve, and if you spend enough time thinking about it, you can come up with a solution. But that solution has to be correct and fit all the constraints. Then you look for evidence, and you find it. That's the challenge.

In these situations, they only report the cells that are clearly visible. There are many other cells present that aren't reported because they don't seem to have the right properties. These are just some of the cells they see and identify as grid cells. We know there are many conjunctive cells, which are combinations of different things, and cells that are very sparse with grid-like properties. We're just looking at a selected set of cells that work like generic grid cells; anything else, they don't report because they don't know what it means.

Someone shared this paper in a group. How do you drop it into Slack? Drag the file from my desktop—where do I drag it? Into the message box, where you type? You can also use the plus icon and pick it as an attachment, or just share the link if you have one. I don't have a link to the real paper; I just downloaded it. I remember seeing a presentation from Tank where he gave a talk, and this figure was in that talk. I immediately wrote to him with questions, but he didn't respond for months until the paper was published. Then he was willing to talk about it. It was frustrating because I wanted to ask about it, but it was only briefly mentioned at the end of his talk. I thought it was a great figure.

Something similar happened with Edvard Moser's group. They had a poster from around 2014 about rats in 3D cages, and I was curious what the grid cells were like for that. The lead author published it recently, but she was unwilling to talk about it until it was published. By the time it was published, it became apparent it wasn't relevant to us. I got really interested, though. If they already have a poster, it feels like you could go up at the conference and ask, but I think it was new results, and experiments are a lot more nervous about being scooped. I'm not blaming them; it was just frustrating for me.

Another interesting thing is the dimensionality and size of these things. I was thinking, is this equivalent to a column in the cortex? Would we see something like this in a cortical column, with six or ten of these grid cell modules lined up? This is much smaller than column one, so I'm taking this as a prototype of what a column would look like in the cortex—six of these modules, but maybe there's a depth to them.

The beauty of that idea is, imagine each of these colored cells is like a minicolumn. The minicolumns might be dense or sparse, but you want to path integrate the whole minicolumn, not individual cells. Even the sparse ones get path integrated, or something like that.

One of our problems is we want to be able to do path integration on sparse patterns or locations, and it's hard to manage how to do that.

We did something different, but the idea is that you're doing path integration on a set of cells that might be vertically aligned.