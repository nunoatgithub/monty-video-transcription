This is part two of the discussion we had a few weeks ago about how we might speed up some of the slower computations in Monty.

As a short reminder, last time I showed an analysis of which functions currently take the most time to run. We talked a lot about KDTreeSearch and possible alternatives for that. I also went over some of the matrix multiplications and other matrix operations we are using, but we didn't have much time to discuss them, so I'll start with those. It looks like the KD tree search is the biggest thing to look at, but all these matrix operations add up to quite a bit as well. If there is a way to speed them up, we could probably apply it to all of them.

I thought it could be worth looking at that a bit more.

How did we end up on the tree search? We had the suggestion to do a table lookup, but it didn't sound like it was going to work. Where did we end up on it? The table lookup does speed up the search during inference, but building the table takes a long time. If we are just focusing on speeding up inference, it's a viable option, but if we want to keep the option of learning continuously and updating our models often, it's probably not the best solution. For updating, it's actually worse, so overall it's not a good idea.

That's unfortunate. All right, next, we're looking at the next best option. At some point, it might be good to go back to the KD tree and dive into it again, but the table lookup is really just for inference. I found another paper that looked promising. They use a special way to search architraves, optimized for searching 3D point clouds, which is exactly what we're doing. I might take a closer look at that one, but unfortunately, they don't have any code online, so it would be a longer time commitment to try it out.

There might be some other alternatives to the KD tree search.

For spatial queries, I think it's like R trees or something?

Their implementation, you mean? I think that's something used for a spatial database, in R3. I've seen this R3 stuff.

I'm not a hundred percent sure what they do. They sort it in a specific way so it's quick to search through, which is also done in KDTree, but apparently it's optimized for 3D point clouds.

There are some bounding value hierarchies, where they chunk them in terms of bounding boxes and then divide among those. That's what I remember as well, but that was used for databases. I don't know if that's also used for this.

I've seen that used in GIS systems, like in spatial databases.

There must be some open source. Postgres has it. It would be good if there were Python bindings. There is a Python version of that. Can you share the link? I'll see if I can find it. Maybe you know the keywords to search for. R three A trees, that was a keyword that brought it up. R three for the GIS systems, A-R-C-R-R dash three oh r, I think that's the name, and this is related to the GIS system, right? R 3G IS. But that's a company, I think. There are a few. Is this related to the matrix operation? That's the KD tree. That's for searching. It's exactly as Eric described: a tree of spaces that you can find within the space.

I'll have a look at that. For now, the conclusion is that we'll wait before implementing something, because we don't want to over-optimize for inference right now and want to keep it flexible to be fast for model updating as well. The matrix operations might still be a viable thing to speed up. It would not be the biggest computation to remove, but still a significant chunk that could be sped up.

I'm not going to go into the exact applications of these operations again, but generally, here's the collection of operations we apply. The main point is that right now, we always do the full matrix operation on all of the hypotheses. We update every hypothesis at every step, but in practice, we don't need to test all hypotheses at every time step. We could just test the most likely ones, masking a lot of the rows in the first axis—where H is the shape, which can be between 1,000 and 40,000 rows. Some entries in K are also already masked, since some locations in space have fewer than K neighbors in the allowed radius. Those could also be masked. 

For example, when we do the dot product, we have a big matrix of size H × K × 3, but almost all of the rows could be ignored, and some columns could be ignored in those rows. The third dimension is also ignored if a row is ignored. The question is whether there is an efficient way to use this fact and ignore a lot of the rows at every step. One idea is to sort these hypotheses so the relevant ones are together, then do brute force on that subset. Of course, you have the cost of the sorting operation, but stacking the most likely ones at the top and just doing those is one possibility. 

There is definitely change in the top K hypotheses, but after a few steps, it gets stable, depending on how many hypotheses you look at. If it's just the top 10, it gets pretty stable after several steps, but it can still change depending on how you move over the object and when you see significant features. If it changes and you didn't have that in your top ten, it might just mean a different row is now grayed out and another needs to be updated. If it's stable, you don't have to worry about sorting and unsorting again, and it can be much faster to update or re-sort the whole thing. You definitely want to do all of the relevant ones. If the set of winners is changing over time and you don't update it, you'll miss that, so you want a fast update. It's faster to sort into a re-sorting list than to sort from scratch. If we sort it, we still want to keep a reference index between which row used to correspond to which hypothesis. In our old sparse matrix implementation, every row was like a pointer, so you could reorder them easily.

Mo, I think in the documentation, if I remember correctly, Marcus did something by Torch.

That was Abby. I was able to do something similar with PyTorch, so maybe we can just move PyTorch from the brothers up. We just moved PyTorch from Monty, so I don't know if we can come back to that. I think Marcus's version probably used pointers for each one. It used pointers, so it's very easy to re-sort. I would roll as a pointer.

Is it faster to sort this and then take only the top ones to multiply, or would it be better to move the implementation onto GPUs and do the large matrix multiplications? Sorting 10,000 should be super fast. Once you sort it, you can use all of these operations on that structure, and you only need to re-sort when your hypotheses change, which is the bigger loop. Internally, once you sort it, you can do everything with that structure. GPUs will be very fast, but you have to worry about data transfer between GPU and CPU, which is usually a bad idea. Once you get on the GPU, you want to do a lot there to amortize the cost of data movement.

Just so I understand, H is 10,000—that's the length? It can be between 1,000 and 40,000 usually. What's the ranking of these things? What's the figure of merit? What's the range?

Do you mean what we would sort it on? Yes.

That would be the amount of evidence for each row. The evidence can be bound, for example, between minus one and one, or it can be arbitrary and grow infinitely. If it was quantized—say, you only have ten values that the hypothesis validity can fall into—would that severely compromise what you're doing, or do you need fine-level precision so these things filter out to a very fine cream on the top?

Ten might be pushing it, but it should still be okay. The bigger issue is if we bound it to a range, like minus 101 or 10 bins, it means we have a time horizon of what we can remember. We need a very efficient policy to see all the significant features in that time horizon to recognize the object. 

When you talk about sorting, are you sorting on a scalar, or on each of the dot products individually?

Right now, we don't do any sorting. If we did, the dot product is used to calculate the angular difference between hypothesis and observation. From that difference, we get an error, which is added to the evidence. We have a separate array for the evidence for each row, and we just take the maximum of that.

Normally, it would be a scalar—the evidence value for each hypothesis.

Is the bulk of the time spent doing the operation afterwards, or computing the evidence?

All of it together—so Einsam, this is the top one, this is computing the dot product between the angle difference. There are also other operations, like computing the difference between observed features, getting distances, and weighing the evidence by however much we want to weigh each feature. To be more specific, you have some amount of calculation to come up with that scalar, which, hypothetically, if you were to sort on and then only operate on those rows with high enough evidence, or all of them that had greater than threshold evidence. I'm trying to figure out how much of the processing is split between arriving at that estimate of the evidence and actually processing the rows once you have the evidence.

I would say basically all of these are used for arriving at the evidence values, except for this one, which corresponds to the maximum—getting the most likely hypothesis out of there. So then it wouldn't help too much. Exactly. If you get down to an evidence number, say 256 evidence levels, and you mapped into that, then you can use a bin sorter. Those are really fast because you index on that evidence and just put it into a list. But if you're spending the bulk of your time coming to that point, then it's moot.

No, that would be done, but it would be a lot quicker. We would have the evidence from the previous time step, and then we only want to do all these operations on the top k ones.

At the very first time step, we would have to do all of these, but after that, we can just take the top k evidence values from the previous time step and update those with the new observation. If that were the case, you can do bin sort in linear time, which would allow you to at least bucket them. Will we actually need to sort the entire array? If we just need the top k, could we just stop? That's why I was looking at what makes it white, what makes it gray.

If you know ahead of time where the white ones are, then it's moot. If those gray ones are potentials that just didn't fall into a large enough quartile or quintile of the things, then that's a different question.

We might have a threshold, like a dividing line between what is good evidence and what is bad evidence, and just sort them into two bins. After the first iteration, you're maybe at 50, after the second iteration at 75, then 80, and you can continue to close that gap. You should see fewer of them showing up with high enough evidence so you can concentrate on those exceeding the threshold. The problem is that the threshold is not going to be constant. In the K Winners of the FPGA, we did one pass through to form a histogram, processed the histogram to find where the threshold was, and then did a second pass through picking off the ones greater than threshold.

It's similar to what we do with the voting at the moment. We scale the votes to be in a range of minus one to one by using the maximum and minimum value, and then set a fixed threshold, like above 0.8, so in the top 10 percentile. We could do something similar here. It doesn't have to be a fixed number of n rows that we look at. There are advantages in getting away from a bounded floating point range to a bounded integer range, because then you can do addressing operations based on the integer values, or lookups, or a variety of other things. With floating point values, you're stuck with doing comparisons all the way through.

If you're just doing binning—storing the index of the hypothesis in one bin being greater than threshold and one bin being less than threshold—then you don't have to do any sorting. You're just binning them into one or the other.

How do you create this big matrix? Do you create it once, and how does this matrix evolve? Does it always get smaller? No. You recreate it every time. It's always the same; this is constant. This is all the hypotheses that we have. This is using the observation and the k-nearest neighbors of this location, comparing that to the stored features in the graph. With every new sample, with every new sensation, you have to recreate that hk3 matrix.

But h is at least fixed?

Yeah, h is a fixed size. It's always the same from the start of an episode. We have to find the k-nearest neighbors of the location in the graph and take the features stored there, which are unit vectors in 3D space. Then we take the observed vector and rotate it by the hypothesized poses, which might be different for each of the rows.

Is there any way you could do some algorithm, either sort or what Kevin was saying, just to pick the top, the white hypotheses, and then only create the matrices for those white rows? Then you don't have any gray elements. We could. That's true. Only h would be variable here, and it would be much smaller.

That would actually be the easiest. I don't think I can help, so I'm going to go so you can optimize your time.

If that works, then you could just keep everything else the same. You have an H prime, which is the top set right now, and that works for the second and third steps, but not for the first step; we still have the tail column.

No, everything would be with H prime once you sort it.

No, but the first step, you don't know what to white sort. The first step, yeah.

and then we would just have to keep some kind of reference to which hypotheses the white rows refer to. The numbers should go down with each iteration. You should have fewer, and it should go super fast. The only time this will cause problems is when there's noise in the data and you might be discounting hypotheses prematurely because the data was noisy, but that could be an optimization on a later pass. We can always come back to any of the gray rows if the white ones become less likely because the evidence is inconsistent with them. We'll come back to older hypotheses; it would just take longer to get there than if we updated the whole matrix. We've also discussed adding some sort of resampling process to reconsider less likely ones. If we have this kind of sparse subset of the most likely ones, we could use that extra budget to add in low probability ones occasionally and resample them. Boosting.

I'll have to think and go through the code to see if this will be an easy change. Right now, it sounds like it should be possible to, before even creating these matrices, select by the highest evidence values and then just keep a reference to which they correspond. In terms of what you were saying, Kevin, would the fastest be a hard threshold? We could have a variable threshold based on how much evidence we've accumulated or whatever.

But then, for an arbitrary number K, specifically the sorting bit—would that be faster, or would sorting the top K be slower? Specifically, looking for the top hundred, or any that are above a fixed threshold. Because you don't know ahead of time what your confidence in all this is going to be, what the range is, you might want to pick your strongest ones. You could use heuristics. If you're getting a bunch of hypotheses and then there's a sudden drop-off in confidence, you might threshold at that point. The point is to bring all these things into a metric that allows you to make that choice. Typically, if anything becomes a fixed K, it makes it easier because all your subsequent array allocations and everything else become fixed. But it really depends on how your data works, how it distributes, how it evolves as you get each additional sensation. I don't know ahead of time what the dynamics would be, but what you were talking about earlier, which I was actually thinking of recommending, is that if you come up with a threshold derived by some heuristic, the ones that fail by a small amount you might want to reserve off to the side, filtering them in, just like you were talking about. That's essentially how we work with K winners: during training, we don't want things to fall all the way off if they happen to just once fall below threshold, because then they're forever lost. Part of the boosting algorithm is to resurface those, give them an extra 10, and see if that brings them above threshold. We'll try them again, randomly sort through there. It gives you the ability to have a memory and re-examine these things. The context changes. There could be something that's way out of band that was just deselected in the beginning. If this thing is highly non-Lipschitz, something wild could come out, and nothing's going to work on that. But the hope is that you're in a space that's relatively smooth and progressive toward converging on a solution. If you can establish some criteria or heuristic to say, "If we work up to a certain threshold based on some heuristic, then we reserve 10 percent more off on the side to filter in if things are getting worse," then you have K plus some delta of that reserved pool to play with. If you have an assorted list, you can play those games and assume the degree of confidence you have is also part of the likelihood that they could be viable candidates.

Thanks. About the integer values rather than floating points—would that still apply if we were doing the threshold? Would Python—I'm not sure the reason you gave—it depends on what you're doing. If you're not actually sorting the list or specifically calling...

If you're not specifically sorting it, then it might not be as interesting. However, if you have a threshold and you want to decide where to put these rows, what's the data structure to put them into? I would still want them to be in priority order. You'd want to try the most likely ones first. Am I incorrect, or are you going to try them all equally?

At the moment, we try them all equally. Even if we threshold, we want to update all of the ones that are above the threshold or in the top K. The advantage of quantizing that value is that it doesn't have to be as fine as I mentioned; I was just using that as a straw man. If you want to deal with these things as groups, you can store them as clusters. Physically, they can then reside at near values in memory so that if you're on a CPU, you're not randomly accessing memory all over the place. You might want to migrate those rows into physically adjacent storage so that when you bring in one of these, you could also bring in the memory for the next couple, and your cache efficiency goes up. That's a second-order effect, but having a quantized address or tag for where to look for these things makes it easier to use a lookup table. For example, if you're looking for cluster five, the lookup table gives you the memory location, and you can process those. It depends on whether you're going to look at all of them equally and do the same thing. If they're scattered across memory, you'll pay costs for that. There are advantages to bringing things close together in memory.

Would we have that flexibility within Python, or is there a way to do that? You basically have a maximum size for what the row could be—your k is equal to 10, so you allocate an array of n cells. Once you process it the first time, you might allocate some subsidiary arrays, k equals 10, h equals 20, something like that. I could do the computations to figure out what would fit into L1 or L2 cache. If you know you're going to bring these in and reaccess them multiple times in quick succession, there's an advantage in keeping them in close memory. This assumes a C implementation, not Python. You can still do it that way—it's just an array. Python will blow any cache coherence you have because the whole interpreter is running for every line of code.

In the memory access pattern, it's still going to be coherent. The instruction cache, yes, but the data cache—if you're accessing coherent rows, that will be benefited. That's the underlying machine architecture. I understand, but I'm skeptical of Python.

How many clock cycles does it take to add two variables together, A plus B? If they're not using SIMD, I understand what you're saying—over a hundred. The plus operator might be overwritten; A could be a class, and everything is interpreted, so dynamic and flexible that it's doing a lot. It's not like C. Even if the underlying arrays were NumPy, they still have these characteristics. NumPy operations are optimized, but in between them, in between lines of Python, it could blow the cache. That's my suspicion. Subutai is right—I'm mapping this onto a C model where you want to pare operations down to the most efficient things possible.

You might be right too. They do try to optimize that. We would have to profile to see whether you're compute-intensive or memory-intensive. If Subutai's hypothesis is correct and you're compute-intensive, what I'm talking about would be a second-order effect. That sounds interesting. Thanks. I hope one day we'll have a lot of this reimplemented. I didn't know if you were there yet with this meeting.

Not yet. I have a quick question to refresh my memory: the things you're using to judge whether an observation matches with a hypothesis—what does that feature vector look like? Is it just normals and tangents, or what features are you comparing observations against hypotheses? You have two different types: features like color, which don't change when the object rotates, and that's just an array of numbers. Then you have curvature directions, which are matrices with threes, basically unit vectors. Which ones are you using to compare in this dot product Einstein matrix? What are the values, the dimension you're collapsing in these products? What are the values being multiplied there? Here, we take the dot product between the observed, rotated—are they X, Y, Z? Are they rotations, positions, what are they? Oh, it's X, Y, Z coordinates of a unit vector. So it's a normal.

There's a very easy way to speed this up: sort your entries in the H matrix along the lines of their orientation. When you bring in an observation that's pointing in a direction, the list is already sorted on those orientations, so you can go directly to the part of the list with everything oriented in that direction. Or do you have to do another transformation? 

They can't sort because they have multiple orientations in the matrix—a three by K matrix—so you can't, because you're going against multiple orientations. They have to reform the entire dot product and then process to see if it's a good fit. For each hypothesis, you've got ten possible neighbors—points you could move to nearby—and their orientations, right? Is there any way you can sort those based on the observation?

I guess we could, but we still have to calculate them all because we also want to get the negative evidence if it doesn't match.

I was just thinking there might be a way to organize the hypotheses and those k nearest neighbors so you could quickly go to the ones most closely aligned with the observation, given they're both in three-dimensional spaces. All right, never mind, it was just a thought.

I think I'll try what Subutai suggested: thresholding before building these matrices to see if that helps. There might be something I'm not considering about why it wouldn't work, but if it does, it could save a lot of time with the KD tree search since we wouldn't need to search for many of the hypotheses' nearest neighbors. That would be a useful side effect. The main question is how to determine the threshold—whether it should be fixed or dynamic. If it's dynamic, is it based on falling below a certain level of evidence or something else? The real challenge is finding the right heuristic for that.

We could save a lot of computation by ignoring objects with extremely low evidence, those that have almost no point updates. That would be a second threshold, where we don't update an object at all if it doesn't meet the criteria. These are all "lazy" optimizations—avoiding unnecessary computation rather than making it faster. The fastest computation is the one you don't have to do, so prioritization is key.

The second topic is whether there are better alternatives to multithreading in Python. We use multiprocessing across episodes, and within those processes, we use multithreading over objects since each object's evidence can be updated independently. For that, we use Python's threading, but it only gives about a two to three times speedup. Ideally, the speedup would be proportional to the number of objects in memory—so, for the YCB dataset with 77 objects, a 77x speedup—but that's not happening. For example, during the demo, it seemed fast because there were only nine objects in memory, but with 77 objects, the speedup is just a linear multiplier based on the number of objects.

If your processes are already busy and using all the CPU, multithreading only helps if there are spare CPUs. You're also thrashing the cache with all those objects in memory, frequently ejecting data from the cache. I'm not sure how Python implements threading; I was always told it was single-threaded unless you do something special. In other languages, there's the concept of affinity, where you lock a process to a particular CPU, and it doesn't help to have more than two threads per CPU because of hardware limits. The only time it switches is if you yield or an I/O operation occurs. Segmenting like that is useful because each CPU has its own L1 and L2 cache, but they all share an L3 cache.

If there are a thousand threads waiting to run, there's no coherency. You might bring in an object, process it briefly, then switch to another thread, which requires loading new data and writing out the old data. This can easily thrash the cache if you don't control the workload. For every unit of memory you read, you need to ensure you're getting enough benefit from it. There's a double cost: bringing in new memory and writing out the old memory immediately. Some of this could be parallelized, but the optimal way is usually not to have more than two threads per processor if they're doing the same thing. I know how to do this under MPI, but not with Python threads, and it's more complicated in other parallelization modes, especially in C.

The issue might be with when the thread objects are created—within the matching step calls. The unit of work we're threading is very small, less than a second. If you're repeatedly creating and destroying threads, that's a heavyweight operation, which is why thread pools are used. If threads run for 20 seconds, it's not as bad, but with small units of work, the overhead of thread creation is too high. That might explain why we're not seeing a 77x speedup. It could help to hoist the threads out to the outer loop to establish the random work.

This is only within the episode, right? You are still seeing a 77x improvement because the episodes are parallelized. It's just within that loop you're not seeing the speedup. I joined late and apologize for that. I want to understand: if you're parallelizing the episodes across processes and have 77 objects, does it actually run roughly 77 times faster than if you ran it all on one process?

Not, unfortunately not. One reason is that episodes can have very different lengths, so the total time is determined by the longest episode. It is definitely faster than just a two to three times speedup. When you spawn these processes, are you creating and destroying them every time this matching step is called, or are you holding on to them?

These are being created and destroyed every time the matching step is called. If I understand you right, Lawrence, it would be better to create them once at the beginning of an episode, have one thread for each object, reuse them in every step, and only destroy them at the end. Both processes and threads: forking or creating a process is extremely heavyweight. Typically, you want to create a pool of these resources, and if you know how many processors you're going to use, you can set the affinity so they run only on specific processors. Otherwise, the scheduler will try to do round robin, even when you're not asking it to, and will swap between processors, causing processes to jump around. You can see this in top K, where threads and processors are jumping up and down because the scheduler tries to be fair and let others run unless you explicitly control it. At the Python level, I'm not sure where that is. It's easier to do under Linux than under macOS. The concept is to allocate resources to particular CPUs and then subdivide that among some subset of threads. It takes more work and thought, but you avoid sabotaging yourself by thrashing.

That definitely sounds like something to look into for us. You have to manage your own work queues: allocate resources and dole them out under your control, rather than letting the scheduler decide.