All right, welcome to the third video in the core video series. Now we're going to dive into more depth on the actual implementation and how we want to implement these modules and the communication protocol. Here I have a couple more notes. The idea is that in this core video series, I'm doing intros for all the older videos because it's been a while since we recorded them. Since then, we've tried a lot of things, figured out some issues, and what we talk about in these videos might not be exactly the same anymore if you look at the codebase now. We might have figured out some things that were open questions back then. I'm just trying to give you a quick intro for context, not going into too much detail. Hopefully, this helps you follow along and understand what we're talking about.

One big thing in this video is we talk a lot about features, pose of features, and communicating poses and features. When we talk about features, you might have a different idea of what we mean than we actually do. In computer vision, features are often 2D edge detectors or patterns recognized with a kernel function. Here, features have three-dimensional poses associated with them. At higher levels, features are entire objects. One of the core principles of the Thousand Brains Theory is that each cortical column can model entire objects. If a cortical column outputs something adhering to the cortical messaging protocol, it outputs a feature that represents an entire object and its pose.

For example, if you're trying to recognize a Numenta coffee mug and have two cortical columns stacked, the lower column models the Numenta logo and the higher column models the cup with the logo. The lower level column gets input over time; it doesn't see the whole logo at once. A sensor moves over the logo, recognizing it through movement over time. It recognizes features in the logo relative to each other. Once it recognizes the Numenta logo, it outputs the ID of this model as a feature, plus its orientation and location. That becomes the input to the higher level model, which models the coffee mug. In that model, the logo is a feature at a location within the model.

The main point is that a feature can be an entire three-dimensional object, and it has a three-dimensional pose. At the lowest level, the features come from the sensor module. To get a three-dimensional pose there, we use the point normal and curvature direction sensed to define the 3D pose at that location. The point normal is essentially a line orthogonal to the surface of the object. On the coffee mug, the point normal would be like this. At the rim of the cup, it turns, and on the inside, it's the opposite. The point normal is always orthogonal to the surface, and the curvature directions are two more orthogonal unit vectors to that point normal. They point in the directions of highest and lowest curvature. On the cup, one would go up, where there's no curvature, and the other would go around the mug, where there's high curvature. Sensing the mug, the pose is defined by the point normal and the two curvature directions. These three unit vectors tell us the pose of the object at that location. That's the input to the lowest level learning module, associated with features from the sensor module. For instance, we might get color or the amount of curvature as features, and the pose is these three vectors plus the location.

In this video, we also talk about how in the brain there is no origin; grid cells don't represent an origin. In our implementation, we do have an origin because we're using Euclidean reference frames, but the origin is arbitrary. Object modeling and recognition work no matter where we set the origin. The only thing that matters is how features are located relative to each other within that reference frame.

Another topic discussed is matching objects using displacements. This was our first implementation, and it had nice properties, like recognizing objects regardless of location, orientation, and scale. However, it had major drawbacks, the biggest being you couldn't sample new displacements not stored in the model. Storing many possible displacements to generalize to new paths on the object leads to a combinatorial explosion.

We implemented a different approach, which will be presented in later videos.

At some point, I mention implementing a policy that uses internal models to move to locations that resolve the most ambiguity. This is implemented in our codebase as hypothesis-driven policies, or in reinforcement learning terminology, model-based policies. We're using internal models of an object plus our hypotheses to inform where to move next. For example, if I'm on this mug and not sure if it's a coffee mug, cylinder, or cup, I would move to the handle to resolve the ambiguity. That's implemented in our codebase now.

We don't implement separate "where" and "what" columns in the current codebase. A learning module can do both; it can do what a "where" column does and what a "what" pathway does. This is combined in a learning module.

That should be all for now. If you have any questions or things are unclear, please leave a comment and I'll try to answer. For now, just focus your 150,000 cortical columns on this video. Cheers.

I was asked last week for more definition on this idea. I don't want to call it the AI bus or the CCP anymore, so we need an answer for that. For now, I'm not going to call it anything.

One of the questions was whether we can be more precise about the function of a module that doesn't get input from the sensor. I tried to address this and went through a series of thought experiments, some of which I'll share today. This led to a set of definitions I'd like to propose. The biggest outcome for me was clarity on a collection of old ideas we've considered over the years. These ideas have appeared in various forms, and we never really reconciled them. We've had a grab bag of concepts and different approaches, but now I have much more clarity on how they come together. There are a few new ideas, but primarily it's about clarity and definitions, so we have a common language and understanding of what's going on.

I'll start with some definitions. The first is "pose." We've been using this term—Ben uses it in his document. I want to make sure we're all using it the same way. Marcus and I prefer this usage, and Marcus says it's common. In this context, a pose is the relative location and orientation between two things. It's not just location or orientation; it's both. In a three-dimensional world, that's six degrees of freedom: three for location, three for orientation. The brain calculates pose frequently between objects, constantly determining the pose of one thing relative to another. We've discussed how to represent this information, such as Cartesian or polar coordinates. The brain uses cellular codes, which differ from integers, distances, and angles, but may be closer to polar coordinates using a cellular code. For now, we don't need to worry about that. We'll specify the pose between any two things and assume the world is three-dimensional. I don't know if the brain makes that assumption, but we can for now. So, in a three-dimensional world, pose is the relative orientation between any two things. For example, I can ask for the pose of this pen to this coffee cup, to this mouse, or to this room. To do that, you need a shared reference point, or rather, each object needs its own reference point.

That's pose. Any questions about pose? It's pretty straightforward. Next, we'll use the word "body." It's a little odd, but we've been using it already. It's a shared reference point related to your physical presence. Imagine a point in your body that's a reference point. Then you can ask for the pose of other things relative to that point, such as your finger or toe. It's a reference point shared among many things, serving as the reference for your body. There will be other shared reference points as well.

Now, let's think about a sensor. We'll define a sensor as a patch of your skin or retina—not the entire retina or all your skin, just a small patch.

Sensors have a location and orientation. You can ask for the pose of a sensor relative to something, such as the body or another object. Think of it as a point with a location and orientation, typically a small patch of skin or retina. We'll discuss more details when we talk about processing. Your body consists of many sensors moving around, like mine are now. My eyes are moving, different parts of my eyes are looking in different directions, and I have tens of thousands of sensors, all moving around. Each has a location and orientation, and I can tell you their pose relative to the body.

Let's start drawing that. For lack of a better word, I could draw a 3D Cartesian coordinate, but I won't. I'll represent my reference point here—a location and orientation we can call the body.

Then I can have a sensor, which is something out here. It doesn't matter what kind of sensor it is; it's something that moves around. This is the sensor. I have many of them, in different modalities—skin, touch, hearing, etc. They're floating around in space, moving. The first thing we'll say is—I'll jump around a bit here—one of our assumptions is that the brain always knows the pose of the sensor relative to the body, always. I might have an object out here, drawn in green, something I'm sensing, independent of the body—a pen, paper cup, whatever. These sensors are moving around, but the important thing is that at any moment, the pose of the sensors to the body is known for every sensor.

and I can explain that in a moment, but doing this in biological terms is quite difficult. If you think about how your brain knows where your finger is relative to your body, you might have to know where the finger is relative to your joints and your arm, then calculate all these things, and there is some error involved. You might use your proprioceptive system. There are various mechanisms the brain could use to figure this out, but we are just going to assume that we know it. In an engineered system, we can say we know what it is—it's just here—so we don't have to worry about that calculation, which is a biological calculation. We are going to assume that every sensor's location and pose to the body is known.

Now, what does the sensor do? I said it's a patch of skin or retina; they have location and orientation. We're going to define another thing here called a feature. A feature is something that a sensor can detect.

The general property of a feature is that it has some measurable property about the world, and it also has a location and orientation. Just think of it as a thing out there in the world. Whatever it is, we're going to assume it has a full location and orientation. So I've drawn it in the same category here. I might be seeing something and say, at that point out here, there is some feature at some orientation.

When you say orientation and location, is the reference point the body? No, it could be any. At this point, I can say the pose between the sensor and the feature is a legitimate pose. In that view, the feature is on top of the sensor, but the direction is up.

At this point, all we can say is the sensor has some location and orientation, and the feature has a relative location and orientation to it. That's the definition of a pose. This is fundamental to the whole orientation. It's relative to the sensor at this point. You could think of the sensor as having a tip of my finger as some location in space and some orientation at that location. The orientation is part of the transcript; it's all relative. As far as a point in the location, think of it just as a reference frame. It's like saying, I'm going to arbitrarily decide this is north, then I can say it's east, but it's arbitrary. Is it arbitrary for every sensor or for the body?

That's my question. Everything has this reference frame. The body is a reference frame, if you will. A sensor has its own reference frame. The feature has its own reference frame. Everything has this property. You could say everything has its own defined north and its own origin.

This is a pretty fundamental, important concept. If anyone can help me explain this better or a different way, please do. I think what he's asking is whether the feature has a local reference frame. I think you just answered that—it does. I'm not using the word reference frame here, but reference frame might imply Cartesian coordinates. When you said they had a pose, there's ambiguity because pose is always relative to something. You could say there's a feature, and the only thing about the feature is that I detected it, I know the location, but I don't have the local coordinate system for the thing. Everything has a reference frame, and you can think of those three axes moving around. If I say, here's my reference frame, then anybody else has a reference frame, and I can calculate the pose between the two. But it's an internal thing. If this feature were to move, its reference frame would move, and then the pose to the sensor would change. If I just rotated it in this spot, in any of the three dimensions of rotation, it would have the same location to the sensor, but the orientation would change. If the sensor moved, it might have a different location to the sensor. If you think of that as a reference frame, everything has a reference frame, and it's arbitrarily chosen initially, like anchoring your grid cells. It's arbitrarily chosen what's north and what's zero, if you want to call it that. The north of the sensor is different than the north of the feature. Everybody has its own local reference frame, and these reference frames are moving around in space. Is anyone else confused by that?

Speak up. I want to clarify something. It doesn't matter if we have a universal coordinate system, because everything is centered with respect to my body. My sensor is with respect to my body, and that's a known transformation. The features I observe with respect to my sensor can also be directly related to my body. Wherever my body is, that's the center. I want to clarify that even further. The concept of pose does not require a universal reference frame at all. It's just two objects, two reference frames, and their pose. I can determine that no matter where they are in the universe. It doesn't matter. The pose between two objects is determined. There's another object we're calling a body, and it's going to have a special role, but it's just another object. It's another reference frame. It's no different than this reference frame or that reference frame. It's just another reference frame, and we're going to be calculating things. We're going to use it in a special way, but it's just another reference frame. There is no universal anything in this system.

Does that help? I started with this because I thought there would be a lot of confusion about it and how to implement it. I wanted to avoid that, although I'm happy to talk about how neurons do this, which I think is really interesting, and how we would do it is also pretty interesting, but it doesn't really matter. It's the concept of a pose that's important, and the concept of a pose works in any number of dimensions. We'll just assume it's three dimensions at the moment.

This is a key insight I had here that I didn't understand before: features also have a full pose relative to a sensor or relative to other things. There's an entire reference frame associated with a feature; it's not just something. Now, imagine you have a sensor—like with vision, I can say I have a reference frame for my sensor, and I can say the feature is at some point in distance and location relative to me, but also it has an orientation at that point. Some sensors don't really give you enough information for that. For example, I might just have a sensor that detects something solid here, but nothing else. I bumped up against something, or something out there that my thing can't pass through, and I know there's something there. That will still work in the system. In that case, the feature detecting doesn't have a full sense of orientation. If I were looking at a surface, I could be rotating the surface and wouldn't know the difference, and something like color may not have any sort of orientation to it.

The most important thing about a feature is it's at some pose; it's relative to other things. This is really confusing because when you think about sound, sight, and touch, you may not realize that, but this is a key part of the whole thing. We're going to assume there's a reference frame attached to every feature. For some features, it's very obvious why there is; for other features, it's fuzzy what the orientation is, and you can't really detect it. But the general case is there is a pose of that to this, or there's a reference to an attachment feature. If I have my finger touching an object, then my finger is generally right up against the object, so the distance to that or the location of that feature from my finger is almost coincident with the finger itself. That's just a special case of this. It's just saying this sensor can only detect things up to a certain distance away.

By the way, it's been shown that if you wear a thick glove, your perception of where you're touching is on the outside of the glove, not your finger—it's on the outside. It's like when you use a tool; touch is not always right on the object. You have a sensor at some distance from that sensor and some pose to the sensor where there's a feature.

Any further questions about that before we go on?

One of the thought experiments I had was interesting. I was holding this pen in my hand, this Muji pen, and not looking at it, just running my finger, touching some feature on this pen—maybe the bottom of the clip, the top edge, or the side. In hindsight, it's not surprising, but it was surprising when I thought about it carefully. I have a sense for what I'm detecting. For example, I'm detecting this edge on the side of the clip, and my sense is where is that? I can sense it relative to my body. I know where this is. The same touch of my finger here changes when I move over here; it's a different location relative to my body, and here it's a different location, but it's the same input on my finger. When I rotate my finger on here, my perception is the edge is still oriented in the same position relative to my body. Changing the orientation of my sensor doesn't change my perception of what I'm feeling; this feature is at some location relative to my body.

I also notice if I roll my finger over a feature like this, I'm invoking different sensory patches, and yet I sense the same feature at the same position relative to my body. I'm invoking different sensory patches, different sensors in some sense; we're actuating this thing, and yet I have the same perception about where it is. It's the same feature. Some people, I remember back in the old days in the old office, used to run my finger on a whiteboard and say, "How is it I have the same sensation all the time, even though my thumb was changing?" That's a reference. I also noticed it doesn't matter which part of my skin—I can touch it with my finger, my other finger, the back of my finger, or my arm—I get the same sensation.

You can think of it this way: imagine I'm going to have a whole bunch of—now we're going to start drawing a system. I'll erase this for now.

Imagine I'm going to draw like this. These little rectangles are sensor patches. These are different parts of my skin. There are tens of thousands of them, feeding information into this processing element here.

As I touch the pen with different parts of my sensory app, these individual columns, these individual modules, are being activated. Yet I have the same perception of what's going on out in the world. We're going to assume that inside these modules, they know the pose of themselves to the body. So models know the pose: module to the body.

If you have my body, I basically know this pose. I'm sensing something, a feature from the sensor, and it doesn't matter which sensor is sensing that feature or what pose we're sensing the feature from—they all report the same thing. What you calculate here is the pose of the feature relative to the body. A module receives input from a sensor and always knows the pose of the sensor to the body. Goal number one is determining the feature and its pose to the sensor. It knows this pose and can calculate it; it's pretty simple. Goal number two is to determine the pose of the feature to the body, which is the output of this process—the feature relative to the body. That's why it doesn't seem to change as I'm doing this, unless the feature moves relative to the body. If I touch an object with any sensor, in any order, I determine a series of features all relative to the body: the pose of this feature, then the next, and so on.

These occur sequentially over time as I touch one at a time.

Suppose there's some communications information here—a bus, for example. I'm not specifying how it works, just that I can bring all this to a centralized location.

I'll add a memory module, something that can learn. It receives features at poses relative to the body over time, and I can store them. These are features to the body.

As long as the object doesn't move relative to the body, that set defines the morphology or whatever the object is, all relative to my body at that moment. This set of features defines the object, but only at its current position relative to the body. If it moves or changes orientation, all this information becomes useless.

There's a different sensory modality for the same feature. What does "feature" mean? I want to leave sensor modality out for now, since there are many modalities measuring different qualities in the world. When I say a feature is a measurable property at some pose, I don't care yet what the property is. Right now, I only care about morphology; it's more than that, but we'll include other attributes the feature may have. At that location, the feature has a pose and attributes. I could say it's green, rough, or give more definition to the feature, but don't worry about that for now. The most important thing is the pose, plus other qualities that sensors can detect. The qualities from vision differ from those from sound, touch, echolocation, or whisking. It's a distraction; it's important that sensors detect different feature qualities, but if I look at and touch a feature, both my eyes and fingers report the feature at the same location and orientation, though with different qualities. Eyes might say it's blue, fingers might say it's cold. I can't share that information between modalities, but that's not the important part of a feature. The most important part is its pose, plus additional qualities from different sensors.

That's a very important distinction. Focus on the pose of the feature.

Now, in neuroscience, this is part of the "where" columns, possibly happening in the entorhinal cortex, which might be the lateral staff—the "where" equivalent. It's hard to say, but it doesn't matter. All of this is in egocentric or body-centric space, representing features relative to the body at any given time. Sometimes we want that, but if I want to learn a model of the object, I want it not relative to my body, but to an internally consistent point of view. We can define an object as having a common reference frame, just like the body has a common point. We pick a point, and if I define features relative to that common point, I have a model of the object independent of the body.

We talked about anchoring, head direction cells, or anchoring grid cells. It's basically saying, I'm going to pick a point that defines the reference frame for this common point for the object. Now I can relate all the features to that common point instead of the body. If I relate features to that common point, I have the definition of the object: a set of features, all relative to a common point, a pose. That is the definition of an object.

We can get there easily if we know the pose between the body and this common point. There's a pose between the body's point and the object's point, which changes as the object moves around the world. If we know that, we can go back and forth between a body-centric model and an object-centric model. This is a transformation based on the pose of the body to the object.

Now I can take every feature and transform them to a set of features relative to the object by doing this transposition. This assumes I know the pose of the body of the object. I may not know that, but if I start out and say this is stable and I'm stable, I can learn this by randomly choosing a reference point for the object. As I go around and touch these things, I build up a list of features, all relative to the object. This is closer to what we did in the columns paper, where we had a set of features at relative locations on an object, but we didn't have the concept of orientation, which was essential. We missed that and only discovered it once Luiz made the robot simulator.

Now, I've learned this, but what if I didn't know it? Later, I come across this object in a different position and orientation and try to infer it. I have the model I learned earlier, but I don't know the transformation. If I knew the pose of the body of the object, I could figure out what it is, but I don't. I have features relative to my body, but how do I figure out which one is over here? Some features have unique qualities that allow immediate identification. For example, if only one feature in the world is pink and I see a pink feature, I know exactly where I am because there's only one model that's pink. Generally, features exist in many places, and you can't assume anything unique about them. Their arrangement is unique, so I need another way to determine the pose of the body to the object.

We've developed a second way of representing a model of an object: the graph. The graph says I have features, and I learn the pose between individual features. I learn the pose between this feature and that feature and represent it in the graph. Marcus started proposing this a couple of months ago, and we've worked with that idea. It's a different representation.

The nice thing about this is that we remove the idea of the sensor and just think about the body. I detect one feature at this location, another feature here, and another feature there. In this model, we've learned the pose between these two features, which is part of what the graph does. The edges of this graph are independent of the object reference point; it's a pose of feature to feature. The origin doesn't matter—there is no origin, just feature to feature. Each feature has its own reference frame. Everything in the world has its own reference frame. The angle and distance of the blue line do not depend on the common reference point. Every feature has a reference frame, and I can calculate the distance and angle between them.

That's all you need to know. This system helps illuminate the set of features. The beauty is that I can't look at a particular feature and say where it is over here, because I don't know the pose. However, I can look at two features and, from my body-centric point of view, knowing the pose of each feature, calculate the blue line.

We've had many discussions about this.

So you can calculate this blue line. Now I have something that's not a guarantee, but I'm looking for objects where I'm not only considering a feature, but two features in relative poses to each other. This is much more discriminatory and narrows down the number of possible objects significantly. As I calculate and observe, if I don't recognize the object with two glances, I can make another observation and calculate another pose. Eventually, I'll narrow down and quickly figure out which object I'm looking at—it might take a few observations, but it's guaranteed to work in some sense. Even if it takes five observations, eventually only one object will match all those poses, and then I know what object I'm looking at. Once I know the object, I can calculate the pose and infer my position. Sometimes one observation is enough if the feature is unique; sometimes two observations are needed if the relative poses are unique, or I keep going until I find a unique object that matches those poses.

How do you recognize a feature now? Good question. Can we come back to that? This assumes a discrete set of features you know, but in a real object like a coffee cup, you may not always pick the same thing. Let's come back to that—it's a subtle point I want to address later. I have some observations, though not all the answers, but I think some are important and will open up discussion. So let's assume you know that for the moment.

Now, notice here, I have a system we've designed that can learn rapidly by sensing one thing at a time using any of an array of sensors. I can only observe one at a time, but by doing that, I'm able to build an object model that I can infer later through a series of observations. That's pretty cool. There's no voting going on here, at least not voting on object ID. Eliminating candidates isn't voting in the sense we've discussed, where multiple columns vote together. This is just internal, within the list of candidate objects, and all within one module. The voting described in the columns paper involves multiple models of the same thing, but within a single cortical column, as you get more sensations, you're narrowing down possibilities. That's still happening here, but I don't call that voting—it would be confusing. The way we describe voting in the columns paper and elsewhere is voting between models, between two modeling systems. I'll get to that in a second.

Is it the same algorithm? I don't know. That's an interesting question. Voting as described in the columns paper does not exist here. In the columns paper, it's three different columns with complete models trying to agree, but here I have one model and multiple sensors. Eventually, you would have multiple modules. We're going to go there next. At the moment, I'm thinking of this as a fast learning system, maybe like the entorhinal cortex, learning very quickly and building up a model fast, so I can recognize it again—like knowing where items are on a table. I can come back and say, yes, it's the same table, I know where that item is.

So I have a system that learns a model of the things in front of you very quickly. You can build it up from any sensor, infer it from any sensor, recognize the object, and make predictions. There's enough information here to solve many modeling problems, but it's only one model of the world, one model of the senses you have. This all leads up to the next extension. Is that okay so far?

Wouldn't it be accurate to say that the list of possible objects also gives you an implicit policy of where to look next, guiding you to the point that resolves the most uncertainty? If you go there, that's a mechanism I didn't consider here. One thing we haven't discussed is the motor output end—how to direct these things to go somewhere. For example, you might say, "We're confused, but if you observe this location on the object, then we'd know the answer." In that case, you could instruct someone to observe that location and report back. That's an outer loop of intelligence. Is that what you're thinking, Viviane? Yes, exactly.

There's a lot I'm not discussing right now. There's no motor system here; I'm just saying if I make these observations and these things move, this is what would happen. Clearly, much more is going on beyond this, but I'm going to focus in one direction first. I might answer that later, perhaps towards what Subutai asked. We're assuming in this model there is certainty about where a feature is relative to the body. What happens when we add uncertainty? I don't assume certainty; there's always some uncertainty. You could have noise associated with each other, and nothing is ever precise in the world. There's no infinite precision of location.

Let's jump into biology for a moment. One way biology represents location is with vector cells. A vector cell has a preferred orientation, shown in 2D, and a group of cells are active around that point. You might say this cell is active at a certain angle and distance, and another cell is active elsewhere. These are vector cells. The speculation is that as you get further out, the regions get larger. If a cell is active, it could be anywhere out here. From the module's perspective, that's all one place; there's no further distinction. It's not an error—it's just that the resolution of space decreases as you get further out.

There's a subtle distinction between my certainty that it's this location and what that location actually represents. Physical space might be quite large, but from the module's perspective, it's all one place. A cell represents that location. From a sensory point of view, if my finger is touching something, it's not going to be far away; it's right here. For my eyes, if they're pointing here, the feature won't be behind my back; it's somewhere in this direction. There's a cloud of possibility, but not a union of locations. The sensor knows the general direction; it's not completely uncertain.

But what if it's conflicting or uncertain? That would be strange. If your eyes are looking one way but, for some biological reason, start detecting things elsewhere, you'd be confused and the system would fall apart. There are physical constraints here. It's an interesting question, but I don't want to go there. If you want to, I prefer to stick with what I've stated so far. Are there other questions about this? I want to continue with the rest.

I think the concepts have to be clear. The implementation details may not, but the concepts should be clear. These are like where columns, but the theory we've had in our papers and what we know about the brain is that there are also what columns associated with where columns. I'm going to argue that each of these can also build a model, very similar to how this builds a model, almost identical. The difference is that these models, if I think about a brain, are going to learn very slowly, and there's a good reason for that. We don't want these models—these would be like V1 or S1—to learn things rapidly, because other things are going to rely upon them, so you don't want to do that. You want to learn models of things that occur consistently over time, for a long period, things that are going to be around for a long time. Not some temporary arrangement, not something for today or tomorrow, but more consistent reality, things that are consistently in the world. These will learn slowly, but they're going to do the same thing as over here, just not rapidly.

A good example is when you learn the letters of the alphabet. As a child, the letters don't look like anything; they're just gobbledygook. Someone has to say, there's this thing called the down arrow, and another thing like this, and we call that a B. You see, I have this feature plus this feature, that equals a B at some relative location. Then you learn another letter, this is a D, it's like that, but it goes this way. In the beginning, you have to think hard about this and practice over and over, because it's not obvious; you can't just see it and recognize it. You have to see these components and their ability, and you practice repeatedly, slowly building up these models of what these things are. Sometimes you can learn it quickly here—I went to kindergarten for the first time, I learned the first four letters of the alphabet quickly here—but ultimately, we want to learn it down here, and you have to practice over and over.

I'll leave it at that for the moment. Now we have multiple models. How would they build the model? Exactly the same way as here. They have the same information. Imagine one sensor at a time is getting input, and that sensor posts a pose of the feature to the body. That information is accessible to all these people, so everyone can learn a model at the same time. My fingertip is exploring an object, but the next sensory patch is over; all can access the information and learn the module at the same time. I'm only touching with this finger, but other parts of my skin could learn the same model, because they all have access to this pose—the feature relative to the body—they can convert that as a pose, as a feature, and build a model from that. It's the same information that this guy's using and the same information that this one's using. We're going to do it slowly, but that's an implementation detail. We can all learn.

Here's a very important thing: when you're learning something new, you can't grab it with all your sensors. That only works when you've learned it. When you're learning something new, you have to narrow down to some piece you can recognize, and then do a series of those to build up your graphical model. Only after you have many models can you grab them with one hand and look at them with all your eyes, but in the beginning, if you see something odd or new, you have to attend to each individual feature to build up that model. That's what's happening here.

Now I have models here, just like the model up here; there's really no difference. It's just multiple copies of the same thing, but these are a little slower, and each one is associated with a particular sensor. I've used a lot of resources here, replicating this for every sensor patch. I now have a complete model, which is expensive. That's why primary and secondary sensory and motor cortices are so big—because I have lots of models of the same thing, down at one level. Before I go on, any questions about that?

You don't have the what columns? This would be a what column, but you don't have them collaborating. I will, in a second. Now, this is where voting comes in. We talked about voting: you have multiple modules, so imagine I have some input here, and some input here, or here. After we've learned all these things—maybe after we've learned the letters of the alphabet—and now I have a bunch of sensory inputs coming in at the same time, each of these systems says, "I see a feature at some location." I can now try to figure out what my model is recognizing. Everybody has uncertainty because they only sense one thing, but they can now vote as we've described. In the columns paper, they have the ability to vote, so they can say, "We all have some features at some locations, and now we have enough information together that we can say at a glance, we can all touch the cup with our fingers, we can all look at the letter with multiple columns in my visual cortex." With one presentation or fewer, I'm able to resolve which object or graph I'm looking at.

What do you gain from this? The first thing is much faster processing. Prior to this, to recognize an object, I'd have to thoroughly go through the features. That's how I learned it—learning requires that. But once I've learned it in this system, I still have to do that to recognize the object, going through one at a time, seeing which of the graph elements, and it would take multiple presentations to figure out what object it is. Here, I can do it with one presentation, potentially one, but certainly fewer than before. We made that point in the columns paper. That's one advantage of this system, but at a great cost, because I now have many copies of this model.

What's the information passed on the green lines? I don't know yet, actually. I'm not thinking about that level. All I'm thinking about at the moment is that there is enough information here to resolve it, so we can say there could be the union of object-like things. That's how we talked about it—this is the voting mechanism we have in the columns paper. That would work, but I don't really know yet how to incorporate the relative orientation component. I don't know if you've thought about that, Marcus. We've said your fingers know where they are relative to each other, and that helps you recognize the coffee cup. I haven't worked out those details yet. Let's just say we have one part of it, which is the voting mechanism in the columns paper. That's an important part, but even that doesn't represent object state. We'll talk about object state in a moment. Can we just leave that? For sensations, as you've shown, you have four choose two—six pairwise things you can calculate.

The blue lines are what you call relative positions of the features on the object. You said four choose two—all pairwise combinations. So you have six of those blue lines now, you can send back to them. You know what those six are. I think that's right. That's information you can use to narrow things down. Again, I'm going to put that into the implementation detail. I'm just trying to figure out who knows what and what you can do with it. There is enough information here to much more quickly resolve what the object is. As soon as these objects are in the graph, you can work out mechanisms to make it happen. I think there's a huge amount of biological evidence to support this idea. One of the biggest problems I've spent my life trying to understand is how we can recognize an image at one glance. I knew that vision was not like that—vision is always moving your sensors, and in touch, you can't even imagine what one glance is. But I can say, yes, it's like grabbing this coffee cup, but it's not an image. How does that work? How can this grab be similar to that grab? How does that all work? This explains that. It says, here's how you can infer an object with multiple sensors in one sensation event, even though the object had to be learned sequentially, a feature in sequence. You can't learn an object just by staring at the letter; you have to look at the individual pieces. You can't learn anything like that. So that was one advantage here: very fast recognition of something.

Just a little aside: if you're going to build a practical system, the speed advantage comes at the cost of memory and training time. In a brain, the fastest you can do one of these things is about 100 milliseconds—how quickly you can move your finger or your eyes. But in a computer, you might be able to do this really fast, and infer the object with rapid sequential observations. Cameras out there say, "Okay, I got rid of it," but in the brain, things are slow. This gives you a real advantage. If understanding something with one glance versus having to look at three or four, which could be half a second, is the difference between life and death. That's all.

You're going to get something else from this. What else do you get? Imagine now we have another modeling system here. This is going to be secondary cortex; this is primary cortex. As before, this module thinks it has a sensor. It says, "I have a sensor." But let's assume for the moment that sensor is not a physical sensor. What it's sensing is the output of these modules, so you're passing up the hierarchy.

This is something a little fuzzy—I haven't worked through all the details yet. Think about it this way: this sensor is again trying to figure out if there's some object in the world at some location relative to my sensor.

This green thing here is an object in the world. Everyone agreed down here that this is the letter A, or the coffee cup, or whatever it is. This is an ensemble of features, and it doesn't matter where these sensors are moving around on the object. I could be moving my fingers, or my eyes could look at different points. The output of the green line is stable. It says, that's the coffee cup, that's the pen, that's the letter B. All this stuff is changing, but this part is constant. It says there's a B out there, or a coffee cup out there, and its position relative to this sensor isn't changing, even though my sensors are moving. My eyes move around, but the B stays at that location. I don't imagine the B moving as I move my eyes, or the coffee cup moving as I move my fingers. Sometimes a sensor detects a feature that is a composite object. It's getting input from these sources, and the feature is composite—a more complex feature.

This module itself cannot know any of the details. If it's saying this is the letter B, it doesn't have any knowledge of what the details of a B are. It just detects a feature consistently, and we call it a B. It doesn't know anything about the detail. That's all you can say. It doesn't know any further spatial discrimination; it just knows there's a feature at some location and pose relative to itself. There's no further spatial discrimination possible. It's an odd concept. The lower modules have further spatial discrimination, but this one doesn't. It just says, it's that thing. I've seen this pattern, and another pattern, and I can build up a model. If these recognize letters of the alphabet, then here I'm recognizing words. Words are combinations of letters in certain positions relative to each other. At this level, I recognize the word "dog," but I don't know that "dog" has a D, an O, and a G. It's just a thing I observe. Down here, they can say a dog is composed of a D, an O, and a G in relative positions.

This led me to the idea of composite objects and how you represent things as part of things. I always hoped that hierarchical composition could be defined within a module, but this view says no—you have one level of composition in any particular module. The letter B has components in a specific arrangement, but that's it; it can't represent another level of abstraction. I have to go up to the next module to say, now these components are arranged in a certain way, and I'm going to form a representation of the whole. There's one level of composition at each level, and it's not like this module can have a model of a word and also a model of a letter and the individual components underneath. Maybe that's true now, but I'm going in that direction, which implies a limit to how many levels of composition you can have based on the hierarchy. Here I have one level of composition, then another, and then another.

For the first time, I had the language to describe what a higher-level cortical column is doing. It's detecting a feature, which is the output of the lower modules, at a particular location, which means this module can't know any of the details. It just doesn't know them. Most of the time, my conceptual thoughts are pretty high up, and I'm not aware of all the details below, but if I want to drill down, I can. There's some way we can come back down, though I don't know how that would happen yet. At least I have a language to think about it. We can come back down and ask, what is that actual feature? Where should it be, given the pose, and so on.

That's all I want to say for now. At the higher levels, you're going to have multiples of these modules doing the same thing, and they're going to be voting as well.

At a more abstract level, you can see why you don't want this part changing very often. If you change it, you're changing what features this module is based on. If I fundamentally change all the letters of the alphabet, then there are different features below, and a different set of objects. If I want different sets of objects below, this module will be confused. You can't do that, so this module has to be very stable. Otherwise, you have to retrain the entire hierarchy. We see these problems when trying to learn something fundamentally new. It can be difficult. I can never learn Chinese, and now I have to learn to recognize Chinese characters, which I can't, but I could look at the features and try to remember them. Then I might be relearning all these modules below and forgetting other things, and the world can get complicated.

That's why these are pretty stable. It also implies that the object you recognize at this level may not actually be a real object. It could be things like letters, or things that occur statistically often. This is just an idea I haven't thought about fully—what is a cylinder?

A cylinder is a set of features that transcend any particular object, but they might be a consistent set of filters or features I detect—round things that are similar. I might learn something like a cylinder-like feature here. It doesn't have to be something you can name; it's just a statistically correlated relationship with features I see often, so I learn to represent them as one thing. That doesn't have to correspond to something physical like a letter or cup; it could, but it could also correspond to subcomponents, types of shapes, or similar things, because it's statistically learned over time. That changes that.