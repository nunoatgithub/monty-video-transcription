Okay, we are going to get started. Thank you very much for joining us, regardless of your time zone. We appreciate you coming to join us live. This is going to be an exciting presentation. Let's get started.

Quickly, the agenda: we'll go through welcomes and introductions, then Jeff will give an overview of the Thousand Brains Theory. Viviane Clay and Niels Leton will showcase the implementation and the work so far. I'll go over how you can get involved in the community ecosystem. We'll have a Q&A session at the end. That's the plan, so let's jump in.

A couple of administrative points: this meeting will be recorded and posted on YouTube, so you'll have access to everything. If you need to leave halfway through because of work, that's fine. You can submit questions throughout the presentation using the Q&A feature in your Zoom toolbar. We may address some questions as they come up and may hold others until the end, depending on whether we can answer them quickly in the Zoom chat.

Please feel free to post questions throughout. With that, we'll start with introductions.

If you're on this webinar, you probably know Jeff Hawkins. He doesn't really need an introduction, but I'll do my best. Jeff founded Numenta about 20 years ago and has devoted his life to understanding how human intelligence works. He has two seminal works: "On Intelligence" and "A Thousand Brains Theory." I highly recommend these books; they define and refine intelligence and what it means to be intelligent. This is his life's work. He also invented the Palm Pilot, founded Palm, founded the Redwood Neuroscience Institute, and invented the Graffiti recognition language. These are his side projects that got us here.

Next, you'll meet Dr. Viviane Clay, director of the Thousand Brains Project. She holds a PhD in computational cognition from the University of Osnabrück and has won many awards, including the Mendeley Fairset Data Award, the IT Talent Lower Saxony for her Master's thesis, and the Award for Extraordinary Master's Thesis from the Rosen Group.

Then we have Niels Leton, also a doctor—an actual medical doctor who graduated from the University of Bristol. He found that being a doctor wasn't challenging enough, so he went back and got a PhD in neuroscience from Oxford. Niels is our lead researcher at the Thousand Brains Project.

Here's a quick shot of the rest of the team—a candid moment where everyone looks thoughtful. I'm Will Warren, bottom center, the community manager for Numenta and your MC today. Honorable mention to Tristan, who isn't in the screenshot because he was off camera—he says he was probably eating at the time. Tristan is also a member of the team.

With that, I'll hand it over to Jeff so we can get started. Jeff, whenever you're ready.

Jeff, you're muted.

Sorry about that. Am I visible? Okay, great. Thank you everyone for being here this morning, evening, or wherever you are. Whether you're here live or watching this recording later, we appreciate you taking the time to learn about the Thousand Brains Project. We're excited about it. As you probably know, but it's worth stating again, the Thousand Brains Project is a new open-source project for creating a new type of AI technology based on brain theory, replicating how humans and other mammals think and understand the world. As Will mentioned, this has been ongoing for many decades and is the culmination of several decades of research, primarily at Numenta for the last 20 years. We call it sensorimotor learning. I will explain what that means and give you an introduction to the theory. Let's jump right into it. I'm going to share my screen and start with a few quotes. Let me make the presentation here.

Brains work on very different principles than any kind of machine learning technology we have today. Twenty, thirty, forty years ago, we didn't know what those principles were, but now we do, and we can say definitively that brains work on very different principles. I'll explain this in a moment. This perspective shows that today's AI technology, as exciting and powerful as it is, is not really on the path to true artificial intelligence. We believe the correct way to achieve this is to understand those principles by studying brains, and we're going to share them with you today.

We're not the only people who feel this way. Here are some quotes from others who have expressed similar concerns or feelings. We have a proposal and an alternative view for how to build intelligent systems, basing them on the way the brain works and sensorimotor learning.

Let's start with an overview of all learning systems. All learning systems, whether biological or machine learning systems, have the following structure: they have some sort of training data that's fed into the system, the system learns a model of the data—often referred to as a model of the world, though it's really just a model of the training data—and then the model is used to generate an output, whatever we want the system to do.

Historically, the AI world has focused on the output. We've judged the success of our systems by what they do: how well they can label images, translate languages, or play games like Go or chess. How the system works internally, although important, was secondary. A few years ago, convolutional neural networks were the hottest thing, and now transformer deep learning networks are the hottest thing. Whatever got the best result was what mattered. But this is not really the right way to look at it.

If we want to understand if a system is truly intelligent, what it will be capable of, or what its risks and rewards are, we have to look at the other parts of the system: how the training data is collected, where it comes from, and how the model represents data. There's an analogy in the computer world: when we think about what a computer is, we don't look at what the computer does. We say a computer is something that has a CPU, memory, program, data storage, and things like that, and then we can call it a universal Turing machine or a computer, and it can be applied to all kinds of problems, large or small. In the future, we're going to think about AI that way as well. The AI system will be defined by how it works internally. An intelligent system will be defined by how it works internally, and it can be applied to many different problems, some of which humans do, and some of which we don't do.

Let's talk about sensorimotor learning, which is the core of the Thousand Brains Project. Brains are sensorimotor learning systems. What does that mean? First, the data we feed into this system comes from sensors that detect aspects of the physical world. The primary sensors in humans are vision, hearing, and touch. We have other sensors too. Other mammals have different types of sensors, such as electric field sensors and the ability to see in ultraviolet and infrared, but it's always about sensing some physical attribute of the world. The output of the sensorimotor system is movement. Everything a human does is based on movement. My voice and speech right now are really muscle contractions. We have sensor data in and movement out.

There's an issue here: as we move through the world, the input on our sensors changes, often very rapidly. As we walk, turn our heads, touch objects with our hands and move them over surfaces, and move our eyes several times a second, the sensory data changes completely. The brain can't make sense of that if it doesn't know how it's moving. Sensorimotor systems have to account for how movement is occurring—how the body and sensors are moving through the world. That becomes a second input to the system. We have sensory data coming in—features of the world—and information about how the body and sensors are moving through space.

Internally, in sensory motor learning systems, research has shown that there is a sense of location: the system knows where the sensor data is coming from, where it is located relative to something else in the world. We refer to that as a reference frame. There could be reference frames relative to objects you're touching, rooms you're in, or your body. All information that comes into the brain is associated with some location in space. Models are learned by associating features with locations. As you move your fingers over an object or turn your eyes to look at different aspects of a scene, your brain is associating what you're seeing with where it is. When you look around a room, you know where things are. That seems simple, but the brain has to calculate and represent that internally, constantly matching features to locations.

For example, imagine you're touching a cup with the tip of your finger. As you move your finger over the surface, maybe on the rim, you might feel an edge or curvature, or a texture—that's the sensed feature. As your finger moves, the brain keeps track of where it is in space and is able to build a three-dimensional model of the cup. It's very much like CAD modeling. This is happening in your brain all the time. We build models of things, in this case a model of a cup, by associating what was sensed with where it was sensed. The movement of the sensor is fed in and updates this location.in a reference frame using a system called path integration, which basically means if it knows where it is, what direction the finger is moving, and how fast it's moving, it can update its location, much like dead reckoning, as ships used to do. This shows an example of a single fingertip. In your brain, a single fingertip could learn a model of something like a cup. You could put your hand in a black box, feel around with a single fingertip, and build a model of something by moving it like that.

But now, turning to brains, they do this in a very interesting way. I should mention, in summary, we learn by moving and sensing, and knowledge is represented by associating observations with locations in reference frames. In this way, we learn structured models of objects, places, and concepts. It's easy to imagine how you can learn models of objects and places this way. It's a little harder to see how you learn models of concepts like mathematics, democracy, or language, but as we'll see, this is how the brain does it. Although we don't understand all of it, it's clear the brain uses the same method for knowledge representation for everything that humans and other mammals do.

Now, if we look inside—one neuroscience slide here—if we look at the brain, this is a picture of the neocortex, which is about 75 percent of your brain. It is the organ of intelligence, meaning all the things we associate with intelligence, from basic perception of touch, hearing, and vision to language and thought, all occur in the neocortex. All mammals have a neocortex. Ours is particularly large, but all mammals have the same structure. If you look at this structure in detail, it's actually a sheet of neural tissue about three millimeters thick and about the size of a large dinner napkin, around 1,500 square centimeters. If you could cut through that sheet, you'd see columns—the thickness of the neocortex is about three millimeters. The neocortex is composed of these columns, called cortical columns, about a millimeter in area. We might have quite a few more than these; no one really knows. You can't actually see them like this in a brain, but neuroscientists have determined these columns are there. The structure of these columns is nearly identical everywhere in the brain. If you look anywhere in the brain, you see the same columnar structure and detailed architecture in each column, whether those columns are doing touch, vision, hearing, language, or anything else. Your brain is made up of copies of the same basic neural circuit over and over again. Other mammals have the same columnar structure; they just have fewer of them than we do.

It turns out that in the brain, those cortical columns are mimicking exactly what I just talked about. Each cortical column is a sensorimotor learning system. Each cortical column builds reference frames and models, based on movement and features coming in. In the Thousand Brains Project, we call these learning modules. You'll hear the term learning modules throughout the presentation today. They're equivalent to what's going on in the brain in a cortical column.

It's a highly distributed system—150,000 minimum in humans—of these sensorimotor learning systems. This is the primary reason we call this the Thousand Brains Theory and the Thousand Brains Project: we have many of these little things that are like small brains, just replicated. The human brain became large because evolution decided to make more copies of this basic circuit.

How do these things work together inside? This is tricky, but this diagram will illustrate it. In this image—you're going to see more of these later—we're representing eight of these learning modules. Each one shows a little reference frame and a box above it. They're attached to different sensors, represented by the blocks on the bottom. These learning modules are connected in two basic ways.

We know this from brain research. One is lateral connections between learning modules, which allow them to reach inference faster and lead to a unified perception. When you look at something, there are thousands of these cortical columns in your brain, each looking at a part of the visual world, each looking at a part of the retina. Nobody looks at the whole thing. Each one can infer what's going on through movement, but if they communicate, they can say, "I think it might be this," or "I think it might be that," and they can vote and reach a consensus quickly. This is why you can flash an image in front of someone's eyes and they can tell you roughly what it is—these columns communicate and reach a conclusion about what's going on without movement. Without this kind of voting, they'd all have to move; it would be like looking at the world through a straw or touching an object with only one finger. In that case, you need a lot of movement to infer and learn, but with voting, inference is much faster.

The second type of communication between these learning modules is hierarchical connections, shown here with two levels. These exist in the biological neocortex and in the Thousand Brains Project. These connections allow the system to learn models composed of objects made of other objects, which is how the world is structured. Words are composed of letters, sentences are composed of words, cars are composed of doors, windows, and tires, and wheels are composed of tires and rims. We learn this compositional structure through hierarchical connections like these.

These are the basic ways the brain and biology have learned to build intelligent systems. Sensorimotor systems are built of these individual learning modules, each a sensorimotor learning system. They're replicated in various ways and connected both horizontally and vertically. This is essentially how you understand the world—your brain is structured like this. Niels and Viviane will discuss this later. One of the advantages of this system is that, as we build intelligent systems, we only need to figure out how these learning modules work. Once you've got them working—which we have to a large extent—you can replicate them and create various types of systems. You can also apply them to different types of sensors. It's a very flexible system, and an incredible variety of AI or intelligent systems will be made this way in the future. I'm going to end with a few thoughts.

Machine intelligence is going to transform the 21st century, similar to how computing transformed the 20th century. We're just starting out—it's like 1940 or 1950 in the computing era. AI is about to experience rapid growth, leading to many benefits. I believe that sensorimotor learning, not today's AI techniques like deep learning, convolutional networks, or transformers, will be at the heart of this transformation. Those other techniques will remain important, but I don't believe they will be the core of AI. In the future, what people consider true AI will be based on sensorimotor learning, which is also central to robotics. This is how we will build truly robotic systems.

I hope the Thousand Brains Project will be a seminal event in creating truly intelligent machines. As far as I know, this is the first time all the material and understanding are available, and we're starting to build these systems. We've been working on this for several years—the theory has been around for decades, but the code has been developed mostly by Viviane and Niels over the past few years. We've proven that this approach works, so we're not starting from scratch. The Thousand Brains Project already has a lot of code, working systems, documentation, and videos. There's a lot to digest, and the concepts are new, so there's a learning curve. It's not like any other machine learning system, but it's rewarding, fun, and exciting. I hope many of you will contribute to the Thousand Brains Project. We've put together a great system and are hoping for many contributors.

With that, I'll pass it back to Will, our master of ceremonies. Thanks, Jeff, for the overview. Just a reminder: you can ask questions in the Q&A section and upvote others' questions. The top questions will be answered. Now I'll hand it over to Viviane for the next part of the presentation.

Niels and I are going to talk about Monty, our first implementation of a sensorimotor learning system. We call it Monty in reference to neuroscientist Vernon Mountcastle, who proposed the idea of the cortical column as a repeating general computational unit in the brain. There's a link here to show that this project already has a concrete, open-source implementation. You can find it at the provided URL, look at the code, run it yourself, or contribute.

Making this presentation was tricky because there's so much to cover, and we could discuss every slide in depth, but this meeting is scheduled for three hours and we'll only scratch the surface.

As Jeff mentioned, there's a learning curve when you first approach this project. We're not just taking an existing AI system or adding incremental improvements to transformers. We're building a fundamentally different system. To understand it, you'll need to set aside some preconceptions, such as the need for huge labeled or internet-scale datasets, and approach it with a fresh mind. I'll repeat some points Jeff made because these concepts take time to sink in. As Will said, feel free to ask questions as I present.

What is a thousand brain system? It's based on the thousand brains theory, named for the idea of the cortical column as a general, repeatable computational unit in the brain that can learn and do a lot on its own. About three years ago, we took this theory and the principles outlined in Jeff's book and started building a system based on them. We proved to ourselves that this approach works, and now we've made it open source and published our findings. It's been especially exciting for Niels and me, who've worked on it from the beginning. The past two weeks since publishing have been fun, discussing these ideas and this approach. I hope you'll be excited too. Let me go through the key principles of these systems.

First and foremost, it's based on sensorimotor learning and inference. As Jeff mentioned, everything is about sensing and moving—both learning and inference involve sensing and moving. The models being learned are designed for this kind of data: they explicitly model streams of sensorimotor input and produce action outputs.

How are they designed for that? One way to model sensorimotor data is using reference frames, with explicit representations of space and the ability to path integrate. For example, if I go from A to B to C, I know how to get back from C to A, even if I never took that path before. Reference frames can be thought of like Cartesian coordinates. As you look around a room, you can recognize objects, know where they are relative to each other, remember that, and model it using reference frames.

And then the cortical column as the general repeatable functional unit.

Each of these columns can learn full models on its own. It's not just implementing an edge detector or detecting a pattern or texture; it can actually model entire three-dimensional objects. It doesn't sense the object all at once, but moves over the object, building a model of the larger object and recognizing it. This point is important because, coming from computer vision, you might expect to give a photograph to an AI system and have it process the whole picture. In the brain, there is no full image anywhere. The retina has many blood vessels, the optic nerve creates a blind spot you don't perceive, and neuron bundles go through the LGN, with left and right visual fields separated. Each neuron in V1 perceives a very small part of the visual space. For example, if you look at your laptop, a neuron in V1 has a receptive field smaller than a couple of millimeters. Each cortical column only sees a tiny patch of the input and never the entire image. Instead, you move your eyes, saccading about three to four times a second, to recognize and learn about things.

Lastly, we have what we call the cortical messaging protocol, which allows all columns—whether modeling touch, vision, or abstract concepts—to communicate using a common language. This makes it easy to have multimodal systems, scale the system, and stack modules. I'll go into more detail on that, but I wanted to outline these principles again. Jeff also touched on them, but it's important to keep these in mind. These principles have guided us for the past three years as we built the system. Every design decision was checked against them, as neuroscience literature shows this is what the brain's intelligence is based on. We always try to keep these key principles in mind when building Monty.

Now I'll go into the actual implementation and concepts, starting with the cortical messaging protocol, which is the glue that holds the system together.

We start by defining what a message needs to contain. The key ingredients are a pose—location and orientation in a common reference frame, which could be relative to the body or some external point. It can also contain a list of features, such as color or amount of curvature sensed at that location. Additionally, there can be auxiliary routing information like confidence or sender ID for internal routing.

This messaging protocol is used everywhere inside the Monty system. For instance, sensor modules get raw sensory input and output an observed state adhering to this protocol. For example, a sensor might output, "I'm sensing this location and orientation, and I'm sensing red and a curved surface."

This goes into the learning module, which tries to infer what larger object is being sensed and outputs a hypothesized state, such as, "I'm sensing a red coffee mug in an upright orientation." It can also communicate possible objects laterally to other learning modules to speed up inference, again using the same protocol. Lastly, a learning module can output goal states to the motor system, telling it how to move.

What does this messaging protocol provide? First, it enables seamless multimodal integration. Learning modules that model touch input can easily communicate with those that model vision, lidar, ultrasound, or any other modality, because they all follow the protocol. It also allows easy modeling of compositional objects. Whatever object and pose a learning module recognizes can become the input to the next module. The coffee mug recognized here becomes a feature in the model of another module. The learning module doesn't need to know where in the hierarchy it is; it doesn't matter whether input comes from a sensor or another module, as it's all in the format of features and poses.

This makes the system very extendable and scalable. For a simple application, you can use one learning module and one sensor, which is already powerful. For larger applications with many sensors, you can use thousands of these modules and scale as needed. Unlike transformers that require billions of parameters, you can build small solutions with just a few modules or scale up for more complex, hierarchical concepts.

It's a plug-and-play, parallelizable system. Each module can use quick associative local learning and run in parallel with others. They don't need to wait or process in serial, making the system efficient.

That's the messaging protocol. Now to the two core components: sensor modules and learning modules. The sensor modules always see a small part of the object and move. For example, with a coffee mug, the sensor module just sees a small patch of the object, like the tip of your finger moving over an object or a small patch on your retina.

And then the main job of the sensor module is to turn raw modality-specific data into the cortical messaging protocol. For example, if you have an RGBD camera, the camera sees this patch; you can extract a point cloud from that depth image. From that, you can extract a location relative to the body, the camera, the agent, or some external point of reference, and you can extract a rotation. We usually define that by the point normal that points out of the surface and the two principal curvature directions. You can also extract other features, such as the color sensed or the amount of curvature being sensed. This is then in the correct format to send to a learning module.

The important thing to say again is that the sensor module needs to be able to extract a pose. Otherwise, it can't send something in the format of the cortical messaging protocol. It needs to be able to extract pose or movement information. This can be done in many ways: you could get a copy of the motor command, or extract it from optical flow, proprioception, or other signals.

The learning modules are really the heart of the modeling system. The purpose of the learning module is to model incoming streams of poses and features using reference frames. Using reference frames means the model is not just a bag of features; it's features at poses. For example, if we have a car and first look at the tire, then the door, and the hood, we sense those as features but associate them with locations in a reference frame of that object. The tire is at a relative location to the door, and the door is at a relative location to the hood. This is what we inferred from looking at the neuroscience and what the neocortex is doing. The whole structure of the learning module is heavily inspired by the structure of cortical columns in the neocortex. If you take a slice of the neocortex anywhere, you find this layered structure of different types of neurons and cells. This structure is found everywhere across the neocortex and is remarkably similar everywhere. We spent a lot of time looking at the neuroanatomy and physiological studies and what we know about the neocortex by now. You can draw some analogies between how we implemented Monty and what's happening in the neocortex, although we don't always explicitly talk about it in the code, but just as a bit of background.

In our current implementation, the learning module we use most frequently is called the evidence-based learning module. I'll give you a quick overview of how that works at a high level. We have a sensor module that converts the sensory input into the messaging protocol, which means we get features and pose, and we get a displacement, which is like a movement in space. It also produces a motor output since it's a sensorimotor system, and since it's interacting with the world, this happens over a series of steps. Each motor output leads to a new sensory input, which gets processed and integrated again, leading to a new motor output, and so on.

As we get features and poses as input, we have some hypotheses in memory and can update the evidence for those hypotheses. For example, if we have four objects in memory, we model these as features at locations. As we get feature inputs and movements, we can say how consistent the series of features and locations is with our models in memory. For example, we might have gotten some features and movements that made this bowl very unlikely, and all these cylindrical objects are likely, but even within an object, some locations are more likely than others. The most likely of these hypotheses becomes the hypothesized pose and idea of the object—for instance, a coffee mug in an upright orientation and location relative to the body. This would be the output of the learning module, which could become the input to another learning module or just be the output of the system if we do simple object and pose recognition. We can also share this information with other learning modules laterally to reach a conclusion quicker in a process called voting. Lastly, we can have model-based policies where we use these hypotheses to generate goal states and tell the motor system how to move next. Niels will go into more detail on those. 

To see the system in action, I included an animation that I will step through slowly to explain this visually. We have a small sensor patch that moves over a coffee mug. This zoomed-out view is just for visualization. We move down the coffee mug and then to the bottom of the mug. With every step, we update our hypotheses. By this time, we have already decided it can't be the banana—very low likelihood. The dice is way too small. The bowl is also not very likely. On the coffee mug, most locations are also unlikely, but there's a little red ring—red means highest likelihood—that goes around the coffee mug at about the height of where the sensor patch is right now. This is where we think we most likely are. We can't really determine yet where around the coffee mug we are, but once we get more distinct features like the handle, we can be pretty sure of where we are and what object we are sensing. This will eventually be the classification. Importantly, we always have a most likely hypothesis. We don't need to sense the entire object to say anything, but we always have a working hypothesis. We update and refine it with every step.

Another example is with a dice, which is smaller. We're moving on the dice, and since it's so small, we quickly rule out the larger objects. On the dice, we have lots of evidence. Since it's such a symmetrical object, it's a bit harder to determine how it's actually rotated. The system can recognize this symmetry and, in this case, detected a rotation that's not the one we showed it, but it's basically the same look. It detected a symmetry, which is what we want in a system; we wouldn't want it to rely on tiny artifacts on the dice.

That's how the system works: moving, sensing, updating hypotheses. At every step, we have a most likely hypothesis of what we're sensing. This example used one sensor and one learning module, but we can use more and vote between them. For instance, we could have five patches sensing different parts of the mug, each connected to a sensor module, with outputs going to five learning modules. Since they receive different input, they might have different hypotheses—perhaps thinking they're on different objects or the same object in different orientations. We can quickly resolve this by sending lateral votes, narrowing down which object is being sensed more quickly and robustly.

This figure puts everything into one image. The three components are learning modules, sensor modules, and the motor system. In this example, we have two sensors: an eye and a finger. The eye senses two parts of the mug, and the finger touches the table. Each patch goes into a separate sensor module, which extracts features—blue for the mug, flat surface for the table. These features and poses go into learning modules, which, through successive movement, build up hypotheses of what they're currently sensing. Ideally, the two would sense a coffee mug and recognize it, while the finger, after moving, might recognize it's sensing a table. Since they're following the cortical messaging protocol, these can become input to another learning module, which might learn a compositional scene of the coffee mug standing on the table—relative locations. The coffee mug on the table and the mug in the table then become features in that compositional object.

We might also have skip connections, sending output directly from the sensor module to a learning module. There are lateral connections for voting and quicker inference, and top-down connections—if we've already recognized a dinner table scene, that might bias lower-level learning modules to recognize what's on the table more quickly. Every learning module has a motor output, similar to the brain, where outputs go to subcortical motor areas across the brain, not just from motor cortex. These go to the motor system, which translates them into actual motor commands. The dashed lines don't need to follow the cortical messaging protocol. The motor system and sensor modules are the interface between Monty and the world, translating between raw data and the cortical messaging protocol used within the system.

Now, to the code. This is the Monty framework. In our codebase, you'll find abstract classes with these names and their relationships. This is a simplified view. We have a Monty experiment, which an experimenter can use to set up the framework for testing and define the tasks for Monty. Monty can have as many sensor modules, learning modules, and a motor system as needed. Monty manages how sensor modules connect to learning modules and how learning modules connect to each other, helping with information routing. The environment is a bit more complicated now, but we plan to simplify it. The learning module has memory and object models. The core idea is that you can customize each class individually without changing the others—plug and play and compose a Monty system however you like. Use as many sensor or learning modules as you want, connect them as you wish, and write custom classes for each box. For example, you can write a custom sensor module for a specific sensor, plug it into the framework, and use the same learning module, motor system, and environment to test a different sensor. We have a distance sensor like an eye moving in a socket, a surface sensor that moves like a hand, and a sensor module that only sends messages when features change significantly. We can add noise for experimentation. You might implement your own sensor modules, such as for sound, lidar, or ultrasound.

You can also customize a learning module. Much of our work uses a graph learning module, where we use explicit graphs to model objects, making everything easy to visualize, debug, and understand system behavior. We've iterated through different learning modules, starting with a displacement learning module using displacements to recognize objects, moving to a feature learning module focusing on graph nodes, and now the evidence graph learning module, which tracks continuous evidence values for different objects and poses. Each has strengths and weaknesses, generally improving over time, but it's been easy to plug in different types of learning modules for comparison while keeping everything else the same. We've also experimented with a learning module using Numenta's previous work—hierarchical temporal memory combined with a simulation of grid cells. You might implement a neuromorphic learning module in hardware. As long as it adheres to the defined interface for the learning module, you can implement it however you like, provided it's built for modeling sensorimotor data—a reference frame inside a learning module is very useful.

If you have that, you can replace these, leave everything else the same, and explore other approaches beyond our evidence graph learning module. We may develop new approaches in the future.

You can also customize the environment. In this case, the data loader and dataset can be adjusted. We often work with the Habitat simulator, a 3D simulation engine. We have also implemented a real robots environment as a simulator. There is an Omniglot environment, where you trace the strokes of different handwritten alphabets. We have one where you move a small patch across an image. For datasets, we often use the YCP datasets, which include 77 household objects that are rendered. Habitat recently introduced a scene dataset with 3D objects arranged relative to each other. We also have a dataset of real-world images. There are many different ways you could test the system.

The Monty experiment defines how you put Monty into the environment and how you measure performance. We have focused on object recognition, but have also explored generalization. Supervised training and the profiler are useful for optimizing the system. There are many other possible experiments, such as object manipulation, washing dishes, mapping an area, or detecting problems. The general idea is that this core algorithm applies to a wide range of areas, and this framework allows you to do that easily. I hope this was inspiring and gave you ideas for interesting tests or components to try, replace, or experiment with. With that, I'll hand over to Niels.

Thank you. I'm going to talk about motor policies in Monty. I thought it would be useful to take a step back, since you've heard a lot about the Thousand Brains concept. There are many learning modules, each developing explicit structured models of objects in the world. They communicate with one another and are semi-independent to a certain degree—these are the thousand brains. You've also heard about the system being sensorimotor, but it's important to emphasize that each learning module is a fully sensorimotor system in its own right. This is a key concept for how it operates and acts in the world. As Viviane mentioned, this was inspired by our knowledge of the brain. I won't go into neuroscience detail, but if you look at the cells in a cortical column, which maps onto the learning module, there's a distinct layer that projects to subcortical parts of the nervous system that control the body.

You may have heard of the motor cortex and think that's the part of the brain that controls movement, while everything else is relatively inert in terms of motor control. That's not the case. Wherever neuroscientists have looked for these cells, they've found them throughout the neocortex. It's the entire organ—every cortical column, and therefore in our implementation, every learning module—sends motor control throughout the system. This is very different from how sensorimotor control is implemented in current AI systems. It's a highly distributed, semi-independent collection of units providing sensorimotor control.

This perspective is useful to recap, since we've talked a lot about motor control and how the system acts, but what exactly is being controlled? In our documentation and code, you'll see the concept of agents. We showed the example of an eye and a hand. An agent is a system that can move semi-independently from another system and has sensors associated with it. For example, an eye can move independently from a finger, and both have sensors.

In our code, we refer to two things: a distant agent and a surface agent. The distant agent maps to an eye or a camera that pivots around. The surface agent corresponds to a finger or robotic digit that samples the environment.

For example, the distant agent can rapidly look around a scene and depends on sensory information that can propagate freely through space, like light. The surface agent needs to close the space to the objects it's sensing, but this provides additional advantages, such as more sensory modalities and the ability to manipulate objects.

Ultimately, this distinction is somewhat artificial. As we move from biological to artificial systems, such as in simulation or robotics, we have more flexibility. Cameras are inexpensive, and you can imagine a robotic arm with cameras on the appendage to sense color.

This gives a sense of what is being controlled when discussing policies. Policies refer to how the system, given the current state of the environment and itself, decides to act. A policy could focus on different objectives. One is inference—recognizing what you're seeing and where it is. Another is learning—efficiently gathering new information to build new representations. A third is manipulation—changing the environment. Our current focus is on inference, and to a lesser degree, learning, but we plan to explore all of these in the future.

Policies can be structured in two main ways. The first is model-free, in AI terminology, which I'll explain in a moment. The other is model-based.

In Monty, the model-free system refers to the early part of the architecture where sensory information enters a sensor module, is sent to a motor system, and then produces an action output. This is called model-free because this circuit operates without building explicit models of what exists in the world.

This is represented by the fact that the learning modules further down in the system aren't involved in these decisions. It's analogous to how subcortical structures in the brain help us carry out actions without involving more deliberate cortical thought processes. Although this is a model-free system and doesn't have explicit models, it can still learn and be quite powerful. It's an important part of how the system efficiently interacts with the world.

When Monty first "opens its eyes," it randomly moves around on the surface of an object, performing small saccades. From the beginning, we implemented simple, innate model-free policies, such as if you move off an object and are looking into the distance, try to move back onto the object to better sense what is nearby.

We then implemented more sophisticated policies. For example, the surface agent has a policy that enables it to move onto the surface of an object and, using incoming sensory information—without an explicit model like "I'm sensing a coffee mug"—make movements that keep it on the object's surface and enable exploration.

By itself, this process might result in the agent moving in a straight line, looping around the object, or moving randomly without a deliberate goal. An early model-free policy to make this more deliberate is curvature following. We've implemented this: in an object like a coffee mug, there are dominant curvatures at any point, and when you feel the surface, you can sense them. These dominant curvatures indicate where interesting features are or where the object changes in an interesting way. Without knowing the actual shape of the whole object, if the system senses these curvatures and follows them, it efficiently explores features like the rim or handle of the mug.

In practice, if the agent starts at the bottom of an upside-down mug and moves along its side, when it reaches the rim, it senses a strong difference in curvature and decides to follow it, seeking interesting features as it moves.

So far, these model-free policies are innate policies we believe are important for efficient interaction with the world. But you can also learn model-free policies, and that's something we're exploring. Any learned policy can leverage innate policies as primitives.

What we're really excited about is model-based policies, where true intelligent action comes in. The name "model-based" comes from the idea that the actions being proposed—what we call goal states—are informed by an explicit model of the world that a learning module has. As shown with pink arrows, every learning module projects these goal states around the system to influence the system's state and, therefore, the world's state.

Whereas model-free is analogous to subcortical structures, model-based is analogous to more deliberate, planned action enabled by the cortex.

We're excited about this because model-based policies are a major challenge and a key goal in artificial intelligence for intelligent action, something mammals do very well. For example, AlphaGo's success in defeating Lee Sedol in 2016 relied on providing an explicit model of the Go board's structure. In the real world, we don't have such explicit structures available; it's not a simple mathematical object we can inject into the system. In complex settings, like a family cooking, there are many objects, each with complex behaviors, structures, and interactions. To act intelligently using explicit models, the system must first learn the structure of these objects.

One implemented model-based policy is the hypothesis testing policy. This allows the system, given its knowledge of known objects and current sensory input, to propose where to sense in the world to efficiently disambiguate between objects and their poses. For example, if the system knows models of a spoon, knife, and fork, and is currently sensing the handle of a spoon, it should move to the head of the object to disambiguate which piece of cutlery it is observing.

Unfortunately, I don't have time to go into detail about the algorithm today, but at its core, it takes the models developed by the learning module, transforms them using the information it receives about the current state of the world, and compares these representations to propose where the objects would be most different if both are present in the environment.

In action, imagine on the left we have the spoon as it's being perceived by the surface agent, represented by the blue line with the red joint. This is the fifth step in the environment, so the surface agent has moved along for some time and has a sense that it's on the handle of one of these pieces of cutlery.

It's uncertain about exactly what it's perceiving, so the middle shows the system's mental representation of its most likely hypotheses about what is present in the environment. The top two contenders are the spoon and the knife. By transforming these representations with the information it's receiving, it can infer that the place it should move to, to disambiguate as quickly as possible, is the red point highlighted here.

On the right, the next step in the environment shows the agent using that information to perform a rapid jump to that location, confirming that it is, in fact, the spoon.

This slide shows the same process with an animation where the surface agent is sensing a fork. On the left is a side view, and on the right is a top view. This also shows a mixture of model-free policies and the model-based policy just described. As the surface agent starts on the object, it senses the curvature, which points it toward something interesting, causing the system to efficiently explore the length of the handle rather than moving randomly, without needing an explicit model at that point. As it reaches the end of the fork, it has a good sense of whether it's a fork, spoon, or knife, and understands its pose in space because it hasn't sensed the other end. At that point, it can propose a new location to jump to, to disambiguate between the final most likely objects. On the right, it jumps to the head of the fork and then continues its model-free exploration. There isn't a tension between the model-free and model-based policies; they work together to help the system act efficiently in the world.

Up until now, you've heard about learning modules, sensor modules, motor systems, and the cortical messaging protocol that binds it all together. What does this really unlock? That's where I'll hand it back to Viviane.

Alright, thanks. So what are the current capabilities of the system? We're actively working on a paper about this, so keep an eye out for that. For now, here's a brief overview of what the system currently does. Many of our experiments focus on object and pose recognition. We show the system a variety of three-dimensional objects in different orientations and test how well it recognizes these objects and their poses. The typical setup involves an episode with a series of steps: each step involves moving, sending one action, receiving one sensory input, and updating the learning module's hypotheses. The next step repeats this process until the system is reasonably confident in what it is sensing, at which point the episode ends. Then we show a different object for the next episode, and the process repeats. Once all objects in the dataset have been shown once, that's an epoch. In the next epoch, we show all objects again, usually in a different orientation. That's our normal experimental setup.

There are also two distinct phases within an episode, especially during learning. We can have a matching phase, where the system is still trying to infer what it is sensing. For example, as we move along the cup, eventually the sensations become consistent with the model of a cup. This can be followed by an exploration phase, where the agent moves further on the object and explores areas underrepresented in its model, using these additional observations to update its memory and model of the object.

And we have a set—if you go to our documentation, you'll find a section called benchmark experiments, where we have tables reporting results on different experiments. Here, I'm showing a table of results on the YCB dataset, testing the 77 objects in that dataset under various conditions: sensing with different types of agents, showing the objects in random rotations the system has never seen before, and adding noise. The last row shows recognition with five learning modules instead of just one. We also have experiments like unsupervised learning, evaluating that in Monty Mead's world. I'll show a brief teaser: we tested the system on real-world data using the iPad camera to take an RGBD image of an object—here, a Numenta coffee mug. We move a small patch over the object, and based on this patch and its movement, the system tries to infer which object it is sensing. We also generated 3D meshes of these objects, presenting a challenging sim-to-real transfer for testing. We generated five different conditions for these images: normal, dark, bright with a lot of reflection, hand intrusion where the hand occludes the object, and other objects in the frame next to the target object. We have results for all of these in our documentation, and you can run these experiments yourself. The datasets are available on AWS storage, and all the code is in the repository if you'd like to try it.

What are the current capabilities of the system? It can learn from very little data. Usually, we train the system with 14 images of each object—14 to cover all the different features, showing each of its six sides and the eight edges. That's very little data; try training a CNN or transformer on that amount. Learning is quick, and it can also learn with little or no supervision.

In Monty, learning and evaluation are very intertwined. The only difference is that during evaluation, we don't update models, but during training, we do inference. We try to recognize the object, and once we do, we can add more points to the object. Since we know the object and its pose, it's easy to do this. We also have benchmark experiments testing learning without any labels, where the system might merge very similar objects. For example, we showed a mug and a red cup, which are quite similar, and the system decided to put them into one object model since we didn't specify they were distinct.

The system can do continual learning without catastrophic forgetting because each object is learned in its own reference frame. We don't have the interference seen in artificial neural networks. We can show images in any order, add new objects later, and continual learning is naturally possible with this approach.

As shown before, it can recognize objects and their poses, even when sampling new points on the object that haven't been seen during learning, showing objects in novel orientations or locations relative to the body, with noisy observations, and while sensing only part of the object.

Here's an example of a noisy object, where there is noise in the sensed locations, color, and curvature, and we show them in random rotations not seen before.

Due to the cortical messaging protocol, cross-modality transfer is available out of the box. We can vote between sensors of different modalities, or train a learning module using touch, then swap in a vision sensor and still do inference, recognizing objects with reasonable accuracy.

It generalizes based on object shape, similar to how humans do. Here, we show a new object never seen before, and the system picks the most similar object from memory based on shape.

If we do clustering, objects are grouped by rough shapes—cups here, round objects there, and so on. Deep neural networks often overly rely on texture or other low-level features, but humans do not. As Niels mentioned, we can efficiently move across the object to recognize it faster. Lastly, we use significantly less compute than deep learning systems, requiring little compute for learning and inference.

Now I'll hand back to Niels. With that overview of the current state, I'll talk about some future capabilities we're working on in research, and also, although applications aren't our current focus, some long-term possibilities for Monty.

It's important to emphasize that we're excited about making this open source because we want everyone to be involved. Some research items I'll mention next are areas where we have concrete ideas for implementation, but as a small team, we haven't gotten to many of them yet. That's where your involvement would be exciting. Of course, we don't have all the answers, so we're also eager to hear ideas for improving Thousand Brains systems and making them even better.

One example of something we're really interested in is hierarchy and composition, which has been touched on in this webinar. Our focus until now has been on getting a single learning module to have the capabilities we believe a cortical column in the neocortex has, which is quite a lot. It's certainly more than is often assumed, and it's a powerful system. Now we feel confident that we have many of those elements, so it's an appropriate time to start chaining them together and gaining the benefits of hierarchy, which are clearly important for understanding compositional objects in the world. For example, a Segway is composed of wheels and a handle, and understanding how those relate to one another is key.

Another area of interest is deformations. For instance, the first time you saw a Salvador Dali melting clock, you immediately recognized it as a clock. You didn't need to be trained on several examples of a clock like this, and you weren't just relying on low-level features. You understood it was a clock with a unique shape and built a full representation. We have some ideas for how to implement this, but it's not something we have yet. A related concept is scale. You can see a saw the size of a building or one smaller than your fingertip and immediately recognize and understand their relative scale. This is another interesting area we're excited to start implementing solutions for.

Another important topic is abstract spaces. Someone asked a good question about this, so I'll touch on it more. The cortex is where the representation of these abstract spaces is found. What's interesting, as Jeff mentioned, is that you might think you need a totally new algorithm or representational space for these concepts, but neuroanatomy shows that every part of the brain, including the prefrontal cortex and other regions, uses the same core anatomy and structures. We believe the same computational principles we've implemented in the learning modules can apply to abstract spaces. This could be anything from a family tree to a taxonomy for classifying species. To be more concrete, we generally feel a cortical column is capable of representing 3D space plus a temporal dimension where relevant, and any abstract space a human can comprehend can be embedded in 3D space. For example, a family tree can be embedded in that space. What defines it as abstract is the way you move through that space—the transitions from child to parent, parent to grandchild, or grandparent to grandchild, which skip through discrete movements. It's likely related to how composition and changes in the low-level learning modules feed into higher-level modules, and how changes in what's represented are understood as movement through the space. The important point is that the same types of reference frames are still relevant; we don't need a totally different type of representational space.

We're also excited about hierarchical, model-based policies. I've already discussed model-based policies and how they use explicit models of what we understand in the world—objects at every scale, from the keys on your keyboard to planning your thesis. These are all objects with structured representations that inform your actions. For this to be truly interesting, any given learning module will only know some objects in the world at various levels of abstraction. To plan complex actions, as humans and other mammals do, you need to chain these together and decompose complex tasks to carry them out. I mentioned earlier that model-based policies are a goal in AI, but we believe the unique architecture of a Thousand Brains system—the way sensory information comes in, motor information leaves, and the use of reference frames—sets us up to develop these model-based representations at the necessary levels of abstraction for hierarchical policies. For example, making a cup of coffee in a friend's home, where you've never used their coffee machine or seen their kitchen, still relies on rich representations at different levels of abstraction. You can decompose the goal into simpler states and ultimately achieve what you need to.

This was just a sample of some of the ideas we've been brainstorming and writing up. As I said, we welcome any and all contributions from the community. If you're interested in the research side, please check out the future work section on our README, where we have documentation about what we're working on and where we'd love help.

So that was more the fundamental research side, where we're going and where we see Monty developing over the next few months and year. Another question is, later down the road, what kind of applications would Monty shine in? This isn't something we're working on at the moment, but we think this is ultimately the aim: to enable us to help people and make useful systems. A useful analogy is thinking about how any given technology has a problem set that it maps well onto. A relatively simple technology, like a calculator, is a great achievement but is particularly useful for things like numerical operations. A more general technology like deep learning has found a variety of uses, and as Jeff said, we don't believe it's going away anytime soon. At its core, it's about being able to approximate arbitrary functions and, where appropriate, perform generative sampling. That's useful for a large range of domains, from predicting the structure of a protein to code assistance, which we ourselves use and find very helpful. The Thousand Brains Project is where we believe the kind of technology needed for sensorimotor learning in action will emerge, which is a huge space of very challenging domains all around us. Robotics is the obvious one, but there are other examples. In general, if you're wondering what Monty would be good at, it's worth thinking about the kinds of things humans are good at: learning with small amounts of data, building structured representations, moving efficiently, and similar tasks. Humans are not good at crunching large numerical operations, which gives a sense of the scope where thousand brain systems will be at their strongest.

This is not an application we're working on, but to give a concrete example, one we've thought was interesting and certainly relevant is medical ultrasound, which has a variety of important use cases around the world. Medical ultrasound is inherently a sensorimotor form of imaging in the medical world because the sonographer uses a probe to capture a 2D slice of the patient's tissue. The thing they're trying to understand is a 3D structure, and any given view with the probe will be suboptimal in some way. It might cut through the tissues in a way that doesn't provide the desired view, or it might be obstructed, similar to how certain objects in a room are not visible because they're behind other objects. A sonographer has to move the probe to build up an understanding of the patient's health. There are various ways to do this, but ultimately, this enables something with an extremely complex structure, like the vasculature system of the liver, to be understood. As Viviane discussed, it's the partial view of the world at any given time, but by building that up over time with movement, you can develop a more complete representation.

Revisiting some of the real strengths of thousand brain systems in this context, sensorimotor tasks are where they will shine. This doesn't need to be physically embodied; it could be in cyberspace or a more abstract control system, but it must be sensorimotor at its core. Ultrasound is a good example of that. Another key strength is dealing with small amounts of data, which is important in many domains, especially medical ones where patient data is hard to come by.

Thousand brain systems can perform inference with very small amounts of compute, certainly compared to many deep learning systems. This could have advantages ranging from greener AI systems to computations at the edge where necessary. The representations developed are structured, which has various downstream benefits, including interpretability. In many domains, especially medical ones, it's important to understand how the system develops its representations, what those representations look like, and how they influence decisions or outputs. We think the thousand brain system is well suited for this. Structured representations are also useful for robustness and generalization. By developing explicit models, it's easier to transfer from training data to test data, even when they differ, which is currently a challenging task. Related to this is the importance of continual learning. In many domains, especially medicine, the field is constantly changing, and it's not ideal if you need to retrain the entire model on batch data to update its representations.

Finally, multimodality, which Viviane touched on, comes in different forms. One is transfer in modality. For example, a system for ultrasound that's trained on 3D models in anatomical atlases could transfer to actual ultrasound, a different modality. There are also voting connections, which enable information to be shared across modalities at the same time. For example, medical imaging could provide information about a patient's health while the sonographic system is operating. All of this can work together.

That was just an example of where these strengths might emerge, but really, there is a large range of advantages that we believe will have diverse applications. There are others I didn't mention, such as the small amount of compute needed at inference, and this is even more pronounced during training because of how rapidly the system learns. The system is able to learn the vast majority of its representations in an entirely unsupervised way, which is important for many domains. The model-based, hierarchical policies I mentioned earlier are crucial for dealing with novel problems and a wide range of tasks.

We're excited about this because sensorimotor problems are everywhere. These could include robotic use cases in domestic or industrial settings, controlling experimental apparatus to accelerate science, or navigating cyberspace for security purposes. There is a large range of problems we hope this will help address. Jeff often remarks, and I think this is useful, that it's hard to anticipate the actual use cases of a technology while it's being developed. We're focusing on developing the core technology, making it easy to use, and getting your help to accelerate this development. We hope it can go out and bring significant benefits to the world.

We're really excited to collaborate with all of you. On that note, I'll hand over to Will, who will talk more about community engagement.

Thank you, Niels and Viviane. That was a lot of new information, and your brain might be melting a bit. I'm going to talk now about the community and the engagement we would like from you. With that, let's get into it.

First, I want to thank the community. So far, we've had an overwhelming number of responses and positive, congratulatory questions coming in. These are just the first week of YouTube comments being displayed here. We were overwhelmed and didn't expect this kind of turnout so quickly. We've also had great responses from our community forum—interesting and informed questions and discussions are happening there. I want to call out D Led specifically, who is not only asking questions but also submitted the first pull request we merged from the community into our repository. Thank you for that fix to our documentation. And to Humble Traveler, thank you for the thoughtful, robust responses, including one about qualia and other interesting areas. Raleigh has been trying to implement on a Raspberry Pi and ran into some issues. This is exciting for us because if you're putting Monty on a Raspberry Pi, it's probably going to be strapped to a robot at some point. We're excited to see what the community does as we progress. Thank you to everyone who starred our repository in the first few days. We grew quickly to 120 stars, which really helps with our trust level within the GitHub community and other systems. Thank you to everyone who has contributed and asked questions so far. We will get to them in about 15 minutes.

I want to give you a quick overview of the systems available to interact with us and the community. Here is our GitHub repository: Thousand Brains Project organization, then tbp.monty. This is where we store all our code, tests, and documentation. If you want to make changes to any part of the project, this is the repository to use.

I also want to mention Monty Lab. This repository holds all the experiments we're building. If you saw the video of Monty Meets World, the experiment is laid out in this project. You can go and have a look. These are projects that use the TBP Monty code to run experiments.

We also have our documentation server. This is an extensive set of documentation about all the concepts we've covered here. It goes in depth with videos, diagrams, and all kinds of information. It's broken into six major steps: an overview of the project, how to use Monty, getting started, how to install it on your machine, how Monty works (covering the architecture behind the code), a section about contributing (code, documentation, tutorials), and a community section with our code of conduct, patent pledge, and a section on future work. These are all the large blocks of work that we or someone in the community can take on and start developing, so we're excited about that.

We also have our YouTube channel, where many of you have been watching our videos. We have over 200 hours of video footage from our research meetings, organized into different playlists. The primary one is the Quickstart playlist, which, despite its name, is six and a half hours long and covers all the concepts in depth to help you get up and running with the Monty project. Jeff mentioned the learning curve for this project is steep. The code and implementation are not complex, but the concepts are new, and you will need to start thinking about the world in a different way. That's the Quickstart series.

We also have the core series, which is similar to the Quickstart series for now, but will expand as we publish more videos. You can explore any of the concepts in greater detail there. Additionally, we have two more categories in a more raw format: brainstorming and reviews. Brainstorming sessions involve the team discussing specific ideas and how to implement them. Reviews are when a subject matter expert presents on state-of-the-art topics in fields like reinforcement learning, robotics, or neuroscience papers that are relevant but not yet covered. All of these are available on YouTube, with more to come in the future.

We have our discourse forum, where community members can chat, ask questions, post topics, and get answers. If you get stuck, you can ask questions about any issues you encounter. The forum has four main categories: General, for questions and information that don't fit elsewhere; Videos Discussions, where each video is referenced for threaded discussion about its concepts; and then Research Theory and Monty Code, which cover the theoretical and implementation sides of the Monty project.

We're also on social media—Twitter, BlueSky, and LinkedIn. Our Linktree address is at the bottom, so you can connect with us on those platforms. We post frequently on these forums, sharing every video, interesting developments, and published articles.

Now, let's talk about getting involved. Community involvement is a core reason we made this project open source. What kinds of problems are we looking to solve? One is moving beyond artificial neural network benchmarks. There's a lot of information about neural networks, deep learning, and transformer networks, which can be overwhelming. This project offers new directions, and we're excited to see your perspective.

If you want to understand the principles behind human intelligence, learning about cortical columns and how they perceive the world in patches can transform your understanding of real-world interaction. That's something I've found fascinating as I've learned these principles.

If you have a limited compute budget but still want to do interesting AI research, running deep learning systems on millions of images is expensive. As shown in the presentations, Monty uses orders of magnitude less data to learn representations, making it cheaper and faster to run. If you don't have access to large datasets for training deep learning networks to human-level performance, Monty is a good fit.

If you have applications with moving sensors, as Niels mentioned, there are many possibilities—sonograms and other problems involving moving sensors. We're also interested in novel industry applications, so if you have sensors that move in ways we haven't seen, we'd like to hear about them.

If you need to integrate multimodal sensors—learning an object with vision and manipulating it with touch, or combining radar and lidar for stable representations—Monty can do this. It's very hard for deep learning networks to achieve this. You need a system that learns quickly and continuously. Most deep learning systems require a separate training phase that can take hours or weeks, which isn't practical in novel environments where you need to learn on the fly. Monty is well-suited for these problems.

If you're tired of using black box brute force approaches, Monty provides a deep understanding of how the system learns, making it more interpretable than deep neural networks, which can be opaque and surprising when examined internally.

That's the problem space. If you have all of these needs, that's great. If you have some, that's also good.

As for incentives, we believe you'll be able to write widely received publications based on this work. The modules we and the community build can serve as the basis for a master's thesis, or you could write a thesis on the entire Thousand Brains approach. You'll become part of an awesome community, including the Thousand Brains Project team and all the people already contributing and asking questions.

You will have your project showcased. If you strap a Raspberry Pi to something exciting, please let us know and we will include it in our documentation. We would love to see all your applications, robotic or not, so please let us know how you get on. If you publish a paper, you can be listed on our TVP papers page. You can contribute code and start adding to this active community. You can also unlock achievements; for example, when you submit an RFC and it is merged, you get to choose a player icon so that on our roadmap, you can see which parts are being worked on by the community.

Let's have a quick look at the roadmap. This is a representation of a spreadsheet that's available online. You can view it now. Each checked-off part represents something we have already built, and the majority are unchecked—these are things on our roadmap that we plan to build in the future. We expect the community to help build these. We do not intend to build all of them ourselves. We would be thrilled if a community member took one of these and tried to implement it.

Let's look at some of these in more detail. In the voting section, you heard about how columns vote together to do faster inference. We currently don't use pose for voting, so that might be an interesting exploration—how much faster can we make inference if we send around pose?

There are various pieces of the environment we want to clean up. One question that came up is that we're not compatible with Windows right now because of our habitat environment, so we want to decouple that so anyone can run it on Windows and other operating systems as well.

There's a lot to do on the motor system. This is key to robotics and our future direction: bottom-up exploration policies, top-down exploration policies, calculating a salience map, and finding areas of interest for learning objects. On the framework side, adding GPU support to speed up inference, exploring neuromorphic hardware approaches, heterarchy and hierarchy, more learning modules, and more sensor modules. These are all on our roadmap, and we would love for the community to get involved.

I want to talk about two processes for making changes to the Monty repository: pull requests, which are for minor changes or smaller work items, and requests for comments (RFCs), which are for major changes to the system, such as a new sensor module or a change to the cortical messaging protocol.

A pull request is straightforward if you're familiar with GitHub. The process is almost identical: you fork the repository, clone it, make your changes—fix a bug, write a tutorial—and if it's your first time committing or opening a pull request, you'll be asked to sign the Contributor License Agreement (CLA). Then you'll go through the normal PR review process as with any code or software changes.

Now, about the request for comments. This was new to me when I joined Numenta, but I've come to love this process. An important premise is that the Thousand Brains Project team is committed to working in public. We want your input, feedback, and contributions. To do that, we have to work in public; we can't make decisions privately and then surprise you later. The RFC process makes major changes visible and explains why we're making them. We solicit feedback from the community to see if they think it's the correct thing to do, the right implementation, or the right technology choice. We're excited to start doing this. We've been using RFCs internally for major changes for about two months, and they've produced great results.

An RFC represents a major change to the system. It's essentially a document that describes what the change will be. Anyone can comment on an RFC as it's in process, and anyone can create an RFC, both within the Thousand Brains Project team and the community at large.

When you create an RFC, you use a template with various sections and helpful text. It includes a summary of what you're changing, the motivation, any rationale and alternatives found in your research, prior art and references (if applicable), and any unresolved questions or future possibilities. You create this document and then a pull request with it. You'll get comments, and we'll go through the normal change comment review process. This may happen several times as we strengthen the idea, add questions, ask for alternatives, or try small proof-of-concept implementations. We then reach a final comment period, where the Thousand Brains Project team decides whether to incorporate the change. If not, we'll close the RFC; if yes, we'll merge it. At this point, the document exists in the repository as something we want to do, but nothing has changed in the code yet. We've just agreed as a group and community that this is something we want to do in the future. Anyone in the community can try to implement these. If you find an RFC that is not yet implemented, we welcome you to try and implement it.

Okay, this is my last slide. I wanted to talk a little bit about the future from my perspective as well. The presentation you just watched introduced an enormous number of new ideas, and it can be overwhelming, but we believe as a group—and we hope the community agrees with us—that sensorimotor learning is the future of machine learning. We think that AI systems in the future will all be based on this or something very similar. Sensorimotor learning is the core of everything we do, and we think the technologies this will enable will be the foundation of a new era of human progress. The technologies we'll be able to build on top of this platform will be incredible. We're very excited about the potential of this project.

The reason we have made this open source is twofold. First, we think this technology should not be in the hands of large corporations that can afford to run huge deep neural network training sessions, as is currently the case. We also don't want this to be in the hands of state-level actors. We believe everyone in the world should have access to this technology. The other reason is that we're a small team, as Niels mentioned—there are only seven of us working on this right now. If we want to make this a reality, we're going to need help from the community. We need you to join us, to build this, and to help us form the foundation in the best way possible. We invite you to join us and hope you do.

That is it. On to the next section, which is our Q&A. We have a lot of questions, and I'm going to state those questions so the team members can answer them as they see fit. Are you all ready? We have over 126 questions. I don't think we're going to get through them all. We may have a separate recording session where we just answer questions and release that as a separate video, but we'll see how we do. I'm going to go through them in vote order, sort by voting, and then ask those questions. If you want to answer, you can.

Here we go. This one was from the beginning of the presentation: How does the Thousand Brains Theory handle the integration of abstract concepts that lack direct sensorimotor grounding?

How are we going to decide who answers these questions? I touched on this one a bit. Was there anything you wanted to add, Jeff or Viviane? I can give a little historical context to this question. It was surprising that the brain works this way. No one anticipated that all the things we do as humans are derived from this common algorithm. It's not that simple, but that appears to be the fact, so we have to start with that assumption and work from there. Niels talked about it nicely for a bit. I wrote about this in my most recent book. I'll be honest, we don't completely understand it yet. Sometimes we think we understand it, and then we realize maybe not quite. We're getting there. Just adding to what Niels said, the concept of movement and sensation does not have to be physical. Movement just means that the thing you're detecting or using to measure something is now in a different location, but that location doesn't have to be physical space—it could be conceptual spaces. Niels talked about it.

To be honest, we haven't quite figured it all out yet, but I'm confident that in a short period of time we'll have very concrete answers to these questions.

Next question from Roderick: How does the Thousand Brains Theory differ from its earlier incarnation, Hierarchical Temporal Memory?

We didn't talk about that here. Hierarchical Temporal Memory was a term we used for a series of technologies and ideas we developed along the way. If I was new to this project, I wouldn't focus on Hierarchical Temporal Memory. It's not that it's wrong, but we've changed the language and the way we think about these problems. One of the key aspects of Hierarchical Temporal Memory was the models of sequence memory and models of neurons, which we still think are true in the brain. But we don't model the neurons or that mechanism in the Thousand Brains Project. There are other ways of doing it. There's no simple answer to that question, but if you're new to this, I would just focus on the documentation the team has put together for the Thousand Brains Project, which is excellent. Someday, over a beer, we can talk about how we got here by doing HTM.

I would add that those two are not opposed to each other; they just look at the problem from different directions. The Thousand Brains Project looks at higher-level principles, like the repeatable functional unit, learning modules or cortical columns, sensorimotor learning, and reference frames. Hierarchical Temporal Memory and previous work at Numenta looked at a lower level, modeling neurons, dendrites, and temporal sequences. In the future, we are open to bringing these ideas into the Thousand Brains Project. For example, the reference frames we use are explicit 3D coordinates rather than simulating them with grid cells, because at this early stage it makes it easier to visualize the system, understand it, and describe it to others. There may come a point where switching to more neural representations is a definite improvement, and we'll go down that route. But there are also advantages to these higher-level approaches, so that's always a balance we'll strike. Maybe, as Viviane mentioned earlier, these learning modules can interact together, and there may be hybrid approaches as well.

Next question from Avinash, an AGI question: Do you think that by scaling these systems to more than 150,000 cortical columns, human intelligence can be surpassed?

I'll start again.

First of all, I don't think it's our goal to surpass human intelligence. Humans are very complex creatures, with emotional intelligence and various capabilities controlled by the neocortex. Our goal is not to recreate humans or something just like a human. We aim to model the neocortex, which is a modeling system itself. To be as smart as a human, you would probably have to live a human life, have human emotions, drives, and needs, and none of the systems we're creating have any of that. Is it possible to build machines that can do things humans can't do? Certainly. I can easily imagine robotic systems that are much more precise, accurate, and capable than humans, and can surpass us in other categories as well.

Yes, it's possible, but that's not our goal. As I mentioned at the beginning of this talk, many applications—most of them—will not be human-like at all. They'll be embedded systems doing simple things, or things in areas we can't even imagine today. The goal of replicating human intelligence is not one we care too much about. It's more about creating technology that understands the world through different sensors, modalities, and embodiments, and can be applied to do things humans generally can't do.

Next question from Konstantinos: At which point is the concept of a cup cut out from the continuous stream of features at locations?

That will be a Viviane or Niels question.

I could answer it on a conceptual or algorithmic level. Essentially, the model of the cup exists in the associations in the cortical column between layer four and layer six neurons. In the implementation, associations are made between features at locations in a reference frame specific to that object. All the features on the cup share a reference frame, so all the locations exist in the same kind of space that you can traverse for the cup. The point at which you actually recognize the cup, or whatever model object you're sensing, is when you have gathered enough sensation to narrow down the possible hypotheses to that object and exclude other objects that might be similar in some regards.

In the cortical column, or if you're coming from the HTM terminology, it would be like the spatial pooler or layer two/three in the cortical column, where that object ID is then represented.

Maybe just one thing to add: if the question is also asking how unsupervised learning is really happening—how the system understands, "I got this stream of information, now I'm going to learn a cup," and then with more information, "now I'm going to learn a separate object"—unsupervised learning is already working in the system. It can receive this kind of information, and as Viviane mentioned, if it senses something and decides that the part it's seen is an object it saw part of before, it will recover that model and start adding information to it. It's not a perfect system, and this is something we're looking forward to implementing soon. We don't really have a sense of forgetting or paring down information at the moment, but that's definitely something we think is important. We have some ideas for how, over time, the system can sometimes make a mistake and merge two objects that were similar enough that it thought they should be one object, but in fact they shouldn't be. We imagine that over time, you'll develop better representations where the merging really does correspond to the actual structure out there in the world, as opposed to a one-time observation from a bad angle that led to an incorrect conclusion.

Great. Thank you, Niels. Jeff, a question from Jeremy. There are a few questions about language, so I'll summarize them in this one question: Do we have a vague or clear path to how language works in this system? How do you think about language later on, and do you think it's important to the Monty system?

I can start on that one. We did talk about language a fair amount. Language is something that, even in humans, comes last. You first learn about how the world works and how to interact with it before you start producing language. We don't plan to start with language as LLMs do, for example, but we think the system is certainly capable of understanding and producing language, and would actually have a much more grounded understanding of language. As Jeff already mentioned, language is also a compositional construct: letters are made out of strokes, words are made out of letters, sentences are made out of words, and the actual words you hear or read can be associated with concepts you've learned through other modalities. If I hear the word "cat," I can have an associative connection to a learning module that modeled a physical object of the cat—how the cat feels when I pet it and how it looks when I see it. That gives us a much more grounded understanding of language. I don't know if you want to add something to that.

We should have more conversations about this topic because it's so fascinating to think about. One thing I would add is that we use language very often to describe the physical structure of things. If someone's visiting and asks, "Where's the post office in town?" I'll say, "You go here, you see this, you go there, you see that," or I'll say, "Where's something in your kitchen?" and so on. I think language probably started as a way of transferring representational knowledge and reference frames from one person to another. "Where did you get that food?" "It was on the other side of that hill over there, and you had to go so much further." That's probably where it started—just a way of conveying the knowledge structure that's already in brains. If you think of it that way, it's pretty simple. It's a matter of taking the outputs of these learning modules and, instead of acting on them directly, sending representations of the different outputs to some system, which can then be conveyed to someone else. Vivian said it correctly: it's not our focus right now. Ultimately, these systems will have language. They may not all have the language we use; they'll have their own languages. Systemically, the goal of language is to take one Monty-like system and another Monty-like system and have them share knowledge and cooperate. If you think of it that way, it's not really hard to do. There are lots of ways you can do that. It's a great question, and I think it'll be fun to work on.

Great answers. Minimi asks, can you give us a feel for how you would use Monty with time series data, or how do you think about time in general?

It depends on what you mean by time series data. Everything that Monty learns from is a time series, since it's a sensorimotor system—there's always a time component in what we're learning, but there's also always a motor component. It's not just a passive series of feature inputs; it's actually features and movement that the system learns from. Learning from just a time series would probably lead to a pretty impoverished model of basically just that time series. We would focus on learning from sensorimotor data.

One thing we sometimes discuss is object behavior. We're still figuring out the details, but we often use the example of opening and closing a stapler as a simple case of object behavior. There are much more complex behaviors, such as understanding how a T-shirt can behave. At a high level, we've discussed two main ways to think about this. One is that any kind of behavior system, or what's observed as changing over time, is potentially dependent on motor actions. In some sense, that's a sequence and can be learned as a sequence, which is where algorithms like HTM become relevant. We've also talked about object behaviors in terms of the different components or subcomponents of an object and how they interact, almost like a graph, with edges connecting these subcomponents. The cortical column may be understanding how a particular subobject influences another. It may be a mixture of these two approaches—one is useful in some cases, the other in others. This gives a high-level view of how we might model object behavior, which is ultimately about how objects change over time.

That's the key: the world is changing over time. One of the first things we did from a neuroscience perspective was to figure out how neurons learn time sequences, like melodies and other high-order sequences. We developed a compelling theory for that, described in our paper "Widening Neurons of Thousands of Synapses," which has done very well. We realized that the same set of cells that can do sensorimotor sequences can also learn high-order sequences. For example, when you first learn to type, you go through a model-based process of figuring out where to move your fingers and how to press the keys. But after enough practice, you just learn the sequence and don't have to think about it—it just plays back like a melody. The same set of neurons in the brain can do both sensorimotor and, through practice, rote sequence playback and recognition. These mechanisms aren't directly part of the Thousand Brains Project today, but as we implement behaviors, sometimes they will be very model-based, and with practice, Monty's systems will get good at them. We have a great model of sequence processing in neurons, but it's not exactly in Monty yet.

Next question from Jeff: When each learning module builds a model of a cup, it has a different ID for "cup." When learning modules vote, they use their own independent IDs. So how does that vote for the same object?

Viviane, this may be a good one for you. I can answer that unless Jeff wants to answer from a Monty point of view. Basically, they just learn associations to whatever comes in. If you imagine a cortical column in the brain receiving lateral inputs as votes, it just learns to associate whatever inputs it gets as it's sensing the object over time, and these associations can then be used. There's no way to have the same ID for an object everywhere across the brain; instead, they locally associate whatever inputs they get when they sense the object.

And in the brain, these patterns that are associated are very sparse activations. You might have several thousand neurons representing object IDs, but they're sparse representations, so only a few hundred neurons are active. These sparse properties allow the system to spread patterns across a large number of cortical columns. As Viviane said, there's no central agreement on what the IDs are—it's just some neural pattern, and each column has its own pattern. They can all learn associatively to link their neural pattern with someone else's pattern if they're observing the same object at the same time. If multiple columns are observing the same object simultaneously, they'll learn to associate their outputs with each other. There's no need for agreement about what each column is saying; they don't even know who else is out there. That's the beauty—these learning models are really independent, and there's very little they need to know about the others for the whole system to work. You have to trust this; it's not always clear, but it does work.

It's also worth mentioning, and maybe Jeff Thompson has looked at the code and that's why he's asking, that the current implementation has an object ID that's in some sense shared between the learning modules. That's really just an implementation detail. We've alluded to being a small team and not having had a chance yet, but if you go to the future work section on our README—at least once the outstanding PR is merged—there's a section on implementing these associative connections. If that's something you're interested in, we'd love help with implementing it.

I didn't know how it was actually implemented in the code. This is a great question because it highlights a general problem we have. We started this project with deep neuroscience ideas, so I can tell you exactly how the neurons do this, but then how do you implement it in code? We have a lot of choices. We don't generally want to model all the details of neurons, so there are other ways of implementing it. As Niels said, we took a quick way of doing it for now, but we want to get back to the properties that the brain uses. It doesn't mean we have to model exactly how the brain does it, but at least we know how the brain does it and that it can be done. It takes some clever engineering to figure out how to do that in code when modeling neurons. That's something we do a lot. Many of the tasks ahead are things we sometimes understand better from the neuroscience point of view, but we can't just directly implement them as is.

Okay, great. We'll do three or four more questions and then we'll call it so the panelists can take a break. Ray asked, what is the basic learning algorithm being used here? Is it some sort of gradient-based learning, or is it based on Hebbian learning or something else? What do you call it?

I would say you can think of it as a form of Hebbian learning. It's associative learning. I mentioned before that right now our system is like a sponge that absorbs a lot of information, but it's not really forgetting things. That's something we want to add—some degree of decay in memories and things like that. But at its core, it is Hebbian. It's not at the extreme biological realism side of spike-time-dependent plasticity or anything. Anywhere we think time is important in the brain, there have usually been ways of abstracting that away. It's definitely not backpropagation of error. We've had some instances where we might use gradient descent within a learning module, and that's something we've debated, but we definitely don't want to be passing these kinds of errors over long-distance connections to do backpropagation.

So the TLDR is, you can think of it as Hebbian—it's associative learning. That's one of the things that makes it so interesting. It's a fast learning system because of this fast associative learning and very local learning within each module. That also makes it robust to catastrophic forgetting, for instance.

Next question from Erica: I'm curious why curvature is prioritized but not planar intersections. My understanding of infant development is that intersections of planes of an object are critical for the visual system to make a mental model of objects. Intersections serve as a relevant parameter for both curvature and tactile input, with intersections of planes being both tactilely perceivable and visually critical input. That was a long one, but I think useful. Sounds like a Niels question.

I'm not sure, to be honest. I'm not familiar with this, but it sounds super interesting. This is a great example of something that could be an RFC. In my case, I'm not very familiar with this. If you want to educate us about how this may be relevant, we're always looking for input. This could be something where the sensor module could be updated—the core stuff will stay the same, it's poses at locations—but as Viviane alluded to, there are many other features that can be passed into the system, and maybe something like this could be one of those.

I would agree. I also don't have a lot of knowledge about this, so I think I'll look it up after this meeting. If you can send more details on the discourse forum, that would be really helpful. To start, we were just looking for features that are simple to extract from a sensor patch and wanted to get something working. Principal curvatures fit the bill since the two principal curvatures, together with the point normal, span a reference frame, and we can easily use that to define three-dimensional rotation of that point in space. That's why we picked that.

Excellent. Bamshad asks, does this algorithm have potential for optimization on current computer hardware, or would it require neuromorphic hardware for better performance?

I'll take a stab at that. At the moment, we don't see neuromorphic hardware, in its traditional definition, playing an important role. Neuromorphic hardware, as originally conceived, uses the analog properties of semiconductors to model the analog properties of neurons, rather than using them in digital form. We're not going down that path; we're not modeling analog neurons in our system right now. Someone could do that in the future, as Viviane said—you could create new learning modules. On the other hand, I think there will be a lot of hardware accelerations. There are many operations here, and the whole system is ideally suited for semiconductor-based computing because you have this basic learning module that you need to create many times. You could optimize hardware to achieve the purpose of these learning modules and then make all kinds of systems by replicating and putting more or fewer modules on a chip or using different types of stacks. I think hardware will play a really important role in the future of this. How much neuromorphic computing hardware will play a role is yet to be seen.

Excellent.

Another question from Allison: In sensorimotor learning, will it be required for a computer to have a reference frame of self—how it incorporates itself in the world? Niels, do you want to take that one?

Yeah, sure. This was something I didn't get to in the policy section, but it's definitely something we're interested in, and we think it is important. I was talking a bit about how you have this almost classical concept of the motor cortex, where a lot of motor activity happens. We think that part of the brain is specialized for modeling the objects that are your own body parts, which can interact with the world. It's the same principle: you have these structured reference frames that model the objects, and there's some behavior associated with them. These would be representations that could be recruited by other learning modules. That's at the lower, more tangible end of what you might call a spectrum of self. In terms of theory of mind and modeling one's own mind, it's not something we've discussed much recently, but it makes sense that, depending on the use case of a system, that obviously becomes more important. That's what enables primates to be such social, intelligent creatures—they model their own minds and those of others and have that kind of theory of mind. So I guess it depends on the use case.

Excellent. One final question for the group: as the software is open source, do you worry about the future of this technology falling into the wrong hands or people using this technology for harmful purposes? How do you think about the implications of bringing this technology to the world?

Did you want to take that one, Niels, what you wrote down earlier? Or go ahead. I can also start off. I think it will actually make it safer, putting it into the atmosphere so that anyone can look at it, see what it does, how it works, and help make the technology safer, versus it just being proprietary technology that a large corporation owns. Also, this is still an early research project, so it's not like this is ready to go—robots aren't going to start using this tomorrow. There's still time to figure out all these safety implications, and we are hoping that people from the community will also help test the system in terms of safety and issues.

There are many reasons to believe, as Viviane mentioned, that it will take time before any system like this reaches a significant level of capability. There are issues with arguments about an intelligence explosion happening overnight, where someone turns a machine on and suddenly the system has reprogrammed itself to another level. For example, a very intelligent system can generate many ideas about the world, but to actually gain new information, it needs to test those ideas. This is a problem in science today—there are hundreds or thousands of theories, but progress is often bottlenecked by our ability to collect new information. Even if a system has some embodiment, it cannot simply infer every possible prediction about the universe and become superintelligent. The time it would take for intelligence to reach anything near that is likely to be slow. We will benefit from more people having eyes on this technology and contributing to it, so we can make it safe, understandable, and interpretable. As Viviane said, everyone should benefit from it, not just one large corporation.

I'd like to add a couple of thoughts. It's a complex question, and Niels has already broken it down. There are everyday problems with people misusing technology, and then there are existential risks, which I don't believe exist. If you're interested in this topic, I wrote extensively about it in my most recent book, "A Thousand Brains." It's a topic I care about: the future of humanity, what we should strive to achieve as a species, and how this technology can help us reach those goals. It's a large topic, and I won't review it here, but if you want my thoughts, you can read the second and third sections of my book.

When I first mentioned this open source project, it was at a talk I gave at Stanford in June, before we officially announced it. Several people approached me about this issue and asked if they could contribute to the open source project, not just from a technology perspective but also from a discussion perspective. I said that would be great. I think it would be valuable to have that kind of discussion. You don't have to be a technologist to participate. I don't know if Will has made accommodations for that yet, but we should have a place in our forums for people to discuss these topics. I would be interested in participating as well.

I think that's a great idea. We should add that and start those conversations now. It would invite more people to join the discussion. Great idea.

Okay, I'm going to wrap up. Thank you to Jeff, Niels, and Viviane for the fantastic presentations and your time today. A big thank you to the audience—over 300 people attended this presentation. We were impressed by the number of questions and the thorough reading you did beforehand to ask such good questions. We hope to address more of them in a future video. With that, I wish you a good day and a good week. I'll leave you with a Thousand Brains QR code if you want to sign up for more information. Thank you, everyone, for attending.