This is the ultrasound device we've been using, which we got online from Butterfly Ultrasound. Here it is in my hand for scale, and on the right is an actual ultrasound image at the top. You can see part of the probe, and below is a cross section of a French's mustard bottle. This serves as a primer for what ultrasounds look like.

Ultrasound has many applications, with medical use being one of the main ones. It's an inherently sensorimotor way for clinicians to understand what they're seeing, and it's challenging to use AI for this because medical fields generally lack large datasets. There are many reasons Monty is well-suited for this use case, but ultrasound is not a very intuitive modality, especially for beginners. For example, this is what the Thousand Brains Project mug looks like in ultrasound, or at least part of it. The probe produces a two-dimensional beam like a fan, which moves in three-dimensional space through a structure. The probe emits ultrasonic sound waves, and the time it takes for the waves to bounce back indicates distance, while the intensity shows how white the image is. Bone, metal, or ceramic tend to appear very white, while soft tissue appears more gray, and water generally appears black. This provides a lot of information, but it's challenging to interpret. For example, the TBP mug image shows a 2D plane through the side of the mug and handle. The top of the cross section of the handle is visible, and the top of the side of the mug is also shown, but not everything is visible due to acoustic shadows, where sound cannot propagate and the area behind appears black. There are also many artifacts caused by delayed or reflected sound waves, which can make the image appear to show objects that aren't there. These were some of the challenges we faced when interpreting these images.

Setting up the system involved many moving parts. The architecture diagram shows the ultrasound device on the left, with an HTC Vive location controller attached using Velcro. When you click the button on the probe, it captures the current view and sends it to Monty with metadata about depth and scale. Simultaneously, the HTC Vive controller streams its location to two sensors on the floor, which then send data to the HTC Vive service, storing the probe's orientation and three-dimensional location. Monty processes the image, requests data from the Vive service, and visualizes it so we can see what we're looking at. It also suggests where to move next for the best inference, and visualizes how Monty is learning in real time. You'll see all of this in the demo.

To give a sense of the setup, we used a phantom, which is an artificial structure for testing or learning ultrasound. Since we were working with objects Monty learned in simulation, like YCB objects, we created a setup with a plastic bag filled with water, resembling an amniotic sac with objects suspended inside. The bag was placed on tripods so we could move around it, and we applied ultrasound gel to the surface for consistent signal propagation. The task was to determine what object was inside the bag. As Will mentioned, we tracked the probe's position using base stations designed for virtual reality gaming, which are highly engineered for accuracy. The tracker looks for invisible lasers from the two base stations to determine its position and orientation, providing good sensitivity for this task.

After obtaining ultrasound images and tracking data, we needed to extract more information from the image. The processing pipeline starts with a customized environment class from Monty that synchronizes both types of information. In the data loader, we examine the full image and find a small patch on the object's surface by starting in the middle and moving downward until a significant edge is detected, then extracting a patch around it. From that patch, we detect point normal and curvature, combine this information with the distance from the top to the patch and the tracker position, and send a CMP message to the learning module.

In the demo, you'll see the iPhone and iPad app. The image is displayed here, along with the depth we're scanning at, and the gain controls how white the image appears. The number of images captured so far is shown; it starts at minus two because the first two don't count, as we discovered. There's a manual capture button. We're set to the default musculoskeletal type, which we found best for identifying the hard objects in the demo. That's the iPad app.

A bit more detail on extracting the patch: there were several difficulties, such as acoustic shadows and artifacts causing trouble. Sometimes the highest peak in edge detection isn't the object; in this case, we want the patch here, not here, because that's an artifact. We had to adjust settings and algorithms to get the correct surface patch. Next, we identify points on the surface and fit a circle through them, which tells us about curvature at that point. We also estimate a point normal at the center, which indicates the pose of the patch. This required further adjustments due to artifacts causing blue dots to appear in incorrect locations. Sometimes we limited the range for fitting the circle because surfaces like the brain are very bent. Eventually, it works reasonably well. We combine the depth, sensorimotor data, and agent location. Looking only at the agent location from the HTC Vive, we get the shape of the bag. After adding the depth from the patch and the offset between the probe and tracker, we get something shaped like a mug. If you've seen it many times, you recognize the mug, but for a first-time viewer, it might not be obvious. It's not a tumor, but it could be many things besides a mug. Visualizing it in 3D, you can rotate it; this is the body of the mug, and down here is the handle. It's hard to get a good screenshot.

It does look like a mug. We added more live visualization to run during experiments so we can see what's happening. We have the input image and the extracted patch, showing how we fitted a circle and the point normal. This gets combined into locations and orientations in a common reference frame, relative to the world. Over time, as we move the probes, each blue dot represents Neil moving the probe to a new location and taking another picture. Over time, we accumulate points that update our hypothesis space, and eventually, we have a most likely hypothesis and a few other possible objects. Eventually, we recognize the object, and in this case, we even recognized some symmetry in the potted meat can. Another feature is showing the user, the operator, where the probe is relative to the bag suspending the objects. We want to show this because, as you'll see, we also want to direct the user where to move the probe. In the future, if Monty is used, someone without extensive sonography training could follow guidance to move the probe and acquire high-quality images. The first step is showing where the probe is now, which is what you're seeing: the bag as a cereal box-shaped white rectangle and the probe as a blue rectangle. As the probe moves, its position in the visualization updates.

In addition, Monty can occasionally reach a state where it thinks moving to a certain location could reduce uncertainty about what it's seeing. This is called a goal state. The visualization shows a new blue arrow indicating where the user should move the probe. The arrow remains until the user moves the probe there. It's up to the user to decide whether to acquire an image at that location; Monty doesn't force it, but provides guidance about where an interesting view might be to clarify what's being observed.

Issues encountered: as Neil showed, working with ultrasound data is challenging. When Will and I first saw the images, we weren't optimistic about the week, but with Neil's expertise, we got something working. Coordinate transforms are, as usual, a huge pain to figure out. Tristan and Rami mentioned that good visualizations help, but it still took a lot of frustration and time. Data streaming was tedious; in the Airbnb, IP addresses would randomly change, internet would drop, and devices would go to sleep. Trackers kept falling asleep or batteries died. Communication between the four components was shaky. We tried more AI-assisted coding, which sometimes helped with hacky solutions but sometimes didn't. It tried to import a module called Monty Python and hallucinated some funny things. We couldn't resist the temptation to eat; some of the dataset tried to justify why it doesn't matter if the hot sauce is filled or not. If it's open, it doesn't affect the ultrasound image.

We also experienced a magnitude 6.4, 6.2 earthquake at 6:00 AM that woke us up. For a couple of hours, we debated whether to go into the mountains in case of a tsunami. Luckily, there wasn't one.

and some family emergencies, like kids falling down the stairs. One of them has a black eye now.

There are many interesting things to explore in the future, especially in terms of reliably extracting features from these images. As Viviane mentioned, if the system picks up on an artifact and mistakes it for the edge of the object, the representation becomes inaccurate. There are various ways to improve this. Currently, we only look at ultrasound as edges in 3D space, which is important for human understanding, but it's only part of the picture. Texture is also crucial, whether it's a lung, gallbladder, or any other object, and that's not accounted for in our scans. Curvature was measured but not fully integrated in a way that allows comparison to simulated or learned models. We have ideas for making the maximum curvature dimension more robust. On the research side, Rami is working on resetting and re-anchoring hypotheses, which would help improve system robustness. We've collected many images along with positioning information and saved them, which could form a new dataset. As Monty improves, we can rerun it on this real-world dataset and observe its progress.

There was a tuna can in the dataset, which was one of the objects we practiced on. You can't see it on the screen, but the most likely hypothesis is the tuna can, which is what's in the bag. On the iPad screen, you see the depth and the ultrasound image. Niels is holding the probe against the bag, and the data streams to both the Windows laptop and the Mac, where Monty runs. The setup includes a bucket to collect water potentially leaking from a urinary catheter bag.

Monty runs on the Mac. The Windows machine is only used for OpenVR, which tracks the marker. OpenVR doesn't support Mac and barely supports Linux, especially on a headless Raspberry Pi without a GPU.

Regarding Monty's capabilities, it uses a shape-based approach to recognizing objects, not just low-level features. In ultrasound data, it's about moving a sensorimotor in space and getting locations relative to each other, which Monty is designed for. The gold state generator Neil showed indicates where to move the probe next. Monty's Intelligent Action Policy uses internal models to suggest the most informative view for object recognition, rather than a random or strictly methodical scan as current ultrasound AI solutions do. It's methodical about acquiring necessary information.

In terms of long-term applications, while it's not ready for use now, improvements to Monty could make it a valuable tool for bringing ultrasound to places where medical imaging is difficult. Ultrasound is portable, connects to smart devices, and is generally safe with no ionizing radiation. The main hurdle is the need for extensive training to use and interpret it. Current deep learning systems require huge amounts of data and lack the sensorimotor or spatial understanding to suggest data acquisition as we've demonstrated. This approach could bring the benefits of ultrasound to many parts of the world that currently lack access. The cost of an ultrasound device is about a thousand dollars, while training a doctor costs around $200,000 per year. Ultrasound devices are becoming cheaper and easier to distribute. Neil highlighted the lack of training data, especially for abnormal cases. Medical data is scarce, and scans of rare conditions like tumors are even rarer. A system that can identify a healthy organ and distinguish unhealthy ones without additional training data would be extremely beneficial.

Does that answer your questions? Yes, it does. All right, it's time for demos.

Should we start and go backwards in the water? Sure. Since we're already on mute, maybe we can play the Benny Hill music. I'll show the setup while Will and Niels get everything ready. Here's our Windows laptop that communicates with the Vive trackers. The trackers are set up down hereâ€”one is visible, and another is over there. This is just wrapping the tracker to the ultrasound device.

This indicates it's on and connected to the iPad over here. The setup was complex enough that we created a surgical checklist to ensure everything was running properly. We have Windows running the server, providing location information. While we're setting up, Terry, do you have an object you'd like us to test on? What are my choices? We have the TBP mug, a tuna can, a mustard bottle, a spam can, a package of ramen noodles, hot sauce, a heart, and the little menta brain stress ball.

I think those are the objects. These are all in your datasets, correct? Yes, these are the ones you can use. It would help if you picked an object that doesn't float. The heart probably floats, the brain probably floats, and the mustard floats a bit. We can try it. What doesn't float? The spam can, the mug, the tuna.

We also have some Issa sauce.

Let's go with the tomato soup can. We haven't tested on that before. The tomato soup can doesn't have a distinguishing feature and looks like the cup. Let's go with something else. The cup has the advantage of a handle. How does the hot sauce float? The hot sauce might be a good one since it has a different shape.

How empty is the hot sauce? You may need to hold it, but I think that's a good choice. Let me fill it with water. Neil said he wanted to travel without hot sauce and ended up eating our dataset.

Not much control there, Neil. I was on holiday for two weeks in America, so it was a long time without hot sauce. America has hot sauce everywhere. When he told us it doesn't matter for ultrasound, it was like an addict trying to convince us. You still have the visualization. As things are shaping up, you can see the setup. While you get set up, it seems all three demo projects rely heavily on networking. That's a robotics-inherent problem. It limits your ability to go to different places if you don't have a good network because of all the data coming in.

If you were to design a full-fledged product solution, you could also use cables to connect everything. This is more of a quick, hacky version.

We're going to start inference. Are you sharing your screen? Yes. Did you calibrate the tracker? Yes. Right now, you can see the probe. Sometimes it gets laggy on Zoom because there are too many devices running, but you can see the probe moving. If you switch over to the other view, you see the full ultrasound image on the top left, next to it the extracted patch and features, and to the side, the relative locations in the world it estimated.

On the bottom row, you can see Monty's current hypothesis of what it thinks it is sensing. Almost all objects are still possible, but the most likely hypothesis right now is Monty's heart. What did we actually put in? Change that to hot sauce. Hot sauce, okay.

In our experiment setup, we have 30 steps to try and get it correct. Do you have a gold state? Not yet. The ultrasound needs a lot of jelly and fluid to work properly; any air and it doesn't work. We're not completely sure all the coordinate transforms are correct. There might be room for improvement.

Can you use Vaseline?

Never mind.

What top hypothesis do we have at the moment? That's surprising.

It's probably the case that the coordinate transforms aren't totally right. This should be doable for Monty. We just need a few more observations. Did we show the goal state? If you go back, see one. Seems like the tracker probe is not calibrated correctly. For what it's worth, that's just for the human; it doesn't affect data collection. There is a suggested pose up here. If you rotate, that was before calibrating, so maybe it's really off. But you can see the principles operating and the coordinate transforms that still need debugging. How many steps are we on now? We'll try to be quicker.

On the ultrasound image at the top, you see zebra stripes. That's one of the artifacts that comes in. Are those reflections or something? There we go. We finished. It's bouncing back and forth, creating a false image at a certain depth. What did it categorize?

Monty's heart. That's a shame. Next, we can try multiple times. We can try another object, but maybe you should demo yours first.

If you also get it wrong, we can have a tiebreaker.