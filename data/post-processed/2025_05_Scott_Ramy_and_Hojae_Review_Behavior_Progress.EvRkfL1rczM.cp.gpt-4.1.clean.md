Okay. So this was an interesting and actually pretty challenging review. I may have spent a little too much time on the early material, wanting to trace the evolution of some of the ideas. Hopefully I didn't oversample on the early topics and neglect the rest. Some of the main themes I wanted to touch on were about where behavior models are and how they operate. Especially in the earlier days, we talked about them being side by side in the same column at every tier. At some point, we pivoted to thinking about them more in the parent column. I'm curious about the biology of that and what implications it might have for the idea of columns having slightly different character in lower versus higher sensory areas.

Do you want to comment as we go? Or should I continue? I don't think every column can have behavioral models. It's just that the behavior model we ended up with didn't act on objects in its same module; it acted on child objects. In the beginning, we thought a learning module could have both a behavioral model and a morphology model, and they could interact, which seemed natural. But then we ended up thinking the behavioral model acts more on the child objects. So behavior is just describing how child objects move relative to the parent, but that doesn't mean there are different learning models. They all have behavioral models.

So the question is, what are behavior models doing at the lowest level? At the lowest level, you could still have a behavior model that can recognize behaviors, but you wouldn't be able to apply the behavior to a child object because there's no lower level. Still, it should be possible to learn about changing things in the world and recognize them again later. It's a little weird to think about what is done with that knowledge.

I think I've said this before: the learning modules at the bottom of the hierarchy are the exception because they're an endpoint. There's nothing below them, so things are a little different there.

I haven't been too worried about it, but it's a good point.

Vivian, what do you think? Other than recognizing behavior, would it do anything?

I think recognizing it is already very useful because it can then vote with other columns that might be at a higher level. Unless we want to add another mechanism, maybe similar to what I was proposing with the balloon last time, where it's possible to apply a behavior within a column. There's another idea too: we haven't really talked about how columns—or learning modules—generate behaviors from these models. Even a primary sensory column or learning module could be generating behaviors based on its behavioral model. We just don't have a theory of that yet, but it's a possibility. A third purpose could be to assign behaviors to locations on a parent object, like knowing that at some location there is the hinge behavior, so the behavior ID becomes a feature on the parent object model.

In summary, Scott, there are a bunch of questions about this, but they're not fundamentally at odds with the overall theory. They're just questions we haven't resolved yet. Would that be a good summary?

Some of these bullet points are just interesting things that came up along the way, which we can get into in more detail. I mentioned pseudo parent objects—Jeff's example of the logo, the magical logo that can move around whether you put it on a fridge or a mug. It's an interesting thing to think about. There was some discussion about model sharing and similar topics. I have one example to talk about. I thought the pseudo parent idea was resolved in my mind; I didn't see any open issues with it, but maybe you do. A lot of these things are just things in my mind that could use clarification. One interesting thing from the pseudo parent idea is that we can use the mechanism for modeling object behaviors to model general movements of objects in the world, like physics or how a ball moves through a room. 

Is it an actual three-layer model we're talking about, or is it not a pseudo parent but a real parent? Does it actually take two columns to have a logo and then a third column that also has the cup? In general, you could do everything with two. That's what we mean by compositional objects. You can have a composition structure that goes down many layers, but you can really only attend to two at the same time.

Maybe the same thing applies here. If I'm looking at an object like a cup with the magic logo that spins around, I find it easier to remember with a little square around it, so you can see what the apparent object to that spinning logo is. Then you'd say, "Oh, there's that square. The square has this moving behavior."

Maybe the same thing is happening even if the square isn't there. That's a good question.

It just occurred to me that the human body is like that magic logo. We have all these parts, but every part can move.

We're not very rigid; we're not really rigid things. Maybe even the human body fits into the same category: there's a parent object called the body, but all the components can move in different ways, so there's nothing static about it. Just a thought that popped into my head.

The next major point is how the behavior model is used to update object models or make predictions on object models, which we've discussed a lot lately. As a reminder, object and behavior models can be co-aligned if we're learning the objects and the behavior at the same time, but we want them to be sufficiently decoupled so that if a behavior is observed on a new object, it may occur at a different orientation, and so on. I find the connection between learning and inference interesting and am curious how that's going to work exactly, but that's just a comment.

The last point here is treating the application of behavior models like sensor movements. I'm underlining it because it didn't really sink in until doing this review. It's pretty fundamental, and it just clicked for me. There are some slides here we don't need to look at, but I included them in case nobody else did.

Does anybody want to look at this? This is the style of this presentation: here's some material, and we don't have to look at it if you don't want to, but it's available.

First of all, I didn't make any slides; I took everything from everyone else. I think Vivian made this one.

We can come back to this one. Let's talk through this figure to recap everything and make sure we're all on the same page. We have two columns that are hierarchically arranged. Sensors send static features into layer four, and changing features, like an edge moving across the retina, move into layer three.

We have coexisting reference frames for the object model—sometimes called a morphology model—and a behavior model. They are independent, not necessarily co-aligned, and each has its own set of rotations that they ask the hypothalamus to apply.

A timing signal comes in from the matrix cells into layer one, which we need for the behavior model. On the next layer up, in addition to relays through high-order thalamic nuclei, we have direct connections that map feature changes into the object model. As I understand it, a changing feature can be a feature into the object model, at least as we've discussed it for higher-order models. But this is an earlier version, and I'm confused about where the projection from region two to region one is. I need a refresher on how the behavioral model communicates to column one. This figure was made before we came up with the idea of applying that connection. I also wouldn't know which anatomical connections we would propose for that, so I wouldn't know how to draw that arrow. But there needs to be a connection—specifically, a connection between the movement stored in layer three of region two down to the reference frame of the morphology model in region one. The big idea is that the movement command has to be sent back down, and that's not shown in this figure. I made that figure about a month ago, before we had that idea, so we're looking at a less optimal figure here.

This is old. Going through these videos from the last couple of months, it feels like we're definitely going to need a "legend of object behavior version," like the presentation Vivian gave with Monty going around the map and discovering new things. There were a lot of twists—legend of Monty, the legend. We're going to need a legend of object behavior version.

To recap, we're taking the compositional object framework and repurposing it with the inclusion of behavior models. This is a relatively new review of the compositional idea, where locations on the cup are associated with locations on the logo, and so forth.

This is older, so this is where we have a behavior model essentially communicating with the system. We've abandoned this approach, right? We're past this. You're confusing us, Scott, by bringing back old concepts. These are like our exes; we don't want to think about them. That's an analogy. It was hard for me to review—hang on, what was abandoned about the previous one? The idea that the behavioral model in the lower column informs the stapler morphology model in the upper column. That's not what I was showing there. It was more about assigning behavior to an object on a location-by-location basis. Different behaviors can exist at different locations on an object and at different orientations and scales. That's the same as having different child objects.

What's the scenario where we would have a behavioral model in the child object communicating with the parent object? For example, if the parent object is modeling the human body, the body has different behaviors at different locations. You would assign the hinge behavior or the ball-and-socket behavior to the joints, and a different behavior to the eyes, mouth, and head. Or if you have an object where—right, so in the right diagram, the behavioral model is not informing anything about behavior; it's just indicating there's a behavioral model at this location. Exactly. If we see the stapler, we can infer it should have that behavior again. I saw it as acting on this behavior here, but it's not for predicting morphology or anything like that. It's just about storing where on an object behavior exists and at what orientation. The figures may be a little confusing in that regard. It might be better to combine these figures. Essentially, you have an object like the stapler with fixed and changing morphology, behaviors, and features. Both are part of the composition of the stapler. They should be shown at the same time in a simpler picture.

Here, I was just trying to make that one point in the figure: both columns would have behavior and morphology models. This is what I mentioned earlier as the third reason to have a behavior model at the lowest column—so we can assign behavior to different locations on an object. With the traffic light, the change of color is at a specific location. Maybe in terms of a summary presentation, Scott, we're looking at snippets that make sense on their own but aren't the big picture, which is what trips me up. Viv, you said this is a particular point, but it's not the overall picture. I keep thinking we'll get an overall picture, but these are just components. Would that be a correct way of saying it? Yes. I didn't attempt to make a comprehensive review of where things are at this point. I just picked a couple of things I thought were interesting and wanted to know more about. I think this slide summarizes a good point: the first idea was to use the same mechanism for modeling object behaviors as for object morphology. We already have a mechanism for learning compositional objects. We assign child objects to a parent object on a location-by-location basis using hierarchy, and we can use the same mechanism to assign behaviors. Are we confident that this right side is actually happening? Is it necessary?

If there's a change—so observation from the sensor goes directly to the higher-level column—then that behavior, like the individual arrows, can be learned on a location-by-location basis at the higher level instead of the lower level, and then assigned to a location on the higher level. But I don't see why not; it could also be in the lower level. I just want to be sure if it's necessary in that particular diagram. Like Hojae said, if we have direct sensory input, you can probably do a lot without that, but in higher regions, you might not have direct sensory input. We already have access to these mechanisms, so I feel like this is just naturally happening. If we have an object behavior ID recognized in a lower-level column, that becomes a feature in the higher-level column. We're not adding any mechanism; we're just using the existing mechanism to deal with a new kind of feature detected at the lower level. But the behavior model in the lower column assumes something is changing relative to a parent, and it's not clear that's actually happening there.

There would be more just local flow detected in the sensor, like movement relative to the object. But to have a behavioral model requires that I have an object and am representing the changes to that object.

It's not just movement. If we had the logo on the left and the logo is rotating, as we said earlier, unless it's the magic rotating logo, which introduces another reference frame, that's not really a behavior of the logo. There would be no behavior model of spinning on the lower column. From the lower column's point of view, it's just a logo and its orientation is changing—not a behavior. The behavior model only exists on the right. With orientation, you can just change the orientation, but you would still learn the timing of it, right? No, I don't think so. I don't think you'd learn that on the left side. The left side is just saying, "Hey, I'm seeing a logo and it's moving." Either it's moving relative to the sensor, or it's moving because the right column is telling me to, acting like it's moving the sensor. This is the high-level review we should be discussing. To me, a behavior is always an object whose features are changing relative to the background reference frame. In this case, I don't see that there would be a hinge mechanism on the left.

You'd have to be recognizing the stapler to realize there's a hinge movement, and the stapler is only on the right. The stapler is complicated because the parent trials are not really known upfront. That's why I prefer the logo and the cup changing—it's easier to understand.

I'm still not convinced that the behavioral object would exist on the left side here. In this particular case, what about if we were in a second or third level in the hierarchy? In the second level, you definitely have a behavior model, right? Would you be able to assign that idea of the behavior to a location in the third level in the hierarchy?

Imagine we have the stapler with a behavior model, which is not shown here. That behavior model would be in the right column, representing how it was learned by observing the stapler. If the stapler is part of a larger object like the desk, would that hinge behavior carry over? I don't think so. You might be right, but I don't see why there's a need for that. It feels to me like there's a stapler and a child object that has some behavior. It could be the stapler, which we learned it with, or something else. I'd want to see evidence for why it's required. I don't see it. The way I think about it is a behavior model is always—like I said earlier in previous presentations—an object where one or more features are changing over time.

I'm always asking that question. If I see this behavior model in the left column, I ask, what is the parent object where that behavior is occurring? I didn't learn hinge unless I saw something on the left that was learned. I don't know what the parent object is there that the behavior is related to. It's not the top of the staple. The top of the staple itself is not exhibiting a behavior; it's just moving through space. As a morphology object, it's not changing. So, not to derail this review, I wrote two open questions for brainstorming: one, do we have behavior models at the lowest level, and second, do we need to assign behavior IDs to locations on a morphology model, or is that unnecessary? Does that capture it? I guess those are good questions. When I think about the stapler, if we assign behavior IDs to different parts and say that it's hinge-ness, who do I assign that to? I can open a stapler like this, but I can also move the bottom half. It depends on how you learned it.

Some of the mug examples I presented previously touched on this problem, where the logo could go up and down. There's no clear home position for it. If you learn the stapler with the staples on the desk and the top moves, then you would learn a model where the bottom is the fixed part and the top is the moving part. If, however, the general behavior was to pull something down from the bottom, you might learn a little trap door or something, and the top part is the fixed part. It doesn't really matter because if I took a traditional stapler and learned it, the top is the child object that's moving. In the future, if I hold it and pull the bottom away, that's like changing the orientation of the entire stapler. It would feel like I'm rotating a stapler, but if I just pull the top up, I don't feel like I'm rotating the stapler. The stapler isn't in a fixed position; it's just the top that's moving. In the other case, the whole stapler is rotating, and the top is stationary as it rotates. In one case, it's just where the reference name is anchored.

It's got to be anchored to something.

Does that make sense? Yes, it does. You can just mentally go through these exercises and see how it would feel if you learned it one way or the other.

Okay.

Let's see if anyone has anything else to say on this slide. I'm going to move to the next, more recent version. This is updated in the sense that here we're leaving out the overall point, which is what these last couple slides are underlining: the repurposing of the compositional object framework into a behavioral setting. This is from Jeff's slides—compositional model over here, composition plus behavior. Here, as opposed to the previous slides, we're really leaving off behavior, which is still a continuing topic for discussion.

I think we've pretty much covered this, but it's worth pointing out that the thing Vivian and I both came up with is that the way you make a prediction about a moving child component of a parent is that the behavioral model sends movement commands to the child object. The child object doesn't really know if it's moving through the world or if its sensor is moving relative to it. It's equivalent. That's how we make predictions about a child as it's moving. That was the key insight.

Over the last few weeks, this next figure emphasizes that point. Just as a reminder, we have a compositional object here. The logo is moving up and down, but in this example, the sensor is fixed. The sensor picks up on movement, which then passes into the behavioral model, into the next higher level. Using a sequence of sense movements, we can infer where we are in the behavior.

We can take the inverse of the movement stored in the behavior model. Here, this line can't see it, and we apply the inverse in the same way. As you said, we would apply movement commands to the sensor to navigate through the object model space. It's a nice idea and a nice animation.

That's just a recapitulation. I understood it, but it clicked a lot better as I was doing this review. It's a parsimonious concept of repurposing the motor commands to perform this task.

There was a two-part issue here. I'm skipping over the first and moving directly into deformations.

For this one, what's special about this balloon behavior is that it's non-rigid. If at some point the inflation stops and we've only kept our sensor in one spot, once you freeze it, it's very hard to make predictions about where the balloon locations are, because every location has changed over the entire surface of the balloon. A few solutions were proposed. First, we just handle it as if there were a million child objects.

Then there was an alternate solution that Vivian proposed, which is essentially a deformation of the object model space. I was thinking about this and whether we'd still have to store a lot of point-by-point data, almost as if we had a whole ton of child objects. Thinking about it more, what that gets you is generalizability. Even though you're storing a lot of point-by-point information, with the deformation you have the ability to repurpose it for other non-balloon objects. The key reason I went forward with this idea is that normally I would say it's unrealistic to store so many point-by-point associations and all the ways every location distorts, how the whole space distorts. But if it's possible to learn this on a common space for all objects and then apply it to any object, you basically only have to learn all of that once and then can generalize it to any object space. That would be the same if you're just trying to remember all the individual components.

There's another way: it wouldn't be specific to any features or components. It would just be the locations at which you expect features to change, but it's not related to any specific features. It's not related to the balloon at all; it's just that the location space distorts. 

How much do you want to keep this as a review session versus making it a brainstorming session, David?

I think it would be worth keeping it mostly review for now. Let's collect the topics we want to go into depth on and just note that I had a thought to help clarify this problem, but we can leave that for later.

I'll write it down and hopefully we'll have some time towards the end. I don't want to dig too deep into just this idea right now. The common space for all objects seems naturally located in a higher tier. If we're talking about a higher-level model with multiple child objects, what space is that exactly, if not a common one?

I have a mug next to a fork or something. Is that its own unique space?

You mean like a temporary scene that we learned? Would that be a space independent of "mug next to spoon"? It seems like we can't create a unique space for every scene; there must be some element of commonality to a composed scene at least. Scenes are a bit different from nested objects, but maybe the mechanism so far would say that if you recognize a new scene, your cells would just anchor in a unique way, and then you have a unique location space. We've also talked in the past about whether this mechanism works during object recognition, when you have no idea what object you're in yet—what space would you be moving through if you don't know which unique location station is there? 

In the grid cell literature, I want to make sure I'm not hallucinating about this. I believe that if an animal is put into an unfamiliar environment and there's no way for the animal to recognize the environment—it's a white box, no features, no walls—I think the grid cells form a sort of common, default configuration. There might be a space for things where there's no obvious way to mark the environment. If you had a spoon and a fork floating in space and nothing else to indicate location, or rats see a couple of features but nothing near each other, with no way of recognizing the environment, there might be a common, default grid cell anchoring. That's what I was thinking of. It just reminded me of what you were saying. You go and say, "I don't really know where I am, but I could at least learn to build up positions with a fork and the spoon," and then suddenly realize, "Oh, this is in this particular space." Then I'll create a new space and associate those. You can have a bunch of children without a parent, and then it just goes to some default parent space.

Is that enough about that?

I'm going to move to various other things I thought were interesting that got dropped along the way.

and, this being the stopping point for me, I want to briefly mention that Nils also had a proposed avenue for this kind of problem, which is a coarse rescaling. I'm thinking of it as parameterization, like scaling reparameterization. He brought up the idea that we may not actually make precise predictions about this kind of thing. I don't remember him talking about that, but the way you phrased it, I agree. This struck me as very close to a scaling problem. We can definitely scale, just like we can change orientation, as part of a movement. The more detail it deviates from pure scaling, the more points you have to learn. The balloon isn't purely a scaling issue, but as it deviates, you have to learn more points. You can imagine something changing where the balloon's expanding, but the local features—hundreds of them around the balloon—are all changing and spinning, which would take a lot of memory.

In this case, I would argue that there are many object deformations that aren't just changes in scale or rough changes. One thing I learned with the compositional research we did is that when the logo bent or something changed, you had to store more information. If the logo did not change in any way, then just a couple of points are sufficient to learn everything. The more unique the thing is, if you want to make predictions, you have to store points. There's no way around it. Imagine a Rubik's cube: when it's rotating, I look at it and all the faces look like a blur to me. I don't recognize those faces; it's just a mixed-up Rubik's cube. But someone really good at solving it recognizes those individual patterns as unique things and knows all the faces. I just haven't done that, so to me, I can't make predictions about what's going to happen because they're random. I could spend a lot of time learning it. Anyway, it's just an analogy.

I think I'm going to have to accelerate this because I've definitely exceeded my 15-minute expectation. I'm sorry, as usual. It's mostly my fault. It's weird that I didn't tell it to do a click-through sequence on the text, but that's fine. I figured out the behavior.

Another thing I thought was interesting: Nils brought up this idea of islands of agreement for neighboring LMs. To be able to tell the difference between a stapler that's opening and closing and a stapler that's just rotating. We've talked a lot about whether a single LM can do this, and even if it can, it would take forever to learn. This is about pooling information from LMs. I found this to be a nice idea: LMs down here, let's say the top is rotating, LMs up here would recognize the stapler at a given rotation, and LMs down here would also say stapler, same object ID, but different orientation. Maybe this is more related to the learning process. When you have two islands of agreement during the learning process, and the islands are distinct, then we learn about the object compositionally. That was brought up. I thought we agreed on the idea that when something's moving, like the top, there's an automatic mechanism for isolating and masking it. That's your second bullet point. That seems somewhat alternate to what you just said, so maybe islands of agreement and intentional masking are two proposals. It sounded like they were related. I'd like to dig into it more and talk about the relationship between the two. You weren't disagreeing in the video, but it seemed like there was some handoff between the two. I'm very confident in the intentional masking now, so I'm trying to understand how it relates.

This last point, which we already talked through, has to do with rescaling. That's an alternative to, or maybe not an alternative to, Vivian's graph distortion idea, but it places constraints on how detailed that distortion model has to be. Or, not constraints necessarily, but it has implications for efficiency. I think this last point—parameterize rather than location mediation—I don't think those are alternates. I think what you mean by parameter is like scale or orientation. As I said a moment ago, you could have one set of parameters that could, in theory, define the entire relationship between parent and child object. But when things get more complicated, you have to do it on a location basis. That's what we learned about the compositional model: it's always done by location, but you don't have to learn many points if parameterization works.

A rotated logo is easy to represent: at one point, it's at a certain angle, and I can path integrate to know what it will be at any other position. But if the logo bends in the metal, I need multiple locations where I learn the parameterizations. It's not one or the other; we use parameters for everything—scale, location, orientation. It's about granularity. The required granularity depends on how much detail you want to learn. For example, with a Rubik's Cube, if you don't care about the details, you won't notice them, but if you want to learn it thoroughly, you have to pay attention.

Another analogy: if you're familiar with math, it's like the feature change sensor module. If the parameters aren't changing, you don't need to add many points to your model. But if parameters like scale and orientation are changing, you need to add more dense points. Similarly, with the feature change sensor module, if it detects no change in features—like moving on a flat surface—it doesn't send many points to the learning module. If features change rapidly, it sends a lot of points. Neuroscience evidence shows that when things aren't changing, sensors don't send anything, so the entire thing is filled in.

I'm going to hop off because we only have 40 minutes left for the remaining speakers. I've been thinking about this problem lately and considered posting it on the research channel to get everyone's thoughts. This is like the inverse problem of a sensor moving around to sense something. For example, if someone traces a letter on your hand, each sensor is different, but it's the opposite of a sensor moving and collecting data—it's different sensors receiving input at different times, and somehow you can temporarily pool them to create a letter. I'm just throwing it out there because it's a good example. I don't think I can do that myself. Have you ever had someone write letters on your back? I don't think I could recognize them, even a letter. I tried, and even though I knew it was a "K," I couldn't tell. 

I've done this with my wife—drawing or writing on each other's backs. You can't do it extremely accurately, but you can recognize basic letters. On the hand, it's more accurate, probably because of the sensory resolution. This relates to the issue of shared learning. I propose that the hierarchy is how this is done. I don't think a single region with multiple columns could do this; it would require moving these data points into a higher-level representation in the cortex, which is also visual in some sense—visual or touch. We haven't resolved this, but it's a good example. I don't think a single region could do this. Through introspection, when someone draws on your back, you try to visualize it. It's not just about touch; you mentally visualize it. That's what happens when you learn an object through touch—if you reach your finger into a box and trace an object, you simultaneously visualize the 3D structure and see what it would look like. This shows that you're feeding this up into a region that has already integrated multiple columns, forming a representation that's not specific to the palm, hand, or back, but is more generic. We haven't dealt with that yet, but it's a good example to help us think.

A couple more thoughts on that, but I'll table it for now. It's the exact opposite of reaching a finger into a black box to learn its shape. It's not the same as tracing something with a finger and relating it to vision, because it's not one sensor—it's many. I'll stop there. It's a good problem, and our theories have to explain how it works. I think it will fall out nicely. It's another constraint on the hierarchy.

Rami, do you want to go next? You also have some thoughts.

I created some slides, which is a bit risky given the complexity of the subject.

I only have a color if you want me to go next. I'm sorry, I only have X color if you want me to go next.

I didn't address everything with my slides. After Scott's presentation, I realize there's a lot missing, maybe the important stuff. I'll try to go through them quickly. If we don't get to you today, Jja, we can pick it up at the beginning of brainstorming week. I'll try to go through this quickly because most of it is a recap of ideas we already know.

We already know about building a morphology model and some lower-level column movements and features. It's building a morphology model and pulling that into an object ID, coming from first-order thalamic relay cells. We expand this to a behavior model. The fancy behaviors—there's a whole lesson on how to do these fancy animations. It's easy in PowerPoint, but now we just have delta features, so that's the only difference. We have the same movements, but they're changing in a different reference frame—the location reference frame of the behavior model. We just have delta features now, so any changes go there, and we're also pulling that behavior model into a behavior ID.

I'm still at the same place as Scott and Vivian, thinking that there's a behavior model everywhere, even in the lower-level columns. That's how I'm going to keep presenting this, by the way. I agree with that too. I was questioning the projection of the behavior model to the parent. I think I still have that in one of the slides. I realize now that there's a discussion on it, but I don't want anyone to think that I'm moving away from the idea that all columns do everything. I think that all columns do everything.

Now let's talk about a higher-level column or high-level region column. In this presentation, I'm focusing on what is being fed into what in the feedforward pathway. I will talk briefly about the feedback, which I realize now is a big thing, but I'm not really drawing it here. What are the inputs to a higher-level morphology model? The first input we have is the larger receptive field features that are coming into layer four, which are just features coming from the higher-order thalamic relay cells. That's also helping us build a morphology model in the parent column. The other input is the object ID. Everything that is static can be associated in the parent column. The object ID is also coming from the lower region column. What that's helping us do is assign, for example, a TVP logo here or a TVP logo there. This is a cup, and it's okay to say that there's a model of the cup in the higher and lower regions, and we're just assigning it. This is a model of the cup. This is still a cup, and it's being assigned at this location. It's just saying this is an object ID of the cup here.

It's getting the object IDs. Another thing it's getting is the relative orientation. I've added a little tag here that says this is the global orientation of the object that we've learned or the behavior.

What is getting here as features is the relative orientation. We can say the logo—these orientations are with respect to the sensor. That's basically saying how oriented the logo is with respect to the sensor and how oriented the parent object is with respect to the sensor. We can calculate the relative orientation and then send it up to the features, and we can store it. We can store these at locations as well. The useful thing about this is that if we know the orientation of the parent object and we know the relative orientation that we stored at these locations, at any location, we can calculate what the child orientation will be because we already have stored the difference and we know what the parent is.

I know this is recap, but I'm just going through it again.

This is the point of confusion a little bit. I'm assuming that if we already have a behavior model in the lower level, we can access it or we can also associate different behaviors at different locations in the higher region. In addition to pulling the object IDs and the rest of orientations, we can also get the behavior ID and say at any of these points we have this behavior. If you think of a stapler, you can say everywhere there's a hinge except for that little deflection plate or the anvil. There's a slider behavior, or there's some other behavior. On the parent object, we can assign different child behaviors at these locations.

This was all just a morphology model, and it's getting all of the static features that we have, whether from the thalamus or from the lower-level region. Now let's talk about the behavior model. The behavior model is only getting changes in those features, so I'm going to go over these features again and how it fits the behavior model in the parent. Right now, this is only the behavior model. For simplicity, I've removed the morphology model. The top text box should say "behavior ID" in the second column. Sorry, this is a mistake—a typo. My attention was just on this, but yes, the behavior model is pulled into a behavior ID over here.

These are the delta features. All of the four things that were input before to the morphology model—now we're going to inspect what the changes and features in here would represent for those.

We still get the features coming from the thalamus, but these are going to be changes in large receptive field features. We're still getting the direct sensory input. I don't know how far this will go, especially to higher-level vision. This behavior is the one where the logo turns to the other—is that this behavior? Yes.

Any change—this is direct sensory input. Any change will be captured as a change in sensory features, direct sensory input. Here we would not store anything, and that's important. Nothing is getting stored into the behavior model because we're not seeing anything here, similar to the lower-level behavior model.

We're also getting a change in the object ID. As this logo changes, we can store something like "TBP to cup" or "TBP to Menta logo." There's no change here, so we're not storing anything. My thinking is that anything we store as a change is really useful in the feedback connection because we can query at any timestamp in the behavior model. We can say, okay, we're in the middle of the behavior. I can see that the change is going from TVP to cup, then I can feed that back into the lower level and bias what object ID I should be seeing now. It will go to layer one and through the apical dendrites connect to object ID in layer three and say, okay, you should be seeing now a cup or a Menta logo or something like that. At least it would bias it, and then it would consolidate with whatever input. The point I've tried to make in the last few weeks is that this type of behavior has a qualitative difference from the behaviors of things moving. There's nothing moving here; we're just changing an ID, and that's very different than if the logo moved or if the staple top moved.

This is really just learning ID swapping. I want to point out that it's quite different, so we have to learn it. I think the mechanism we have allows us to learn it, but it's misleading to think about initially because there isn't any movement information to be passed between the components, and it's very specific. I can't turn this behavior—learning that one logo turns into another—into a different object with different logos, because there's no way I could predict that a Ford logo turns into a Coca-Cola logo. There's no analogy there.

There's a qualitative difference we talked about before: if we look at object models, there's a difference between oriented features, like oriented edges or the orientation of the child object to the parent, versus specific features like the object ID of the child object. We talked about maybe the mini columns, and then the idea of the child object is specific activations in those mini columns. It's very different. The way we represent the color is different than the way we represent the orientation of the edge. That's a good analogy for what's going on here. Let's be very careful—this is, to me, the less interesting example, not very useful in terms of the creativity of a human or animal. I think it's a good, easier example to start with to explain the behavior models, since the first step in the mechanism is storing changes. Although, again, these changes are changes in ID, not changes in shape or movement. Most of what we want an intelligent agent to do to manipulate the world won't fall under this category.

The vast majority will involve things moving and actions. But this is probably still going on. To be able to model these kinds of behaviors, we have to explain this one, but if we focus on it, we'll miss the key parts. You would never have thought about sending movement vectors down to the child. There are no movements.

I think Ramy's getting there. It's a really nice presentation, by the way. I spent a lot of time thinking about this, and it took me a while to crystallize the differences here. This is a key difference, so we don't want to focus too much on this, but we can keep going. I don't want anyone to think this is typical behavior—it's a corner case and is qualitatively different from most. The way I think about it is that there is no movement we can pass into the location reference frame here, but there is a change we can pass into the object ID layer by what's happening.

I wish I had more slides to cover the movement part, but I'll try to talk about them when we get there.

The other thing is the changes in relative orientation. I'm not sure if this is an old idea now or if it's still valid, but basically, in addition, in the morphology model, we were getting the relative orientation between the child and the parent and storing that. Here, we're getting the change in the relative orientation. So, say we're at 50 degrees and we're going to 60 degrees, 70 degrees, whatever it is—it's a change in that relative orientation. This is not just a change in orientation. Remember the example where you could rotate it around one end of the logo? This is not just about the orientation changing; I have to learn this on a location basis. This is a change in location and orientation. It's tempting to think we're just changing the orientation of the logo, but no, you're changing its position and its orientation. The example I gave is proof of that. You can't isolate this as just orientation features; it's orientation and location at each location. In the morphology model, we're storing the relative orientation only, or we're also storing the relative orientation and the location. On the morphology model, you store the location and orientation at a location.

The features that are stored will be the relative orientation of the child to the parent, but that relative orientation is stored at a location in the reference frame of the parent. If you have this logo rotating, you have both a change in orientation and a change in the locations where that logo will exist. This is just an additional thing that will need to be learned. You can't just change orientation; that's not possible. The proof is, if I change the orientation based on a different point on the logo, I'd have different predictions. There isn't a way of just saying "change the orientation of the logo"—it's actually changing its location and orientation at multiple points.

But that doesn't invalidate the slide you're showing. Everything you're showing here is still valid. It doesn't invalidate it, but I know there's a tendency to think we're just changing orientation, and I'm trying to banish that thinking. I have to think more about this and get back with questions. That was part of the problem learning compositional objects—we thought you could just learn the orientation, but you can't. There is no home point, no anchoring point. I've said it enough already.

The idea here for the feedback is that if we're storing the change in relative orientation and we know what the current orientation is, the behavior model will be able to instruct the morphology model to know where to expect the morphology of the child object as it changes. That's still valid. The parent is going to send a movement command to the child, and the movement command can include changes in orientation and just displacement—non-orientation changes. Both of those are valid movements. There could be a rotation at some point, and it could be a change in orientation. When the movement command gets parsed by the child, it results in an updated orientation and location of the sensor.

Okay. In this case, what you said is correct: the parent column can send a change in orientation to the child, expecting the logo to be at a different orientation. At the same time, it also updates the location on the logo it expects to be on, because if you look at the little sensor patches, they change—not just the logo's orientation, but also where the patches are on the logo. It will also move you through the reference frame of the child object.

This would assume that all of these movements we store, whether for scale or for the logo moving up and down, are just sending the movement back to the reference frame—the location reference frame of the morphology model. We would just be inverting that, and by inverting it, we're basically staying within the reference frame, so nothing changes in the child reference frame because we've inverted the movement coming from the higher-level parent object. Scale is a bit more similar to orientation; it's more like a general transform applied to everything that goes in, to all of the movement vectors that go into the child column. Movement of the whole child object is then actual movement vectors being applied to that reference frame.

That's interesting, because in the brain I don't think it would be that way. That may be fine for Monty. If you think about the information coming from your retina—going back to the example of watching someone play a video game—if they're changing orientation, going forward, backward, turning left, turning right, there's one signal coming from the retina, and the cortex figures out how to update the location and orientation from that vector. In that case, I wouldn't expect the brain to send back a signal saying, "change your orientation" or "move in some direction." If it modeled what you're getting from the retina, it would just be a movement vector that the lower module has to interpret: "What does this movement model mean?" But we can do whatever we want in Monty; it doesn't have to be that way.

I still feel like it's a qualitative difference because the orientation applies to the entire reference frame of that child object. You apply the same orientation to all of the movement vectors that come in from the sensor. But in this case, we don't know that. If you were thinking about multiple fingers, the orientation of different fingers next to each other could be different. Those are separate columns, so each has just one orientation relative to the object. You could say, "Is it the change of orientation at this point?" because if you go to a different point, you might have a different orientation. You could say, "I'm applying it to the entire object," but really, you're applying it to this point and can interpolate that it's the same orientation everywhere, but it might change. The logo might have a curve or something, but that would have to be learned by hierarchy; you wouldn't be able to do that within one column.

All of this is learned in a hierarchy, right?

You were just talking about the child column, which is maybe at the lowest level, changing the orientation at which it thinks the object is relative to the body. We can do that reference frame transform, but you can only say that's true at this point. If you move to another point, you might get different instructions and say, "Yes, we could, without further information, change the orientation of the entire object." But often there is additional information, and the orientation, as with the logo that bends, can vary at different points. That's in the parent column, where we have different orientations at different locations. That's basically what's stored. So as a child, I can say, "At this location, I have this orientation." I can then predict what's going to happen elsewhere, but if I go elsewhere, I might be told that the orientation has changed. That's more what's in the mini columns in layer four. What I thought we were talking about is what layer six sends down to the thalamus to transform the movement vector into the object's reference.

Maybe there's something we should discuss offline. Maybe it's a misunderstanding, because I feel like—

I see what you're saying, Vivian. I get your point. I'll leave it at that. I would be very confused if we disagreed on that, because it's a very foundational point. I just want to make sure that everything's done point by point. If you don't get any additional data, you can interpolate, but everything's done point by point. You can't make an assumption about an entire object. For a child object, it's point by point, and you interpolate unless you get some additional data.

In the case of a level that is stretching like this, this will be solved with the mechanism of restoring the movement of the parent object in the behavior model. We'd be using that to change the—no, I don't think so. In your previous image, you were touching on the issue of scale. If you go back in all of our previous talks, we've avoided talking about scale. This is saying the scale of an object is changing at certain locations.

again, at certain locations, it doesn't have to be. We can look at it and say the scale of the entire object is changing in the wide direction, but it could be distinct scales. It changes, or it could become wobbly shaped. So we're changing scale on a location-to-location basis, just like we can change orientation on a location-by-location basis. I feel this is the distortion problem I'm trying to get at. How would we deal with those kinds of distortions? It's not just scale; it's scale in one specific direction. But if you think about it on a point-by-point basis, you can do that, but not in the lower column. If the logo stops at the stretch part and then you move, then you learn it. You learn it. I don't know. I think if I did that, I wouldn't learn it as a new word. I would just learn that's a logo that's stretched. It's a new one. This identifies an area that we haven't really addressed, which is the scale issue. I think this is a scale issue, and scale and orientation can both change point by point if we want them to, or they can change all at once. This is a good example. I don't think we've dealt with scale enough. The summit disagrees. I don't think we've talked about that.

The other change that may become a feature or a delta feature in the behavior model is a change in the behavior ID. We haven't really talked about this too much, I think. I couldn't think of an example, so I didn't put something here. But if a behavior ID—so a static behavior ID is fed into the morphology model—but if the behavior ID changes, I wonder if that gives us some sort of compositional behavior. I can't imagine what that is. I can't either. I think we talked about that before, that we can't think of a good example of a compositional behavior, so it might just not exist. But we may have the mechanism to do it in a compositional way like this or a hierarchy. I can't even think of anything that would make sense in this regard. That's because the behavior ID is usually over time, so you can't think of something changing at a specific location. No, the behavior ID is consistent over time. But what you're saying is, I have a staple—at one moment the hinge goes up, and another moment the hinge goes sideways or something like that. I don't know, like changes from one thing to something else, but I don't see how that would happen.

I'm still questioning this feedforward projection as you're showing it. I'm just trying to cover all the things that were static and going to the morphology model. Now what happens if we're getting changes of those into the behavior model? Maybe that answers it—we don't really have compositional behavior. We can't have those.

No, I'm not sure. Can you reach that conclusion? I think what you're changing is the behavior ID.

We said no, but that doesn't mean it isn't composition. It's not compositional behaviors; it could be just a—I'm just thinking about it. Every time I try to think of something, it ends up being a causal relationship. I turn on the light switch and the light turns on, but I don't know if these are compositional behaviors or just causal behaviors. I think we're getting sloppy with the language here.

Just to confuse myself, I started thinking of breaking off a logo into different behaviors and thinking through, why do you say those are different behaviors? That's one behavior.

Some of them are stretching, so they're moving. It doesn't matter. It's one behavior.

A behavior is defined as changes to the features of an object. Those changes can include everything like orientation, scale, rotation, location—it doesn't matter. This isn't multiple behaviors because these are all changing at the same time. But wouldn't you say, in this case, we would decompose the logo into multiple child objects that are basically the words of the logo, and then each word has its own behavior because they move independently? But they don't really move independently. They're moving relative to each other, but they're moving together at the same time. It's not like one word is floating around and the other word is floating in a different direction. The timing is synchronized, which would be easy to do with the timing signal, but it just seems like this is one behavior. This doesn't feel like there are four behaviors going on here. If we do this on a location basis, then every little feature is just creating one big behavior. It's orchestrated—all these changes of the features are changing together. We could define a behavior of a stapler: when you open the top, the deflection plate rotates. This is just a flavor of that. You've got two parts of the object that are changing, and they're changing independently—not independently, they're changing at the same time. They're tied together.

I don't think this is one behavior here. This would be one behavioral model. This is not three behavioral models. I don't think, "Oh, the word 'project' is going to move up and down, and then separately the word 'thousand' is going to move up and down." No, they're all changing together. We have to come up with—this is a single model. This feels very strong. It has to be a single model. I feel very tempted to break it off into separate objects because they are separate objects, but not separate models. It's one behavioral model. Even as you're describing it, you're breaking it into different parts. Just like the deflection plate—the deflection plate and the top of the stapler can be one behavioral model, even though they're separate child objects. All I'm doing here is turning, instead of having one child object, the stapler, I'm having four child objects.

Each one is all part of one behavioral model. It seems obvious to me: if these things all change together consistently, then it's one behavior. I don't view this as four behaviors.

Conceptually, when you look at it, you don't feel that way. You just feel like you know what's going to happen. I think we could build a behavior composed of all these smaller behaviors happening together, but I believe it can be done with one behavioral model. I don't think multiple models are needed. I'll have to work through the details, but you identify this as a single thing, not as multiple things. You break it apart, just like I break apart the top of the stapler from the bottom, or the top from the deflection plate. This is an intentional mechanism. You have to attend to all three or four of these components to learn this behavior, but it's really no different than the staple top and the deflection plate. It makes it more difficult to apply movement vectors to the child object to make predictions. If we decompose it into four child objects and those move, then we can apply the movement vector and expect to be in different locations on the words. But if the words are all moving in different directions, we can't do that anymore. We can't just apply the movement vector to the whole. 

This is a good example. Romy, you've come up with several good examples where we ask, how does our behavior model solve this problem? All I'm arguing is that this is not four behaviors. This is clearly, in my mind, one behavior. If these components moved independently, it would be four different behaviors. If the word "project" could move sometimes and other words didn't, and they independently did their own thing, then sure. But since they're all moving together, we group them as a single behavior. I agree that we would be learning a single behavior eventually, but it might be easier to break them off and assign simpler, primitive behaviors to these, then build a higher-level behavior. We'll have to see. Let's leave that. I don't think it feels that way at all. Maybe that ties into compositional behavior. I don't say, "Oh, look, the word 'project' is moving," or "Look, the word 'thousand' is moving." No, they're all moving at once. I see it immediately. There's no question in my mind—this is one thing after I've seen it repeat once or twice. That's one thing. 

So we have this as a question to tackle. I think this is a good extension of our behavioral model to see how we handle this. Clearly, we have to break out the components, just like we broke out the top of the stapler.

That just tells me that as part of a single behavioral model, I have to send different movement vectors to different child objects. The behavioral model has to manage that.

On my last slide, I'll just talk about the first point. I confused myself a bit making those slides. This is a question of whether, if we have a change of logo, is this considered a behavior ID in region one that would be assigned to a morphology model in region two, or is it just a change in the object ID, like from TP to NOA, in region one, and we'd store it as a behavior in the model of region two? I feel like it could be stored either in the morphology model as a behavior of the lower region, or in the behavior model of region two as a change in the object ID of region one. I strongly vote for number two: change in object ID. You're right. All these considerations of behavioral models in region one are misleading. I'm just stating my opinion. I'll make my point in court in two weeks or whatever. 

Sorry, OJ. That's my fault. Is it okay if I still present? Do you guys mind if I start? Feel free to drop out if you need to. I'll skip the review portion of mine since everybody has reviews now. 

Can you see the XRA? I think this is also in our group, like XRA on the brainstorming scenes or something like that. Everybody has done the review. The presentation Vivian gave was mid-April, less than a month ago. There's a huge connection to our current setup and assumptions. I thought this was really good. If Will could put the video here—if this is a video on YouTube, please click here to see it. It's really good. Of course, it's the research meeting I missed, so no wonder I was confused for so many weeks. It's not a problem. 

The other behavior movie I thought was really good was Jeff's. Sorry, I don't have a good screenshot, but the key takeaway was this section. There are other important words here, but getting this movement out from the behavior model was a novel idea that helps us predict what the sensor will see. Vivian had the same idea, but we used such different language that we didn't realize it was the same idea.

I think it's important to revisit what we need to solve at the beginning of the brainstorming session and update it as needed. From what I understand so far, the final open questions for me are about how we learn the behavior model and how to generalize the behavioral model to novel objects. We talked about associations, but even after reviewing the video, it's still not very tangible for me. Last week, Jeff presented a framework for slow learning and fast learning. Sorry, Jeff, I still don't fully understand what you meant. I know you were going to try to apply it to object behaviors, and I look forward to that, but so far, this was not about object behaviors—it was just about learning in general. Sorry, I didn't understand that. It is what it is.

I want to explain how I personally think about behavioral models. This is a very low-tech kind of behavioral model, rolled out in time. We have to learn some kind of change over time, so we're actually learning multiple things. For example, with the stapler opening, for a particular location, imagine there's a stapler—at this location, it's moving in a certain direction at the next time point. The same applies for t equals three. I think the stapler change opening is actually a little bit complicated.

The mug changing color is a little easier to understand. In the behavior model we have in the little square animations, I just labeled X and Y to indicate a change in physical space, like a particular location in a stapler moving at a certain velocity at a certain time, or a mug changing color. Imagine the mug is all green and changes from green to blue to red. At a particular location, say the handle, the color changes from red to yellow, then from yellow to green to blue.

In this case, the arrow or vector is not movement in the physical sense, but movement in color space. In my mind, this movement can be sent to the sensor: "Hey, there is movement in color space," so the sensor can still predict, for example, a yellow color instead of red. This is still "movement" for me, just not traditional physical movement.

We need to be careful using the word "movement" here, because movement implies you can do path integration, which is not happening in terms of location. There's no possibility of path integration here. I can't say that red always turns to yellow; there's no movement vector that says that. These are just random points—red became yellow, but it could have become green or turquoise. There's no movement here, just a change in time, and that would be the better language to use: changes in time.

I agree that these changes are tied to the location and orientation, perhaps, but not to the actual feature. This is not saying it's tied to the redness of the cup; it's just that whatever it is, it's changing by a certain amount over a certain amount of time. The reason I'm saying this is because it's relevant—we don't want to learn a change for every feature. I don't want it tied to a feature so we can generalize to a different object later. If we learn something that goes from red to yellow, that behavior of changing color should be applicable even if the starting color was pink or something else. But how could I apply one object's change from red to yellow to an object that's now green? How would I know what it's supposed to turn to? There's no way of generalizing; it's not a behavior that lends itself to that kind of generalization. I could say that another object's red might turn yellow. Maybe you think of a traffic light—you can apply that red, orange, green behavior. But if you see a traffic light with a blue light, you can't predict what the other lights will be. There's no path integration to color unless you're very familiar with the color wheel and color composition. You could say, "I just moved 30 degrees on the color wheel, so I'll apply 30 degrees to the new color," but that's pretty rare. By drawing this axis, I was implying I'm thinking about hue, like the color wheel stretched out. It's not a big detail for this section, so I'll move on.

In the behavioral model, we've always drawn in two dimensions because it's easiest, but there can be change in any of the features in any n-dimensional space. We're just drawing in two dimensions for simplicity, but I wanted to separate that this is not necessarily physical movement, but movement in any space of that feature.

For me, learning behavioral models seems like learning vector fields, except we don't want to learn every single point. Vector fields are usually dense, but I don't want to learn every single arrow in every location of that object.

If I fill in the above arrow example of the stapler opening, what's implied is that at this location, the staple is moving a little bit up. Same for that location. For that location, there's no movement. I think the way we can learn behavior without having to learn every single point is by interpolating. Maybe that's a simple answer, but I don't see how to do it otherwise. We wouldn't store anything if there is no change. In the case where you have dots, you just wouldn't store anything.

I was thinking about interpolation across time and across space. For example, if the stapler is opening quickly and we only observe behavior at time equals one, miss two, and then catch three, if we learn a behavior—these arrows at time equals one and three—then if we need to guess the morphology at time equals two, we can interpolate between those and get an approximate morphology model. I don't know exactly what the morphology will be at time two because it never stopped there, but if I had to make a prediction, I would interpolate and apply those movements to predict the morphology.

That's interpolation across time. For interpolation across space, I was thinking about the example Jeff gave last week, like squeezing a ball into an egg or oval. This is a case where all the vectors in the vector fields are different and difficult to learn because we either need to break it into millions of children, as Scott said, but again, I think we can interpolate. If we know we're squeezing on the top by a certain amount at one time point—squeezing both ways, but less left-right and more top-down—then even if I'm not observing other locations on the circle, I can assume by interpolation that there might be some behavior there if needed. I don't think we need to store these; we just need to calculate on the spot if we need to predict the morphology of a different patch or location for whatever reason.

When I think about it, and as Neil mentioned, I don't think we have an exact movement for every patch in our models, but we need to make predictions on the spot, and I think we do it approximately. Whatever mechanism we use for interpolating a morphology model, we'd probably use for the behavior model as well.

I'm not sure if this gets around learning every single point of the behavior model, but this was one idea I had. The difficulty is that depending on how we learn the behavioral model, it will significantly affect how we generalize it to other objects. We're learning a behavior on a location-by-location basis. If we learn this on a stapler, how do we apply it to a book? We need to transfer the locations of the stapler and map them to a book. I'm not exactly sure how to do that. Even if it's not tied to a feature, it's still tied to a location and orientation. I don't have an answer to that, and maybe we'll revisit it when we get to a better understanding.

Oh yeah, I didn't take 30 minutes. Yay.

Cool. Those were really nice graphics too. Open question: interpolation in time and how to avoid storing too many morphology models or key frames. I think interpolation is a likely solution, but we can talk more about that in the brainstorming week.

Thanks to all three of you for putting together those presentations. That was really nice—a good recap and also nice to hear these ideas in different words and framings. A list of questions came up today. Maybe I'll put them into a document and share them so you can add anything else that comes to mind, and hopefully we can answer them all in two weeks.

I made my own list too. Jeff, you mentioned you had some more ideas around distorting objects, but maybe we talk about that next time. I'm going to work on that. A lot of things we talked about today are interrelated, which makes it difficult to conceptualize everything. I was thinking about the role of the balloon in terms of changes in scale. There's a general theme I'm working on, which is interpolation and the range of data points needed to make correct predictions. We saw that with the logo on the cup: if it's just the logo, you don't have to store many points, but if the logo is bending, changing scale, or deforming, you have to store more points. There's no other way; it's just logic. That relates to what Hoja was talking about in the behavioral model over time and space.

I came up with a list: the role of hierarchy, the attention issue—how do we form representations at the top that are independent of modalities? The example of the logo where the parts move separately is worth thinking through. The idea of changing ID versus changing orientation, scale, and location—we have to make that very clear. Issues like the balloon and a circle becoming an oval need a clear theory.

Any of those things I'm going to work on. Today was good for getting us back in the mood for this. I may not see you—I'm going to be on vacation next week.

Offline is not to think about this stuff, right? I'll come back on Sunday, and I think our research meeting starts on Tuesday. I might be thinking about this over my vacation—sometimes I do, like waking up in the middle of the night with ideas. I usually have the best ideas when there's no pressure, like on the weekend. I hope you have a good time with the Hackathon. I hope it works out great.