Really exciting today. We've got two topics. First, Rami will present some of his recent work on a key stepping stone toward compositional objects, which should be interesting. Depending on how much time we have at the end, we'll discuss some changes we're considering in the code, reflecting conceptual updates about model-free and model-based policies coming together and how that's orchestrated in something like a subcortical structure. Scott will take the lead on that. Rami, do you want to start? If we have time at the end, I have some thoughts about how we might start tackling the broader picture of complex goal-oriented behaviors. That sounds relevant—just an idea about how to approach it.

I think we'll have a lot of time at the end. My presentation is a little short—famous last words. Presentations are never short, but it's an open discussion.

This is hypothesis resampling. I'll start with the motivation. We're starting to tackle compositional object recognition. Previously, we had one object in one episode and tried to figure out what that object was. Now, we're enabling compositional object recognition, and there are some changes we need to make in the code. One of those changes is figuring out how to deal with multiple objects in the same episode. I'll explain what I mean by that.

Can I ask a question, Rami? You say recognition—we have to learn compositional objects, infer compositional objects, and predict compositional objects, predict components. Is this about all of those, or just one? I'm not sure what compositional object recognition means; it seems broad.

This is mostly about inference. For example, inferring a cup with a logo is different from just a cup. In this case, the lower-level LM recognizes the logo, and the higher-level recognizes the cup with the logo at the same time. But this is more the motivation for what Rami's been working on. He's been working on the lower level, where you have the lower-level model, recognize individual child objects, and move your sensorimotor from one child object to another and switch your hypothesis.

That's right. This is basic object inference, improving object inference as a stepping stone to enable compositional objects. Right now, there are no results with compositional objects, but compositional objects would be impossible with how Monty currently is. It's worth going into this because with compositional object recognition, you have two learning modules or regions: one recognizes a mug, and one recognizes the logo. Both are standard, as we've done already. It's a matter of learning the composition, and if you say compositional object recognition, you're defining, for example, a mug with a logo, which is different from a mug with something else on it.

If you want to recognize a cup that has child objects on it, the lower-level LM will have to switch between objects. That's okay.

Even though we have co-located receptive fields, the higher-level LM tries to recognize one object, but the lower-level LM moves between objects to give different object IDs. The problem is that in the codebase, there are difficulties with some assumptions about how the Monty experiment is structured. I'll go through these assumptions.

In Monty, we have an experiment split into episodes. Every episode is a different object, and we assume a single object per episode. It's coded so that at the beginning of every episode, there is an explicit reset signal that resets everything, deleting all previous hypotheses and reinitializing a new hypothesis space. At the beginning of every episode, we use the first sensory observation to initialize a hypothesis space, which is then used for the rest of the episode. After the episode ends, we reset that hypothesis space. This has enabled us to parallelize episodes and do interesting things, but every episode is very independent.

There's too much reliance on the first observation in Monty because that's what we use to initialize the hypothesis base. After that, the hypothesis base is fixed, and we just update the evidence scores. If the object changes, or if the sensorimotor moves onto another object or the object is swapped, that first observation is no longer valid. This is the current problem, and with compositional objects, we want to be able to move between objects in the same episode.

The first thing we started with was defining a benchmark so we could measure our performance against it. The benchmark is simple: we define an experiment with multiple episodes but remove the reset signal in between. Whatever first observation we come up with is used for all episodes. We looked at the results this would give us. What I'm showing here is the maximum evidence score for the best hypothesis we have for recognition. At the top, I'm showing which episodes I'm giving to Monty. The first one is the mustard, then a strawberry, then a banana, and then the spam can or potted meat can. It does reasonably well on the strawberry because the hypothesis base we initialized was based on an observation of the strawberry, and it accumulated enough evidence to confidently recognize it. But as soon as we start giving it a different object and it tries to recognize using the invalid hypothesis, it cannot accumulate much evidence. All the hypotheses just decrement in evidence.

This is not good. This is what we expected to happen. It's not a surprise, but this benchmark shows that, as expected, Monty in its original version cannot handle this. That's what Robbie solved. Originally, we would have a reset signal here to initialize a new run.

This is a different view of what's happening. We're defining a different metric called the theoretical limit, which uses the ground truth. We know exactly how the object is oriented. We look at the best hypothesis we have based on pose error and define the limit of Monty: if Monty were to choose the best hypothesis, what would be the pose error? As it switches, the pose error jumps because it's using its old hypothesis, which cannot work well with the new objects.

Does that metric make sense to everyone? If you imagine when you initialize, you have a set of hypotheses. The nice thing about Monty is those are all informed by that observation. If I feel a surface here, one of my hypotheses is the mug like this, but also a mug on its side, and so on. That's the set of initialized hypotheses. All of those, valid or not, will have a distance to the ground truth rotation, which we can measure. Across all the hypotheses, we look for the one closest to the ground truth, even if it has no evidence. Even if Monty hasn't had a chance to explore, that hypothesis might not have much evidence, but if Monty picked it, it would be the best of all possible choices. That's why it's the theoretical limit: it's not guaranteed Monty will select it, but it exists in the space of possible hypotheses that could accumulate evidence.

The theoretical limit is the lowest one of all. At the start, it will be constant in these cases when we change the object. With the first object, we initialize our hypotheses based on the first observation, so the theoretical limit is good. Once we change to another object, the hypothesis space for that object was initialized based on a totally different observation, unrelated to the actual existence of the object. None of them fit the real existence of that object particularly well, so the theoretical image jumps up around step 50. Even if Monty did perfectly on the second object, it has no chance of getting the correct orientation because it's not in its hypothesis space.

Hypothesis resampling is the idea that we don't need to rely too much on the first hypothesis. At every step, we resample new hypotheses based on each new observation. We also don't want the hypothesis base to explode in size, so we delete some hypotheses that haven't accumulated evidence in a while.

The first step is to look at the new observation. We assume we already have a hypothesis base initialized, then look at the new observation and initialize a few more hypotheses, adding them to the hypothesis space. One detail is that we're prioritizing graph nodes that match the feature we're seeing. We're not sampling the full hypothesis base at every step, but sampling some and prioritizing those based on graph node features.

The next step is removing hypotheses from the hypothesis space. The naive way would be to delete hypotheses with low evidence scores, but that doesn't work well because after switching objects, some hypotheses will have high evidence for the previous object and will remain. The idea is to remove based on the slope of evidence accumulation rather than absolute evidence values. We track the slope and delete the ones we don't need, refining the hypothesis space over time rather than keeping it fixed and tied to the first object.

Just one note here. We also had an idea of resampling based on something called Offspring hypothesis, which is based on the evidence scores. At every step, we would look at which hypothesis had the highest evidence scores and then resample offspring hypotheses based on those. We would sample from the distribution of those high-evidence hypotheses, taking one with high evidence and sampling something close to it. The goal is to fine-tune or closely refine the hypothesis space so that it approaches a better theoretical limit. One of the inspirations is the particle filter approach often used in SLAM techniques, where you can start with a sparse sampling of the possible hypothesis space. Once a certain location or orientation becomes more likely, you can sample similar locations and orientations to refine your estimate and make a very exact prediction, even if you didn't densely sample all possibilities at the beginning.

One note about this is that it doesn't help much with switching from one object to another. It assumes we have the same object and are just refining the hypothesis space for that object. Because it's based on absolute evidence scores, it relies on what it thinks the object is to sample new hypotheses. When we switch from one object to another, the high evidence scores don't mean anything, so it doesn't help us switch between objects. It's not geared toward compositional objects, but it's useful because it can help us recognize objects faster, which might be needed for behavior. We might come back to it.

The whole idea is that instead of sampling a large hypothesis space at the beginning based on sensory information, we continue to refine the hypothesis space over time. At every step, we're doing this.

These are the results with resampling. This is the same figure: we're switching between objects, and as we switch, this is the maximum evidence score. We can see that we're able to switch from one object to another, and Monty is able to recognize these objects as we switch. For the first episode, we're expecting it to do well, accumulating evidence. This is where the improvement happens: for the second object, it now has a hypothesis to accumulate evidence for, and then for the third object as well. As we switch, we're able to switch Monty's recognition to the correct object.

For the theoretical limit, it also makes sense. Even though it increases as soon as we change the object, it continues to decrease as we resample more and more evidence scores and hypotheses. Monty is able to do well on these hypotheses because we now have hypotheses to accumulate evidence on, so we're doing well for all the objects.

This is the last slide. This is a comparison on the benchmarks we defined at the beginning, where we do not do the manual reset of Monty between episodes. We ran the benchmarks with no resampling and full resampling. One thing we're investigating now is using a smaller hypothesis space, which would make Monty faster. As you can see, when we use 70% of the hypothesis space, Monty can run faster because we're always refining that space, and it doesn't affect performance much. The performance is about the same. I was confused about this, but I see there's a third bar that's really low. I thought it would be the same, but it doesn't make sense otherwise. You might not have been at our normal Monday meeting when Rami first presented these results, but in a big understatement, he said this is a nice improvement. We basically go from 0% accuracy on this benchmark to close to 100%. It's a huge jump. This figure shows two key improvements: being able to switch between objects without any supervisory signal, and being able to have a much smaller hypothesis space size. Before, if it wasn't in the hypothesis space at the first step, it couldn't be recognized at any time during the experiment. Now, as we resample all the time, we don't have to have everything in our hypothesis space at the first step.

It seems we can make the hypothesis space much smaller without much impact on accuracy.

My question is, even though the effects are small here, could you speculate as to why for the distant agent the 70% size went down a little and for the surface agent it went up a little in terms of percent correct? I don't have any idea. There's also something with the runtime: the surface agent is running faster, which I'm also not sure why. Even without resampling, the surface agent is much faster, and this might already be in the original benchmarks. For the policy, it just converges faster because it explores the surface more efficiently, which means there's a proportional decrease in the number of steps. I'm pretty sure we've seen that before.

We didn't get the improvement in the surface agent's runtime that we saw in the distant agent. These are things I'm still looking at as I wrap up this project.

I'm going to go over an interactive visualization, which may make some things clearer.

This is a new visualization that Rami put together. Thank you.

Okay, this is my other computer, so you'll be seeing the side of my face.

Basically, this is the simulator showing the ground truth of what Monty sees. Just like with the previous figures, we have the object that Monty is seeing in the simulator on top. It will see a strawberry, then a banana, then a mug, and so on. This is the evidence of which object Monty thinks it's seeing over here, but in the simulator view, that's what's actually happening. The most likely hypothesis is plotted as the maximum evidence here. As we move between these, we are not seeing your cursor. If you can, turn on a laser pointer or be more descriptive in what you're pointing at.

Maybe just highlight—in the simulator, Monty is the red circle, and the direction it's looking is that line. There was a green dot on the MLH side on the hypothesized object, which is where Monty thinks it is. I missed that, but we'll get to some comments about attention in a moment. Is the green dot over there? You can see it just peeking around. That was not visible to me.

Sorry, I'm not able to have the cursor shown. If you just say "on the bottom X," then it'll be clear. The bottom plot here is the one we've seen before—this is Monty's evidence. At the top of that chart, we also see which objects Monty is talking about, resampling, and changing. I thought I was looking at a cup. This red blob is a strange-looking cup; it looks more like a strawberry, but I'm trying to make a cup out of it. It's a strawberry. Some strong priors on your behalf, because we were talking about cups the whole time, and I'm trying to make that thing a cup. At some point, I realize there's a green thing on top—it's a strawberry because it's small. You zoomed in on it, too. That's funny. I wasn't doing very well on my inference. We do have a cup in Scott's new dataset, so we can use cups in the future. The mental cup doesn't exist in the YCE dataset. That's fine. It's just funny. We have a mug over here. It doesn't matter. Pick the strawberry; it's great.

I was just showing what we're seeing: this is the agent, and this is what the agent's looking at. In the simulator view, the red sphere is the agent position, and the black line is what it's looking at. These are the patches. On the right view, in the most likely hypothesis, this is what Monty thinks the object is, and also where the agent or the sensorimotor is on the object, shown as a green sphere. The green dot represents the sensorimotor. Does that green dot represent the actual size of the sensorimotor patch? No, it's way too small for that. These experiments have noise, so sometimes you'll see on the right side that the patch location is not exactly on the surface. I'm just trying to understand how much of the object is being sampled by this learning or sensorimotor module.

You can look at the whole experiment from the beginning. On the simulator object, on the top left, you can see how many steps it looks at. I think Jeff's asking about the receptive field size. Maybe if I share my screen, I can show what it has been for other objects in the past. The green dot seems way too small for a sensorimotor patch. It's larger than that—maybe hard to describe. Ramy, if you stop sharing for a second. Thanks. This is actually something we show in the paper. For example, with the mug, you can see the rim and just underneath. I assume the red dot in that figure represents the size of the patch. I've always assumed that. Is that not true?

No. Go back to the image you just showed. The patch is still pretty small, but it's bigger than before. It's an important variable, the size of that patch. Obviously, the smaller it gets, the harder it is to learn and infer. You get more details, but it makes thinking about other things much more difficult. That will be interesting to experiment with now that we're bringing in compositionality and hierarchy. Then we can have a higher-level learning module with a larger receptive field.

Was that for a surface station or distant agent?  
Ramy, on your visualization—distant agent. Although what the sensorimotor module sends to the learning module is just a basic summary statistic: the center location of the patch and the color at the center. The main thing influenced by the size of the patch is what kind of curvature is being extracted from locations in the patch. It's still not saying anything about how things are arranged within the patch. Anything within that patch will get averaged out in terms of its curvature, so you don't want to be modeling every single dimple of the strawberry. It's not like you get a lot more information into the learning module if the patch gets smaller, but it's helpful for visualization to imagine.

It's confusing to look at and see that tiny thing. We're not really sampling that point; we're sampling an area around it. Otherwise, you would learn the curvature of every detail if it was just a point. You don't want that. It would be helpful to show that the green dot represents the area of the patch, or even in the simulator, since the patch only exists in reality on the left.

I don't think the size of the green dot varies. I thought it was just clipping into the mesh because there's noise in the locations it receives. If you take a hypothesis and move the sensorimotor over the object, the area you're sensing is an important variable, and it's missing here. That was confusing. It would be nice to visualize, and we did that in the paper figures, so I wasn't confused by it there.

Another question: you said it's the distant agent, so on the left where we're seeing a dot and a line, presumably the agent is further away than where that is for a distant agent. Is that just some fixed distance from where the observation was taken? I'm not sure if it's much further away.

Is it possible the surface is really close to your eye? Scott found a great video of people studying t-shirts. We should share it; it's the fever dream that Monty experiences.

Does this distance really matter at this point? It's not a practical distance for a vision system necessarily, but from a theory and implementation point, does it matter? It didn't seem like it would. I think the agent is probably a bit further away because the not-zoomed-in view gets a full image of the strawberry, which seems difficult at that distance. Since it's a 10x zoom-in view for the patch sent to the learning module, I used the agent position, which didn't seem to be displaced by much. Maybe the camera just has a wide field of view or something. I can dig deeper.

Ramy, I have a basic question. What we're doing here is the lower learning module deciding it's no longer on the mug and now on the logo or something like that. What about the upper learning module? It's sampling the same spaces, the same points. Why wouldn't it do the same thing?

Having the stacked LMS on top of each other, both looking at the same observations, is something we're still working on in terms of how to enable. It's an open question. That's something we wanted to talk more about, maybe at one of the next research meetings, because we're all a bit unclear on that right now. Besides giving the higher one a larger, lower-resolution receptive field, I have a lot of thoughts about this, so we don't have to wait. Maybe Rami can walk through this visualization first.

As we move, you can see the vertical line in the bottom figure moves as I change the step. As I move, and we're still looking at the strawberry, as long as the most likely hypothesis is the same orientation and object as the simulator, Monty has a good idea of what it's looking at. As soon as we switch the object to a banana, Monty takes some time to delete the wrong hypothesis and refine the space. After some time, it thinks maybe it's a different object—the Lego piece, then maybe a mug. Then it thinks it's a banana, but in a different orientation, and it starts looking at different parts of the banana until it figures out the correct orientation.

The same thing happens at the end with the mug. Let me just fix the orientation a little bit, and this is what it starts with. Sometimes it gets the mug, but then it thinks maybe it's an inverted mug. You can see that by looking at the opposite side. It thinks the sensorimotor is on the opposite side, which would make sense if the mug was inverted.

After it fixes that, we can see that it's now looking at the rim and it thinks it's on the rim.

After some time, we get the same results with all the objects. In these figures, Monty is not getting any resets between the objects. It's able to switch between them and quickly decay the incorrect hypothesis.

There is an interesting observation with the mustard container, where it thinks that it's 180 degrees rotated based on the observation it sees because it's looking at it from the side. It hasn't looked much at the texture, like the barcode and all the text. It's just looking at it from the side, so it thinks it's 180 degrees because it's symmetrical along that axis. I imagine if you looked at the hypothesis, the relative differences for the symmetric rotation would be quite similar. Even though this is technically the most likely hypothesis, they're probably quite close, and it would be very quick to change to the correct one once it looks at any of the texture. If it sees red here, it would quickly switch to the other one where it has different features.

You can see that it thinks it's a different orientation, but it's very close.

This is it. Very nice visualizations. Thanks, Jeff. It's beautiful to see how the rotation hypothesis quickly refines itself and how, even if it's wrong initially, it's wrong in meaningful ways along the axis of symmetry because it hasn't seen the disambiguating evidence yet. There were places where it figures out the right orientation once it looks at discriminative features like a handle or the rim of the cup. Sometimes it only takes one step, but it's also interesting to see that Monty figures out the correct goal state where it wants to be to basically disambiguate the orientation. It's interesting to see that it picks the right places to look at, like the handle of the cup or the rim. Overall, this was really nice work with the hypotheses sampling. Awesome. That could solve that new benchmark. I definitely agree that visualizations can be really useful, like the thing that Scott spotted. That's an interesting aspect of being really close.

I think it's going to be super useful for debugging. That might actually be partly from the jump to goal state, where we have this desired distance, and I think I was just using whatever the default parameter was for the distant agent. That might actually be closer than we want to be. We might want to smooth out the sensory observations we're getting. It's something to revisit, and I'm sure there will be other examples like that. I'll look into it just to make sure. It's also occurring to me that when you change the viewing distance, you change the size of how big that receptive field falls on the object. If the stored model assumes a certain distance, like how the patch size falls over the object, then all of a sudden you're much closer and seeing a bigger part of the object. The amount of curvature, how you estimate the curvature, could be different because you're averaging over a larger area. That's the beauty of Monty—it shouldn't be too sensitive to that. It might be unexpected, but in general, I think it's going to be very different from that same distributional shift. If the deep learning system saw that, where it's used to a zoomed-in view or vice versa, it would be surprising, but it hasn't made any noticeable difference thus far. After a goal state is achieved and you're really close, we don't see any noticeable drop in performance after that. It seems very robust. I think it would be better without it or a bit further back, but that's something to consider.

Rami, have you had a chance to look if this improves performance on the normal noisy experiments? We expected that maybe as a side effect because it removes the reliance on the first observation, which might also be noisy. I'm still looking into that. I ran some experiments on the default benchmarks and have some Weights & Biases results to show, but there are some things with the pose error where I've seen the pose error slightly increase on some of these experiments. I'm debugging it to make sure, because it doesn't make sense for it to be increasing. That's one area I'm looking into.

I have a bunch of thoughts. One, we're ready.

Could you bring up the mustard container again to share your screen? All right, just stop right there. That's good. Oops, it's going to adjust. All right, that's good. So I have two general thoughts.

I believe it feels very strongly to me that when we look at an object like this mustard container, which has a lot of details on it, there is an intentional event that occurs when we focus on a specific part of the object. It's not that V1 and V2 are just scanning over the object and V1 magically picks out logos and words while V2 picks out the whole object. Both V1 and V2 would be looking at the entire object, and then you would attend to a specific part of it. When you attend to that part, only V1 is focused on it, while V2 is still processing the whole object.

We have to keep in mind that in these systems, there are many columns looking at these objects. For V1 and V2, the entire object is covered by many columns, regardless of whether the columns represent the same size of receptive field. For example, when I look at this mustard container, I might recognize it as a mustard container without reading the words. If I just flash the image in front of you, you might say it's a mustard container, but if I ask what the words say, you might have no idea. However, if I put a rectangle around the word "Frenches" and tell you to focus on that area when I flash the image, you'll see the word "Frenches." The same input comes into the retina, but by telling you in advance to attend to a specific part, you perceive the word. The retina receives the same impression, and you don't even have to fixate on the rectangle for "Frenches." You could fixate anywhere and still see the word in one case and not in the other. This shows there is an intentional mechanism that allows you to break out a part of the image and infer what that part is.

When looking at a compositional object like a cup with a logo, at first you don't actually see the logo. You see that it's there, just as you see text on the mustard container, but you don't know what it is until you attend to it. There is a general morphology or model of the object, which includes colored areas and so on, but the actual details require attention.

For example, if I'm holding a coffee cup and say, "This is my cup," and then notice some text on it, I might realize I've never actually read what the text says. I have to attend to it, and then I see it says "Copco." I could have seen the cup a hundred times and even fixated on those spots, but only when I attend to it do I see the word. There is an intentional mechanism required for this, and that attention mechanism acts as a reset, allowing us to attend to a subset of the input space and expect to see something different. The work you've done is useful, but I think it's not sufficient to solve this problem.

When I look at a compositional object, I first look at the whole thing and then start attending to details—what's over here, what's over there—and each time I attend to a detail, I infer a child object completely.

Somehow, V2 is not attending to that subset; V2 still represents the entire object, but only V1 attends to the subset. That seems necessary to do this. We have many columns in V1, and some will be on the attended subset and some won't, so we have to determine which ones to pay attention to. If that's the main mechanism, it's like windowing the area of voting. Initially, this seems to be a model-free process. I don't know what the word says—maybe "Frenches," "yellow," or "40"—but I know there's something there, so I attend to it to find out.

These things aren't invisible without attention; they're present in the overall model of the mustard container, just without the details of the child object. Since I don't know the details, it can't be a model-based attentional mechanism, at least not initially. It could be that there's a border of red or blue, or some letters, and I choose to attend to that. This process happens constantly—every time we move our eyes, we're attending to different subsets of the world and assigning those subsets as children to a larger portion of the world.

Even V2 might be on the retina with multiple objects present, like a coffee cup and a mustard container. V2 might attend to the subset that's the mustard container, while V1 attends to the subset that's the red area.

When I attend even closer, I see it's a flag—I didn't notice that before. I've looked at it many times and didn't know it was a flag. When I focus more carefully, I see the flag, but if I'm attending to the word, I don't see the flag, and vice versa.

I think we need a mechanism like this. When we do this, we often infer the flag or the word using many columns. It doesn't require movement, or very little movement. I can quickly attend to the red area—oh, it says "frenches"—or attend to the words, "there's 40." I don't have to scan over it as I would with a single column. We need to think about multiple columns voting to infer the word "frenches," "40," "yellow," or the flag.

Those are the two things I think we need to add: an attentional window, which initially is model-free, and ensuring that multiple columns can vote on that. A real system is very quick. I recognize the logo in one flash because multiple columns voted on it. I know the word "frenches" because I've seen it before; multiple columns vote on it. We might reset based on confidence or prediction error. I've discussed this with Niels and Viviane—prediction error of what? If you move or attend to a different place and have a prediction of something there, and it's not there, you start resetting or sample more aggressively. That seems to recur in some situations.

I'll show you something funny. I'll be right back. Two seconds.

You say, in some sense, your slow tracker is a prediction. I thought you might enjoy this. Can you see this? You can see my mic here. Maybe I can see up here if you hold it higher. Here we go. I took this to my talk last week, and when I got home, I noticed something immediately jumped out at me.

I don't know if you can see that it's chipped. Whoa. It's a nice chip. It's almost like I'd like to have a cup like this because now I can talk—you see, it didn't break the whole cup, just made this perfect little chip. It's quite satisfying. Normally, it comes off the surface and it's ugly. This is more like a cut. You can see the inside. I actually have the piece and could glue it in, but I don't want to. Anyway, my point is that's a prediction error. If I had this cup all the time and used it every day, I'd take it out of my bag and not think anything about it. You get used to it; your model gets modified. But the first time I see it, it's a prediction error. This is what "On Intelligence" was all about. There is an issue, but I'm not sure. In some sense, this prediction error means there's a new feature on this mug I didn't see before. I think it's the morphology model that's messed up. I don't know. I think prediction error will be important in general, but for the potential mechanism, it's more about gating—which inputs we're actually giving to the learning module to infer the object. It's about really giving attention to the red parts of the flag or only the French lettering. We have to think about multiple columns and getting that gating right. We're basically saying there's an area—maybe of the retina or space in the world—where we need to attend and ignore everything else. What do you infer in that area? That ties into Scott's work, which is about explicit motor attention—only moving your eyes in a certain area, but having a model-free policy that keeps you on a child object until you recognize it. So, only circadian within the French.

It might be that I don't recognize it right away. Maybe it's a composite object, like the flag plus the word, that I haven't seen before. You have to spend some time in that area, exploring and building up models, and then you can move on.

It is nice to have the same sort of model-free mask, or whatever could inform voting, as much as it can inform the policy. We can still get it to work with a more sequential approach, but it would work even better with voting. Once I've really studied this French's mustard container, I'm not sure if the attentional mass would be model-based or not. Initially, it can't be. We were thinking that, as you said, initially it would be model-free, but model-based would probably be really useful for things like robustness to clutter. For example, I can see a chair behind that table, so you have a hypothesis of a chair, which gives you some sense of where it is in space. When I think more about that, it feels like attentional mechanisms are about attending to an area in space, not just an area on the retina. You're attending to a depth in space and an area that's exposed, and I don't know how that happens. I don't know how the brain does that, but that seems to be what it's doing. We could start with attention to an area of the retina or the array of central modules, but it seems like it's more than that in brains. Brains are able to say, "I'm attending to that table, which is about 10 feet away, and I have something right in front of my eyes that is partially blocking the table. I don't see it at all; I'm not attending to it." It's about paying no attention to things that are not 10 feet away. It's strange.

The issues you're bringing up have been a topic of intense debate in the RFD I'm working on. You've identified a big section of the kinds of issues we've been discussing, including whether we're attending in retinal space versus world space. It ties into another RFC really nicely. You're bringing up things we haven't talked about, like voting and using the same attention mechanism for voting, which would be easy to extend and conceptually appealing. We haven't really conceptualized it as attention so far; we've been talking about it as policies. Thinking of it as attention is more general, as we can apply it at high-level modules that might not influence the policy as much, and use it more broadly.

Given our distance sensorimotor system, which has distance built in, we could attend to volumes in space. That's what we were thinking of doing. It would make things easier. For example, combining what your model-free attention or sensory input tells you to look at with your model-based one is much easier if both are working in space rather than in retinal coordinates. Then your eye might detect a loud noise over there, and your model-based system says, "Oh, someone's operating machinery," and those match in the space they're referring to, without actually working in 3D space. If you're trying to do this in retinal or binaural auditory coordinates, it's much harder to get them to work together.

In biology, with eyes, it's trickier to attend to space. You have to use binocular cues, parallax, and other factors. But with the sensors we have, it could be much easier. Let's combine this together. Romy, this is great work. It's going to be useful all around for general inference, like deciding whether we're on a new object or not. To solve the composition object problem, we have to assume that two regions are attending to different things—one to a larger object, one to a smaller object—but both are multi-learning module arrays getting some sense of the same input. We don't have to assume that B2 is getting larger receptor fields in V1. That's not a requirement. Therefore, V1 has to attend to some volume or point in space that V2 is not attending to. With multi-columns, we can infer that sub-child object very quickly.

In the ultimate system, with Montys running around the world, they'll be just like us, building up these models as they go. The nice thing about thinking of it as attention is that we can apply a different mask to the lower-level learning module than to the higher-level module. If we want the lower one to infer the logo, we can gate it to only send observations within a certain area of space, while the higher-level one can get observations in a larger area. Now that I think about it, both V2 and V1 have to have their own potential spaces; they both have to be attending. Maybe that's a general principle that's always at play.

If I give you a blank screen and ask you to fixate on a point, then show you an image, you would know where to attend. I'm not sure what you would do—maybe you'd attend to everything—but very quickly, you might say V2 is attending to the mug and V1 is attending to the volume of the logo. Then V2 attends to the logo and V1 to the letter, that kind of thing.

A question about that: could the higher region be initiating the attention for the lower region? That would be a model-based attentional mechanism, right? We talked about that. I guess it could, but I don't think it can initially. That would be after we've learned the compositional object, almost like in the backward connections, or generally if we've learned a compositional object and know where the child object should exist. That information could be used to predict the child object and know which area to attend to. If we're looking for a child object somewhere, that's where the high region can initiate that attention. But we're also talking about model-free attention, which cannot be initiated from the high region.

I think that makes sense.

I'm going to stop sharing. Thanks. This feels like a nice transition, unless you had other topics, Jeff. We've been talking about bringing in more model-free policies to guide us in moving to, attending to, or staying in an area of interest. This raises questions about how to coordinate model-free attention or policy signals with model-based ones.

The interaction between model-free and model-based action policy is what you're referring to, right? This leads to the idea of having something like the basal ganglia or a mixture of subcortical structures—a goal state or action selector that receives these signals and tells the motor system what to do. Is this for inference or for executing a task goal? Right now, I think it's focused on inference. It's not a complex hierarchical task; for example, when you're typing on your laptop, you generally rely on model-based signals. But if something falls over, you attend to that, or if you're walking while talking to someone, there's a bottleneck in how much can be done. There may be a system that prioritizes and allows certain actions to happen. It's really about picking where to attend next.

Are you going to present some work on this, or are we just discussing it? We don't have any work on this yet. It's something we plan to work on, but we wanted your feedback before going too far. Scott has been brainstorming on it. We've talked about this before: model-free attentional mechanisms are really hard to resist. For example, if you tell a classroom to look at the whiteboard and not look away, but someone opens the door, everyone's head turns. It's unstoppable in some sense and hard to overcome. In some situations, unexpected behaviors occur. Maybe it's an evolutionary argument—animals might attack from the side, so when you see movement in your periphery, you have to attend to it. I don't know if it's a general mechanism, but there are cases where model-free attention is dominant and hard to overcome.

It feels like a scale that can vary. Except for the most extreme model-free signals, you can block it off. For example, prominent colors on a screen catch your eye, but if you're focused on a task, you block those signals and don't let them drive you. It's hard. Some websites are filled with ads and videos popping up everywhere. In Firefox, there's a tab you can click to get rid of all that, leaving just the text. I have trouble not paying attention to all the distractions, and it's a relief when they go away and I can just read. It's difficult to tune out extraneous things, and that's how advertising works.

Maybe it makes sense for Scott to go through the basic proposal. There are two mechanisms we're considering: one is saliency-based, and one is more about defining the region of interest to stay within. Scott, do you have slides or just the pictures from the RFC? Can I refill my coffee cup? Should we take a quick break? I'll take a couple minutes.

So, to present our RFC about model-free and model-based policies—a very quick note: have you heard the term RFC, Jeff? It's "request for comment." It's a proposal to do something to the codebase, and someone vets it. You can see how many comments there are; everyone weighs in and adjustments are made. If you want me to look at this, let me know, but generally I'm not looking at these things. I can drop in a human-readable version if needed. If you want me to comment, that's fine. If you don't mind doing it this way, it's easier for me. Whatever works.

It's a little non-obvious how this relates to computational objects, because this proposal has some new architecture that seems generic or useful in many situations. The top-level idea is that we were thinking of something like a superior colliculus, so you can take a wider field of view and use it to identify, for example, the label area on a child object. Based on these model-free signals, you can choose to attend to the label area and stay within it. That's the basic idea behind this RFC, and it's what we were just discussing. This RFC proposes a couple of new items and classes to add to the codebase to support this.

In this version of Monty without the proposed pieces, we normally have observations of a cup or other object that go into the sensorimotor module, which then goes straight into a learning module. The learning module can emit goal states, but it doesn't have a goal state selector. I don't know what a goal state selector is, but I'll introduce it in a second. Maybe it would have been helpful to have a version without the data components, but it's alright. Normally, there's no goal state selector; the learning module just goes straight to the motor system, which can either execute model-based actions when requested or otherwise just take control and do model-free actions.

Today, we have no model-based action policies, right? That's not true—the hypothesis testing policy is a model-based action policy. When that comes through, it always takes over. It's almost the opposite of what you were describing before, where certain model-free signals always cause you to switch. A model-based policy for inference is going to be better than a model-free policy for inference. Earlier, we were talking about a model-free policy for looking at something else or stopping what you're doing. I don't think it's black and white. We'll want situations where we generally rely on model-based signals, but there will be cases where a model-free signal breaks through the attentional window and says, "This is really important." As Scott said, what he's going to propose is a pretty general solution. We started with a specific problem, but I think we settled on a general solution that will solve many problems. For example, right now, the model-free policies are happening within the data loader and the motor system, so there's a lot of sensorimotor processing happening in places that shouldn't be doing sensory processing. That's one of the things Tristan has also been working on. It's going to be outside of the model, so the question is where to put it. Are you suggesting it shouldn't be in the sensorimotor module? It should be someplace else? No, I'm suggesting it shouldn't be in the motor system or in the data loader; it should be in the sensorimotor module. I don't have a good sense of what the data loader is, but we don't want sensorimotor processing happening in the motor system.

This proposal adds two new items: a salience map and a sensorimotor module (name subject to change, but that's what we call it so far), and something to help manage the combination. This generates model-free states—the salience map is our stand-in for the superior colliculus in the sense that it makes salience maps, perhaps some primitive types of segmentation or at least segmentation within visual fields to help you stay within a label area or some constrained area. There's a lot of work on salience maps in vision, and it isn't clear-cut whether these are actually happening all the time or if it's a poor explanation of what's going on. There's a lot of research on this as to whether these things really exist, but it's an okay idea.

One interesting point from a brief look at the literature is that some people argue that, at least at some point in the CUI and similar systems, these saliency maps are in a more egocentric 3D coordinate space rather than a topographic one. Even if it's all model-free, how do you combine signals from audition and vision to tell you where to attend? It seems like the superior and inferior colliculi do that, so it would make sense that they're operating in a shared space. As you said earlier, maybe it doesn't matter much for us because we can get 3D very early on, but it's nice that this might not be too crazy from a biological perspective either.

To clarify, this is just a different type of sensorimotor module. Originally, we thought about introducing a whole new component to Monty, like a subcortical sensorimotor processing area, but then realized that we already have sensorimotor modules for this. The only change is that sensorimotor modules can emit goal states as well, which are locations to attend to, picked based on model-free criteria. These are sensory modules, but they don't feed into a learning module; they're outside of that. They receive raw sensory input, so they have that in common. Their output, like earlier sensorimotor modules, is also in the CMP format because the goal state is a CMP signal—a location and pose in space, potentially with other information. It's a location in space, but it can't have a pose or an ID because it's model-free.

This brings up some interesting points. We were thinking there could be something like a model-free ID, which is really just a temporary label: "this stuff is one ID, that stuff is another ID." I have no idea what these objects are; I'm not going to learn about or model them, but I'm just indicating that I think this is different from that. As soon as you use the word "ID," it implies a model. There could be a subcortical model—it's been shown that even humans have subcortical systems that detect shapes like snakes. It's not a detailed model; it's a very crude detector for something long that wiggles a certain way, and that's built into your genes.

You can have some sort of models down there, but they have to be very crude. It can't be anything specific. The general assumption is that it could even be learned. The reason I like this is that if we think about Monty as learning modules and central modules, and there's this common architecture, this whole separate path you're proposing here is independent in some ways. It could work any way you want—it could be a signal from the internet saying, "Go look in the hallway down here." It's just a separate system that says you need to pay attention to something over here, and how it actually works should be independent of how learning modules and sensorimotor modules work. Is that correct? 

I think that's fair. There was a conscious decision or realization to have the output of this be goal states as well, because if it's a location in egocentric space, that makes sense. I think it has to be that. What else could it be? That's what you mean by goal state. I'm still struggling with that word, so we'll have to get into it a bit more—exactly what goal states are is still evolving. Unlike the other CMP signals, we've sometimes said it can be a bit less defined, as some of the features or properties are optional, but I think location always has to be there. In that sense, it fits with this. We have to figure out the mechanism for what we call the saliency map in this diagram, but it's really anything we want to use as a model-free, attention-based detector, and then how it interacts with learning modules or model-based action policies. I wouldn't say it's independent of sensorimotor modules.

At least, how I see it, it's basically a sensorimotor module. We still have sensorimotor modules, learning modules, and the motor system, and the sensorimotor modules are responsible for turning raw sensorimotor information into the CMP and doing the sensorimotor processing. Now, we are giving the sensorimotor module the capability to send messages to the motor system directly instead of to the learning module. I think there should be separate sensorimotor modules. If you want to call it a sensorimotor module, it should be separate. For example, I may be looking at something and hear a sound, and I'll turn my head to the direction where that sound occurred, even if it's behind me or off to the side. It doesn't have to do anything at all with what the sensorimotor module is actually sensing at this moment in time—the one being fed into the learning module. Your sensorimotor that's detecting sounds can at any time also send the sounds to learning modules, but maybe not. I'm arguing the opposite. You could think of it as independent, saying I might be an animal that doesn't understand sounds at all but has a really good vision system. The sounds could still be very useful for attentional mechanisms. There's no requirement that you send it to a learning module, but I don't see why a sensorimotor module wouldn't be able to send its messages to a learning module. It could, but the trick is you don't want people to think that sensorimotor modules that send signals—you could combine these two things in a single module, but conceptually they're not combined. Conceptually, they're different. You don't want to fool people into thinking that a sensorimotor module has to do both, because it may not. I don't know if a touch sensorimotor or surface sensorimotor really has an equivalent to this.

It would be like feeling something hot, or a sensorimotor module disconnected from cortex would be like the enteric nervous system in the GI tract. Let's go back to vision. I have a vision system with a single learning module and a single sensorimotor module, so it's just like looking through a straw. My attentional mechanism is always going to be bigger than that. It's going to take the entire retina and look for movements or something like that everywhere in the receptive fields. That is a sort of independent sensorimotor ability from a single central module. The same thing is going on with touch. In touch, I could have an unexpected temperature, but generally, if I have a single touch sensorimotor and it's moving over an object, the idea is that if I feel a bite on my arm and recoil from it, that's a different system altogether. It may be combined with a single sensorimotor patch—a patch of skin—but it's almost certainly different actual sensors in the skin. That's why we drew the saliency map SM to get this large receptive field as input. I don't see a problem with this large receptive field SM sending something to a learning module, or why a small receptive field module couldn't send something to the motor system, like for small adjusted micro saccades or something like that. I know it seems like you want to combine these. I would just caution against that. If I feel a sharp pain, like a bite on my arm, I'm not sure that is a useful signal to send into a learning module. I would want to attend to it visually, and I would also recoil from it. My body would automatically recoil and move away. Those are more reflexes, right? But that's another category of things. I could take all that and put it into this bucket of outside-the-cortex, model-free action policies, and they can be all kinds of things. It could be reflex reactions. I could have new sensory modalities that only work for these attentional mechanisms. As I said, I could hear something even if I don't understand sounds. I like the idea of separating them and treating them differently, although whatever it is, it's still going to send a location signal into the motor system.

I think reflex reactions are different, mostly because they wouldn't go through the goal state selector and wouldn't compete with other signals. They act as safety guards—reflexes you perform no matter what else is happening. Those don't use or send a location; they directly trigger muscle contraction. If I feel a bite on my arm, I'll automatically move away, which is a primitive reflex, but I'll also turn my eyes and head to look at that location and visually attend to that spot. Both happen simultaneously.

From a code perspective, it makes sense to call it a sensorimotor module because it takes in raw sensory input and outputs a CMP signal. When people learn about Monty, they need to understand it's not like other sensorimotor modules. It can play a similar role, but it's serving a different purpose and doesn't have to be tied to a specific modality. Anything about it can be different from a typical sensorimotor module. Even if it shares many commonalities, we can introduce it in a way that makes it seem distinct, even if under the hood much of it is the same. You could call it something like an attentional sensorimotor module or pick a term that differentiates it. This is a different class of sensorimotor module, and if it works on the same CMP principles, that's fine, but it really is different in many ways.

In my head, it's not categorically different. It seems like a continuous shift—when is the receptive field large enough for it to be intentional versus too small? In humans, every part of our skin is tuned to this; there isn't a single part that can't, under certain sensations, respond. These sensors—pain, heat, sharpness—are a completely different sensory system that covers the entire body and is always active. The entire visual field, including the periphery, is always active. You can't attend away from it for these purposes. In that sense, they seem different to me—very large, global things that are always occurring.

Scott proposed calling it something else, like an attentional sensorimotor module, and then we can see if it makes sense to feed them into a learning module.

This might support your point. We were talking about having this sensorimotor module produce potentially lots of goal states. What is a goal state? Is it a point in space to attend to? Yes, it's a point in space to attend to that moves my sensorimotor patch. That's what it is at a minimum, and it should generate that. If I feel a bite on my arm, my visual system will immediately attend to that location in 3D body or world space. It has to generate that.

We don't necessarily want regular sensorimotor modules to emit lots of goal states if they're not going to be used. There should be a way to maintain separation, so we don't do unnecessary work for modules that don't need it. Sensorimotor modules, like learning modules, can have a goal state generator. If a sensorimotor module isn't wired to the motor system, doesn't have a large receptor field, or we don't want it to generate goal states, it might not have a goal state generator or only a primitive one that emits goal states for sharp pain, for example.

Personality is also a goal state, so that one would have a complex goal state generator. When would the salience model or salient sensorimotor not generate a goal state? That one would always have a goal state generator, and it would be complex, processing the whole visual filter to determine goal states. Current sensorimotor modules that feed into learning modules wouldn't necessarily need a goal state generator, but the one responsible for attention would need one, though it might not need as much other sensory processing machinery as regular sensorimotor modules.

One thing about the code organization: in a goal state selector, these model-based goal states coming from the sensorimotor module with a goal state generator are privileged and are being separated. The goal state selector, as seen in the RFC, can distinguish between them. That could be a case for calling it something else, like an attention state, if we want to say these are distinct from what learning modules send, since the goal state selector can tell them apart. Another proposal could be that it's an attentional state with two modes: one advisory, and the other related to reflexes, where some attentional states can't be ignored. This could be an advisory attentional state, but another argument is that having the goal state selector tell these apart might be a relevant framing.

The saliency attention sensorimotor may not be active all the time. For example, the pain sensorimotor would be, but a visual saliency feature detector might only produce a saliency detect under certain conditions. When it does, you would attend to it. You could always give preference to the saliency sensorimotor module, but it doesn't always have to output something. Generally, the goal state selector wouldn't need to know where the goal states came from; it can select based on the confidence or relevance the sender gives to the message. For example, it might treat model-free and model-based signals differently, which isn't unreasonable from a neuroscience point of view, since some inputs come from hardwired subcortical structures and others from cortex. These are very different signals, but the system can learn that certain signals are more reliable than others.

There are two ways to do this: you could pass in a confidence value, and the goal state selector would decide what to do, or you could allow the saliency generator to make that decision and only send important signals. I personally prefer the latter, but both approaches could work. It feels cleaner to burden the saliency detector. In Monty's current form, model-based is clean and generates goal states, and then we build another system on the side that is embodiment, sensorimotor, and task dependent to determine how much we care about pain, unusual things, or other factors.

I don't know if one approach is inherently better. We probably need a combination of both. The saliency module shouldn't output thousands of signals and make the goal state selector do all the work, but there should also be some kind of confidence. If there's a very salient signal, like sharp pain, it would be sent with an alarm signal attached to the goal state. Classic saliency is like detecting high-frequency components in a visual image—something interesting to look at. Separately, pain on your arm is another kind of saliency. These are separate systems with different priorities, but for simplicity, it would be nice to put them on a spectrum.

Maybe you need different, task-specific detectors, like detecting looming shadows—a primitive reflex in small mammals—which is hardwired and separate from detecting high-frequency textures. From the goal state selector's point of view, if it's all just signals with various confidence levels, some can really demand immediate action.

You could have a confidence signal coming in, but it should also be able to ignore certain signals depending on the embodiment. For example, a robot on Mars might not care about looming shadows because there are no predators. For a biological animal, that's important. With Monty, which models the world, under different situations, embodiments, tasks, and sensorimotor types, you have to provide these saliency signals. Some would be graded, like high-frequency visual components, and some would be binary, like danger or pain. As long as the system can handle that variation in a way that doesn't impact Monty's models, it's fine. It's up to the system designer to implement any kind of saliency they care about.

This doesn't really impact the way learning modules and general sensorimotor modules work.

Yes, I think it's both. Both options are available to us in the future. We can try the simple confidence-based approach and work in a more separate stream if that seems appropriate. There are still many open questions in the RFC. You could do everything with the confidence-based method, allowing some central modules to force their way and say, "I get a hundred percent, you have to do this," while others might just suggest, "Hey, look over here, you might want to check this." I was originally thinking about it as separate, and for a first pass, since I'll be the one implementing it, I thought maybe we'd just do the confidence thing first. It also has the advantage of making things simple for the goal state selector. Confidence is good as long as you allow someone to have a binary choice.

There's no absolute requirement; it's almost like confidence is scaled from zero to one, but you can also send in infinity, and that one will always get acted on. You just need to have that ability. Even in vision, you might have multiple vision sensors that do different things. You could have ones detecting motion in the periphery and others detecting spatial frequencies near where you're looking, within the object's boundaries. These are really separate ideas, not just one thing. Now that we're talking about this, it seems like there are all these different systems at play, and they're very much application, embodiment, and sensorimotor specific—task specific, almost. We need a lot of flexibility for someone to come along, just like with embodiment in sensors. There are many choices for what you'll use and what you care about as a system designer in terms of these saliency sensors.

Are you happy with that, Viviane? You seem unhappy about it. No, I'm happy with that. Does that make sense to everybody? The goal state selector—maybe we don't need to talk about it too much now. It's something I think we can leave for the future, but I've been thinking about how much hardwired intelligence we want to put here in a model-free sense. The simplest thing is it just gets all these confidence signals and picks the maximum one. But if it's something like wanting to pay attention to a part of the world, I probably need to think more about it before proposing anything. There might be greater complexity there, and that might map onto something like the basal ganglia. I also think the goal state selector should really be an area of interest selector, a volume of interest selector, or something like that. It's not just a point; you're attending to some volumetric space.

We might want to start thinking about the goal state selector that way. It would still work for a single learning module, and as long as the sensorimotor module lands inside that space and senses in that volume, it'll work. But if there are multiple learning modules in parallel, we need to care about which ones are sensing in that volume. Maybe we should expand the idea of the goal state selector to a goal volume selector. That was what we tried to show with some of the circles being bigger, like the learning module could attend to a broader area in space, and then, based on saliency within that area in the feature space, it would pick a specific spot. That would generalize to always picking a volume of space, and at any point in time, it could change its volume to be bigger or smaller depending on its needs.

The motor system eventually has to move to a specific spot, but that's an interesting question. If I'm moving a bunch of sensors, like on the retina or on my skin, and I have two fingers, I want them all moving to the same volume of space.

I don't know what that signal looks like. It's like, move all of these to this area. Each one is getting a different location in that space. That's interesting. Part of the way we were thinking about it was a combination of things. One is you can have a set of goal states so they can, in some sense, tile space, but you can also have confidence and a blurriness, almost like a Gaussian around a location. That can be smaller or larger, so you can say, "I really want to attend to this exact location in space," or "look approximately over here." This also applies if you need to place an object in an environment—sometimes you care about a location very closely, sometimes it just needs to be approximately satisfied.

The more I think about it, the more I feel like it's always a volume, and your volume can get so small that it looks like a point, but it's always a volume. It's never a mathematical point. For example, I want to put this cup on top of the tray on my desk. That's not a spec; that is an area of the tray matching an area of the cup. It's not like a point on the cup and a point on the tray. Even if you're threading a needle, which is probably about as precise as humans can do, there's an area—it's not a single point.

It reminds me, I saw this exhibit in a museum in LA, and they had all these curio-type things. Someone had made these 3D sculptures of individuals that all fit within the eye of needles. There was the pope, there was Elvis—these tiny little things, so microscopic you need a microscope to see them. Even the space of a needle can have a lot of detail. It's called the Museum of Jurassic Technology. It's definitely worth seeing.

We were talking about the goal state and what it could do. That reminded me—one thing we discussed is maybe giving it a short-term trace, a bit like a short-term memory, but just of where it's attended to in the past so it can bias itself not to go back there. I think the literature in vision shows that too; there's some sort of inhibition of previous locations—"inhibition of return." You already looked over here, now look someplace else, don't look there again. That would all be in the algorithm of the saliency detector, not in the model. It's a model-free parameter, a model-free decision.

It sounds like we just burdened the saliency central module with all this stuff we don't want to think about right now. So, you reckon the traces are in the saliency detector, because it doesn't actually know what we've attended to—the goal state selector knows what we've attended to, so it feels like it'd be easier to do it in the goal state selector.

All the saliency modules are going to be sending lots of different signals, but the goal state selector could say, "I went here, let's try somewhere new."

I could make the argument either way. It feels like something that's independent, it's model-free. The goal state selector is model-free in any decisions it makes. It can be informed by goal states that are model-based, coming from the learning module, but it itself is model-free. But you want the salience detector to say, "Don't keep sending me the same thing, I already saw that." That could also be like a recurrent inhibition or something.

Doesn't the sensorimotor module know where it looked? That's how it can get information. Neil is arguing that the real sensorimotor module knows what it looked at, but the saliency sensorimotor—yes, I think that's a fair point, Tristan. It could get that from there. Somebody has to know it, so I don't really care as long as it's not in the model system. Tristan makes a good point—it should know it just based on the sensory input it gets. It should actually know it better than the goal state selector, because maybe the motor system failed in executing the goal state.

That's a good point. The sensorimotor module would also have a sense of 3D space. I was just thinking, if the sensorimotor module is outputting in three dimensions, it basically needs to map after it moves. Its direct input is going to change, so it needs to solve that correspondence between what it suggested attending to before and what it's seeing now. As long as that's in 3D space, which the output at least is, then you could do that.

It's pretty complicated. If you look at someone's saccades looking at a face, it might go back and forth between the eyes multiple times, or eyes, nose, mouth, eyes, nose, mouth. So it's not just "I looked at it, don't look again." It's more complicated than that.

I was planning on encoding the "don't return here" not by omitting goal states for areas you've been to, but by reducing the confidence value. My bias was toward sending a good number of goal states—locations to look at—and the confidence values can be weighted by how recently you visited. It would be up to the goal state selector, as I conceived it, to perform that weighting, though it could be done in the salience map. The decision to reweight goal states and confidence is something that could be a modular component of the system. The way to proceed is to give ourselves flexibility, since we haven't thought through all the issues and want to be able to rapidly try different attentional mechanisms in the future. People may need to experiment and finalize implementations, so let's make sure we don't code anything that restricts us from moving things around and trying different strategies. That's the best suggestion I have right now. It seems like we're making this up as we go and don't know enough yet.

Another consideration is that we don't know what users will want to build, such as what they might make for a salience map or other components. From an extensibility perspective, you're right, and it's the same with the sensorimotor module—we don't know what sensors will be used. That's the beauty of the Monty architecture: it has learning modules that are generic and should be the same in theory, regardless of the application. Then you have sensorimotor modules, which have a non-generic interface to the world and a generic interface to the learning module. We'd want the same with saliency sensors: a common interface to the goal state selector or attentional volume selector, so people can adjust confidence and behavior as needed. I like thinking of the salience map as just another sensorimotor module. Generally, sensorimotor modules can have a goal state generator attached, and can connect to the motor system, the learning module, or both. You have flexibility; you don't have to do any of it. Code-wise, we allow sensorimotor modules to send messages directly to the goal state generator. I don't see a point where a sensorimotor module would send a signal directly to the learning module, but maybe someone has a use case, like a small receptive field sensorimotor module that sends messages to the learning module to perform microsaccades for a specific feature. That's fine, but in documentation and descriptions, we should be careful not to confuse people by talking about this corner case right away. Mechanistically, it's fine, but it's a hard concept and could be confusing.

This was really helpful. It's encouraging, and it sounds like you're on board with what we've been thinking. It's a new shift in how we structure things, so we wanted your thoughts. We'll have to go through a similar exercise when we talk about model-based behaviors. There can be specific elements that aren't generic to learning modules and relate to embodiments and their features. We'll go through a similar process for motor degeneration. Cortex is cortex, and everything below it requires us to pick and choose what to implement.

That was a fun discussion. I'm glad we're thinking about these things. Thanks for sharing, Scott. I guess that's good timing unless there's anything else. I can present if you want. I thought I had some thoughts—sorry, I've forgotten. You don't have to, but that would be nice. I just need to share my screen.

That's good enough here. I'm going to go in case anyone has meetings. How long do you think? I thought our meetings went beyond 10 o'clock now. They can, but if people not on the research team want to stick around, it's just a matter of how much longer this will take. The basic ideas can be presented in a few minutes; how much time you want to discuss them is a separate question. Can you see my screen?

You see my screen, yes or no? Yes. Okay. It's just a Word document labeled "goal learning behavior." 

I want to start thinking about the overall problem of having a system do your laundry, make coffee, build buildings, assemble IKEA furniture—how would I even think about this problem? How do I put myself in the right frame of mind? These are just some thoughts along those directions. I'm calling this goal-oriented behavior. That's what I mean by goal-oriented behavior: we want this thing to really work and do something in the world. 

How do I even define the language of this problem? That's what I started thinking about. It starts with the models and what they represent. If we want to specify a goal to a system, it has to be expressed in the language of the models, because that's the only thing the models understand. Goals have to be in the language of the representations in each learning module, because that's the language we can use to communicate with the system. Goals and tasks must be expressed in the language of models. In other words, goals have to relate to representations found in a column. I can't ask it to do something that the learning module has no language for. The solutions are similarly restricted to the language of the models. If the model doesn't represent something, we can't ask the solution to come up with another way of doing it. 

This doesn't mean there won't be model-free motor policies and so on, but when we're talking about these goal-oriented behaviors, we're talking about things based on the model. This is a simple statement, but it helps me think about what things I can express to a column to tell it, "I want you to do something." 

One way to tackle this problem is to define a base set of goal-oriented tasks that aren't very interesting on their own, but can be combined to solve real-world problems. What are all the things a column can do, and how do I turn those into a behavior? It's the inverse of what columns learn and understand. You'll see in a moment that I want to take what we understand about how a column learns sequences and ask how a column can generate sequences. 

Here are some items I've come up with. The first one: we know how to infer the pose of an object to a sensorimotor. So, something I might ask a column to do is move an object to have a desired pose to the sensorimotor. I don't know how to do this yet, but it's reasonable to ask a column to do it, and a column has the knowledge to do this. How would it generate behaviors to do this task? 

Similarly, an inference task is to infer the pose of an object to the body. This is something I know brains do. The goal task would be to move an object to have a desired pose to a body, which is similar to the sensorimotor but not exactly the same. Another one might be to infer the pose of a child object to a parent. The inverse would be to move an object to have a desired pose to a parent—reorienting, for example, the orientation of silverware on a place setting. 

We're also working on an inference task to infer the behavior of an object so we can recognize and learn it. The goal task would be to cause a desired behavior to occur on an object. I flip it around: I've observed this, now I want to make it happen. 

Another would be to infer high-order sequences, like melodies or object behaviors. The goal task would be to create that desired sequence—make the melody, or make the thing swim or run. Another might be to infer an object ID, and the inverse would be to find an object with a desired ID. Instead of asking, "What is this ID?" it's, "Here's an ID, go find it. Where is it in the world?" 

This is not a comprehensive list. I made this quickly, wrote it last night in about 20 minutes, so not a lot of thought went into it. But I felt this is a way for me to at least start thinking about the problem. We should be able to do all these things as these basic tasks.

elements of behavior, and then we can string them together. I think that matches well with what we discussed as a brainstorming session a year or two ago. We talked about object, goal-oriented behavior, and that's when we came up with the term "goal state," which we used because it's the same format as the internal state of the column. The internal state would be what the column outputs to the next column, like detected object ID, detected object pose, detected behavior, detected sequence. The goal inverse would be desired object ID, desired object pose. If you send a goal state to a column, you tell it, "I want you to do something so that what you're detecting matches this goal state." We've had these ideas around for a while, but they weren't clear in my head before. 

I could, on apical dendrites, specify my desired location or my desired ID. What I haven't done is tackle these one at a time, like asking how layer 5A cells would do this. Every one of these things requires movement in the world. It's not about intentional mechanisms here, and I've never really asked how the column would generate a signal in these layer 5 cells, which is a movement vector to achieve these individual tasks. I think that's interesting. If I'm understanding, the new part is the suggestion that there's a potentially finite list of goal-like primitives that use the language of goal states as we've described before. You can only operate within a model that's known, but maybe there's this finite list that gives us the expressiveness we need to do almost anything.

I started thinking about simple tasks, like screwing a nut onto a bolt. That's complicated—I have to find the parts, align them, and know there's a behavior related to threads and a screw. I thought, let's break it down. We could call these goal states, but breaking them into atomic pieces is helpful for me to think about.

That's the point of this conversation. It gives me some clarity. For example, the hypothesis testing policy we implemented is a primitive where sometimes a column wants to tell the most likely things it's sensing. It might have a primitive for that. On the other end, behaviors—many tasks depend on the behaviors of the world. You would learn behaviors and then, informed by those, know how to act. But maybe we never connected those. Behaviors can be arbitrarily complex, but maybe on top of behaviors, there's a set of relatively simple, finite subroutines. Breaking it down into these components is important because we have to do all of these under different, complex goal-oriented behaviors. Once you can do these base ones, you just combine them in different ways, but you have to be able to do these.

The logo on the coffee cup is an example that exposes lots of problems. It gives me something to aim for. How can a column move an object to have a desired pose? I don't know yet, but I know it has to do it, so it's a problem I can focus on. The history of Monty has been starting off not having things implemented at a very neuro level, and we've always tried to see how far we can get without that because it's easier to debug and understand. But there would be long-term benefits to having a system that's fully interpretable and doesn't have a bunch of neural elements. Those benefits include interpretation, safety, and more. Once we get to this kind of stuff, it would probably become fuzzy learning, but maybe it doesn't have to be. Maybe it can be more hardcoded. Whether it's neural links or not, you could imagine L5 projecting to another layer, causing a sequence to happen, or causing a particular ID to be realized. It doesn't have to be neural solutions. Thinking about the neuroscience helps me understand the solutions, but by thinking about it, I realize these low layer 5 cells are almost certainly representing movement vectors, at least in the modules we have today—movement vectors starting out in the reference frame of the object, then converted to the reference frame of the sensorimotor orientation. How we implement it doesn't have to be neurons, but that's the base element: you've got a movement vector, and that's likely what the layer 5 cells represent.

I was confused about your comment about neuroscience. I don't think this has to be a neural solution, but it's a positive if we can come up with a list and implement it without having to use less understandable representations like neurons with Hebbian weights, or applying deep learning outputs on top of these things. If that's what you're saying, then yes, we really should be able to completely understand that if a column can only output some sort of movement vector, that movement vector is learned based on the movement inputs it has received in the past. It's learning to model what movements exist in the world related to its sensorimotor experience, and now it's able to generate those movements.

How do we solve these problems? How do you do these individual parts? As we get into this, we're going to find all kinds of interesting complications and challenges, but personally, I feel like I can sink my teeth into these things. I now know where to begin my own exploration. Thinking about high-level tasks like making coffee and folding laundry isn't base enough. I need to decompose it into the atoms of movement, the atoms of behavior, and then we can rebuild it up later. At least some of them, when we've thought about it, feel like simple associations can do a surprising amount. The example of a lamp or light switch—when you learn that behavior, you attend to a particular location, the state of the lamp, the switch changes, and then the state of the lamp changes. We need to work out the details, but it's not too crazy to imagine that you move into a location in a behavioral sequence, and the light switch was in one orientation, but now in the real world, it's in another orientation, so you just output a goal state to invert that.

That feels encouraging. I was telling David the other day that I want to have all the theoretical constraints of Monty done within a year—all these big theoretical ideas. To me, this is the big area. I have no idea where to begin, but I have good ideas about composition, tension, and sequences. We came up with some great ideas together about behaviors of objects. This thing has been looming in the background—how are we going to start thinking about making a pot of coffee? Neil, you've broken it down, but it wasn't broken down in a way I could think about it.

I feel like we're starting something that might take a year, if we're lucky, but we should be able to do this.

I just don't want to overpromise. This is the beginning of how to think about this project. That's definitely helpful. The last big open question, after object behaviors, is abstract spaces—maybe that's the final one. I keep thinking that somehow it'll become obvious along the way, so as opposed to focusing on it, I have no way of really attacking that right now. Don't worry about it. Cortical columns or Mount capsule seem to be right, so let's keep doing the things we know we have to do, and maybe that abstract stuff will just fall out. I think it will. It seems like it has to, because it should just be a matter of columns getting different kinds of input, instead of having to add another piece of machinery to the column itself. Maybe it's just a question of where that abstract movement comes from, but I don't know. It's probably going to be something quite simple, just like changing states is movement. I'm not too worried about it. It's more of an annoyance—something we have to figure out sometime—but it's not going to require introducing quantum effects to the sensorimotor module.

One thing I find encouraging, and that's become more clear with recent papers, is how much can be done with associative learning. In machine learning, anytime you don't understand something, you reach for deep learning or a complex system to solve it for you. But when we look at how neurons learn, it's some variant of associative connections. We've gotten so far relying on that, so it seems encouraging that the rest, even if a lot of it is inscrutable, will be the same.

We'll figure it out, and if we design robots building houses on Mars and they don't have deep thoughts about philosophy, it'll be okay.

Fair point.