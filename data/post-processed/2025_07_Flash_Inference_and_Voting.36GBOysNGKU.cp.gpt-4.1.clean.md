This is interesting; it shows several things. It shows ODS and SECS, which are green. It shows when you think you're fixating but you're drifting—that's in yellow—and then there are micro SECS, which are red.

It's confusing. The point is that your eyes are moving all the time, even when you think they're fixed. You think your fixation is in yellow here, and you can see the eyes drifting during that fixation. Then there's this microsaccade. I'm not sure if that's the same as drift or jitter. They talk about jitter; I think those are different. I think the drift is more smooth and less controlled, but then there's jitter and microsaccades, so I'm not sure. The jitter is like even smaller than the microsaccades. Bruno recently told me that even during the smallest of these movements, the retinal ganglion cells that are activated are completely changing. There is no stability in the signal from the retina.

This is surprising. He also pointed out that we're able to determine detail that cannot be captured by the resolution of the retinal ganglion cells.

The brain is relying on integrating input from the cones, the photoreceptors. The retina is already doing some sort of time integration of information. I don't have any conclusion, but it's something to keep in mind: when we think the sensorimotor is fixed, it really isn't—it's moving and things are changing all the time. These changes lead to changes in the retinal output and the cortical input. I'm not making any assessment; I'm just pointing out that when we try to think about the problem of having a non-moving sensorimotor, at least when it comes to vision, it doesn't really exist and it's constantly changing. That can impact the way I think about the problem. I just wanted to pass that on because it's not obvious that this is happening all the time, but it wasn't obvious to me. The drift is much bigger than I would have expected. I thought sometimes the saccades are much larger, but this image shows a little arrow on the bottom right that says one degree of viewing angle. I wonder how long they were viewing it for, because from a quick Google, it sounds like you might have half a degree of drift per second or something. You'd have to be staring at a point for quite a few seconds to drift that far. I don't know. That's an interesting question. I honestly didn't read this paper, so let's not make any grand assumptions about it. All I'm saying is that there's a lot of motion going on, and even the jitter in these microsaccades is sufficient to change the output of the retina. Of course, it depends on how quickly these things change. I didn't find anything that said how the eye movements work. I didn't do research on it; it wasn't the most important part of the work I was doing, but I was curious about it. In the end, I didn't conclude anything during this study. What I realized is that there's no easy solution to this problem. Maybe somebody else does, but let me walk you through my thinking about it, if that's okay. 

Can I ask a quick question? Has anybody seen a trajectory when the eye is doing smooth pursuit movement—how much jitter or variation there is if we're intentionally doing smooth pursuit? Is it less sporadic?

First of all, I don't know the answer. It's a good question. I would be careful with the word "intentional" because you cannot intentionally do smooth pursuit. It's a reflex, and you can't consciously make your eyes do smooth movements, but the eyes will do smooth movements to follow a moving object. That's my understanding. It's still an interesting question as to how accurate they are. I don't know. I think it's thought that the microsaccades are intentional, whereas jitter is potentially just neural or muscular noise. I thought the saccades were intentional, but not the microsaccades. Maybe it's an open question, but I think there are people who think they have various purposes related to things like attention. They may have purposes, but you're not aware that you're doing it. Intentional as in from an evolutionary point of view, whereas jitter is just the resolution that biology can do. With drift, or with smooth pursuit, you'd still have jitter and maybe drift, but maybe you don't have microsaccades given the nature of it. I don't know. The question about drift is whether you're just correcting an error or if they're functional as well. I've heard arguments in both cases. I don't want to spend too much time on that, so I'm going to move on.

This is a summary of lots of notes of confusion I had. Boiling it down: what is the problem? The problem is we can recognize objects that are briefly flashed on the screen, in theory without sensorimotor movement, but it's questionable if there's any sense of movement at all. Without sensorimotor movement, our learning modules—voting acts like a bag of features, and therefore, presumably, Monty with no movement can't do flash inference.

If we allow even a small amount of movement, we're no longer a bag of features. It's not that Monty is useless; in fact, in the frameworks paper, we showed how just a few movements can really make a big difference in voting.

But if we assume no movements, then it's like a bag of features and you presumably couldn't do any of the things you guys are doing.

Therefore, the relative poses of sense features must be used during flash inference. That's a logical conclusion, unless my logic is wrong. This ability is missing in Monty. It's not clear how important this is. To clarify, Monty does have that ability, but we just don't know how it might be done in the brain. Monty has the ability because what we give as input to learning modules are locations in the common reference frames. We can easily use those to have the relative poses between voting learning modules. The question is, if we wanted to change that to giving only movement vectors—only displacement as input instead of absolute locations—how does that work?

A lot of my analysis is thinking about the neuroscience here, and I wasn't even aware of this.

How do you solve the problem that there are end modules, and therefore, who updates who? I really struggle with this issue. Even if I know the location of each learning module relative to each other, how do you update the learning modules to take advantage of that? I'm thinking all neuroscience, so maybe I'm completely off base here.

Maybe you can explain that to me. Maybe, Niels, it's worth pulling up the picture from the paper where we visualize that. Basically, we calculate the relative displacement between the sensory input that goes into the two learning modules that are voting. The sending learning module says, "I'm here. I think I'm here. Given our relative displacement, you should be here." But if you have a hundred learning modules, how do you do that? It does get computationally expensive as you scale more learning modules in its current form. You can imagine each learning module that's receiving votes—let's say this learning module is receiving votes—it would do a different offset. There would be a different displacement for each set of votes coming in, depending on which learning module it was coming from, which column. Each learning module has to go through each other learning module one at a time. You could do it in parallel, but they're done separately. So it's an n squared problem, right? In Monty, you can specify a connectivity matrix of which learning module can vote with what other learning modules. You can make that sparser, or you could think of it in the future as being an intentional thing. Also, we send a lot of votes right now, but you could make votes much sparser. You could imagine sending just three or four really important votes per learning module.

My whole analysis might be off base here because I was thinking about cortical columns and what the representations have and how they would do this problem. I don't think they can do it the way you've shown here, because this requires serial processing, going through a list of other columns. I don't know if it's necessarily the serial nature, because it can be done in parallel. The issue is, you need to do this center displacement. It depends on how you're accounting for it. If you accounted for it before it came into the column, then you could do it in parallel. That's the issue. With the lateral connectivity, it seems to go straight to the layer that's meant to be—explain this to me, because I'm confused about this. If I'm one of a hundred columns and there are ninety-nine other columns out there, how would I process in parallel to accommodate all of their relative positions? All of the hundred columns would send out, "I'm here, and I have these hypotheses about where I am on the object." All the receiving columns would take that "I'm here" signal and calculate the difference to where the receiving column is, and that's the displacement. You have to do that by column basis. You can do it all at the same time. You can update all of them in parallel because they're just all sending their hypotheses out, and they might send their hypotheses to all of the other learning modules. But I'm one learning module. I can receive all these hypotheses. How would the neural system process that?

At least computationally, it can be broken down as a parallel computation. In terms of what neurons would do, an alternative would be where, as long as each column was voting on more of a central location, this would require that different learning modules that know about different objects have a similar sense—not necessarily the center, but if I'm a learning module that knows about phones and I'm communicating to the next level, "This is where the phone is in space," if there's some sort of agreement between the other columns that are laterally connected about where this phone is in space and how to describe that, then they could vote directly on that. If they're all saying, "The phone is at this coordinate," which localizes to this thing, then they're all going to agree if they're seeing the same thing. The displacement calculation happens once within each learning module because they're taking their object-centric sense. Rather than voting on where they are on the object, they're saying, "Where's the object in the environment in some egocentric corner?" They do that calculation internally and then project that.

I'm still confused by this because we don't know what the object is yet. Nobody even has a reasonable—this would be a hypothesis, but based on that hypothesis, they could calculate a location of the object. In the real world, in a single sensation, it's very unlikely any column would have any reasonable hypothesis of the object.

All I know is a feature. The system might know—someone might know—a column in our current implementation, a learning module or column, at least in our current implementation, wouldn't know where it is in egocentric space. But we can imagine somebody does.

I still don't understand how I could say, okay, we have a whole bunch of people in different points in space, relative to the body, in these different features, but I don't know what the shape of this object is. I don't know what the object is. Another way I looked at it was the following: imagine I'm a column—thinking political columns, not learning modules—and the representation I have in a column. What could someone tell me that would let me change my hypothesis, make me more accurate? I couldn't figure out the answer to that question. Someone could tell me what the object is, but that's the whole problem. No one can tell me where I am on the object. They could tell me where I am in space, and I could go to other people one at a time knowing where they are in space. Maybe someone could calculate an OID or something like that, but I'm just confused by this. Can we—what's the answer to that question? What could a cortical column be told that would help it infer rapidly? What we basically do in Monty right now is we tell it where it is on the object, so in the object's reference frame, what location. But how do you know the object's reference frame if you don't know what the object is?

There's one assumption that I don't know if it's reasonable, which is that the object was learned by all of the columns at the same time in one specific location, so they all have learned the object in the same orientation, a specific egocentric location—like the mark was on the table and all of your columns learned the mark at the same time while it was in that orientation. Is that a reasonable assumption? I thought it was at the time, but I don't know. Maybe we could accommodate it. Right now it's a strict requirement, but we talked about ways you could accommodate it in an unsupervised way. As long as that generally holds true and you see the object in many different instances—in the future you might be learning more about the mug in a different position—but the point is it's not like some of the columns are getting one version of the mug in one location in space and other columns are getting this version of the mug in another location in space. The mug is basically presented to all of them as a single object. Over time, it could be any instance in one place, but in other places, a different place. I think that would be fine. I think that could be accommodated. I think it's only an issue if you almost have a VR headset that's projecting two different images to each eye or something. It's mostly an issue if you learn it by looking at it and then, but not by touching it, and then you try to infer it with touch in a different orientation. I think that's general—maybe not something we need to expect, but be able to do.

Maybe I'm confused by this because when I think of reference names, I'm always thinking of grid cells and how they work. There's no shared reference frames. It's not a shared reference. They all have their own reference frame. They don't even have to—they're like different spaces. The main thing is that as they're learned, they need to be aligned in the orientation so that if we then apply a displacement to both of the reference frames, it moves you through them in the same way.

I think I totally misunderstood this, so I apologize for that. I was trying to imagine how this stuff would occur in the brain and wasn't really cognizant of how Monty was doing it today. I'm not sure how much my analysis would be valuable. I'm still struggling with it because I reached some conclusions thinking about the neuroscience, how difficult this would be to do, but not knowing how you guys did it using non-neurons. I should probably just get myself educated on that more. I saw it as a fundamental problem I couldn't resolve. I felt like I should be able to resolve it, but I couldn't get it resolved. Even if I had egocentric representations or columns, how would they help each other? How can a column be told where it is in some location space, based on all these other columns when nobody knows what the object is?

For what it's worth, I would say that part does work now. Even if you just get one observation and it's inherently ambiguous—just a surface normal and some curvature at one point, which could be anything on pretty much any object—when it's combined with all the other observations through voting, it can disambiguate the object. Maybe I should read the K paper more carefully again. I just don't see how it gets resolved. I made some writeups and visualizations on this back some years ago when I implemented that. I can try and dig them up again and send them in the research channel. This is a common problem I have: I think about a problem one way and reach some conclusions, and other people think about it a different way. My conclusions seem very solid to me in my language and thinking, but when people describe it in other languages, they say it's not a problem. I still feel like it is. I haven't been able to see where the failure of my logic was.

Did you come up with an alternative?

I came up with—do you want to discuss, or I thought it was worth maybe discussing. Maybe this is just some of my observations. I'll go back to sharing my screen.

I thought about the problem as having two classes of solutions. One is the fully independent assumption, which assumes that sensorimotor patches can have arbitrary poses relative to each other, with no restrictions. This requires a central method for columns to communicate the relative poses of their sensorimotor patches—a central place where information is gathered and then communicated back to all the columns. I think this is what you've implemented and what my initial assumption was, but I had difficulty getting it to work, so I might be missing something. There's also an alternate approach, which I call the partially independent assumption, where central patches rely on their relative position on the retina or skin. This could use local communication between adjacent columns, rather than a centralized communication system. I didn't start here, and I haven't reached a conclusion, but this idea became more relevant as I thought about it. Let me talk about these two classes a bit and explain my thinking.

The fully independent assumption is attractive because it's a universal solution. It allows any kind of sensorimotor patches in any orientation, and it should work regardless. When I started thinking about common cortical algorithms and inferences, I wondered what the evidence was for a central repository. When I talk to other neuroscientists, they point out that columns aren't arranged randomly—they're organized in topological order on the retina, with fixed relationships, which is consistent with the partially independent assumption.

I decided not to focus on vision and instead considered somatosensory input. Sometimes, the sensors on your skin are not dependent on each other, but that's not even true for the skin on one hand. When I talk about this problem, I usually mention two fingers on two hands, because I can imagine moving the tips of those two fingers independently—one sensorimotor patch on each finger. At least those would seem to be independently movable.

That's why I described the problem with fingertips on both hands.

Even the fingertips on the hands are not completely independent. I cannot move my fingers entirely independently. To prove that a fully independent solution is necessary—a solution that works for any set of poses of sensorimotor patches—I had to come up with an example that didn't require relative positions. The retina doesn't fit that requirement, and most of the skin doesn't either. Are you following me so far? I can't see you, so just say yes or something. I never felt totally comfortable with that either.

Ignore this next paragraph for a moment. When I actually tried to do flash inference with my fingertips, it's hard to do, but it's an extension of the experiments you did. Try to touch something with just your fingertips—don't get multiple sensorimotor patches on your finger, just one little tip. It's very hard to do. I couldn't convince myself I could actually do it. However, when I grab a mug with my hands, my skin wraps around the mug easily. My fingers or palm wrap around, and in this case, I might be using the local connections between columns. I don't think I could come up with a solid example showing that we actually use a fully independent solution, even though that's what I initially assumed.

Then I considered the evidence for the partially independent assumption—the idea that local connections between columns help, and the columns know they're adjacent. I already mentioned that touch inference works best when I wrap my fingers and hands around an object, contacting long, continuous surfaces. My skin works much better than just fingertips, but after that, I have mostly negative evidence.

As I mentioned, we do not have an unambiguous example of flash inference where all the central patches move independently. I haven't proven that yet, so it still might be possible. The fully independent solution is an N squared problem, and you told me you've solved this.

It's just not clear to me how it works when you have these N columns trying to help each other.

Maybe you have, but that seems like a separate issue. Isn't it N squared with normal voting, even if it's just on object ID? The voting mechanism we have works for that because it's tied to the next point—voting. If columns are inferring the same thing, you can vote on it. You can vote on object ID. In Monty, there's a shared dictionary of object IDs, but in neuroscience, each column defines its own SDR for an object. You can do associative memory pairing, and it works for N problems. We've shown that it works. It's N squared, but you don't have to go through it thoroughly—it all resolves instantaneously. So it's N squared, but not in the sense that the voting mechanism is just a settling.

Maybe one way to describe it is that the computation between any two columns is a bit more bespoke. Did it make sense, Viv, what I was trying to describe earlier about having a general location for the objects that could be common to all the columns?

again, if they've learned the object in similar locations in the past, that feels closer to voting as it's currently formulated, where they're just voting on one thing. It's not like each—it's kind of like before the column projects. Let's say one column is here and this is its hypothesis. Before it projects the location of the object, it works out, "Okay, I'm here. I think this is where I am on the object, so I'm going to project the center of the phone to everyone else." Another column is here and says, "Okay, I'm going to calculate the location of the center of the object and project that to everyone." In that sense, it's more like a common ID, a common location. Functionally, it would probably be the same as what we do at the moment, but just a different way of calculating it.

I wasn't able to get your screen up in front of me to see what you're saying. Oh, sorry. Let me try that again. One column thinks it's here on the object—that's its hypothesis. Right now in Monty, we communicate that, and if it's voting with a column up here, it communicates that and calculates a bespoke displacement between these two columns. Instead of that, this column believes it's here, so it works out where the center of the object is—some kind of arbitrary, agreed-upon location. Within the column, it figures out, "Okay, this is the center of the object; that's what I'm going to send to everyone else." The other column also says, "Yeah, the center of the object is here; I'm going to send that to everyone else." In that sense, they're speaking a more common language. I guess that makes sense, although we don't have a variable like that in our learning module, or at least in the cortical column. Each cortical column would have to have a hypothesis about the center.

What I was trying to get at earlier is it might be similar to the location of the object that you would want to send up the hierarchy. Let's say you're representing where the phone is in the room or on the table. You want some sort of location representation for that. Maybe over time, columns generally agree, "Okay, if we're going to talk about the location of this object, we're talking about here," and that might be center of mass or something else.

That's interesting, because after years of struggling, we abandoned that idea since it didn't work for positionality and other things. I've gone completely the opposite direction, saying there is no center to an object—there are just points in an object, and those points are aligned with other points in other objects. It would be a big ask for me to abandon that and go back to the old way of thinking, because it just didn't work. I felt such a relief when we came up with the composition idea, the column-by-column approach. I guess you could be voting on some sort of center of mass, but I don't know. We'd have to have some cells in a cortical column doing that.

But I don't even know how to begin thinking about that. The only thought is it feels a little bit similar to the L5 projection, in that before voting you'd want to convert it into an egocentric representation, because ultimately that's the space where the voting happens. We can imagine maybe a column has something it could change into an egocentric location.

But I agree with you. I also feel uncomfortable with the idea of a single location for each object, unless maybe it's a fuzzy thing. But the fuzzier you make it in space, the less refined the voting will be. It was funny—I had a paragraph earlier and said, "Don't read it yet," which was the idea that if I just hold my hands out in space in front of me, not touching anything, I have a sense of the volume they occupy. It's a pretty crude sense; it's not a detailed shape, but I know where my hands are, which one's on the left or right, how far apart they are. If I move my hands together and they touch something, I have a very good sense for the size of that object. That's an observation in support of the independent hypothesis that says there is knowledge somewhere in the brain as to the volume your sensors occupy at the moment.

I assumed that wasn't in the allocentric columns we've been modeling so far, that it was some sort of egocentric thing, but I couldn't figure out how to make that work. That's evidence—it's there. I clearly know that when I touch something, I have a sense for the volume of it and where its boundaries are, at least some of them.

But then I couldn't figure out how to get that back into a usable form. I guess your idea—if I had a central point, we could all be voting on that central point—then yes, I could bring that back into a column. That's right.

All these things just made me a bit uncomfortable instead of confused. Maybe I'll just not—again, I didn't reach any conclusion, and maybe my analysis is not very useful. Let me just finish going through it, if you don't mind. You can decide whether you find it useful or not.

This paragraph really got me for a while. You just partially answered it: what information could a column receive that would allow it to infer without movement? I don't think it can be told its location, because its location is in the code—it's sharable, but no one else knows anything about its codes of location. As you pointed out, it could be given something in a common egocentric value, somehow converted into a local-centric value. I'm not clear how that would work. Sorry, I said it can, and that's not totally correct. The object ID is what we use in voting, but that just pushes the problem elsewhere. Somehow, it seems like I need to use the column's model, but I think you gave a partial answer to this question. I'm open to suggestions, and you at least proposed one.

One interesting thought is that standard voting makes sense in neurons because, even though two columns don't know the same object ID, they vote by forming an associative connection between these two SDRs. In another instance, when we see that object, one can say, "I think you should predict this SDR," even though that SDR doesn't look anything like my SDR. That seems harder to do for locations, even if it's egocentric, because you can't just make an associative connection. Let's say it was in location 1.5, 0.12, and it was the same in the other one. They each have unique location codes. Even if you associate those, now they're in different locations. So how do they share that?

Voting requires that you learn these associations from stable patterns. Hojae, this discussion reminded me of a page in our documentation. I'm going to share the screen real quick. Let me stop sharing again. It's about the difference between sensors and agents. An agent can have multiple sensors. Right now, we're showing an eye agent and a finger agent. If there's a human agent, we would have sensors on both eyes and touch, but one agent. Even if the sensorimotor—sorry, I'm lost, Hojae. What's the difference between agent and sensorimotor again? Say that again. In my head, the agent is something that moves; maybe a sensorimotor is something that moves. An agent is the thing that's moving, and all those sensors attached to an agent are moving together. That makes sense.

In that description, we would technically describe a human as a hierarchy of agents. The lowest-level agent is where the sensors are fixed relative to each other, like the partial correlation you were talking about, Jeff. Even if the sensors on the eye and the sensors on the finger move independently, eventually the highest-level agent—in this case, the human—would be able to consolidate the information about where the sensors are relative to each other to calculate, to do the vote. I feel like the human agent is the central location where all this information comes together. This is confusing, because if the agent is just the thing that moves a bunch of sensors, then the human agent doesn't exist in my mind. The fingertip that moves is one piece, and the retina that moves is one piece, but as soon as you have fingers and eyes, you have multiple agents. I wouldn't say there's a human agent; there are just multiple agents moving around. In other definitions, you can see it differently.

I was thinking we have one agent where the sensors are at two different locations—groups of sensors at two different locations. One human agent has both these sensors and those sensors. I think Nuno was talking about the hierarchy: two agents, an eyeball agent and a fingertip agent, and maybe an orchestrating agent, like a human, that controls both. Eventually, in my mind, there's at least one agent that knows all the sensorimotor positions. I'm not sure where you were going with this, but it goes against the idea that the agent is just the thing moving the sensors or a fixed set of sensors.

How does this relate to what you were just talking about? Maybe I'm making things more confusing. Going back to the post, fully independent or partially, if there's one agent that eventually knows all the sensorimotor locations, then we can always calculate the relative positions of the sensors. What does that mean from a column point of view? Rather than talking about agents, it's useful to think about columns. An agent is almost an abstraction we've come up with mentally, but at the end of the day, it's columns communicating with columns everywhere.

Maybe there's a mapping sometimes between columns and agents. If it's possible to frame it that way, I think it'll be clearer. It probably aligns most with the higher-order column. You're saying a higher-order column would know where the finger is relative to the eye and then project down to the finger and eye columns, telling them their relative location so they can vote with each other.

I think it could help for sure. Those are ideas I've considered, like whether there's some sort of hierarchical composition happening, as we described with Hojae, but it's still too fuzzy for me. I agree. Would the idea of a phantom limb or those illusions help here? They seem to support the idea that our egocentric coordinates are learned in the learning modules and not innate in the neural wiring. For example, when people put a fake arm next to theirs and someone hits it with a hammer, they react as if it's their own arm. You rapidly learn that your arm is now over there, which suggests against hardwired egocentric locations and more about learning modules having a model of egocentric locations.

There are two things here. One is the idea that you can fool yourself into thinking the rubber hand is yours, but your real hand has to be pretty close to it. It can't be off in the other direction; it has to be right next to it, and you just can't see one. That would still argue for a certain range of accuracy. It reminds me of grid cells, where you think you're in one location, but if the room is distorted, your location space is also distorted. I'm not sure if that's related. Obviously, the brain knows where your limbs are, up to some accuracy. Maybe those illusions, not the phantom limb but the rubber hand illusion, work because they're within that accuracy range. You can distort it slightly and fool it by sensory input, but if your hand is behind your back and you see a rubber hand in front of you, it's not going to happen.

That's a trickier problem. One observation I had is that when I imagine my finger going along an object, like the rim of a cup, and then I hit the handle unexpectedly, that's basically an activation on the side of my finger. The tip of my finger is following the rim, and then the side hits the handle. It feels very local, as if the side and the front of my finger are two separate things, and the brain has to figure out where they are in the world. The set of sensorimotor patches on my finger seem to work together; they aren't independent. They know exactly where they are relative to each other, and those things don't move on the digit of my finger. I can move the digit, but the patches themselves are fixed. It's not a strong argument, but with that example, it feels like hierarchy could help, at least. You might have a column for each patch of skin, and maybe a column for the sleeve of the finger, but it doesn't feel that way. It feels more like I have one on the side and one on the front.

It's almost like the finger is made up of these central patches, and they're working as a unit. They recognize certain patterns among themselves. For example, when my fingertip feels both edges of a chip in a cup, it feels like my fingertip knows that, not just a bunch of sensorimotor patches communicating through a central mechanism. It's possible that both centralized and local mechanisms are at play. There could be approximate local processing happening without external input, like localized lateral processing within neighboring areas of the retina or fingertip. But for longer-range connections, like from one finger to another or from the finger to the eye, or from visual to auditory cortex, that might require routing through something that does a reference frame transform or uses hierarchy to model where things are relative to each other on the body.

In vision, what if a similar mechanism is happening? Local patches of the retina, like sensory patches on the fingertip, might know certain things and vote locally. If they're all able to do this, could you solve the flash inference problem under that assumption? Would there be enough information from local interactions to lead to a global solution?

and I couldn't convince myself one way or the other on that, but it still seems possible. As we showed in the first paper on Thousand Brains, with just a few movements you could generally settle on the right solution. That didn't involve any centralized voting, and with a few movements, you're no longer doing bag of features—you're narrowing down very quickly. It felt like maybe local voting on my skin and local voting on the retina would be sufficient to solve the problem. I couldn't convince myself either way about that. Would that be biologically plausible? In development, everything is connected, and local patches are predictive of each other, so those connections would stay alive, while connections that don't predict each other would get pruned. You'd get patches where local partial things work together, and uncorrelated things are no longer required. 

In the brain, imagine we're on the cortical sheet, in some somatosensory area, and next to it is another region, maybe a different modality. You might have a vision region next to a language region. Presumably, local connections that aren't correlated would be lost, just as you said. One hypothesis about synesthesia is that those connections remain when they shouldn't, leading to associations across modalities that are incorrect. For example, between adjacent columns—a color column and a sound column—there should be no connections, but if there is one, your sound has a color.

I like the idea that maybe it's a combination of both, and it's just associative learning of what remains correlated and what doesn't.

But it still raises the question of how space is taken into account. I feel like you still risk a bag of features, even if it's local. Maybe not. 

One thing we could do—there's clear evidence in my mind that I have a sense of the space my hands are occupying even if they're not touching anything. That suggests a central idea of location, of an object somewhere. I can't deny that. The idea of my fingertips recognizing little nuances on a cup is also strong for me. I could argue that both of these things are happening. We could decide to pursue them or not. I didn't work on what an actual mechanism for local voting would be.

I could think about that. I think it's possible that multiple mechanisms are going on, just like Viviane said. What we want to do about this, I don't know. We don't understand either of the two, but both. It sounds like you have a solution to Monty already, so why are we doing this? The main motivation for questioning how we're doing it currently in Monty is because we were thinking about sending movements as input instead of locations in egocentric shared space. If we only send movements, we don't know where sensors are relative to each other anymore, since we don't have the shared coordinate system. That was where the question came from. Is that also an issue for the output? At the moment, when we send a command, we need to convert from object-centric space to egocentric space, and that requires the column to know where the object is. Even if we—yes, that could be a movement too, so it wouldn't matter in this case.

Also, just as a reminder, we're over an hour in. If you want to keep digging deeper on this topic, or see if there's anything else—I had one last observation. I wonder whether there's any evidence that even in visual flash inference, even though we present it and it goes away quickly, there's not an existing trace that gets processed later. You are able to attend to different parts of the flashed image even though the image is no longer there. I don't know if there's—yeah, I think they try, and who knows. In psychological experiments, this idea of flash inference of a complex scene is called the gist. Can you get the gist? Things like street scenes or whatever. Often they follow it with masking—they'll show it for 70 milliseconds, then show some white noise. The idea is that the white noise should prevent recurrent processing, but no one knows for sure if it does. That experiment was inconclusive—how do we know the mask is actually having that effect? People can still recognize it with masking, but it's a bit harder, which supports that the mask is doing something. If they did those gist of flash inference experiments with unusual orientations—no, I can't remember.

Even if we assume they know their relative locations and that is a fixed arrangement, if we want to recognize things in any orientation using flash inference, it would still require reference frame transformations between different sensorimotor sets or columns. If we assume canonical orientations work well but non-canonical ones don't, that could be due to more hardwired associative connections. That fits with some evidence: if you give people less time, they're worse at atypical orientations. If I did that flash inference exercise with images upside down or inverted, I'd have a lot of problems. I did. I could do that experiment anyway. I hope this wasn't a waste of time.

Not at all. No, definitely not. I don't know. I think it was interesting. I still don't understand Monty well enough—that's my failure. If Monty gets more and more capable, I'll be behind.

Maybe just briefly, one example Scott has mentioned before that's worth thinking about more is the inverse of moving the finger over the mug to recognize it: someone drawing on your hand or your back, and you recognizing that object. It's interesting because it's a little like flash inference, but it requires integration over time. It's not like something is pressed into the palm of your hand and you see everything at once; it's traced out.

I don't think we should discuss it now, but it's one to think about.