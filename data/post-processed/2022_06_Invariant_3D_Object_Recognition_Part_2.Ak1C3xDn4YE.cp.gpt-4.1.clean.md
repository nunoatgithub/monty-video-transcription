Okay, great. Thanks everyone again for tuning in. This is a follow-up from yesterday's presentation, so apologies if not everything is super clear, especially for those who weren't there. If you attended the journal club I did a week and a half ago, hopefully most of this will make sense. I'll be continuing the discussion about view spheres and preferred views, tying this in with the motivating question of rotation invariance. The first half will focus on that, and the second half will look at what we do when sampling points of the input space that are unfamiliar or lack a learned representation.

For rotation invariance, the general approach I'm describing isn't fully rotation invariant; you have to test multiple possibilities to arrive at a solution, so it doesn't automatically handle rotation invariance. I'll explain why that is and how to address it efficiently using heuristics, experience, and how view spheres can help. I'm not going to talk much about this, but particle filters come up a few times and could be a useful way to handle the brute force nature of this problem, since particles can be parallelized easily.

Can I just ask about the particle filter things? I made a comment in Slack. I watched that video, and it struck me that the way they described particle filters was very similar to the union property we use in sparse representations. Have we talked about that before?

I think we have, at least a little bit. There are definitely analogies. Based on what that video showed, it seems that's exactly what we're doing with unions: multiple hypotheses are occupied simultaneously. It's not a probability distribution, but multiple hypotheses are being narrowed down through sensory movement experience. The one difference is you need to be able to read out each particle separately to do things, which may be how they implement it in particles, but with the union you do it simultaneously because of the sparse properties. Other than that, it's basically the same hypothesis—multiple possibilities being narrowed down through continued inputs. That is exactly what we do in temporal memory. They also resample around the most likely ones, but those are details. Some of that is hard to do with unions because you can't read out each pattern independently, but there may be ways to do something similar. In particle filters, these are probably floating point numbers and very precise, and if they have to resample, that's possible, but there are those differences. The idea is that you have multiple accurate hypotheses, you start with many, and then narrow them down in parallel. Maybe in the actual particle filter code, they do them one at a time, but theoretically they're done in parallel, and that's what unions are doing. The union is not a distribution in the sense of a normal probability distribution; it's multiple hypotheses. The language in that video was very clear and similar to what we do in temporal memory. I thought that was good and wanted to point that out before we go on.

I think Marcus has discussed some of the similarities, at least when I've spoken to him. In the columns plus paper, he has a paragraph about the similarities. I don't remember if I've posted on Slack, but I think it could be helpful because there may be some efficient libraries for particle filters that we could use. That might be an alternative, or temporal memory could be an efficient method for particle filters—an efficient mechanism for doing it, if you could combine those two. Brute force sounds undesirable, but when I thought about temporal memory, it can be very efficient. So, why even debate whether we want a perfectly rotation invariant representation of objects? In general, invariance comes at some cost in sensitivity. A classic example is max pooling in CNNs, which reduces the spatial sensitivity of the feature representation, helping with translation invariance, but also coarsening the spatial code of features and potentially making them have undesirable properties, like being approximately bag-of-feature detectors. If humans use a search-based or semi-search-based solution rather than solving this problem perfectly, that might represent a good trade-off found by biology. That way, we can focus on representations that handle other challenging advances like translation, scale, and novel examples of objects.

This isn't a new idea. One obvious thing to do is to test the most likely orientations when you come across an object. "Most likely" can be a combination of experience and heuristics based on local features. To talk about this, I'll loosely define the principal axis of an object. Every object has this, and it's basically with reference to how the object is normally placed in the world, due to constraints like gravity. For example, a coffee cup.

and it doesn't specify rotation around the principal axis. So a coffee cup can obviously be oriented around that in various ways, but at least the principal axis tells us if it's right side up, upside down, on its side, or in some other implausible orientation. With experience, you learn that some of these are more likely than others and should be tested more frequently, forming a denser part of the distribution over possible rotations.

Interestingly, humans seem to do something like this. In a study, researchers looked at how quickly people recognize line drawings of objects like airplanes and boats oriented at various angles. Recognition was fastest when the object was in an upright, typical orientation, and people could tolerate slight deviations. Turning the object completely upside down made recognition much slower. Notably, the worst cases were deviations from upright or upside down—being completely upside down was better than being slightly skewed, and 90 degrees was better than slight deviations from that. This suggests we have non-uniform priors over typical object orientations when recognizing things, testing hypotheses with greater likelihood for more common orientations.

However, sometimes objects won't be in their typical orientation, so it would be helpful to have another way to constrain the hypothesis space and make recognition faster.

One idea is that local features, particularly principal curvature, can provide some information about orientation. For example, a flat surface will generally be either parallel or perpendicular to the principal axis, often representing the base or side of an object. This doesn't always hold—polyhedral dice are an exception—but it could help suggest possible axis directions.

Even if the local feature and principal axes aren't flat, the principal curvature gives us maximal and minimal curvature, and one of these is typically oriented either parallel or perpendicular to the principal axis. Are you defining this principal axis with respect to gravity? Essentially, yes. For example, with a cucumber, you could argue the principal axis is almost horizontal. I haven't decided if that's the best way, but in general, it doesn't matter much because the principal axis is usually either one way or the other, not at an arbitrary angle. A natural way to define it might be the object's most typical orientation in the environment, which is simpler than thinking of it as shape-based and is more gravity-based.

If it's shape-based, you first have to learn the object's shape and then decide how to parse that. Bringing this back to view spheres: for a view sphere like this teapot, the features are on the view sphere relative to the learned principal axis. When inferring an object, it can be at any orientation in the world, so you need to infer both your position on the object (on the view sphere) and the object's orientation. This can be conceptualized as a sphere where the principal axis points to a location on the sphere's surface, representing an orientation hypothesis.

For example, if you see a feature, you would have multiple hypotheses running in parallel about the object's orientation. This is what I mean by a search-based approach.

You're just testing along one axis, so you don't represent rotation around it. I'll get to orientation—if you have an estimate of the principal axis, that estimate may be wrong, but let's say this is one of your hypotheses, and this is another. If you see a spout, that tells you what the rotation is about the principal axis. One benefit of the view sphere is that you can only see a feature from one particular side. As long as you have an estimate of the axis, which could be wrong, seeing a feature gives you some information about the rotation about the axis.

But isn't that feature the same problem as the whole object? I'm looking at the spout and have to figure out its orientation. I'm assuming the spout representation tells you some information about orientation as well, like whether the spout itself is upside down or right side up, which just gives more information and makes it easier. I'm assuming the spout representation is invariant to rotation, but why would it be? It seems like another object and just the same problem as the whole teapot. I haven't figured out how to do this.

That's a fair point. I was imagining a local feature detector that could recognize things because it's a smaller feature, generally simpler, and you've encountered it more often in the environment, so you'll have encountered that orientation more. This would be more dependent on training. I'm not sure I buy that. What I was saying yesterday is that you have to assume this is in some sort of hierarchy or multiple cortical columns, and every cortical column is doing this. You might have a cortical column that knows about spouts and is doing the exact same thing, but at the level of the spout. Then you have a higher level, maybe V4 or IT, operating at the level of the full object. It's the same operations, the same thing. The problem is that it has to occur at each column. We don't want to rely on hierarchy, but the point is, the spout is not easily recognized. If I isolated it from the teapot in this view, I'm not sure I'd figure out what it is.

It's certainly not, "Oh, there's an oriented spout, they already know the orientation of the whole thing." I don't think so. If you isolated the spout and rotated it 180 degrees, you wouldn't be able to recognize it. In this picture, I don't think I'd be able to recognize it at all if I just isolated the spout. You could make a hypothesis based on the symmetry of that image, or there's an implied principal axis of the symmetry, but I don't think I could get it from this table.

I'm not sure this is a problem because you have lots of cortical columns at different scales operating together and voting on things, and the only self-consistent view is this thing. So you're saying orientation would be voted on. Because you have multiple hypotheses for orientations, but we still have to ask how each column's hypothesis is generated.

Everybody's rotated here, so all inputs are rotated, and everyone would be confused. Everyone has to solve it.

I don't know. At some level, I'm approaching it from—sorry. That gets back to the idea that it doesn't happen instantly; it happens over time because these unusual orientations take time, as Niels showed earlier. There is some sort of search or settling process. There are two different things: the brain could literally be trying different orientations, still voting on it, but going through them surreptitiously, as opposed to having a distribution of orientations and the right one comes out. That doesn't seem right to me. It seems like, because it takes longer, it's not just a voting thing. There might be multiple orientations being tried. It can still take longer, even if you're trying multiple orientations. It could be both—a combination. I would agree that there is some voting and settling going on, but it still seems like it wouldn't be that some of these columns have it right from the beginning. Everyone's going to be confused by this. It's a pretty confusing image. That would be part of this, if we go down the road of testing some of this out. How would this happen in parallel across cortical columns? It seems plausible. Multiple cortical columns could figure out these unusual orientations. But are you saying that some of them guessed right to begin with and they win out, or eventually it will guess right? It could be some combination of parallel processing. It seems to me that it has to be some combination.

If I look at this teapot, in introspection, what really clues me in on the orientation is the little knob on the top. That piece has more shape to it than the other parts, so I focus on that. The spout itself is very confusing. Is it touching the table? I don't see a table. The table is on the top right? I don't see a table. I just see the teapot floating in space. Interesting. It's difficult because here we're looking at the whole object at once. How well would we do this? All these tasks looking at people recognizing objects at weird orientations show the object in its entirety at once. It's not requiring that they saccade over a tiny part. Are they able to saccade, or do they just get a flash of it? I'm assuming they're able to saccade. How much time is involved? Actually, not in this case. That's funny. Very quickly.

They're basically processing it in parallel, which provides additional signals about the orientation of an object. My subjective experience is that when I look at something, I feel like I want to rotate it; it feels more comfortable. That's the question: why does it feel like you want to rotate? There's an implied principal axis in the image, symmetry, and that would imply it's not correct—principal axis boundaries of the image itself. I'm not trying to ignore that. There's something that wasn't part of the test here. You may better cut the images around, just give away the orientation.

In 100 milliseconds, you don't really get a chance to do much at all. The maximum is two seconds, I think. You typically get three to five in a second. In a hundred milliseconds, you would not be able to do it in just one presentation.

The motivation for this is that the hope is that one of the benefits of the view sphere is that it reduces the degrees of freedom for testing orientations of the object. In this case, you're getting a sensation which, in the reference frame of the view sphere, is along the side. Then you just have to test all the hypotheses about the orientation of the principal axis. You know where you are with respect to the principal axis. If your first sensation is the lid, then you need to test all the hypotheses about the orientation of the principal axis, but you know the orientation of the principal axis because it's pointing at you. If you test somewhere in between, along these surfaces, you generally get more information about both, assuming that features are not symmetric on every surface. If features are symmetric on every surface, then you have to test for all the different features that coincide with that. At least this should constrain the number of orientations you have to test.

In the example where you first feel the spout and have two hypotheses, then you rotate the object with one hypothesis and see flat, which is inconsistent with the other hypothesis. You see the lid, so you know the principal axis was the correct one. It's the standard idea. This would help limit the total degrees of freedom of pose—both location on the view sphere and orientation of the object. The general pose is four degrees of freedom: the azimuth and elevation on the view sphere, and the azimuth and elevation of either the principal axis or the orthogonal axis, depending on what you first started feeling.

We're trying to figure out the orientation of the object, and you're limiting it to four degrees of freedom, but that seems like a lot. I typically think of orientation as three degrees of freedom. Four degrees of freedom also includes where you are on the object, so it would be six degrees of freedom if you included both the orientation of the object and your position in x, y, z space. If I just think of the object as a whole, not what part I'm focused on, it's orientation—so orientation is only two degrees of freedom.

It bundles them together. You get a coarse orientation with two degrees of freedom. You get a full description of the orientation as well as where you are on the object. In general, orientations in 3D are three degrees of freedom, and your location is three degrees of freedom.

This is trying to do both with fewer total degrees, but you're having to describe orientation with more degrees if that's all you cared about. It assumes you're centered on the object. You're holding it at a fairly common distance.

The distance doesn't necessarily matter so much. It's more that you're rotating it consistently with respect to your eye, not moving it all over the place. If I'm presenting this to the eye all at once in a flash, we have to assume there's voting going on—multiple models trying to model this at once. Otherwise, I have to think about only seeing a small part of the object through a straw, in which case I'd have to move my eyes. We're looking at a situation with multiple models, multiple columns, each sampling the object in different places, and they have to vote to reach a consensus. In this 100 millisecond example, there's no saccading going on. I'm just stating facts in this situation, not drawing conclusions.

This doesn't say anything about rotations about the principal axis. That's the degree of freedom that's missing here. That's what comes with the view sphere—with the position, it comes with the position of the view sphere. It's because we've taken this complex object and flattened it out to a sphere, and you can only move along the surface.

Doing this without movement isn't something I've assumed. At this point, this is being solved by rotating the object. I approached this with the idea of how to do this with flash inference. The earlier experimental data was with flash inference. I'd have to double check, because that could have been a corrected estimate of how long it takes them to respond, as opposed to how long the stimulus was presented for.

I assume that was how long it took them to respond. That was my assumption. So after stimulus onset, they flash an image and measure how quickly you can push the button or identify something. No, it has to be an offset because you can't have zero. The shortest time a person can perceive something is 60 milliseconds. Could you show it again? You asked me today if it's going to be a press or whatever it is. It is stimulus duration. It is mean stimulus duration. Oh, it's stimulus duration. So what does it mean? It means if the stimulus was any shorter than this, you wouldn't be able to recognize it. This is the time at which you can recognize the object, up to some accuracy level, probably. Exactly, probably something like that. I need to look again to be sure. But it would be something like that. Just to be clear, is the response identifying the object or identifying the orientation of the object? The object. So, presumably, assuming there's some verbal omission involved rather than purely foveal omission, you might get a general impression. Exactly. You're going to get the whole image, the whole thing is going to be presented to you, but I'm going to assume that a soda straw is not enough to solve the problem. It has to involve voting of some sort. Otherwise, you can't do it with a straw, and we're not moving the eyes because this is a stimulus presentation; you don't have the option to move the eyes in this period of time.

What I'm trying to get across is that it may be that the first things I discussed about testing most likely orientations and heuristics are more general and not specific to the view sphere. What I was trying to get across is that if we adopt this approach, we can reduce the degrees of freedom that need to be considered, which can make the system faster, no matter how it's implemented.

This is just to make it a bit clearer how you can test with this kind of probability distribution. For example, with particles, you might initially have a prior that the principal axis is either upright, inverted, or on its side—anywhere along the equator. Then, for example, you get a sensed curvature feature, which has the principal curvatures oriented a certain way. That gives you some evidence that the principal axis might be oriented like that, or around that equator. Then you update your sampling based on that.

Niels, one thing about when we went to the four-dimensional thing: are you assuming it is possible for the principal axis to be pointing head-on at you? Yes, that's what I was saying. If that's what happens, then you know exactly what the principal axis is, because the feature you're seeing is associated with the exact direction of the principal axis. But then you need to test the orientation about the principal axis. If the principal axis is head-on to you, you don't have any features in line with it to validate it. In this case, it would be the lid. If you're looking directly at the lid, you may or may not have enough information. I'm assuming there's more information if the principal axis is parallel to the plane of vision rather than perpendicular to it.

If that was the case, is that true? What do you mean about more information?

If you're used to seeing the object with the principal axis vertical, it's not perpendicular to you; it's parallel to the plane of view. That's your most common viewing position, with gravity obtaining and you're coming in from the side.

It takes a little bit of mental effort for me to say that if I've not seen the thing from the top and suddenly I'm shown the picture with the lid being the foremost thing, it might take more effort to recognize it because the principal axis is now head-on and all the features we've seen so far have been oriented with respect to the usual view. Isn't that the same as saying if I'm seeing the object from a viewpoint I've never seen before, I may not be able to recognize it at all? It's because I just haven't seen it from that direction. Potentially, but what I'm trying to say is that it's not the most common. The most common means I haven't seen it very often. It may or may not have a lot of features. For example, if I look at the bottom of a car, there's a lot of stuff there that could identify the bottom, but unless you spend time underneath cars, you may not recognize it at all. Where I'm heading with this is that if the capability of recognizing the principal axis is only one-dimensional—in other words, its rotation in the plane—it still gives you a lot of affordance in figuring out what the object is. If you allow the principal axis to come out of the plane of the viewing direction, you're asking a lot more of the system.

You would expect it to be slower.

With this approach, you would expect it to be slower if the principal axis is coming out of the image. By definition, if that's the case, then you're looking at an uncommon view.

I'm saying that in many situations, having one dimension of the principal axis identified is an easier task than when it's foreshortened. It will definitely be easier and preferable, but there's always the possibility that the object will be oriented differently. In those cases, you might need to use other mechanisms to resolve it. You might not be able to do it with flash recognition, but a secondary mechanism could help. Sometimes, a flash inference won't work, and you'll need to attend to different features and figure out what those features are. Other times, you can do it in a flash, even at different orientations. As we discussed earlier, it's a combination—sometimes you have to break apart the system and attend to different components, and other times you don't.

I'm surprised by how well this works. There are classic examples, like distinguishing T's and L's in the periphery with very short presentation times. If the T's and L's are oriented right side up, you can recognize them pre-attentively and quickly. But if they're skewed at 45-degree angles, you can't. When you fixate at a point and the stimulus is shown in the periphery, recognition becomes difficult. There is an effect of the familiarity of an object and being able to process it in parallel.

What puzzles me about this experiment is that if there's a mental process going on, it takes longer if the object is at an angle. Why does it matter how long it's presented on screen? The y-axis shows presentation time. Typically, once the stimulus is gone, they mask it with something random, which wipes out your processing completely. It's not like the image is gone and you can still think about it in V1 and V2; the processing is disrupted. If you didn't mask it, the cells might keep firing, and you could mentally manipulate the image. You have an impression, and you can still manipulate it, even without new input. There are neural mechanisms for visualizing things when there are no signals at all, so there is circuitry for both.

That experiment is consistent with the idea we presented a few months ago: each cortical column may explore a few different orientations in parallel, and different columns at different orientations together figure out the best one without additional sensory equipment. If you have more cortical columns representing likely orientations, those will converge more quickly.

One last thing about rotation invariance: how do you actually learn the principal axis? I mentioned it's relative to how the object normally sits in the environment. I was talking to Super Tyre about this, and maybe we can initialize it randomly and iteratively update it based on the frequency of encountered orientations. The way you phrase it as being dependent on gravity makes it easier. Making it shape-based is trickier.

Gravity would be a good starting point. Assume the object is oriented vertically relative to gravity on first impression. Most of the time, that's true. Sometimes, you might see it at an angle the first time, but usually, it's in a natural orientation if it's gravity-based. Over multiple sessions, you would encounter the object in different orientations, and your principal axis could be updated to reflect the most common orientation.

We typically learn about the world through repeated experiences. Even in a single session, I'll manipulate an object and see it from different angles. It's hard to imagine only seeing a coffee cup once at a weird angle and never manipulating it. In the columns paper, we randomly initialized the orientation at the beginning and never updated it. We only anchored the grid cells and assumed the orientation was always the same. We didn't really solve these problems.

It's interesting to think about this without movement, just as a flash, which forces you to consider the loading. On the flip side, it also has to work with just a single cortical column and with movement. It has to work in both scenarios.

It's the same process—voting in parallel or voting over time.

It's still not clear to me, and maybe this is just a misunderstanding, Mike, but how voting really deals with the spatial arrangement of the columns. From my understanding, that's the key difference: with inference, you're doing path integration and taking account of the spatial arrangement of features, whereas voting is more about the hypothesis at this location. When the voting is taking place, the columns don't have any sense of where they are relative to one another, and they need to. We didn't show that in any of our papers, but we talked about it in Monty—every column knows the position of the sensor relative to the body, and this is still being accounted for. That's been discussed. I even have it in the Thousand Brains book I wrote; I was saying it's these little agents in the town, but now the agents know where they are relative. I remember that bit. We never implemented this, but it is a core piece of Monty. Every module, every cortical column, knows its position relative to the body, because otherwise it's just going to become another bag of features.

This allows it to work whether it's touch plus vision, one cortical visual field versus another visual area—it all works out. At the end of the day, you're estimating the orientation, the pose of the object relative to the body, and you know the pose of the sensor relative to the body. Now you just have to figure out the pose, and you have the pose of the sensor relative to the object. Those are the three quantities you're working with.

I'm just thinking out loud, but I keep coming back to the idea that there's a good chance a lot of the orientation is being handled in the thalamus. One of the things about the thalamus is that it allows many large areas of the cortex to be processed at once because it's all close together there. It's not as distributed; this large area would be one, but the equivalent processing center in the thalamus is close enough that they can all be connected directly. That would imply that the orientation resolution is being applied equally to all the columns. It's not like each column is independently solving the orientation; they're solving it together.

You can have multiple hypotheses, but only one is tested at a time—something like that. We thought about the processing speed; it seems like it should be all at the same time, just specific orientations. I wasn't thinking about speed, just that the thalamus is a way of applying something quickly to all the columns. It suggests that individual columns aren't independently trying out different hypotheses about orientation. There's going to be a single attempt that applies to everyone. If that doesn't work, there's another one. It moves away from the classic voting on orientation, more towards everyone contributing a guess, but the thalamus tries one. It's not going to multiply the hypotheses at the same time.

The paper I read might have been more of a hypothesis than anatomy, but the notion was that for the LGN, a certain amount of edge detection was going on. Does that correspond to your thinking? No, that's not related to what I was just saying. I'm not going to make a relation, but is that the case in your knowledge? No, actually, it's not the case in my knowledge. I wouldn't be surprised if someone made that claim; it seems like someone makes a claim about everything. The general view is that it depends on the animal. I think they've discovered that in some animals, but in primate V1, or even cats, they have not seen that. They see only center-surround responses. The paper went on about image processing, suggesting you could come up with the notion of orientation by voting on the number of edges in a particular orientation. I think in primary vision, you don't have that; it's just center-surround. That doesn't appear until you get to the couple and see it gets the fortune. I think that's a pretty solid result. It would be a nice voting result.

I don't see it as voting necessarily, and I don't think the thalamus could vote. It's more like it's just doing a transform on the input. I agree; I wasn't saying voting was occurring there. I'm saying they're producing inputs that could be voted on. You can produce the edge in the thalamus, the retina, or the cortex. If you're trying to center the notion of orientations on the thalamus, I think the thalamus could test or reorient the input and give you a chance to try it out—like, what if I rotate this thing 30 degrees? Then the cortex looks at it and says, that looks normal, right? It's more of an implementation of a deorientation mechanism. We don't have a necessary mechanism for hypotheses to produce; the cortex would have to provide those because it's through the perirhinal form.

I'm always struck by how your head is constantly tilting left and right when you're reading or doing anything, and you don't even notice. It's totally compensated, even though the input on the retinas is getting twisted as you move. There must be a mechanism for compensating for orientation changes. You'd think this would have been discovered a long time ago, as animals started moving around. It would be a fundamental mechanism to overcome orientation differences. Now we're trying to extend it to complete objects, even upside down. Talking about animals, how old is the thalamus, evolutionarily speaking? I don't know that.

It's very tied in, at least in mammals, to the neocortex. I don't know about birds. They seem to have, on a physical level, a compensation for body orientation. They're very good at keeping their heads straight. I would argue the opposite; birds like pigeons have their heads going back and forth. People ask how they see, but the perception of the bird is that the world is stable. It's the same as our eyes moving around, but we don't notice. It doesn't bother us. The pigeon's head goes back and forth, but the world seems stable to them, just like a saccade for us.

There seem to be strong homologues between bird structures and cortical structures. There are these blobs that seem like a section of cortex in some ways. I think there are similar neural mechanisms going on, even though birds don't have a cortex. Multiple people have made this analogy, calling them blobs in some of these programs, suggesting they might be in some other cortical structure. But they're not all that many; they're just blobs.

The last part of the presentation is to go through the patchwise representations. In particular, this is an issue not just for views, but in general, if we've learned nodes or viewpoints and have features associated with them. What happens when we sample at inference nodes we're not familiar with, and how do we handle that? This is just an exploration. Are we seeing objects or parts of objects we've never seen before, or are we seeing them from an angle we've not seen before? For example, you've primarily studied principal viewpoints, but you may not have seen it from a particular angle, or it could be new features—just a different orientation. Exactly. I thought that's what we were talking about all along.

When you learn objects, you associate features with certain viewpoints, but ultimately, it's a semi-discrete representation of space, whether it's a grid mesh, a view sphere, or even a 2D plane. You need a way to handle the continuous space between these points where you actually subscribe to memory—a particular representation. There's a mechanism for doing this. I'm going to focus on the feature representation and how to make it robust or able to handle this. The goal is to avoid learning every possible orientation of an object. While that might allow recognition from any orientation, it's not efficient in terms of memory or learning. How can we subsample the space and still generalize? We seem able to interpolate features, even for completely novel objects. I'm assuming you've never seen these particular objects before, but you can make a reasonable guess in your head of what you'll see if I move them. It probably wasn't what you predicted in your head—it was likely something more like this: the ring and the fur continue. You may have also predicted more of those little bumps. The basic idea is that, even for novel objects, we can do a reasonable job, given a set of local features, of predicting a nearby feature.

There's work from DeepMind in 2018 on a related problem. They had a simulated room full of 3D objects, and the agent received multiple viewpoints—this view, that view—and their orientations. Its task was to predict, given an arbitrary viewpoint in the room, what it would see from that angle. It could learn to do this using a deep neural network. The idea was that it developed a representation invariant to particular orientations and able to process the orientation to give a particular viewpoint. The thought is to learn a long-term function that approximates, given a few local viewpoints, what you would see at an unknown point. This can be used with temporal memory principles.

In an experiment like this, there are two ways: one is to give it a very small set of images and let it build an internal model that allows it to rotate the object; the other is to give it a huge number of images, so the novel one is just a little different from one it saw before. Which is it? It's probably a combination of both. They showed examples where this worked with objects not seen before, and it could generalize to some degree for different numbers of objects, but it eventually broke.

How much training does it get? I think a lot—millions of images. We're trying to find a more mechanistic way, with internal representations for orientation and location, which I believe the brain does. I don't think deep learning networks have anything like that, though some practitioners claim they do. It makes a big difference. The example we use is: you walk into a room, see the arrangement of furniture, leave, then approach from a different direction. You can imagine what the room would look like and recognize it right away, even from a novel perspective, like the ceiling, which you've never seen. I can do this for very novel situations. I don't think an infant has that capability—you have to learn it. But I don't learn it by being presented with millions of different situations. I can learn this in a novel situation from one direction; I don't need to be shown many directions. I just do it from one direction. I don't have millions of training examples—no training at all.

Representing space: there's a mechanism in the brain, learned or evolved, that doesn't really matter. If I present something new it's never seen before, give one presentation, quickly scan it, then come from a different direction, I recognize it and can make predictions. I didn't do that by receiving knowledge of those rooms. He's saying this system can do that, but only to a limited extent. It's much more limited and low level. What I'm trying to show is that, for a completely novel object, just given continuity of color, texture, and edges, you're able to infer what you'll see. It has nothing to do with familiarity with the object or how it changes—it's about low-level statistics.

I would object to this particular example, just to be clear about the one on the left. I clearly envisioned it being a ring, and I know what rings are. If it was even slightly different from that ring, I would know it was wrong. On the one on the right, I can't make any predictions about the individual bumps and features. All I can do is make a general assessment—if you show me a different arrangement of bumps than that square, I wouldn't know what's wrong. So there's a difference. That's fine. What I'm going to describe—it's fine if it was a different arrangement of bumps. What we don't want is for the predicted representation to be as meaningless as the smiley face; we want it to be something meaningful. The one on the left is not based on statistics; it's based on the fact that I recognize a ring. If it were based on statistics, I might just as well assume that the brown fuzzy stuff is where the block is. I don't argue against that. I agree. The problem is, at some point, we need to deal with the fact that we're sampling space discretely and need to interpolate between points. When I think about these things, I try to be careful.

It may help if I motivate a bit more what this is trying to do. Of course, this is a coarse example, but let's say when we studied it, we'd seen the teapot from the view of the handle and the view from the lid and memorized those. Now we're getting a view from a new angle, and we're trying to estimate what we'll see there because there won't be anything written to memory for that view. If we just try to use the nearest node we have in memory, that may not work well. Here, this is a coarse case where we have a feature represented here and a feature represented there, and some in between. The point is, there will be many situations where we move off the learned points, and we want to estimate what's there. For example, in this case, this is where we currently believe we are based on our history of inputs. In memory, we also have this representation of the lid at this pose. The question is, how can we use this information together to make a guess about what the view will be? I'm suggesting using a neural network, which is a function approximator, to approximate the mapping given these data points—the views and the poses, both from memory and from our input history, as well as the pose of where we're going to be—to estimate this possible viewpoint. When we move there, we can compare the internal estimate to what we actually experience, which can give us a sense of whether we are actually at this query point or if this is completely wrong and we're potentially on another object.

The key point is how to train the network to approximate this function. I think it would be useful to combine ideas from both contrastive semi-supervised learning, which is what these top two terms are showing, and a term more like what the DeepMind paper was doing. To break this down, this is the loss function—the neural network is being trained through backpropagation to minimize it, and it has three terms. The gamma parameters are scalars that determine how much influence a particular term has on the whole equation. The first one, simim, is a similarity measure where a higher value means two representations are more similar.

The first term is about getting the features to be invariant to noise. This isn't about interpolation; the first two are about having useful features generally, because interpolation isn't the only problem to handle. This means you get a viewpoint, perturb it in some way—add Gaussian noise, flip it around a mirror axis, change the color slightly—and you want the representation to remain relatively similar and invariant.

The second term says that you want representations of different features to be different. If you take a viewpoint at one point and a completely random viewpoint, ideally from a different object, those representations should be different in feature space. F is a neural network as well; this is after the input has been transformed by the network, and the representations should be different.

The last term is key for interpolation. You have another neural network sitting on top of the first one, taking these transformed features and their poses, potentially taking an arbitrary number (up to K) of representations, and the query point. You want it to output what you will see at that viewpoint. Based on recent developments in machine learning, something like this should develop representations that can do a decent job of using a few viewpoints from memory and from what we're actually seeing to predict what we'd see in an unknown space. It could be interesting to see whether introducing sparsity into these representations could help make compositionality more direct. For example, a handle and a lid are very different; ideally, the representation that combines them from a novel viewpoint would have both aspects, and sparsity may help with this. I wonder whether we want to move away from SDRs and use real-valued neurons, at least for this point, because SDRs cause problems with gradients—you can't do gradient passes through binary representations. It could be a mix of SDRs and real-valued neurons.

Connecting this back to earlier ideas, the fact that humans have preferred views could be a parsimonious way of capturing useful views without storing all possible views in memory. This interpolation function could then handle more novel orientations.

It's important to note that although this is based on long-term learning and deep learning, it still depends on path integration and rapid learning with heavy-style feature representation. So it's a mixture of the two.

At the neural level, if we're implementing this as columns, it may be that if you move somewhere and there are no known features based on the movement, a lack of dendritic input initiates this kind of interpolation, which could be more top-down feedback. One thing I haven't included in the interpolation equation, but which might be interesting, is a global hypothesis about the particular object, which might also constrain or inform what you're going to see at a particular pose. This might benefit from something like the continual learning dendrites paper approach, where you have context applied to the neural network.

The main difficulty is what happens if inference starts at an unfamiliar point. I've described how this works if we've visited nodes that we are familiar with, but it's not clear what to do if we start at a new point.

This is most important for the first instance or sensation. What you could potentially do is interpolate location: given your sensory input and stored features in memory, you could find the K nearest features in feature space and interpret the pose by weighting the contribution from each feature based on similarity. For example, if we're using particle filters, the estimated pose would be probabilistic and wouldn't have to be a single point. This would allow you to start path integration from a point on the sphere or graph without having visited it before. It might be computationally slow to compare to all possible features, but hopefully, you'd only need to do this for the very first sensation, and you could set a threshold so it's only triggered if the first sensation is very unfamiliar. If it's familiar, you just associate your location with the node linked to that familiar feature.

If something feels unfamiliar, why not just move and try to find something familiar? You can do that as well. That seems like the simplest thing to do.

A lot depends on how sensitive the features are in practice to matching, and how close you have to be for it to count as familiar. There's always the question of how to deal with variances, but as a general principle, if I see something I don't understand, I keep looking around to find some part I do understand. That would avoid this slow process, mostly for the first instance.

That's everything I wanted to talk about. The next question is whether we want to, at least for the HCM work, do something like view spheres or planar views. To my mind, planar views would be useful for translation invariance, but I think it will require some form of sphere-like coordinates to deal with rotation invariance.

That's everything. Thanks very much.