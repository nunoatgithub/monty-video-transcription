There wasn't a specific plan for today other than continuing the brainstorming on the various tracks we've been discussing. I guess there were some write-ups that Jeff and I shared, and Viviane, you mentioned you had other material you might be interested in discussing about hierarchical behaviors. She also put together one slide.

I want to talk about your proposal, Viviane, the one you posted today about driving inputs coming from orientation. I don't understand that; it's different from the slide I put together. I've been thinking about this topic a lot, about these higher cortical columns and what drives them. I don't know if we want to jump right into hierarchical behaviors, but we could. Sometime today, I want to go back and understand your proposal better. I'm up for starting with that.

Maybe you can bring that up today. That might be better to start with, so let me load that up.

Are you doing that? I'm not sure what's driving everyone else's thinking, but in my mind, we have a lot of uncertainties about what happens in the higher order regions. We have a pretty good idea of what's going on in the first and second sensory regions, but the higher order regions are still a bit of a mystery. We're trying to come up with a language that describes what all cortical columns do, and that language should encompass higher order regions and things we think they might do, which we don't know very well. That's the general goal here. I've been exploring one idea: maybe all cortical columns get some sort of sensory input, which would be helpful because that's how we describe our columns. Last week, we looked at the basal ganglia to see if it could provide that kind of input, but we didn't reach a conclusion. Then you have this proposal, Viviane, which I'm not sure I understand. Maybe we can discuss it. I wasn't talking about this particular diagram unless you want to start with that one. I thought I'd show this one first because it's the thing we all agree on. I thought this was a nice diagram for everyone who isn't as familiar with sensorimotor concepts around receptor fields. I'd never seen it visualized like this, so it was nice to see how layer six can—first of all, we have center-surround ganglion cells as input to these LGN relay cells, which then get pooled into a representation of an oriented edge, which then drives neurons in layer six. The nice thing they show here is how input from layer four can be modulated by layer six, turning this receptive field. I thought this showed how the receptive field is enhanced, not turned. This is just showing enhancement.

Enhancing, yes. I think the idea of changing orientation is new to us. I've never heard anyone else talk about it before, but if I'm wrong, that would be great.

I've always seen it described as gain modulation, just enhancing or suppressing the alternative representation. If you look at the lower right, you see the before and after; it's a cleaner result.

You could read those words to the preferred orientation.

I think they're just saying there's an orientation and it's cleaning it up. It's not as strong as what we're discussing—not even close. They're giving a minimal role to the thalamus, just shuffling the papers and straightening them up a bit, not doing anything particularly useful. It shows the arrangement of things, but this picture makes no distinction between layer six A and layer six B, which we know is a significant distinction.

The main reason I was looking for a picture like that was to visualize how the center-surround cells get pooled into an edge representation. The arrangement down there is nice, and it also shows the layer six backward projection and population activity.

Anyway, to the main diagram I wanted to show—the resolution is a bit low, but I hope it's okay.

Maybe Eric, let me share it in a different way. I don't know if the resolution looks as bad for you as it does for me. I can read it myself, but I'm not sure about others.

Let me check one second.

While you're finding that, I could follow up on the initial point. Oh, you got it. Can you see my mouse cursor moving around? Yes. That's nice; I can move my cursor on my laptop and my iPad screen. This is the classical diagram we all agree on: direct sensory input goes through the thalamus to a cortical column, layer four, and can be modulated by the layer six B projection. I think we all agree on that, but let's be careful. That diagram has two projections, and this is the one defining minicolumns, the one defining orientation. We're not dealing with anything else.

With orientation, it goes up and activates a set of minicolumns for different orientations observed on the retina. That's why I'm drawing the arrow at the upper part of layer four. Just to be clear.

Another thing we all agree on is that this direct sensory input can also go to hierarchically higher regions, like V2. The three columns shown here are supposed to be hierarchically arranged, with the one on the left being the lowest. There's a detail here I'm not sure we resolved: whether the green arrow going up to column two actually came from the LGN or from the next region up in the pulvinar.

I used to think it would come from the next limbic region up, but I believe some papers suggest it could come from a higher-order thalamic nucleus. I like that idea, though it may not be correct. I think I read about this in the shallow brain hypothesis paper. Someone recently showed a paper indicating a V2 column gets input from the LGN, which didn't make sense to me. My memory is that we found evidence for both: input from LGN and higher-order nuclei, but probably more abstract sensory input comes from higher-order thalamic nuclei. Maybe it comes from both. In the Thalamus book by Sherman and Gallery, and in their papers, they discuss how the thalamus is arranged or divided by its projection targets, with every neocortical region projecting. That was my working assumption for decades, but then we saw a paper—maybe the one Neil mentioned—saying LGN projects to V2, which didn't make sense. Let's ignore that data point for now.

Now, onto the new part: the hierarchical region that doesn't get direct input from a sensorimotor source. This adds complexity, but let's start with the simpler case. This region could get the orientation of the child object and the parent object. As discussed before, we can get the child object orientation and the parent object orientation, and the system can calculate the orientation of the child object relative to the parent. That can become input to the minicolumns here. It could be the only input if there's no sensory input, or an additional input if the column also receives direct sensory input. The mechanism is the same: the orientation of the child relative to the parent, similar to the orientation of a feature relative to the parent object. In this case, it's the orientation of a child object instead of a sensory feature like an edge.

For the third column, if it's not observing something from the retina, we've assumed it infers an object because it needs to know the object's orientation to pass back to the system. But how is it observing or inferring an object? If I knew its orientation—the rightmost orange-yellow arrow at the beginning represents the orientation of this column. Typically, layer 6B encodes the orientation of the object relative to the sensorimotor system. In this case, we don't have that, so it's just a hypothesis at the beginning, similar to the lowest column, where we orient the incoming feature. The mechanism is the same, but the incoming orientation is of a child object, not a sensory feature. At the lowest level, we don't know what it is at first, so we send down a hypothesis and rotate it. If I'm not getting any input, what's this third column modeling? It's modeling locations of child objects relative to each other. For example, if it models a face, it would model the orientation and location of the mouth, eyes, and nose relative to each other.

In that example, we would move our eyes to look at the eyes, nose, and mouth, so there would be motor output from this column directing the eyes to those features. Even if I'm not getting input from the retina, I am still driving the retina—moving it, but not getting direct input. It could still have motor output that moves the retina to another child object on the face, but the individual components like the eye and nose would be recognized in the lower-level column from sensory inputs.

I'm still trying to internalize this. I'm intrigued by it for the same reasons you like it, but I'm still struggling to fully grasp it. The reason I like it is that it's basically the same mechanism as used below. We've discussed how features from the sensorimotor system are treated similarly to features from L3 to L4 hierarchically—the child object becomes a feature on the parent object, and the child object orientation becomes a minicolumn input on the parent object. You mentioned there's no reason to have a spatial pooler in this third column. I'm not sure why you said that.

I guess it depends on where we say the spatial pooler would be located. I was thinking of the pooling happening in the thalamus, where we have all the gang no surround cells coming in. I don't think so. The edge representation, to me, the patient pool is always the double bookcase cells that define the minicolumns. Let's put that aside for a moment.

One of the things I've been thinking about, and I didn't really write it down or articulate it yet, is how we learn abstract things. I was working on a hypothesis that abstract things are still tied to some sort of sensorimotor abilities or sensors. When I learn mathematics or anything, even right now, we are defining abstract ideas about the cortical columns and using pictures to do it. There's a visual element to these abstract concepts. We're talking about spatial pooler and these connections, what they're doing. We're doing abstract thinking right now, and yet we're presented with a visual input. When I think about things, I have this sort of visual mental model, even if I can't always articulate exactly what it is, but there's a visual aspect to my thinking, spatial arrangements of things. I was questioning whether abstract thinking is really going to be based on the same sort of sensory-based concepts we've already developed, which would be consistent with Mountcastle's proposal and the common learning algorithm. Here, it would still be grounded in sensation.

I like this because I was having trouble making that work, but you haven't separated the sensorimotor or vision from all of this. You've still got the motor sense of vision, but you've gotten rid of these sensory input senses of vision, or at least directed sensory input. The child objects are ultimately grounded in sensory input. Of course, but this particular column on the right is not getting direct sensory input; we still get movement information. We still have relative locations in that. This is the thing I was confused about and wanted to understand because it seemed right. I thought originally you were divorcing this completely from sensory input, like from the eye, but you're not. We're still moving the eye, we still have motor commands, and we still have a reference frame that's moving around, but we're just not getting direct sensory input from the eye. That's a simple idea I didn't understand.

I was confused by this because we would still get motor input going everywhere. Maybe the first time you said this, you used language that tripped me up, like saying we no longer have to have input directly from the eye. I thought, wow, okay, but that's not true. We do have input from the eye, but it's motor input, so it's a hybrid as you pointed out.

Okay, I didn't understand this before, but I think I understand it now. At least I understand enough to think about it myself and play around with the idea. It's an intriguing idea that when we learn mathematics or anything, we make diagrams like this. Even if I learn something through language, I was coming up with examples where I could teach somebody something through language without a visual input, but I typically would, or at least often, describe it through language using visual scenarios.

The language would describe something physical to teach an abstraction. So the question is, are all of our high-level thoughts really grounded in vision? I'm not sure the same mechanism would work well for touch or sound. It may be that all of our abstract thoughts are derived from visual processing of some sort. Why would that work for touch? I was trying to come up with examples of learning abstract thoughts through touch, but it's hard to think of. I guess we use somatosensory descriptions a lot with abstract concepts as well. If you can grab something mentally, it feels like the key thing is if there's movement involved. Sound and smell, at least for humans, don't involve much movement, so we don't generally have that many reference frames. I was trying to come up with real examples of learning abstract thoughts through touch or communicating them by describing touch sensations. Maybe there is, but I couldn't think of it. We still have abstract thoughts. Maybe if you're not born blind, you rely more on the visual sense, but if you are, you still use somatosensory input. Maybe if you're trying to describe a texture, like fur, you could say a mathematical concept is fuzzy, but that could be visual too. I ran into a wall with my thinking. Maybe, but even then, I picture it visually. Maybe in the end, I should say it is envisioned, maybe I'll back off on that. Blind people can think abstractly, that's certainly true. Perhaps all abstract thoughts are based on or derived from physical models of the world that we can sense. That's where it starts.

In that case, it doesn't matter if you touch or see it, as long as you're building up the same model of the world. Sometimes it's easier to build those models visually, much quicker, because you can glance around at many things without touching them and see their arrangement. A blind person can't do that; they have to physically touch things.

That's a bit of an abstraction, at least for me. It might be helpful to think about the world we start with, and in hindsight, it sounds obvious, but it wasn't obvious upfront. We start with a model of the physical aspects of the world, and somehow we continue to build abstract concepts with the idea of them having a physical structure underneath.

In the case of vision, even abstract thought can involve visual elements, such as the images we're looking at now and movement relative to those images. We ground all abstract thoughts in physical models of some sort. This idea isn't entirely new; it's something we've considered before, but I'm still struggling to gain a deep understanding of it. I can articulate the concepts, but I haven't reached a point of complete clarity. This discussion is helpful.

This perspective is helpful for me.

To add another layer, as you mentioned, we receive feature input like color at lower levels, and at higher levels, we get object IDs through layer three to layer four projections. For context, I was trying to understand the analogy for the blue arrow versus the green arrow at the lowest level of sensory input. Why are some inputs rotations and others not? That's what I was exploring.

Some inputs are driving and seem to be transformed by the thalamus. Can we back up and show the picture again? I prefer to see the image while discussing it.

I removed it too quickly. I don't want to discuss it without seeing the image. We had some clarity: the rotation input in higher hierarchical areas is represented by the green arrow, while the blue arrow is a bias in input, not the main driver for L3 to L4. It makes sense if, at the lowest level, some inputs are orientations important for robust recognition, while others, like color, are less critical. Sometimes color is an edge, sometimes a texture or pattern. That was my point—maybe one is model-free, representing various perceptions, including emotions, while the other is derived from center-surround and becomes orientations. Everything makes sense except for the blue arrow on the left.

The blue arrow on the right represents SDRs, which can indicate an object or behavior. It's independent of orientation and doesn't require transformation; it's a bias indicating observation of a feature or object in a column, influencing what should be seen in another column. Because it's an SDR, it can't be rotated.

So far, we've assumed every input from the retina and thalamus is a center-surround receptor field. This makes sense, but does it have to be? Could some inputs be center-surround and others not? My understanding is that every cell passing through the thalamus is a center-surround receptor field; ganglion cells from the retina are all center-surround. That could be incorrect, but that's what I've read. I like the center-surround concept because it only activates when something happens—there's no point in reporting inactivity. A uniform field of illumination won't activate a center-surround cell, regardless of color or type. That's my understanding.

It's hard to explain the blue arrow on the left, which comes from the thalamus and projects to the mid or distal dendrites of layer four cells. The green arrow drives which columns and minicolumns are active, encoding orientations at locations. The blue arrow helps differentiate; it's a bias, not a driver. The blue arrow is not a driver yet. How would a center-surround field make sense for that input? It's substantially different from the blue arrow on the right.

I think it would be interesting to revisit the anatomical evidence and consider how it fits with other inputs coming to layer four, since there are many possibilities, including potentially the superior colliculus. One paper I found didn't have much detail, but it explored how color blobs respond in animals viewing a uniform field of color. Even when the entire visual field is filled with color and there are no edges, you still perceive the color. They explored how this triggers blood cells. My understanding is that if you were immersed in a uniform color field, you would be aware of the color when it first appears, but over time it fades away. The current dogma is that all information content neurons or axons going to the cortex pass through the thalamus, with no exceptions. There are matrix cells projecting to layer one and other cells projecting into layer one, but everything else goes through the thalamus. As far as I know, everything that goes through the thalamus is center-surround. If that's correct, we need to understand what's happening for the blue arrow on the left. At Menta, we originally thought all the cells in the minicolumn would be driven by that blue arrow, and the synapses on the layer four cells would be near the cell body, but they're actually far from the cell body. So, whatever comes from the retina to layer four cells is not driving those cells. I'm just reminding myself of that.

One interesting complication is that, at least in the case of color blobs, they're defined by not being in layer four—they're above and below. You probably need to switch to Neil's screen. I can see it now. Do you mind if I stop sharing my screen? I had a couple of questions about Viviane's diagram. I was just quickly showing this, but we can go back. The column doesn't exist in layer four, so what happens there? Are they just not color sensitive?

It's a good question. I don't know enough, but I know that often cells are thought to be insensitive to color, but the more they're studied, the more it's clear some neurons respond to both edge and color. Classically, color blobs are not found in layer four—this is dry eight V1. Is it clear from this diagram that 4C beta is not color? Is color only starting in layers five and six? I think it's meant to be five and six. That's interesting, though ambiguous.

Sometimes color blobs are shown everywhere, so it's hard to know. I remember sharing a diagram that made it look like they're only in layers two and three. It seems worth going back to the original papers to understand what's really going on with color blobs. I like the idea that if an input is center-surround, it could detect changes or frequency shifts, not just uniformity, and the same could apply to colors. One way to think about color blobs is as another sensorimotor system, co-located and moving with other sensors. The color blobs could be a modeling system based on color, and there's a separate modeling system not based on color, but both work on edge or change detection. There are two parallel systems running. I like the idea that center-surround is a fundamental element, and everything is built from that, whether it's center-surround color or non-color.

Let's go to Jose's question about the diagram. There are questions about a couple of arrows, like the light green one going from the eye to the higher order thalamus.

We've talked about retinal ganglion cells going to the LGN to V1 or V2, but I wasn't aware of direct sensory outputs from the eyes going to higher order thalamus. Isn't that what we discussed earlier, Hojae? We know V2 gets information from the retina, right? So the question is whether retinal ganglion cells project to LGN or to a cortical thalamic region dedicated to V2. V2 and the pulvinar. I thought the idea was that LGN projects to V1 and V2, but we were discussing earlier that the classic description, as Viviane mentioned in Sherman and Gil's book, is that the retina projects to a region of the pulvinar, which projects to V2, and the retina projects to LGN, which projects to V1. Niels mentioned a paper suggesting V2 gets input from both LGN and a higher order nucleus. So I think the light green arrows shown are the more standard dogma of what's thought.

I think an LGN to V2 projection was newer, but as Niels suggested, maybe copying too, though we didn't understand that. Does that make sense? Yes, that makes sense. We just have to go back into the literature to check it. The basic idea is that whether or not this data is now here or we move it over to the right, it's still input that goes into this V2 column.

The second question is about the red arrows, the movement. Is this simplified in the sense that the eye movement goes to the superior colliculus first, then to the thalamus, and then to the columns? Or do the movements go from the eye to the thalamus, especially lower order? I thought it would go to the higher orders and then to the columns instead of stopping at the relay cell area. This represents the magnocellular cells getting input from broad receptor fields in the eye, representing movement of the eye. These cells project directly to the thalamus and then to cortex. The eye movement itself can be detected by the cortex; the cells indicate the eye's speed and direction. There are other sources of movement information, such as movement output from the column that goes to subcortical regions, which then goes to thalamus and can be rotated to provide information. The vestibular system also has strong projections to LGN and the pulvinar, providing signals for movement, such as head rotation. Other sources can provide movement signals, but the relay cells represent movement and go through the thalamus because the movement's orientation must be changed based on the object's orientation to the retina. The source of the movement command could come from the eye, the superior colliculus, or the pulvinar. There are many possible sources, as shown in one of the earlier diagrams in the paper, which illustrates multiple sources of movement information going into the thalamus.

Is the movement of the visual stimulus also accounted for in the red arrow, meaning the object itself is moving? If something is moving, those cells would be the input to the shorter receptor field. Magnocellular cells would then go to layer three for the behavioral model.

If magnocellular cells are pooled over a large area of the retina, that means the retina is moving. If the entire image on the retina is moving, the retina is moving. If only a smaller part of the retina is moving, the retina is not moving, but the object is moving. That is not shown on this diagram, as Viviane mentioned. That would be another orange-red arrow going into the layer between layers two and three.

That's part of our movement model, indicating that something is changing in the behavior model. We can learn the model of its behavior because we see it's changing. I know this is a lot to keep in mind, especially if you don't think about it every day, but it does hang together pretty well.

I have a question about the connectivity between V2 and V4. Between V1 and V2, it's like a one-to-one connection between columns in the hierarchy.

Not exactly, but it converges at V4, so the blue line would represent multiple arrows from many other columns. There are different connections, and we can discuss whether they're converging or not.

This is best examined in the hierarchy paper. The compositional models require columns that are one-to-one. If I want to know what is at every location on the coffee cup, I need to know the child object at each location. Two columns, one hierarchy above the other, can correlate what each is looking at on the cup and the logo. Some projections should be broader. The blue arrow between the second and third column could represent many columns in V2 simultaneously voting on the same object, so it doesn't have to be one-to-one. It could be many columns in V2 to many columns in V4, and you do see that kind of spread. Does that make sense? Yes, thank you. I'm amazed it can make sense; it seems so complicated.

Any other questions, or should I stop sharing? One thing I thought would be interesting is to think more about movement in abstract spaces.

I was considering how important direct motor input is when things get more abstract.

Maybe in that case, we could have a motor output signal, the brown arrow, but not send it subcortically to control a body part. Or we can inhibit that control when we want to. It's common that when people think in abstract spaces, they move their eyes or gesture, but if needed, they can force themselves not to move. I don't know what would be driving it in this case, but if the high-level column is in an abstract space, moving through it would activate different neurons, like grid cells in layer six, which can recover new representations in the layer below. You can move through that space.

I was thinking about the proposal that there is no abstract, but we're still moving. 

Jeff, you're quiet or sound far away from your computer. I was, but I had my headset on while making coffee. Can you hear me now? Yes, now I can hear you well. I was only ten feet from my computer, using my earphones.

So my point—what was I going to say? Let me turn my video on.

Going back to our earlier discussion about Vivian's proposal, I was saying that movement is still in visual space—the features are not, but the movement is. Now you're saying that's different than how we used to think about it. Previously, we thought movement occurred in abstract space, but the current proposal suggests we don't have an abstract space; we have visual space. I'm confused. I thought it would be interesting to talk through this with some concrete examples.

Now we're reconsidering what we discussed earlier. I'm not saying we should abandon it; I think it's okay to consider alternate hypotheses. We haven't settled on anything. One alternate hypothesis is that movement, even in abstract thinking, will be visual or tactile in some sense. There is still a physical movement correspondence, even in higher-level regions, which is different from our previous view that concepts live in an abstract space and movements in abstract space are not physical.

I tried to make up some examples of that in my book, but I could be wrong. I don't mind talking about abstract movements, but I want to point out that it's an alternate picture from what we currently have. It's not the same. Let's keep going and not abandon the idea of abstract movements yet. I'm fine with that; I just don't know how to make sense of it.

I guess this is definitely not a well-formulated thought, but it's something we've discussed before: you can imagine flow and movement at the lowest level as bit patterns changing, with a flow to that movement where things tend to move together. If the input is from a hierarchically lower level but still from a fairly abstract object, that object can change, and that change could be a form of flow. For example, consider learning the concept of a parent versus a child and the movement between those. When you first learn about that, maybe you were a child in a room and your friends came over with their parents, and there was a discussion about who was whose parent. In that sense, it's initially in visual space, but maybe in a higher-level cortical column, it's getting represented differently.

Let me add a counterexample: when we want to learn about family trees, we almost always resort to visual images like genealogy charts because you can visually see the relationships. I would argue it's hard to understand the concept without being exposed to that. It might be more that, say, the third column here is at various times representing child and at other times representing parent, changing based on what you're thinking about or perceiving. As far as the fourth column is concerned, there's a movement in the form of a transition between those.

I'm not following this. Those lines are getting confusing. Did Viviane draw this? I drew this as Viviane was trying it. Let me make my argument. The circle is still supposed to be the Thousand Brains model. I was drawing a version where there's no motor input from the sensorimotor system. I agree that even with abstract concepts, we usually learn them visually or in some structured way that we sense, but inference or thinking about it can happen without actual sensory movement. For example, you can have visual movement without moving your eyes; I can imagine moving my hands without actually moving them. The question is not whether we physically move, but whether we use the same process as if it were visual space.

My grandmother was into genealogy, and the only way I could learn about my relatives was by looking at genealogy charts. It's too complicated otherwise. Even now, when I think about my family, I have a visual picture of it—almost like a chart, with relatives positioned horizontally and vertically. I always picture the older generation visually higher than the younger generation. In my mind, there's an image of relative position, almost visual, where generations go up and down and people within a generation go left and right.

For math, it's hard to understand concepts unless they're visualized well. Most things, whether calculus or vectors, are best understood with visual language. If it's simple, I don't need it spelled out visually; I can imagine it. But when it gets complicated, like the image we're looking at now, I wouldn't be able to have this conversation without the picture in front of me. Otherwise, I'd forget what's where.

There are people with aphantasia who don't think visually at all. How does that work? How do they know? They report not seeing any visual imagery in their mind's eye. They must have some imagery or concepts in their head; it's not just a blank. I wonder if it's more about the nature of consciousness—maybe those representations exist, but they're not consciously aware of them. How do you know they don't have it? It's through self-report.

Is this something you can diagnose and say, "This person can't do things"? It's a well-known phenomenon—a spectrum from very visual to zero. Is there a deficit? If you have this condition, can you not do certain things or make mistakes, or is it just self-reporting? Are they just as capable as everyone else and simply say they don't have any visual inputs or imagery?

I'm not sure. If there's no deficit, then I don't trust it. I have to think about whether it's visual, but it's not really visual. With parent-child relationships, I imagine the parents are above the children. That's an arbitrary way to think about it, but that's how our charts usually show it, and that's how I've come to internalize it. If I ask people about their parent-child relationship, they must have something they think about, but it just doesn't pop into their head somehow. Maybe they just don't. They would be able to draw a family tree or something, I would assume. If they're engineers, they must be able to draw concepts and project those models onto paper. If I had to draw a family tree, it would be much harder for me to draw it upside down. It would be easy to draw it the classic way, with grandparents at the top and going down from there. If I had to do it the other way, I'd have to think through each step.

We should be careful about generalizing from our experience of literacy and seeing charts everywhere. If you are literate, working, living with your family, and you don't draw charts or write, you could still conceptualize your family. Illiteracy doesn't mean you don't understand pictures or words. I think an illiterate person could have no problem understanding a family tree. They've probably seen them, seen signs, and get around; they just don't know words. What I mean by illiterate is not even bothering with signs, just conceptualizing by being embedded in living with your family. You understand who's who without having to draw trees. But you probably have a sort of impoverished understanding.

It would be very difficult to know. Introspection is dangerous, but it's also quite useful, so we shouldn't abandon it. Is there still some kind of visual input coming in when people learn about or think about these things? It would probably be very hard or impossible for a child in the kind of society you're describing, Tristan, to learn about the concept of family and parents if they only ever encountered children and parents in isolation. If a parent was described but they never saw them in the same room together, the concept of a relationship between these things would be very hard to grasp. By seeing them together and looking between them, you understand that one is older, and their behaviors tend to be stereotyped between each other—the parent picks up the child, scolds them, has that sort of patriarchal or maternal relationship.

Let's review. I think we all agree—knowledge is all going to be structured in reference frames. The only question is, are those reference frames grounded in or derived from physical sensors? Someone can build the same models through touch or vision; it doesn't have to be just vision or just touch. You can build the same models of the world through these things, but there is a structure to it, and there has to be a structure. It's impossible to think without structure. It's an axiom that knowledge is structured in reference frames. We're just arguing about whether those reference frames are still related to the physical sensors we use to learn about the world. The "que one" hypothesis says yes, they are, and therefore we'll have visualizations. Even if I don't have eyes, I will still visualize the relationships between things I've learned, because we all learn the same model of the world in some sense. The alternate hypothesis is that the reference frames used in high-level abstract thinking have no correlation at all to the physical world, and movement and sensory inputs are just completely abstract, which I find appealing—or maybe something in between.

With a family tree, it's very useful to look at it and perform those eye movements, but the precise eye movements are less important. The physical arrangement is less important than the abstract arrow that connects them. Their relative placement is very different from how you understand a physical object. The relative placement still exists even if it's not visual. No matter how I think about parent-child relationships, I put the parents above the children, whatever that means. It doesn't mean they're physically above; it's just some sort of imagination that there's an orientation to these relationships.

It's interesting how we learn to have a more abstract understanding of movement, where the exact placement doesn't matter. The relationship remains abstract but still correlates to the physical world, like an up and down orientation, which has a physical aspect. Even when considering parent-child relationships, I imagine them in terms of a physical relationship, such as parents being above children in my mind, and that's how it's taught.

Maybe a middle ground proposal is appropriate. As Neil says, in abstract space, movement seems more constrained or slightly different than in physical space. The most extreme case would be having no physical movement to learn these abstract reference frames. To implement this, I would take the layer five motor output from the lower column and send it to layer six at the abstract model column. Layer five of the lower column would learn to send proper movement commands to move through abstract space, or layer six would learn to interpret these layer five motor commands. However, layer five outputs are typically described as always projecting to something related to physical movement.

That's where the middle ground solution comes from. This could be the most extreme case, but it's really hard to learn and may not be realistic. I'm not sure what this case is. Basically, it's about learning an association of how layer five movement commands could be interpreted in abstract space. If we believe that layer five cells always project to something related to physical movement, then the output can't be purely abstract; it must relate to some physical movement somewhere. From an evolutionary perspective, maybe that was always true, and perhaps in humans, the part that connects to subcortical structures is atrophied or chronically suppressed physically because it doesn't matter anymore.

The belief, based on what we've read, is that every cortical column projects subcortically to a motor behavior system. Sherman told me that everywhere they've looked, that's what they've seen, but they haven't looked everywhere.

You could argue that maybe those projections exist but don't do anything, though that's a weak argument. It could be that these projections don't exist everywhere and that's incorrect. But if they do exist everywhere, it's hard to argue that the behavioral output of a column is completely abstract and unrelated to physical movement. You don't have to execute the physical movement; you can just think about it.

The middle ground solution is that it's useful to support learning of abstract spaces with actual physical movement, like looking at a family tree. There might be the possibility to send an actual motor command subcortically, but as Neil said, maybe that's atrophied or doesn't happen often when thinking through abstract space. You could inhibit it, but inhibition still means the physical movement is present and just not executed. That's fine; that happens. When people aren't holding themselves back, physical movement often accompanies thinking. There are studies about eye movements during recall. Even if the eyes don't move, it doesn't matter. The point is, in all kinds of behaviors, I can imagine picking up my coffee cup and what it feels like without actually doing it. The neurons that would fire are going through their motions, and I can suppress them, running the system without acting.

The real issue is whether there are truly abstract spaces with no correlation to physical movement or changing positions in a space corresponding to the world, or a space that can be explored by sensors. That kind of space is very hard for us to learn. I would say it's more the opposite: abstract spaces are usually simplified versions of physical space, with more constrained movement.

For example, family trees have very discrete relationships. When I think about family trees, I imagine images of them rather than where my grandparents or parents sat in the house. It's an abstract representation of a family, mapped onto boxes and arrows.

I'm not going to reach a conclusion. I'll state what I'm going to work on and then see if Viviane has suggestions. Others can take different approaches at this point. I'm going to focus on the idea Vivian proposed: movements still correspond, in some sense, to physical movements. We might imagine these movements relative to images, but they're still physical movements. There's no sensory data coming in, so I can switch from learning by looking at images to thinking about it without the image in front of me. I've established a reference frame from the visual image and can work from there. That's not the hybrid approach; it's all motor, and sensorimotor is sometimes sensors and sometimes internal representation. In the drawing, the dashed arrows indicate that when we think about this abstract concept, those are not active. We're not sending a motor output or receiving actual movement information. Instead, we move through the abstract space mentally. During learning, we would still use those to learn the abstract reference.

I'll go with that. It's a hybrid in some sense: movements are still related to physicalness, but the things you sense—the blue arrow at the top—may not be present. There may be no sensory input to layer four or layer three at this point, nothing equivalent to the blue arrow. The real mystery is the blue arrow on the left that doesn't fit in this scheme. That's how I'm going to think about this. I think it's a promising approach; it may be wrong, but if I'm going to spend some days thinking about this, I need at least one proposal to start with. That's the one I'll start with. Others can take different approaches. I wrote down some short, general neuroscience questions that I might post to the group in case anyone's interested in looking them up, or I might try looking up some myself. Do you want to show it now or just post it later? Give me a minute to clean them up, and then I can share. Do you want me to share my one slide while you're cleaning them up? Let's go. I forgot about that. It's really just an idea to throw out there, maybe something to think about more.

I tried to clean up the hand-drawn diagram I put in the object behavior write-up. This isn't anything new, just a non-hand-drawn version. In case we need an image to talk about, I can pull this up again. It basically shows the object model in green arrows with the static input and voting on object ID, and the behavior model with purple arrows, with changing inputs and an additional time input. Again, we can vote on behavior ID.

They have two separate reference frames that we move through in synchrony. As we discussed before, object ID can become a biasing input to layer four in the higher cortical region. What I want to talk about more is what happens hierarchically with behaviors. Can we have compositional behaviors, or is there a similar projection of the behavior ID up here? I have three thoughts on this for now, but I don't know what you think. I couldn't think of any good example of compositional behaviors where it's important to compose behaviors of other behaviors. There are many examples of behaviors being applied to compositional objects. We can have a compositional object, and different parts of the object can have different behaviors. I couldn't conceptualize a compositional behavior composed of other behaviors. I'm putting out the claim that there are no compositional behaviors; behaviors are simply applied to compositional objects. If you look at this picture, there's just the green arrow going up, and we have compositional objects being modeled here and up here. We would then recognize behaviors at locations on those compositional objects.

Maybe that's a radical claim. You caught me on something I wrote to Viviane, where I started listing tasks like making coffee—the example Niels always uses. She said that's not really compositional behaviors, just a series of tasks. She's right. I tried to think of composition behaviors, and nothing came to mind. Maybe it depends on how you define compositional behavior. There are situations where you need to decompose things or understand behavior as a series of actions.

Just like the way a phone works: turning it on is the first step to accessing a series of things. Going into an app is the first part of accessing a series of things. Each level is associated with different behaviors, but I don't need to understand or interact with all of them at the same time. I would define compositional behavior as taking the behavior ID recognized below and applying it to a location in the behavior model above. By applying, I mean learning an association. At this location in the parent behavior, I learn there's a child behavior. In the phone example, I would solve it as having a compositional object with different behaviors. Opening the app on your phone would be going to a different object ID, which then has the behavior of moving from one object ID to another, like from your phone home screen to the app home screen. That app home screen has a different behavior associated with it.

Maybe the coffee example is useful. The behavioral state of the coffee machine being on is important for higher-level planning, like brewing coffee, but it's not necessarily compositional. That information is still being passed up. Viviane mentioned the key element earlier: when we think about compositional objects, there's a child object assigned to locations on a parent object. For compositional behaviors, there would have to be a child behavior assigned to a location on a parent behavior.

Imagine a disc with an interesting pattern, say a letter A, that spins counterclockwise 360 degrees and then stops. When I see this disc, I know it has that behavior. I could attach the spinning A to the upper arm of a stapler. When I lift the upper arm, the A spins. I could also put the spinning A on a door, and if I turn the door handle, the A spins. These are examples where the spinning A has its own behavior. When I see it somewhere new, like on the stapler arm, I don't have to learn its behavior; I already know it spins 360 degrees. Now I know it's part of the behavior of lifting the stapler arm.

I was thinking about a similar example. Originally, I thought I could solve this by applying behaviors to compositional parts of the object. The A has the spinning behavior, the stapler has the opening behavior, and the A is the child object on the stapler. But now you're bringing in the idea that the child behavior is dependent on the parent behavior. If you open the stapler, then the A spins, and only then. Maybe it does need compositional behaviors. The key is that once I've learned the spinning A behavior, I can assign it to a new thing with its own behavior. That's the key to compositionality: you don't have to relearn it.

I still feel like I can solve it by having compositional objects and applying behaviors to the child objects on those composition objects. But the main part that made me question whether we need it is the dependency between child behaviors on the same object. The parent behavior would need to learn these dependencies. This is why the definition of compositional behavior is important. There are objects where you have one behavior for a child, another for a parent, and you might want to associate those two.

One way to describe that is compositional objects where each object can have a behavior. No one is denying that's important. The question is how behaviors might interact across the hierarchy, which is a bit of a gray zone. It's hard to follow these conversations, but I realized that the behavior isn't the spinning A; the behavior is spinning. The way we define behaviors, it's the changes in morphology or movement. The details, like "there's an A," are independent of the spinning behavior. The behavior is spinning, and I've applied it to an object.

Now the question is, does spinning in one section of a hinge count as a hinge behavior? Something is moving at some angle; we might call it a hinge. It's just the movement we're encoding, not what it is. I have another movement, which is spinning, but not a particular A—just the movement. Do I associate spinning movement with that? As soon as I say it's an A that's spinning, it's an object with a behavior, not just the behavior itself. These are complex things to think about. Maybe I was wrong with my spinning A example; maybe it's really about composition. I'm changing my view on compositional behaviors.

I think it might be useful to have the ability to associate a child behavior with a location of a parent behavior, especially to learn the timings of when child behaviors start and stop, and the dependencies between them. For a real-world example, instead of the spinning A, we could think about a car. The car is complex because it has many behaviors at different locations, and some of these behaviors are dependent on each other. If I turn the key in the ignition, that's one behavior—the key ignition turn—which starts other behaviors. Maybe a better example is the lamp switch: there's a switch on the lamp that we can turn on and off, and switching is the behavior. The parent behaviors are whether the light bulb comes on or off. We're confusing two things here that seem similar but are really different. One is true compositional structure, location to location—there's a behavior at some location relative to the parent behavior in the same reference frame. There's another series of tasks, like making coffee or turning the light switch on, where you're trying to correlate and learn causation between behaviors in one place and behaviors in another place.

If we define a child behavior as being assigned on a location-by-location basis to a parent behavior, as suggested in the diagram in front of us, that's a more limited definition of the child-parent relationship. Although I argued the spinning A was an example that showed this exists, now I'm reconsidering. Perhaps the spinning A is an object that has behavior, not just spinning. We switched positions. We'll talk again next week. Whoever posted the Wozniak Coffee Cup Benchmark—Neil's been using that too—maybe that'll become our thing. But that is not compositional behaviors, at least not in the sense we're discussing. That's something else, and we don't even have the language for it yet. We're trying to get the world in some state using lots of different behaviors, and that's different; it requires temporary memory. You have to remember what you've done so far and where things are. That's definitely more complicated because it's motor, policy, and planning—a combination of behavior and changing the state of the world. It looks complicated now, but we'll come up with a solution that isn't complicated; it'll be as complicated as our behavioral model. There will be a basic mechanism that explains all those things. We start with complicated examples, but the goal is to find the simple underlying methodology the brain uses and apply it in many ways. I think that's going to happen.

One parting thought on behaviors in hierarchy: I've been thinking about modeling object behaviors, but I keep seeing things like smashing objects together and hearing the sounds they make, or taking a pencil and drawing with it. There are many things learned about how objects interact. I think object interactions will naturally come out of the same mechanism at higher levels in the hierarchy. Where we learn more scene-like representations, we can learn object behaviors that represent interactions between objects. If I smash two objects together, what sound do they make? If I move a pencil over paper, how does the paper color change? If I pour water into a cup—this is what we were just talking about. This third bullet corresponds to making pots of coffee; it's a combination of things interacting. I was arguing a second ago that it's a different way of thinking, not compositional behaviors. It's just behaviors being learned on compositional objects or scene representations. I don't think it falls into the bucket of outputting actions to achieve a goal or putting the world into a certain state; it's more about learning a model of how objects interact. You have to do that to make the coffee pot—you have to learn how liquid flows and how lids open. That's part of what you need to learn, but I would say it's represented as a behavior model the same way we talk about behavior models within one object.

It might be something different. My first guess is that it's something different. Behavior models on an object can be structured things, like staplers and things that move and change, running.

It seems like these kinds of interactions between objects feel different to me. In general, when we've been talking about things like the stapler, we've been deliberately trying to avoid discussing causality and how things interact. The staple has always been described as the arm moving on its own. The missing piece is what causes the system to change, what causes object behaviors, and how you learn those associations.

I wasn't trying to bring actions into the picture right now or discuss how we move objects, but more about the stapler—if you push it down, it puts the staple into the paper, but it wouldn't block. That kind of knowledge, how it interacts with another object, and where that would be modeled is what I'm considering.

Here's how I've been thinking about this: how we interact with the world involves many complexities and things we don't understand. I suggested carving off one piece—the modeling of object behaviors. It's a small but challenging part of the bigger issue, and I think we need to understand this before tackling other problems. This is a foundational component.

We have a pretty good idea of how we learn behaviors, so now we're debating what's next. For example, if I'm going to make coffee, I need to know how the lid of the coffee pot moves, how the faucet on my sink goes up and down and turns—these are behaviors I have to learn, but that's just the entry level. Now we're debating how to approach the rest of the problem. I wasn't trying to introduce another problem; I've been thinking about it for weeks and realized it might be solvable with the same mechanism. I'm not talking about manipulating the world or learning causality, but about learning the behavior of two objects interacting—applying behavior to a scene representation or a parent object that represents both interacting objects. That could be true, but it gets complicated quickly. For example, a rubber ball hitting something is different from a ball of yarn or a steel ball. Throwing a round object at something introduces complexity; these attributes matter. It's not just the behavior or a simple change. We've always defined behaviors as changes in features and orientations of objects, but now it's unclear how we learn distinctions like a rubber ball versus a steel ball or a baseball.

I was thinking that the object model ID would inform which behaviors can be applied. If I infer a rubber ball, I can infer bouncy behavior, not the metal ball smashing the ground. We haven't discussed how we actually learn that. The combinatorial side is interesting, especially regarding computational behaviors—how a rubber ball interacts with a jello ball, for example. You may have learned about rubber balls and jello separately, but you can predict, to some degree, what will happen when they interact, even if you've never seen it before. So, is it the combination of their behaviors?

We have another week of brainstorming after the hackathon, which is two months away. Last time, our topic was understanding object behaviors, and knowing that in advance helped us prepare. I'm considering what the topic should be for the next session. We're circling around it here. Making a pot of coffee seems too ambitious for the next two months—maybe by the end of the year. For now, I'm wondering what would be a good topic. Object behaviors are meaty and self-contained; we could define and represent them. We don't know how we learn them yet, but we do know how to represent them, which is great. So, what should the topic be for two months from now? I'm putting this out there so that over the coming days, someone might suggest something, or it will become clear. That's the goal we should be aiming for, and then we'll have a better chance of solving it at that time.

It still feels like the object behaviors picture is incomplete. We've gone on a tangent discussing features, but it was important to develop clarity there. Hopefully, we can return to core object behavior discussions with more certainty, as in the stapler example. Last time, we had ideas, but then questioned what L4 and L3 are doing. That's why we went on this tangent. We resolved how to represent behaviors, but not how that representation translates to predictions about physical observations, like what the stapler cup will look like when rotated. There are missing pieces. That's the next big question: how do we make predictions when applying a behavior to a new object?

We have a good list of open questions collected in that document. It starts with how to make predictions and extends to using behavior models to output actions, like in the coffee pot example. I'm hopeful we can solve the prediction of the stapler top based on the behavior model before our next meeting. Maybe some of us should work on that. I don't think it's a big enough problem for me; I want something more substantial. Maybe I'm wrong and it's harder than it seems, but visually, the pieces are there.

We didn't finish it, so we should do that soon and then have a meatier topic for two months from now. Using behavior models to figure out how to change the state of the world would be a big topic, tying into causality and learning. But I need a specific example. The lamp ones are good because you need to learn the causal relationship between the switch behavior and the light behavior. That's a simple causality example: change the state of the world so the light is on. That's our task. We started practicing drawing lamps on the whiteboard.

I'm always conscious of autonomy, not that I have to run, but it's already after 10.

We need to finish how behavioral models interact with object models, and then move on to the lamp model, the switch, and learning causality. Does that make sense? Talking about that example might help us figure out whether we have compositional behaviors.

Also, thinking about how behavior predicts the morphology of an object might be related and could play into that.