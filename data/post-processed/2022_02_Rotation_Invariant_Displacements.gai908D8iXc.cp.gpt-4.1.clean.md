This should be pretty quick, about five minutes.

This is an alternative to using the 3D displacement I previously used to match graphs. Before, to match graphs, I used the difference between the previous x, y, z location and the current x, y, z location—the displacement between those two points in space. That displacement is not rotation invariant, which means you can't recognize the object if it's rotated using this representation. If you rotate the object, you rotate the reference frame, and if you don't know how it was rotated, it doesn't work. It's assuming a priori that you don't know the reference frame. Given that the displacement is relative to the object's reference, you can't do it. If you calculate the displacement dependent on the object reference, you could, but I'm assuming you don't know that. With this, it would still work without knowing the reference frame.

In the paper where they introduce this, for two points M1 and M2 with normals N1 and N2, D is the displacement I previously used, which equals M2 minus M1. The feature F is defined as the length of this displacement and three angles. The first angle is between the normal of the first point and the displacement between the two points. The second angle is between the normal of the second point and the displacement between the two points. The third angle is between the two normals.

Those four numbers denote the feature between those two points in space. My intuition was that the third angle is determined by the first two, but apparently not. The normal to the distance D is just a linear distance. D is the displacement, but for the feature, we use the length of this displacement—how far point one is from point two.

If you look at the picture, the idea is that these three point pairs would all have a very similar feature attached to them, or the same feature. But if you move this bar up, so you have this point and this point, the normals are still pretty much the same, so the angles would be similar, but the distance would be different, so it would have a different feature.

If D is very small, you have a representation given whatever the angle difference between the two normals is of one curvature. If D is very far apart and you're not sampling anything in between, the curvature would be less because you have a greater distance over which to swing that normal. The bigger D is, the less knowledge you have about the shape. As D gets smaller, as D approaches zero, you approach the instantaneous curvature of the figure. If we had a sensorimotor that samples at equally spaced intervals, D would always be the same and wouldn't be super informative. But if D can be different, like with Cades, it can have a lot of information.

If D was along the edge of the shape, it would be very informative—not a straight line D, but if every point was equally spaced along the curve, D would vary a lot and capture them.

That's the basic idea: use those four numbers to quantify, instead of the displacement between two points. That would be the attribute of the edge of a graph. Using that now solves the problem of detecting objects from new rotations. On the left side, this is detecting five different objects rotated the same way they were learned, and on the right, detecting them in a different rotation. It's pretty much the same performance. It works quite well. The bigger question is if we can assume we have knowledge about the point normals. Do we know, essentially, the perpendicular?

In the case where vision and touch are very different, with touch, you would know this in some sense. If you imagine your finger moving around the object, you inherently know where the surface is relative to your finger. Whatever patch your skin is touching is at a tangential point, so you would know this. If you know the local 3D curvature, then you have the normal. But how would you determine the local 3D curvature? I think a touch sensorimotor can naturally determine it. The question is if we could also determine this from a patch of vision.

Wouldn't need these points for teachers, but could you use that to get these normals? You were talking earlier about the curve. The tricky thing in vision is that it depends on various factors. We might be able to learn this in vision. If you had touch as your ground truth and you're getting these visual inputs under different conditions, could you do it? You could use your touch to train the vision. We could assume we know the ground truth here. Could you train the vision system so that, given a visual input, it could learn these normals? You can also use movement to train. Once you have a ground truth, you can move and see how it changes. As you move, you know how the angle changes, but the lighting and input will also change, though in a complex way.

If you have a point cloud, you can have the point normals. In the other work we discussed, we assume we have the point cloud, but the point cloud doesn't give you the normal directly. You can approximate it, and if the points are close together, you get a good curvature. But from a single point, maybe it's not too hard to do. I'm using a point cloud right now to get the normal. You asked how we could do this, but now you already have a way that assumes we have a point cloud, which might not be the case if we're just moving a finger along the edges and only get one point at a time. We don't have the whole point cloud to estimate the curvature. A finger moving along the edge intuitively feels like you know exactly where the edge is, its sharpness, and its orientation. Sometimes I'm sensing the normal. We should be able to get the normals from just one sensation instead of the whole cloud of sensations.

I think it's possible to cheat a little here. When I'm actually touching something, is there really one column in my cortex that's active? Probably not. I don't know if I can ever get just one column touching something, because the receptive fields on the sensory organ overlap. When you touch something, multiple columns get input, and they have short-term connections between them. It's very possible that we can integrate several columns instantaneously to get more information. When I touch something, I know what the curve is—it's not just a dot, there's an edge, it's oriented, and I can feel all that.

Using multiple neighboring columns could solve a lot of these problems, like detecting edges and curvature. It's clear that we are able to do that. I'm conscious of it; I can tell you what the edges and the surface are. I just feel it, so it's clearly there. We don't know exactly how it's done, but multiple columns could do it.

In the cortex, the lower layers have a broader receptive field than the upper layers. The lower layers look at a larger part of the object, and the upper layers look at a smaller part. One could argue the lower layers determine curvature because they get input over a broader area, while the upper area determines the feature at a specific point.

There are tricks like this. A single column acts like multiple columns in the sense that it's detecting over a broader area, then integrating that information. At that point, it can determine curvature, and then you can say, "At the point of the curvature, here's the feature."

With touch, you have two modalities simultaneously. Your finger is impeded and can't push beyond a certain point, so you can move your arm and hand with your finger in contact and trace out the coarse shape. On the fingertip, you have the fine detail—the texture and confirmation of the local curvature. You're getting a multimodal input of space with touch. I was arguing that curvature is detected by the broader input and the feature, like texture, is detected by the smaller input at the top. They can confirm each other. I know that's true for vision; I don't know about touch, but it's probably true. I think it's a general property of the cortex. Anyway, we can put that aside. We can just assume that with some sensorimotor system, if you had a touch sensorimotor, we could get this information.

I don't know about the visual world, how easy it is, but if you have a working point cloud, then I guess it is. Two more notes in general: even though those features are rotation invariant, we can still extract the pose of the object once we recognize it, because the matching procedure narrows down where on the object we are. Once we know where on the object we are and where in space this point is relative to us, we can calculate the pose of the object relative to us. That's one side note.

The second note is that it might make it easier to add more slack for the sampling problem. If you just have the x, y, z displacement, it's difficult to decide whether you want more slack in the x, y, or z direction, or if the displacement could be longer or more rotated. Here, those factors are nicely separated into four variables, so we can say we don't care as much about the length of the displacement matching exactly, or we don't care as much about the angle. It would be easier. 

Here's an example: this model was stored in memory with a hundred points from the surface, and here there are two hundred points from the surface, with different features and connections between them. At least for a short time, it recognizes the mug and then thinks it's not any object that it knows anymore. That was just a quick test. With those four variables, it could be easier to explicitly add slack in either the length of displacement or certain angles, so it could be somewhat invariant to changes in scale. If you have more slack on the displacement, you could have larger or smaller scale. I think it might allow for scale if you allow only the distance factor to vary.

It's funny because every time I think about that, it seems nice, but sometimes the scale varies in direction. What if I showed you a coffee cup that was oval instead of circular? You get it right away. It's stretched in one dimension but not the other, and you immediately recognize and learn it. It's weird—scale isn't just scale. Distortions occur in non-uniform ways. With those distortions, it may be important to consider hierarchy and features as well. For example, if you have a very distorted cup or something like the Dali clock, if you only feel the morphology, you probably wouldn't recognize it if it's highly irregular.

Once I've learned it, I could quickly learn one of these distortions. I would recognize it as a cup right away, but I could definitely learn this object and know that it's a cup. I would know I could put liquid in it, grab the handle, and drink out of it. All those other things come along with it. I wouldn't say this is something completely new.

I was thinking about how much distortion we can tolerate. Here are three examples. The ring is a little rotated, it's all tilted, but for example, the last one—if I rotate it this way, it doesn't feel like a cup anymore. It's more like a cartoon character, maybe a face. When we ask how much we can tolerate, are these objects I'm actually learning now? Let's say you learned a cup, and now you see this thing.

So, to the question: would I label this as a cup? I would see it as different, yet somehow the same.

It seems like a pretty good technique in general, right, Viviane? The point pair features. I like it. It's a very nice description of how two points in space relate to each other, or how two planes relate to each other. Two points with rotation attached to them, or two planes—really two tangential planes. I think it has a lot of nice properties and solves some problems. I just wasn't sure if we can assume that we can get those normals; that would be the major difficulty. But it sounds like we can, right? If we have the point cloud, but let's say we don't—you have a partial point cloud from a sequence of observations. You can get the normals for the point cloud. You could train a network to retrieve the normals from a partial point cloud. It wouldn't be perfect, but at least you don't have to assume you have the full environment cloud.

In some sense, whether I'm touching something or looking at something, I'm going to learn the shape of this thing. I have to be able to determine these surface normals. How else could I do it? I can't think of any other way. I have to do that, and I'm able to do that. I'm looking at this chair in front of me, and any point I look at on this chair, I have a sense of where the surfaces and edges are, and I can describe where the point cloud is. I can do it with one eye closed. Somehow that information is there, so you should be able to do it.

There's also a variant I presented two weeks ago, which was instead of the normal, it was three displacement vectors in sequence. In a similar way, you get the angles between them. So it's two displacement vectors to define your plane, and the other one is—yeah, you don't need the normal in that case, just displacement.

Pardon? I'm confused. Aren't we trying to determine the displacement? Let's say you have three in sequence, but I thought this is how we're going to determine this displacement. No, this requires a displacement too, because you need the distance between the two points. But that's not just the distance; it's the angular—it has to be the 3D distance. This is just a linear distance here, not 3D distance. It's a normal distance, just a scalar. 

Anyway, to learn the morphology of an object, it seems like we have to be able to detect these normals—the boundaries on an object, the plane change, the normal. That sounds like a pretty safe technique to use, right?

Yeah.

I feel like if you can't do that, it makes things very difficult. You can't tell the curvature of a surface. Is this a worthwhile idea to pursue further? Any objections or alternative ideas?

It would be interesting to look at how we could extract those normals from a vision patch or a small subpoint cloud, for example, from a touch sensorimotor. Didn't you just say that we have the point cloud and can extract these? Right now, the touch sensorimotor always just gets one point. It would need to get several points in close proximity, and then we can get the normal from that. At each image patch, don't you get a whole bunch of pixels with depth?

It depends. How I was using it before, I just get one point in space. But you could get multiple; it's all there. The system is getting depth at every pixel. I think you are using the mesh directly, not the sensors. For the cloud I just showed, I used the mesh directly. But doesn't the mesh have a little angular? You have the surface and you calculate the normal. A real system wouldn't have access to the mesh. We're getting the mesh because we're using Habitat.

If it's operating on the depth patch, then you have depth at surrounding pixels and you capture the room.

Sounds doable, right?

It's just a question of how much noise it could tolerate. You'd have to try it to find out. There's also the issue that if you have a plane that's nearly normal to your view, your sampling is much coarser in the Z dimension for that surface than for something that's normal to how you're capturing these Z coordinates. The mesh is almost a ground truth there. If you look at what you're getting as a sample Z image, you have to be careful about which points you associate near to each other. 

If I was looking at a plane that's normal to me, that would be an easy one. The hard one is if I have a cube and the top of the cube is just going off at a slight angle. You get fewer samples off that plane of the cube than you would from one that's normal to your viewing direction. When you take a cluster and average to construct a normal from the XYZs, you have to be careful about how you choose them. You could be on a corner, which is ambiguous.

I've been thinking about things like tubes and faces. When I look at an object and rotate it, it seems like I have a bias toward remembering the surfaces. If I look at a cube, I remember what's on each face, but I don't focus on the corners. I rotate to the next face, and so on. I know it's a cube, but I don't try to learn the displacement from an angle. I remember faces straight on or from the side, which is why we're bad at drawing faces at angles. When we observe objects to learn them, if I'm learning a cube, I'll probably learn features on one face that's orthogonal to me, then rotate to another face. I don't really memorize what it looks like from a corner.

You need to at least recognize the boundaries—maybe not the corners, but you need to know where the edges are to define anything. The theory I'm working on is that the basic way we learn displacement of features is in 2D, and this 2D displaced set of features is wrapped around a 3D morphological object. We try to get those features into a 2D presentation so we can better recognize their relative positions, not when they're really skewed, but by rotating the object to see that.

If I'm learning what's on a coffee cup and rotating it, I don't look around the edges. I look at what's facing me, then keep rotating to what's facing me. I don't try to imagine what's around the corner. I just rotate until I see it clearly. There are ways to get around those problems. We've also captured those edge cases. Kevin, you're talking about heuristic averaging to get the normal, but we're thinking more of learning a system that maps a partial point cloud or sequence of images to normals and then learns to capture those.

You have to be careful—there's going to be some context you'll have to recognize to know how to weight those contributions from the points. But it's a learned system. It could be learned; it just doesn't pop out of a simple analysis or diagram. That relationship could be learned, but it's going to take some training. Not so much edge cases in the histological sense, but things that become salient and are used as anchor points to decide how to proceed. I'm not disagreeing; those could be learned. It's just that, mathematically, if I was to do it by hand, it's not necessarily straightforward.