All right, can you see my slide with the laser pointer? Yes. Perfect. I'm going to explain the evidence-based learning module in more detail and give you a better understanding of what it does and how it works within the Monty framework. I've spent a lot of time thinking about how to best explain how everything works and fits together, and I hope it's understandable by now. Feel free to interrupt and ask questions or suggest better ways to explain things. Let's go ahead. I'll start with the general framework first. It's not specific to the learning module, but since Jeff doesn't really know the framework yet, and as a short refresher:

We basically test object recognition at the moment—object and pose recognition—on the YCB dataset, which contains 77 different objects. We test these objects in different rotations and locations in the environment. The objects are floating in empty space, and there's only one object. We have the object in Habitat in an empty space, and we have a sensor patch that only perceives a small part of the object.

Usually, it's an RGBD camera, and from this little patch, we can extract a three-dimensional patch, which consists of XYZ coordinates of where each pixel in the patch would be in space using the depth image and the location of the agent and sensor. From this patch, we can extract a pose—a location in space and an orientation. The orientation is defined by the point normal, which is a vector that points out of the surface, and the first curvature direction, which in this case would point to the side of the cup, and the second curvature direction would point upwards. These are the directions of the largest and smallest curvature. We can also extract pose-independent features, which don't change if the object rotates or moves, such as color at a location or the magnitude of the curvature.

This is the general format: everything inside Monty happens in this format—poses with features, or features at poses.

We currently have two different types of action spaces. One is the vision action space, where the agent is fixed in one location and can tilt up, down, left, and right. The second is the touch action space, where the agent moves close to the surface, always perpendicular to the surface, and can move around the entire object. To visualize: in the vision action, we just tilt and never see the object from the other side, but we see it from one viewpoint and can explore it. This is the viewfinder for visualization, and this is the patch that would get sent to the learning module or the sensor module. In the touch agent, it can move around the entire object. It moves a little slower because it wants to stay on the surface and not fall off. It looks like the object is rotating, but it's just the sensor moving around the object.

There's a distinction between agents and sensors. An agent is whatever moves in the environment and uses the policy we define to move. We can have a variable number of sensors attached to an agent, and all the sensors attached to an agent move together. For example, patches of skin on a finger would be multiple sensors attached to one agent, or patches on the retina would be multiple sensors attached to one agent.

We can have different policies. We can have a completely random policy, like this orange one that randomly moves along the object. We can have bottom-up policies, where we use the sensed features to decide the next action—here, we use the sensed curvature directions and just follow these directions and switch at some points. We can also have top-down policies, where we use the models we've learned to decide the next action. Typically, the learning module suggests a motor command, which gets executed by the agent. That would be the green line—we move, and the learning model could suggest moving to the handle, and then we keep moving there.

You said an agent can have multiple sensors associated with it. Have we ever used that in any scenario, or is it always one sensor per agent right now? For example, all of the voting experiments use one agent and multiple sensors.

I will show some examples later where we use one agent and five sensor patches. In that case, the voting module uses sensors that are very close to one another, not patches in very different locations. Exactly. It's impressive that the voting works as well as it does. You would expect to get more disambiguating information if they're further apart than if they're right next to each other.

They're not millimeters apart; they can still cover further parts of the object depending on the perspective. We haven't really tested yet with multiple agents that independently move around the object and can see opposite sides at the same time.

We have to discretize time in three ways. First is steps—the smallest interval. A step is taking one action and observing a new observation in the environment.

The second is the episode. We can take a variable number of steps until an episode ends, for example, when we recognize the object. When the episode ends, we reinitialize the environment, reset all the hypotheses in the learning module, and show a different object. The last one is the epoch.

This happens once we cycle through all of the objects. A new epoch starts, and we start again with the first object, usually showing it in a new rotation.

That's the current testing setup.

The whole system is designed to work from scratch, starting with no knowledge about the world. Nothing is in memory yet. The sensor moves on the object, recognizes that it doesn't know this object, and begins exploring to collect information. It then builds a graph from these observations and stores it in memory. When a new episode begins, the sensor moves on the object again. It might recognize the object as the graph it previously built and saved, allowing it to explore further and collect more observations, which are then used to update the graph in memory. In this way, it can gradually learn complete graphs and models of objects. Ideally, it should be able to do this without labels, but currently, we usually perform supervised pre-training by providing labels to learn a complete model. These models are loaded into memory at the beginning of each experiment so we don't have to start from scratch and can ensure the models are complete.

There's a difficult period when confronted with an unknown object, where you must decide whether you're learning a new object or updating an existing one. Every episode has two phases: matching and exploration. If, during the matching phase, none of the models in memory match the observations, a new graph is created. If the object is recognized and matches a stored graph, the graph is extended with new observations. For example, if in the second episode you start on a part of the object not yet in your graph, a new graph might be created, even though it should be the same object. In the future, graph consolidation could address this. Alternatively, the exploration phase could be extended to ensure the entire object is explored when building the initial graph. This approach is practical, dividing the process into exploration/inference and learning phases.

Theoretically, during exploration, you could also try to match, but this is omitted to save computational time and instead focus on collecting many observations to build a good graph.

Currently, supervised learning is used most of the time. The object is presented in various orientations, and the model is told the object ID and pose, allowing it to learn a complete and accurate graph. This is an aid used for now, but experiments with learning from scratch are also conducted.

At a high level, the Monty model consists of an environment and agents with sensors that perceive it. Each sensor patch sends raw sensory information to a sensor module, which converts it into a common communication protocol: features at a pose. A pose is a 3D location and rotation, or pose features like point normal and curvature directions. This is sent as input to the learning modules, which have lateral connections to vote with each other and can output their most likely hypothesis at each step—again, features at a pose. Features can include the ID of an entire object model. The input and output of a learning module are in the same format, allowing them to be stacked to recognize increasingly complex objects. The third output from learning modules is a suggested motor command—a location in space that can be translated into the agent's action space and executed in the environment.

Some aspects, like the hierarchy, are not finalized yet and are still being brainstormed, but this is the current setup.

Regarding the motor output, the learning module suggests a location relative to the body, which the motor system translates into the agent's action space. The suggested location depends on whether the module is matching or exploring. If matching, it aims to disambiguate between hypotheses; if exploring, it targets areas where the model is incomplete or uncertain. Implementing goal-oriented behavior is a future consideration. Currently, no goal condition policies are implemented except for object recognition. The framework should support goals, such as achieving a particular object state or condition, and the learning module could output a suggested motor command for that.

So far, this hasn't been needed for current objectives, so it hasn't been implemented.

Today, the focus is on the learning module and the evidence-based learning module specifically. Any questions about the general framework before moving on?

Okay, so at each stage in the hierarchy, there's an SDR being generated to represent it, or when you have poses and features coming from the sensory modules. I'm assuming the learning modules are doing some kind of encoding of the features. Right now, we're not using SDRs—we're considering it for the future—but currently, it's explicit: X, Y, Z coordinates, and then features are a list of continuous numbers. At some point, you assign an object ID when you think you've identified it, or maybe candidate IDs. Is that right? 

Yeah. So here at the output level, right now, object ID is just a label. If it was supervised, that's the actual name of the object, or it's just a number, like the sixth object that I learned. We're thinking about turning this into an SDR at some point, but it isn't right now.

To quickly recap, there are currently three different graph-based learning modules we've implemented: the displacement-based model, which matches incoming displacement to the edges stored in the graph; the learning module that uses features at location, which matches to the nodes stored in the graph; and the evidence-based learning module, which also matches to the nodes but uses evidence columns for each hypothesis. All of them are location invariant—the object can be anywhere in space and you can still detect it. All are rotation invariant, but the node-based ones have to explicitly cycle through testing different rotations, whereas this comes naturally with the displacement learning module. Only the displacement module is scale invariant at the moment; the other two are not yet, though it's not logically impossible to do this.

The downside of the displacement module is that we have to sample the displacements stored in the graph as edges. We can't sample new displacements, which we can do with the other two. We also can't sample any new locations not stored in the graph. We can do this with the feature-location module, but it decreases performance a bit. With the evidence module, it works quite well. With this one, we can also deal with noise reasonably well. Another advantage of the evidence-based module is that we have a most likely hypothesis at every step. Knowing the most likely object and pose helps us communicate this further up the hierarchy at every step and use the motor policy more efficiently to test this hypothesis.

We'll look at this one now. 

Could you explain again the downsides, the new displacement from the new locations? I didn't quite follow that. Why can't the displacement module deal with this? I'm not sure what they actually mean by sampling new locations. Is it sampling during learning or inference? I'm confused by that.

Basically, if we learn a graph and store a discrete number of nodes and connections between those nodes, and if we use this displacement module, in order to recognize this graph, we have to sample pretty much exactly the same edges we've stored in the graph. I remember that from before; I just didn't connect that with the world here. Okay, thank you, that's sufficient.

This is the general structure of the learning module. We have the pose and the feature coming into the module. It has a memory of graphs it has learned before, and this memory is used to initialize its hypotheses. We use the displacement between the current pose and the previous pose, and the current features, to update the hypotheses. Each hypothesis has an evidence count assigned to it, which I try to visualize here with colors. Some locations on the objects are more or less likely, as are rotations of the objects. With an incoming displacement and feature, we update these hypotheses. Then we can send out votes and also receive votes, using them to update the evidence counts again. Each learning module has three types of outputs: one is the vote, one is the current most likely hypothesis (the most likely object and its pose), and one is an action suggestion.

A quick question: what reference frame is the pose in? Is it relative to the body or to the model?

It's relative to the model and to the body in a sense. We have a location in the model reference frame, and together with the location of the sensor, we can transform it into a location in the world. The rotation of the object is relative to the learned model.

How is the best pose and feature different from the output of a vote?

Votes output all possible locations and rotations—all possible poses. This one is just one pose, the most likely pose.

Why do you have them separate? Do you need one on the right for action policy?

The most likely pose and ID are in the same format as the input, so this can be sent to the next higher-level learning module. The votes go laterally to other learning modules and are used to update the hypotheses. It's a list of possible poses; here we just have one. This pose can be used to calculate a displacement to the previous one, as we do in the beginning. Here we have a list that gets directly sent to the hypotheses, and it's a different way of updating the hypotheses.

In my mind, I've always thought of the vote as the collection of possibilities being passed off. But you're saying there's a set of possible objects that's voted on, and then we only pass off the most likely, correct?

Yeah, since we haven't really implemented hierarchy yet, this is not set in stone. We could also pass all the possible poses to the next learning module, but I was trying to have the output in the same format as the input so it's easily compatible. Right now, this is just used for logging, to see if we detect the object. With an SDR, you could represent a union of these things. Then you wouldn't have the representation problem. It's still an interesting idea that you don't pass up the union, only the one. I think with a union, it would be more difficult to calculate the displacement to the previous pose.

You start getting a bit of a combinatorial explosion there. That's why this is a very interesting idea that never occurred to me. This would require two different sets of neurons: one set representing the union, one set representing the hypothesis. It's quite believable; I just never thought of it, and it's worth considering further.

As I said, it's not something to do right now because we're not using hierarchy at the moment. Niels and I were thinking maybe during the By the Bay Week, we can do more brainstorming and narrow down the hierarchy. We'll decide on these things, but that's just how I've been thinking about it so far.

We might pass up both to the next level; there could be two channels passed up. The most likely one could be through the thalamus, potentially the layer five path, and the other one, the voting one, could be the layer two/three up to the layer four pathway. There are distinctions between layer 2 cells, layer 3 cells, and subdivisions of them. I think that's more likely what would occur. I think the five pathways will still have a motion pathway.

It's interesting to think about a new idea that I don't think I've mentioned before. The main reason I did it like this is to keep the modularity of the learning module, so we can have the exact same structure and easily stack them on top of each other because the input and output formats are exactly the same. The inner workings can always be the same, no matter where in the hierarchy it is.

Are there any more questions for now?

Okay.

Real quick, what is a graph? How do we represent the models inside the learning module? We have the 3D object that we explore with the sensor, and then we turn that into a graph where nodes represent 3D locations in space, and edges represent the displacements between these locations. Nodes can also store features at these locations, such as color and curvature. Additionally, we define the morphology with point normals and curvature directions. These help with the rotation of the object. Point normals point out of the surface, and curvature directions point towards the minimum and maximum curvature. In the evidence learning module, we don't use the edges of the graph, so theoretically, we could just store a point cloud.

Now, how does the matching process work? This might be a bit complicated. I hope this example helps. In this example, we have a cylinder, which is mostly red but has a little blue corner. We start by collecting one sensation down here. We haven't moved yet, so we've just collected the first observation, and we use this to initialize our hypothesis space. This is the model of the cylinder; each point is a node in the graph, and at each location, we store the point normal and curvature direction. We sense a location in space, which doesn't help us yet, but we also sense the point normal and a curvature direction.

Since the object could be anywhere in space, the location doesn't help us. We could be anywhere on the object, but we can use the blue and red lines to narrow down the rotations of the object.

If I were at this specific location, the object can only be in two different rotations. The cylinder can only be standing upright or upside down. If I were on the edge of the cylinder, given the point normal and curvature direction, there are only two options for how the cylinder might be rotated. These are different for every location on the cylinder. In some cases, like on the flat top of the cylinder, we have to sample more than two possible rotations because the curvature direction is meaningless. On a completely flat surface, we don't have curvature directions, so we sample a few possible rotations perpendicular to the point normal. We do this for all objects. Here you can see it again, colored by the rotation. On the rotation, are we dealing with just two three-dimensional rotations or three-dimensional rotations? Three-dimensional rotations. So, basically, any change in the orientation of the object.

On the object, what do the gray arrows mean exactly? Is that the two rotations of the object? It could be upright or upside down, right? Exactly. And then at the edge? For example, here on this point in the model, we have a point normal that points out like that. If we align the observation with this stored point normal and curvature direction, the cylinder would have to be tilted like this or like that—tilted 45 degrees or minus 45 degrees. But that's just looking at the pose. Wouldn't we normally have a feature that would be very different there, and we could not be there at all? I'll get to that in a moment. This is just the pose hypothesis.

We're not looking at the feature in this step yet. This is just possible locations and rotations initializing. As you can see, the top of the cylinder would have exactly a 180-degree rotated pose of all the points on the bottom of the cylinder. The sides would be 90 degrees rotated from that along one axis. Now, this is the second part you mentioned. We also observe a feature. Our model stores the color and curvature at these locations. With the finger up here, we observe a red color and a convex curvature.

Now we can update the evidence for these poses and locations. At points where all the features match, we have the highest evidence. Where only one of the two matches, we have medium-high evidence. For example, here we have a red feature, but the curvature is different because we're on the edge, or here the curvature matches but the color doesn't. Wherever neither feature matches, we have zero evidence. Does that make sense?

Why do you say the color doesn't match? Because the object has a blue patch. Exactly. All the points here in the model store blue as a feature layer.

In a real vision case, the color could appear quite different, even if the actual color is the same. We're not addressing that here. For example, we usually look at the hue, which isn't as influenced by brightness and similar factors. However, we don't really account for that at the moment. The evidence value is not binary; it's a continuous value based on the distance between the perceived observation and the one stored in the model. If the red is just a little off, the evidence is still quite high, but if it's a completely different hue, like blue, then it's low evidence. Features can only add evidence; they cannot subtract it. Even if the features do not match at all, it's still zero evidence. We don't subtract evidence as we do with morphology. This way, we should still be able to recognize the morphology of the object, even if the features don't match. That's an important consideration.

The model consists of discrete nodes, and we can interpolate between them later. We initialize a discrete number of hypotheses, defined by how many points are stored in the model. The model itself is a set of discrete points. During inference, we don't assume we're exactly on those discrete points, but if we're not, slack variables in the system allow us to start somewhere that's not a point. Even if I start somewhere else and move, the next observation will still be close enough to another point to recognize the object. Experiments show this isn't a big issue, especially since our models are much denser than the example shown. If the model were very sparse, it might have an effect, but with our models, it's not a problem.

Evidence is stored only on the nodes. There is no other evidence, such as displacement, stored on the edges. Evidence is for the hypothesis and moves around the space; it's not stored in the graph. Features are stored at the nodes, but transitions, like changes in color or curvature between nodes, are not stored. If features change a lot in a small space, we store more points in the model; if they change little, we store fewer points.

Now, moving to the evidence update: we move the finger from one point to another on the model. We sense red and curved, resulting in a displacement of a certain length and direction. We have all these hypotheses, and we calculate search locations by taking each location and rotation hypothesis, starting at the possible location, rotating the displacement by the hypothesis pose, and wherever this rotated displacement ends up is our new search location. For example, we start at a location, rotate the displacement by the possible pose, and the search location ends up elsewhere. This is repeated for all hypotheses, resulting in one search location for each.

Taking a closer look at one hypothesis: with a sideways displacement, the location and pose hypotheses could be the cylinder upright or upside down. We rotate the displacement and apply it to the location, resulting in two search locations. Zooming in, we draw a search radius around the search location and look at the nearest neighbors within this radius. For each neighbor, we calculate the evidence using the observed features—curvature direction and pose-independent features like color and curvature. For pose features, it's the angle between them; for others, it's the real value difference. This is weighted by the distance to the search location: points farther away provide less evidence, and points closer provide more. This ensures we stay on the model surface; if a point is right where we're looking, it's more certain to be the correct one than if it's far outside the search radius.

If the pose hypothesis is wrong, rotating the displacement incorrectly could place the search location outside the model space. If that happens, we might be off the object, or our pose hypothesis might be incorrect. However, if we went off the object, we wouldn't have an index observation. We don't actually go off the object, but a wrong hypothesis about the object's rotation could result in a displacement that ends up outside the model in model space.

All right, so you're relying on the fact that if you move it just a little bit, even if your orientation is wrong, you'll have some useful information. It also works with large displacements, but generally, most of the hypotheses will be wrong, so most of the search locations will end up outside of where the object actually is in the model space. We don't want to consider these hypotheses anymore, or at least we want to decrease their evidence.

Earlier, you mentioned that in places where things are pretty constant, you store fewer points in the model. It seems like there you'll just have lower weights because the density of stored points is fewer, but actually, it should be more dependent. I don't know how you take that into account.

It might be that this weight factor doesn't actually help a lot with performance. I would have to test that. Intuitively, we would want to weigh that, but if we have a lot of noise or don't store many points in that area, it might just be a hindrance. Maybe I'll do an experiment without this weight factor.

Or maybe different parts of the object could have different weighting factors if you have to include it, but that's more complexity. That's a good point—maybe I'll just try that. Maybe it's not even necessary to weigh this.

We basically calculate how well each of the points in the radius matches the observed features, and then we take the best match to update the evidence for the hypothesis.

Even if, for example, one of these doesn't match well—like if we're up on the corner, we have a red color, but everything else doesn't match, the point normal doesn't match, the curvature doesn't match—that one would have pretty low evidence, but that doesn't matter. It's just another point in the model. As long as there's one good match in this radius, we get a high evidence update.

An evidence update means we add this evidence to the accumulated evidence so far.

As another detail, the search radius is not exactly circular; it's spherical, influenced by the point normal. We want to search far along the surface of the object, but not far out of or inside the object. In this example, the point normal would be pointing upwards. The search location is in the center, and color represents distance. Points that go in the direction of the point normal or down into the object increase distance much faster than if we go along the surface in any direction.

Earlier, we talked about the idea of an action policy that follows the surface, but it sounds like you're not doing that. You're doing an action independent of following the surface; it could be off the object at some point. Wouldn't you normally be following the surface? Why is that? Wouldn't it be on the object?

No, these are not actions; we sense the displacement. There was an action performed in the environment, but then we add this displacement to all of our hypotheses. For every hypothesis, where did the displacement come from? This is the action we performed. Why wouldn't the action be constrained by the object?

We could only take actions that keep us on the object, like the fingers following one another. That is the constraint, but then your points will all be on the object. You were just describing how the points could be off or in the object. I don't understand how that could be.

This is just for the search inside the model space. We have the displacement applied inside the model space. This is not an actual movement. We look up here, we look down here. We do this for all the hypotheses. We want to search here on the model hypothesis. We don't move here; it's just looking at all the points on the surface in this area.

That's not an observation in feature space or object space; it's a prediction made by the model.

For example, let's say this is the object we have in memory, and we also have serving. Let's say I'm touching it here, but one of my hypotheses is that I'm touching it over here. Now I make a displacement from here to here. If my hypothesis was that I originally touched here, then I should expect something over there, but now we're outside the object. This is the one we're considering—being outside the object because my hypothesis was wrong. I was at the wrong location. I should have thought I was here.

If you think of it as making a prediction, like counterfactuals—if I moved in this direction, what would I expect to see? If that's not consistent with my current evidence, then I'll discard that as a counterfactual that's not likely. Is that right?

Let me go to the slide; maybe it visualizes it better. For example, if we test the hypothesis that we were down here on the bottom of the cup and sense this displacement, we would say, if I had been here in this hypothesis rotation, then now I should be here. But if we do a search radius around here, there are no nearby points in the model, so we want to decrease the evidence for this hypothesis because we're not on the model surface anymore if we follow this hypothesis.

Is the displacement an actual movement of the sensor in our hypothesis space? We have one displacement with the sensor—the actual displacement that we sensed—but then we test all our hypotheses given that displacement. I think it's like a trial displacement; it's not an actual displacement. It's a mental manipulation—you're imagining.

If it's not an actual displacement, then I have no data to say it's off the object or not right. How can I understand if it's just imagining?

No, where it says displacement, that's an actual movement. That's what they're saying, but then I was told it wasn't. No, that is the actual movement. There is one movement, which is that displacement, but depending on your hypothesis, that one movement can lead to very different predictions. That's what's being checked for every point, for every possible pose hypothesis—you try that movement out.

So, make predictions. Back to my earlier point: why would I try a movement that takes me off the object? I just require my action policy to keep me on the object. I never test a displacement that takes me off the object. We stay on the object, so our movement in this example is: we were down here, we moved the finger up here. That is an actual movement in the world. We stayed on the object surface. This is the displacement we're sensing from here up to here. Then we take that actual sensed movement along the surface of the object and compare it to all the hypotheses we have. With a lot of the hypotheses, this is not going to match. That's it.

But isn't it the case that you can't guarantee the movement is going to keep you on the object because you don't know where on the object you are? You just have a bunch of hypotheses. You can take your best guess, but you can't guarantee it's going to keep you on the object, can you?

In the real world, we're going to stay on the object. The policy makes sure we stay on the object for all the actual movements of the agent. But for testing hypotheses, if I have a wrong hypothesis, this actual displacement that was on the surface is going to end up somewhere out here in model space—not on the model surface anymore. I understand that piece. I'm just wondering how you can guarantee the original movement is going to keep you on the object if you don't know where on the object you are. That could be a low-level action policy, just keeping contact. Your finger never comes off the surface when you're touching an object; it just tracks on the surface. I don't know how it does it.

For example, in the implementation, we have some simple heuristics. If we do go off the object, we don't send those observations to the learning module. We perform some corrective movements to go back on the surface, and when we're there, we send it back to the learning module.

How do you know that you're correctly translating this displacement from the real-world objects to your graph? Is this the reason why we're not invariant to scale now?

I hope I understand the question. We basically learned from a smaller cup of coffee, and now you're traveling through a bigger one. The displacement is going to be different. So that's not going to work if the cup now has a different size; we wouldn't recognize it unless it's a very similar size. The length of the displacement is fixed, and we're not going to test different lengths of it.

If you'd also stored the rate that the properties are changing along the edges, you can decide, of all the nodes you're at, which directions you can go in that graph that correspond to the actual change you saw when you made your displacement. There's a matching delta in the properties that will correspond to the transition from one node to another. That could be an idea. Scale here is still an open issue—how to best do it. We could do the change in features. In a lot of places, features don't really change much, except for the point normals. One option is to re-anchor the hypotheses and use that to rescale how we scale the displacements, but we haven't really figured it out. Another option could be to learn models of different scales, separate models.

If the scale is similar, it would still work because we have some slack, but we would have to store separate models. Not necessarily, because if you're moving along the evidence that your observation says the property changed by this much, you could do a rescaling in the model. If you were to go in this direction along the graph and expect a change this much in that direction, you could predict how far along the graph you're actually moving. You may not be going one node; you may be going two or three nodes along because you have more of a change in the feature than you expected by just going one node. You can do some sort of auto-alimentation or something like that.

I'd have to think about that one. The question is whether it also works if we don't move continuously, but instead make saccades from one location to another. Would that also work? With physical objects, I'm not sure the scaling issue actually comes up. If you're always in real coordinates, a larger coffee cup is actually a different coffee cup. It's not the same coffee cup anymore, but this relates more to the generalization question of how we can recognize similar morphologies. If it's the exact same, the coffee cup never changes in size. The perception on our sensor may be different, but as long as we're translating back into real body coordinates or real world coordinates, you're never actually going to see a scaling issue. You argued it's a generalization issue. I think I understand that, but on the other hand, if I see a miniature coffee cup, I can make predictions based on my previous model of a larger coffee cup. Somehow, I'm able to bring in a model that was learned at one scale to help me generalize. I think it's more of a generalization question rather than just recognizing the same object. You're still able to use a model that was learned at one scale to inform a new model on a different scale, so they're not completely independent. We have this goal of trying to recognize morphologically similar objects, and when we think about it that way, it's not just scale—it could be other small distortions. The top could be a little bit narrower, the bottom could be wider, or it could be curved. There are all these other things that define similarity, and scale is just one of those things. Maybe it's obvious, but it's not obvious to me. In vision, we see things at different scales all the time, at different distances, and we never get confused by that. If it's a small coffee shop, it's just further away. I've never really quite understood that. That seems like it's going to be part of the solution we have to think about. Scale invariance can be the actual size of the object, but it's just your viewpoint and scale, at least when it comes to vision. For distance, for example, we don't have that issue with the system because from the depth image, it will just convert it to different X, Y, Z coordinates, but the scale in 3D space will still be the same. Actually, we could also solve the scale issue with hierarchy. For example, if we learn models of multiple size cylinders as separate models, but then encode them with similar SDRs, a higher-level model can learn that a cylinder plus a handle is a coffee cup. It can get the SDR for a small cylinder or a larger cylinder, and both of them, if combined with the SDR for a handle, could recognize small and big cups.

The problem with my argument is that after the holidays, I think my dimensions have definitely changed. My fail has increased. Should I go on with this for now? Let's go.

Now let's move again. We move to the right, into the blue part of the model. We get a different displacement that points to the right, where our previous hypothesis ended up. At the end of all these rotated displacements, this is now where we start. We start at the end of the previous displacement for every hypothesis. Again, we do the same thing: we apply the displacement to the rotation of all the hypotheses to the sensed displacement. We get new search locations in the model space.

We compare the points in the model space around the search locations to the sensed rotations and features, and we update the evidence. Now we can already see that we have a most likely hypothesis, which is having moved from here to here, because only this sequence—starting here, moving upwards, then moving right—matches with all the features we have sensed: red, blue, and the curvature staying constant, the convex. Many of the other hypotheses, like the ones that are completely off the object, have very low evidence counts. Some, like the ones that go around the object, are still likely. It might be that we just sensed one blue observation due to noise, so we might still be here, or the cylinder might be upside down, or we might have moved down. We basically just add the new evidence from this displacement and features to the existing hypothesis evidence counts. Does that make sense?

You're exhaustively testing all the nodes in the graph as hypotheses. You're not discarding the ones that are completely implausible. Not right now. Since we can do this all with a single matrix multiplication, we just do it for all hypotheses every time. There is an option in the code to test only the n most likely ones, so only the ones that have a positive evidence count or similar. We might actually do that and try to use some sparse matrix multiplications. For now, it's fast enough to just update all of them every time.

In this matrix, what are the rows and columns?

It seems like you might be eliminating rows and columns as you eliminate hypotheses, or are you actually eliminating individual cells within the matrix? If you're eliminating full rows, it's a much easier optimization. You don't need to worry about sparse matrix optimizations.

Let me get back to you on that. I don't have it off the top of my head what the dimensions are right now. That's all right. My guess is you're eliminating rows or columns, and in that case, it's much easier to optimize.

Basically, this procedure gets repeated at every step after every movement. We look at the displacement, apply the hypothesized rotations to it, check the sensed features with the features stored in the model, and update the evidence.

This is how it looks in practice. Here we see the sensed displacements at the top. This is the viewfinder, and this is the sensor patch, which is what we actually use. These are three or four models in memory, showing how the evidence gets updated. For the dice, which is a small object, all the hypotheses are already far from the model and very unlikely. After moving down the mug, and at the bottom, we have already made almost all hypotheses on the bowl and the banana very unlikely. On the mug, since it's pretty symmetrical, we have a red ring of likely places that moves up the cup as we move up the cup.

We have a most likely hypothesis at every step, which is already the correct hypothesis. I'm letting it run for a while to show this visualization.

We move, and after every movement, we update the locations and evidence counts until we recognize the object. Another example is the dice. We move on the dice; here's the sensor patch, which is what the sensor module sees. The model never sees this wide view—it's just for visualization. After a few moves, the large objects are mostly excluded. The dice is quite symmetrical, so we have a lot of positive evidence on each side. We also have a pretty stable, most likely hypothesis after a few steps. It's not the correct one—the target is 90, 0, 180, but the most likely one is -90, 0, 0. If we look at how this one would appear, it's over here; it basically looks the same. In this episode, the learning module detects symmetry, so it reaches the terminal condition by detecting symmetry between those two poses. It then says this one is the most likely, but notice that it is a symmetrical object.

Does that make sense before I go to multiple patches?

Is there a notion of uncertainty? If you have a couple of hypotheses that are really high and everything else is low, then you can be more certain, but if everything is close to equal, the most likely hypothesis is not going to be very likely, right?

We do use a list of what is still possible, and I'll get to that a bit later. In this case, we still have many possible locations on the dice.

It doesn't only look at the most likely one. We still consider all the ones with the highest evidence counts, which gives a notion of uncertainty.

In a practical scenario, you have to make a decision, so you'll want to use the most likely hypothesis, but there may be cases where you're not very confident and others where you are more confident. You could look at how many other possible hypotheses there are. If there are many, then there might be less confidence in the most likely hypothesis.

That's what we use as the terminal condition: we need to have only one or very few possible hypotheses to classify the object and end the matching procedure.

Let me move on before the company meeting starts. This is multiple sensor patches now. Here we have one agent with five patches attached to it. The patches can have different locations, different zoom values (so they can be larger or smaller), and different resolutions, for example. They all move together. The agent moves, and all those sensor patches move together in sync, even though their positions in 3D space can vary greatly. Some can be on the rim, and others on the inside of the cup, which in 3D space can be far apart.

Each sensor patch sends its raw observations to the sensor module, which turns them into features and a pose that gets sent to the learning modules. The learning modules output a most likely object and pose at every step and have lateral voting connections between each other. These voting connections can speed up object recognition. How does the voting work?

In this example, we have two sensors. One senses the rim of the cup, and the other senses the handle. They send their features and poses to the learning modules and have their first evidence counts. This one would think it's most likely on the top or bottom of the cup, given the sensed color and curvature. The other would think it's most likely on the top or bottom corner of the handle, given the color and curvature sensed there. Now we'll do an example of the first learning module sending a vote to the second one.

To do that, we first transform all the hypotheses from this learning module's space into the other learning module's space by calculating the sensor displacement between the two fingers. We determine the displacement between this sensed pose and the other sensed pose, and then apply that displacement to all these points, placing them in this model space. For every hypothesis here, it says, "If I were here, then given our sensor displacement, your sensor should be over here." It then sends a vote for this location, doing this for all its hypotheses. For most, it will be far off the model in this case. For example, if it says, "If I start here, then your sensor should be over here," it would expect a handle on the other side of the cup.

Does that make sense?

I'm a bit confused. What is actually being sent? The possible poses are being sent, and the evidence. Bottom arrow. Where is the sensor displacement?

We take all the possible poses and their evidence and transform the poses by the sensor displacement. The learning module on the left does not know the sensor displacement of the learning module on the right. In this case, it's not fixed because we are looking at locations in space; one patch might be offset, as in the example, or inside the cup. We assume we know the displacement between the sensors. The system knows this, but the learning modules don't; the Monty model knows. Monty handles the vote, manages all communication, and transforms the vote from one module to another, calculating the sensor displacement from the difference between poses. In the brain, this would be within the cortical column, not an external system. There may be intermediaries determining this, but it's not clear. With 100,000 cortical columns, there are 100,000 squared relative displacements, which is a huge number. In the implementation, when the learning module sends the vote, it also sends its pose. The receiving learning module can use the pose to calculate the displacement. That makes sense.

The receiving learning module then transforms all other learning modules' hypotheses into its own model's reference frame. This displacement changes if it receives a vote from a different learning module, requiring a different transform to align the votes in the same space.

All these votes in the model's reference frame are used to update the hypotheses. We go through all the points stored in the receiving learning module's model, use a search radius as before, look at all the points in the search radius, and take their weighted average to add evidence. This is scaled to a range of minus one to one and added to the evidence count. After this voting update, the highest evidence would be for the top panel, as it receives high evidence from voting connections and had high evidence before.

We have the evidence for all possible poses of an object. Taking the maximum gives the object's evidence, and the maximum within that gives the most likely pose. To get possible matches, we threshold: everything within, for example, 20 percent below the maximum evidence is considered possible. So, anything very close to the highest evidence count is also possible. For example, the mug and a larger cylinder might both be possible. The same thresholding applies to poses. In the mug, only certain points are considered possible, while in the cylinders, all points are about equally likely. If there are no positive evidence values on an object, it is not considered possible.

This threshold is dynamic, based on the maximum evidence. It's a parameter: the percentage below the maximum that's allowed. The larger this value, the more certain you need to be for an object to reach a terminal condition, meaning only one possible object is significantly more likely than the others.

There are two versions: one where the evidence value can grow infinitely, and another where evidence is bounded by weighing past and current evidence. If evidence is bounded, a fixed value threshold can be used instead of a dynamic one, or a softmax could be applied.

Lastly, we need to define a terminal state—when to end an episode. There are three terminal states: timeout (too many steps), no match, and match. If there are no possible matches (all evidence counts are negative), we say no match, create a new graph in memory, and learn a new object. If there are possible matches, we check all learning modules to see if the number of possible matches is one. If not, we set pose to none. If there is only one possible object, we check if its evidence is above the minimum required. If yes, we look at all possible poses within that object and see how many possible poses remain.

If all of the possible poses have a very similar location, we say the location is unique. If all the possible poses have a similar rotation, we say the rotation is unique. We also have a check for symmetry. I'm not going to go into too much detail right now, but basically, we determine the pose if either location or rotation is unique, or if we detect symmetry. Then we set the pose to the most likely hypothesis pose; otherwise, pose is none.

After we do this for all learning modules, we concatenate all the predicted poses for all of the learning modules. If we have decided the pose for enough learning modules—a parameter we can set for how many need to be sure of their pose—then we check if all of the learning modules agree on the object and pose. If yes, we say it's a match. If no, we take another step and continue matching.

That's the terminal slide. Very nice. Just in time. Thanks. Did you do all the lettering yourself?

That's amazing. Really great. I tried something new. I liked it. It's good. Great. Thank you.

There's a lot in there. I know we talked about this before, but going through the holidays, I forget things. There are a lot of big ideas. I think that was a very clear explanation. Have you run simulations of this and the performance, and how slow it is? Thank you. That's basically the results I showed before the Christmas break. So this is just a review of what you showed before the break? It's not? Okay. It looks good because you have new diagrams or maybe new language. I just couldn't remember if all this was done before or not. You're getting reasonably good performance speedup, if I recall. Is that right? Detecting one object takes about one second or so, usually, depending on how many models we have in memory. Nice presentation. Really nice. Thanks, and thanks for all your questions. I'm always trying to figure out how to explain these things best.