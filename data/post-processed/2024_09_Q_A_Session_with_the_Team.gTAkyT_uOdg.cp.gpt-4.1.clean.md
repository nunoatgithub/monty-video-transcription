Okay, I'll open it up to the floor if anyone wants to start with the first question.

Sure, I'd like to ask about using the different learning modules for pre-training versus continual online learning. Should we only use the displacement graph learning module during pre-training, or is it okay to use the evidence-based learning module at any point instead of the displacement graph learning module?

I think you can use both for supervised pre-training. If we do supervised pre-training, there's no inference being done. The displacement and evidence matching modules are both equivalent.

The reason we used to use the displacement learning module is because it would store displacements between nodes in the graph as well. We're not using them right now to recognize objects; we only use the points in the graph to recognize them, not the displacements. We thought it would be better to store everything possible in case we want to use them or try different things. We can keep the displacement learning module code so we could get displacements if we want. It doesn't take long to train the models, but if we're not using the displacements, the models will take up less memory on disk because a fair amount of the memory is storing all those displacements. It would probably be easier for people onboarding if they just need to understand the evidence learning module. The config looks simpler.

I imagine that would be easier. I agree.

Hojae, do you have a question? Oh, sorry, I'm going to go to another room because I can hear Scott twice. Oh, okay. Sorry.

I hear it now. It's weird. We're here, but because we're on Zoom separately, it's like we're distant—remote in the office.

I know you posted a question, Will, about drawing out how the learning modules and sensor modules would map onto the body. As part of my role, I'll have to describe this to different people, so it would be useful to be able to say what's happening in the human body, what our best guesses are, what information is traveling from the sensor to the thalamus to the cortex, what the cortex is communicating between itself, and how it communicates with various parts of the brain. There are areas where we have no idea what's going on, but we know something important is happening, and this is an area of active research where we want help. That would be great, but I think that's going to take a while to generate.

The other thing is comparing our technology to other AI technologies. For example, on a SaaS website, there's a pricing page where you can compare against competitors. I was thinking of having a checklist—one for Monty with lots of checkboxes, one for concurrent neural networks, one for reinforcement learning, one for OpenAI, and one for DA. In that checklist, we could indicate when we think the other technologies might be able to do that capability, since everyone's trying to get to the same place. When we think about what demo to build, it would be useful to pick something that's going to take others two years to reach, so we can focus on doing it right. Those are the two things on my mind.

Nice. In terms of the checklist, I think I mentioned this to you once when we had a one-on-one. I started last year working on a blog post comparing deep learning to the Thousand Brains theory, but it got put on the back burner and stayed there for a while. The plan was maybe in October or November, once we open source it, to revisit that, since it might help introduce people to the Thousand Brains theory. Maybe we could use both what's already in that post and other ideas to come up with a checklist, as you're suggesting, that really emphasizes the differences.

One of the problems we've faced for decades is that the AI world comes up with benchmarks—things humans can do that are difficult, and then tries to exceed human performance. They basically say it doesn't matter how we do it; as long as we beat the benchmark, that's all that matters. The argument for the Thousand Brains theory is the opposite: the mechanism matters. This is a very hard argument to make, and a lot of people just don't accept it. Over the years, I've tried different approaches. One is to show that the brain works on different principles than these neural networks, and we can talk about those principles and ask, since they're clearly different, don't you think it's important to know those differences? For the most part, people say no, they don't care.

More recently, I've focused on sensorimotor aspects. This is a system that works by moving through space and is inherently capable of movement. It generates movement and requires movement to do learning and inference, moving its sensors through space. Almost all other learning systems use data fed in as a stream of words or tokens, or labeled data, assuming that's all the information needed for training. But sensorimotor systems and brains learn by exploring and moving through space. That argument might work better for some people because it's easy to find examples of things we learn that are important and can't be learned unless you move. I would never say we're going to beat a chatbot in high-level language or composing poems or doing math, but we need to focus on systems that are sensorimotor. These could lead to sophisticated robotics and desirable learning attributes, where we don't need a dataset to train on—the system can explore the world on its own and learn new things that no one else has learned before. It's a tricky thing, but I've never doubted that studying how the brain works is the path to true AI. Lately, I've been focused on the sensorimotor part.

I like the idea of a comparison table. The only problem is that much of what our system can do is a concatenation of "ands." If you take one category, like recognizing objects, you'll find better expert systems for that particular task. But if you add in very little training data, continual learning, self-supervision, a sensorimotor setup, and noisy conditions, then it becomes unique—nothing else can do all of that. It will be tricky to show this in a comparison table in a fair way unless we emphasize that, maybe as a diagonal, with a fully green column for us and a diagonal of point solutions for others. There's a risk people will say it's trivial to combine these, and deep learning or transformer people will say they'll get there and train systems to do everything. They believe that. I never want to tell them what they can't do, but deep down, I know this system works completely differently.

Can I throw out another idea? This is what I used in a short Stanford talk recently. The key thing about AI is not what it does, but how it represents knowledge about the world. Sensorimotor learning systems, like brains, build a structured, three-dimensional model of actual things in the world by exploring them, whereas a chatbot has no sense of three-dimensional structure in its models. Its models aren't like that—they don't have reference frames and can't move. You can build an argument that the nature of the internal models really dictates what the system can do and its power. Monty and the sensorimotor learning system have a very different internal model of the world that actually captures its structure. That seems to work for some people.

There could almost be two different comparisons. One is how knowledge is represented and what principles the system works on. There, we'd have very few comparisons: we have sensorimotor learning, deep reinforcement learning has that too, but the others don't have structured models or the other properties of our system. Then we could have a second table with capabilities, where Monty would have the whole column green, and alternate systems would each have one cell green where they're particularly good. That ties in with the paper we want to write about Monty capabilities—what do we compare to? For most comparisons, if we want to compare Monty to an alternative system, we need to compare on at least two dimensions, like number of samples plus accuracy.

One of the things we discussed was creating videos to introduce the Thousand Brains Project. I imagined one of the introductory videos would cover the idea that brains work differently, explaining how they function and what attributes result from that, which would help with education.

I also want to mention a marketing term you could try: "data-less." In certain contexts, that's accurate. I'm thinking of how "serverless" became a polarizing term that everyone had an opinion about, and maybe "data-less" could serve as a similar hook. What does it mean? It can mean whatever you want—it’s the beauty of the term. You don't need to gather all the data; if it's a sensorimotor system, it gathers its own data by interacting with the world, rather than being forced to use a fixed dataset. People might argue that we send our kids to school and they read textbooks, so some of that happens with humans, but mostly, no one gives a baby a training curriculum. That will spark debate, which is the point.

I show my babies images of different emotional states and types of people on TV so they can learn, since we live in a place with only white people. I want to expose my kids to many different faces so they don't grow up isolated.

Do we want to address the second part of your question about the sensors and how they connect to the brain?

We could use a whiteboard and try to sketch it out.

I was excited to use my new iPad as a whiteboard, but I haven't set it up yet.

I think mine is downstairs too. We could talk about it.

I think I shared the whiteboard. Do you see anything? Now it's coming up. Showing a whiteboard. I'm not sure what you'll put on there, so I'll just draw a stick figure since that's all I can do with my mouse pad. You're not using your iPad? I'd have to get it; it's not at my desk.

This will start looking odd once I draw a face on it.

Maybe we can have a zoomed-in view of the brain.

Pretty good on a mouse pad, Viviane.

Actually, that's pretty good. It's not really zoomed in, but that's how I feel right now, that person.

I don't know if I should start talking through how I think it maps, or—here, I can do it if you want. All right, let's say we have a hand with sensors.

Basically, the sensor module would be a patch of skin on the finger, which converts information like pressure and temperature into the cortical messaging protocol. In this case, it would convert it into a spike train that travels to the brain, going through the thalamus. This might get crowded in this small view.

Maybe we can just talk about it from here. We would have learning modules in the somatosensory cortex if we're discussing touch. There would be learning modules everywhere, each connected to a patch of retina, skin, or higher-level learning modules. They would send some rotation hypothesis down to the thalamus, which might modify the signal before it goes into the learning module.

Are you disagreeing? No. I'm tempted to add my own thoughts. Go ahead.

Every part of the body has sensors. Each part has a patch—on your back, your toes—and they all send signals up to a cortical column. The difference is that some parts of your body have high acuity. For example, if you take a column in the cortex, the area of the fingertip it represents is quite small, while the area of your back could be several centimeters. They're all working on the same principle, taking sensory input from a patch of skin. In the brain, there's a map of your body laid out in cortical columns, but it's highly distorted because the areas represented on each column can vary. This is called the homunculus—a little picture of your body on the surface of the cortex in the somatosensory cortex. The hands and lips are really big because they have high acuity; the column represents a smaller area of skin there. All parts of your body are represented this way, giving a complete representation in the somatosensory cortex.

The retinas work the same way. They're patches of sensory tissue, also represented in the visual cortex. A column in the visual cortex represents a patch on the retina, but this can be highly distorted. Patches in the center of the retina are very small and have high acuity, while patches at the perimeter are much larger. If you look at how images are represented in the cortex, it's very distorted—like a fisheye lens. No one actually looks at that image; this confused researchers for a long time. The same applies to the auditory cortex, though it's harder to understand. You have an array of sensors along your cochlea, and each patch of the cochlea gets a column in the cortex.

The general rule is that anything coming from the sensory organs goes through the thalamus before reaching the cortex. In the visual system, the retina sends signals directly to the thalamus and then to the visual cortex, but auditory and somatosensory signals are usually preprocessed along the way. For example, information from the ear is processed in the inferior colliculus, then goes to the thalamus, and then to the cortex. Older parts of the brain do some preprocessing on the inputs, more so for touch and hearing than for vision. All information goes through the thalamus to the cortex, which is why the thalamus is called the gateway to the cortex. Now we know that when information goes from one part of the cortex to another, part of it also goes through the thalamus. It's not just about getting information into the cortex; it's also about transferring information between regions. I hope that helps explain it.

That's the classic homunculus picture. If you think about vision, it's in the very back of your head, but the body representation goes across the top of the head. You can see how much each column represents—hands and face are large, the rest of the body is proportionally smaller.

It seems like this mapping is built in at birth, but it changes as you grow. It is built in, but if you have early trauma or even trauma later in life, the representation in these columns can change. For example, if part of the cortex representing the hand is damaged, other parts of the cortex can take over those functions. The map can shift around. If someone is born blind, their visual cortex can be repurposed. So it's genetically determined but also very plastic.

The largest areas of the cortex are for vision—primary and secondary visual cortex (V1 and V2)—and primary and secondary somatosensory cortex. The auditory cortex isn't as large, and there are many other parts of the cortex not directly associated with sensory organs. These are part of the hierarchy, and their purpose and operation are more mysterious because you can't just probe the skin and see a cell light up.

Why doesn't vision have preprocessing like the other senses? That's a great question—no one knows. It's an interesting clue that vision doesn't require it. The retina does a lot of processing; it's a complex organ, so it's not just photoreceptors sending signals to the cortex. In the auditory cortex, for example, we have a sense of where sounds are coming from. When we hear a sound, we know its location around our head and how far away it is. This is largely due to very small timing differences between sounds arriving in the left and right ear—differences too small for most neurons to process. If you sent those signals directly to the cortex, it couldn't handle them, but specialized neural centers evolved to extract that location information before sending it to the cortex. That's an example of why you might preprocess something. There's a lot of literature on this, but with vision, the lack of preprocessing makes it easier to study the system since we don't have to account for as much complex preprocessing.

Okay, so just to echo back what I had: the journey of information from the sensor to the brain—your finger touches something, and the nerve essentially sends back feature data. There's no location yet; it's just, "this is rough," or "smooth," or "hot." There's location data in the sense that it goes to a part of the cortex representing that part of the skin. It doesn't get put in a big pile of data for someone to sort through. There's myolingo, which is before it's part of the cortical messaging protocol; it doesn't have that pose yet. Your finger might be at a different location relative to your body, and that information needs to be integrated somewhere. Also, if you're rotating your finger around a point of contact, like on an edge, you'll get different sensations on your finger. At the point of the fingertip itself, it doesn't know too much; the brain knows it's coming from the fingertip. What Will is getting at is that at some point we need to integrate proprioceptive information and information about how we moved our muscles to know where the finger is in space relative to your body, and the question is where exactly that information would be integrated.

If you think about vision and audition, the information from the retina doesn't tell you anything from a patch in the retina. It doesn't tell you where it is in space or relative to an object. The general principle is that the brain has to be fed movement vectors, not location data. I used to misunderstand this. The most important thing is, let's say the retina senses a patch of information, and then it has to know how the retina is moving through the world. It's a combination of movement data updates through path integration; it essentially eliminates hypotheses. Viviane and Niels could talk about how we do this in Monty, but the information comes in from the retina and doesn't say, "this is on the side of the coffee cup." It doesn't know that. Internally, there are different models, and it's going to take a sense, move, sense, move, and ask, "what are the only things consistent with this series of sensations and movements?" Maybe it's a coffee cup. Then, you're on the coffee cup, and every movement from now on can be path integrated to say, "here's the new location on the coffee cup." But there's no direct information from the retina saying where it is in the world.

Touch is a little bit complicated because we have this proprioceptive system, which provides some information about where your fingers are relative to the body, but it tells you nothing about where things are relative to objects you're touching. That has to be inferred through a series of touches and sensations.

So is it true to say that—sorry, go ahead, Viviane. You can go ahead.

Okay, so is it true to say that if you didn't have any models in your cortex yet, vision wouldn't know anything? There'd be no way it could figure anything out until you have the models in your cortex already. If you didn't have any models in your brain, nothing would make any sense at all. You'd have zero idea of anything. The only way to learn models would be through one of your sensors that has location information about where it is, so it can build a model, and then your vision system could use that. The most important thing is for it to know how the sensors are moving. You could, from scratch, take a fingertip with no models at all, but as long as you know how the finger is moving through space as it's touching things, you could build up models.

It's just like sticking your finger in a black box or some oddly shaped object you've never touched before. As you start moving your finger around, you begin building a model of it. This might be a useful contrast with smell, at least in humans, which is a much simpler, almost model-less way of representing things because there isn't much movement information. Maybe it's different for dogs, which can sense wind direction, sweep their noses, and do a lot more movement through space. Without movement, all you can really build is similar to what you have with smell: you sense something and can associate it with something else, but it's just a vague cloud of information without structure. This is dangerous, and this is good. Dogs, as you mentioned, Niels, stick their noses on the ground and on objects. For them, the nose is like a really great finger. They know how it's moving through space and can build structured models of the world through smell. They also get a sense of smell flow; their nose is wet to detect wind direction by which part is drying out. They can tell the direction of footprints and know which way you're walking based on the smell of your footprint, so they know the shape of it. This is an interesting example. For humans, smell is a poor sense; we don't really build models based on smell. We can associate several smells with different models and states, but on its own, it's not a very good sense for us. For other animals, it could be a very good sense because they have a much better sense of movement, flow, and direction, and better sensory detectors like wind direction. All these work on the same principles. The learning module does the same thing. That's the mind-boggling thing about all this—it just goes to the cortex like every other sensory organ. 

Going back, if you have no models and only have vision, you could still learn as long as your eyeballs can move. You would learn something. You can learn as long as your sensors can move and the brain knows how the sensors are moving—literally, they're moving in some direction at some velocity. That's all it takes to learn. You have some sense of distance based on stereoscopic vision and other cues. That's a challenge for vision. Even people with one eye can learn too, so the issue about depth is something we've never really addressed in our work. I don't think we have. Viviane, some of you have, but I don't think we ever do. It seems like we use a lot of techniques to detect distance. One of the primary ones is when you move your head slightly left and right—even with one eye, you can tell the relative motions of things in the foreground and distance. Parallax. That seems to be a really important one because you can do that with one eye.

There are a bunch of tricks the brain uses for this, but we've never really dealt with that. It's much easier to just say, we're sensing on the object, we know we're on the object, and as we move, we know how we're moving relative to the object.

Maybe some of the extra layers or subtypes found in the visual cortex do some of this depth inference processing. That's purely speculative. Cortex is very uniform, but one of the biggest exceptions is the primary visual cortex. In some mammals, not all, there's what's called a striate V1, which means it has extra stripes or layers. No one really understands what they're doing, and we've speculated that the unique thing about vision is related to sensing at a distance, so maybe these layers are involved in that—doing comparisons left and right, the parallax thing. It's a reasonable hypothesis, but we haven't really explored it.

Some animals don't have striate V1 and still see pretty well. For example, I think dogs don't have striate V1, but they still see. They may not see as well as we do, but they see.

I have a question related to transmitting movements versus locations. In Monty, we are actually communicating locations in a common reference frame—location relative to the body—and the learning module infers the movement between two successive locations. We've been saying this is pretty similar; it just changes where we're calculating the relative movement and when we communicate that. But there's one crucial difference: we really need this location in a common reference frame for voting. When we vote, we need to vote on locations in some common reference frame, and I don't see how that can work if we're only getting movement information.

We also have to vote on orientation, and we haven't addressed that either. We would need the orientation in some common reference frame or orientation relative to something. Is it mostly the orientation that matters? With the vote, there's usually some displacement between what the two learning models are seeing. If you pass the displacement, as long as it's oriented correctly—if I have two fingers moving on an object and they want to—how would you know? If I have the location of each finger relative to my body, it's easy to calculate the displacements between the fingers and use that, but if I only have how the fingers are moving in space over time, then I have no idea how they're located relative to each other.

This is the argument for why there are two maps: object-centric and egocentric. I can't think of the word for that pathway—the where and what pathway. Allocentric. The where and what pathway is the formal term. There are two separate processing streams for all sensory organs, at least for touch and vision. One represents space relative to the body, and one represents space relative to external objects. That fits with what Viviane was saying: you need the egocentric space because, with that, you can determine the relative position of the two sensors. Is that right? Yeah. Have we done anything like that? No. Right now, we're just assuming that all learning modules have access to that information.

That's something reasonable; we could probably do that with brute force engineering instead of modeling how the brain does it. We've often argued with the allocentric, or the what pathway, that as long as they agree on a particular allocentric reference frame to use when voting, it doesn't matter too much. There is evidence of that—it's almost like anchoring to an object in the environment, and then you do calculations relative to that. That might just be an extra complication for us. Take Vivian's example with the two fingers: you want to know where the two fingers are relative to each other. In a biological system, that's really complicated because of noise, joints, and proprioceptive information. But if you had a robot, it's a basic engineering problem, so why bother to do it the way the brain does? As mathematicians or engineers, we do it differently.

To me, it's a pretty important question for us to figure out because it determines the cortical messaging protocol—whether we're sending locations or displacements. That's the one place in the cortical messaging protocol where we might make a bigger change in the future, from locations to movements. In case it's not obvious, this coding has to occur. There are so many mysteries about the brain that were resolved by understanding that each column is doing its own sensorimotor learning. Yet, we have this ability to do flash inference: you can flash an image in front of you, and without moving your eyes, you can do limited object recognition. I can flash an image, and you say, "That's a boat," "That's a car," "That's Abraham Lincoln," or whatever. There's no ability to move your eyes, so all these models are getting just one impression and have to reach a consensus. We know it has to occur, and the idea of voting came up in that context. For a long time, vision researchers ignored the importance of movement because of the idea that you could do flash inference. They said movement can't be important because you don't need movement to recognize an image, at least for limited recognition.

We know it's happening; we just have to figure out how we want to model it. I have a question regarding this, because we're talking about a body-oriented reference frame. We can engineer physical things, but are we able to engineer the corresponding body-centric reference frame in abstract reasoning in space? Do we know all the consequences of using an engineering substitute for this? No, we don't, and we also don't know if that happens in abstract spaces. I don't know what the equivalent of flash inference is for abstract spaces. That may only happen in touch and vision.

That primarily happens there.

When it comes to abstract reasoning, all we know is that the cortical architecture is very much the same. It must be working on the same principles, and we know that's sufficient, but many other parts are mysterious.

Nice picture, Niels.

This was just while we were talking about the common communication protocol. Jeff, Viviane, and I were discussing some motor policy topics, and this question came up about displacements versus location. One of the policies we've recently implemented is moving back on an object: you sense a wine glass, move along it, and then make a saccade to the mug. This isn't in the straw world, but you sense that the evidence you're getting is now for a very different object, so you move back to where you were until you recognize the first object.

Are you learning the object, or is this after you've learned the objects? If I've already learned the wine glass, wouldn't I know that moving towards the mug is going to be off the wine glass? That's part of it, but also, assume for this point that you're not certain it's a wine glass yet because you've only gotten some evidence—you've traced out a little bit, but suddenly you're getting new input. The question is, there are various ways we can communicate to the learning module, whether it's at a scene level or an object level. If it's modeling the wine glass, it knows it's now very far off its model of the wine glass, or its most likely hypothesis—it's moved very far away.

If it's going to propose to move back, it has to communicate somehow to the eye that it needs to go back to this point in space from where the eye is currently looking after the saccade. The details here really matter. Maybe a more general question is, we have some top-down goal from the learning module that's representing objects or scenes, saying, "I want you to move to this particular point in space from where you are right now." This is one example of why that might happen, but I'm still confused by this. If I have a model of the wine glass and I know I'm on the wine glass, then if I move off of it, I know I'm moving off the wine glass and shouldn't expect to see the wine glass anymore. But if I want to go back to the wine glass, it's an interesting question—oh, I had to have some sort of temporary memory that I was looking at the wine glass. If I could invoke that temporary memory, then I could just say, "Oh, I can go look at any part of the wine glass. Let's go back to the wine glass and look at the stem." If I can just invoke the memory that I was sensing a wine glass at this position in this orientation, I could go anywhere; I don't have to go back to where I was.

It seems to me more of an issue of temporary memory. I was looking at one object, and now I'm looking at another object. I don't want to forget that I was looking at the first object. That's where maybe the scene representation helps. That would presumably have a representation that the wine glass is here in space, and that doesn't change when you move on to a different object, whereas the low-level learning module might be like, "Okay, I'm now on a new object, so I'm forgetting about wine glasses, now thinking about mugs." For a moment, let me just not call it scene representation. How about I just call it temporary memory of the things I just looked at? This is something we haven't really dealt with much, but the vast majority of our lives, we're going around looking at things we know and recognize, but they're in new arrangements. You're walking down the street, you enter a shop, there are tons of things around you, and you continually focus on different objects, building up a temporary composite object. You're basically saying, "Right now there's this cup on the counter, and there's this bin over here on this desk," and you have some temporary memory of this. If you left the shop and came back in again, you'd remember all this stuff, but a day later, you'd have forgotten it. Some of these things could be addressed by having, and we think a lot of this happens in the hippocampus, in the hippocampal regions.

Yes, having temporary memories of composite objects. If I'm recognizing the wine goblet here and I jump over and recognize the coffee cup, I'm building up a composite object of the wine glass and the coffee cup that's only going to be good for a little bit of time. That's how I know where to go; I can just walk through this composite object, saying, "Okay, let's go back to the wine glass." So we know the representation, whether it's in the hippocampus or whatever, the temporary representation of this compositional arrangement knows where it wants to go on the object.

Now it needs to communicate that in a useful way to the kind of motor system that's actually going to carry out that movement.

For example, one way we thought it could work is if it's not going directly to a subcortical thing—let's say it's a fairly complex movement, or maybe this is happening subcortically—but basically, if it's communicating a location in egocentric coordinates, then the eye can, if there's something modeling where the eye currently is in egocentric coordinates, know the eye is at this point in space. Whether this is subcortical or cortical, it's like an eye model. It doesn't care about what it's looking at, just that it's looking at something at this point in space at that distance, and then it's receiving a sub-signal.

It's basically that common cortical messaging protocol question in a different application. The learning module knows where to move on the object's reference frame, but how to communicate it to the actuator, to the finger, to move to that location? We would have to communicate the location in body-centric reference frame or in the finger's reference frame. Maybe not. These are hard things to think about, but again, I'm thinking: you have a wine glass, and let's say you're just looking at the wine glass and now you're looking at the cup. In my mind, you've built a new temporary composite object of wine glass, cup, and whatever else you looked at recently. Now the question is, given this composite object that has a bunch of features, I want to go from one location on the composite object to another location on the composite object. That's no different than moving from the rim of the cup to the handle of the cup.

It's the same thing. These are just features of the object and of the composite object.

If I can solve the problem of how to go from a particular point on the rim of the cup to a point on the handle of the cup, I think you've solved the problem of how to go in from the wine glass. That's fair. That's maybe a simpler way to frame it because it is the same issue, but I think it is still a question. It's just a question of whether we communicate a location in body-centric reference frame that we want to move to, or if we communicate a displacement, like a movement that we want to make. I think it's a displacement. I'm not sure. This is what I think it is. To me, it feels like you don't have to go to body, the biocentric coordinates.

You've got a movement in the object's space, right? I want to go from the rim of the cup to the handle. That is an allocentric movement—go this direction from here to here. Then all I have to do is run that back, take the orientation of my finger or the eye, and calculate what that is in body-centric space. It's like when you're reading text: if you tilt the text, you're still reading it, but it's moving in a different direction, though you're still moving in the same direction on the text. That would be the way I'd approach it. We generate movements in the allocentric space and convert them to the egocentric space based on the current orientation of the sensor to the object. I'm not sure if that would work in all cases, but it seems like the first place to start. It does feel like displacement would be the most natural.

Can we use the word "displacement"? What do you mean exactly in this case? Communicating a direction in the allocentric space—displacement—would originally be in allocentric coordinates, but because it's a displacement, as long as it's oriented and scaled correctly, it could be interpreted in another space. The way I look at it is we have two points on the cup, or two points—one on the cup and one on the wine glass—it doesn't really matter, it's the same thing. We have to calculate, at that point, maybe what you're calling displacement: a direction and distance in the orientation of the object we're modeling. The one complication is something like the eye. If it wants to move to this place, the eye isn't going to make this exact movement; the eye is going to rotate in the socket to enact that displacement in perceptual space.

Why are we rotating the eye in the socket? Because we want to look at the new point. We move the eye. I'm trying to move away from the finger example, because the finger feels more like we're still moving in a similar space, but the eye is moving through perceptual space. The movement of the eye itself is very different, so there needs to be a structure that knows how to do that mapping. With the finger, how I move it from one point to another depends on its orientation. I was oversimplifying the finger, but the eye is definitely a very different type of movement. The only reason the eye is different is because it's doing it at a distance. I talked about that earlier. That's the thing we haven't really dealt with. There's an extra level of calculation because I have to know how far away my eye is from the object to know how far to move the eye.

Exactly. I'm curious if you feel like that is something you might have—a learning module that's modeling eye space and is receiving, or if it's a subcortical structure that's doing that. The only thing I need to know is I have to have it—somebody has to keep track of how far away this object is from my face. Right now, I'm looking at boats, and the further boats are, I have to move my eye less; if the boat's closer, I have to move my eye more to scan from left to right. We have to take account of how far away this thing is, which is actually the same as keeping track of the scale. I associate distance—it's far away or close—but it's also the same as if it was a larger boat and a smaller boat. Those are intertwined. There's a scale issue, which is tied into distance. I'm not sure why we need to have a separate modeling of eye space. I know which direction I have to move the eyes; the only question is how far. That is an issue of scale, and scale is tied to distance. I don't know how to tie those together, I don't know how the brain does that, but it's pretty simple. You have to know the scale of the distance, have a scale factor, a direction, a distance in allocentric space—like the distance on the boat or the cup—and then scale it for how far away it is, or what size it is right now.

Does that make sense? That's an interesting point, the connection to scale. As long as you have the orientation and the scale, then it is just a distance, or just a normal displacement. This is an interesting question; I don't know if we've ever really talked about it, but scale and distance are obviously tied together, but not always. I can have a small cup and a big cup right next to each other, and they're just different scales. Or I can have the same cup from a distance, and it's the same as the small cup, but I don't see it as small anymore. Somehow the brain—maybe it's in those extra layers in V1—is keeping track of the distance of the thing.

Which we don't really have in touch. We do have in vision. In audition, we have a sense of distance.

I try not to think about audition too much; it's too complicated. Maybe we also—I don't want this to turn into a brainstorming meeting. Sorry, that was probably longer than needed. It's always so fun to do that. A chance for the newer people to ask some more questions too. Sorry. No worries. This is also very useful. Thanks, that was really useful feedback, actually.

I have thousands more questions. I could take over the whole meeting if you want. Go ahead, because we were taking it over. Okay, I have a hippocampus question. From my understanding, the hippocampus is responsible for temporarily building scenes, keeping track of what's happening right now, and using models I've built to quickly populate that scene. If I go into a coffee shop and look around, I can identify a human, coffee cups, a machine—I've seen all these before and have models for them in my neocortex. I use that to build a scene in my hippocampus very quickly, and I'll remember it for a day or two. If I go into that coffee shop many times, the whole coffee shop slowly becomes a model in my neocortex that I can use and recall. For example, the coffee shop I went to every day two years ago—I can still recall it in my mind, remember where all the objects were, and so on. 

My question is, what's happening there? Where is the information going back and forth? How do you think about this? 

The way to think of the hippocampus, and this is a simplification, is that it's like the rest of the cortex but with super fast learning. Everywhere else, you have to form synapses to learn. In the hippocampus, you don't. There are things called silent synapses—tons of extra synapses that aren't doing anything, so you can turn them on instantly. It's the same mechanism as in the rest of the cortex, very similar, but much faster. That's useful because I can quickly build up a scene. People used to think that memories were transferred from the hippocampus to the neocortex. As you experience something repeatedly, like your coffee cup or the house you grew up in, which you haven't been in for 30 years, you still have a map of it. Somehow it gets transferred and becomes more permanent in the cortex. They used to think it was actually transferred, but I read a paper that said it's not actually transferred—it's learned again in the cortex. If you're exposed to something repeatedly, it's slowly learned in the cortex. The cortex can't learn it quickly, but with repeated exposure, it can learn slowly. So you have fast learning in the hippocampus and slower learning mechanisms elsewhere in the cortex. No information is being transferred back and forth; it doesn't work like that.

Does replay during sleep still help to relearn it in the neocortex? There's so much literature on that, it's really confusing. I don't have an answer. People were looking for how information gets transferred from the hippocampus to the cortex—that was the theory. The idea was that replay during sleep is when it's transferred back. I don't know; it's confusing, and no one really understands sleep. But I read a paper that was pretty definitive, saying it's not being transferred—it's being relearned independently of the hippocampus. That makes more sense to me, because I don't know how you would transfer a model from the hippocampus to the cortex unless you do this replay thing, but then what are you doing—training it backwards? It makes more sense that if you're exposed to something repeatedly, it gets learned in the slower neocortex. This also fits into the Thousand Brains Theory regarding what models are learned where, given various resource constraints. 

There's evidence that when you highly train something, like a musician, the memories used to perform move further down in the cortex.

If you're new to music and just trying to play, it has to be processed very slowly, going up multiple levels of hierarchy or into the hippocampus. But for a musician, it's been relearned and is in the lowest levels of the cortex—no thinking is required, it just happens. I've argued that when we read letters or words, if you're a good reader, much of that probably occurs in V1, which is contrary to what most people believe, but it's such a highly trained skill. We don't have to look at the letters and think about them anymore; we just recognize the word and move right through it. 

The first time you learn anything, it's probably in the hippocampus, and then with practice, it's relearned elsewhere. The things you practice the most and are highly learned are in the lowest levels of the cortical hierarchy. 

I have a question following that: there's procedural knowledge in many professions that people do all their lives, but they forget it as soon as they stop participating in that environment. This is why we have checklists and other tools. How does that play into what you just said? Do people forget it right away? 

Yes, for example, pre-flighting an aircraft or procedures for flying instrument flights. If you're flying every day, you remember all the rules for the airspace and who to contact. But if you step away for a month or two, you start forgetting the sequences—how you're supposed to execute them. If someone stops flying for two years, they might not be able to fly. Remembering which procedures to execute to enter controlled airspace, for example, just goes away.

In general, it's pretty obvious: we forget everything, and our memories decay. Neurons are forming new synapses and losing old ones all the time. Forgetting is not surprising. Even something like my childhood home—my memory of that house is now far more impoverished than when I lived there. I only know the basics, and maybe even those are wrong. Maybe the airspace example is easier to forget because it's such abstract knowledge, so you can't really learn a model of it in the lower levels. 

Also, getting it right is so important. It's similar with surgical checklists—often they're quite basic, like counting how many sponges went into the patient and how many came out. People can remember to do that, but it's really important to do it correctly, so you make sure you have a checklist. Someone who hasn't ridden a bike for several years won't be as good as when they last did it, but they can still vaguely remember. I'm sure, Tristan, you could fly a helicopter. I have the motor skill, just like riding a bike—I could execute helicopter flight, but I don't know the sequence of how to turn the engine on. That's gone.

We can speculate on these things, but it may not be that fruitful. Maybe that's a long sequential memory, versus our ability to remember things in general. We forget everything, but maybe some types of things, like sequence memory, we forget more rapidly. I don't really know why. It doesn't bother me. 

The bigger question for Monty and the Thousand Brains Project is that real brains are always forgetting, and there are reasons for that. We have biological constraints on material, energy, the volume of our head, and so on. With Monty, we may not have as many constraints. Maybe we can just keep adding neurons, synapses, dendrites, or change the learning rules, so the system hardly forgets anything. That might be possible. There are things in biology we don't necessarily want to emulate, and I don't know yet whether it's helpful or just necessary from a biological point of view.

I think there are definitely different speeds at which we forget different types of information. For example, children—one of the last things they learn to store in models is where they learned specific information, like who told them something. That's also one of the first things you start forgetting as you get older or develop dementia—you forget where information came from. So it seems like the type of information has different rates of forgetting. We also forget names of things. I have a theory about why that is, but yes, there are different types of forgetting.

One thing you asked earlier about the hippocampus: it might be helpful to have a visual picture of what's going on in your head. The inner cortex is a sheet of tissue about three millimeters thick. It goes in and out of folds, around the side of your head, and folds up into the inside, underneath other parts of the sheet. At the very edge of the sheet is where the hippocampus and entorhinal cortex are—it's a continuation of the sheet. The sheet stops looking like prototypical neocortex, with all its layers and cells, and starts degenerating into something different—a three-layer structure. The entorhinal cortex is sometimes called the three-edge structure. 

Imagine you have this cloth or sheet, and it's now not looking the same as before—it's getting thinner and folds back onto itself. The hippocampus lies on top of the entorhinal cortex. One speculation about evolution is that these are older structures. The entorhinal cortex and hippocampus—what happened is the entorhinal cortex was folded back, and on top of it was the hippocampus, which became the six-layer neocortex. From there, it starts being prototypical for the rest of the sheet. You might have started with this wrapped-over structure of two three-layer structures that became the six-layer structure, which becomes the cortex. It's a continuation of the same thing. This theory—it's not my theory, but someone else's—says the neocortex is basically just a better-engineered version of the old structures.It figured out, "Let's just tune this up and then make a lot of copies of it." The fact that the two are aligned on top of each other is literally how the cortex became the six layers of the hive. I hope that's helpful and makes the argument that these aren't really different structures; nature just preserved them in a different form.

I have another question. You're building a scene in the hippocampus very quickly, almost instantaneously, but there might be uncertainty about the scene you're building. For example, we think this is a glass, we're pretty sure it's a glass when you put it over here, but does the hippocampus control motor action? Does it have a say in how you're going to move your sensors around?

I am not aware of the literature on that.

I would guess that if the model of the scene, which is just a composite object, exists in the hippocampal complex, then the hippocampal complex would have to direct behaviors relative to that model. Who else would do it? You need the model to direct behaviors. If I walk into a dining room and quickly notice where all the dishes on the table are, and I sit down and want the green beans, now I know which way to reach to get the green beans. I think that would have to be directed from the model in the hippocampal composition. Who else would know how that is? I actually don't know anything about the literature. Maybe Niels or Viviane does. I don't know anything about the literature in terms of how the hippocampal complex directs behavior.

I just did a quick Google and it says that the hippocampus shows motor activity, and another article says recent evidence has shown that the hippocampal complex is involved in the generation of ML behaviors and overnight consolidation of motor memories.

That's all very true. I would expect, just as in the rest of the cortex, these layer five cells direct the motor output of the cortex. I would expect you would find the equivalent somewhere in the hippocampal complex. It's probably known. I bet there's something that Ali can do—the work from the researcher Ali Pasha last week. Would he be able to reach the hippocampus and do an experiment there? Would that be interesting? I think I asked him that question, didn't I? I can't remember what he said, how many millimeters he can go. I think it was hard to get to. I think he said he has to be intrusive. Oh, that's right, you had to put a thing in so you can get the light. But I can't imagine that it wouldn't have motor output. I feel like it has to. If that's where the model exists, that's where it has to be. No one—you need a model to generate motor output.

I seem to remember, when we were looking at one of the Shuri and Gilliam papers, that some of the L5 outputs from prefrontal cortex went to the hippocampus, implying that the motor output of prefrontal cortex could be modulating memories, which I thought was interesting. I don't remember that, but wouldn't there be an alternate interpretation? Remember, the motor output of every cortical column—those layer 5 cells—goes two places. It goes subcortical to generate the motor behavior, and it also goes to the next higher cortical region, the next higher cortical column. So wouldn't the prefrontal cortex, its motor output going to the hippocampal complex, just be the feedforward motor command, like everywhere else? That might be the same as layer 5 cells going to the next higher region. The hippocampus is just the higher region. That's how I look at it. I view the hippocampal complex as a bit wonky; it's not as regular in structure as the rest of the cortex, but it does basically the same functions. Learning a scene is just learning a composite object, that's all it is. The motor output from prefrontal cortex would presumably go someplace subcortically and also upstream to the hippocampus. If we can explain why layer 5 cells project to the next higher region, then we can explain why it's projecting to the hippocampus.

That's a really good way to think about it. It's definitely true. We also need to have short-term memories of more abstract things, and the hippocampus is not just seen in analysis. They've shown it's short-term memory of almost everything. That's where you build programs, I imagine. That's where you're building software—in the hippocampus, like short-term memory. You get interrupted and now you can't remember any of it. You fall off your bike and you can't remember anything. That's too soon. Oh dear. I had a real concussion this last Friday. I don't remember this, but I was sitting on the ground at the end and someone was yelling at me, "Jeff," and I didn't even hear. I didn't know I had broken my arm.

Anyway, more questions? In that paper that said there isn't transfer from the hippocampus to the neocortex, that's not what REM is for, what does it say—what was its position on if the hippocampus is damaged, then you can't form any new long-term memories? Did they have an opinion about that?

I don't remember. I read this paper a long time ago, Will. I can't cite it. I don't know the author.

Alright, so there's this basic idea: if you remove the hippocampus, you can't form any new long-term memories, right? That was evidence—this is the famous patient where they took out both of his hippocampi and then he got stuck and couldn't remember anything.

That seems contrary to the theory I just told you, that memories are not transferred, and I believe that paper had an explanation for this, but I can't remember it. Let's see if we can recreate it. Why, if you don't have a hippocampus, you can't form any kind of temporary new composite memories of anything? Why would that be? I'll leave it at this: if you remove some part of the brain and just cut it out, all kinds of things happen. Who knows what's going on? What is REM sleep doing? We don't know. No one really knows what REM sleep is doing. If you interrupt it or cut these other lines, I don't know. There are a dozen reasons why, if you remove someone's hippocampus, they can't form long-term memories, other than it having to be that the memories are transferred from hippocampus to cortex. It could just be that there was cabling going through there. There are all kinds of reasons. That's the crudest thing you can do—just chop out some tissue. It's surprising that the thing works at all. I wouldn't lose sleep on that one.

Can I introduce an abstraction to understand Monty a little better? This is more about sensor modules and maybe it will touch on the benchmarks a little bit, and maybe about the whole model of the eye, maybe why it's important or not needed. It might be related.

We can think about humans as patches of sensors. Sensors are everywhere—receptors distributed across the body. For example, photoreceptors are only in the eyeballs; as far as I know, I don't have them on my skin because my skin can't see. Proprioceptors are in muscles, so we have different distributions of receptors. Our photoreceptors' movement is constrained by muscles, joints, and bones; we can't rotate the eyeball backward. Bones and muscles give us constraints.

For Monty, at least the current version, it's completely virtual—it's not a robot yet, which would also constrain movement. Technically, Monty could have a single patch, like a finger, but it doesn't need to have just Merkel cells for pressure. It could have photoreceptors, pressure, temperature, proprioceptors. I think we have a Monty surface that is not just touching but also seeing, as Niels pointed out.

Technically, we could make a "super nose" Monty that can move the nose everywhere and build a model. Because we're not constrained, this goes into the benchmark discussion. If we have sensors everywhere, a lot of AI benchmarks are based on how humans perform, but it's hard to think about benchmarks for Monty because we could put sensors anywhere and move them at lightning speed. When comparing the agent versus the surface agent on benchmarks, more generally, we could have a distant agent that's just an eye with photoreceptors. That scenario is closest to object recognition and classification in AI/ML.

One of the key ideas is that we're building a very general-purpose sensorimotor modeling system. We're not trying to build a human or simulate human sensors and movements; it should be very general. We could imagine a learning module navigating the web by clicking on links and so on. This is part of the key idea: we could have superhuman or supernatural sensors and movement, and it would still be possible to learn and model that with a learning module.

Related to benchmarks, we don't have a benchmark for what a superhuman or super agent with all the sensors would be like. The problem with existing benchmarks is that they're designed for the technology people already have and the tasks they've assigned themselves. Often, those aren't the best benchmarks for us. We have all kinds of new capabilities, so I've learned to be very defensive about benchmarks. I don't want to do these benchmarks; we ought to design a system that works on these principles and define our own benchmarks for sensorimotor learning systems, as opposed to benchmarks people have done in the past, like labeling images. Existing benchmarks aren't our friends—they're the friends of the people who created them for their systems. We have to prove Monty is different and better in ways most people don't think about today.

Moving forward, we have polar sensory modules. Getting into the code, we have a habitat distance sensor module and a habitat surface module for each distance. Do we want to go down one level and create generic classes for different receptors, like a photoreceptor class, thermoreceptor class, or proprioceptor class, and combine these sensors into an agent? Right now, we have distant sensory modules and surface sensory modules, and they both might get touch and vision, but maybe we need to separate out distance—whether it's on or away from the actual sensors. The distant one doesn't get touch; there's not really a sense of touch in any meaningful sense at this point.

In general, the view is that the distant agent could get anything that's electromagnetic radiation or any sort of wave-propagated information, like electromagnetic radiation and sound. Once we open source this, anyone can take any kind of sensor—ultrasound, any sensor, even an abstract sensor on the web—and write a custom sensor module for it that takes the raw sensory data and converts it into the cortical messaging protocol. That way, anyone can plug any sensor into the system and plug different sensors into the same modeling system. You could have a self-driving car with LiDAR and vision; all those sensors can have their custom sensor modules, convert the information to the cortical messaging protocol, and go into generic learning modules. All these learning modules can boot with each other.

You can build the sensorimotor and cortical learning modules in any arrangement and density, and hook them up to any kind of sensors. It'll work. We're actually at the end of this meeting, and Niels already wrote that he has to go to the next meeting.