So if it's all right with you, I thought I would try to summarize and organize the different ideas we had yesterday to make them more consistent. The big insight, or the odd thing we came up with a long time ago and discussed yesterday, is that there are two types of models in a cortical column: a morphology model and a composite model. This confused us for a long time because there are clearly composite models—things made of other things that can be rearranged. But then there are things like circles, egg shapes, or ovals that have no components, yet you recognize them. They have a shape, but no features you can point to at specific locations. That was always a problem for us.

If we adopt the idea that there are two types of models, how does the composite model relate to the feature model we talked about? It's the same thing. In a primary sensory region, there are no subcomponents, but there are things like color or texture. If you consider those as components, they are not necessarily related to morphology, but they are something at a location that could be read at that location. If we have a model that's purely morphology, containing only information about point normals and relative locations—like a model of a human with arms and legs—would that be a composite model or morphology? I'm not sure. Let's go through this conversation.

My initial concern is that it feels like morphology models are only at the bottom, whereas we had a way of describing it where morphology models can also be compositions. I don't think of it that way. I just think, here's a column, and how would a column do both of these models? Maybe I didn't think about how a column would have morphology models. The way we talked about it before was that there are two models: morphology and feature model, instead of morphology and composite model. That's the same thing in my mind. If you want to call it feature model, that's fine. A feature can be a component or something specific beyond point normal at that location. I would call that composite. If you don't want to call it feature, that's fine, but we started using the word composite elsewhere. You'll see in a moment, we think there are two types of input to layer four: one defines point normal, and the other defines what is at that point normal—maybe nothing, maybe just point normal, or it could be a coffee cup, a logo, or the color red. In my mind, those are all on a continuum. I don't make the distinction. If you want me to change that to feature model, I will, but the feature can be other objects.

Conceptually, the important idea is to keep the distinction: one is just a point normal edge or something, which could be an internal edge, not just an external edge, with no other detail beyond that. The other is that there is some detail at some location, and there could be a whole range of different details, including whole other objects. 

I'm also remembering how we talked about direct connections to higher levels of cortex, which would create the morphology model in the higher level. That was at least one possibility we discussed. For example, with a person, you would have a broader, more coarse point normal over a bigger region of space, resulting in a more coarse-grained morphology model. You could have a stickman, but then zoom in on an arm and have the morphology model for an arm. I'm still confused about this, but we can come back to it.

Last year, we had all these pictures of stick figures, and then they started dancing, and there's this Elvis Presley dance I want to get to, but for now, I want to cover what we talked about yesterday. The idea we had yesterday was that there are two different types of models, and they're really separate. You can have a morphology model without any features, like a circle, egg shape, oval, or maybe a cylinder. I'm not sure what the extent of it is; we'll come back to that. Then you can have specific versions of that—a morphology with specific features, which then becomes unique. For example, a cylinder with a handle or without a handle, a cylinder with a logo or without a logo.

This is, I know we've talked about this in the past, but I had forgotten about it and came back to it. There's a clarity in my mind that's driving all the confusion from the conversation we had yesterday. Before I go on, are there any other thoughts about that? There are these two types of models, and we can work from there. The features would be more like colors, and I'm going to describe them in terms of anatomy. The features come into layer four and have a large variety of possibilities. They're basically indicating that there's something at this location that's not a point normal. This also explains recognizing the fruit face because we have the morphology of the face, which is still a relative location of eyes, nose, and mouth, but the features are just different. It's just a different object ID. The sub-handle is still unclear. The border between morphology and features is a little hard to define at the moment; I think it's a fuzzy border. Take something like a word.

A word is composed of a set of letters, so it's clearly a composite feature-based model—these letters in these positions. Once we learn to read, we also create a morphology model for the word, and I can read a word without looking at the individual letters; I just recognize the shape. I can drop things out, and as long as it has the basic shape, I'll still read through it and won't even notice if something is missing. You recognize it by shape. There seems to be some sort of spread between these; it's not necessarily hierarchical.

That's another issue.

I want to get back to the idea from yesterday. The idea was, and maybe we had this before, that this requires two separate reference frames. You have to have one that's very unique for the unique optics, and one that's less unique for the morphology optic, but they're tied together and not independent. They have to be updated together. It requires two types of input to layer four, and we do see that in the cortex. One would be to determine the point normal, and the other would represent whatever that feature is at that point, if there is one. If you look at the inputs to a cortical column, there's an input that goes onto the distal dendrites of layer four cells. We used to think this would be driving the activity of the cells on the proximal dendrites, but it doesn't look like those are on the distal dendrites. We also see an input to the layer below layer three, the border. I've always believed, and there is evidence for this, but I'd have to look at it again, that there are these cells I mentioned yesterday—bipolar cells, specifically called double bouquet cells. These cells are very skinny, with very branchy dendrites that go down one minicolumn. They're controlling a minicolumn of some sort. My guess is that this input is determining the point normal, which is basically the edge orientation of these minicolumns. These double bouquet cells are complex cells with many synapses. The argument is that the spatial pooler is actually occurring at the level of double bouquet cells. They are essentially saying, "My whole minicolumn is active," but these cells are performing the spatial pooler operation.

That's one piece of it. That required two types of features: one like color, and one like point normal information. From the previous region, this would be called "logo," and this would be like the edge of the coffee cup or something similar.

Then we talked a lot about two different reference frames. It's quite distinct—two different reference frames.

That was a real challenge. We discussed how to have two reference frames both simultaneously updated. We talked about two possible ways of doing that: one was to use phases, where the same set of cells at one phase is sparse and at another phase is dense. We also talked about having multiple sets of cells, where some are dense and below that are sparse versions. I'm going with that one at the moment. This one's a little harder to understand and might not be right, but that's the one I started with. Partly because you can now see parallel construction here, which is nice. It doesn't mean it's right, but it's nice. In the lower layer, this would be layer five, and this is layer six—specifically six A. This is layer four here. There's a sensory input right at the bottom of layer five, just above, and it's very analogous to the one that's right here.

I don't know, but I'm going to argue that there are double the K cells that control the upper regions and double K controls of the lower regions. There's literature on this from a long time ago. I haven't looked at it recently, but there is some knowledge about the connectivity of these double K cells. I'm going with the idea that the same thing is happening down here in some sense: you have this input here, which is defining your set of grid cells—the classic grid cells, however they get created.

There might be, like in the Tank paper, six guitar modules, and each one has a cell active in each Ingram. The idea is that below the active ones, there is a more sparse representation. We talked about different ways of doing that. One way is each little slice, or the random SDR learned—that was another approach. The idea here is you have two sorts of representations: the standard grid cells, and then these very sparse representations, which would be difficult for most researchers to see.

The two models are defined between the black connections here, basically saying, "Hey, based on this point, normal, and the crude location of some sort—not the very sparse location—I could define a morphological model of a knot." There are some issues with that, but that was the idea we came up with yesterday. Then there was another set of connections between the sparse representation here and the sparse representation here, which would be the model of a unique object that has that morphology. So, same morphology, but now unique. I can have a lot of unique versions of basic shapes, basic morphology. That was a new idea yesterday: you'd have these two types of our classic model between location and feature. Now there are two of them, two reference frames, and two sets of feature types.

That was pretty cool. I really liked that. We talked before about having the morphological feature model and how that could work in layer four and lower level three, layer three. It's a nice idea how that could tie in. I don't think we had that idea before. This connectivity is an idea. Niels, you went there yesterday. In terms of the connectivity, that's how they could connect and that's why it's important. At the moment, don't ask yourself if the neuroscience looks like this—does this make sense logically?

We were talking before about the two models, and I think that makes a lot of sense. What we were talking about before with the sparseness of the lower level is particularly important. For example, even just how we would implement in Monty, we don't want to double the number of models we have of everything. The morphological model is going to do a lot, and the more feature-based model is just going to store a few points. You might move quite far before it even predicts anything. The feature model may have nothing to say until you get to a feature, and it would nicely fulfill that requirement. Number two, we want to avoid relearning the whole morphology just because we have an object with different features on it.

This is a very speculative idea—it's theoretically driven, but really speculative. The good news is that we can probably test it. There's probably literature out there, and this makes very specific predictions about the connectivity between double bouquet cells. I know I've read at least one paper on that in the past. I would look for this sort of separate connectivity. What do those double bouquet cells project to, and what's their distribution in the upper and lower layers? What do they look like? This basically says there are two separate types of connections. We might typically classify it as between layer six and layer four—that's the classic view. This is a little more subtle. These cells are different from those cells, and there should be connectivity like this. That is a new idea. I never searched for literature on that specifically, so we might see some papers that say this is impossible, or people have measured the receptive field and response properties of these cells. There's some literature there. It's not a lot, but it's there.

If you're a neuroscientist, you might say this is completely speculative, and it is, but it's dramatically derived. If we believe this is true, I would have never thought of this idea of just two models five years ago. We worked our way into believing that. If that's true, then there is a mechanism—there aren't too many mechanisms that would work, and this is the only one we've come up with. There should be literature that supports or invalidates it. It's testable to some extent. This is not one of those things where you say, "Oh yeah, it could work anyway." No, these are very specific predictions about these connectivities and even these cells—the idea that you have grid cells, but then you have other cells that are sparse versions of the grid cells. In the grid cell literature, you might find that too. There are all kinds of grid-like cells, but this is a very specific prediction: we're looking for very sparse connections or cells that, on a particular object, would always become active in the same location on the object. Basically, a place cell—think about that.

We don't need a whole set of place cells. Are place cells more sparse than grid cells? I don't know by definition, but if you only need to encode a few locations and don't need to densely encode space, it makes sense to use the grid cell encoding of space for path integration, and then have these additional cells that encode, "Okay, there is a unique feature here." I think the idea is that these all look like place cells.

Did you have any more thoughts in general on how we could get these sparse encodings of location? We talked about the two methods yesterday. I have no more thoughts about that.

In some sense, if we could prove there is a sparse representation like this, then exactly how it's derived isn't that important—just like exactly how grid cells are derived isn't that important. It's useful to know and to explore the holes in the theory, but it's more important to know that these cells exist than exactly how they're derived. I didn't think about that further. We put so much on the board yesterday, and I was all over the place and couldn't remember things. I'm just clarifying what we said yesterday.

The big insight at the end of the day was that these two different models could exist simultaneously, with a denser and sparser version. We had to come up with a way that when grid cells are updated, the proper sparse version of the grid cells has to be updated to do path integration. I found it more comfortable thinking about two cells per layer, but maybe the other method you mentioned, where you could pick randomly, would work too, but you'd have to learn it somehow. That has always been a challenge—how do you update a sparse SDR for location? If I thought of this as many different grid cell modules, and the cells move based on path integration, then it would work. I could spend more time thinking about that. It would be fun to come up with a mechanism and see how it would play out with those phase distributions and so on. At the moment, this was just my thinking.

Then, just playing out the examples we were talking about yesterday—the soda can versus the mug.

I guess this whole thing about something like a handle, which is morphological—how do we factor that in? One of the first examples we talked about was a cylindrical morphology object, and then it wasn't clear: are we transitioning away from a morphology model when we move to the handle, or is it just returning to the earlier question? The first issue I had here is the capacity of the morphology model—how many things can you represent? In this scheme, as I've shown here, it seems too limited. 

If you think about the upper layers, every minicolumn typically classifies, for example, orientation edges, and they may only represent eight or ten different orientations. As you move through the columns, they cycle around. There's evidence that's not correct, but if you accept that and there are very few, I can say, "Okay, I have these point normals at these eight different locations." Now, how many different locations could I use? If I just use a one-level grid cell for this, there aren't that many. If I take the tank paper and there are six grid cell modules, but they're all going in sync, there are maybe 30 different points on that grid cell. I don't know how many—maybe a hundred, but it doesn't really matter. Basically, I only have a hundred different locations I can represent by that. 

If I take the pure dense grid cell model, I don't think the upper layers are the problem; the problem is the lower layers. If I just take grid cells and ask how many different locations I can have, it's pretty limited. So I can't really learn too many morphological models if I stick to the dense grid cell. 

What could be a way to do it? If we think of these grid cells as implementing transition models—that's a term we talked about two years ago—basically, the grid cells. Normally, you think of grid cells as encoding 2D planes, right? But let's say in the neocortex, we can have grid cell modules that encode 3D shapes, like a cylinder. They encode that if you go 360 degrees around the cylinder, you're back at the original location. That would mean we need custom grid cell modules for different basic shapes. We wouldn't want to have—why would we really cost the grid on trees? Because they need to know the basic properties of geometrical objects. So you would have one for a cylinder that knows you go around, one for a sphere, and there aren't that many basic shapes.

The path integration properties on these three-dimensional objects would work because, as in high school art, you often compose objects from basic shapes. A cup, for example, is essentially a cylinder plus a half-sphere or a torus. We would have one grid cell module that models movement along a cylinder, and for the handle, you would use the torus grid cell module to navigate that 3D space. I wouldn't want multiple grid cell modules for this, but could the same grid cell module—what you're suggesting—is that grid cells aren't just encoding location; they're also tuned to action, since they need to know special path integration properties unique to those shapes. For example, in 2D, if you exit the plane on one side, you enter on the other, but that's not always the case in 3D.

You could model all of these with a 3D coordinate grid cell module, but it would be a separate issue to know what actions are valid on a particular object. You would still get path integration if you just had the cylinder model. The alternative is to have a 3D grid model that represents 3D space. Maybe it's not just a 3D issue; this also exists in 2D space. For example, a rat in a maze goes up one arm, down another, comes around, and returns to the beginning. They show that these are modeled as separate spaces, but the animal doesn't really know it's back in the same spot unless it observes the same cues.

One of the key things is that path integration only works within a local area and is noisy; it doesn't work well over long distances. Imagine walking in a big house with a hundred rooms. As you wander, you might be surprised to find yourself in a room you've already visited because you lose track. You build your map, and within a local area, path integration works, but as you go further afield, it becomes unreliable. You might come back to the same room or a different one and not be certain where you are. Maybe that's a scale issue—grid cells don't work at large scales. With 3D objects, you usually don't have that problem. My intuition, and what I've read, is that the idea of returning to the same place again isn't necessarily encoded by grid cells in this kind of three-dimensional circling context. Ideally, if you receive updated signals when you return to the same location, the same grid cells are active, but if you rely entirely on path integration or dead reckoning, that's not guaranteed. If you're moving around a mug or a house and getting updated information about your location, the real issue is edge wraparound: when you run off one edge, you need to know you're coming back to the same edge on the other side. You don't seem to get that in a 2D plane, even though it does happen, because you don't really know where you are.

Do grid cells know in advance that they're coming back to the same location through path integration? My intuition is that they would have to learn that. It's too much to ask of them. All the evidence we have for grid cells so far is that they work best in two dimensions and not well in three. I remember the bat papers we read. That's why I'm suggesting we don't treat grid cells as general 3D modeling units, but instead as more like a 2D plane, with the added knowledge that you can wrap around. My intuition is that it's a 2D plane without the knowledge of wraparound, and you discover that you've wrapped around when you return to the same place.

For example, if we have a cube, the grid cell module would model a space like how you fold a cube out of paper. But that's only one folding; there are other foldings where you might wrap around the square on the bottom. That would be the extra knowledge the grid cell would have to encode. For example, in George's work, he explored this by taking HTM-type models and extending them to learn sequences where you visit every transition and every edge.

And so that's one way to learn it, but then you're not getting the benefit of grid cells, which generalize to novel paths out of the box. I wouldn't want them to learn this for discrete transitions, but rather for continuous space. Basically, there are not that many basic objects, so we wouldn't have to learn this for all possible objects in the world—just for a sphere. I think the system has to make no assumptions about what the basic shapes and objects it's going to learn are; it just has to handle any. Personally, I don't think this is the way it's going to go. There are too many oddities about this, and grid cells don't seem to do this. We also have to match it to the anatomy, right? Didn't you also say that they don't work very well in 3D space? It's almost like you're doing 2D and then another 2D, and somehow you patch them together.

Take a die, like a die from playing games. Most people have no idea what number will be on an adjacent surface, but there's a pattern to it—they're all the same, and no one learns it. They don't learn to do what you just described. It's almost like you see a face, then another face, and another face. But I think it might be a different issue there, because with dice, how you move around it and where you get morphs. If you just look at the morphology, not the features, you know how you would move around the die, and then if you go around four edges, you're back in the original space. The issue is that you don't learn the features on the different edges and which edge they're on, but that might be because it's a symmetrical object and you don't really know how to anchor them on the die.

If I have a unique representation in the morphology model for every location on that, path integration should tell me what's on the next face. But maybe you don't have a unique representation here because it's symmetrical, so you don't know which face. Even the features don't tell you the orientation of the die. You need to look at two faces to know the orientation, because even then you won't know what's on the other faces. You just don't learn the transitions from the face of a cube.

But if you have a coffee cup, which is not as symmetrical—when I started the Momento logo, I could take a blank coffee cup and put the logos back on there like they're on this one. I would know how the logos are placed on this cup. I would be able to draw the same feature structure on this cup from memory because it is not symmetrical. I know the handle is here, and there's a top and bottom of the cup.

I've always been surprised how, if I asked you to draw the cup, you can draw it like that, but if I asked you to draw it from some angle with the letters, it's really hard to do. You just can't recall that. You don't have a map of what it looks like at an angle like this; you only have a map from planar views. That just seems to be an observation.

I'm going to go back. I still think there's a capacity issue here. Even if you're right, Viviane, that there are different models for different shapes—which I'm reluctant to accept at the moment—I don't know how that gets represented up here. I just don't know how it gets there, where they are, how many there are, or who decides what they are.

The world could be composed of one set of shapes, and another world could be composed of another set of shapes—who knows? One related idea we discussed was having a view sphere, like grid cells on a generic 2D sphere surface, where you're always looking in and projecting onto that. This means you don't need different ones for different objects. That might fit with the dice; if you can't really orient it, your view sphere becomes a mess in terms of which specific features are on which face. I remember there were some limitations with that approach. I would prefer we make no assumptions about the basic shapes of the world and let the system learn them, whatever they are. It will have some capacity limit and will learn whatever it can from the statistics of the world.

I'm not saying you're born with these; you might still learn about them. You don't really learn nuance after a certain point, or it takes a long time. One question is, can I do this with the same set of cells and learn all these different morphologies up to some limit? I think what you're arguing is that we have different sets of cells with different morphologies, and that's the part I have trouble believing. It seems like you have one set of cells that will learn some morphologies up to a limit, but there's no assumption about how many or what the different shapes are. I think you could reuse a lot of the cells; you would just need some kind of modulating cells that tell you to restart at one point once you go a certain distance.

We have a system now that could, in theory, learn any morphology, though it is limited. To me, the issue is capacity, not differentiation of shapes. It's really a capacity issue. If I take grid cells at face value, there's no evidence that there are different sets of grid cells for 3D, cylinders, or whatever. As far as I know, this phenomenon could be explained in different ways, but it's very difficult to learn a totally new shape. If you have a cloth that's wrinkled up and it's now a static object, it's a weird, abstract, or statue-like shape. Is it a complex shape or a novel shape? It's a novel shape. A folded piece of cloth is a very complex shape, but a simple shape—I feel like there's no simple shape we don't already know. I could come up with one.

It's interesting that the egg shape is a shape we know, but if I was never exposed to eggs, I probably wouldn't know it. The head is egg-shaped. Do you know the grebel objects? In some psych experiments, these are novel 3D objects that people have to learn. Even a two-dimensional object—they can learn it. They definitely learn better than machines, but they all come from generic objects. I just drew a two-dimensional shape; I don't recognize it, but if I studied it for a bit, I would recognize it. After seeing it a few times and practicing, it would become second nature. Maybe this is the way some face masks look or a bicycle seat. My point is, it's easy to come up with novel shapes that we don't already have a memory for.

Earlier, I gave the example about words. You recognize a lot of words by their shape, and I think that's well documented. It's clear to me that I do that. There are new shapes you weren't born with; they're just the shape of a word. As long as you get close to that shape, you'll recognize it as that word. I don't think there's a preordained amount of these things. Morphological objects are continually learned, like everything else, and there's a limited capacity.

My problem is that if I just do what I did here, the capacity is way too limited. That doesn't seem right to me. But that's not really a new issue, is it? This mapping between this layer and that layer is new, but the fact that grid cell modules aren't really sparse enough is not a new reference rate. Most people don't think of them as representing unique space. They don't think about the idea of a unique location. I thought that was Eli Feet's idea, at least, and that inspired Moser's work. We have lots of grid cell modules, and it gives a single unique location. We've adopted that here again by saying these extra cells below—it's just that we couldn't get the map to the anatomy correctly. This is a variation that could be mapped to anatomy, because there are not enough grid cell modules.

In the tank paper, there are six grid cell modules, and they all seem to be moving together; they're not mapping independently. Even though you have six, it's really like having one. Where are all the others? I need twenty of those, and they all have to anchor differently. I couldn't find any evidence for that. This is better because here, they're very sparse, and you wouldn't expect to find grid cells. You'd find more plate cells, as you said. The lowest green cells don't look like grid cells. If you probe them, they just don't look like it. Maybe we should review what we talked about yesterday—why these are sparser, or how they would get sparser.

The black ones would look like grid cells. The black ones look like grid cells. You were here for the tank paper, where we showed the six modules. We talked about missing ones—sometimes there was a missing one—and how you could scan above. Perhaps the missing ones capture some sparsity, just starting to get sparser below a certain level.

One idea is that you have a grid cell module, one of the six, and here's a cell that acts like a grid cell. Underneath it are a bunch of other, much sparser cells, like a minicolumn. Imagine six grid cell modules with an active cell in each. One possibility is you go down a bit, and now there's another similar layer, but only two of these modules are active. There are two cells; the others are not active. For six modules, choosing two gives 15 combinations. If you keep going down, you could have many such layers, each picking two out of six. Each layer acts like a grid cell module with only two cells active. That would be 15 to the 15th possible combinations. Each of these layers' cells would move around just like the grid cells. Anchoring means which of the six modules gets an active cell—that's the anchoring process. Each of the 15 layers could anchor differently. It feels a bit wonky, but it builds on the idea of a minicolumn structure, which we do see, and you're picking a sparsity.

When you do path integration, it's not random in each one. The active cells move together. That was one idea. If it worked, it would solve all the requirements. Viviane had another idea, which I never really understood. She started with the same basic premise, then thought she could just consider this all as one big structure and fix some random sparseness below. I didn't see how that worked because it didn't do path integration on these cells. Maybe she can explain. The idea was to pick random SDRs that are active when a specific state occurs—when the grid cell module is in a specific state, you associate that SDR with that state. The grid cells still do path integration, so when they're back in that state, the same SDR is active. You don't really wrap around; you just have to recognize when you're back in the same spot, and then it's the same thing. They don't really do path integration; they just keep changing randomly until you return to the same spot.

One thing is, we use a different random SDR depending on the object, so the location is specific to the object. That happens here too. It's easier to do it here; you just pick a random one. The path integration component of this example is wonky—you're not really doing path integration. The argument is that path integration still happens in the grid cells. The only issue is capacity: the active grid cells need to activate the correct SDR below. The correct one has to come from above. In the beginning, if we don't know which object we're on, it would activate a union of SDRs, then that gets narrowed down. It throws away the whole path integration idea. Path integration would still be done by the grid cells, but down here, there's no path integration—just random patterns associated with the specific object. Implicitly, by learning, you form an association. Every point has to be learned; every single point in that unique representation has to be learned. That's not really doing path integration; it's more like memorizing every location and recognizing when you return.

If you move in some direction and pick a random thing here, without feedback from above, you can't predict the next proper location. I don't think it would work. Now that I see it again, your proposal is probably better, but we would also need object information to know which to anchor. Once you've anchored, path integration works. Once I've picked which two of the six are active in each layer, path integration works. The two active cells in each layer move around, and I can accurately predict the next location, even if I've never been there. I would always get to the same SDR. The two active cells in each layer stay within that field and move as you move. The active cell changes, and it could move completely within the six modules—these are artificial boundaries.

You'd basically just pick two cells. It would essentially mean you don't always need the top-down feedback; you never need it. It just needs the anchor. You just need the anchor. In the very beginning, it would still be less sparse, more like a union of possible states. I don't know what the very beginning would be, but you need the anchor. If you did it before you anchor, I don't know what happens, but you need the anchor. That's nice, right? You need an anchor. Nice shape there. I'm trying to draw an angle. I need to take a 3D drawing course at some point and practice drawing in 3D. Last year it was drawing forks, this year it's drawing cubes.

I wanted to mention, this reminded me a lot of things you've talked about before, where you have the grid cells and then the columns, the minicolumns are the movement vector cells. Maybe, but that doesn't fit into this scheme, I don't think. I was just wondering if we need these to potentially enable the general 2D grid cells. The top layer is like the general 2D grid cells. There are the general 2D grid cells. These are 1D vector cells, but I guess these could have two properties. One is there's a lot of them, so they could be sparsely active. Then they could do path integration at least over local coordinates, potentially over small distances. Why wouldn't they just work like any other grid cells? What we were talking about is they only kind of work in 1D. I'm giving up on that one right now.

By the way, that thing I talked about, many columns of cells that are all in different phases—other people have argued that actually there are different dendrites in different phases, so you don't have to have different cells in different phases.

It's confusing. In today's conversation, I've abandoned that idea, just put it off to the side. I'm not trying to accommodate that here. I was just thinking, I don't know if this helps, but somehow the active grid cells here help re-anchor these to a specific representation that was learned, and then we do path integration over short distances here. No, but then this is a different scenario. That path integration was my phase transition. That's not what we're talking about here. When I move this, this cell would stop becoming active, and this cell would become active, and then this cell becomes active, just like up here. So movements move. In this proposal, movements are just like grid cells. They move horizontally in their plane, or vertically. The phase is vertical. We're not doing phase anymore, remember? But I'm trying to give an example: all the cells are active at the same time, and then the phase shifts up and down. I'm trying to go back to that. Maybe we can incorporate the two. That would be really nice.

I was just wondering if that helps in terms of this being a more sparse code, so it potentially has what we need to sparsely reactivate or uniquely encode a location. It can at least do some path integration, but maybe it's more limited over local spaces. Maybe this is what helps us get into the right space. If this code is going to be meaningful when you're at a particular location or object, you need to reinstate the particular phases that are active. If I think about cells having phase as a derivative of the grid cells, then I would need to anchor the phase. The anchor would be anchoring the phase, which then reflects itself in which grid cells are effective. Remember, those phase cells are not grid cells; they precede the grid cells.

Just bear that in mind. To clarify, in the drawing in the top part, there would be a cell active in each of the six. That's what they observe. That's what Tank sees. That's what grid cell researchers see, right? They see a single cell most active, and then in the direction of movement, a little bit less active, then a little bit further. It's like a phase procession: this fires, then this fires, then this fires, and so on. You have a series of cells here, and on each phase they go bing, bing, and the center would be the middle cell. That's why the grid cell location is fuzzy, because it's complicated.

I'm not suggesting problems, but if this brings higher-dimensional path integration—higher dimension, because it's more than one dimension. We didn't agree if it was two or three. You're conflating the idea that grid cells represent 3D. I'm just trying to be open to that possibility, and then this is like local path integration, plus sparse location code moving it.

For example, at a particular location, these ones are active, and then you move some distance and another sparse code is active, but you're moving up. That wouldn't be the case for grid cells; it would be the next minicolumn over that becomes active.

That's a good point. Remember the actives. If it's the same exact structure here, like the double bouquet cells, but if it's not like the double bouquet cells, then they might be different. This one's in that plane and this one's in this plane. There are a whole series of questions about how grid cells get derived. It's a very complex topic, so I'd like to compartmentalize our issues compared to those. My comparison is that we have a dense SDR and a sparse SDR, and they both do path integration, not unlike the random thing. Even in the sparse one, this is a variation of what we had in our paper—it's a variation that might be compatible with the observations because we don't see lots of these grid cell modules down here anchoring differently, but this thing would make it work in a way that could fit the biology. It's the only way I can imagine how it fits the biology, but at least it gives me a way to say I have a unique SDR and a less unique SDR, a less unique representation location. I'm happy with that. I don't need to worry at the moment too much about exactly how it comes about, because I have a possibility.

The only concern is that this needs all the hardware to actually enable the grid cells. The idea is that this is the hardware for the grid cells, but it can be reused. It does more than just enable grid cells; it also gives you a substrate for location. As you go down to the cord, you'd see a bunch of cells that all affect the same grid cell but at different phases. I'm not sure that's observed. Would they? Because this is the grid cell, and these are vector cells, so they're different types of cells. It's not like this has the same receptive field as these. The problem with those phase cells is that someone else has to read them out.

Imagine these are all active at different phases. They could be read out up here because maybe all these activities are coming up here, and these guys detect the set that are in phase—the specific location. That could work. The problem is that you would expect to see cells that are going in a particular direction; they're motion-sensitive cells, going upwards or down. Not actually going up and down, but meaning every cell in the minicolumn would be moving in some direction—different phases—and every cell in the next minicolumn, so the active cell would move up or down, but the direction that the cell would be different, not active. The in-phase cell would be moving up. They're all active.

When we talk about phase, sometimes we're talking about a frequency band, so you could be the phase within that, or it's phase relative to what? The idea is that you have a set of cells, all firing at the same frequency. The frequency is based on the velocity of movement in a particular direction. The firing rate represents the movement in that direction, but if you look at them, they'll have different phases over the beta cycle. They all look like they're firing together, but their spikes are offset, 360 degrees. Can you also just have a sparse code where one cell is active, like a 1D grid cell? To go back to the grid cell theory, as you move up through this, if you're moving in this direction, then this cell would be active, then this cell, and then this cell, but not the mechanism for how to do that. The point is that a grid cell has to look at all these all the time and pick out which one is in phase with the beta or gamma rate.

If we can't do this, then it's a problem because this isn't a sparse code, and we need some sort of temporal code. The interesting thing is if all these cells are active and you're taking all those cells and feeding them up here, in essence, it is a sparse code because it's very sparse. One of these cells is only going to respond to everyone who's in phase, everyone in the same part of the phase cycle, and therefore it looks like a sparse code, even if we have a temporal code. That was the speculation—that you need something like this to get it to work. There are a lot of questions running through my head right now.

Classically, going back to Hubel and Wiesel, they characterized the cells in the lower minicolumns as being directionally sensitive. Some papers show that directional sensitivity is different than the directional sensitivity up here. That's a movement thing. These kinds of cells, or all these cells firing in the same direction, are what Albu observed. I don't know if they ever looked at the phase; they probably didn't.

but they did observe that all the cells under the minicolumn down here—the ones they report on—seem to be the ones that fire constantly. There are many other cells that are not firing constantly; it's only maybe less than half the cells that have these properties, but that's what they report. So, there is a set of cells that look like this. They all appear velocity sensitive to a particular direction. They didn't report on a phase shift, but if there was a phase shift, that would fit into the grid cell theories of how grid cells operate. 

When I look at this projection up here, if I just look at the population activity, it doesn't look like a grid cell at all. It looks like a bunch of cells like this. But if I were to recognize which ones are in phase at any point in time, it would be very sparse. In some sense, these are almost like reading out location. Imagine I'm one of these cells here and I have a dendrite, and coming in on that dendrite are all these different axons from nearby. At any point, I fire and I want to learn to recognize the pattern. I would learn the ones that are actually firing in phase, and the other ones would look like they're out of phase. So, in some sense, this dendrite of this layer four cell is acting like a grid cell. It's saying, "I recognize a specific location."

This is a specific location because these cells are all peaking at the same time, and that's the only one I'm going to pay attention to. Yeah, a place cell. Basically, it responds to a specific location. Although these cells themselves don't look like they're responding to any specific location. I guess the only thing is that this layer is still important because these can't do path integration over larger spaces very well, since each one is only integrating in one dimension. If they were constantly active, they would do path integration because the active cell is the one that's in phase with the beta cycle.

Which one is in phase with the beta cycle tells you where you are along some dimension in space. But it's only local, because if you start going in a different direction, or even if you do something simple like not going in a square, you're not necessarily going to reactivate the same ones. So, this kind of re-anchors these periodically; these have to be path integration.

Either these re-anchor them, or the features you're observing do. This is a very interesting idea.

I don't know how much time we should spend on it today, but it's a really interesting idea to take the concept of minicolumns representing this.

There are a whole series of problems here. These cells only make sense as you're moving in this direction. When you start moving in another direction, these don't represent that—even the opposite. If I go in the opposite direction, the phase doesn't shift down here; there's another set of cells that represent this direction. You have to anchor these cells phase-wise and then start tracking the distance. In some sense, this minicolumn of cells represents where you are. The phase represents where you are along some distance in a particular direction, as long as you're moving that way. But if you're moving the other way, they don't represent it; another group represents where you are. It's like a one-dimensional grid cell, but only in one direction. In the other direction, you need a different one-dimensional grid cell. Maybe these are really interesting topics. I'm interested in pursuing this idea.

I'm just wondering if we've dealt with the basic issues here well enough. This is a detail of how you would implement this. It's a really interesting idea that you would not see these as grid cells; you would see these as just direction-oriented cells, and those all get passed up to here, and this group recognizes them as unique location codes. I like that idea a lot, that the actual detection of the in-phase cells might occur up here. It doesn't explain why I have grid cells down here, because the grid cells have to do it too, right? If there are real grid cells here, then these don't look like grid cells. These look almost like place cells, actually, because this cell will become active at a specific place with a specific feature.

these would look like directionally sensitive cells. I still have to be able to generate—why do we have grid cells then? If this is the predecessor to grid cells, these are not grid cells; these are required predecessor grid cells. We have to generate grid cells. What's their purpose? I feel like their purpose would be as a recurrent connection. I agree; these are a way, like a temporary memory. This is where we are right now, so you don't forget where you are. It's some sort of holding place. It's not very high resolution. That's the interesting thing about it. It's not very high resolution.

I don't know if you have predictions from L4 going down to the minicolumns in L6. If I get that connection, our theory is that it would bias that, so you would have to—now what would happen here? You have a cell that's active for a specific location with a specific feature, or a set of cells that represent that. Now you're projecting that down here, so this is going to be a core signal telling it, or updating, predicting which one should be active. There's going to be a lot that are predicted. This is sparse population activity up here, and this sparse population activity is sufficient to specify exactly which cells down here should be active, or which cells should be active in the right phase. It would be like, "Okay, this pattern says you have to be in the peak of your phase right now," something like that.

Is it a bit easier to simultaneously encode multiple hypotheses with this? I don't see how you could, because you could have different phases, but you can't have two cells—either a cell is in phase with the beta or it's not. I don't see how you could have multiple cells that are. So you're saying it's at the peak of the beta? The idea is that they peak at the same time as the base frequency, and the other ones don't. Anyone looking at both of them—I suppose you could try to have multiple cells here at the same base frequency, peaking at the same time, but it seems like the mechanism would force these guys to be phase distributed. There's going to be a mechanism that forces them to be phase distributed. These double bouquet cells could do that; they send an axon down here, and all you need is some sort of delay. Literally, the physical delay going down could do it. It's interesting to know what the axonal propagation properties are. Maybe they're slow enough to do that. I don't know how you could do multiple hypotheses here.

If you had multiple—well, something would happen. What if I had multiple SDRs here? Say I had two active SDRs here. I have some non-union up here, and I first bring that union down here. It's going to be conflicting data. Some of these cells will be like, "Hey, I should be in sync with beta," and this guy says, "I should be in sync with beta." Who's going to win?

If these two SDRs up here were in different locations, meaning on the morphological object or different locations on the morphological object, then you'd have different minicolumns. One SDR would be trying to set one set of minicolumns in phase, and the other would be trying to set the other set of minicolumns in phase. Then you would have two hypotheses—two sets of minicolumns: one set to represent the phase of one SDR, and the other set to represent the phase of the other. If they weren't overlapping, if they didn't have the exact same location on the morphological object, then you could have a union down there. If they're on the same location on the morphological object, then you'd have to pick one or the other.

You mean the union would be activated by the layer four representation? Or imagine I just have narrowed down to two SDRs because I thought the screen connection is an inhibitory connection and the layer four representation just narrows down the cells. Is that what you're saying? You thought that because that's what the neuroscience says. Is that true? I thought that was also in Noman's papers before, that the layer four-six connections—if I remember it right. I wouldn't think that. Let me see. That may be true. Remember, going back to the Thompson paper, she showed strong reciprocal inhibitory connections between layer four-six and layer three-five. Does that mean it's only inhibitory through large basket cells and double bouquet cells? I don't think that could be true. At the same time, they might be excited too, because in the Thompson paper, I remember she showed the axon rising up here and forming lots of synapses up here. And you're saying those are inhibitory synapses? So here, that's not the figure.

At least in the mento code, it's excitatory. The main inhibition is implied. So you have these axons coming up here, and from all these synapses up here—big screen. I was confused at first why the layers underneath, the dense grid cells, also follow the same grid cells. Do you understand now? I'm leaning more towards the other one on the left there, because I was also thinking it's more like community college, the way we have layer four and layer three. The important thing is that path integration has to occur automatically, even for sparse representation of space. It just has to; that's how you have to do it. If I'm in some unique location on an object and then I move, I have to always go to the right, the same one. You can't learn it; that's the whole point of path integration.

It is interesting, the idea of combining the phase precession thing on the left there as an ultimate way of doing what I'm suggesting here, which I thought was kind of wonky, but it would work. This is a little bit better, because it was still also in code, the way that bit cells move from one minicolumn to another on top of the dense. This guy here would have to say what minicolumn would go to, and then it would encode the path integration in which cell gets activated, although I don't know how it deterministically sets. What if I move this minicolumn to this minicolumn because I'm moving in a direction?

No, I'm moving in some direction, and this guy has to move. There's no longer a correlation between these cells and these minicolumns. That's no longer the case. These are not related to that. Actually, at this point, they're totally separate. They're not related, and this one moves from here to here. That minicolumn doesn't move from there to there. No, that's not the way it works. It looks like that the way you drew it, but would it really be like this? You just have a set of minicolumns here, and then you have a layer of cells up here.

These are your grid cells. These are the grid cells in here, but these grid cells do not correspond to these minicolumns. They have no correlation at this point in time, because when you move—if I'm moving in one direction, these minicolumns aren't changing. It's just that which cells in phase are changing, whereas these cells do change. These are moving around on the grid. These are just the in-phase cells moving up and down, so they're not driving this the same way. They're driving this, but not in the column basis.

I'm just helping Mohamed do something, so I'll be here a couple minutes.

I know it is a bit confusing, but think about it this way: I'm moving in some direction, the active grid cells change, but all it means is if I'm moving in that direction, there's a cell in this column. The minicolumn represents moving in that direction. The minicolumn is active throughout the entire movement in that direction, and all I'm doing is moving the in-phase cell up and down.

Because it's a velocity-controlled oscillator, it's firing a little bit faster, so all the cells are going a little bit faster, and therefore, which ones are in phase with the beta cycle changes. When you move in a particular direction, only several minicolumns are active, but as you move, which minicolumn is active doesn't change. It just says, "Okay, I'm moving in this direction; therefore, which of these cells become in phase with the beta?" That changes. We assumed that the phases are on the minicolumns—that was the proposal. That's what Niels proposed. I made this proposal; he's trying to bring it back in. If we don't assume that, then they just don't do anything. Then we're back to this idea where the minicolumns represent locations, and grid cells may, and so which minicolumn becomes active would—that's this idea. Then this is the way I understand.

I'm still a little hazy on the phases. If this idea is you have a set of grid cells here, and then these minicolumns are not a predecessor of this, but the minicolumns are not tied together. This is a different set of cells; it doesn't tie to minicolumns.

Each of these grid cells is detecting in-phase patterns down here, but it is independent of minicolumns. I was hoping it was as simple as we know—a dense layer, and we're just trying to incorporate context into them. That's what this idea was, right? I was hoping it was. Talking to Ram here, it is confusing, and this idea we're pursuing, which I really like a lot: if you have a layer of grid cells, those grid cells are not—they don't have any correspondence with the minicolumns beneath them. A grid cell that's active doesn't mean the minicolumn below it's active. That was confusing, so it's okay. Over here, that's what I was suggesting. This is where each cell up is like a grid cell, but it's a sparse SDR. Each minicolumn is active during a movement in a particular direction. As you move in a particular direction, the active minicolumns don't change. All that changes is that since the velocity of the cells—the frequency at which they're firing—increases with speed, then which cell is in phase with the beta cycle changes. That's all.

It feels like this would actually work, if it does help re-anchor the vector cells or movement vector cells periodically. That would at least help, because that always seemed like the main limitation of that approach: you couldn't path integrate over slightly larger distances, or I'm confused by that. I don't know if that's out of the below. I thought that's what we were talking about just before.

Maybe you can ignore it, but what we're trying to do is go back to the big picture. The big picture is we have to have a sparse SDR, and we also have a sparse SDR.

that represents location, and we propose one mechanism for doing that. That mechanism doesn't really have capacity for the less sparse one; it doesn't have enough capacity. We hadn't resolved the capacity of the morphological model, but now we're talking about the details of how you would get the more sparse model. This is a way of getting the more sparse model that might fit better with the biology. Take the grid cells out of the picture for now. If you assume that at any point in time you can anchor which cell is in phase with the beta cycle, then as you move, you read that out as a sparse SDR—meaning it's the cells that are in phase right now with the beta cycle or are sparse.

So it solves that problem. It doesn't tell me at all how I get the less sparse SDR for the morphology.

I don't have an answer to that. You're still concerned that one layer of grid cells isn't right. Although it's less sparse, it's not sparse enough. I guess that's one layer of grid cells; it's not very sparse at all. It's just like one out of thirty. If you have a few morphological objects, then it's less of an issue.

I'm debating whether to bring up another topic related to this. Did you want to look at that Thompson paper about the activity? I think we brought it up. You weren't going to argue that it was negative; it was inhibitory connectivity. Did she say that?

I'm not sure if that can—oh, I think that's from the 2010 paper. Let me search for that one.

Oh, that is okay. That's it.

There's a general rule about cortical connectivity that's important here. I just saw inhibitory connections. Here it says, in contrast to the relatively weak excitatory input from layer four to six, there appears to be relatively strong inhibitory input. So, the layer four input is on distal dendrites or something like that. If you look at the soma and see how much depolarization it has, it's very little. So they say it's weak, but they're not taking into account the integration and dendritic spikes. You follow what I'm saying? They'll say, "I measured the response, it's weak, it doesn't do anything," but that's what it looks like until you get a dendritic spike, and then it's not weak.

Are they mostly based on measurements or on anatomy? I also looked at the anatomy, like connectivity. Why would she say "relatively strong"? Relativity is—here's a general rule of cortical connectivity: you have an excitatory cell projecting to some other place. There are a bunch of cells here, a bunch of cells there, and it's going to make some connections. The general rule is it will almost always, or very often, project to an inhibitory cell and to excitatory cells at the same time. The inhibitory cell synapses tend to be right on the soma or near the soma. That's why you might say they're strong. You're not recognizing a pattern; you're activating inhibition, global inhibition. These inhibitory cells respond very quickly to almost any input. It's not about detecting a pattern; it's about having an immediate inhibitory response, whereas the excitatory inputs will be on distal dendrites and look like a weak response.

That might match with this sentence here: "Layer four double bouquet cells with an adaptive firing pattern, bundle of fine descending unmyelinated axons and more distal dendritic targets. Also in layer six, again, double bouquet cells with an adapting firing pattern, a bundle of fine descending unmyelinated axons and more..." Is that a sentence? How do I parse that sentence? It feels like there should be a comma somewhere. I'm not parsing that sentence. I think it's saying the double bouquet cells in layer four go toward the distal dendrites in layer six. Let me read it again: "Layer four double bouquet cells..." What's the verb? I think it's also "innervate," right? The layer four cells, fine unmyelinated axons, and more distal—also innervate layer six. It's just describing them. That is a weird sentence.

That sentence is saying these cells go to the distal dendrites, down to layer six. I'm not sure what cells in layer six they're talking about, but they're saying, "I see an axon from those cells coming down to layer six." That's the exact data I was saying you would want to see.

Also, for what it's worth, you're talking about the delay in case they're unmyelinated and also fine. The narrower they are, the slower they are.

That might be a delay. I don't know what he says also, because they must activate these cells. It says they also activate these cells. We want the slowness here because we want it here. The only odd thing is that this is under inhibitory inputs to layer six, so I'm not sure if these are inhibitory. These double bouquet cells—these inhibitory connections are reciprocal: layer five interneurons project to layer three, while layer six interneurons project to layer four. Another weird thing about these: I think the double bouquet cells sometimes act as excitatory cells and sometimes as inhibitory cells. They have this duality. I don't know how they do this, but there's some explanation for it.

It just stuck in my head that they do that somehow.

I still want to get back to the issue of capacity.

Let's say these grid cells are not actually the ones that project; they aren't the code we're going to use for our morphology model. They just don't seem to have the information for the morphology model. They're too limited. What I need is something that's less dense than the dense one, less sparse than the sparse one.

I just need a little bit more capacity. How do I phrase that right?

Maybe these grids are acting as a buffer to keep track of which cells should be activating at a given point in time.

I need something less dense than the dense one, less sparse than the sparse one—a less sparse and a sparse version with separate connectivity between the two. I was hoping it would be these double bouquet cells, but they can't just be grid cells. Classic grid cells don't have enough capacity. Do we have to have the classic grid cells there? I just assume the grid cells exist because we didn't have anything to explain how the cortex could represent location in a reference frame, and grid cells seemed to fit the bill, or at least the grid cell mechanism as a whole. So we just assumed there'd be grid cells, and people see grid cells, so they're there. It seems to be, but they see them only in the fMRI study with abstract concepts. There was a Chinese lab that saw them in V1 and S1, and then other people dismissed it. I can't believe they're there.

The Chinese lab sent me their papers and said, "Look, we found them in V1 and S1." When I talked to someone—was it Tim Barons?—he dismissed it. I think he was arguing that they weren't in egocentric or object-centric coordinates or something, but I don't know if they actually tested that. A few people I've talked to about it said it can't be right, it doesn't make sense, or whatever. I say, don't dismiss it—try to work it out. There were at least some concerns about the statistical measures they used, but I believe they're going to be there, everywhere. If we can figure out a different mechanism, like maybe just these 1D movement vectors, then the data for grid cells in the brain might be explained in different ways, like just moving through conceptual space that they measured there.

I don't know why you want to bring in conceptual space; that just confuses me. I just mean if we find a solution that works without the classic grid cells, it's not like this experimental data is a problem. I do need a solution that has path integration. I have two different hypotheses: we have to have two different populations of location cells.

One would be for morphology and one for feature-based models. Going back to that assumption, I'm thinking more about how much of an issue it is if—yeah, to your point, Vivi—okay, we have the grid cells, but let's assume those are just for stabilizing and re-anchoring the vector cells. If we just use those vector cells for both, and down in the lower layers, like the phase that's ultimately the location code that reactivates both double bouquet cells for morphological features and L4 cells for features. In general, we might have more morphological features, which will be dense, whereas features will be sparse, but it's ultimately the same substrate. Here's one way it could work.

I'm just thinking if that's actually an issue—what's an issue? Just reusing, thinking in terms of Monty. We have a model, and at every location there's a morphological feature, but at some locations there's a non-morphological feature.

It would be a separate thing whether we fuse them. But wouldn't you need—we're assuming there's a unique location for unique objects, and then there has to be a non-unique location for the morphology or less unique. When I'm on the coffee cup, I have a coffee cup with a logo and one without a logo—two identical coffee cups. I want to be able to use the standard representations. There should be a common spatial representation for both, but a unique one that's only useful for the one with the logo. So I want to have two of these. I don't want all of the cylinders to have this; all the coffee cups have the same—is the ID feedback enough for that, like from L3/L2? I don't think so. I think I need two.

I need to have one be a sparse version of the other.

In terms of how we might implement it in Monty, you have the same graph, but when you have a strong hypothesis that you're on a particular object, there will be unique locations and unique features that you will predict at a location that you wouldn't have predicted otherwise. Then you have to have some way of knowing you're on locations on a cylinder, but also knowing you have a unique location on the cylinder with the logo. I have to have both of those, and it has to be unique, because another cylinder with a different logo or a different site feature needs to have a different sparse SDR for location. In Monty, it would be like having one morphological model of a cup, but then having multiple possible feature maps for that cup. For each feature map, we still need to have a mapping of where the feature would be on the morphological model. The locations have to track together, right?

So when we know we are on a coffee mug, on the body of the mug, we have multiple possible feature maps that could be there—one logo, a different logo, whatever. In the feature map space, we need a mapping of which locations on the coffee cup morphology correspond to which locations in the feature map space.

Although the beauty of doing this with SDRs is that I can just create a sparse version of the first one and it works. In the multi implementation, it would also be straightforward to make the mapping work. I'm just thinking in terms of how it might work at a neural level. Rami, you're good at reminding us about top-down feedback. I'm considering how, once you have an ID here—whatever the IDs are—you have a unique location for the general morphological reference frame, like a coffee cup, and that has to be shared across all coffee cups.

Let's say you're at the location associated with the logo on one of the coffee cups. That's going to send a positive signal, but only with a sufficient prediction. Maybe it gets sparsified based on a particular object. If you know you're on that object, that's enough. Assuming you get the input, let's say the Numenta logo has two logos. You see the logo on one side, go around to the other side, and you're still on this object. Ultimately, it doesn't make sense to predict you're on the Numenta logo unless you know you're on the Numenta logo. Even if it's on one side, it can predict that when you turn it, you'll see it. Until it's the Numenta logo, we don't want these details. I've seen it once and rotated, and I still know I'm on the Numenta logo. It is persistent.

It's the Numenta logo, the top of the logo. You go to the location where the logo is expected, and you get the prediction of that feature, which is biased by the ID. If you're on a different cup without the logo, you have the same location active because that location is associated with the Numenta logo in one object's instance. That sends some bias up to layer four, but in the absence of the bias from the ID representation, it's not enough. I think what you're trying to say is we're going to have a single reference frame, and the prediction will be unique based on the object ID. I don't like that. I don't think it's going to work. Maybe I'll have to try to convince you that it's not going to work. It doesn't seem like it's going to work. I thought that's what AP Denverites were all about. That doesn't make sense.

How would we know this is a unique location on that object without having the object ID? I think the proposal is you'd have a totally different location representation for the location on the Numenta mug versus another location. How would it be totally different? I need to continue to do path integration. The sparse code, the additional part—isn't that what you were suggesting? Both would be a subset of the dense code, so they would be similar in some regards. Are you proposing a subset of dense code, or are you proposing that the ID tells you what to do? I'm not talking about the ID. I think Ramy was asking what the alternative is, and I was trying to paraphrase what we had been talking about before: the dense grid cell code and the sparse grid cell code. The sparse grid cell code would have a unique location only for the Numenta logo. When you're not there, what's the location representation? How do we go from a dense to a sparse one? Where's the sparse one and where's the dense one? I thought that was the two populations we were talking about before in layer six. Before, I had it as the grid cells. Grid cells aren't sparse enough; they're too dense. They're not going to work for us.

I was just trying to paraphrase the previous argument, not saying it was right. I was trying to say to Ramy, since we were talking about how to capture this, that I was just talking about the ID. Ramy was saying, if we're not using the ID, what are we using? One thing we discussed before was the two populations of grid cells. Then we're back to where we were before. So we don't have an ID, we have a union, and the sparse grid cells—until we know which object it is. If there were two cups that both had logos in the same position and we don't know which mug it is yet, I think you would have a union of the locations associated, integrating on multiple.

If we're going for the idea where the minicolumns are just phase distributions, then we have a solution that works for the sparse code. I can code that in layer four and recognize that pattern. I'd have to recognize different patterns going in different directions. If I approach a point this way, it's a different pattern code than if I approach it another way, but it could work.

For morphology, I need a less sparse code that follows the same rules.

Because the first thing I said is we have to have two models that share path integration. That's the bottom line. We need one model for morphology and one for specific features, both following path integration. The morphology model is accurate for different unique objects. Did I miss something? What's this proposal? I thought there are still grid cells, but we're not using them. The idea is that grid cells serve as a maintenance function for other cells; they're not actually encoding anything. They don't have enough information to code anything, so we couldn't use those two steps. If we tried, we'd have a capacity issue. The capacity issue is that, if you take the tank paper, you see there's a limit to how many different locations you can encode in a grid cell module—maybe 150 or so. It doesn't work.

I'm thinking about whether top-down feedback, the ID thing, would help in terms of path integrating over multiple objects. That was another issue we discussed yesterday: how do you test multiple hypotheses at once? If we assume there's just one, not separated into morphology and feature, in terms of the location representation—like for mugs—we just have a common one. Then, as we move through that, we're testing all hypotheses because the location isn't unique. What's unique is the activation in L4 when you get the ID bias. The ID has a problem too, because we need two separate models. I need to have an ID for the morphology model and an ID for the feature model. I can say that's a circle, and I need to be able to identify that and pass it on to someone else.

But that feels like maybe a separate issue. I feel like I need to have two.

I have the same issue here: I need an ID for the common object and an ID for the unique object. A convenient way of doing that would be to sparsify the ID for the common object.

Should we maybe just collect the main issues right now and then take a little break? Sure, just add to the list. I don't think we're too far from having an answer here. I think it's pretty close. There are some really good ideas here.

The best summary I can give is what I just said: we have to have two models that track together from path integration. The surest way of doing that is to have a sparse version of the morphology model, which has a set of representations for features at locations. You want to sparsify the locations and the features. That would be the simplest way of doing the two-model approach, and we also get the capacity of the morphology model. We have to have more of the feature models than the capacity models, because many feature models share the same morphology. Therefore, we need higher capacity in the feature models than in the morphology model. But they also might be more sparse, encoding less information—each feature model is probably just a few points of information, whereas the morphological model needs to be densely encoded. I guess that's true for morphology, but even the feature models might need to be dense. For example, you want to have colors at all locations, but if there's a crazy pattern, you probably wouldn't memorize that. Or if a cup was covered in hundreds of logos, you'd probably just assign some sort of generic label; you wouldn't memorize each one. It's an interesting question we haven't really addressed. If the whole cup is red, I don't have to learn red at every location—I just know it's red.

Maybe you do, because different columns are looking at different points and they're all learning it's red. I think the new idea today is revisiting the idea on the bottom left here as a way of getting sparse coding of locations. The only reason I like it is because that's what is observed in cortex. You've got minicolumns of orientation movement preference in lower layers. We know cells that look like that exist. No one's reported that they're phase-shifted, but that would be a prediction of the model—that the cells in the minicolumns are phase-shifted. What we don't have is a denser model.

We don't have a good working model of the denser model yet.

I'd like to think more about ways we could reuse the same substrate. There might be issues with it, but at least that's one solution. If we can find a way to reuse the same population for the reference frame, then we have the population problem from a neurobiological perspective. But you do want separate connections. You want to be able to make a unique prediction. They still have to be separated. They could be the same substrate, but the models have to be somewhat separated.

Can I throw out one last thing before we take a break? This may be a total red herring, but in vision, in a V1 column, remember this has to do with the slabs. In V1, if you move your probe in one direction across the column, you see the minicolumns change in orientation, cycling through again and again. If you go in the other direction, the minicolumns all have the same orientation.

So why?

Why is it that all these minicolumns seem to have the same orientation? They don't seem to be different from each other, whereas if you go in the other direction, they're all different. If you look on the surface and say, okay, the minicolumns—all the cells in the minicolumn here—stick to the penetration like this, all these cells respond to the same orientation. Then you go to the next minicolumn, and all the cells respond to the same orientation. As you move in one direction across the cortex, the orientation changes continuously. In the other direction, the orientation doesn't change at all. I thought they were like those pictures with the pinwheel color, right? This is a well-understood phenomenon, but then you overlay the pinwheel on top of it. If you look at the Hubel and Wiesel ice cube model, you can just go to Google Images and see it.

Maybe that one—yeah, right there. In these pictures, if you go along this section, the orientation is changing, and then it's the ocular dominance—left eye, right eye—but within this little rectangular block, there are multiple minicolumns, all with the same orientation. You might even see it better here.

And then you still have the blobs.

This one up here.

A very small picture—maybe it doesn't show up there. Anyway, you can look it up. I'm pretty sure this is the way it looks: you have a slab of minicolumns, all with the same orientation, and the next slab over has a different orientation.

I've always wondered about this. What's going on there? There's some redundancy, but I don't think it's just redundancy. It's some part of a representational scheme. I don't know if this applies anywhere else. This might have to do purely with three-dimensional image viewing, like getting perspectives. There are a lot of things it could have to do with, or maybe nothing relevant to our problem. In this picture, it looks like it might be spatial frequency that's encoded along that direction. Is that what it is? I don't know. Maybe they just suggest that. I didn't know that—when was this modified ice cube model of cat V1 column? I can share that paper. What does that mean? They show that direction and the ocular dominance columns, but then within one ocular dominance column, they seem to assign spatial frequency, like sinusoidal gratings. Why would you do that? Coarse ones versus fine grain—why would you want that? It could be a total red herring, maybe something specific to vision. Vision has 3D perception of depth, viewing at a distance, calculating the distance of things. There's a lot going on in vision that doesn't occur elsewhere, certainly not in other sensory areas. Maybe we shouldn't pay attention to that.

The reason I thought of it is that I could use this other dimension to create a different way of generating unique morphological models. It gives me two variables: which cell is active in the minicolumn, and also which of the minicolumns—something about this dimension. I was just trying to think of other ways of increasing or changing the capacity of the representation. Unless it's probably a red herring and I shouldn't have brought it up.

Okay, we're going to take a break.