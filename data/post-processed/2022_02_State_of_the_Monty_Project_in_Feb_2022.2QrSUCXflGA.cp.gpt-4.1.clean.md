I've been working on a bunch of problems and would love to present the solutions, but I'm not ready to do that. Instead of just saying I'm not ready, I thought I'd walk you through what I'm thinking and working on. Maybe that will be helpful, and we might get some questions.

This is just a list of topics I'm trying to address. I've made this list—mostly in my head, but I wrote it down this morning. I'll show you these lists in a second, but they may not be complete. Over the years, we've made a list of all the different things the models in the brain have to do, and it's quite extensive and difficult.

We're trying to figure out how to do things like melodies or other tasks. Right now, I'm trying to resolve all of them. I have a long list, and I'll show you what that looks like in a second. I'm trying to be more ambitious and see if we can tackle all these at once. I've separated out hierarchical models because, up to now, we've focused on what a single column can do and pushed off things that require multiple levels of the hierarchy. That means the models themselves are arranged in hierarchical sets, and I'm trying to deal with that now, which we've avoided in the past. Some of the things we're trying to solve require that.

I'm trying to do this in two modalities at once—touch and vision. When you think about touch and vision, there are important differences. I've become aware of those and adjusted my thinking. Another dimension we've always tried to understand is a single column versus multiple columns, like a single column versus columns voting. I'm now beginning to think that even a single column is insufficient; you might need several local columns next to each other, at minimum. You can think of a spectrum of how many columns or models are involved. We used to think it's one to many, but now I think it might be very few to many. That's a new perspective for me.

By the way, Lewis, I think there's a problem with those sensory presentations from yesterday. I didn't think about it last night. When we touch things, we almost always rely on multiple different pads of sensors on the skin that are not the same. For example, when you slide your finger on something, the edge of your finger bumps into things, and you might need multiple columns to detect curvature and similar features. I had a question for you about that.

Another thing I've done is look at the toolbox of neural mechanisms. We've defined two major neural mechanisms for inference and model building: the temporal memory algorithm and the temporal pooling algorithm. These are both cellular layers and are useful for narrowing down and inferring. I've been trying to understand them at a very generic level. The temporal memory algorithm can be applied to different types of data. Another way of thinking about the modeling problem is to ask what mechanisms could allow these models to exist. I've been thinking about those generically, trying to understand them better.

The thing I'm not doing, which we still need to do eventually, is action policies, goals, and behavior as part of the models. I haven't been working on that; I've just been thinking that's too much for now.

I've made a list of all these things and I have all these ideas I'm trying to resolve at once. I'm trying to figure out the structure of models in the cortex that solve all these problems and meet all these different constraints. We have pieces of the answer and pieces that are still mysterious to us. I'll walk through some of these items in more detail, and then on the third page I'll describe my current thinking about solutions.

Let's talk about the list of model capabilities, starting with a single level—meaning a single column or a single level, maybe a couple of columns next to each other. These are some of the things we've discussed. We know we have to do inference and novel poses; that's a problem we have to solve. There are 2D objects and 3D objects. For example, a circle is 2D, and a coffee cup is 3D. This presents an interesting dilemma: how does the system deal with both?

Lately, I've been thinking about objects defined by morphology—objects with continuous surfaces but no salient points. For example, a circle or a cylinder. I've talked about how we can recognize and learn them. It forces you to realize you can't just say there are features to figure out by displacement. Then there are objects defined by features at some relative displacement. For example, a word is seven letters in a certain arrangement; that's it. There's no morphology to the word, just the arrangement. If you change the order, you get a different word. Another example is a bicycle, which is composed of wheels, a frame, pedals, and so on.

I've been asking myself if these can really be the same. I'd like them to be two points on a spectrum, and that's what I'm working on now. Can the same mechanism do both? When I talk about solutions, that's my current approach. I'd rather not say there are two types of objects. I've said that before, but I'd rather it be a continuous mechanism. I'll get to that when we discuss solutions.

Another thing we have to deal with is changes in scale, and there are multiple ways this can happen. There's absolute scale—imagine a coffee cup and a miniature version of it. I can manipulate the smaller one just like the bigger one with no problem. I'd notice it's the same thing, just smaller. Then there are other changes in scale, like perspective in vision. For example, in a picture of a truck, the rear is smaller and the front is bigger on the retina. That's a different type of scale change, which doesn't occur in touch but does in vision.

There are also changes in scale that aren't absolute. You can take an object and stretch it. For example, imagine a fatter version of the same object. I can easily imagine and learn that. I could take my cup and stretch it in one dimension. That's a strange thing.

We'll come back to that. Scale is not a singular thing; there are various complexities about scale.

There’s a distinction between static objects and changing objects. Changing objects are sometimes referred to as behaviors of objects. A static object never changes, but we do have objects that change in morphology—the actual object itself can change its shape. The stapler is a perfect example of that. There are plenty of examples of things changing their shape and the arrangement of their features. Some changes are in features: the morphology of the object changes but the features stay the same, or the morphology stays the same and the features change, like the icons on your phone’s display. That’s unusual.

There’s a spectrum of these changes. Some are very predictable, like the stapler, which only has a certain set of behaviors and is fixed. We can learn those. Then there are unpredictable ones, like a t-shirt. A t-shirt has a shape—you can imagine what it looks like—but half the time it’s in a pile on your floor, bunched up. We’re still able to manipulate and work with it. It’s the same t-shirt; we don’t get confused and think it’s something different just because it’s a pile on the floor. So there’s a spectrum of morphological changes and changes that can occur on objects.

We are able to infer and represent the object’s state. For example, I see the stapler open or closed, or a different icon on a display. I know that, and somehow there’s a state to that object. If I have a model of an object in one form, how do I impose that state on it? Is the state saying, “No, the state was normally closed, but now it’s open”? There’s some strange stuff going on that we need to figure out.

I’ve also talked about class versus specific instances. That’s another problem we have to deal with. What’s a cup versus my cup? What’s a cat versus a tabby versus my cat? These are all points on a spectrum of classification and inference. We seem to be able to do that pretty well—we can recognize specific things, classes of things, and the class can be broad or narrow. We can have different labels at different levels of the class. That’s another unusual aspect.

In that case, you’re considering that one specific object would belong to multiple classes. Your cat would be a cat and also your cat. So it’s just two different classes an object belongs to. All the words make sense, but I didn’t understand the original question. My question is, are you thinking of different classes? There’s my cat, and there’s—well, I wasn’t thinking of different classes. I don’t know what the right word is. Maybe it’s like an ontology: you have animals, cats, and then my cat. Even “my cat” seems to be on a spectrum. You can break off pieces along the way and say, “It’s got the same set of features, but the arrangement is slightly stretched,” or “It’s the same exact morphology with a different set of features,” or some combination. You can put labels on, so you can say, “That’s a tabby versus a Siamese cat,” and then you can also say, “That’s my tabby cat.” I guess all I’m saying is there’s a continuum—a class of labeling abilities.

I don’t know if my cat is a special, unique thing. It just seems to be a spectrum of similarities, and we can label those. Somehow my brain is able to put different labels on different parts of the spectrum of shapes and features. It’s surprising that you can do that. It might be easier to say, “We’re just going to do one or two classifications,” but it seems to be multiple ones. These are open questions; some of these are not really clear how you specify them, but it seems you are able to do this.

WordNet does that for every English word, for all common English words, like a hierarchy. It doesn’t have “your cat” or “my cat,” but everything about that is based on semantic versions of words. This is based on the physical structure of some object in the world. Maybe they achieve a similar kind of result, but they’re very different—one is semantics, the other is physical. You could look at it statistically and use word usage in the world to make that connection.

No, I think it’s semantics. But the semantics come from word usage—where does it appear? They don’t sit there and code it down. This is heavy.

Fine. That's not what I'm talking about here. Here we're learning a model through sensorimotor inference. We're sensorimotor modeling—touching something, seeing something, building a model. I need to classify that model and do inference, including at novel poses. I'm able to do inference at different levels; I can put a label on different changes in that concept. There are changes in morphology, size, and feature sets, and those seem to be the three things you can do. We're able to pick off pieces and label them.

The only reason I brought this up is that the graph matching algorithm or development should be able to handle those. It's never going to match one thing; each object has to be several things. We can't return a single option—each one has to be several things. Why is that? The cat is going to be an animal, a cat, and maybe even garbage. But couldn't I just say, "This is a coffee cup," and be done with it? Does it have to be something else? There is your coffee cup. Maybe not. Maybe it's both. Maybe, I don't know. I guess I wasn't enforcing that it had to be multiple things. I'm just saying it's capable of doing multiple things.

The problem I get is when I think about how neurons do this. Basically, our classification is the temporal pooler layer, where we form a stable representation for an object. You can classify that pretty easily, but it would be very difficult to classify that temporal pooling cellular pattern in you. You could associate different classes, but it's just complicated in the brain. Maybe it's easy in the method you're using. So again, I'm thinking about this in writing mechanism, so maybe you've got a solution to that. That's fine. But anyway, it's something I think about. It gets back to this whole issue that objects are stretchable and morphologically changeable, and the feature sets can mix and match, yet you still have a model that works for all of them. You can say, yes, that is a feline animal, but it could be a tiger or a cat or something else.

Lucas, was your original point that it's not just one thing belonging to multiple levels of the same hierarchy, but that you can have different group operations? So a cat is my cat, it's a feline, but it's also—what are things that are fuzzy? Jess's jacket, my cat, a stuffed animal. Switchers. I guess that's what I'm asking. I think even horizontal—that gets back to the point we discussed a month ago: there is no one class for anything, and it's not like a car. I agree with that. There's a spectrum of patterns that represent this thing you're observing, and you can assign some subset of those patterns to some label. Exactly. Associations.

It's interesting because if you think about the representational structure you're dealing with in the brain, you have to be able to do that somehow. It may not be hard; it's just a thing that has to be done.

I didn't say these are all unsolved. I just said these are things we need to do. That's a good point. I haven't been thinking about this particular point too much, but I've been in your category: I'll just be able to associate this with different labels, but I have to have flexibility. What qualifies as a tiger is also quite flexible. There's a range that's a tiger, a range that's a cat, a range that's a wild animal. There's a whole bunch of stuff. Not a hard one. I just put.

We haven't really spent a lot of time talking about hierarchical models. At Menta, we've tried to avoid that and focus on a single learning column at a time, but I'm thinking about it more now. It's actually quite interesting and weird. So, what is a hierarchical model? It's when you have some object models that are part of a bigger object model, and you can break them down into different types.

Here's an example: we have a morphological model on top of another morphological model, like a coffee cup with a circle on it. The circle is a morphological model, and the cylinder is a morphological model. There are no features in these things yet—I'm just placing a circle on. I have to be able to accommodate that. Then you can have features on morphology, like the classic example of a logo on a cup. The logo consists of individual features arranged relative to each other, and now I can place that arrangement on the cup.

This is bothersome to me because if I'm thinking about morphology and features as separate things, it gets complicated to handle all these cases. You can also have features on features, like letters. Letters can be considered objects; a letter could be in a word, and the word can be in the logo. These are just relative arrangements of pieces. Other than the letter itself, it's not morphological—it's just displacement of features.

It seems we can go very deep hierarchically in these compositional models. We talked about this in the framework paper; they could be recursive, but you have to do this in a finite number of levels in the hierarchy. For example, a bicycle has a wheel, the wheel has a tire, the tire has a stem, the stem has a screw hole, the screw hole has a valve, and the valve has a little piston. There are many levels to that hierarchical composition, but I can't rely on endless levels in a hierarchy in the brain. That's not how it's going to work. Somehow, the mechanism must accommodate recursion within the same set of neurons and not get lost.

Regarding the features and letters example, consider a letter to be a morph. Letters are morphological features, but now it's an object. So, I have this object called the letter, and the arrangement of the letter is features relative to each other. The arrangement of the word, like "memento" relative to the "i" or the triangle in the logo, is also features to features. It's like letters arranged in a certain way, and then the word is arranged among the components of the logo. We discussed this two weeks ago, thinking along the lines of a pixel color at a specific point.

We have to be careful with the term "features." That may be a different use of the word than I'm using here. What I mean is that a feature is just another object. You could say objects on objects—letters as objects, then a word as an object, so you have letters on objects and then another object, and so on. I think I'm going to move away from the word "features" in the future.

You keep saying your circle has no features. It has no—what's the word I used up there? Continuous surface, no salient points. That's the word I used. But they have boundaries, and those are salient, though not specific. "Salient" means something is more interesting than the rest. In this case, there's nothing more interesting about one part of the circle than another; there's nothing to direct you to one point as the thing to pay attention to. That's a description of the west. I'm just saying there's no salient points. So, nothing you can fasten on, but I didn't say there's no features—I said there's no salient points.

We've talked about the solution to this as being a continuous feature of curvature, and we'll get to that in a second. When I first started thinking about building models using displacement, it was always about this object being at this location and that object at another location. We can calculate the displacement between them, but there are many things that just don't have that. I understand your distinction. If that's how you're trying to represent it, there are no points where you could say, "This is the thing I want to calculate my displacement from." That's what that means—no salient points, just random points, but that's not going to work.

It has to come down to local curvature at any point, and then learning that continuously so you have a continuous representation of the feature space, as opposed to a punctuated representation. That forces you to think about touch and vision. I mentioned earlier there are differences. I've always pretended there aren't, saying it works in both cases, but that's not really true. Vision has scale that changes with perspective, and that doesn't happen with touch.

That's a difference, and there are going to be mechanisms that deal with that difference. When you touch something, you know exactly where your finger is—let's say you do—but the thing you're touching doesn't change in scale based on how far away you look. You know the depth, but you don't have to infer that from scale, and scale changes with vision. The interplay between vision, touch, and scale is interesting. For example, you can imagine picking up a car with your fingertips, or imagine touching the whole side. You don't actually think that's a change—a little car, a big car—there's an optical illusion. When you look at things on the horizon, you're very bad at judging their size. You think the moon is really big. If I look at a ship across the highway, it looks bigger than it actually is, and you just can't shake this.

It's maybe like the columns in your brain are saying, "I know what this model is, and I'm doing it independent of scale right now." The model separates the perception, and that's related to that problem. But I don't think it's a small shift; I don't think the moon is tiny. It's an interesting problem in the neural mechanisms.

What about the voting layer—the temporal pool? I included that. In fact, that's the main method by which it does inference in my mind. You can narrow down by passing in a series of sparse representations, but its inference power really comes from voting with other columns. I included that, and the spatial pooler is included with temporal memory.

There are two cellular layers: a cellular layer called temporal memory, which includes a spatial pooler, and a temporal pooler, which includes voting.

What about attention? Good point. We don't have a neural mechanism for attention, at least in our papers. We don't have a mechanism for it, but it needs to go somewhere in here.

How about knowledge transfer? You could argue it's not a separate thing. For me, it comes up under learning. That's another thing I didn't list here. Would that be a neural mechanism or a property somewhere else? I think it's in my head when I think about learning. I didn't have that as a question. What about short-term memory and learning? That's not anywhere here. I think short-term memory is not an issue I'm particularly concerned about, but learning—yes. There's an assumption under all of this for learning to movement, but specifically being able to learn things quickly. I didn't break it out because I'm assuming that's basically the hard thing. For us, learning quickly is not hard. We form synapses quickly—humans do. All of our neural mechanisms could learn quickly. We've never introduced any neural mechanisms that couldn't learn quickly, so that's not a hard problem for us at all.

Shouldn't it be in here? I assumed it's okay. It wasn't a problem I'm trying to solve because I think we don't have that problem. You learn from very few observations. I didn't see that as a problem for me to think about because nothing we've done is slow. The whole property and the way sparse memory works is it just happens. There's nothing we have to do. Other approaches have problems with that, but none of the mechanisms we've considered have a problem with that. As long as you can form synapses quickly, you're done. There's no settling, no multiple passes. All these mechanisms could do one-shot learning.

I'm not worried about that. What about senses other than touch and vision? I'm not worried about that either. Other senses are not nearly as interesting, and I think there's very diminished benefit from adding more. I think a little about audition at times, because audition has some of the capabilities we have with touch and vision. You can locate a sound, but there are limitations. It's really relying on temporal patterns, which we do automatically. I didn't put that in here. I could have put in learning sequences, because I think that's part of the whole thing. The way I look at it, a temporal memory algorithm can do both. It can learn sensorimotor inference, and if things repeat in time, it will learn sequences. That just comes for free, so I didn't have to put that in.

What about an airborne molecule analyzer? That appeals to me, like with the coffee example. We're trying to get core fundamental learning things right, so anytime I think about another sense—smell, touch, sound, taste—I have to ask if it adds anything fundamental to the algorithms. This is not a practical exercise to figure out how to implement an aroma tester. It's about getting a fundamental, base mathematical principle by which models are built. Then I could throw in color, tonal qualities, chemical sensors, and a whole bunch of other stuff. I can add these other attributes into the system, but I don't need to worry about any particular one.

Some of those are not getting at the core essence. Think about vision: you can see pretty well in black and white. Color adds, but it's not really essential for most of what we do. You don't have to have color to understand vision. That's not true at all. You have to ask what you're adding. I put in a little category for the side—yes, we could add these somewhat important but not essential components to inference. I could add color to something and distinguish between your cup and my cup by color. That would be useful, but I put that in the category of other sense things that are not so spatial.

Anyway, that's how I do that. This is already a huge list of things to deal with simultaneously, so I'm still making some choices. One way to look at it is: what is the minimal set of senses someone could have to build a workable model of the world? 

Helen Keller, with a little bit of pre-learning because she lost her vision after birth, built a whole model of the world just through touch. She was very young when she lost her sight, but she had enough experience. The key thing was that she had learned the word "water" and finally made the connection. She wasn't like someone with real language, but she had several months of vision and some audition. Her big epiphany was realizing that words represent things in the world, and it took a long time before she understood that a pattern represents a thing and another pattern represents something else. Her language ability was very small, so you could say she was close to being born without these senses, but borderline. Someone born blind doesn't need that type of depth perception. People born blind can get along; they have deficits, but their model of the world is pretty good. It's workable—they just can't always infer the right thing because they can't infer at a distance except through hearing.

I think Kevin's point is a good one. I'm not trying to build a minimal set for a workable model; I'm trying to get at the core essence of model building. It doesn't mean that vision and touch are the only things I care about. Those are just two that are important and self-sufficient, and they give us the ability to look at two things and see differences. It's important to understand that the core components of model building can't rely on what vision has versus what touch has—the core components have to be the same in both cases.

Does that make sense?

Touch and vision: briefly, about the single column idea. When you touch something, it's not just a single column getting input. If you run your finger along a surface, you bump up against something—this lip here—and you're not sensing that with the same part of your finger. It's a different part of your sensory organ saying, "Hey, there's something over here." That's one of the first clues: it's not just one little patch moving over everything. It's a patch that's moving, and then other patches kick in and hit things. If you try to do inference imagining just your finger and one little thing, it's really hard. Even when you grab something, you don't grab it with just the pads of your fingers—this part is touching this, and that part is touching that. It's not a perfect system. I sense a bunch of things at different points, and each one reports, "I'm sensing a border here," but it's not the same patch.

Another thing is detecting curvature, which is problematic, but it's not if you assume multiple patches are sensing at the same time. When I'm touching the edge of my cup or the edge of this whiteboard, multiple patches on my skin are getting input. I don't need a single sensorimotor patch to detect an edge; I can rely on several in a row, and together they vote on an edge. That's a big change in my thinking. Sometimes I like it—it basically says inference is on a scale. We used to think the scale was one to many; now I think it may be a few to many. In some ways, that's nicer. It's just a different way of thinking about it, and I'm willing to accept that as a premise now.

I've talked about the different mechanisms, but I haven't thought much about action, policy, goal, and behavior. This is the list of things I have in my head, plus more that I can't think of right now. Trying to resolve all these things at once is hard.

Some of the solution ideas I'm working on—you've heard of all these already, so I just thought I'd go through them explicitly. I'm working on the idea that one mechanism could handle both morphology and feature displacement. At times, I've considered that maybe there are two mechanisms: a morphological model and a feature displacement model. But thinking about the circle, the sphere, the oval, and similar shapes made me question that idea. Morphology could be defined by curvature, which is just a local displacement—an instantaneous displacement at some point. If I move my finger along a morphological object, I'm continuously storing displacement, which is continuously storing the curvature at that point. I think that can work with reasonable amounts of memory. I haven't worked through all the details yet, but I think it can work. Then you could vote, and all those things would work.

The second problem is figuring out the displacement, for example, between the team house and the coffee cup on this table. This might be even harder because the location of my coffee cup isn't just one thing or one place. Do we pick the centroid? I don't like that idea. The original solution we had for displacement in the framework paper didn't rely on that kind of thing; it didn't require a specific point. The frameworks and columns plus stuff—doesn't that do that? It didn't work. The only reason it didn't work was because we didn't have the reference phase information. Now we have a handle on that. It wasn't just that we could never get it to work—even with reference phase, I could never get it to work. We were using multiple grid cell modules for that, and derivatives of multiple grid cell modules. Now we're not looking at multiple grid cell modules anymore. That mechanism might not be right, but it still did that, and it didn't do orientation—that's the reference frame. I don't know if we could have gotten it to work with grid cells and reference frames. When we worked on it, I brought that issue up, and we just could not get it to work. Maybe you can tell me how to do that. There was also the issue of the curvature of the cup—it didn't handle continuous shapes. We did it, and it was beautiful, but it didn't solve all the problems. We knew that in the paper; we brought it up in the paper. Now we've moved away from these multiple grid cell modules, so we've moved away from multiple displacement cell modules. We have some parts of it there. I think that's a really interesting lesson. There are some clever things about it, and that's why I brought it up. It's tempting to say, "What's the displacement between these objects?"—just point here and there and calculate displacement. But it doesn't seem like that's what we're doing. It's more like what we did in the frameworks paper, but somehow different. The frameworks paper didn't work completely. It's got to be more like that. I now feel more comfortable doing the continuous displacement than the point-to-point displacement, because I don't know how to do that. So I'm working on that.

In terms of morphology, the temporal memory mechanism—we've talked about this—can do this. It can learn continuous displacement, and it can, as with sequence memory like a melody, do point-to-point in some sense. It's close; I just haven't gotten all the pieces to work yet. The hard thing I'm struggling with is the issue of "Where's the location of this object?" It feels like it needs to be more like what we did in the frameworks paper, but I don't know how to do that, especially since we're not using multiple grid cell modules.

All the evidence suggests that was wrong.

That's a good point. It required quite a few grid cell modules because you're trying to create an SDR across multiple grid cell modules. If you need a minimum of 10 or 15 bits to form a dendrite to recognize a pattern, you'd need at least 15 of these grid cell modules, but they don't exist. The grid cell modules in the entorhinal cortex are stretched out, they don't seem to communicate with each other, and they're very different.

There was a paper—maybe the pine tank paper or something sensorimotor in the V1 model—but it doesn't appear that we're doing that. There aren't multiple grid cell modules in a single column or even in multiple columns nearby, and there doesn't appear to be enough grid cell modules, even if I look at neighbors and other things. I could be wrong, but at the moment, as I pointed out, there are other ways of achieving the same result.

If you think about the tank paper, which showed what a grid cell module looked like in a plane view of the entorhinal cortex, it was composed of six phase groups, about the size of a quarter of a column. Maybe the quarter-column grid cell module looks like that, with multiple phase clusters, each representing the bump. In each one of those would actually be a minicolumn. If I use temporal memory on that, I could form a very unique representation of space, not by using multiple grid cell modules, but by using a minicolumn of active cells in each of those.

Imagine that the minicolumns of the temporal memory are not features—they are the grid cell locations in all those phase clusters. That actually fits really well, looks beautiful, and there's evidence to support that idea. That's an alternate view of how you get to a unique location using grid cells without using multiple grid cell modules. That seems to be the better way to get unique locations. It sounds like the same mechanism, but it's not. You need to get the right papers in here.

The way we did it in the framework paper was to assume there were multiple grid cell modules, lots of them, and we had a bump of activity in each one.

These modules had to be different—they either had to have a slightly different scale or a slightly different orientation to each other, so the bumps don't move in unison. They move around, and if you sample across them, each one gets anchored independently. So, if I'm on an object, like a coffee cup, and each one gets anchored independently on that coffee cup, then the combination would be unique. As you move around, it continues to be unique. It's a very large space.

The alternate proposal is that a grid cell module is actually like a temporal memory, and it's actually a layer of cells.

It's a layer of cells, and these cells really represent minicolumns.

And so you have a bump of activity moving along in the minicolumn space, and yet you're picking individual cells in here. This is much closer to the temporal memory algorithm, where you're applying grid cell input versus feature input. In this case, you'd end up with very unique representations of each location. You could define a space of this object where all the points are unique, but you would still get path integration at the minicolumn level.

In the Tank paper and other papers, what we found, or what they pointed out, was that individual grid cells often would not fire when you expected them to, and they did so reliably in a particular context. I would have preferred to see it very sparse. They didn't report very sparse, but they did say individual cells would be expected to fire because the bump is here, but the cell is not firing, and it does so always in this context reliably. So it's not random or an error. That's evidence that at least moving in this direction might be possible.

I thought about when Tank did this with the phototechnique, where they can look down at the active cells, and that technique actually looks at a very narrow layer. It doesn't look at the depth at all, so they wouldn't see the activity of these cells down here. Anyway, that would be my current hypothesis, better than the previous one, because there are a lot of things wrong with that, and there's no evidence for it.

Like the duck analogy: it's going along, but underneath it's paddling furiously with all those things they can't observe. I hadn't heard of the duck analogy, but yes, that would be like it. It looks like it's three—it's more like the duck's feet are just little pieces of them. If the duck wasn't SDR, then yes. Okay, jellyfish.

This is how I went back to the basic normal mechanisms, like the temporal memory mechanism, which includes the spatial pooler. In this case, I wouldn't have a spatial pooler here. I have something that's equivalent to the spatial pooler. The spatial pooler takes some sensory input and turns it into a sparse, or not too sparse, but some sparse representation. Minicolumns here—I already have the grid cell as a somewhat sparse representation of space. If I just assumed that the minicolumn was actually representing that, I don't need a spatial pooler. I substitute the bump for the spatial pooler with the bump for this.

The same mechanism says, "You give me something that's not unique, that's measurable in some sense—it's measurable because it's path integration. You give me something that's not unique, I'll form a unique representation for it." Now I can associate this non-unique bump with a unique location on the coffee cup all the time, and nothing else is going to be in the same space. It also gets around the whole issue of the grid cells repeating, because in the minicolumn space, you can only represent a small number of things, and they have to repeat. But in the cellular space underneath, it's a very large space, and they're not going to repeat.

I talked to Ben and B about this recently, maybe someone else. I'm working on this, and I think that's a reasonable part of my toolbox right now. That's the same sum, and so what I'm actually thinking about is that there might be multiple cellular layers in a column working on these principles. We've talked about how one could be taking displacement as your input, meaning equivalent to the output of the spatial pooler, equivalent to grid cells. Displacement says, "Here's a displacement," but I can also represent that displacement uniquely in the context of this particular object at this location. Therefore, in the context of the coffee cup, I know that if I have this displacement, I go this way, I'll have something else. In a different context, it would be a different thing. So you go from measuring displacement to representing it in the object space.

Okay, so solution ideas.

I already mentioned this one. I'm trying to get to the idea that there's really no distinction between morphology and feature displacement. It's one way to think about this. Imagine you're a primary column and you're getting input from a sensorimotor.

I've always said that this input you're receiving can't really be subdivided. Imagine all the signals get mixed up on the way up, so you can't detect power here; they're just spatial, just powers. Imagine this is your input coming into the column. We detect what we might call a feature, and ultimately, you'll do temporal pooling to achieve a stable representation of the object. That's the goal of inference: as you move and touch, you have a changing input, but you end up with a stable representation of the object's label. This is why the world seems stable even as your eyes move or you move your fingers. You don't think the laptop is moving. The core essence of inference is that multiple inputs are transformed into a single, more stable output.

In this case, the input could only be what's coming from the sensorimotor system, which is impoverished. Let's say I can detect curvature, or curvature plus color and other things, but I'll focus on curvature. Now, I have another column somewhere else that's getting the output of this column's input. The sensorimotor system can't detect thousands of things; it just provides limited information. However, the output of the scene can be quite large—there could be many morphological objects this column can recognize, maybe hundreds or thousands. The input to this column is limited, but the input to the next column is not. Now I can build a compositional model where the input comes from other columns, not just the sensorimotor system.

If I call these objects, these are objects the lower level has already recognized. Now I have many of these objects to build a decompositional structure. I've started using language where you have one object coming in at a time, then a pooled version of a compositional structure, and then another object coming in, forming a compositional structure. In this case, the input may be very limited; in the other, it's not, but they're doing the same thing. By the nature of the input to a low-level column, you're going to learn morphological structure. This low-level column can't learn something like a bicycle because it can't detect wheels, but it can learn morphological structure. The higher-level column could learn a bicycle because it has access to all these morphological structures and can assign them accordingly. I'm trying to explain how the same mechanism, depending on the input, can process morphological objects or what we might call feature-to-feature displacement. That's the general idea I'm trying to get to work.

Does that make any sense?

When we think about displacement, what's really important is the relative arrangement of things. The actual distance between those things can change at times. For example, my coffee cup is rigid, but if I stretch an object, I still see the coffee cup because the local feature arrangements remain in the same relative order, even if the distances aren't exactly the same. Imagine a sheet of features you can stretch; as long as the features stay in the same relative order, you perceive it as the same thing. It's like a logo on a t-shirt folding or wrapping around a coffee cup—the actual distance between features isn't as important as their relative arrangement.

If you change the relative arrangement, you get a different object. I can stretch out the letters in a word or bunch them together and still recognize the word, but if I change the order of the letters, it's no longer the same word. When we learn displacement, we're able to recognize that two features are in the right arrangement, even if the distance is off, up to a certain point. If I separate the letters of a word too much, I just see a bunch of letters.

We were talking to a high school this morning about the t-shirt problem—how you recognize a t-shirt in the dark or pick it up from a pile of cloth. You move your hand along it, find an edge, and decide whether you're on the bottom or top. What you really want to do is get to the collar, which feels different, and then move your fingers along the collar. Once you reach the two seams, which are always a bit toward the back, you know you're holding the collar and can tell which side is front or back. I find these two features, stretch them out, and then I know where I am on the object. On different shirts, it's different, but the method is the same. The rest of the features, like a pocket on a t-shirt, are all relative to each other. Even if the pocket is bent or curled, you still recognize it because the relative features are in the correct positions.

Now here's the big one I've been working on. This is maybe the big opening, and we've talked about this briefly.

The original idea behind the model is that you have a reference frame from the model, and that model has an orientation relative to the body. You can take a three-dimensional model and put it in any location and orientation to the body. That would be the clear engineering solution here. We've found a lot of evidence for that, though I didn't write down all the evidence here, maybe just a few of them.

This is the third point: models have only one orientation dimension, which is vertical. There's a lot of evidence that we learn models in the vertical, and a particular one dimension of orientation has to be correct. When I gave the example of writing the E and the 3 and the M and the W, it's not like I see the M and the W as the same thing rotated. I just see one as an M and one as a W, and that's it. The models depend on which way is up. It's not like there's an independently floating model out in space and I have to figure out an orientation. There's a lot of evidence for this. There's an evolutionary argument: when you're walking around the world, trees don't go upside down. Everything seems to be in the same orientation, but you're in different positions. There's the head direction cells argument, the E versus 3 argument. One idea we've tossed around in the past, which I think has a lot of validity, is the following. We have two things to do this. The idea I'm working on right now is that we have to have the correct north orientation—it has to be one—but the other rotational orientations don't really matter. There's this idea that the thalamus is able to compensate for changes in north. The clearest example is that as you walk around the world, your head is always tilting one way or the other, and when you're reading a book, you don't even notice it, but it actually completely changes everything coming into your brain when you tilt the book 10 or 20 degrees. I'd have to rotate the reference, and you don't seem to do that. Remember the example I gave where I lay in bed and showed the pictures out my window? When you're lying in bed, you see this horizontal scene and your eyes are scanning the scene, but your eyes are actually going up and down because you're lying in bed, and yet there's no effort in this. One of the ideas is that the thalamus is designed—one of the things it does is this rotation in the vertical orientation—and the vestibular system, the inner ear canals, actually feeds into that. From an evolutionary point of view, an animal needs to see the world in its upright position, and as it's moving around, at any point in time it might be tilted this way or that way. The vestibular system says, I know you had to tilt it, I'll correct it for you. It's not like I'm rotating the model in my head; I'm just rotating the inputs and the motor commands. If I need to move horizontally in the scene, given the current orientation, I have to move my eyes this way, so give the same command and I'll change them for you. I know it's a little weird, but it seems like this is happening. Now I can compensate for the vertical in some sense. If it gets too much, we know that if I rotate something around 180 degrees, your brain has to slowly rotate it back. It's a mental effort to do this. If I should recognize something that's upside down and it's not clear right away, it takes a linear amount of time to get it to a position where you can actually imagine it.

Now the second question is, what about orientation like this? I assume I've got this called vertical, but with different views, wouldn't I have to rotate the model? The example—can I borrow that coffee cup? Not much. Imagine we said there's a reference frame for this cup, and the reference frame includes the 3D orientations. If I learned this model with the handle on the right, as I see the handle on the left, I'd have to rotate the model and figure out the orientation, infer the new orientation. The idea I'm working on is that you don't do that at all. What happens is, if I think about displacement as always being in a two-dimensional arrangement—imagine they're always 2D—I'm looking at some surface and calculating the distance to the next thing, but I'm only calculating its planar angle, not its three-dimensional angle. If I look at this coffee cup and I learned the displacement here, and here, and here, I end up with, instead of a three-dimensional model displacement, this sheet of displacement. It's not a physical sheet, but that's how you think about it. It's like I'm unraveling the coffee cup in a linear way.

The displacement would be more like, imagine just on the cylinder, you have the cylinder. I'm unrolling it: here's the displacement when the handle's on the left, here's the displacement when the handle's on the right, and here's the displacement when the handle's not visible. When I infer the object, I just have to match the correct displacement. Basically, I'm matching some subset of this. At any point in time, I don't have to rotate the model. This is the model, and I'm going to recognize a piece of it and say, okay, that's the model. I'm inferring the model, the object—the coffee cup—at the same time. I'm saying this is what it looks like when the handle's on the left, and this is what it looks like when the handle's on the right. I don't think this is going to take a lot of memory. In fact, it might take less memory to do this. It's a little bit weird, but I deduced a long time ago that if we're going to wrap an object, like the logo around the coffee cup, the logo has to be a two-dimensional displacement field. The coffee cup has its own morphology, and sometimes you can wrap it around that. You put it on a flag, you put it on a t-shirt, and so you force yourself into thinking about displacement in two dimensions. Then there's an underlying morphology to the thing.

In this case, there is no orientation to the model. You infer the orientation by recognizing and inferring the objects. Once I infer the object, I'm in some part of this model. I can't see these pieces over here right now, so I don't know that, but basically I'm making a series of observations about displacement and I say, oh, those match the model at this point right here. If I was looking at a different angle of the object, I'd say, oh, they match this part over here. That kind of thing.

It's a little bit strange, but it gets you around the idea that you have to figure out the orientation. You do have to get the north, but after that, there are issues with this. I know there are issues, but it's very appealing at the moment because it solves a bunch of problems. It leads to the suggestion: imagine I now have a model that represents different states. You could almost say this is a model of the coffee cup, and this is one state of the cup, and this is another state of the cup. This is the state of the cup when the features are arranged this way, and this is another state when the features are arranged that way. I don't think this takes a lot of memory. I haven't proven that yet, but I think it doesn't. Then I asked myself, could all states of the model be represented somehow in this same basic scheme? Could I think about the stapler being closed and the stapler being open as part of a larger sheet, a set of displacement where different regions represent the different states of the object? I haven't worked this through yet, so it's a very fuzzy idea, but it would be nice. What's nice is you can imagine a model of an object where all of its states are represented in the same set of displacement. You can't view all those displacements at once, but when you infer the object, you're inferring the state. I don't have to have a separate thing that says, oh, this is a stapler, what state is it in? I just say, because I've inferred it in this section of its displacement space, then I know its state. That's just by definition. Movement of the object would be moving between these regions, like when the state goes from open to closed. I'm going from a bump of activity here representing the closed state, and over here a bunch of activity in the open state. I don't know if this is going to work, but that's what I'm thinking about.

So this is displacement arranged in 2D like a sheet, and all displacement is thought of as lying on a 2D sheet. Different views of an object are represented in different regions of the sheet. Inferring a cup with a handle on the left versus the handle on the right does not require determining the orientation of the cup. Both views are stored in the model. 

So what about the known special features, like color? I told you I didn't want to talk about that yet.

The way I think about it goes back to how we were thinking of the grass earlier. You can associate with, imagine there's some point in the object, and you can link from that point to other objects.

Maybe a feature, maybe even something like color—I don't know yet. The point is, you could say, somehow I have to get the mental logo only visible on one of these things here. It's continuous until it disappears, and so it's only located in one place, yet somehow the logo has color associated with it. At some point you'd say, at this point, this is the letter N or this is the word "menta," and I have a link to that over here. So I can't put it in this model, and so I somehow...

The underlying morphology and shape of this object is not completely dependent on the color. If I know the color, I might isolate where I am, but if I don't, I can still infer it. As soon as a set of attributes is out there, and you have this thing you're perceiving in front of you, maybe it matches some of those attributes but not all. Recognition would be similar: at some point, you would say it matches all the things you know about, so now you have identity. That gives you a soft way of associating things from the most important aspects of whatever is being stored in the two-dimensional map.

That attribute is going to be part of that state now. I think the attribute is not critical—it's like having the lights on or off, because different states of the light exist; some of them are broken. Once it's the same light, they have different states: one is on and one is off. There's no color there; it's just that the light is on or off. Many models of the world, as I'm hearing you talk about this list, don't include what the lights look like, where they are, or which ones are on. If you ask me to draw a map of where the lights are, I would assume, but if you ask how to represent whether the camera is on or off, there's an LED there that's off. That's an example. My point is, you have this sort of graph that represents, in some sense, the shape of the object, and you can link to specific things that might be associated with that point.

The actual mug would link to the logo, but the logo wouldn't be part of the mug because we've already learned that separately. You can still infer this shape here independent of the logo, but if you know the logo, it can help if you've learned that. My point with the lights in the room is that I haven't learned what the lights look like or which ones are on, and looking at the lights doesn't help me identify the room. But if I studied lights every day and thought about what kind of lights are in here—if I'm a lighting guy—I've got it memorized. The next time I'm in the room, I know it by the lights. But for most people, that's too much memory. These links let you temporarily look at what's associated with a place, but you may not remember it. Some attributes are really helpful; for me, the lights wouldn't be, but the arrangement of the two screens and the windows would be. If the main thing is to recognize where you are and whether you're in the same room, you just need to know a sufficient number of attributes for your perception to say, "I know I'm in the same room or not." You don't have to memorize every detail.

If I've spent my life as a lighting engineer, I might take in other attributes because I'm geared to add those. For example, on my sailboat, I know every little detail. You'd walk up and just see a sailboat; I would see someone with a clip in the wrong orientation. It depends on how much time you spend looking at and interacting with something. My model of the boat is very rich because I've spent a lot of time with it; yours would be more modest. Anyone can do that, but at a certain level, you say it's sufficient for you to operate. I'm being bombarded by details all the time, but if something comes up that I need to observe more closely, I focus on it. If someone says they're looking for a book, I might hone in on that. It's flexible.

I'm trying to understand object behaviors in this idea of a single model. Instead of having a model of what an object looks like and then a separate one, it's not like our old views were displaced or behaviors were wrong; let's just have a different mental model for it now.

That, in some sense, I have these memories of all these different displacements. I can memorize the views of an object in different states as what I observed, looking at this thing in terms of its displacement. As I manipulate the object, I'm looking at different parts. There's this continuation between, for example, the cup with the handle on the right and the cup with the handle on the left—they're really the same object in two states. Similarly, I might be able to do multiple states of the object's deformation or morphology. I would say, "Now I know it." That's all I'm trying to do. I'm trying to prove to myself that a two-dimensional displacement—primarily two-dimensional—would work. That's not so obvious either.

So, I have a couple of questions. I'm not done with this; I don't really know if any of this is going to work. I just think it's such a fascinating idea that I want to dig into it a little bit.

One question would be: where would a model like this live? What neurons would be responsible for this 2D sheet? It's pretty simple. I talked earlier about how you could feed displacement into the temporal memory. Instead of putting in a spatial input, imagine what you're representing in the minicolumns is, let's say, local curvature at that point. I haven't worked through the details, but the basic idea is that all this could be represented in this sheet of displacement, all in a single layer of memory. It doesn't have to be spread out. At any point in time, you say, "I know where I am," and at this point in the model, these are the displacements I should see. Because it's a highly unique representation, if you feed in displacement into the temporal memory, into the minicolumn structure, the system is representing displacement, and I'm feeding in, as context, the unique location on the object. Then I can say, in a unique location on the object, this displacement is represented uniquely, and therefore I can link displacement.

At point A on the coffee cup, there's a curvature to the right. Let's say that's the way it goes, and something curves this amount on another object at point A, it would go a different way. If I have a unique object and I have the displacement, then I can learn the transitions of displacement like I learn the sequence of notes in a melody. It could be unique for the object. If I didn't have a unique representation of the object—if this could be two different objects—then I might say, at this location, if I move to the right, I'll get one displacement on object A and a different displacement on object B. As soon as I move, I would know which one it is, the same way the temporal memory narrows down from notes. It would narrow down a series of displacements associated with locations. So the same 2D sheet here can actually represent multiple objects, and in fact, multiple objects simultaneously. It's very much like we did in the columns plus paper. There, we were feeding in a unique representation of location as context, but we were using observed features as the inputs to the temporal memory. Here, I'm going to switch out features and put in observed displacement, and you basically get the same thing. Now, it's just different, and it'll narrow down on things. This representation is still specific to a specific scale, to be very specific.

I've already mentioned that scale is a problem, right? We don't want it to be scale-specific in this case. What does scale mean in this?

Let's imagine we're just talking about curvature, and we're going around the cylinder. The cylinders can be bigger or smaller. The scale would say, "When I move in this direction, I should feel it rotating away from me this way, not like that way, but this way." If I go this way, it's straight; if I go this way, it's curved. That's my prediction. My prediction says maybe if I consider the distance fixed, then I expect a very specific curvature. But what if I get a sharper curvature or less? I need to say, "That's not the right curvature, but it's in the right direction," something like that. Then I say, "It's a tighter curve than I expect, but it continues to be a tighter curve," and somehow I have to say, "This is a different scale."

Infer the scale on the fly. Based on the amount of motion and my model, I might say, "Moving this much only moved me this far on the object." It's the same thing, but it just means the object. It's complicated because I had unique objects that were different scales in that size. I wouldn't want to confuse them, but I also want to distinguish differences in scale.

The hope is that locally, you always have the same displacement, yet the scale and deformation mean the distance traveled is changing. It's sharper or less sharp, but as soon as it starts going the wrong way completely, then it's not a scale issue. I think about it along this line: if it's roughly the same, then it's this way. Since I don't know where I am on the object, at least initially, I represent it as an input. I have some coverage here and a displacement in a certain direction, just going over there. I observe another local coverage at this point, so I have a sequence, a spatial sequence of those local coverages. Those recognize the object, but somehow the actual specific differences matter too. I would know if this is the one I had before. I know the difference between the little Met coffee cup and the regular Met, so I don't have to say, "Oh, I adjusted, it's a coffee cup." No, I say, "No, this is a small one." That tells you it's an uber parameter. It's something that the model, when inferring, says, "Yes, I recognize the shape, but it's not the right dimension." It's not the dimensions I learned. The model has to include those dimensions because I learned a specific coffee cup, so that's got to be part of it. Then I'm able to infer it even though it's not the right curvature, which is not as steep. One way to think about this is to say, "If I can recognize everything just like I observed it, it'd be a perfect match." But if things aren't as I observed, I'm able to keep going, even if the distances are wrong. At some point, it would say, "Okay, they're all wrong by the same amount." That's why I could say it's just a smaller version of the shape.

Or maybe I don't know yet, but I think it can work. Some concepts might just go by direction, without taking location into account. Then you would recognize, "Oh, this is a coffee cup or a species of the family of coffee cups." There would be others that take location into account. Once I've figured out, "Oh, I'm looking at the coffee cup," maybe I would rather have one pattern for everything.

Maybe the evidence would suggest that columns are different in that regard, with different input. Going back, imagine this is a grid cell module. This is a bump representing location in space, and these actual cells are unique to any particular object, that location on the particular object. Then imagine you have another set of cells here that is taking in displacement.

Same idea: there are minicolumns here, but individual cells say, "Here's a displacement in the context of a particular location on a particular object." Imagine the displacement feeds into this, and this feeds back to this.

Your movement means you know how you're moving and what you're detecting for displacement. If I start narrowing down here, I'll start narrowing down here, and when I'm narrowed down here, I'll make different predictions about this. The basic idea I'm working on is that you can be modeling this temporal memory layer, which basically says, "Give me something that is not unique to this object, but I can detect and determine, and in the right context, I'll make it unique." Then they can feed into each other. At one point, I'm getting the displacement, and next, I'm getting the path integration from grid cells.

These could interact back and forth, and you basically narrow it down to where you are and what you know. Then you could predict what the displacement is next, because these patterns would link to each other. They could link to each other, but they're linked to these things for certain reasons. It's a rough idea, but the way I think about it is the temporal memory layer gives something you can measure that's not unique and can form a unique representation of it. If you have multiple ones working, with different sorts of inputs, one could be detecting this and the other detecting that, and together they narrow down the answer. You can even think of the temporal pooler layer as an extension of that. We haven't hypothesized it works on the minicolumn idea, but the temporal pooler, like I said, has a union of ideas. These are all the different objects I could be looking at. Then you have these inputs from other columns in another context that say, "Given the context of these other inputs, I can narrow this down." Since you narrow this down, you could feed this back and say, "My neighbors told me it can't be those things, therefore, I'm going to tell you it can't be those things, and you can't really do this." There's a balance between narrowing down as you move and narrowing down as you get input from other sensors.

There’s one perception I find interesting, which is at a slightly higher level than what we’re discussing. If you come into a room and someone has broken an object into two pieces that are sitting on the table, you instantly recognize it as broken. If you see an object you’ve never seen before, and it has a divot and a piece lying on the floor, you quickly infer that it’s broken. You notice two objects with matching color or features and can match that a piece came from the main object. You can recognize that something is broken.

I don’t fully agree with your second point. If it’s truly a novel object, not like anything I’ve seen before, I don’t think I would know it was broken. If I see something that’s kind of like a cup in two parts, I might infer it’s broken. For example, imagine a big sheet, and you might infer the bottom of the stapler is part of that sheet. As you move, you’d expect to see the top of the stapler, but it’s not there. Then you see another piece elsewhere and realize the bottom is missing. At some point, you recognize something is wrong—it’s missing components that are supposed to be connected. You predict it should be there, but it’s not. If I walked into a room and saw only the top of a stapler, I’d say it’s missing the other half. That model could explain it: I recognize it’s part of a stapler, but I’m missing the other part. I expect it to be there, either in a closed or open state, but I don’t see it.

That’s one problem. I think I can explain how that happens—piecing together the fact that I see a topless stapler here and the bottom elsewhere seems like a higher-level cognitive process. It’s about chaining things together, which has the potential for handling this more abstractly. The system definitely has the ability to handle missing parts, broken objects, or things bent out of shape. If I break a coffee cup into two pieces, the broken edge becomes a feature of the object, and I can recognize it as broken ceramic. That’s getting beyond my current thinking; I’m trying to figure out how to extend the model into that.

The hope is to get to the core idea of how models are built and inferred. Once you have that, you can tweak the ideas and figure out other things. The ideas I’m working on are unusual. I predicted this a while ago: the idea of a two-dimensional displacement sheet, wrapping around morphology, or even the idea that only one orientation matters—vertical—and the others are not represented externally, but within the model itself.

If I get this to work, it provides a new foundation to tweak and figure out other things. If it’s wrong, I have to start from scratch. It also makes things simpler: the retina primarily gets a two-dimensional perception, which makes things easier. Representing 3D has been a challenge from the beginning. The question is how two-dimensional sensory organs build 3D models. There’s even a question about whether grid cells can represent three dimensions; Marcus and I have reviewed some papers, and it’s questionable. It would be great to model things using two-dimensional structures that capture three-dimensional shape.

There’s the concept of self-organizing maps. In binocular vision, the two halves of vision are deeply intertwined. That’s taking something higher-dimensional and forcing it into a planar configuration, which is what self-organizing maps do. When you get the pinwheels of orientation, that’s a multidimensional signal impressed into a planar representation. The power of self-organizing maps, which may be happening in cortex, is that you get multidimensional inputs with a signature, but it looks odd when imprinted there. I read about that a long time ago.

The question is whether it’s important to know more details or if the general idea is enough. You collapse these structures onto two circumstances, but do I need more detail? When you build it, there’s a gradual process where you move things closer using a metric of similarity. It’s not clear that we’re collapsing 3D structure onto a 2D surface; there are different ways to handle higher-dimensional attributes.

To some other object or thing. It's not clear to me that the concept of collapsing these onto a surface is the right metaphor. I'm not sure. It's different because they do dimension reduction, which we don't want to do here. It's not dimension reduction. We still represent all the different states of the object, like rotation or opening and closing the stapler. We still represent all the different possibilities. If you look at dimensionality as being the attributes you have, and the ability to localize the detection of orientation, or which kind of lines up in a pinwheel thing, or where the two halves in visual fields are put close to each other, there's a morphology in the cortex itself that suggests you're putting multiple dimensions together. I don't believe the only thing those columns are doing is orientation. It's just that when you look for it, you see that particular pattern come up. They're probably doing a lot more. I think orientation is just one example.

I think it's an interesting observation about self-organization. I'm not sure if it's the right metaphor for me to work on right now, but I think what is being said is we're really not trying to do dimensionality reduction. Sometimes I'm trying to finesse the 3D structure by learning 2D points of curvature. The way Marcus and Merko do it is a completely different way of representing high dimensionality. Lots of different grid cells, each with a preferred direction, and you project everything according to that. You have a bunch of these, and together they represent the full space. I don't know if that principle could apply here. It's an interesting question. It's pretty generic, but the idea that I have to be able to make these projections onto different planes happens here. That's just the proximal synapses. The basic principle laid out there is an extension of the principle we had in the columns paper, which is in the framework paper: you have multiple modules representing the same thing, and together they represent a rich representation. We did it in a two-dimensional sheet, a two-dimensional plane. Now we're representing different slices of three-dimensional space. Same basic idea. There would be a group of cells with a preferred direction, and another group with another preferred direction. It doesn't have to be good memory.

That's another way. I'm more comfortable with that one. I feel pretty certain that's going on to some extent. That's how we did the nick stuff. I always can't remember what that was, but basically I could do these multidimensional mappings with the template. It was just a demo, a hackathon thing.

It can map anything to anything, so you can just learn all these different maps, the location. In some sense, this is where the granularity of locality is represented. It could be way down deep, or you could organize at a larger level. The data I've read suggests that. It's an interesting notion: how distributed is it, how localized could it be? The theory is that everything has to happen within a single cortical column. In a couple of ways, if we start thinking of compositional objects, you might have to go that way. That doesn't mean the entire sheet is involved. The maps are pretty much at that level.

There could be a granularity boundary here. That's another interesting idea. Can I change the topic? Sure.

The weird thing about V1 is it just gets input from the retina. V2 gets input from the retina and from V1, and that's the general rule as you go forward. The next after V2 gets input from V1, V2, and the retina. How do you make sense of this? One thing I ask myself is, what if I was trying to learn the coffee cup or a device-specific object? I have a cylinder and I'm trying to put the logo on. Where would those associations be? One could argue that might be saying I'm object.

It's a morphological object, but I'm being told there are features detected in V1, which is the logo, and therefore it flips it around. The unspecific morphological object might be represented in V2, and the specific feature, like the logo or the word, is in V1. If V1 is determining the morphology, but this is the feature you should associate with that point on the morphology, it flips it around. I used to think you have this morphological stuff lower down in the first region. I'm just pointing out that's an interesting twist to consider as I figure out how to do these things.

What if it's involving multiple regions in a hierarchy? Who's representing what, and what are the links between those two? In the past, I've thought we shouldn't do this, but now maybe we should think about it. We also have feedback going downwards. That's the point—there's a huge amount of linkage both ways.

It's not reconciling as much as it might be just forming links between the features. The morphological object in V2, or the second region, whether it's S2 or whatever, is saying, "I'm on this point on the surface of the cylinder, and I'm going to associate that point with what you're representing down there in region one." So you're saying there's a logo here. Fine. When I'm at this location, I'm predicting I should see a logo, so it's telling me one thing, and then V1 would be saying, "I'm seeing a logo; what's important is what object, what overall thing?" I think that's the key, where one layer...