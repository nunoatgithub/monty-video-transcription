I think the first topic is Niels talking about compositionality and, more specifically, how to evaluate it. In Monty, we have stacked LLMs, and we want them to look at objects, evaluate compositionality, and identify the object. How do we evaluate that? It's not very straightforward. Sonia will discuss the challenges, and if we have time at the end, we might talk more about behaviors. Viviane has notes on the "can worms," which is about compensating for object motion in the morphology model to predict morphology features accurately.

This is the topic, but we may also discuss time if there's time.

Go ahead, Niels.

Thank you. I'll get started. I don't have slides, but I've been putting together a document I was going to share. It's not as polished as I hoped—it's already 15 pages and growing because there are many elements. I probably won't go through it in detail, but I thought it would be worth covering at a high level and discussing some of its points. Maybe we can discuss it a bit now, and when I share it, it'll be clearer.

Why are we talking about this? What should you start thinking about? It might help us find solutions, as many open questions remain.

As Rahmi said, it's about how we evaluate learning and inferring compositional representations. How do we benchmark that? How do we convince ourselves Monty can do it? How do we mediate that? What do we need to enable Monty to learn and recognize compositional objects? That includes things like supervision—telling Monty this is a child object with this label, for example. It can also include things that might be out of scope for implementation but are important, like attentional windowing.

There are also notes to myself in here. I'll focus on some of the figures. This should be familiar: compositional objects and modeling them. For example, a logo on a mug—the logo could take different forms, and we can quickly associate these together. In the long term, we might have the concept of a handle as a child object. For now, we want a basic version of compositional representations to emerge and be evaluated. For context, when we have a single object scenario, we show an object, put it in the habitat simulator, and know what object we've put in. Evaluation is straightforward—we know the object and its pose, and we can compare Monty's output to that and see how well it's doing. 

As soon as we add multiple objects and create our own datasets, there are two ways to know the ground truth and evaluate. The simplest, which we've used before, is to have one object to evaluate, so we know all its properties. With multiple objects, we need to know what the sensorimotor module is actually on and what the learning module should be observing, and what the correct inference is at that time. We can do this to some degree through the semantic sensorimotor in Habitat. I believe something similar might exist in MuJoCo, where we can read out which object the agent or sensorimotor module is currently sensing.

Part of what I've been thinking about is how we can still benefit from the semantic sensorimotor, how we can avoid needing it, or, even if we have it, what open questions remain about how we evaluate Monty.

Going back to the document—actually, maybe just to show—oh, sorry, go ahead.

When you say evaluate, now that we have two LLMs, are we going to evaluate both, or just the higher-level one? Great question. I feel confident saying we want to evaluate both—the higher-level and lower-level LLMs.

It's generally more straightforward to focus on the higher-level one. That's what we can do to begin with, but these are the kinds of things we need to decide. Please stop me for questions or comments. In case you haven't seen the semantic sensorimotor before, this is a rapid demonstration. The sensorimotor and agent move around in a multi-object environment. Sometimes the sensorimotor module points at the golf ball, sometimes at the mustard bottle. Up here, you have a label from the simulator. With multiple objects in the environment, we can get that label, which makes evaluation more straightforward.

To start, even if we have access to that information, let's say we have a lower-level and a higher-level learning module, both observing the mug with the logo. In this setup, we want the lower-level module to represent the logo and the higher-level one to represent the mug. But it might be equally valid for the higher-level one to represent "mug with logo."

This one may feel more correct, but it raises questions: how do we learn "mug with logo" without discarding or restarting, given that we already have a model of "mug"? How do we branch that off?

The mug is in the lower level. Is it in the higher level also? I'm assuming it's in the higher level, but in general, we would want models to be in both levels, because you could have a mug that's a child of a logo.

In the very beginning, the first time we see the mug with the logo, we would just have the mug representation in the higher-level column. Over time, you would learn that compositional object and the association of where the logo should exist on the mug. The question is, would that be an update to the existing mug model, or would it be a new model that starts learning "mug with logo"?

It feels like maybe a branching of some kind would be useful, almost like a fork or a branch, as if it were a repository. You wouldn't start over from scratch, but this is now a new object with prior information. From a biological perspective, it's odd to think about. It seems possible if we represent it as an object with different states—the mug can be in different states, and depending on the L1 input or representation, it expects certain features or doesn't. Those could be the logo or no logo. But it seems hard to just make a fork of an object, like a copy, and then learn extra things on it.

Maybe both happen at the same time. The first time you see the logo on the mug, you recognize the mug, but it's in a different state with extra features. As you see it more often, or in parallel, you're learning a new model of "mug with logo."

If you learn it at first as a different state of the existing object, and then encounter them independently, you could assign a new object ID to that state. They would still share a reference frame, so it's unclear how much that makes sense. For example, "mug" and "mug with logo" could be "mug in state one" and "mug in state two," or "mug in state two" gets its own object ID because you've seen it. The intersection of the state and the reference frame ID would give you a unique ID.

A standard mug is its own ID because it's state zero. I like the suggestion of using states to help with this. In terms of what we do next, I don't think we have to solve this problem to begin with. We could already look at many interesting problems about representing positionality and having that emerge by considering the situation where we learn the low-level objects in isolation. We can see whether this happens, because regardless, we need to do additional learning before we get "mug with logo." If we only have the first scenario, we wouldn't have any compositional object; we would just be recognizing two different objects. It would already be interesting to see that you have different learning modules recognizing different objects when they're looking at the same part of the world.

The assumption is that if you do further learning, you would get "mug with logo."

Intuitively, I would have said start with the second one: never learn just the mug model, just learn the logo, and then see the mug with the logo on it. In addition to storing the raw color values, we store the logo ID and learn a compositional model. Then we don't run into issues of how the two pay attention to different things in different areas and space. We just look at modeling a compositional object, versus the first one, which is not modeling a compositional object. It's more about these two learning modules paying attention to different parts of the world.

If we learn "mug with logo" without ever having learned "mug," what's the actual benefit of the logo? Are we really learning a compositional object, or are we just learning the whole thing as an object, with the logo being associated at each step? This could just be random information we're passing in. But when we recognize "mug with logo," we can make accurate predictions about the logo itself with feedback connections, and those are the benefits of modeling compositional objects. The logo could exist on different objects as well. If you see the logo, it informs which type of object the higher-level column is seeing. It's not just colors; the logo is only on certain objects in certain locations. Storing that object ID in the lower column helps a lot with inference and predicting in the lower column what logo features to sense and where.

The second scenario is the whole modeling compositional objects idea. I'm not saying we want to stop here. Ultimately, we want the kind of confide tissue. The question is, do we need to do the first one to get to the second one? Do we need to figure out how to pay attention to different parts first before we can model compositional objects? Off the top of my head, I feel like we don't necessarily need to. If we only do the first one, we're not modeling compositional objects yet; we're really looking more into attention.

I'm not sure—maybe there are arguments why we can't do number two without doing one first. Generally, when I've thought of compositional objects, you would have a mug to begin with, and then you would learn "mug with logo" because you could have different logos. If we jumped straight into this, we'd have "mug with TVP logo," and that would be a totally different object from "mug with Menta logo." It might be a useful stepping stone, but it feels different. I think it's a different object because in your cupboard you have a cup with a Menta logo and a cup with a TVP logo, and they are separate objects. It's not like that cup ever transforms into the other cup. In my mind, I have both cups in my kitchen, and they are two separate objects. They share some morphological similarities, but I'm not sure if they need to be learned or represented in the model itself, or if we could just have a general morphology model of that big coffee mug type.

A different learning module could just know generic mugs. For scenario two, are you also learning the mug without the logo? At the lower level, I would argue we don't need to, at least not to investigate modeling compositional objects. But then, what does the sensorimotor output? What object ID do we store when the sensorimotor is not on the logo?

It could be "mug," and then "mug with logo" could be associated with points on the logo. If the lower-level column loads the mug, when it's not on the logo, it should output just "cup" or "mug." That one could have a model of a mug too.

If you're learning this TVP mug, the lower level is going to see the whole object during learning. It could be that it's constrained and the size of objects it can represent—the lower level in the hierarchy might only be able to represent objects up to a certain size. It would maybe never learn a large object like the cup, but only small parts on it, like the logo. But I don't see an issue with it being able to learn the mug either.

In some ways, that would be better for evaluating this because we could see that we're binding a different object to different locations. The question is, how does the brain do that? If it's only seen mugs with logos, how does it extract features that are only about the mug without any logos, so that it could be in the lower level and used with other logos? We can set it up so that the lower-level column first just learns the logo in isolation. The first time you saw the Menta logo was probably not on a mug—it was probably on the website or something, in a pretty isolated way. You could argue that you can learn it in isolation first and then recognize it projected onto different types of objects.

It's rare to see it in isolation, but with the cup or mug, you only see it with logos. Is that true in the real world? There are normal mugs all the time. If you go to our Thousand Brains website, it's pretty isolated up here. There's a lot of white space around it, and you can clearly see that something else starts down here. We would probably need some mechanisms to do this kind of model-free segmentation and try to stay in that area while learning it. The second approach would be a statistical approach: seeing a consistent pattern on different backgrounds, so the backgrounds get averaged out over time, while the consistent part remains in the model. That's probably a mixture of both approaches, and that's something we can definitely work with. It's a more general kind of unsupervised learning, which we definitely want to address.

The aim is to see whether things like location-by-location binding in the hierarchy—this proposal—give us what we believe they would in terms of being flexible about these compositional relations and predicting the appropriate thing at the low level. It's fine if they learned it in a semi-supervised way where we showed, "Okay, this is one object." To your point, Viviane, we can already evaluate that kind of thing by just having a unique ID at the top—even "TVP mug with bent logo" is a totally different ID. I would argue it's reasonable to say those are different objects, at least at first. Unless you see it bend, I would say we classify those as different objects. If you see the logo rotate on the cup, those are different states of the same object, but if you just see two cups with a bent logo and a non-bent logo, I feel like we would say those are two different cups.

I don't know if this is helpful, or if it's a tangent. If it is, feel free to ignore it. I think just hearing "supervised" might be a source of confusion when trying to figure out what an object is. It might be more approachable to do what Niels is saying: supervised learning, where we say, "This is an object, this is not, this is an object." We are just testing compositionality here in a supervised manner. To Ram's point, whether we're learning cups with logos or not, that's all very fuzzy, and I don't think we know how we learn what an object is. Throughout this conversation, I've realized we don't actually have a definition of what an object is. Still, we can experiment with compositionality by saying, "This is an object, this is an object." How does the compositionality work, without getting caught in the weeds of what is an object? For example, I know what a tree is, but every tree is different. So, what is an object there? We don't need to get into that to make progress. Let's not get into that, because we will confuse ourselves further. That's probably a different problem to solve—what is an object. It is a different problem.

The point I was trying to make is, if we give Monty just a mug with one logo and tell it to learn it, it'll probably just learn this as one object. It won't need compositionality until it sees that same mug with a different logo. That's when it decides, "Now I need a more compact representation. I need to extract the cup from the mug, from the logo, so I can use them on different things."

The same goes for the logo. If it only sees the logo on that same cup, it's the same object. But once it starts to see the logo in different places, it needs to factorize that, to make a more efficient representation of the logo that can be used elsewhere. It might even start the other way around: you might first learn a bunch of different objects, like learning letters in preschool. You learn those in isolation, on paper, and then you recognize those letters in different places, like on a cup. It could be similar with other things—you learn generic models of cups, curves, letters, and then you learn the composition models. It's very difficult to factor out totally new features you've never seen in isolation before.

This is a special supervised case, where you have a curriculum learning approach: focus on the little things, then start building. Both are approaches to learning objects. But we also can't ignore learning where we just see the object and later decide it's breakable into different parts, like when we see a stapler at first and then realize it's two different parts—not just because of motion, but maybe because we saw that part of the stapler on something else, or that handle on something else.

We still need to solve the problem, but as Tristan just said, we don't need to solve all the problems at once. We could focus on how to represent compositional objects and how to infer them based on those composition models. We can give ourselves a few crutches around learning, like supervising a lot and showing things in isolation. From there, we can move on to how to do this with less supervision and remove some of those assumptions during learning.

I totally agree. To validate the problem of compositionality, both learning models need to provide supervision of what is an object at which level, and start to make those binding associations. It's definitely an important problem to get back to at some point.

I think that was already a useful discussion. Right now, we can think more about it, but as you're suggesting, Viviane, we start by learning the logo and the mug in isolation.

Then we would learn "TVP mug," for example, as a new object, and we can provide a supervised label, at least to begin with. Then we can look at things like associating locations on the logo with locations on the TVP mug.

Let me just write this down before I forget.

That was the opening figure, because the question was: assume we have access to the semantic sensorimotor information. What would we actually be looking for at each level? At the high level, it doesn't really matter too much, because no matter what dataset we're using, if we're presenting the mug with logo, the sensorimotor modules are never pointing into the void. They move back onto the object when they go off into nowhere. We're always going to be looking at the mug with logo. As long as we get that representation to emerge—like "mug with TVP logo" and not "bowl with Mento logo"—then this one's correct.

I'm just thinking about this "mug with logo" thing instead of just "mug." I know this is an edge case, but there are stickers and magnets. You can put a logo on something, peel it off, and put a different logo. I don't want to overcomplicate things in the beginning, but in the general case, think about a dinner table. If I take a fork off the table, it's not an entirely new setting of the table and the arrangement. It seems like "mug with logo" might be overly constraining—too specific for the exact setting we're trying to compose. My TVP mug is different from my Numenta mug. They're distinct items.

I think if you're freely combining objects with each other ad hoc, you wouldn't learn both the forward and backward connections. If the fork can be anywhere on the table and it's still considered the dinner table, there's no way to learn, "When I'm at this location on the table, I expect to be at this location on the fork." You certainly couldn't learn the backward connection if there's constancy, and with the forward connection, there might be some tolerance for features being at slightly different locations, but at that point, you're just recognizing the table and forks as individual items, not as a specific set dinner table model that allows you to make predictions about where things are on the table.

Does that mean each dinner table would be its own object? If I take the fork off or move it around, do we have an entirely new object? I guess it depends on how much you've attended to the fork and how important that is. It would likely be a quickly learned model in the hippocampus or something similar, which you can use to make short-term predictions, like "I just saw the fork over there." That would have its own kind of ID. It's not that you would say it's a different table now—the stable table model you learn stays the same. But for the scene representation you just built up, it's not some generic dinner table ID; it's "this is my dinner table right now" ID.

One way to describe it is that in region two, you would have many columns representing "mug," and also some learning this specific instance of a mug. Both can help inform how to act in the world. If someone asks you to pass the TBP mug, you know which one to pass. If someone asks if you have something to pour coffee into, any mug you see is valid. One important thing to keep in mind is that there isn't necessarily one output. All the learning modules can output different things. I might be recognizing generic mugs, modeling the logos, or modeling the mug with the logo, and all of those are correct and valid interpretations of reality that can be active at the same time. There doesn't need to be just one overarching classification.

It seems we don't need composition or hierarchy to do "mug with logo." We can just do "mug with logo" with a single LM, which is what we have now. The point of learning a composition model is that it gives you a lot of predictive power and helps with inference in the future. If there is a constant relationship with the logo on a specific mug—like I have the Numenta mug here and know it has the logo on the front and back—even if I'm just seeing the front, I can predict what I'll see if I turn the mug around because I've learned that model of this specific cup. If I see this for the first time, I would still recognize the Numenta logo and that it's a coffee cup, but I wouldn't be able to predict what's on the other side. Does that make sense?

I think Scott's point is that if we learn the entire object with the high-level one, unless we constrain the high-level one in some way—like it gets coarser sensory input or can't access certain details—there needs to be something distinguishing the logo model in the low-level column from the logo submodel in the "mug with logo" model. If those are equally detailed, it's hard to differentiate. They wouldn't be equally detailed; the higher-level model would get a larger, lower-resolution receptive field. That's an important distinction we need to ensure; otherwise, there won't be a clear difference. If we constrain them by size or resolution, what happens when we want to zoom in on a logo and look at subcomponents like letters? Do we need to bring the whole thing closer so the top-level one has a good enough view of the logo?

We have to deal with nesting beyond two levels eventually. I'm concerned that by constraining each by a certain size or resolution, we might lose the ability to nest further, down to the letter or even subletter level composition.

I may be missing something, but if you need lower-level details, you would use the model stored in the lower-level LM. That's the detailed model, and when you store the object ID in the higher-level column, you could use that to figure out the morphology model in the lower-level LM. But what if the logo itself is compositional? A logo is composed of letters. The idea is that you would shift what R2 and R1 are representing, and this is why it's important that both learn multiple objects. At that moment, when you're focusing on the logo, R2 would represent "TVP logo" and R1 would represent letters or the "TVP neuron" thing. The way the attention shift works is something we haven't fully figured out. At least, I don't know how to implement that, but it's a good point. If we're looking at the logo in isolation, R2 would be learning the entire logo while R1 would be learning the letters. But then R1 is also going to represent the entire logo at some point when R2 is looking at "mug with logo." During learning, they would potentially all be learning the logo, but each with their own kind of model.

If that's the level you're attending to, at imprints, we need a way to shift the attention. It seems that at the low level, you only get the child representation. When we shift the attention, do you mean the entire learned model is moving down the hierarchy? At some point, no, because it would be at both levels—it's just constraining. The example Jeff gave was the mustard bottle with the French logo. Initially, when you look at the mustard bottle, both R1 and R2 say, "It's a mustard bottle." When you narrow in and look at the logo, R1's input is constrained, getting a more narrow input and representing the logo, while R2 still gets a broad input and keeps saying "mug" or "mustard bottle."

Does that mean that theoretically, during learning, R1 would learn TP logo and mug with logo? Yes, that's what that would mean. Every lower-level thing is learning multiple levels of composition or versions of some object, while the higher ones, if constrained to size or resolution, learn fewer. It would also imply that we need some amount of attention during learning. This is for unsupervised learning, not necessarily the basic supervised setting. If we have attention during learning, we can say, "I'm learning the logo, you're learning the letters," or "I'm learning the mug with logo, you're learning the logo."

Or not even learning, just representing. It's almost as if learning, maybe through attention, only happens at one level—building new models—and can rely on inference happening in other learning modules. If you're learning "mug with logo" at a particular level and already know the logo, the previous level isn't learning anything; it's just representing what it already knows, like logo and mug.

At least it feels like that might help with some of that.

If the lower model already knows the logo, it would just recognize the logo and not start learning "logo with mug" and feed that as input to the higher-level model.

Maybe another interesting example to think about, and this is actually a dataset we could potentially work with, is the Omni Cloud dataset. For those who don't know, it's a dataset of alphabetical characters from many languages around the world, including made-up languages like the Alien Alphabet and Futurama.

Each letter has been drawn by, I think, 20 different people.

The idea is that you can test generalization, but it's also a challenging dataset because it's very small—there's very little training data. It would be interesting to look at because Viviane had previously set up a data loader so we can actually evaluate with it. Traditional deep learning methods struggle with it because of the amount of data.

I think the Amazon Turks who were writing the letters must have been using a mouse pad or something. If you look at the Latin characters—this is a G and a K—even as native Latin alphabet users, those are funky. For this dataset, if we want to do it in a supervised way, do we have the strokes? Do we have the low-level strokes? Exactly. The dataset includes the individual strokes, as well as timing information about them. You could imagine that if we work with it and it works well, this could also come into behaviors and motor output, and we could even get Monty to learn to draw these.

You do have that breakdown. We actually evaluated Monty on it before, and the current status is that Monty can learn and recognize the same version of each character quite well. But as soon as you show a new version of the character, it doesn't do well. The global shape is similar, but the local shape is very different. This showed the need for hierarchy, where you represent strokes individually and then the arrangement of strokes, instead of the arrangement of all the pixels. The lower-level stroke could be a little rotated, but I don't know if we'll be able to do different scales with it. What if the lower-level object is at a smaller or larger scale than what we're used to, or what the higher-level model has stored? Do you mean if we're deliberately changing the scale, or just natural variation? Just natural variation, like we see here. Also, are these strokes low-level enough?

They may not be. There could be a circle and then a little thing. I don't know if they're the most primitive, low-level ones we could use.

That's something to find out. I think the scale and the distortion from the way people have written it are as much a challenge as the variation in scale, or it's a similar magnitude. It's not perfect. I think they basically showed the ground truth characters to people and then asked them, regardless of where they were from, to draw them.

Not everyone uses the same stroke patterns, so all of these won't be recorded in the same way.

The order matters for Monty—the order in which they were drawn—but there's variation within each stroke. For example, here, someone has used three different strokes for this thing.

This comes back to supervision and such, but this would probably be different low-level object IDs from this stroke. That could lead to some issues. We might want to find the most canonical version of each letter and then try on that, or draw it ourselves.

At a conceptual level, when you think about this dataset, it fits with the "mug with logo" view. For a particular letter, you don't have that letter independent of the binding of the child characters. It's clearly a compositional object, but it doesn't exist independently of the binding of the child characters, if that makes sense.

It's not like we have a generic Korean letter that then gets assigned five different strokes to become Korean letter two. It was always learned this way from the start. Maybe, to your point, Viviane, later on you develop a certain degree of a generic representation, but I thought this was an interesting example for this discussion and as a potential dataset to focus on first.

I think it's a nice dataset because it's very simple, two-dimensional, and already has a defined composition. It's a relatively well-known benchmark that, as far as I know, hasn't really been solved yet. It would be great if we could show some results on it. I was thinking in terms of the semantic sensorimotor aspect: if you add objects to the environment one by one, you can assign unique semantic sensorimotor IDs to each, and it will track them. We don't need that for GL because we're not putting them into Habitat; it's a separate data loader, and the loader knows which stroke it's on. I was wondering if we could read out which stroke is being sensed, or pass labels for the different strokes. That could be interesting to see what the low-level learning module is seeing. Could we read out the ground truth stroke currently being sensed? If the order is different between versions of letters, the strokes will be different, so you get the color of it.

That's fine—it's not a make-or-break issue, just not an advantage for Omni Cloud.

With the "logos on mugs" scenario, we don't have much variation in mugs with slightly different logo placements. This is a different task. We could start with just the first character or version of each character, or hand-pick ones where people followed the same order. We don't have to start with the generalization task; we can start with inference on the one we learned on, or one or two other variations.

For the mug with the logo, we can control the scale of the low-level object so the lower-level LM always recognizes it at the same scale. In the examples Niels showed, we have different scales of lower-level objects—some strokes are small, some are large. I don't know if the lower-level LM will recognize them at different scales, since Monty doesn't handle scale well at the moment. Or are we just going to handpick ones with similar scales?

If you show the examples again, Niels—

As a first example, like Viviane was saying, I would be tempted to start with three, say 1, 2, 3, and we can even change the order if needed. We can hand-label and say, "This is the upright stroke" as a first example. I would start with just one, doing learning and inference on the same one, and see how the compositional object is being learned and represented.

Are we talking about learning strokes in isolation and then learning letters? That would be the direct analogy to the mug example. We need to be able to display the strokes individually as well.

It seems like this is a really small dataset, and there's nothing stopping us from making our own if we want. We could make something similar ourselves, but people know about the Omni Cloud dataset, and it already exists and is integrated into Monty, so it would be simpler. For early prototyping, if we want to go super simple, we could make a few strokes and a few letters. There aren't that many strokes in English either. For example, the letter eight has two small circles—those are strokes. The letter seven has a vertical line. Whether it's GL or not, we could have a lot of control over how we want to do it. That's interesting. Why do we need to do handwritten recognition? Why not do compositional learning on actual typed languages, like letters from the English alphabet? We still want to learn the strokes in isolation first. We can figure out all the strokes in the English alphabet—vertical lines, circles, and so on.

I'm just worried because, in the examples you're showing, the bottom right and top left have different scales at the lower-level strokes. If scale is going to be an issue, it's easier with typed letters, but how would you get variation into them?

Why do we need variation if we're trying to learn compositionally? That's why I'm saying let's just take the first character of each GLT character, like the first version of each Omni character. It's the same as just taking the typed letter. Just forget about all the versions—take one version, learn on that version, do inference on that version. It's the same as taking a typed character.

I was thinking that strokes can be directly reused in the typed versions. For example, if we have a little circle as a stroke, and we're using a typed version, that circle could be reused at the same scale in different letters or digits. If it doesn't matter that strokes get reused, that's just an assumption I hadn't voiced. If we want strokes to not be completely diagnostic of the high-level object, that's part of compositionality: we can borrow common elements, and no single piece is necessarily fully identifying of the whole. That's the nice thing about the cups and logos example, where the same logo appears on different objects.

I think we can do a lot of different things. The appealing aspect of the OmniGlot dataset is its simplicity and integration—it has all the factors. We can just look at the first character, forget about generalization, and use it to test the basic plumbing of learning compositional objects. From there, we can move on to more complex things, like logos on cups or typed characters with straight strokes reused in different objects, or different versions of characters in the oligo dataset.

Once we can handle compositional objects, we can really test a wide range of combinations and capabilities. For now, I'm thinking we should find the minimal viable test bed to see what Monty learns as a compositional object. I think we're all eager to get a compositional evaluation working during the hackathon. Another main contender is the dataset Scott developed, which is closer to what we often describe and may be a better test bed for things like a logo with a bend or a rotated logo. However, it presents challenges in evaluation and plumbing. We don't have access to the semantic sensorimotor for the logo, so we don't know which logo is present. That's not a critical issue, because if we're presenting a mug with the Menta logo, we know what high-level object we're expecting, and we know the Thousand Brains Project logo is somewhere in the visualization. We can check if the lower-level learning module represents the TVP logo, whether or not it always matches exactly. There are some unsupervised metrics discussed later that we could use to get a sense of performance.

From a supervision and semantic sensorimotor perspective, I'm not too concerned about this. My biggest concern is the 2D nature of the logo and the challenges that presents. If we learn this logo in 2D, we don't have a good way of representing 2D objects right now—we don't extract edges or similar features. Essentially, we'd have a collection of colors at locations in a 2D plane as the model. When inferring and the logo is wrapped around something, it could predict colors at particular locations following movement, but it wouldn't use morphological features. That's what the 2D sensorimotor module would be for, so we would need that.

In terms of an action plan for the hackathon, that puts me in favor of OmniGLT for now, but this is something we should revisit. It could be a way to work up to more complex cases. We could test the basic composition modeling in Monty on the first version of each character in OmniGlot, implement the 2D sensorimotor module, consider attention mechanisms, and then test on this dataset afterward. If we make different arrangements of 3D objects in a scene, doesn't that solve the problem? We have semantic sensorimotor, and we already have models of the objects, so that's fine. Is that what you mean? Sorry, that's a great question.

A potential dataset, which I've provisionally called Inception YCB, would embed certain YCB objects inside others so they're partially visible, like you're suggesting, Rami. This was meant to be a dice—I don't know what happened, why that got deleted.

So, a dice just turned into a strawberry?

I had the dice, then the strawberry, but I wasn't happy with the rotation of the strawberry, so I thought I deleted this and put in a new one, but I must have deleted the other one.

Exactly as you say, Rami. If we construct them in Habitat, not in Blender or something else, but by adding the objects in series, we would have the semantic sensorimotor information and morphological features, since everything is still 3D.

Another option is the dataset Ramy put together and that we set up at last year's hackathon—the dinner set. The main issue with that one is that we'd want Scott's CIC policy working well, because those objects are separated in space. To stick co onto them wouldn't work; if we're on the bowl, how do we get onto the mug? Here, we would smoothly move over both objects.

Before we have the Cade policy? Yes, before we have the Cade policy. Oh, I thought you meant the Cade policy wouldn't work with it. No, we would want the Cade policy in order for it to work. I understand now. In theory, you could crawl over the table, but it might go everywhere. If we're talking about hackathons, the current inhibition of return policy is more likely to zip between objects, alternating observations, rather than staying on one for a while and then moving to the other. In the short term, we could supervise the policy so it does a few steps on one object and then moves to the other, since we're doing supervision anyway for what is lower level and what is higher level.

I wouldn't use the dinner set because we wouldn't have the semantic sensorimotor. But if we have habitat, or just three objects in a triangle or some arrangement, and then change them, we would have different arrangements. We could control the scale and move between the objects to build the compositional object.

The main thing is to have a clear concept of child and parent, so it's not just two objects next to each other. It could be a big triangle and a small circle or sphere.

The main challenge is figuring out the positions and locations to initialize the objects to look like this. Maybe we could import the models into something like Blender based on their dimensions, work out the relative assets, and then set it up in our configs.

This one was my least favorite, but if we go with it, can we use the multi-object setup we already have and say the composition object is how the multiple objects are arranged in the scene, and the child objects are the individual YCB objects?

If we use the distant agent, it could probably move between them. The surface agent is stuck on an object once it's on it, like a continuous surface. I think we currently use the distant agent. Did it move between objects? We tried to implement a policy to prevent that at one point. We would just have to adjust it slightly. Right now, it adds objects randomly, so we would need to change that. We could define set A as compositional object A and set B as compositional object B.

In our definition, there's nothing saying the void cannot be like that. It doesn't have to be continuous; the objects can be spread out. It's just that, practically, the policy right now, every time it goes into the void, it goes back. If the objects aren't continuous, either through occlusion in visual space or their surfaces touching, the policy won't go from one object to another.

Is there a problem with supervising the policy, like after a terminal condition on one object, manually moving the agent onto another object? I feel like splitting the policy is always surprisingly difficult to implement, but I could be wrong.

I don't think it's an issue in principle. We're introducing a lot of supervision here deliberately.

This is the main thing to talk about and for us to think about. Maybe we can decide over the next couple of days or on Monday which dataset to work with. My preference right now would probably be OmniClot.

Did you have more on the supervision and evaluation part? We already talked about most of it. On evaluation, nothing major, but just to show more concretely why it would be useful to have the semantic sensorimotor information: we can have plots like this for both the low-level and high-level learning modules. For the high-level module, during a given episode, it will be the same composition object being shown. It would be interesting to see the low-level module recognize a stroke, and if it rotates that stroke, it makes sense. Just things like that.

That's why it would be helpful to have that information.

A few ways to get around that: one is the feature Hojae added, where we can scrub through. This is probably good enough for the hackathon, since there will only be a few episodes. We can scrub through and see what the sensorimotor modules are seeing and manually confirm that it's on a stroke, and that's clearly what's being observed by the sensorimotor module.

If that makes sense. I just had some notes on the longer term. I think this will become more important in terms of other metrics for performance. Looking at prediction error is generally useful, particularly as we move toward having the system discover composition without us specifying it. The semantic sensorimotor isn't just a matter of practicality; eventually, we don't want to rely on it. If the representations are good, the prediction of what the learning module will see and what it actually senses should match well, resulting in low prediction error at each step.

This example shows that at the start, the most likely hypothesis may be wrong and the prediction error is high, but eventually, a good hypothesis emerges and the prediction error becomes very low. When we move on to a logo, because we lack a compositional representation, the error spikes. If we know it's the mug with the logo, we would expect the error to be lower. To your point earlier, Scott, if we have a high-level learning module that learns "mug with logo" as a single object with a coarse representation, it might do an okay job, but the error would still be high. With a true compositional representation, the high-level learning module can work with the low-level module, and the low-level module's prediction error might be much lower. The high-level module could tell the low-level one, "We're moving onto the logo, get ready to make predictions at your level of resolution."

I think that's a great idea. We should try to add prediction error as one of the measures. It would be a useful metric for symmetry and similar aspects, to know the prediction error of the most likely hypothesis at any step. Even if the pose is off, if the metric is low, that's what matters in any application—whether you can predict what you'll be sensing. If that's low, it doesn't really matter what the semantic sensorimotor is saying or how we identified the objects. I don't think this is just a long-term goal; it would be great to add during the hackathon.

Generally, we will want to manually inspect what's going on, but it would be nice if, at the end of the week, we have a benchmark with measures we can rerun as a test bed to see how those measures are affected by any changes we make in the future. That's what's nice about it—this would be useful information, broken down across the episode, and the mean prediction error would also be an interpretable summary statistic. As you said, that's something we could have in a table to see how it changes as we introduce new architectures and so on. This is just for clarity, showing an example where predictions are particularly bad.

That was most of it. The last figure I had was about this attention-type thing. If we can talk briefly a bit more about the evaluation measures: one difficulty in evaluating the representations of the lower-level learning module is that it changes throughout the experiment. It's not like the high-level one, which reaches a terminal condition and then we check if it classified correctly. We need to think about when to measure it, how much time to give it after moving onto a different subpart, and how confident it needs to be. That will be one of the major challenges with modeling composition models. Right now, we have the same confidence threshold for the lower-level model to send something to the higher-level model as we do for any model to converge and terminate the episode. We'll have to think about the terminal condition and how to adjust that, because currently, in a hierarchical experiment, the first time we're sending an observation is when the experiment ends.

We have to rethink that and also consider at what points we check the performance of the lower-level learning module and how to ensure the high-level model gets enough input.

Given that it takes some time for the child object to be recognized, after it's recognized, you still have to keep moving on it or integrate that past information into the model. I think those are some big questions that would be good to figure out, especially the first and the last one. The second one, for example, with the prediction error—if we just measure it across the episode, it should always get lower if it's doing a better job, even if it spikes for a bit when we go into a new object. If the hierarchy is working right and the high-level module tells the lower-level one, "I think you should be on this object, ramp up your evidence for it," then we should see that metric come down. We can also examine the number of steps before it drops.

For the lower level, with compositional objects, we won't be looking at ground truth anymore, but just prediction error, at least in that case. If we do have access to ground truth, we could look at what proportion of steps the label in the low-level module matches what's actually being observed. Seventy-five percent would be pretty good, whereas ten percent would be pretty bad. Those are measuring the same thing, but one is a supervised and one is an unsupervised measure. That metric can be hacked by adjusting the policy to just stay on the child object after it's been recognized. That's true. This is similar to when people discuss the free energy principle and ask, "Wouldn't you just go into a dark room and close your eyes because then you can perfectly predict everything?" The response is, "That's what people tend to do for eight hours a day, but before they start predicting hungry." Anyway, that's a tangent.

Just a quick question: why do we need to evaluate the low-level elements if we're just getting the high-level correct? I think we want to understand how the system is working and that it's working as we think it is. If we only look at the high level, we might convince ourselves everything's working, but it could be doing something strange. It's a fair question; we might not need to look at the ground truth comparison to the low level, and it probably won't be realistic in more complex scenarios soon. It might be that we use prediction error at every level and only classification error at the highest level. In the long run, we might just look at how well the system is interacting with the world, like the motor outputs. But as a first step, as Neil said, it would be good for us to see what's going on.

It would be nice to be able to inspect and figure out that it's doing the right thing. But for a benchmark, does the performance at the lower-level LMS go into the benchmark? Is the total number of accuracy that we need to report for the compositional case? I could think of examples where error would matter, because if we want to know what we're seeing when we go to the logo, we can't rely on the high-level one. Otherwise, we're just trying to learn a giant supermodel of everything at one level. If we want to see compositionality doing something useful and interesting, it doesn't necessarily need to be supervised, but we do want to measure and report how well the low-level learning module is doing.

Another measure I would add for the lower-level model is how many of the steps actually send something to the higher-level model—basically, trying to get it to be confident enough most of the time to send information up. Maybe it's not something to be concerned about, but that's my main concern at the moment: that it will take too long before it gets confident enough to send something up the hierarchy. That's something we might want to try and improve in the next months.

We can see how the hackathon goes, but if we focus on adding the prediction error metric, we could potentially try both Omniglot and Scott's logo object dataset. There's still the 2D stuff. We could have two teams. One pitch could be to implement the 2D sensorimotor module, and then at the end of the week, it's evaluated on the logos or marks.

I don't think this is a topic that a huge team can work on, because it's hard to break down.

We can also break into teams and do the same challenge of compositionality and see who solves the brain first—who gets the lowest prediction error. Someone will put their Monty in a dark room and win.

The last thing I wanted to touch on was this attention-type question. From the discussion, it seems we agree that any learning module in the hierarchy, at least to a degree, can in theory have a model for a particular object. For example, we'd have TVP logo here, mug here, mug with logo here, and potentially mug with logo here as well.

This raises questions about how we learn that and how we get different representations, given that at any point in time the sensory input could be consistent with multiple of those. If you're looking at surface normals and curvature, maybe you'd recognize it's part of a mug. If you're looking at the edges, maybe you'd recognize it's a logo.

I don't think this is something we need to solve now, but it's probably a multifaceted issue. Based on previous discussions, different sensorimotor modules would focus on detailed versus coarse information, and that would be reflected in what the learning modules learn. We've also discussed how some learning modules might receive input more consistent with 2D representations, including both features and movement information. Jeff also mentioned how we could window attention, which ties into policies that affect which learning module receives a narrow input and focuses on the child object.

I just thought it was worth being aware that this remains an open question and something for us to consider.

That's basically everything I had for today. It's not just that the input is consistent with both objects, but also that if the lower-level model recognizes the logo and then moves onto the cup, there will be inconsistencies with the logo model, and we're not currently getting that input. For example, when we're not on the logo anymore, it's not like we're in a void as with the YCB objects—if we move off them, we just don't send input to that learning module anymore. Even inference on the child object is difficult because it will receive all the surrounding inputs that are not on the object. If we move off the logo and onto the mug body, but narrow the attentional window, that changes things.

If we don't have an attentional window mechanism, wouldn't it just be, "Now I'm on the mug"?

Especially if the system knows the mug and the hierarchical composition object has learned mug at locations, it could help. It shouldn't keep thinking there's a logo; it should just recognize that it's not. That's the out-of-reference-frame movement issue, which is why it might be something to revisit later. It feels like we have enough of an idea to start on something next week. I'll try to clean this up a bit to share, but I don't think there's any secret knowledge here that hasn't already been considered or that we won't work through next week.

Did I miss what we're doing next week? Are we each going to present an idea, or is it just looking through the spreadsheet and discussing the ideas there?

The idea is that everyone picks a project idea from the spreadsheet—ideally not more than one per person, but if you have two great ideas, that's okay. Try to figure out which one would be best to pitch. Then do a short pitch of that idea, about two to three minutes, focusing on what it would be about, why it's important, the potential impact, and positive outcomes. 

After all the pitches, we'll see who is interested in which project and try to form groups. Some projects are very involved; for example, if I work on the stacked one, it might require two or three people full-time. Some projects may not be feasible to do alongside others. Is it a rule that everyone should have one main project? You should have one main project, but you can help out on another if people want your opinion, though you shouldn't be doing major work on multiple projects.

With the pitched ideas, try to consider whether it's a good project for a two- to three-person team to make the most of teamwork.

That's everything for