Viviane Clay: As always, I'll start with a shout-out to the community. Today, I rearranged the slides because we're getting more substantial contributions, so I wanted to highlight those first. There have been quite a few community PRs this month. Here are several PRs from Mason, mostly centered around adding Ruff, a style formatting checker, and adding rules for that into the code that we were previously ignoring, making sure the code adheres to those.

There has also been nice work from Agent Rev. We shared the final presentations from the Brighton Retreat, and one of those was the attempted Python upgrade and separation of the simulator. Agent Rev took an interest in that and inquired about the repositories that Tristan and Willa put together. The next step would be to integrate this simplified simulator protocol into Monty, so he opened an issue on GitHub to get more information. Tristan outlined a long series of steps needed to do that and mentioned it's a lot of work, so no one should feel obligated to do it all. Agent Rev said, "I got this, I've done crazier back in the day," and has started working on the list, already merging two PRs for refactors, with two more open PRs.

Progress is being made on that topic, so thanks a lot. Since this is a bigger chunk of work, I also added an item on our future work table and put Agent Rev on there. Thanks a lot.

Anna, who's been on our future work table and mentioned in past months, is almost done with her item. I think on Friday, I approved her PR, so it's ready to be merged. This was the second big step in refactoring the data loader and dataset into an environment interface. She wrote out notes on what had been done, ran benchmarks to ensure performance stayed the same, and updated all the figures in our documentation, making sure the tutorials still run. She put a lot of effort into this, and I'm excited about it because it accomplishes something I've wanted for years: we initially used machine learning conventions like data loaders and datasets, but Monty doesn't work on datasets—it works in environments, by interacting in environments. We wrapped some code around that but still used the old naming. Now it's much clearer in the code how it's supposed to be: before we had the data loader and dataset, and now that's turned into the environment interface.

Monty now uses this to interface with any kind of environment. Thanks a lot, Anna, for making these changes and doing this refactor.

There are also a couple of cool projects I've come across this month. This is only what people are sharing on the forum, so if anyone else has cool projects, feel free to share too. Spencer mentioned he's working on integrating SDRs from HTM into Monty, and a couple of days later, shared a GitHub repository using SDRs encoded for column-based AI. I'm really interested to see where that work goes. Joseph posted some sample code and questions about making changes to voting and testing some multimodal voting. There's been a lot of back and forth, with Niels and Rami giving feedback on those ideas.

A big thank you to Agent Rev for answering people's questions on the forum. For example, someone had issues running a tutorial, and he quickly chimed in with the solution and linked to a recent video for another question about reproducing the LEGO robot setup. Thanks a lot. There has also been good input, feedback, and suggestions. Rich Morin suggested Mermaid support, which is a way to integrate diagrams into posts, and now that's supported on Discourse, thanks to Will. Someone else proposed people we could engage with and why this might be useful, and Yitouk recommended another podcast that Nomanta has been on before. Thanks for the suggestions. On the video about the Q3 and Q4 objectives, Agent Rev made suggestions about how to structure future work and how we might use some GitHub features. There are some really interesting discussions happening. For example, on the proposal to engage with some researchers, Agent Rev did a detailed breakdown of how their work is different from ours and how it relates. There was interesting discussion around voting and the cerebellum, with Humble Traveler, Agent Rev, and Rami adding thoughts. Another interesting thread.

There was also a thread about whether Monty implements biological mechanisms for strengthening somatic connections between neurons.

There were many other interesting posts—these are just some examples. There's one on a video about learning sequences and behaviors, some thoughts around melodies and how they could be modeled, and thoughts about pre-wiring connectivity between different pixels. There were a couple of first-time posts on Discourse, so welcome to the community. Hope to see more from you. People are also sharing the Thousand Brains Project on social media. Emma has been calling us out on several posts or replying to people's posts. Just today, there was another nice post on LinkedIn about why reference frames are so crucial and important, which I thought was a nice description.

Michael wrote something about the Thousand Sprints book and how it's different from large language models. Hadi invited Niels to present at the Deep Learning and reinforcement learning debate series, which was later shared and re-shared, so thanks for sharing that. Greg Robinson wrote another nice blog post about different new types of AI, and the Thousand Brains Project also features in there, along with Meta's JPA, Google's Genie, and other new approaches that are coming up.

Thank you to everyone for all the great and varied contributions. They are truly appreciated.