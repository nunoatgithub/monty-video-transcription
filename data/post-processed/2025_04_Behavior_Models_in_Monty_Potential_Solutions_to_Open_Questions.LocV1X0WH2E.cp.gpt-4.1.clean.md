Should I start, and then we go through the thoughts and questions?

All right. Do you see my screen? Okay, so first of all, what I want to show builds on a couple of assumptions and things we've discussed before. To state them explicitly, it assumes we have two types of models in each column or learning module. One is an object model, which models features at locations—that's what we currently have implemented in Monty. The other is the behavior model, which uses the same mechanism but models changes at locations in a sequence. Both model types have a unique reference frame for each model instance. Each object model has a unique reference frame, and each behavior model has a unique reference frame, and they're independent of each other. Both model types can be recognized at arbitrary location, orientation, and scale by applying transforms to the incoming movement vectors and pose-dependent features as well—basically rotating the incoming movement and features to transform them into that unique reference frame.

Both model IDs and changes of those IDs can be associated hierarchically on a location-by-location basis. The model ID and pose can become a feature at a location in the higher-level object model—the logo can become a feature on the cup. Model ID and pose changes can become a change at a location in a higher-level behavior model.

Can you just leave this screen up for a second while I read it? I don't want you to go on. Sure.

One of the things I'll come back to is that it wasn't clear to me that the behavior model has to have a feedforward projection. I think that's what your second bullet point at the bottom is saying, or maybe not—all ID and pose change, is that the behavioral model there?

At the higher level, this would be the behavior model. At the second level, we have the behavior model, and that one only stores changes if the model ID at the lower level changes. For example, if the logo suddenly changes from one logo to another, it's not necessarily behavior that's being passed up; it's just a change in what is represented in the low-level column. If a feature changes, those two bullet points are the same thing, right? They're basically saying you have a model ID pose, and that model ID pose can change. They're mirroring these two. This is the main rule I'm following: object models only store static features. I understand those two points. The ones at the bottom—you added some more words, and I'm confused by what you're saying there. At the bottom, I'm thinking of the higher-level learning module or column. The first bullet is basically the object model at the higher level, which stores features, which can now be child object IDs. The second bullet corresponds to the behavior model at the second level in the hierarchy. The changes it stores don't just have to be changes of low-level features; they could also be changes of the child object ID or pose.

So if the logo rotates or changes, are you just trying to say that anything can change in the model? As part of a behavior, the ID of the child could change, the orientation could change, the scale could change, the position could change. Is this saying anything more than that, or are you just spelling that out? I'm just saying that we can learn hierarchical models on a location-by-location basis, and which information goes into which higher-level model depends on whether it's changing or not.

Maybe it's obvious. I need to take that again. There were a lot of words in there, and it wasn't obvious that's what you're saying. It's the same as the first two bullets, or almost the same, right? Exactly. I just felt it would be useful to state it again, because at least in my mind, I was a bit confused about that in the beginning—trying to think about how we learn hierarchical behaviors, are the behaviors made up of other behaviors? But now it's clear to me that behaviors at higher levels are still just changes. They could be changes in the object ID, the object orientation, or changes in the behavior ID. It's not like a flow of behaviors hierarchically composed; even at a higher level, behavior models are only changes.

Maybe a clarifying point is that behavior is not passed up as a feature. Last week, that was one of the things you proposed—you saw the behavioral model being passed into layer four. It is passed, and I commented on that, but I concluded that didn't make sense last week. Are you saying you've also abandoned that? No, I would still say that the behavior ID is passed up as a feature, but it becomes a feature on the higher-level object model. Since it's a static feature, it's not something that's changing, and that's how we assign behaviors to different locations on an object.

We'll come back to it, but if a behavior can be a static ID, then it can also change. You could imagine the stapler goes from opening and closing to being on fire or something. Maybe I'll just go to the next slides, because I show examples of exactly those things. These are a lot of words, and that's the text on every slide. I think I understand this stuff pretty well, but the words are confusing to me. Maybe I'll just go to the concrete examples.

Here's the basic mechanism again, just showing it with the new way of presenting it.

Sensorimotor movement and features are transformed from the body's reference frame into the object's reference frame, then sent to the lower-level learning module or column. The movement shifts your position within the object's reference frame; for example, you might think you're in the middle of the logo, then after a movement, you expect to be somewhere else. The feature is either stored during learning or compared to what's already stored.

This enables inference. In Monty, during inference, multiple transforms are applied to test several hypotheses simultaneously, such as the object being in different orientations. Movements are oriented in different ways to see which matches the model. Once the object is recognized, only one transform is applied, and predictions continue in that model's reference frame.

The object model consists of features and locations; in Monty, these are locations in Euclidean space.

You've moved away from a literal column interpretation here, not showing the thalamus or layers of cells. I find this a bit jarring because I'm used to the previous images, but that's just my perspective. Others may find this approach helpful. I tried to focus on the conceptual mechanisms. The black dot could represent the thalamus or a relay cell. It might be interesting to hear from others less familiar with neuroscience to see if this is easier for them. For me, it's not easier, but I can work with it. If it helps others, we should continue.

I'm trying to make it more abstract to draw a comparison. We can switch between the two, but I try to keep the spatial arrangement the same, with object orientation in layer six. The column diagram is like a circuit diagram, showing information flow, which I find helpful. This version is more conceptual, with blobs relating to each other without real connections. If it helps others, we should keep doing it.

It helps describe what is, but lacks the constraints that inform feature discussions about what ought to be.

It's missing constraints for exploring alternatives, but with constraints absent, it's straightforward to explain what is. That's good input. I'm not saying we should always use this approach, but I wanted to simplify things to explain the mechanism.

One simplification I made is showing the object model as a single entity instead of associative connections between layers four and six. This is closer to how it is in Monty, where it's a point cloud of feature points at locations in 3D space or a graph. The blue box in the diagram would represent the associative connection between locations and features in the circuit diagrams.

If we stack two learning modules or columns hierarchically, with a lower-level and a higher-level one, we assume they are co-located in space, meaning both sensorimotor patches are at the same location. The higher-level one has a larger receptive field.

Both can receive raw sensory input and movement information, moving together through space. The higher-level column also receives input from the lower-level learning module, which is the pooled object ID. It doesn't get information about the object model, structural graph, or point cloud learned at the lower level—just the object ID. This object ID can become a feature on the higher-level object. For example, the logo exists at certain locations on the cup, and the logo is a feature at those locations. We also want to store the orientation of the logo relative to the cup, so we calculate and store the relative orientation at those locations. Each point in the cup model stores that a cup feature exists there, along with its relative orientation.

This is how Monty works today. Object ID and pose are the CMP output of a learning module and become the input for the next learning module. They are processed in the same way as the original input, so the object ID becomes a feature and pose in the higher-level learning module. This is the current state of Monty.

That really helps me. That was very good. Cool.

One thing that's not yet in Monty, but is on the roadmap for modeling compositional objects, is the feedback signal. If a column has recognized the TBP cup, it wants to tell another column to expect the logo at a certain orientation and location. Here, the constraints of neuroscience come in. From an engineering perspective, you might think of a different solution, but in the neocortex, the TBP cup learning module or column can send a feedback signal—a context signal—to the lower learning module. This signal can then be associated with the logo and its location. The lower module doesn't know the structure of the logo or where it is; it just knows it's detecting a cup. The unique location on the cup can be sent down, and the lower module can associate that signal with a specific location on the logo. Whenever it receives this specific signal from the higher-level module, it knows it is at this location on the logo. It's a coincidence that always happens: when it gets this input, it is here and sensing this. When the lower module hasn't recognized the logo yet, this feedback can bias it to recognize it.

This feedback is not for the cup itself, but for a specific location on the cup. In this case, the patch in the higher-level module would be on the logo area of the cup, and it tells the lower module to expect a logo in that region. The location of the feedback changes, and that's important. There's a location on the cup that biases a specific location on the logo. Since the receptive fields of both modules are co-located, they always appear together when the sensorimotor system moves over the cup, allowing the lower module to associate the signal with a specific location and orientation on the logo.

It's not just like voting, where sensing the cup always means sensing the logo. If you look at a different part of the cup, like the bottom or handle, it wouldn't predict the logo. Only when the sensorimotor system moves to the area where the logo is does the lower module expect to see the logo.

One thing we might implement to mirror the anatomy is columns that aren't exactly co-located but can still predict the presence of a logo more generally. This would be equivalent to the L1 connection, the spreading axon terminal point. In this context, you're predicting a specific location in a specific low-level module, but as a high-level column with a wide receptive field, you also expect that many lower columns should be seeing the same object.

The spread in layer one isn't strictly required; it's something we observe and can explain. If designing from scratch, you might not predict it, but it helps with noisy input, obscured signals, and voting. The key point is that as the sensorimotor system moves on the cup, it predicts different things in the lower-level module. The width of the spread in L1 influences which columns are affected and depends on how predictive the signal is for a particular object. If the logo is small, the spread is small; if the logo is bigger, more columns further away might connect to the signal. The physical spread is the same, but which synapses are formed depends on the context.

Are we associating a single position in the higher-level column with multiple positions in the lower-level column? No, it's a single position in each. One location on the cup is associated with one location on the logo, but this association can happen multiple times.

When an association is formed, it's between one location in each reference frame. The logo may exist at multiple locations on the cup, and each of those locations can have a backward connection to bias different locations on the logo, but each connection itself is one-to-one.

To bias multiple locations on the logo, you would need associative connections to multiple locations in the lower level, but we only want to bias one location in the logo. It's a one-to-one mapping: if we're at a particular location on the mug, we're at a particular location on the logo.

In terms of how many one-to-one connections we form between them, it depends on how complex the compositional relationship is. If it's something simple, like a small logo on a flat surface, a couple of connections might be sufficient to describe that relationship. But if it's a logo with a 90-degree bend, you need more associations to know, given that you are on the mug, where you are in the logo space, and also the orientation of the logo or child object. A follow-up question is: as we move with the sensorimotor in the lower-level reference frame, does the location in the higher-level reference frame change, or does it stay the same? It changes—the sensor is moving in both reference frames. The only complication, which we could try to avoid for now, is if the two columns have different inherent scales, like the size of the locations are bigger in one than the other, then it's not an exact one-to-one match. But if you assume they're the same, then as you move the sensorimotor, it moves in both the logo (the lower reference frame) and the upper reference frame the same way—they're co-located constantly.

Try to show that with the pink errors: they both get the same movement input because they're moving together, so if one changes location, the other will as well.

It might be useful to think about how, when you look at something like this cup, your eyes are constantly moving around and attending to different things. Every time you move, it makes a prediction about what should be there. You might think, "There should be the logo," and then, "Now there should be the word 'thousand.'" By focusing on "thousand," it predicts it should be an "A." You're constantly bouncing around between objects and their features, and it feels seamless, like you're just looking at this cup. In reality, you're constantly looking at the hierarchical arrangement between features and a large object. It just feels seamless—you say, "I just know there's a logo there," but you know because your eyes move to that location or you're anticipating moving to that location. Then, there's a logo. This process is fascinating in terms of your mental thoughts and what's actually going on as you move your sensors. It feels like one thing, but it's actually bouncing around all the time between these two different models.

Any other questions on this one?

Moving on to behaviors: it's basically the same image here, but the big change is that we get movement and changes instead of movement and static features as input. Whenever the sensor detects a change, that gets routed into the behavior's reference frame, and then we store changes at locations. There's also a temporal component, indicated by this being an animation—there are changes at different locations and at different points in a sequence. The fact that it's already a change—the difference computation—occurs pretty far upstream, at or near the level of the thalamus. There's no difference being calculated at the level of the cortex.

There's mixed evidence about where direction selectivity emerges. In some species, there's direction selectivity very early on, even in retinal ganglion cells. But also, layer 4B alphas either extract or get directionally selective input. It's generally quite early on, so I would put it in the thalamus as a general rule of thumb. Even earlier, if you go back to the idea that there are two basic types of outputs from the retina: the parvocellular and the magnocellular, and they have different response properties. One responds quickly and one responds slowly. The quick-responding ones detect change, and the slow ones only become active when something is not changing and becomes static. The center-on, center-surround receptor fields coming out of the retina are divided into zones that are best at detecting movement and zones best at detecting static patterns. It occurs there, and then in the cortex, it gets converted into, for example, an edge moving in a certain direction.

Some input is better suited for one versus the other, but the place where it gets interpreted as "this is moving in this direction" is when you combine a set of center-surround cells.

Hey, this is Jamie. He's about to go to school. Hey Jamie. Bye bye. Have fun at school. I do not remember going to school that young.

That makes sense to me.

What I was trying to think of is that a feature is a little higher level than just motion. At some point, a feature is more complicated than just motion. The way I view this is you start off with very simple center-surround fields—some detect changes, some detect static things. Any center-surround receptor field detects anything that's not uniform; that's what they respond to. This is sent to the cortex and thalamus. There's little evidence the thalamus does any transformation, so it gets to the cortex and is quickly converted into things like minicolumns that represent edges or moving edges. That would be an output of a spatial pooler. You start off with raw bits—changing bits and static bits—meaning there's some sort of change or non-uniformity occurring at this point. In the cortex, it figures out what to represent. It's not clear to me that directional sensitivity is determined by the cortex; it's not determined by the retina or the thalamus.

And then maybe, and I'm not sure if that's where you're going, Scott, with your question, but if you're thinking of extracting change at a higher level—like if the object ID changes or object orientation changes—that could use the same mechanism as detecting small temporal offsets where a pattern changes in the cortex. It's basically the same mechanism used with the on-off cells.

Jamie, are you going to stay with grandpa? No, he's saying goodbye. I'm Skipper, not grandpa. Oh, okay. Bye, Jamie.

Does that make sense, Scott? Yes, that makes sense. I have some thoughts or questions about the memory of things that receive magnocellular versus parvocellular input, but that's not really pertinent to this conversation, so I'll just hang on to it.

It's also something I want to read more about and discuss further, maybe at a future meeting. I have a backlog of papers on optic flow and movement detection that I've been meaning to read. It could be an interesting topic for a session. I also read a couple on that last week, so I should probably send a brief summary of what I learned.

Back to the general mechanism: we are extracting different movement information and storing these changes in the reference frame of the object behavior. In the same way we convert movement and features from the body's reference frame into the model's reference frame, we do this here to get into the behavior's reference frame by applying the behavior orientation to that input.

If we were to implement this in Monty, it would be analogous to what we do for object models. We'll be laying down points at locations—what we call graphs right now, but it's really more like point clouds since we don't use edges. Instead of storing static features like color at those points, we would store changes at those locations, and we would need another dimension for time, storing a graph of changes for each step in a temporal sequence.

Does that make sense?

Now, how does this way of thinking solve particular problems we've been discussing? I'm going into the new solutions I presented last week. The previous ones were included in the last slides, but one of them is that we want to associate objects and behaviors. If I see a stapler, I want to know it has a certain behavior. If I see a behavior, that might bias what object I think I'm perceiving. A proposal I made last week is that we can associate those by using hierarchy. In this example, a behavior model is at the lower level and an object model at the higher level. Here we have a hinge behavior in the stapler, and this hinge ID—the behavior ID—is one of the outputs of this column that becomes a feature in the object model. We would do this again on a location-by-location basis, so each of these circuit locations on the stapler could be associated with this particular behavior as a feature at that location, plus the orientation of the behavior—the relative orientation to the stapler—using the same mechanism as if this were a child object. It's just a child behavior, a feature at locations and relative orientations at those locations.

Does that make sense? What does this model give? The other thing we've discussed is the reverse, where the behavior model exists at the higher level and the morphological model is at the lower level. I proposed that as a solution for predicting morphology, but this way assigns behaviors to objects. Usually, the object is the larger thing, and the behavior applies to the whole object or parts of it. The behavior is never bigger than the object itself, so it makes sense that it is the child of the object model.

But in this instance, it's not clear to me that it needs to be across columns. Oh, because of the relative orientation calculation here. That makes it easier, and we want to do it on a patient basis. Is this an okay time to delve into more complex topics? I have an issue with this too, and I wrote up an alternate example suggesting that the behavior isn't in the lower column. Should I reserve that for later or dive into it now?

If you want to bring it up, go ahead. In the writeup I did last night, I gave an alternate example: imagine we had a cup with a logo on it, and the logo rotated. It's like before, but now the logo rotates. The behavior is that the logo on the cup rotates 30 degrees and then rotates back.

In this example, it's different from the stapler. In the stapler, parts are moving and parts are not. Here, the thing that's changing is just the logo. The logo itself isn't undergoing any behavior; it's just presented at different positions relative to the cup. So now the question is: where is the behavioral model of the logo rotating on the cup? Is it part of the logo or part of the cup?

If I were to say it's part of the logo, the logo could be rotating on its own, and I would just compensate for it using the mechanism we have. I wouldn't necessarily say that's behavior; I would just say the logo has a different orientation now, so there's no behavior associated with it. I think I'll have a slide on that—not with this specific example, but basically, since in that case the relative orientation is changing, this would become an input to the behavior model here.

The question is, would there be a behavioral model in the lower-level column? No. There's an example where there isn't a behavioral model, or at least not relying on one, in the low-level column. Then I asked myself, could that be the way to think about all these situations? If I think about the stapler—imagine the stapler is like the cup, except the top of the stapler is the logo and the bottom is the cup. Now I have a part that's a separate component moving relative to the other components. It's not self-contained on the cup like the logo is, but there's one part that's stable and one part that's moving. That's very analogous to the cup and the logo.

Then I asked, do I need a behavioral model in the lower-level learning module? I don't think I do. It's a bit of a different example because the logo on the cup is already a compositional object, so we already need composition to model that. Then you have the child object change relative to the parent. If I were to draw this, I would probably draw three levels to show it all. But I would still say at the lowest level, there is a behavior, which is just the local optic flow detected by the sensorimotor system. It's not related to the child object itself changing its orientation. Is that a learned behavior, or is that just a moving object? It's just an object that's moving. It's rotating, but is that a behavior? Is that going to be associated with something?

I would still say that's a general behavior model of how bits are changing if something is rotating. That could be, although I was working on the alternate hypothesis, and I don't know which is right. The alternate hypothesis is that the logo—the stapler example is confusing because we have the same object, with parts moving and parts not moving. The idea that the logo and the cup could rotate is a simpler one. I didn't feel like I needed to introduce any kind of behavioral model associated with the logo. It's just a feature on the cup, and its position is changing, but the cup could learn that. The way to think about the stapler is: imagine if, as soon as I see the top moving, I isolate the top.

Imagine I have a morphology model. The stapler is represented in both the lower level column and the higher level column. As soon as it starts moving, I focus only on the top of the stapler in the left column. It's like a logo—I'm attending only to the part that's moving and ignoring the rest. I immediately split this into two parts. On the upper column, I'm anchoring to the base of the stapler; on the left column, I'm anchoring to the top of the stapler. Now, it's just like the cup and the logo again.

I think the stapler example was very difficult for us to understand, but if I think of it the way I just described, it's pretty simple. It's just like a logo changing on the cup. Then I ask myself, why do I need to have a model of behavior in the lower column? The feature in the left column is just changing its position relative to the rest, but I don't actually learn to lead the behavior there. The reason I describe it hierarchically is because it seems analogous to compositional objects. The parent object might have different behaviors at different locations and orientations relative to itself. For example, I had this animation of the mailbox where you can open the door or move the signpost indicator up and down. One object can have many behaviors, and the behaviors are in different locations and orientations on that object. Using the compositional object mechanism seems like the perfect fit for this.

I'm not suggesting abandoning the compositional object mechanism. I think it might be possible to do it within a column, but I can't come up with an example where the behavior would change its orientation. One of the big reasons we model compositional objects like that is because the child object might change its orientation and scale midway, so we need to do this dynamically. I'm not sure if there are good examples where the behavior would change its orientation, but I think it's hierarchical.

The question I'm asking is: we have these two models, the morphology model and the behavior model. In the logo on the cup example, we have a model of the logo and a model of the cup, and we're positioning the logo on the cup in different positions and orientations. The question is, when that relationship changes—such as if the logo rotates or changes in some other way, or if some other feature of the cup changes—where is the actual behavioral model describing that change stored? It could be associated with the left column, in which case, in the logo example, there would be a behavioral model of something rotating. Or it could be in the higher learning module, which says, "I have an object, and some part of it is rotating or changing," which feels necessary since it's relative to something. If it's just rotating, then relative to what?

The thing I wrote last night was trying to get at the heart of this: we have composition objects. In fact, every object we've created in the system is a composition object. Sometimes the trial features are very simple, but they're all compositional objects. Behaviors are when the features that comprise the composition object can change in various ways—they can change their ID, orientation, position, locations, and so on.

That could all be stored as part of a composition object called a stapler or a cup with a logo. I need to keep track of how the features change on that composition object, which would suggest storing it in the right column, not the left column.

In terms of discussing hierarchy, it's also useful to think that even the lowest level column is, in a sense, a hierarchy relative to the incoming information from the sensorimotor models. From the cortical perspective, it's compositional. Even in that case, it's compositional—features at locations, even if the features are simpler. It's always compositional. In the case of the stapler, we can learn a model in multiple levels of columns. You can learn the stapler in an inefficient way, with all the local feature changes happening at many different locations, or you can represent it more as a compositional object, where it's the top of the stapler that's moving. It's the same mechanism; the former requires more neural representations because it's representing many different changes.

That's a good way of phrasing it. I don't have a very strong opinion about this; I'm just pointing out that, as I've been thinking about it, it's not clear to me that we need to use a behavioral model in the lower-level column in this example. It's not clear to me that you have to do that. If I have a behavioral model on the left side, it's going to be a behavioral model of the top of the stapler, which is equivalent to the logo on the cup. The logo on the cup is just an object on the cup, so the object itself has to have this behavior; the logo would have to have a behavior of rotating or moving. I could describe that as every time I see the logo, maybe I imagine it rotating, or maybe every time I see a cup, I imagine the thing on the cup rotating.

I think the next slides will help a bit because I'm also not saying it needs to only rely on this. There would still be a behavior up here for the stapler, which is much richer than the one down here. This behavior is really just local, like optic flow or local bits of changes, whereas the behavior in the top-level column really represents the top of the stapler changing its location or orientation. There can be two behavior models, and at least how I'm thinking of it, the top level would be richer. I understand that. I'm not necessarily disagreeing; I'm just questioning whether we actually have to have the lower model, the behavior like that. Would it be sufficient for the behavioral model to be only in the April model? You probably don't need it in all cases. I'm just trying to illustrate a mechanism, and maybe in this example we wouldn't need it if we have learned a behavior of the top moving relative to the stapler model. I would also find it helpful to work through what this representation helps us predict. It's not about predicting morphology, so it's presumably about predicting feature changes.

Basically, what I'm trying to achieve with this is associating object and behavior. The example I showed so far is basically learning that, at this location on the stapler, the hinge behavior exists. When we see the stapler, this kind of backward context signal can bias us down here to expect the hinge behavior, so we know there's a stapler and we can push it down and open or close it without it actually moving at the time. That's the main benefit I see in it.

If the morphology of the child object changes in a way that we cannot represent as a change in orientation with thalamic transformations, that would require us to build behaviors in the lower-level column. For example, if the top of the stapler bends, we cannot represent that with a change in orientation relative to the higher-level column, and we'd have to learn that as a behavior of the lower-level object.

An example would be if you have a logo and every two seconds, two of the letters on the logo swap places. There's a behavior of the logo, and wherever we have a logo model, that behavior would have to be represented. You wouldn't be able to represent that as part of the mug; it's not the mug doing that, it's the logo. It's not a mug behavior, it's a logo behavior.

The transformations, like a change in orientation that's happening in the thalamus, are very limited in terms of what they can represent as a behavior in the higher-level column. I don't think the thalamus is learning anything along behaviors; it's just doing a transformation of orientation. There's no capacity to learn, and these transformations are very limited in terms of what kind of behavior we could learn. That's what I disagree with: can we learn any kind of change? That's what I'm trying to get at with the open question of how we represent changes in locations instead of just changes in relative orientations. How do we represent that locations of part of the object are changing?

It's hard to imagine any example where the locations are not changing at the same time as orientation, like in the stapler top as it moves—locations on the staple top are moving to new locations. That transformation applies to the whole child object. When we apply a transformation in the thalamus to a child object, it's applied to the whole object, with this child ID and everything. But that's only sometimes. We came up with examples where the top of the stapler, as it went up, bent in the middle, or you can come up with all kinds of examples where even a change of orientation isn't applied to the entire feature. That's one example where it would need to be in the lower-level column as a behavior.

I'm more expressing that I don't think we understand it very well, because I can think of examples where it's not obvious. The idea that the logo rotates on the cup—is that really a behavior of the logo, or is it just that the cup rotates things? I can make arguments both ways. Let's say every time I saw the Thousand Brains Project logo, it would rotate every two seconds. That seems to be a behavior of the logo.

But relative to what? If I just rotate the logo, I can compensate for that with the thalamic mechanism—it's just the object in a different position. For what it's worth, behavior tells the thalamus how and when to compensate, like how to compensate for the rotation. You don't have to recognize the orientation of the logo all the time; instead, the thalamus is directly told, "Now the orientation should change," and the speed.

I'm trying to understand what the behavioral models are in the left column and what they are in the right column. It seems to me that if there's a behavioral model in the left column, it's going to be fundamentally different from the one in the right column.

For what it's worth, the rotation and the IC transformation—it's helpful to clarify that although we are robust or invariant to it, if you either rotate your head or rotate the text, we can still read, but we are also aware of the change in orientation. You're aware that the line is at an angle or that your head is at an angle.

I don't think it's because the thalamus is undoing it and making it invisible to us. For example, my head is tilted right now, and I'm not immediately aware of it; I have to ask myself if my head is tilted. What you become aware of is if something moves relative to you.

If I move my head, I'm less aware of it, but if the object moves, I'm very aware of it. It's like, "Oh look, something's rotating here."

I'm just countering points, not because I have any fundamental belief, Niels. I think it's still confusing. It's hard to know where you would store that information. I agree, that's confusing. At least the information exists at some level, but if that's a learned behavior, how do you store that long term? That is weird.

The behavior is not going to be stored in the thalamus. I can imagine that a logo has a behavior where whenever I see the logo, it's rotating relative to something—a background that isn't rotating, or maybe there's some apparent object that's invisible in some sense. It's got a reference frame, and the position of the logo in that reference frame is changing. Is it fitting relative to the body or relative to the sensorimotor system?

Maybe it's really relative to itself, because if I held the whole cup rotated, then the logo would be at an angle and it would rotate, but not relative to my eye—it would be relative to the cup or a certain location. I feel like a rotation of the logo doesn't require a parent object. It's the same as the way I'm able to recognize any object in different orientations. It's the same mechanism. But if I'm going to say it's rotating, our definition of a behavioral model or behavioral object is that you've got a reference frame, and then there are changes in that reference frame. So if I want to have a logo that's rotating, I can't rotate the reference frame. I have to say the position of the logo in some reference frame is changing.

Otherwise, I'm just inferring it at different angles. I can recognize the logo at different angles, but that's not a behavior. It has to move relative to something. I need a reference frame that says, "Oh, the logo is now in its rotated position, and now it's in its unrotated position." Maybe the next slide will help with some of these things. I'm talking now about relative orientation changes.

Now, also about behavior models in the high-level column. Basically, if the stapler opens, it's a child object that's changing its orientation. I know it's also changing locations, and it's a bit more complex than the logo in the cup. Maybe I should have used that example, but we talked more about the logo in the cup. Basically, we have the hinge orientation changing relative to the stapler—the difference between the orientation of the stapler and the orientation of the stapler top. It's now changing. Previously, when we just learned a compositional object, we had a relative orientation and would just store this in the model. But now that it's changing, this error goes into the behavior model reference frame. Now we lay down that change as a point in that reference frame. This kind of head-level hinge behavior represents the sequence of changes in relative orientations between the stapler and the stapler top.

I'm confused. We don't—it's not the tabletop orientation that's changing. It's on a location-by-location basis, isn't it? I have to look at the orientation at each location.

It's not like I can apply rotation to the entire top. I'm thinking of one time step—the sensor is at a certain location. That's why in this behavior reference frame, there are multiple points being laid down. It's just illustrating. Technically, as it rotates, all the locations are changing. If I go to one of the new locations, it's got an orientation, which is different from what that same equivalent was before. I'm struggling with this idea that we can separate out orientation at all. It's just an orientation at each location, and whatever it is, that's what it is. But the orientation at a given point in time will be common to the child object.

Assuming we're attending to things that are moving together, one of the big insights with the compositional models is that we have to do it on a point-by-point basis. That was the big insight. We can interpolate between points, but we can't assign an orientation to a whole component or a whole set of things. It's on a point-by-point basis. We have an ID, we have an orientation. It's easier when the logo on the mug or the notebook can be learned much easier if it is a common orientation, but that's just because we can interpolate between points. I don't need as many points, but it is still point by point. If they rotate in different ways—if parts rotate clockwise and part counterclockwise—we would segment this into different child objects.

But I'll say it again: there isn't an orientation applied to the entire child object. There's no way the brain would know that. It just goes on a point-by-point basis. There's this idea of a child, and it's at this orientation at this location.

That was the big insight of the compositional models. I don't think what I'm showing here is against that. It's basically saying we have a relative orientation between these two. Previously, we would store that in the object model at a location, but now there's actually a change being observed. Instead of storing that relative orientation at a location here, we store that relative orientation change at a location here. And then, where was the second one?

Oh, on the hint, the one that ENT—you moved your cursor, so I couldn't see where you moved it. Okay, I got it. That relative orientation change could be different at a different location in that behavior reference frame, but it would be at a different location. I don't think you can separate location and orientation; you can't say the orientation is changing independently. The behavior is that any point can have a new ID and a new orientation.

I agree with that. So, this idea that we're changing the orientation at the top—yes, that's you. Mechanistically, we can't say that; the system will learn some points and can interpolate between them. I'm not talking about changing the whole orientation of the object; I'm talking about laying down one point in this reference frame and associating it with a change in orientation at one location. The problem is the language we use sometimes, like "we're changing the orientation of the top of the stapler," which implies the orientation applies to the entire top. Logically, that makes sense, but that's not what's going on.

Reacting to that, the idea that we can store or change the orientations at some location doesn't make sense to me. You can't separate out orientation change and location change; there's no way to do that. Maybe it's worth clarifying what we mean by location changes. For example, something can just move up, so two parts of an object can be—I'm explicitly not talking about child objects moving location relative to the parent object, because that's the big issue I pointed out last time and will try to propose a solution for in later slides. I'm leaving that out here because I'm not sure what the solution is.

When you say change at locations, do you mean different locations in this behavior reference frame have different changes in orientation, or do you mean whenever something changes orientation, it will also have a movement associated with it through space? You could argue this logo could rotate on a point, or it could go up and down; when it's rotating, there are changes in location, but it's not like the location of the child object is moving. This is the big insight: when we started looking at the logo on the cup, it had a different orientation in the middle or different shapes. You can't think of it as a single thing; it's a point-by-point thing, and you interpolate in between. When I think about the behavioral model, imagine I have a morphology model, and then a moment later the morphology model is changed because of behavior. The actual positions of the features and the orientations at time one are different from the positions and orientations at time two.

You and I could say, "That's the top of the stapler moving," but it doesn't know that. It's just a set of morphology descriptions that flow through time. What we want to do is learn a behavioral model that says, "I don't know the specific details, but I can predict how the morphology model would change. I can predict what the new morphology model would be, and I'll use the behavioral model to do that."

I'm trying to get very abstract here. There is no concept at all in the high-level model of what the child objects will do, what they are, or how they're going to move. It doesn't know anything about that. It just says, "I have an ID, I have an orientation at this location at this time."

The fact that there is a child object with its own reference frame and it's rotating—we'll take advantage of that. But the parent object doesn't know anything about that. It's not aware that the top of its stapler is rotating. It just says, "In a series of times, I have different morphology models, and I'm trying to learn how to close between them." This language is so hard to use.

I think I'm agreeing with everything you're saying. Just to double check, we are still communicating the orientation of the child object here? No, we're not. We're communicating the orientation of a child object at one location. It's not the object itself; it's just at some point where I'm focusing or attending to on the top of the stapler—what is the orientation and location at that point? But it's still the global orientation that this lower-level column applies to the movement input it receives to move through it. That is under the assumption that the lower child object is not changing, that it's a single thing with a reference frame, and we're going to take advantage of that. But the parent doesn't know that. The parent doesn't know there's a single thing that's moving; it's just detecting any kind of change. Isn't that the point of breaking off the child object with attention? The higher column can assume it's one thing because the lower-level column is only going to communicate at any given point about a unified thing. I'm not sure how that mechanism would work, Neil. Somebody is clearly saying, "This thing is moving, and I need to treat it as a singular object," but I don't know how that's happening. This is why I like thinking about the example of the logo rotating on the cup. It could avoid that whole issue, which we can come back to later. In the case of the stapler, where I had to break off a piece, it makes it harder. Maybe it makes sense to move on to the next slide.

Maybe it'll be easier to talk about after the whole problem setup is stated. It would also be nice to get to the movement correction stuff so everyone can think about that. I have a logo on a cup example on the next slide. It's not rotating, but for completeness, I want to show learning feature changes. So basically, we have the no matter logo or—hang on—we have a logo on a cup. I don't know why it's not animated. Here we go.

We have a logo on a cup, and the logo can change between the Menta and the TVP logo. That's the behavior we're discussing—the graphic on the mug changes. At a certain time interval, each logo is shown for two seconds, then it switches back and forth. The low-level column can recognize both logos.

Is the entire logo changing or just the graphic? Just the graphic, not the words. For simplicity, let's say there's no text on the mark. The TVP end cup has this specific behavior where the logo keeps switching. Whenever the logo changes, the object ID feature coming into the high-level column changes, and that needs to be routed to the behavior model. The change between these two object IDs from the lower-level column would be modeled in the behavior model. That model would learn the temporal frequency of the changes. This behavior can also be used as a feedback contact signal for the lower column to know when to predict which logo at what orientation.

There are a lot of assumptions that would need to be addressed. If you'd like, I can go into that, but we only have 13 minutes left. If we're going to try to finish, let's keep going. Viviane, if you want to get to the point in your document, maybe do that now so people can think about it for next week.

How could we use this mechanism to predict morphology during a behavior? The idea is that the behavior model stores changes, such as relative orientation changes of the top of the stapler relative to the whole stapler. This feedback signal could inform the lower-level column how to update the expected orientation of the child object. If the expected orientation of the child object is updated, all input for the child object is in the correct reference frame, and all predictions about its morphology would be correct. It has a detailed model of the stapler top, so it can predict all morphological features. We wouldn't have to learn any key frames; we would use this behavior to know how to modify the incoming movement vector to remain correctly located in the child's reference frame.

All my slides discussed orientation changes, but there are also location changes. Currently, with compositional objects—not behaviors—locations are encoded by assuming co-located receptive fields and making location-by-location associations.

Now, a child object can move relative to the parent, and that movement will be stored in the behavior model. The behavior model in the high-level learning module will need to update the object model in the lower level to update its expected orientation and location in the reference frame of the child object. We need a way for the high-level behavior to inform how to adjust the incoming movement vector so we move correctly through the child's reference frame and make correct predictions about its morphology. It's straightforward for orientation; you can update the expected orientation in layer six. However, we currently don't communicate or store relative location changes of the child object to the parent. That's the problem setup.

Potential solution one, which I just added at the end of the document—it's not my favorite, but I wanted to mention it—is to communicate the object location relative to the body. Similar to how we calculate the relative orientation of the child to the parent, since they're both in a common reference frame, we could calculate the relative location of the child to the parent and store that. You could then recognize changes in the relative location and store that in the behaviorist reference frame. It would be straightforward to implement that in Monty. To get the location of the child relative to the body, you take the location of the sensorimotor relative to the body, which is already in the body reference frame, add it to the location on the object, and output that. That would be an easy mechanism to implement in Monty. I think we had reasons why we didn't do this before, but maybe it's worth revisiting. I want to get to the other idea quickly.

The second idea is to rely on motion information from the sensorimotor. The high-level module still gets direct sensory input, which will also communicate movement in the sensorimotor sensors' receptive field. That kind of optic flow could inform location changes at this level, and those optic flow location changes could be applied to the child object's reference frame. The big assumption is that the high-level receptive field is large enough to track the entire child object's movement. We've discussed before how the largest receptive field detects global movement of the sensorimotor, but more local movement can indicate movement of the object itself. This approach assumes that the movement of the child object would be detected from the raw sensory input and stored in a high-level learning module's behavior.

and then to apply that motion, no matter whether we get it from having body-centric location information communicated or from the optic flow information, we basically have two scenarios. One is applying motion where the sensorimotor is following the object, and the other is the object moving through the sensor's location, so the sensorimotor is static. In the first scenario, the sensorimotor location relative to the body changes—the sensorimotor is moving, but the location on the child object remains the same, always at the same location on the stapler top. In the second scenario, the sensorimotor location relative to the body remains the same—it's not moving, but the location on the child object changes, and we need to be able to deal with both scenarios.

For scenario one, where we are following the object with the sensorimotor patch, we are getting movement input here, but also movement input to the higher-level reference frame. That movement would go into the behavior's reference frame to move through that reference frame. As the sensorimotor patch moves, we are also moving in the behavior's reference frame, but in the child object's reference frame, we want to stay at the same location. Even though our patch is moving and we're getting movement input, we don't want to move in this reference frame, only in the other one.

A way to do this could be to take the stored location and orientation changes and apply them to the child object's reference frame. For orientation, we can adjust the expected orientation. For location changes, we can apply that to the incoming movement vector, and in this case, they would cancel each other out. The movement of the sensorimotor following the stapler is exactly the opposite of the movement stored in the behavior model, so we get what we want: we don't move in the child object's reference frame.

Even though the sensorimotor is actually moving, in the second scenario, where the sensorimotor is not moving but the object is moving, we are not getting any movement input here, but we want to be moving in the child object's reference frame to make the correct predictions of which features we want to sense at what point in time. We're also not moving in the behavior's reference frame because there's no movement input from the sensorimotor patch since it's static, but the sensorimotor patch is detecting optic flow, so it can detect this kind of local movement or location change of the child object.

That would be stored in this model. Again, we can take the stored location and orientation changes and apply them to the child object's reference frame. The same mechanism applies to orientation, but for location, we take whatever location transform is stored in this model, invert it, and that becomes the movement input. Since we don't have movement of the sensorimotor, nothing cancels out. We are moving in the child object's reference frame, which is what we want.

That's it. It might take a while for that to sink in, but I think this worked well because last time you gave a nice overview of everything up to now, and then it was quick, 20 or 30 minutes at the end for the new ideas. It's been similar today, where we spent more time talking about last week's material, but I think the discussion we had today wouldn't have been possible the first time around. I think there's a system here. Next time, we can dig into these details. It reminds me a lot of when we worked on compositional structure, which, if you recall, went on and off for years, but it was not steady. We spent so much time arguing about what the problem was, what problems we were trying to solve, and we kept introducing new complications that the solutions wouldn't work. I think that's where we are now. My sense is that this isn't right yet. We're thinking about this wrong, there's something we're missing, and the final solution will be more orthogonal or simple in some ways. We're in the process of arguing, like with the composition object, when we said, "Yeah, but the logo could bend," or "We could have this on top of this, on top of this," and how do we understand how many levels of hierarchy there are, and so on. Then we came up with a straightforward solution. I think we're not there yet. We're still in the argument about what the problem is, and it reminds me a little of displacement cells. That was the simplest, completely wrong example we had, but it was the first attempt.

So, it's good progress, but I don't think we're there yet. I think it's an interesting idea, and it would be good to work through the details next time. One high-level observation I wanted to make before we wrap up: Viviane, if you can just show one of your slides again with the animation of the movements.

That's perfect. Thanks. No, like that other one you just mentioned. Thanks. I think it's worth considering how, at the moment, we've talked a lot about not wanting key frames, and I think that's true, but in some sense, our behavioral model is still like a key frame in that we're learning changes at locations throughout time. It's quite a densely sampled model. If you imagine something as simple as a stapler opening, there are changes that emphasize the importance of hierarchy. If there's any way to do that efficiently, we can't represent a bunch of tiny local changes everywhere. That doesn't necessarily mean it's just one single object moving, but thinking about that again, it feels relevant.

The alternate solution is interpolation. Interpolation has to work really well because anytime we have something that's not uniform—let's say the top of the stapler breaks into ten pieces and each one flips over—then you have to learn a lot more to capture that behavior. But when the whole top moves at once, you don't have to learn much. To me, the solution to that problem is interpolation. You can't sample all the points, and you certainly don't want to for simple behavior. Somehow, the answer is to interpolate within the behavioral model. Maybe I can't store all the points in the behavioral model, but I need to interpolate between them to make correct predictions. It's just as you said: it's not a dense model, but it has a lot of points—a lot of story points.

That's a general problem. I thought that was a small elephant in the room to consider. I've argued before that even our morphology model has the same problem. We sample much more densely than we should. We've been avoiding it, and in that case, we're hoping hierarchy helps. It's probably not an either-or. I feel like hierarchy and interpolation are both right. Hierarchy certainly helps a great deal. You could argue that you could break a problem into many hierarchical components, making it more efficient, but then you have a lot more data storage in the hierarchy. There's no free lunch. The hierarchy is going to be a big component, but interpolation will be a component too. It feels like it has to be, and I'm not too worried because I think we can get the interpolation part to work.

I agree with your point. Hierarchy is going to be an important component of this.

All right. I'm excited to think about this. I just hope I can stay focused. Sorry, I didn't want to present the whole meeting again. That's all right. I didn't even plan on making slides, but I felt like with composition objects, it's easy to just draw on a whiteboard during the meeting, but with behaviors, I need a temporal component to show what I mean. So I decided to make some animations and some slides to explain them. The animations were nice. I don't know how you did the one with the hand-drawn stapler. That was pretty cool. I loved the Walt Disney-style stapler. I was sitting here thinking, am I getting too old? Do I have to learn how to do this? Is this the new requirement? I don't even know where to begin. Robbie did some amazing stuff in PowerPoint. Last time, I made them all with After Effects, but it's such a complex program. I was talking with Bill the other day about how, fifteen years ago, there was this really easy program, and we found a similar one that's just a web interface. I tried using that today. It doesn't have all the features I wanted, but it worked reasonably well.

All right. I'm going to try to work on this.