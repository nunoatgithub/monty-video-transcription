Throughout the week, we did a lot of pair and peer programming on Brighton Pier. We also did some mob programming—Niels, Jeremy, and I formed a three-person team and took turns every 15 minutes switching who was programming, while the others watched on the big screen and gave input. It was an effective way to make decisions and figure things out quickly together, using a whiteboard to talk through issues as they came up.

Here is a short video of Martin learning on the screen, with some people watching and contributing, learning how Monty learns. There's Monty moving around.

We also had daily check-in calls at 5:00 PM UK time and 9:00 AM PT with Rami and Hojae, who unfortunately couldn't join in person. Hojae got sick last week.

One evening, we went to a real British pub, had some pints and British food, and this is the prize you'll get to award to the best team. With proper UK weather in the background, it has an umbrella and a sun hat because you never know what you'll need. Is that a Monty Python reference in the corner of the can? You mean the knight? I imagine they're probably alluding to it unofficially, but I don't know. There aren't any budget Glorious Spam. Monty wasn't the Holy Grail. Didn't they have a skit about glorious spam? Vikings in the skit, you guys in armor.

Scott, did you have any trouble in customs carrying those? The only trouble I had was with the extra suitcases. I didn't really remember what it looked like after that, and I didn't put a tag on it. I had to inspect the numbers; it was pretty generic looking. I should have put a ribbon. Very generic—basic black suitcase. There are millions of them. The police were standing around waiting for someone.

Now, the idea is that each of the four teams has a maximum of 10 minutes to do a quick presentation of what they did this week and why it was important. It's great if you take less than 10 minutes, but try not to take more. For the judges, hold your questions until the end of the presentation, and then you can ask all your questions. Sound good? Sounds good. I love those graphics. Thank you. Question: how did you choose the teams? We first chose the project, then people signed up for the ones they were most interested in, and we made sure to get people on each team and tried to mix departments a bit. I'm interested in their project, but I'm not going to work on something about mark timing. Giving a two-minute warning at eight minutes as well? Sure, I can give a two-minute warning. I think we're going to be done in four minutes. Cool, let's try. It's a lot of work, but there's much to show.

We've been Team Python 3.8 to 3.anything. The reason is that we're currently pushing a big boulder of a GI up a really steep hill, and the Habitat Simulator is forcing us to stay on Python 3.8, which is making everything slower and harder. That was our project: we need to stop Habitat Simulator from slowing everyone down. This is a simple diagram of all the problems and things that slow down the engineering process, which Tristan put together. Viviane and other team members contributed, but essentially, it's a Riyad street. We're going to focus this project on that green blob at the bottom, which, if we zoom in, looks like this. There are a bunch of boxes that this proof of concept was going to try to fix and deal with.

This is the existing architecture from our proof of concept's perspective. A benchmark, currently constrained to Python 3.8, contains an experiment—Monty doing all the work—and the Habitat Simulator, which is what's tying us to Python 3.8. Essentially, a benchmark starts up, builds an experiment, and then Monty starts requesting patches from the Habitat Simulator, doing things with them, and sending movements to the Habitat Simulator.

We wanted to get to an architecture that looked like this: the benchmark, which again contains the experiment of Monty, can now run on any version of Python. We'll look at what version we got to in the demo in a moment. We're extracting the Habitat Simulator out over the network and making network calls to it instead of running it in the same process as the benchmark. We opted to use gRPC, a network protocol that Google released in 2016, which is good at sending large amounts of data back and forth over the wire.

This architecture will also allow us to start using different simulators, like Isaac Gym and C++, or any simulator that comes out. We should now be able to switch them out and use them much more simply.

What we did was pair programming—Tristan and I worked together, refactoring the existing classes and removing unnecessary elements from the protocol sent to the simulator. We reviewed all the code that interacted with these elements and reorganized it. We also added type annotations to all objects in the code.

On the left, you can see the old version: just a NumPy array with values like 0, 0, 0, which could mean anything. On the right, with the new typing, it's clear that it's a Euler angle (X, Y, Z), so anyone looking at the code in the future will immediately understand its purpose. This also made the code much smaller; for example, a function that cloned data now does the same thing in less than half the lines.

Typing allows us to write more concise and meaningful code. The class `SimulatedProtocol` is the Python version for creating a simulator instance, with functions like `add_object`, which takes parameters such as name, position, rotation, scale, semantic ID, and possibly a primary target object for compositional or occluded objects. There are also functions to take a step, reset, or close the experiment.

On the simulator side, the gRPC side, there is a receiving object written in a system called Protocol Buffers, developed by Google for network data transfer. You can see functions like `add_object`, `step`, `reset`, `close`, and `remove_object`. A message sent across the wire is set up here. When an observation is returned, it contains an agent observation, a sensorimotor observation, and repeated sensorimotor observations. That's the code. I won't go further into it, but thousands of lines were changed over the last four or five days, resulting in this exciting progress.

The demo doesn't have much to see, but focus on the idea that at the bottom here—oh wow, it looks much more lively on my screen, but it's probably more readable on yours. It's moving faster than we can read on the actual monitor. On the bottom right, what's important is that Python 3.8 is running. This is the simulator—actually, 25 simulators are running. Those are just different numbers of simulators, and they're getting messages, moving the simulator around, and sending back what the simulator sees to Monty. The exciting part is up here: that's Python 3.12, where Monty is running. This is the new stuff. 

Isn't 3.12 older than 3.8? No, 3.8 became obsolete last year, and 3.9 becomes obsolete soon. This is 3.12, not 3.1.0.2. That's newer than 3.8. 

That's it—it's devaluating. We're 95 episodes in. This is what a normal benchmark run looks like. One caveat: we can run on Python 3.12, but it's about 10 times slower because everything has to go across the network. The serialization of data—essentially just sending screenshots across the network between the two processes—makes it about 10 times slower than if we ran everything in the same process.

Did you anticipate that? At some level, yes, but I didn't know how much slower it would be. Ten times slower is interesting. It's hard to tell if that still makes this valuable; it's a big hit for this separation. It makes it questionable whether we should adopt it or not. Maybe next hackathon we can figure out how to make it faster. There are other benefits: any problems with installation are now associated with that. For example, maybe some of the basic transforms could happen in the simulator. 

If it worked faster, we could install Monty the way people usually install everything in Python: pip install monty. Right now, to install Monty, you have to use a conda environment, which makes the setup more difficult than what Python users are used to. We can use later Python versions and the latest dependencies. A bunch of dependencies are updated in this 3.12 version. We can run on our MacBooks natively. Right now, we're acting like we're on Intel PCs—we're lying to the environment—but the new version runs natively on MacBooks. We can measure Monty performance separately from the simulator because now Monty is running by itself and the simulator is a completely separate process. The big one is installation problems: Monty would just be "install Monty" and it's installed, and all installation problems can now be blamed on Habitat. If it's not working, it's because you're installing the Habitat simulator, so that's the Habitat problem. That separates the blame nicely. Other people can write simulators, as Will mentioned. Installing on Windows is currently difficult for anyone because of Habitat, but people don't realize it's because of Habitat. Contact us if you have issues. Habitat is a significant constraint, so this is one way of removing it, for a 10x slowdown in the POC. We think it'll get faster.

Is that 10x slowdown overall or just for this piece you tested? It's just a guess based on what I've seen—an order of magnitude, not a precise measurement. To get a representative number would require more work. The benefits are great, but the speed hit is pretty bad. 

By the way, I'm getting a weird echo when I talk. Maybe someone has their mic on—maybe Terry, if you mute yourself, it looked like it was coming from her. We've never had to think too much about the amount of information being transferred for sensory processing and things like that. There's probably a lot of low-hanging fruit to optimize, especially the images we're sending across. The beauty of Monty is that it needs very simple input features, so it's probably excessive what we're sending right now. With the touch policy, we could send four actions at once and just get back the results. 

The encouraging thing is that now I can play high-resolution video games over the network using a cloud computer in real time, so we know that sending data fast at human-like 60 Hz plus is possible. We just haven't done it here yet, but that could become an entire effort. It's not impossible to do it very fast.

Got it. Tristan, excellent. 

Just a question: you said you're running 25 instances of the simulator. Could that be why it's slower, or is that not the reason? Today I learned that when we run benchmarks, every episode creates its own simulator. When you run in parallel and there are 16 CPUs, then you have 16 simulators running.

Interesting.

Any other questions from the judges?

Who wants to go next? I'll go. All right.

OJ and I paired up on something we've been wanting to do for quite a while, which is to improve our model-free saccades. Saccades are eye movements where you dart your eyes around to scan objects more quickly and identify what the object is more efficiently. In real life, as a human, you might encounter an unknown entity and need to rapidly identify it by scanning your eyes across it, since it could be a threat and you may need to react. What you might do is look at one eye, then the other, the nose, the mouth—you're not aware of this, but your eyes dart around to the most salient, interesting, and informative features.

On the right here is what Monty is currently doing with the distant agents. As you can see, it just starts somewhere and then does a sort of random walk with fixed lengths. It may or may not find anything interesting or informative for a while. Monty is a bit like this, but where we want Monty to be is more like a human: just a few cool, calm, collected motions to rapidly identify the object.

By the way, whose dog is that? I think it was yours, Hojae. We're just talking about the model-free eye movements, because we still have model-based eye movements—this is just the model-free ones.

So, better sound detection. Just to ask yourself, how do we know that dog is friendly? You can tell right away the dog is friendly. What features of the face tell you that? It's just interesting.

The idea this week is to have a salience showdown. We're going to try to use metrics and possibly pit them against each other, or at least pit them against what Monty has right now. We'll see who walks away from this fight and who's the fastest draw in the west. Are you really trying to get that spam trophy? Is that the idea there?

What was the prompt you used for that image? The very first thing we had to get going was what's called inhibition of return. If you have a really salient location and aren't encouraged to explore other parts of the image, there's nothing preventing you from just staring at that one interesting part or constantly returning to it. What we're seeing here is on the left: we've started our gaze at the center of the cup, and that little dark spot indicates a lower likelihood of returning to it after a couple of steps. We've got a couple of these points now—those are spots we don't want to return to. Over eight steps, they accumulate and help push us to explore other parts of the object.

The red arrow is where you fixated, and the gray areas are where you're going to go next. It's part of the visualization. The other ones are where you have been, correct? This is just part of the pipeline we had to get going first to start working with the salience maps. Even this alone took us from an average of 39 steps at best with a random walk policy to identify an object, all the way down to 21 steps. That alone gave us a pretty enormous performance benefit, so it's a good start.

There's a question about how you did that. You're trying to find salient spots, I assume—or not yet? Maybe right now it's just inhibition of return and you didn't really focus on saliency. At this point, there's no saliency; it's just uniform on the object. There's a trade-off between saying, "Don't go back there," but also, some places you could go are not very interesting. It's a balance between picking salient spots and not returning. Right now, it's just random don't return. Or at least don't return for a while, because these spots do ease over time. That central spot is less pronounced after eight steps. Eventually, you return—maybe after ten steps—but you're not completely barred from going back.

After inhibition of return, we tested some non-deep-learning-based saliency methods. Here are the four methods we focused on—there are more, but we didn't implement them all. One is called spectral residual. This one does a fast Fourier transform, filtering out low frequencies and keeping the interesting parts, which are usually high-frequency areas like edges. The BIOS one on the top right is something bespoke that Scott made—maybe you can explain that a bit more, since we're going to focus on it. Minimum barrier is a recent (2014) computer vision method focused on picking out foreground regions from backgrounds, which is good for segmentation if there's a cluttered background. The last one is IDK; this is a very bio-inspired method that combines Gabor filters and has an implementation related to center-surround differences. This is a more complex, bio-based method. Next slide, Scott.

I've also tried this on natural images. This is just one example, but it makes a demon dog, basically. Arthur is the good boy when he is sleeping or low energy; the demon boy is when he's having zoomies. That's what I see. Which filter or method is this? Spot. This is IDK. You can see his eyes pop out, which is what you want to attend to, and the teeth. Interestingly, there's something above his right ear that's not really interesting but is highlighted, which feels like a binder or some kind of model-free segmentation method to help keep constrained motions on the object. I had to look really hard to figure out what's going on up there. It looks like it's because it's blue and there's no other blue. That's weird. 

For example, we could apply different salience methods. If I apply the foreground-background segmentation first to get Arthur out, and then apply this IDK, we will focus just on the interesting parts of Arthur. Depending on the method, what it picks as interesting is very method-dependent. Clearly, this doesn't capture the biology completely because there are a bunch of artifacts. But it gets the teeth, which is pretty good.

The method we're going to be working with for the rest of the talk is essentially a combination of filters known to exist in the early primate visual system: luminance-based filter, depth space filter, red-green opponency, blue-yellow opponency, spectral residual, and one that favors continuous oriented regions. It's a collection of six, and we can weight them. There will be a lot of tweaking and weighting, but this is our approach.

Spectral residual is similar to the FFT-based method Jose explained, looking for higher or low-frequency content above the average in the area. This one needs quite a bit of tweaking, especially the one that came out of the box. OpenCV looks a lot nicer, but there's still a lot to be played with to get it to work better. For spectral residual, think about moving across the image: if you go through a line, it'll be high frequency if it suddenly encounters an edge. Spectral residual is good at edge detection.

Two-minute warning, by the way. Maybe try to hold back questions a bit more, and we can discuss in more depth in the research meeting.

We did get our improvement from 39 average steps per episode down to 23, which is great. It's actually a little bit worse than inhibition of return only, and we thought maybe the reason is that we might get better performance if we pre-trained our models using the same salience-driven motions, rather than the naive spiral scan, which just takes even points across the object—a very machine, artificial way to learn models. We then decided to learn our models again. The default is the spiral scan, and this is what it looks like if you only go to the most interesting points. This year, it's the spam can or potted meat can, if you prefer. If you go to the most salient points, that actually helped and drove us down to 17 steps per episode, which is fantastic.

One of the things we hope to get out of this is sparser models, which we'll discuss later. Here's the summary of results: this is our new state of the art. If we learn our models with the salience model and perform inference, the top row is what we had, and the bottom row is what we have now. We've more than halved the number of steps. We're more than twice as quick at recognizing objects and have drastically reduced the number of timeouts, which is a huge time sink. It's hard to quantify and put in the same category as steps, but it's a significant change. We've also boosted our accuracy a bit, so it's very promising. I'm really excited about this.

I think we have a winner. We're very excited. Not only did we get what we wanted, but we did it in a bio-inspired, principled way. It's a pretty exciting result for us. Thanks for listening.

That's great. That's very exciting. You've been holding back these results. Every day we're checking in, talking about the results, and today showing them—boom, mic drop. I don't want to take too much time here. It's interesting: Will and Tristan had this green theme, and you guys have introduced the red theme. Now we have blue. I don't know if we have time for questions or if we want to move on. Do you have any more questions? I was just chastised for asking too many.

Maybe you have questions you'd like answered to make an informed decision about the winner. I don't know Terry Celeste, but I have a question similar to the one I asked Will and Tristan: this is one part of an overall system. This is the model three part, and what's not clear to me is how this model three part interacts with the model-based part.

This should get us to model-based decisions much quicker. If it normally takes 20 steps before making your first model-based motion, or a motion derived from some kind of model-based hypothesis, this should get us there in at least half that time. After we go to model-based, do we stop using the Ency-based completely? Or does it continue? It would just go back and forth. Most of those steps are model-free, and then every once in a while it takes a model-based step, so it keeps switching between the two. It doesn't take a model-based step until it has some kind of hypothesis. In the beginning, if it doesn't have any hypothesis, it can't really make an informed model-based decision. 

Rami, do you want to go next? This presentation is about EMA evidence-bounded scores. Right now, we have evidence scores that just grow unbounded. Every time we get a little bit of evidence on any of the hypotheses that Monty has initialized, it just keeps adding them infinitely, so the evidence keeps growing. There are a lot of downsides to this. I will talk about these in a minute, but what does EMA stand for? Exponential moving average. We are averaging the evidence so that it stays bounded. 

I presented a little bit about this before, but right now in Monty, we have an experiment. An experiment is composed of multiple episodes. Every episode gets its own object, and Monty is just trying to recognize that object. After it recognizes it, we move on to another object. We reset all of the hypotheses that we had; the hypotheses could be about the orientation of the object and where the sensorimotor is on the object. We reset all of those hypotheses and initialize new ones that match the observation from the new object. This has problems when we are doing compositional modeling or composition inference. Viviane and Niels might talk about that in the presentation, but mostly, what this has led to is the idea of hypothesis resampling, where every step we're always resampling more and more hypotheses and deleting the ones that are not performing well. 

Actually, initializing hypotheses at any time in an episode does have problems. One of the problems is how do we initialize it? What kind of evidence do we give it? We could have a hypothesis that's initialized at the beginning of an episode, and it's not a very good hypothesis, but it has been there for a while and is accumulating evidence at a lower pace than another hypothesis that is added later in the episode and is accumulating faster. When it comes time to evaluate them, the one that was added earlier actually has high evidence. This is a problem. One simple solution would be to use the slope of the hypothesis, basically normalized by the age of the hypothesis—just divide by how many steps it has seen. But there are concerns about whether neurons are able to do that, whether they're able to keep count of the age of the hypothesis. It's one of the approaches we're considering.

The goal of this hackathon was to test the idea of keeping the evidence bounded in a range. We're doing this exponential moving average, where at every new step we get the input evidence, multiply it by some alpha, and add to it one minus alpha multiplied by the old evidence. We're always averaging, and it's always going to stay within the bounded range of the evidence. This has a lot of benefits. Hopefully, it will solve this problem because the evidence is always bounded. It will plateau once it reaches a certain point. I'll show you some profiles of what it looks like on simulated data.

Another honorable mention is the idea of membrane potential. We're also thinking of using this, and it's also bounded. We had to do a few tricks to get it to be bounded, but it's basically also bounded. The idea is that we would have to threshold the evidence scores so that we can convert them to spikes. Every spike deposits some charge on the circuit or the mind brain, and then it decays when it doesn't get spikes. The problem with this is that when we threshold these evidence scores, we're losing information about how good the evidence is, or we're losing detailed information. We don't want to compare it to spikes just yet.

I've presented this before. This was the result from hypothesis resampling. On the Y axis we have the evidence scores, and these are accumulators; they can grow up to infinity. Those bars at the top are the episodes, and I'm switching between objects. As we switch, we're deleting the hypotheses that are not performing based on their slopes and similar metrics. We're deleting some of them, and new ones are being added, and they keep accumulating more evidence and winning. This is a good scenario, but it's missing some details. One of the problems is if you're just looking at one of the episodes, not all of them. Within one episode, we could still have the problem I discussed before: a hypothesis that has been accumulating evidence for a while not getting deleted, and a newer one added halfway not winning the race at the time we're evaluating it.

I reran everything with EMA, and this is what it would look like with the EMA evidence scores. The biggest difference is that it's always bounded; it doesn't grow above about 1.5 or 2, which is the highest step evidence we would get. The evidence always stays bounded, but if the hypothesis is good, it will be above the others—the most likely hypothesis will be above the rest, as you see here. If the color matches, that means it's good.

Here are more examples of what you would see. These are real hypotheses, with a few random ones included so you can see what the profile looks like. On the left, we have the evidence scores—these are unbounded, and the blue lines represent those unbounded evidence scores. On the right, we have the orange line and the evidence EMA, which is on a different twin axis. When you see a line labeled four or three, that means we've moved from one episode to another—the object has switched. You can see the decline in evidence EMA, and I'm using that decline to delete the hypothesis.

The bottom left one shows a decline before switching to a new object. There's a bit of decline still in the range of one, but the decline increased a lot when it stopped getting any evidence. Some of these might have a lower slope. Would you consider that a failure in that case, since the EMA score went down prematurely? It depends on how you threshold it. I should have set a constant limit on the Y axis—this is 0.3 to 0.9, and this is 0.6 to 1.2. 

Why would the EMA decline at all? It's at points where the evidence doesn't increase, so it drops because of that. If the step evidence is zero and the current evidence is one or 1.2, it decays. These are noisy experiments, and not all points are well represented in the model, so it's not easy to get a lot of positive evidence at every step.

I didn't understand the actual algorithm for EMA, so I didn't realize it had a natural decay. It's quite similar to a leaky integrate-and-fire neuron. It's leaky in the sense that when you're not getting input, it decays. By changing the alpha factor, we can change the timescale for that decay and the rise. This might relate to the multi-compartment properties of neurons, where dendritic spikes last much longer than somatic spikes. We might use a mixture of these alpha constants to get something similar, since it's beneficial to have multiple time constants.

The current system, the blue line, is an unbounded increase with no decay, correct? It can decay, but very slowly, unless it gets negative evidence. That sort of decay seems to be one of the key things missing, but the EMA has that. We've wanted to do that for a long time. Bounding the evidence from above to a specific value makes things simpler and more interpretable. Right now, if an experiment has been running for 400 steps, some evidence values are super high, and in another experiment, they're not, which doesn't make much sense. The bounding alone didn't make sense to me, but with bounding plus decay, it does.

This is a visualization I'll talk about briefly. The main parts to focus on are the ground truth object in the world—its rotation, orientation, and so on. This plot is a correlation between the evidence slope and the pose error. Each dot represents a hypothesis in the hypothesis space. Some hypotheses have good ground truth pose error, others have high pose error; that's just how Monty initializes them. On the side is the selected hypothesis. If you notice the red dot, I can click on any of those to select a different hypothesis and inspect its evidence scores from initialization to deletion. All the information on the right is specific to a particular hypothesis, one of the dots on the previous plot.

What I noticed is that when we're running these experiments, this is currently how Monty does things. It would end up choosing a hypothesis that has a pose error of 3.11 radians—this is 180 degrees—as the most likely hypothesis based on its evidence, because it has accumulated a lot of evidence. As you can see here, this blue line has accumulated a lot of evidence because it has a high age, meaning it was present for many steps and had plenty of time to accumulate evidence. Meanwhile, we have a better hypothesis down here with a low pose error, which looks much more like the ground truth, but it's only been present for 21 steps and has accumulated 22 evidence points. When you compare 22 to 42, of course the one with 42 is going to win, but the one with low error is the one we want.

When we add EMA, that resolves this problem. You can see the EMA in the orange line, and it's bounded nicely. It ends up being a lower score for the hypothesis with higher evidence, even though it has more evidence, because it has a lower EMA. The one we want has higher EMA but low evidence, and this is the one with low pose error. EMA resolves these issues because it's a slightly better metric than just comparing accumulated evidence.

I ran a few benchmarks to make sure everything looks good. I did a lot of benchmarks, but I'll only talk about one experiment. This is one of the big experiments: the noisy 77 objects surface agent. The alpha here is one of the EMA parameters. I noticed that we kept the same performance; some alphas increased performance a little, some decreased it. The problem I've seen is with pose error—some have increased the pose error, and in most cases, pose error has increased. Also, runtime has increased quite a bit. That's the part I'm going to investigate more. There are parameters that could tune this, like the x percent threshold, which determines when terminal conditions are reached. There are many details that could be tuned to help decrease runtime, like the difference between the orange and green, without affecting performance. More investigation is needed, but EMA is a plausible approach. There are other options, but this is one of the good contenders.

Viviane, Jeremy, and I were working on compositional objects for our project. This goes back many years—since at least 2017, people in the Thousand Brains Project have been talking about recognizing mugs, and quickly that became recognizing a mug with a logo on it. It's become something the Thousand Brains Project is known for. When one of our first contributors joined, the natural thing was to pose with a photo of a mug with a logo on it. It's core to the DNA of this effort to develop intelligence, and that's no coincidence because the world is compositional. This example of a logo on a mug might seem simple, but it's essential to intelligence and how we understand the world—as a hierarchy of compositional objects and their relationships.

Many of you know this is a central theme for our research: getting compositional objects working. A lot of the cool things people are working on, including what Roi just showed and what Scott and Hojae showed, are about enabling us to do compositional objects well. But we've been missing a way to actually evaluate and experiment with compositional objects. Scott made this dataset a while ago with objects with logos, but we had no framework, experiments, or configs to enable us to perform these experiments and measure them. That was the key aim of this project: to implement that and get a new benchmark with compositional objects. If we don't do that, Monty will continue to exist in a non-compositional world, where everything is a blurry cloud of dots associated with colors and surface texture, and we don't want that. That was what motivated us to do this work.

This is an insight into Monty learning about compositional objects. At first, it learned about the logo in isolation, moving frantically. By the first or second day, we got it together, calmed down, and moved more slowly over the object. This provides a peek into the learning framework we set up, which required several tweaks because learning compositional objects has more intricacies.

The process is split into two phases. Phase one is learning object parts: we show Monty the logos and different objects in isolation, and both the lower-level and higher-level learning modules are learning, each receiving a label. We're giving Monty some crutches for now, not going fully unsupervised yet, to help it learn compositional objects. We show them in isolation, tell Monty what it is, and it learns these models. The lower-level model gets a smaller, higher-resolution receptive field as input, while the higher-level one gets a larger, lower-resolution input. They learn different models: the lower level is densely sampled, and the high level learns sparse models based on sensorimotor input. These are not compositional models yet, just based on sensory input.

Phase two is learning the compositional objects. We show Monty the logo and different logo-object combinations, using the same sensory inputs. Now, the lower-level learning module does inference, trying to identify the TVP logo, mentor logo, disc, cup, cube, or whatever it's sensing, and sends that object ID up to the higher-level learning module. The higher-level module stores the input from the lower-level model into a compositional model. For example, a model of a cup is learned from the lower-level module's input, which sends the mug ID up to the higher one. We still give the higher-level one a label, indicating the compositional object, such as "TVP mug" instead of just "mug." 

We also have a comparison condition called the monolithic models, which serves as a baseline where no input is sent up through the hierarchy, and both modules learn compositional models based only on sensory input. In the "sad world" that Niels showed, the models wouldn't be compositional. It's basically trying to learn the mug with a logo as one thing—learning compositional objects, but not building compositional models, just monolithic models. That's what we compare to, to show the benefits of learning compositional models.

One of the things we needed to do was update how we measure performance. Initially, each pair of lines in the experiment represents the lower-level and higher-level learning modules. At first, the system would indicate confusion, but we've changed it so that it now says it's consistent with the child object of the actual object being viewed. For example, if we're looking at a cube with the TPP logo, it recognizes a cube, which is a subcomponent of the object. We created a mapping of subcomponents or child objects for each object. If either module sees that, we say it's consistent with the child object.

I don't remember exactly what these graphs are measuring, but they compare performance on these new metrics. On the left, the lower-level learning module's accuracy is shown for monolithic and compositional learning; on the right, the higher-level module's accuracy is shown for both learning types. As expected, the lower-level module doesn't know about the TBP mug with a logo, but it does recognize mugs and logos, correctly identifying child objects. This enables us to measure performance despite simulator limitations that restrict our knowledge of the current object being seen.

We developed ways to collect CSV stats and detailed JSON logging to compare prediction error, including the 1db prediction error, and to analyze how those are measured. Prediction space is a new measure we added, which doesn't require labels and assesses how well the learning module predicts its next input. It's a useful unsupervised way to assess performance.

This shows our final test bed. The ultimate aim was to have a new benchmark, so we had to change many things to implement that. This final artifact will go in our documentation and shows current performance in Monty with these compositional datasets. The goal was not to solve compositional modeling, but to measure and run experiments that evaluate it. In the top right, you can see different levels where we increase the difficulty of the compositional model or object being presented, progressing from a flat disc with a logo to objects with curvature.So the logo is distorted. Finally, there's the well-known logo that's bent halfway through, and how you would represent that. What these results show, as expected, is that as the level increases, it gets harder and accuracy drops. If you look at the top, you see the monolithic models versus our compositional models. The compositional models achieve better accuracy, but more significantly, if you look at the number of matching steps, they're almost 10 times more efficient at converging to the correct object. That's because the low-level learning module can recognize something simple, like seeing the disc or the mug, and send that information up, enabling the higher-level learning module to use it, rather than both modules working independently and always trying to recognize the TVP mug with logo.

In this case, both are recognizing the same object. In the top two, they're both evaluated on the level one dataset, so they're looking at the same thing. That shows the compositional architecture is already beneficial. However, we don't have clever policies to focus our movements on one child object, for example. We also didn't have the resampling that Rami talked about, which is much better since we'll often move from one object to another and need to be flexible with our hypothesis base. Those are benefits we can start bringing in.

The best baseline is the one that leaves lots of room for improvement. Today, we briefly tried integrating Rami's hypothesis resampling and actually learned some compositional objects that contain two different types of object IDs in the higher-level model. At some points, it recognized the TBP logo, and at other points, it recognized the disc, but there's still a bug. If you rotate it, some of the locations are wrong, so we think something is being messed up with the locations, which is why it looks off right now. It was exciting to see the lower-level model switch between logo and disc hypotheses and send that up, with the high-level model integrating it. There's lots of follow-up work from this week.

We opened the draft PR. It wasn't as simple as just creating some experiment configs; we had to write a lot of code and file changes, and also updated our documentation with these new benchmarks that anyone can run. We also found several bugs and issues independently in Monty that we'll open independent PRs for, which were not related to this test bed but came up as we worked with it. The legend of logos and mugs continues. Woo. So you suggested we get extra credit for finding bugs.

No.

A couple of slides on the performance measurements. Here, it also goes 20 minutes faster. Is that right? Yes, because it takes fewer steps—45 versus 350. That's great, considering we made the whole thing go 10 times slower, and you have made it go faster. If you make all the improvements, we can use them and it will feel the same. We did this week and came out very strong on the other side.

Now, do the results change if you change the color of the logo? Is it the color of the logo, the text, the font of the logo? Does that make any difference to your results, or is it basically just focusing on text versus area? That's a good question. Right now, it would be sensitive to the color. The way it's learned, the logo is with a specific color. One of the things we have in our plans, maybe for September, is to have a sensory processing system that understands 2D things better. It would pick up on things like edges, which would be independent of color, and that would be a much better model of something like a logo. Generally, with 3D objects, it doesn't rely too much on color. Color can help infer faster, but it can also recognize the same object in a different color. Since our current processing doesn't really pick up a lot of orientation features on a 2D object like the logo, as Neil said, we need to add some extra processing for that.

The generated spam—can you not use the handle at all? It doesn't understand. Three zero. Put another copyright on it, or composition. Put a logo on it and then just put copyright.

Very impressive what you all have done.