I started thinking that the little movement vectors are not really movement vectors at all—they're just a feature. It's like nature said, "Here's a feature." Instead of just saying it's an edge, it's an edge that moved in this direction to get here. But beyond that, there's no internal knowledge that this is movement; it's just a static feature. At this point in time, there was an edge moving this way—not that it's treated differently than the color columns. That was the idea. I like that idea.

Now we're recording. Do you want to start with the review? I don't care, whatever you want to do. Up to you. I feel like that already ties into the review. That's the new setup. I can draw a picture of that if it wasn't clear, or do you want to at least cover the big concept? Sorry I can't be more helpful here, but just—do you want me to do a review of the whole thing? I thought someone else was going to do that. I said, "Oh, I thought someone was going to do the review and I was just sitting here listening." Okay, this is a review of what we're working on.

Up to this point in our story, we have a theory—a biological theory and an implementation—of learning and inferring the structure of physical objects. That's been working pretty well. We can do that with remote sensors or non-remote sensors. Then we've been working on how to handle when the world is changing, when the structure of objects is changing. We're still thinking about things that are not like gushy, mushy things like t-shirts, but like a stapler opening and closing or traffic light lights changing. We wanted to add the ability to learn a model of behaviors. The idea is that, in addition to the model of structure, a physical object column could learn the behaviors of those objects, and then we could apply those behaviors to other things.

We had to figure out how to learn the different morphologies and the changes of an object. We first started with the idea that there might be a single state variable representing the state. Then we said no, that's not going to work because the states of objects are very complex, with multiple states at once and multiple behaviors happening. Then we thought maybe we could learn the behavior or the state at different points on the object. Instead of having a single variable representing the state of the stapler—which doesn't work—we could do something like we did with compositional objects and say, "Maybe on a point-by-point basis." That made a lot of sense, but it didn't work. Originally, we said maybe just like compositional objects, it's a hierarchy or something like that, but that didn't work.

Then we ended up with an idea that's fairly complicated but very elegant. In a column, we've always said there's a reference frame with a location in layer six, and a feature coming into layer four, and there's a connection between these. We pair the feature with the location, and that's how we learn the structure of the object. Then we said, what if we did the following: there are actually two reference frames and two models at once in a column.

There are two models here. The one we've already talked about is static features at locations. The other would be, instead of representing static features at some location, we represent when things change at one location. This is a change model, meaning we only store something in this model when something has changed. The other model only stores things when it's static. In vision, you might think of this as the parvocellular pathway and the magnocellular side of the channel. These are bits that only turn on when something is not moving; these bits only turn on when something is changing.

We have two reference frames. The reason is that we're going to learn models of the change or the behavior—this is a behavioral model—and a sort of physical morphology model. Morphology is a little bit of a stretch; we'll call it morphology, but we're still struggling with how to do more than morphology, like color. Anyway, we have a behavior model and a morphology model, and they're both being learned at the same time. As the sensor moves through space, they're both moving through locations in space. When something changes, we enter it in the change model; when something is static, we enter it in the static model. It's the exact same mechanism, tracking the same way.

The difference is that these two models, even though they have two reference frames, anchor separately. Anchoring represents an object. Our old way would anchor on the object and say, "This is a coffee cup" or "This is the stapler." The new model anchors on the behavior. It's only going to represent all the things that are changing in this reference frame. The advantage is that, even though they're co-learned, in the future, in another situation, I can observe changes and recognize the behavior independently of the new object. I can take a behavior I've learned from one object, observe it on some portion of another object, and infer that behavior. "Oh yeah, this other object is doing the same behavior as something I've learned before." They co-learn on one object, but they're separate because they anchor separately. That's really the issue.

This change model may only occur on some subset of the main model. If I have a bigger object like a traffic light, the only parts changing are the three lights.And so the model of the behavior of the traffic light only involves those three locations. It doesn't involve anything else. It doesn't know about any other morphology. It's just that I see three things changing, and I can place them on top of a traffic light or in some new environment in some other way.

This idea that there are two of these things working together—two different models—and that they could be separated out was a really great idea. We've speculated something like this might be occurring in the past. The physiology and anatomy of the cortical comp suggest this, but I never really understood why it would be like that.

This idea here is that we have these two models going together. That is a summary of the major accomplishment we had last week. One more detail is the timing. As soon as you said there's one more detail, I remembered. There is time, and I've always speculated—and we'll just write time up here—that time is a signal we need for sequences. I've written, speculated, and hypothesized that the time signal comes from the matrix cells of the thalamus, and it's projected broadly across the cortex onto layer one. It presents a time signal so that you can learn the timing of sequences. The change model almost always involves time; at least there's one change in time, but it might be multiple steps.

When a stapler's open, there's a whole sequence of changes in the location space of the stapler as it's opening or closing. For the traffic light, the lights go in a sequence of certain timing. The way to think about this model of behaviors—the behavioral model—is that it's really like a four-dimensional model. It's a model of space as it's changing through time. There really is no concept of time in the morphology model. The morphology model is a set of features that are static in time, and then the behavior model says, "Well, these are a set of changes. At this point, there's this set of changes. The next moment in time, there's this set of changes. The next moment, there's this set of changes." By adding time and representing it as this four-dimensional model of behaviors, you can learn any behavior at all—anything that can be represented, any combination of changing features in space, in any order.

Do we think that the melody would be represented as a behavior because it has time? We were speculating at the end of the day yesterday. I didn't want to go there, because our melody—the very first thing we did in terms of neural mechanisms—was how to learn sequences, and that's the temporal memory algorithm. The Derma paper was about ten years ago, maybe longer. One of the things with the melody model was that it never dealt with the invariance of pitch. It would learn a melody in one key and wouldn't be able to recognize it in a different key. This has always been bothersome, and I never really resolved it. I wasn't too worried about it. She kept saying, "Oh, we'll just figure out the intervals, something like that." I don't want to dismiss those comments, but then I realized this, in some sense, suggests the solution to the melody problem. You're learning the melody in a particular key, but the melody is basically a series of snapshots, and it's the change that really matters—the melody is almost all behavior. You learn this behavioral model, even though it was aligned in one particular key. Then you can play the behavioral model back, which is the melody, in a different key, and it works. This works because music is represented as notes on a log scale—it's always like a doubling, doubling, doubling—so you can basically shift the melody somewhere else in the range of frequencies, and it works.

This suggests a solution to the melody invariance problem, which I never had before, but I realize there are still issues with this. There are lots of issues with this basic idea. We haven't worked out all the details, but it's such a nice idea that it feels likely to be correct. I would say it's really likely this basic idea is correct, even if we don't understand all the details yet.

That's a summary of what we agreed to. It's not a summary of the things we are still struggling with. The magnocellular cells—are these going only to V1, or are they also going elsewhere? I'm just pointing out that from the retina, there are two basic types of input to the cortex. One is center-surround cells. The parvocellular cells are center-surround; they only become active when something stops moving, when it gets stationary. The magnocellular cells are center-surround, which only become active when they're changing. They just represent change and static, regardless of what you want to call them—whether color or anything else doesn't really matter. On the retina, something has changed at this location, or something has stopped changing and is static at the location.

It's not even the idea of movement or edges that's determined in the cortex. What features that becomes in these mini columns is determined in the cortical column itself. The basic idea is that anything that's changing could be modeled here. Anything that's changing is represented here; anything static is represented here. I want to use the parvo and magno to point out that that's exactly what we see from the retina. We've been using the magno cells for representing movement of the sensor, but this is now representing movement of the object. Remember, the upper columns have cells that are responsive to movement. Most of the cells that people are able to identify in a cortical column, even V1, are responding to movement of some sort.But the upper ones are responding to movement of small objects, which would be movement relative to the object. Down here, the movement would be the eye movement relative to the world. In the higher regions, there will also be movements of parent objects. I didn't say anything about hierarchy or parent objects—I'm trying to visualize what that change would look like in the hierarchy.

Last week, we were mostly focusing on a single column. The idea is that there are two models: a behavioral model and a non-behavioral model. We call it morphology, but it could be applied to concepts anywhere else in the world. The right way to think about it is: here's a model of things that aren't changing at the moment—a static arrangement of things—and then there's a model of things that are changing, and they're kind of overlaid on one another. We could call it a morphology model, but that sort of implies looking at physical objects, whereas it may not be physical objects. For example, some columns could be color columns, and they could be like a mini column. We were talking about the blobs in V1, so some of the mini columns would represent color. That's not really morphology, but it sort of is. It says, "Oh, there's color at this location," or, "There's a new color at this location—it changed to green." The word morphology helps us think about it from a low-level sensory, like a primary sensory cortex, but the real way to think about it is static and changing. That's the more generic way to think about it.

We were struggling with lots of issues. This is a summary of what we did. Did I miss anything else? No. I think it's a really beautiful solution because it takes the mechanism we already had and does a little tweak. Instead of storing features, we store changes at locations, and it solves the issue of being able to apply any behavior to any object without having to see it before. It also allows you to learn any type of possible behavior that you can imagine can be stored in this way. It's very flexible and you can apply this to anything. The beauty of it makes us feel like it's right, which is sometimes dangerous, but in this case, I think it's a good thing—super elegant. That's the fun, exciting part. We haven't talked about the challenging, difficult parts. That's the summary. I'm done unless I missed anything else.

Misha, you're the only one who hasn't heard this before. How does this sound to someone who hasn't been in all the brainstorming meetings the past week?

This sounds great. Are there any more details about how change is actually modeled on a lower level?

I didn't understand that question. How do you model the object changes? This is the model: it's changes at locations in space. There are changes occurring on an object, so the model is a model of changes. It's a reference—basically, the model is very much like the morphology model. It's the same idea. You can look at different locations on the object, and the change model—the behavioral model—is only going to store points that have changed.

Is it a delta between features? I think the right way to look at it is just a change. You don't really know; it doesn't record what it was. My thinking now is: first, we're going to detect a change, and what I want to store is what is now there—what is now at that location. For example, if it changed from red to green, I would just say at one second, a point changed to green. I don't say it was red; I just say it changed to green. If there's an edge that appears at this point, I would say I don't know what it was before, but right now there's an edge here that got here by moving in this direction. It's not actually recording any kind of delta at this point. That information could be inferred. You could assume that if it's not changing, it would be the same. If I say that something changed at time one, then whatever it was before was what it was before. If it changed at time one to green and at time two to yellow, at time two I don't record that it was green, but it is green because the last thing it changed to was green. This is constant in time for that.

It's sort of like the same object, but one is masking for changes and one is masking for static features. A useful visualization is having the two reference frames above each other: one is features at locations, and one is changes at locations, which we've drawn a few times. Maybe that's it. I was trying to do it on a virtual whiteboard, but I think it's just more distracting. We have the stapler, which is features at locations—so, all right, we have an edge here and here and here, and there might be a color at all of these locations too. Those are the static features. Once the stapler starts moving, before the stapler moves, there's nothing actually stored in the behavior model. This is a different reference frame, but they're collocated. As you're moving your sensor, it moves in sync through those two references. As soon as the top of the stapler starts moving, the upper model says, "Hey, things are moving at this location." Then it would lay down: now there's an edge, now here's an edge, now here. At the next time point, you would say, "Now the edge is here and here." Not just an edge, but an actual moving—well, this is the thing I came up with yesterday. I'm going to try exploring more today. It was very intuitive for me and everyone to think of it like, "Oh, it's a moving edge."And of course, it is a moving edge, but I think the way to think about this—at least, I'm going to push for it for a while until you convince me otherwise—is that it's really just a feature. The feature itself could encode, for example, "I got to this edge by coming from the left or from the right," but that's not what happens. The column doesn't know that; the column just says a feature appeared here. There's no ability to know directionality. Isn't that kind of going back to the snapshot view we discussed last week, where you have a series of more static snapshots over time? But here, we have a change at a location. We're not talking about the morphology model; we're talking about the behavioral model right now.

What was described there was features at a location at a point in time. Imagine, in the stapler example, you could think of it as a series of snapshots, but it's really a flow through time. There's a beginning and an end to the sequence, and everything in between is just the flow of time. The changes are where it began and where it ended. I'm not sure it's really snapshots. In fact, the idea I just suggested—that the behavioral model only records behaviors and the static model only records things that aren't changing—would imply the following: if I have a stapler that always moves continuously and then gets to the top, I would learn what the stapler looks like when it's closed and what it looks like when it's open, because it stops at those times and I can observe it. But I actually don't learn what the stapler looks like in between, which is kind of weird. I don't really learn what the stapler looks like in all these in-between positions. It's basically: it looks like this, then I have a move, then I have this behavioral model that says all these things are moving, and now it looks like this. That seems a little weird, but I like it if I can make it work.

Is it possible to revisit the issues with storing feature changes at locations? I feel like, at least until yesterday, that was the view. That might be both a useful recap and a segue into what challenges you're referring to. That's my question to you, because you said this alternative is to address some issues with that. I think it's a very small tweak on how we talked about it before, but it does change the way you think about it. It's still a small tweak that changes things: for the static model, many columns would represent oriented bars or color. They represent the orientation of a feature at this location, or color. Then we extended it to color, and that's where the problem started occurring.

For the behavior model, what actually comes in or is detected from the magnocellular pathway is changes. We have a lot of activity if there's a bar moving in a certain direction. The minicolumns would represent, for instance, movement in one direction. If I understand right, what you're saying is that this column doesn't store anything about where it came from or where it moved to. It would just store that the feature means that, but the column doesn't know anything about it.

Here's the problem I saw: we started off thinking about these moving edges, and that made sense—the staple is moving, the morphology is changing. Maybe now we know how the edges are moving. Then we added color. We looked at the color blobs and said, "There are columns that represent color. They don't really have movement; they just change one way or the other. They can be green or not green, or change to yellow, or whatever." The concept of movement wasn't really there; something just appeared or disappeared. The minicolumns there detect and represent change.

Then I asked myself, would these minicolumns be treated differently? Why would the ones that represent moving edges be handled differently? Would we say, "We know that's moving, so we can do some sort of path integration," and the others don't represent movement? That's possible, but the more generic case would be to think that the minicolumn feature could be an edge moving in a direction, but it really just means there was an edge moving in this direction and now it's here. The cells don't know that it's movement; it's just a thing. I could have the behavior of "there's an edge that appeared at this location," so it wasn't there before, now it is. That would still describe the behavior. If nature added a motion part to it, then this feature is two separate features: an edge at this location that had been going this way, and an edge at this location that had been going that way. They're just two different features. Then it's much easier to infer behaviors, but the neurons aren't taking advantage of the fact that it was moving. It's just dividing up the feature space into more refined features. All the minicolumns could be treated the same: a feature appeared at this point in time, or a feature disappeared, or something new is there. I don't need to make a distinction between color and moving edges. The movement will be captured in time.

We don't have to represent movement in the model itself and then apply the movement to the locations of the static model. We just represent that feature change right now, and that's nice because we can imagine learning a behavior where we don't have movement—we just press a button and a feature appears over here. It just appears; it doesn't move there. We would still store that in the behavior model without detecting movement. The color example was like that.There was no green dot here, now there is a green dot. It didn't move; it just appeared. The edges are like that too. If an edge just appeared instantaneously, how would that be represented in the behavioral model? It couldn't say which direction it was going from—maybe it would encode that, maybe not.

The direction of movement is encoded by the sequence in time, not by distinguishing direction at once. We can distinguish the direction of movement by looking at the cell response, but the model doesn't know that. The model only knows what time tells it. Here, we wouldn't store the direction; we would just note that at this point in time, a feature appeared here. The behavioral model says the feature itself is an edge that was moving in this direction. Imagine when edges move to the right, they're green, and edges moving to the left are red. It's like saying this is a green edge, this is a red edge. It's part of the feature being stored, but there's no way to take advantage of it. The neurons don't know how to use that movement idea; it's just a feature that appeared. They can distinguish the two directions, but they don't know it's a direction. It's not like they apply it to something—they just know it's different.

I'm going to say when something appears. Why wouldn't they know it's a direction if those mini columns can be sent? There's no mechanism taking advantage of that knowledge. There's no mechanism that says, "These are moving, let's path integrate in this direction." It's just a feature. If you're trying to infer where in the state space you are, it's helpful to know that in that part of the state space, edges were moving in this direction. It's very helpful to differentiate different behaviors, but we're not taking advantage of that knowledge. No one's using the fact that it's moving to do something with it. It's not like we're going to project where this edge is going to be next. Whatever appears next, appears next.

We do detect the direction in the mini columns and can associate the directional change with the location in this reference frame, but we don't apply the direction to anything. That makes sense—we're not applying it. I used to think these movement vectors would be used for path integration and predicting where things would be based on these movements, but no, it's just saying, "Here's a feature, and here's another feature." Path integration would be cool if it worked, but it feels like if that's ever going to happen, it will be a slow, deliberate process. It's hard to imagine how you'd apply it to all these points.

What got me to this idea was thinking about the color example and asking why color would be treated differently than edges. I want to be able to put any attributes I want in the mini column, and they're all treated the same. If a color is just changing, like red to green—is that almost the same? This is not red to green; it's just now it's green. I wonder if that's like the open and closed state of the stapler—those two extremes are essentially encoded by static features, not by change. I think so.

If we had something where color was slowly changing, you actually don't notice it. This is a visual illusion. You look at something that's blue, and then slowly it turns to green, and you have no idea it changed because it didn't change enough to record it, so it's just lost. You can't see us if we don't move.

If we all understand this idea, we can move on. Everything we know about how things move through space is mostly encoded in the time slices of the behavioral model, which greatly assists our inference of what behavior is going on. If we're trying to recognize behavior, it's greatly assisted by knowing which way the pieces are moving. If I saw someone dancing with motion capture dots on a body I don't recognize, and I just showed you one set of dots, then blanked the screen, then another set, and so on, you'd have much more trouble understanding the behavior than if the dots moved. If the movement of each of those dots is encoded as a feature, then the behavior pops right out. It's very helpful to have that knowledge to infer something, but we're not taking advantage of it in terms of applying motion to the object.

That's the idea. It may not be right, but we can go with it for a while. This just changes my thinking a little bit about the models.Now we can think about any attribute, regardless of what it is. It could be high-level concepts; it doesn't really matter. Many columns represent the presence of an attribute, and the behavior model just says, "Okay, now there's an attribute here that wasn't here before." That's simple—maybe not right, but simple.

I'm imagining a scenario where you learn the stapler and its behavior. Later in life, you walk into a room and the stapler is in an intermediate position. How did you recognize it? I don't know. It's a good question. I don't know the answer. I also don't like the idea that I have to learn every position of the stapler. That seems impossible. If all I have is the stapler opening and closing, one of the problems is you can't learn every position as it's moving and the morphology is changing. You don't really have the opportunity to learn what its morphology is in those in-between points. That was one of the things we were struggling with, right? So we got rid of one problem, but now we introduce another: how do I recognize it when it's static in between?

I think you would actually take a bit longer to recognize it. What you could be doing is recognizing the top of the stapler in one orientation and the bottom in another, and then you have to move yourself, follow the path of it using the behavioral model, which takes some time. We can try some extremes here. Imagine if the stapler opened very quickly—there's no possible way you could ever learn the in-between points. You just can't. But you could imagine it. I think Vivven probably has the right idea: it would probably take longer to do that.

It's also interesting—there are examples where a change happens very quickly and there are actually no intermediate states, but the brain perceives that there are. It perceives it moved. Did we look at some examples like this last week? I forget, but there are examples where something appears, goes away, and something else appears, and your brain thinks it moved there, like Christmas lights when they light up, or movies at 24 frames per second. That's an example, but it works even with fairly slow animation. These are things that you wouldn't expect to know to movement. It's not like I show you my hand here and then here; it's like they show you the letter A and then the letter A again. We don't normally see letters move, right? Maybe if I saw it in a movie, I might say, "Oh yeah, I know their hand is moving, I'm going to expect it to continue moving." But this is an object—you don't expect it to move. You show it again, and the brain says, "Oh, it moved from A to B, from here to here," when it didn't. But you perceive the motion. This is just a data point.

I could actually imagine in-between states that I never saw. I could say, "Can you show me what it looked like between these two points?" "Oh, yeah, sure, I can." So, it would be right here. You never saw it there. That's because you're anchoring to a behavior of the movement. You saw these two features and you think this is the behavior that happened—the movement. But what does it mean? I didn't actually observe it. All I observed is there was a state of the object here, and then another state there. Some feature moved from point A to point B, and the behavior that actually occurred is this feature disappeared and another appeared, yet we perceive it moved.

Then I could ask you, "What would it look like in between?" and you'd be able to tell me. It seems trivial, but you anchor to the behavior. I'm just using this as an example: I can predict what it would look like between two states that I never actually saw. So, this is the question: how do I predict what the state will look like if the stapler goes "whoop" and "whoop" and "whoop"? How do I predict what it will look like here? You might say, "Well, I saw it moving through that position, so I learned what it looked like there." But the example I just gave says no—you never saw it in between. You can infer what it would look like, but it was never actually there. The stapler never actually appeared in the middle.

If this is right—and I'm not saying it is, but if it is—and we only record static things and changing things, then the only time you learn is when the stapler is closed and when it's open. When you're moving in between, there is absolutely no morphology model; it's just movement models. That's like the two dots moving, and you can infer, as I said, but it would take longer and you'd have to infer what it looks like in the in-between state, obviously using the behavioral model. The behavioral model would encode all those intermediate states because those features were changing throughout their cycle. The behavior model doesn't code it, but—

So now this gets us back to the basic problem we were struggling with: how do I predict if I've learned this model with a black stapler of a certain shape, and now I'm looking at a pink stapler with a slightly different shape? When it opens up, I can predict what it would look like when it's open. I will be able to predict what the pink stapler top looks like when it's in this open position, even though I've never seen it like that.How do I do that? In general, it feels like that kind of path integration or slow thinking to imagine that is definitely easier with some sort of child-like representation or breaking off of the subobject, even if that's happening in one column. We're not trying to predict point by point how everything is changing, but rather, in the behavioral model, the stapler top rotates, so I approximate that rotation. 

We looked at two ideas related to that. I proposed that this could be done by having two columns in a hierarchical arrangement, where the parent object is the entire stapler, but as part of the stapler starts moving, we invoke the model of the stapler in the lower column, attending only to the moving part, as if the rest of the stapler is occluded. The lower column watches the entire stapler rotate, but the rest is excluded, making good predictions about the top. Vivian suggested that maybe this could all happen in a single column, alternating between the full stapler and the top of the stapler, keeping both models in the same column, including part of it and rotating it.

I wrote some thoughts on Slack about how that could work. It's a combination of those two ideas, accepting a parent-child hierarchy but trying to put it in one object. One way to view it is that the input into L3—the movement and all that low-level information—tells you where you are in some behavioral state space. Attending to just a part of an object gives you a region-level state, not a location-by-location state or a global state for the whole object, but a state for a location region.

It's almost like having two behavioral states, and the high-level one, which doesn't cover the whole object, is what you condition on when predicting with the morphological model based on its rotation. If I move, what am I going to observe? Using change detection, can I isolate a region of the parent object by enclosing the change and say, "Okay, that is a separate item"? In order for that to work, when you go from one subregion to another, you have to shift the attentional window. You could be attending at one point to the top of the stapler, then to the base plate. That region-level state is necessary to predict what you'll see following a saccade. The very low-level changes coming in at L3 are necessary to infer where you are, like, "I'm in the top of the stapler and it's moving in X direction."

That doesn't address the question we were asking: is it in a single column or multiple columns? I was thinking as a single column. What you're saying is that we have the information to break off a part of the stapler to consider as a movable subunit. That could be ascribed, at least in a single column at a given point in time. The whole object is always "stapler," but the behavioral state is more specific to whatever region is changing and being attended to. You can only really make predictions about that. Wouldn't the behavioral state include—I'm thinking about the stapler opening with the behavior I mentioned, where instead of the whole top moving at once, part of it opens like a little door as it goes up? There are multiple moving parts. In that example, there were three moving parts: the bottom of the top and the two little doors that flipped up from the top. The behavioral model includes all three of those things.

The behavioral model of the stapler would include all of them, I think, but maybe I'm wrong. Because those individual pieces are moving, every piece has its own surrounding moving edges, which you could use to isolate the three components: one component, another component, and another component. I think you're saying I could attend to those three—I could attend to one door opening, the other door opening, or the base of the two doors. In that case, you probably couldn't understand the whole movement with a single model. Ultimately, I would think, "I'm going to learn that this is a compositional object." A stapler has the foundation, then a plate that rotates up, and two more plates on top that open in different ways. There would be four components—three child objects to the stapler—and those three child objects can move independently, but they're moving in sync too. They are moving in sync, but I might still view them as independent objects. You want to apply a general hinge behavior to both doors, but you also want to represent that the movement of one implies movement of the other.So I think the parent object—one way you might think about this is, ultimately, you might say a model of the stapler says, "I have a stapler with three child objects." The three child objects are the moving plate, which is the bottom or the top, and these other two parts. Now I've described it as a parent with three children, and the children are moving in a certain way. That's not how I started, because I didn't know that until these things started moving. This gets back to the idea: when do we break off a child object? It seems really nice that you could break off a child object if part of something moves independently. That would be a good reason to call it a child object. You'd want to say, "Staplers were one thing, and now they're four things," or, "They're one thing with three child objects, and I have new subobjects," something like that.

I also wonder if maybe it's something in between, where depending on how complex a behavior is, we're more likely to rely on hierarchy with location-by-location binding. But with some behaviors, it's sufficient to do more localized attention within a column. It seems like you have to do the localized attention before you've learned that it's a child—before you've learned separate child objects. Eventually, you could learn to represent the top of the stapler separate from the bottom and then solve it with hierarchy.

Can I talk about an adjacent issue we've discussed? By the way, is it possible to put on another laptop screen just to see you guys? I feel like sometimes people naturally use hand motions, and it would be useful. Thanks. This is an idea we already talked about, but I think it might be essential to solving all these problems. This is the idea of where certain attributes get represented.

I was thinking about it in terms of melodies. Let me start there because it's an easy example to make it clear, but the same problem exists in physical objects. In the melody example, the melody consists of a series of intervals—that's the definition of a melody. In the behavioral model, it just says there's a series of intervals at certain timing. The behavioral model can track multi-art voices and things like that; it all works. But when you hear a particular melody, it's in a particular voice—meaning there's an instrument, a person singing, or a dog barking the melody. That voice is a complex thing: it's an attack, it's overtones, and how those overtones decrease over time. That's what makes a dog sound different than a saxophone or something else. In some sense, that is the feature that's implemented at that point in the melody. Without any additional knowledge, I would assume that feature appears at each note in the melody. If I'm hearing a saxophone, unless I know or have learned that it changes from a saxophone to a dog barking, I'm going to assume there's a continuity of the saxophone. Yet the saxophone sound isn't part of the melody, but it's also itself a time-based dynamic thing. It's not a static feature. I can't represent it in the morphology model because it has its own time, and I can't recognize a saxophone note without having its own time signature and changing its own behavioral model. It's a much faster one; it occurs very quickly in time versus note to note. It's faster than that, but it's still there.

So I'm asking myself, where is the feature that is not part of the behavioral model of the melody? It's just a feature that occurs at each point in the melody, but that feature itself requires a time-based model. Do you think, just for recognizing the temporal signature of a saxophone, that could be done more subcortically or as pre-processing of the audio? There would be pre-processing of the audio, but I don't think it could be handled subcortically. I think this is the more general problem, and it relates to the problem we were dealing with: how to represent features on an object.

Imagine our stapler—as it was opening, there was a little symbol on the side, and that symbol was the Starbucks symbol. When it's halfway open, it changes to the Pete symbol, and when it's halfway open, it changes to the Phil symbol. That's not something like black or something simple; that's a new feature that appeared, and it's a complex thing, like a child object. I can say there's a child object at that location that appeared, but those are very complex child objects. I can't represent them in a set of mini columns, unlike edge or color. I can't represent coffee logos or logos in general, or images in general, by a set of 100 mini columns. It's just not enough information.

That tells me I'm relying on a child object recognized somewhere below, yet I don't know how to represent it in the model—in the parent object, in the stapler object. I don't know how to say there was a Pete's logo at this location. I don't know how. This was another thing I was posting on Slack about. It feels like if you use multiple mini columns—essentially an SDR across many columns—rather than assuming one mini column is dedicated to a particular type of child object, that at least makes it more tractable. But I don't think I appreciate that in V1, we look at many columns, and they tend to correlate with a very simple feature or an orientation. But it may be that, in general, features are represented by active mini columns. It could be, although it doesn't feel right. I don't see evidence of that anywhere. Mini columns tend to have well-understood receptive fields. They tend to represent things that can be—there aren't too many of them. Everywhere I look, that's only really in V1. Well, I don't know. If I—then I think a lot of the mechanism wouldn't work. How does this—then you would still use neurons within many columns to define a unique feature at a location, right? Even in the best scenarios, even if I have a millimeter-wide column, I only have several hundred mini columns. I don't have thousands; I've got several hundred.And mini columns tend to be active not as SDRs but more like, if I look at the edges, there might be 10 or 12 mini columns representing edge orientations, and at any point in time, three would be active—think three or four in that range. It's not like I have this big vector and I'm sparsely selecting from it. Even if I use all the mini columns in a column, I don't think there's enough—maybe a few hundred at most. That's not really enough to represent meaningful SDRs. I would need a minimum of 20 active mini columns out of 200, which is a 10% sparsity. It just doesn't seem right. You might be correct, Neils, but I don't think that's right. It feels like something else is going on.

Again, think about the melody. The thing I'm looking at actually has its own time basis. I can't represent the saxophone note with a set of mini columns in the static model. I don't know how to do that. It seems like I have to be using a hierarchy of some sort, and even then, I just don't know how to make it work. It almost feels like there's a separate representation of the saxophone note stored somewhere else, and I'm just reusing that in every attack, assuming it's the same thing over and over. But it's stored separately, not as part of the morphology or behavioral model.

In the melody example, I have the change model, which is the actual melody, and the morphology model, which would be the actual note I'm hearing at this moment. Somewhere else is saying, "What should that note sound like?" It just feels that way to me. I might be wrong, but that's how it seems.

It seems like part of the solution is that the behavior model only encodes changes. Even if I get input for a completely new instrument I've never heard before, playing a melody I recognize, that feature input to layer 4 would just remain constant if there's no change encoded in my behavior model. Maybe that's what you're saying—if it's a novel thing, like I've never heard dogs singing a song before, or a voice I've never heard before. I'm not sure where you're going with this. That would solve a melody that alternates between piano and saxophone notes, I guess, but you can learn that transition. If there is a change in the feature, it's stored in the behavior model. If it's not in the behavior model, I keep predicting the same features.

What I'm struggling with is how to apply the saxophone to each note—how is my brain predicting the saxophone note if I haven't seen a change? Where is that saxophone note occurring? Where did I store it? Where is it temporarily held? It would just be temporary, but where is it? Unless I learned this melody the very first time and it's a really famous saxophone solo, then I would know there's supposed to be a saxophone for that part. But it can't be stored, because you can hear it in a new voice or instrument and still predict the same signature sound.

You would just have a memory or constancy in layer 4 of whatever feature you get as input. I would say, first of all, it could be just a random set of stuff, but if I don't represent it in mini columns, I can't represent it as an SDR. The whole point is that the mini column is the unit of change, not the cell. If something is going to change in the object, it's recoded as the unit of the mini column, because I have to do these changes in high-order sequences.

The mini columns have to define everything. I can't rely on an SDR to do that, and I can't transition SDR to SDR. I have to represent the saxophone as a set of mini columns, and Neil is saying I can have a lot of mini columns, but I don't think we have enough. In my mind, the mini columns have to represent something physical—something that can be detected, not something inferred from a sequence. It's got to be like an edge, a color, a texture, or a sound starting. I haven't heard an answer to this yet.

There could be some analogy to the way we talked about color on Monday. The center of the column would represent morph or color, and the outer, inter-blob areas would represent morphology—some kind of separation like that between the model of the intervals in the melody and the texture. The problem is, I still have to represent the saxophone with a set of mini columns, and the same set would have to represent every other instrument I might hear. There aren't enough mini columns to do that, and it just wouldn't work. There's no single thing I could detect that says it's a saxophone; it has to be a combination of things over time.

If you had a low-level column where the sound of a saxophone was almost like an object itself, that's what I'm getting at. But stepping back, the number of mini columns feels like a general issue with anything hierarchical. If we're going to pass the object ID from L3 to L4 in the next layer, we need to be able to uniquely encode that. This is a general problem. We didn't really deal with it in the past; we just waved our hands and said, "Oh, we're at some edge or point on the object, there's a feature there, the logo is there." We didn't ask how the logo ID gets encoded in mini columns. I just assumed the logo would be some high-order state of the mini columns that are active, like there's an edge, and the individual cells I pick in there are going to represent the logo.So, but that is weird because if I was at a different point in the object, I wouldn't pick the same cells. It didn't work. We never tried to get it to work. I think there's a general problem for our morphology model. We don't know how to encode logo in layer 4, something like that. What we're struggling with in these behavior models is the same problem.

Just to clarify on the saxophone example, the issue you're pointing out is that detecting the saxophone is itself a temporal behavior, not a problem of constancy. If it was color, it would be easier. The reason I brought up the saxophone is because it proves you can't do that in layer 4. Layer 4 cannot learn the sound of a saxophone over time. I can't do that. It proves I can't just treat it as some static SDR that I pass in. I can't represent it in layer 4. It just proves I have to have another place that's doing inference over time. That's just because the saxophone is an over-time kind of detection. If it were just color, you could represent it.

Here's what I'm getting at. Take a column—the mini columns in my mind always have to represent something you can actually name. It's a thing. They're not part of a distribution. Mini columns are always like: it could be the center is moving in this direction—that's a thing. Or there's an edge at this location—that's a thing. There's a color at this location—that's a thing. I can enumerate all the colors, all the orientations, all the different directions I can move. So each mini column can mean something. It's not a distributed representation. Yes, there are multiple ones active because when I'm moving in a particular direction, multiple mini columns will be active because some are moving like this, some like this, and some like this. But they are not distributed representations.

Do we know that for sure? Probably most experiments that have evaluated that have used very simple, you know, a Gabor filter moving. We don't know that for sure, but our mechanism for sequence memory assumes it. If you say the mini columns are distributed, then the sequence memory can't—couldn't a set of mini columns predict a set of mini columns rather than one to one? The sequence memory has this basic property where you represent something in many columns and then you represent it uniquely in the cells. The thing I'm representing always has to be represented in many columns, and then I can say, okay, that thing I can now represent different things. I can say there's an edge to the object, and now I can say, oh, that edge—I have many different ways of that edge occurring at the left end of the tiger, or at the front end of a stapler.

Those are like the—what was the name of that guy? What was the name of those cells? Border ownership. Border ownership cells, thank you. It feels like it would still work. In some of the HTM papers, you do have multiple columns active, but with only some cells active. We did this when we first wrote the temporal memory algorithm. We didn't understand a lot of this stuff, so we basically said, oh, there's this mechanism, we learn high-order sequences, but always when an input came in, like an element in the sequence, it always invoked the same set of mini columns. It didn't invoke a set of cells, it invoked the same set of mini columns.

Why wouldn't that happen here? Let's say the saxophone—to represent all, I have no way of representing saxophone versus oboe versus dog versus xylophone. I guess, okay, maybe the numbers just don't work out, but it seems in principle the sequence memory would still work. I could learn any arbitrary sequence of SDRs, and that's what sequence memory does. But the whole point of the sequence memory is you have to come in with a particular item that you're now going to represent in many different contexts. That's the whole point of it, which is if it was a child object—whether that's saxophone or logo or whatever—let's say that activates mini columns 4, 22, and 109. Every time you see logo, it's going to be 4, 22, and 109. But I'm saying I don't have enough mini columns to do that. It's not going to work. A mini column can't be—I just don't have enough mini columns. It seems to me like I can define certain things in an object: its edges, its colors, maybe textures. But when I assign the Phil's logo, I can't represent Phil's logo in a small set of mini columns. From every other possible feature, there are thousands and thousands of objects that could be a feature on the edge of the stapler. I can't represent them all in mini columns. There just aren't enough mini columns to do that. I don't see a way around it. I'd like it to work, but I don't see a way around it.

It feels like for this reason, not every column could learn about every object. No, it's going to somehow rely on separation. It somehow says that until further notice, keep assuming the same thing that happened before—it's the same logo or the same sound of an instrument or whatever—but that is not going to be detected in the parent column. It's going to be detected in the child column. The child column—just think about our compositional model, how we do composite objects. There's a back connection between the parent and the child, and that back connection is very specific. It says, on this location on the parent, you should be on a particular location on a particular child object. The parent doesn't take any extra storage in the parent object. The parent object doesn't know what that is. It just says, I'm at a location you learn to associate with whatever you are doing down there.We have the right bandwidth going backwards, meaning I can associate a very specific object with a specific location on the parent object as long as I'm going backwards. But when we go forward—associating a particular child object to a location on the parent object—that's the part we're struggling with. There is a connection between the parent and the child; it's just that I'm struggling with how and where to store that child going into the parent. Maybe I don't need to. Maybe you're not storing the full child, just something that guides the lower level to integrate the sensory input and the higher-level guidance that's coming. Isn't that what I just described—the back projection? The parent can tell the child, but the parent doesn't know what it is. For example, I'm a coffee cup. I don't actually know what's at my location here. I just know that at this location, someone else may have learned there's a logo there at a particular orientation or feature. So, as the coffee cup with a logo, I don't know what that is, but you do because you've already learned what you're doing at this location. So, parent to child: you take care of it. That works in the back direction. I think that's what you were just describing. There is an association between the parent and the child where the parent doesn't really know anything; it just says, "I'm telling you my locations. You figure out what was there." The parent knows just enough to guide—it's very specific, like this is a location on the cup and nowhere else. We're right here on this point on the cup. You can associate—well, you were a logo at some point on the logo. The backward connection works. It's like if I was in the melody, I could tell the child, "I don't know what voice this was, but you did, so you keep going." I think we talked about this maybe two years ago when we discussed hierarchy and whether we actually need to learn the connection both ways. We did talk about this. At some point, I argued that it's a bit inefficient to actually store the location of the child on the parent and the parent to the child both ways, because you're encoding some redundant information. What if the parent doesn't actually—what if we just have the connection from the parent associated with what the child should be perceiving, but the forward connection only tells the parent that a feature was observed at that location, or maybe something general about this feature, like there's a round feature there or an elongated one? Let's follow this a bit. Layer three is the "input" to layer four in the next region, right? That's the output of the child—layer four. Right now, we're talking about layer three representing the moment. I'm talking about representing maybe behaviors. My point is, what if we said we're not passing up the object ID, just the orientation? Or maybe you just pass the orientation of the child object. Well, I'd have to know the orientation of the child, and maybe I'm actually passing up something about behavior. Maybe it's important to tell the parent, but I don't know. Can we make this work if the parent or child doesn't know what it is? You're saying just the orientation. That would be clean, because you could still make predictions about the features of the child object since the parent has learned the backwards association. You can still recognize the higher-level object just from the relative orientation of the child object. For example, if you have the brute face, the banana mouth still needs to have the correct orientation, but it doesn't matter for the parent object whether it's a banana or a carrot or whatever. This is a pretty radical idea, but not in a bad way. It just means we were thinking about it all wrong. This idea says the child object doesn't actually tell the parent object what it's viewing. It just says, "I'm viewing something at this orientation." Then the parent object says, "Well, whatever you're viewing, here's where I am. You do as you will." I like that. It definitely sounds possible, but it's also kind of weird because, just thinking about how you conceptualize a compositional object, at some point you do cognitively know that the low-level pieces are part of the high-level piece. It's funny—do I actually perceive the child and the parent at the same time? Do I perceive them as a whole, or is it like, "Oh, I'm at the coffee cup. Look, there's the logo. Now I'm looking at the logo. Now I'm back at the coffee cup"? There are lots of examples where there are child and parent objects, and you don't actually notice any of the details of the child object until you attend to it. You can get rough information about the child object from skip connections from the sensor to the larger, lower-resolution receptive field projected directly to the right level, telling it some color information. 

Are we debating here if the hierarchy is represented in one column, or just whether the lower-level column actually sends an object ID to the higher-level column, or if it just sends orientation of the child object and then the parent object learns an association—a backwards projection? That tells the child what features to expect. Let's think about the melody example. The melody is—the voice is a mini sequence. Let's say the child object learns this mini sequence and can play it back. At the beginning of the mini sequence, when the attack of the note occurs, that's when I want to tell the parent, "Hey, something happened here. There's a new thing here." The parent says, "Okay, there's a new note occurring. I don't know what it is, but there's a new note occurring. I'm in my sequence here." It's associated back at that point with the location. I have to think through this.There's an association back to the child, which essentially says, "If I'm predicting the beginning of a new note, you should play back the sequence you last played." The child object assumes it's going to play the same thing repeatedly. Every time you get a new note in the sequence, it tells the child object, "It's a dog barking," or something similar. We'd have to work through the details on this, but it would be reinstating more of a behavior in the low-level. It would kick off that behavior, starting at the beginning of the sequence, because I want to play the whole dog bark or the saxophone note.

I'm telling the parent at this point that something started. The parent doesn't even know what it is—maybe I haven't even identified it yet. The sound of a saxophone takes some time to play out, but it's important to note when the melody starts. You want to say, "Okay, the note started now," or "The voice started now." It will play out in the child object, but the parent object notes that something started. Later, the parent object can say, "Play that sequence again from the beginning," expecting the note to start here, and the child responds, "I'll play back the saxophone note." There are a lot of questions about this, but I think it's a good idea.

By the way, this doesn't mean we no longer do temporal pooling in the child object and the parent object. We still might label these things. I think we have to—like, we're now no longer setting up the name of the item to the parent. We hypothesized we had to do temporal pooling to come up with a name for this object and then pass it to the parent, but now we're not doing that. We're just saying an item occurred, and we have to calculate its orientation or something like that, but we're not telling the parent what it was. I'm not telling the parent, "Oh, this was Phil's logo," or "This was a Starbucks logo," or "This was a saxophone." You still do temporal pooling and vote on that.

I'm saying we don't have to throw away temporal pooling completely because one of my tenets is the brain has to do temporal pooling in every column; otherwise, you could never name anything. So, we'd still have to do temporal pooling, but the output of that is not being sent up. It's being sent laterally. In some ways, I like that. There are a lot of issues about sending names up the hierarchy. I'm just walking through the consequences of this. From a compositional hierarchical point of view, I no longer need to have temporally pooled representations from a child, but I still want that for voting and other reasons. So, we're not getting rid of temporal pooling; we're just not passing it to the next column. I'm just walking through the consequences.

So, what exactly are we sending up? Just orientation. The hypothesis right now is we're sending up orientation—whatever that is. Orientation of the first part of a sequence, or of the whole object. Basically, a child is saying, "Of the frame, something occurred here." The child is basically sending up an orientation: "At this point, whatever we're looking at, here's the orientation of my object." And that's all it's sending.

I'm thinking of a message, and the parent is basically sending back, "Okay, we're now going to connect between you and me." So now, when I'm at this location, you're going to invoke the exact right point in your object, but no one knows what the other is doing. They're in separate rooms.

Could a compromise be that it sends something up, but it's not a super unique fingerprint, or at least L4 is insufficiently—well, what are you trying to prove? Why go there? I think it's to bias, because we can recognize a face made of fruit, but I'm pretty sure we would recognize a normal face much quicker. That information can be useful to narrow down the hypothesis.

One thing you could do, or even with the new mug, is: how do you recognize the Numenta mug unless you pass at least some of that information up? Otherwise, you just have an orientation of a logo. Sure, you have the skip connections, but those are kind of too coarse to learn another. Let's assume the only way SDRs work is by association. So, I could associate the logo ID with something in the cup. I could associate it with a location on the cup, but that's a lot of memory. You can associate two SDRs. That's what you can do. You can say, "Okay, two people have two representations, and they're kind of coincident," and I can say, "My bias is yours." It's just like voting, but now I'm saying something like the logo could vote on "cup with logo," something like that. We just can't represent it in layer 4. It has to be SDR to SDR.

I'm with you. I think you're right, Neil. I think we have to still somehow bias the cup. Or it's just a highly redundant situation: I cannot decode the logo from that SDR in L4, but if I have the representation "mug" in the higher level, then the bit pattern is sufficiently telling me, "Oh, it's probably the logo with the mug." That exact same bit pattern may also appear for guitar or Phil's fills or some other thing. So, it would only get confused if you were trying to reuse the same bit pattern in a similar context, which maybe is unlikely. I don't know.

Then, I guess the more familiar you are with an object, maybe the more neural resources you can dedicate to actually having a unique L4 representation. Maybe the default is it starts at just orientation and then gets more specific. Let's walk this a little bit, taking your idea and walking back some of the things we said.

Child objects have a child object ID. I'm looking at the Numenta logo. I could associate that with any kind of SDR in the parent object. For example, every SDR in layer 4—which, for the moment, let's assume is the edges of the object, or edges of anything, or just any location—I could associate the logo object ID with any specific location in the parent object that has the logo on it. Going back to what we were trying to get rid of, I can associate it with that.

So, I could say, "I know that doesn't work." I'm trying to make sense of what you were saying. Does it make sense what I was trying to say? I accept that L4 probably doesn't have the capacity to be like, "Oh, this set of mini columns active means Numenta logo, and this set means Bill's coffee cup," or whatever. But if there's some overlap in them—like, this set of mini columns means Numenta logo, and it also means chair, and it also means basket or something—no, I'm not going there. I don't want to push back on that. You might be right, but that doesn't strike me as correct. In my mind, I guess I'm just thinking it would bias the high-level representation.

Let's go back to what I just said: you can associate SDRs with SDRs. So, I have an SDR someplace, and I have an SDR someplace else. We can associate it one way or both ways on the map. This is a really good mechanism. It's high capacity. We can do millions of these things—well, hundreds of thousands of them—between two populations of cells.It's great. This pattern can invoke a pattern over here. What I think you can't do is go from an SDR to a set of mini columns, because the set of mini columns is not an SDR, and I don't think you can do that. It's a low-dimensional SDR, but it's not really, because the mini columns themselves have to have a specific, constant meaning. You can't say this mini column is now part of one SDR and then, a moment later, part of another SDR, as long as it reoccurs when that feature or object occurs in space. If that's not specific, you lose the ability to do high-order sequences. I need to be able to do high-order sequences with mini columns. Do we need to be able to do it just in the mini columns? If we have the behavioral reference frame and the structured reference frame, that's where the sequences exist; it doesn't need to be only in L4.

In my mind, a layer of mini columns is a way of taking a representation that's not unique and making it unique in context. I can't make the mini columns be unique; they have to be non-unique. The set of mini columns has to represent something that's repeated over and over again. Now, I want to represent a unique context, so I can't use the mini columns as an SDR. I could still learn sequences of it, but it would throw away the idea that you're representing something differently in context. This is true for the grid cells. You're representing locations, but only when you look at the set of components is it unique in context. There's a feature in layer four, but in context, I have a unique way of representing it.

I just don't want to go there. I keep trying to make it work, but I don't think I can go there with mini columns. Maybe I can try and write what I'm thinking. You could try to write it down, but I don't think it's going to work. I do think the idea that we could take an SDR in the child object and associate it with an SDR in the parent object makes sense. This is the bias of, for example, a banana doesn't suggest a face, but a nose does. I can still recognize the face with just a bunch of orientations, but if I see a nose, it's more likely to suggest a face.

So then I'm asking myself, if I have a child column and a parent column, and in the child column I have an object ID, which I say is an SDR, what could I associate it with in the parent column? That's also an SDR. Where are there SDRs that would be associated with this? One would be the actual location on the object. The location signal is a unique SDR that could be suggested by this, but that's not very good because it would suggest all the locations that have—well, it could. What if it's kind of like a lateral voting connection? We don't always have to vote on the same object ID; it could just be biasing.

What I was thinking is, if the higher-level column has learned a model of a mug, how does it know to predict the Numenta logo down here? I think we solved that problem. But if it has learned mugs with different logos on them and it doesn't get the logo ID as input, how does it know this is the Numenta mug versus another mug? If we could vote, that could solve it, because we could learn an associative connection between the logo and the mug. The logo would bias the high-level column to detect the correct mug.

We do need to send something up here, right? You need to send something about the logo to this guy. That's the assumption. The problem with this is, well, it's not a problem, but it's basically saying I'm associating the logo with the entire object. Maybe that's right. It's certainly not specific to where it is in the object. It wouldn't say where it is; it would just bias that this is the Numenta cup versus Pete's cup. That would then tell the backward projection what features to expect at that location.

What's interesting about this is, in layer 3, they say layer three cells predict to layer four in the next region, but they also say layer three cells vote with layer three in the next region. So it would be that layer three to layer three. I don't understand, but you're saying this is the voting one. Basically, we could just assume we're voting, but we're not voting on the logo; we're voting on the object ID. If the lower level is detecting an eye, it makes it more likely that this one would detect a face, but it's not telling it the eye is at this location. It's just giving some context for the potential objects.

If you look at a Picasso picture where all the facial features are mixed up, that's what's disturbing about it. You keep thinking it's going to be a face, but then you can't make it work. You see the nose and think, "Oh, that's a face," but it doesn't work. You see the eye, but it doesn't work. It's suggesting what you're saying: every time I see those components, I think, "I'm voting it's a face," and then it doesn't work. If you see a banana for a nose, it's less annoying. It's okay.

The suggestion here is a simple one: we're voting on object ID between parent and child. Voting doesn't require that they use the same SDR to express the same object. If we vote laterally on "cup," this one has a totally different SDR representing "cup" than the other. There's no issue; it's just whether there's a good association.

In the hierarchy paper, I was just editing it and there was language in there...I don't know who wrote it, but there's language in there that says—in fact, I might have just removed this language or commented it out because it was long and complicated—why is there a discrepancy between parent and child object? I said, what can the child object tell the parent object? The child object can say, "I'm seeing a logo. You should be looking for something with a logo." That's all it's saying. It didn't say anything else. It didn't say, "You should be looking for something with a logo at this location." The language I wrote was, "There's a logo down here," because I was thinking there could be lots of objects with logos, and it can't tell where it is on these other objects. You can't say what object it is. It just says, "You should be looking for something with a logo." 

This could also bias it to detect the NATO t-shirt, for instance, if the child detected the NATO logo. You should be looking for anything that has a logo on it. That's how I wrote it in the paper, although I wasn't thinking along these lines. I didn't think it through; I just wrote it down like, "Oh, that makes sense." Or somebody wrote it. I think it was me, but I just deleted it, I think.

So, does this work? Does this solve all the issues here, at least for the parent-child thing we're voting on? It kind of works. In theory, it could work, but it also feels like it's not great that we're not passing up a feature that's more unique to a location. One of the advantages of hierarchy we've talked about is that we'll converge much faster because you could say, "Now you should be seeing a logo because of where you are," or, "Now you should be seeing..." We do have that if we're on the logo. If we're on the cup, the cup model will tell the child model, "You should be viewing a logo exactly at this point," and we can make a very specific prediction. So, the backward projection is right; it's just the forward projection that's the weird one we're dealing with here.

That's what I was referring to. When we looked at these lateral connections across hierarchy, they weren't as prominent as we thought. If this is going to be integral to hierarchical processing, we should go back and do that. This is one of those situations where I really want to understand the details of the experiments, because everyone reports that layer three projects to layer four, and that would be their bias going forward. They're going to report something different. What is their methodology? How are they looking? I'm not saying the empirical data is wrong, but it's one of those cases I just don't accept at face value. I want to dig into it.

By the way, on the topic of layer three, we now kind of put the behavior model there as well. Could we also say that the behavior model is in the upper part of layer four and the morphology is in the lower part of layer four? It seems odd to have the output layer be the behavior model. The reason there wouldn't be upper and lower layer four—I'd rather do upper and lower layer three—is that the behavioral model requires apical dendrites to layer one; it requires time. The general rule is that layer four stellate cells—it's mostly stellate cells—don't have apical dendrites, so they wouldn't get any time information. That used to be, "Oh, all layer three cells are stellate cells," but now people say, "No, some are pyramidal cells." In general, the stellate cells, which are primary layer four, don't get time, and the behavioral model would be all cells that get time; they all have to have an apical dendrite. That could be either upper and lower layer three, or upper and lower layer two. Some people have made that distinction between upper and lower layer two and upper and lower layer three, and other people say, "I don't see that." 

I would say stellate cells—not apical dendrites—can't be part of the behavioral model, and the behavioral model has to have apical dendrites. Within layer three, we can definitely have two layers. Some people really make the layer 3A and 3B distinction, even though it's very vague. There are subtle differences, and I don't know if anyone knows about them. I wouldn't do upper and lower layer four; I would do upper and lower layer three. Behavioral models are up there someplace. Layer three and four works too if we can say that there's a different subpopulation of layer three that projects. It could be upper and lower layer three; that's already been proposed. Any one of those could be the behavior model. As far as I'm concerned, we have lots of cells up there that are unaccounted for.

Then we'd have to look at what it requires: two sets of cell populations representing a location that can anchor differently. There'd have to be two. Here's something that's weird: layer 6A and layer 6B look so parallel that I would think those are the two reference frames, one for behaviors and one for objects. We've thrown in layer 6B as orientation. Orientation is something that does not have to be represented uniquely to the object. You don't need many columns to represent orientation. Head direction cells are never unique; the general idea of head direction is also not unique. It's not like, "I'm facing this direction in this object." I'm just facing this direction. It's possible that layer 6A and layer 6B are the two reference frames we need for objects and behaviors; they would look like it. Maybe the layer 6B cells that represent orientation are a small subset of layer 6B, and maybe not all of them. I don't know yet. We have to work through that. On the surface, if I had to say, where are the other two locations, I would say 6A and 6B.And then I say, where are the two—what's the layer equivalent to layer four? I would say it's in layer 3A or 3B or 2A or 2B, something like that. That would be the suggestion.

Shall we take a short break, by the way? I need to take a break, get some coffee, and use the bathroom. I might sign off for the evening. We're recording, so you can catch up later. Please record it. Good contributions, Neil. We'll miss you. Catch you next day. Have fun. It's a really cool discussion. Nice evening. Thanks.

It feels like we're doing a puzzle. Sometimes you're stuck, and then you suddenly put one piece in place and can add a bunch of other pieces, but then you notice you put another piece in the wrong place. In "On Intelligence," I describe how the brain works like a puzzle—a jigsaw puzzle. The problem is there are a million pieces, and you're missing many. Many pieces have images on both sides, so you can interpret them differently. This piece can be interpreted as A or attributed as B. You're trying to solve this puzzle with all these missing pieces, and you don't know if you even have enough to solve it. The pieces can be interpreted multiple ways. That's the analogy I made. It's so hard. In theory, as you have more pieces, you end up putting a bunch together to solve some subset of the puzzle. That was like the temporal memory—a small subset of the puzzle, but we figured that out. Then, the morphology of objects—we think we have some of those pieces. Now, let's connect them together and see if we can figure out the missing ones in between. Sometimes we have to make up a piece because no one's observed it yet in biology. "Oh, this piece should be here, but we don't have it yet." Some pieces have fallen under the couch. You lost them.

The other thing I said was, you have some pieces, and then someone comes to you later and says, "You see all these pieces? They were wrong. Let me take them away and give you some new ones." That's when scientists come back and say, "That wasn't the right observation. This is the right observation." It's like, "I've been trying to figure out those pieces, and now you take them away from me." It's so hard.

Do you think you'll end up writing another book? I don't know—only if it's necessary. It's not fun writing a book. I just thought it would work. I've thought about different books I could write, very different types. I could write a book about future humanity, expanding on the last part of my last book. I could write a book just on brain theory—everything we've learned in a cohesive format. More of a documentation than a big seller, more like a different form of documentation. Do we need that? We have videos and all this stuff. Maybe books are old school. Do we need a book? I don't know. My bias would be toward the brain theory book—give it a year or two, whatever gets worked out here, taking the concrete learned aspects and augmenting the last book. You could really just break, but then that book would go into lots of details that would turn off most readers. It would be a small seller unless it somehow got popular, like "A Brief History of Time" by Stephen Hawking. That was a huge bestseller, but if you read it, it's a hard book. I got lost over and over again. Why was it such a huge seller? The great title, it was short, became famous, so everyone felt they should buy and try to read it, but very few people made it through. That's the exception—you can have a really dense book and still sell a lot. It probably sat on a lot of bookshelves. I'm sure it did. I was really motivated, but I couldn't get all the way through it. I still have it on my bookshelf. Did you start reading it? No. I had it on my to-read pile for ten years. If you do read it, it's hard. I found it hard, and I love that topic. It's too difficult and dense. I can't even say it was easy or well written. It's always tempting because it's pretty small. It's so small, but it's a great topic. We learn all about time, and it's fun, just really hard. He was a very famous guy because he had a disease—he could barely speak, won all these prizes, and did great science. So, let's read his book.

Where do we end up here? I think we're going in circles a bit. I think we have a clearer idea of how we project forward, and maybe we've concluded that Hills is going to push back on the mini column thing. I don't want to go there, but for the moment, I'm going for the voting. SDR is—the child object basically just votes on what the parent object would be. That's probably one directional. I don't think I would need it the other way. Some of these layer two and three connections are one directional, so I don't think I need to go the other way. I don't think the cup has to predict that there's a logo, because you can't be certain there's a logo. You'd probably use the same mechanism for behaviors. If you detect a behavior, that can also bias what objects you're expecting to see. I'm thinking about that—maybe.

I was just wondering if we would pass up the behavior ID, but I guess that's the question. When we talk about compositional behaviors, as a field we haven't really dealt with hierarchy where there's no fixed relationship between the child and parent objects. For example, I want to do some behavior, and I can do it in various ways using different parts of my body and different things.So there's not a clear breakdown from a behavior point of view—it's not like you do this, then this breaks down to that, and so on. It could branch off in different places. My point is we have to deal with hierarchy in a broader context of behaviors that we haven't really considered. The voting or associations between layer three in different regions—like hierarchy here—the idea that the child to the parent uses the same voting mechanisms we've had elsewhere. It's just SDR to SDR; they don't even need to know they're in different hierarchical layers.

Is there a difference between this and just associative connections between the SDRs, or is it the same thing? It is associative connections, but the way it works, it has to be associated between the two different regions and also within the parent, within each column. The many columns themselves, or the unique representation, is just like wanting to lock into something. You don't want to just say, "Here's an SDR." It's more like, "No, we want to settle on this one or that one." It's either a cat or a dog; it can't be both. So pick one. It's a vase or a face, not both—pick one. That locking-in effect also has to occur within the column.

That's pretty simple. Basically, all the cells just make associative connections to other cells—nearby or far away. It doesn't make any difference. A bit off topic: at one time, I was thinking about the voting and realized we're voting over all these columns. You might think that requires huge numbers of connections between columns. Say I have 5,000 cells in column A and I'm trying to vote with thousands of other columns. If you take any particular cell in column A, it can associate a link to a subsample of cells in all the other columns. It doesn't have to connect to all columns—just some subsampling. We just pick randomly, and another cell connects to some other subsample of all these columns. A particular cell doesn't have to connect to even half the columns out there, but if you do that, it still works. They all settle, so you don't need massive connections.

I don't even have a name for that—the propagation between all of them at the same time. It's just going to keep settling regardless. The point is, a cell doesn't know if it's connecting to something nearby, far away, or in another column. It really doesn't know. It's just trying to associate its patterns with somebody else out there, and they don't have to be coming off a specific population. It used to be that a cell would have to connect to at least 20 of these cells, but really it just has to connect with 20 cells anywhere that represent the same thing.

This is a bit off topic, but I think where we're going with this is that we got down this path because we were talking about, in the notes of the melody, that a child object would be the only thing that knows it's a saxophone. A saxophone may vote on the song because maybe it's associated with a particular melody or whatever. But maybe not. If I'm hearing a song with a dog barking, I wouldn't have any association with the dog bark and the melody, so that wouldn't work. But it doesn't matter, because what happens is the child object recognizes the dog bark. It says, "I start here," and has this note—what we're calling an orientation. I don't know what it means in songs, but something happened here—a change occurred. The parent object doesn't know what it was, but there's an assumption somewhere in the system that it's continuing to be the same thing unless told otherwise.

That's an interesting question: if halfway through the song it goes from dogs to violins, who's storing that transition? I think that transition is somehow stored in the parent object, in the behavior model object, but somehow it has to tell the child object to do something different now. It would, right? It would basically say, "At this point, you said violin; at another point, you said bark." So it would just naturally—whatever—the parent object doesn't actually know. The parent object just says, "I'm going through the melody," and then at some point, "Oh, at this point in the melody, you're associated with something, and at that point, it could be associated with violin, at another point, with dog." I don't even know if the parent object needs to know the voice change. I don't think it does. Who else would know that? It would make the correct prediction because the parent object says, "At this location, I heard a dog voice; at another location, I heard a saxophone," and you learn that association. It would just be stored in the layer six backward projection.

I was thinking it's part of the model, but it's not actually part of the model. That would be an implication of this—it's not actually part of the model of the cup or the saw. The equivalent on a stapler: imagine I have the logo of Pete's and a coffee cup, or Starbucks, whatever. The model stapler doesn't know—it would have the voting connection, but it doesn't know at this location there's a logo. It just knows that at this location, the child object is told there should be a logo. The parent object doesn't know that. This also means that if the logo changed from Pete's to Starbucks, the parent object wouldn't know that. The two know it because together, the logo says, "At this location, you should see something," and then at this location, here I am. The child object says, "At that location, I have the Starbucks logo; at that location, I have the Pete's logo." But the cup wouldn't know that. The cup would have no knowledge of that.Is that what that implies? Would it lay down? So, the associative feedback connections from layer six to the logo in the child model—would that be the reference frame of the static model or of the behavior model? Can you say that again? I just didn't hear the words. So, the backward projection from layer six telling the lower-level module which logo to expect at what point in time—would that be coming from the reference frame? I don't know. That's a good question. We now have the option of sending back both reference frames.

I would suggest this might be the clue to our hierarchical behaviors, because you could have a hierarchy of behaviors and a hierarchy of models. If I pass back the specific location in a behavior, that could invoke a child behavior. Hierarchical nested behaviors could work the same way as hierarchical nested objects: a parent behavior wouldn't know the details of a particular point in the sequence of behaviors, but the child would.

In the case of the logo changing, that seems like a behavior, right? Would the lower-level module learn the behavior of the logo transition, and then the higher-level module would just tell the child to expect that logo change behavior? Now, what if—okay, really weird here—I have a Starbucks logo, and as a stapler is opening, the Starbucks logo changes. It changes from green to red, or the little lady's hair goes up or something. At some point in the sequence, that occurs. I'm trying to make up examples. I think the idea is that you might pass back—I'm trying to come up with examples where there's a hierarchical behavior, like a change in the stapler, which is a change in the subobject, but the stapler doesn't know the change in the subobject. I think that needs further enumeration.

We now have the ability to say we can vote on behaviors, we can vote on objects, we can pass down unique locations and behaviors, and we can pass back unique locations and objects. What do we want to do about that? More tools in the toolkit here. That's a good idea, though. Why not? It seems like it's a useful thing to do at some point.

Where am I in this behavior would tell the child object where it should be in its behavior, or something like that. Or start a behavior, or something like that.

I think the traffic light might encode the global behavior of the sequence of which lights turn on and off, and then the child object would encode the actual feature change of each of the lights, like turning from a dark green to a bright green. Maybe, although I think the change there is the same—if I represent color in the parent, I could do that, couldn't I? But maybe it would be more useful to abstract it away, to have the general sequence of a traffic light behavior encoded separately from how exactly the traffic light looks.

I sometimes see certain traffic lights—most of them are moved to LED, but not all of them. I can tell the difference. When the ones are LED, some of them have broken bulbs, so you see broken little patterns. That's a specific one I know—where I turn near my house, it's always missing these little dots or something like that.

To have a more general representation of that red, orange, green sequence versus the actual change in color and brightness happening on each location of the lights. You might also have the lights at slightly different locations relative to each other, different distances.

Do we want to summarize what we think we just learned this morning? I'll try to write that on the board.

So, what have we decided? When it comes to behavioral models, we often need two levels of hierarchy at minimum. I'm not saying that's always the case—let me organize my thoughts. The problem I was dealing with is how to know the constancy of a feature or how to project that something will remain the same. We use a child object to maintain constancy. An object is needed to predict constancy of features—constant features, like it’s still black. The child object can also be used for this.

I thought the whole child-parent hierarchy discussion was about solving the problem when the lower-level feature, like the saxophone, is actually a temporal thing itself. That was the next point, but I didn't think it actually solved the constancy problem. It does, because the parent object doesn't know what the feature is. The parent object doesn't know there’s a saxophone or that it should hear a saxophone. In the case of the stapler, it knows it continues to be black, but it doesn't know what the subcomponent is supposed to look like in its new orientation. This was the saxophone problem: the child object itself has its own sequences, and the features are sequences that need to be played back. If the feature is something less temporally complex, like just a color or an edge, you don't need hierarchy. Those are two separate problems.

I also assume the behavior model doesn't know what the feature is. Parent behavioral models in general don't know the specific feature. They are being voted with the feature, but they don't know the feature or feature IDs. Isn't that also true for a behavior model in the same column? In general, a parent object cannot know the feature ID of a child object that's projected to it. Voting is not the same as knowing it; it's not stored. The parent object doesn't know it. They're connected via back predictions, but nowhere in the model of the stapler or the mug do I actually store the logo. Nowhere in the model of the melody do I store "saxophone." In the stapler, if there was an ID like the coffee logo, that logo is not stored in the model of the stapler.

Is it a requirement to have two hierarchical levels to apply a behavior to a new object? No, we didn't say that. Is that required? It feels like this is trying to solve the problem of applying a behavior to a new object. I thought we had a solution to that within one column. The hierarchical problem is independent of the behavior problem. We started talking about hierarchy because we started talking about objects made up of complex subobjects—compositional objects, like the saxophone, which is itself a temporal object. I don't think there's a requirement to learn behaviors with two hierarchical columns. When I was writing this, I said it's not always required.

One high-level thing is that in a parent-child relationship, the parent never knows the ID of the child. That was a new idea today. It's not related to behaviors, but it's a new idea. Parents don't know the ID. I used to think that unique ID was being memorized, but it's just being associated, not the same. That's a big idea, separate from object behaviors—a new idea.

In behaviors, if there are things you can represent, like edges and colors, that can be represented in a set of many columns, those don't need extra help. But if the behavior is an object itself, like a logo or a saxophone note, that requires a hierarchy. If we need to model compositional objects or compositional behaviors, we need hierarchy, but we don't need hierarchy for behaviors themselves. If I want to make a prediction based on a behavior, I might need a hierarchy. If I want to predict a particular logo, and you have a compositional object, you need hierarchy. If you just want to predict a color, you don't need hierarchy.

This idea that the subobject ID is not stored is related to behaviors because we were struggling with representing a saxophone note with an SDR, as Neil was suggesting. We were bouncing around ideas about that. In my mind, that discussion was separate from behaviors; it was about compositional objects. We got there by thinking about behaviors, so it's important to know for behaviors. A child object ID is only used for voting. That's a new idea. We didn't really have that before.And that can happen independent between hierarchical levels and between different objects—the child object can vote on the parent object ID. Also, I used to think there’s a sound of a ceramic object hitting the table, and I recognize that could apply to many different objects. They’re just voting; this sound could apply to these objects, different sounds, and they vote together.

Is that okay to start with? I wanted to state something as a conclusion, just to make sure—Neil may think otherwise, but I wanted to say something about minicolumns. Minicolumns can only represent things that are easily enumerated and observed. Do I need to say that? What would be your next point?

Only parent objects can make predictions about the child object ID through the backward connection. The child object is only used for voting. It's not a new thing; it's just how it works. The back projection is the only way to connect a parent and child relationship—this is the only place that's actually recorded. There's no forward connection like that.

In behavioral sequences, sometimes the change is in a child object with its own behavioral model, time-based. You can compose object behaviors the same way you compose object models; we would use the same mechanism to have a child behavior as we would to have a child object. Your backward projection could be viewed as: at this moment in time, you are observing an object in some behavioral state, and you invoke that object in that behavioral state. Imagine in layer 6A, there were two grid cell locations back. You are associating the current parent state with whatever object and whatever behavioral state of that object at that point in time. For example, at this point in the melody, it's a saxophone, so it starts like this and progresses like that. I'm at the beginning of the saxophone—something like that.

Can you start a child sequence mid-sequence? I was thinking about that. Imagine I have the logo, and the logo is changing as I raise the object. There's a sequence of changes to the logo—maybe it's an animated logo. The Starbucks lady goes, "Okay." As you raise the object, she goes, "Hi." If I stop the stapler halfway, I'm halfway through the sequence, and she's halfway through her sequence. If I could say, "I'm here in the sequence in the stapler," it could be associated with both the Starbucks logo and where she is in her sequence. I can stop anywhere, even in a place I haven't stopped before. Somehow, I need to figure out where her hand should be in that behavior. 

That's an example. What would be required to do that? You'd have to replay part of the sequence. We haven't solved the problem of knowing what something will look like in the middle of a sequence if we've never actually seen it. If I'm right, during the sweep of the logo or the stapler, there was no morphology model learning, only behavioral model. How are we able to predict what it will look like if I stop? Somehow, we can do that, but we haven't solved that problem yet.

In behavior sequences, sometimes the change is in the child of its own behavioral model. This could be like a saxophone type of thing. Can we also talk about feature consistency? That's the part we haven't completely addressed yet. We have a guess, but I don't think it's a complete answer because we haven't dealt with it fully. Part of the solution is that a child object replays the same behavioral object sequence—same object, same behavior—as recently done, without any instruction otherwise. That would imply we need hierarchy for a check, but no, it doesn't. It just means I assume I do the same thing if you could just use the hysteresis. It could be just saying, "Okay, I heard a saxophone, and we're in the melody," and he says, "Get back to him, play the same." 

Now we get back to the problem where we're treating object ID differently than features, but we want the same mechanism to deal with constancy. The color of the stapler doesn't change, and the logo on the stapler doesn't change. I think the color is easy. In my mind, this is like taking the behavior of the stapler, applying it to a new object, and making sure the new object's features are consistent and do not change. Taking a behavior, like the stapler opening and closing, and applying it to a novel object we haven't seen, making sure the features of that new object are consistent over time. But how do we ensure that? I'm not suggesting a solution—it's the same problem. That's the same problem of applying behavior to a new object. I would agree with that. We never said it would require a hierarchy to solve it. I would also say we shouldn't require hierarchy. 

Now, we don't have the child object ID in a mini column anymore, so we can't use the same mechanism. The parent can't tell the child, "You're supposed to be hearing a saxophone." The child itself has to say, "I'm assuming it's a saxophone until I'm told or I've learned otherwise." What about color? If we have one hierarchy, I don't need hierarchy for color, just like I don't need hierarchy for edges. The constancy is in layer four features, right? I think both the parent and child columns could work the same way in this regard. They could both assume constancy until told otherwise.

Let me talk out loud to clarify my ideas. Imagine I'm a child object, and we're learning a song. I hear it played with a saxophone, and halfway through, the saxophone changes to an oboe. I'd be hearing saxophone, saxophone, saxophone, then oboe, oboe, oboe. I don't know about the sequence; I haven't learned a sequence of those. My sequences are saxophone sound, saxophone sound, saxophone sound, oboe sound, oboe sound. The parent object is going through a melody and says, "At this point, these are saxophone notes, and these are oboe notes." That's the backwards projection. When we play the sequence again in the future, the lower level doesn't know about the sequence, but it would say, "I'm supposed to play saxophone, saxophone, no sax, now I'm supposed to open." I've memorized the song that way.

If I now hear the song with a dog barking, the backwards projection wouldn't predict I should be hearing a dog. It doesn't predict a change; it just says you should start another note now. So my lower column says, "What should I do?" One thing we could do is just say, "I'll just keep barking." That's what I did—I'll just keep barking. There's no other instruction. Otherwise, wouldn't it tell the lower column that something will change? Would it? That's interesting because I can't predict the change. What's the equivalent of a change from a saxophone to an oboe? From a dog to what—a cat? You wouldn't be able to predict what feature to see next, but it seems like the behavior model would say there will be a change. 

If I imagine listening to this, I've learned this beautiful melody—Beethoven or something—and it's got saxophone...He didn't know saxophones, but anyway, violin, violin, violin, oboe, oboe, oboe, and it's alternating back and forth. Now I'm hearing the dog—okay, the dog is singing this song, but it's Beethoven's melody. At the point where the transition would occur from violin to oboe, I might think, "Is it going to be an oboe next?" That's the only thing I could predict. I would literally think, "Is the next note going to be an oboe?" because that's what normally occurs here. I wouldn't say the next note is going to be the oboeness of a cat or a dog. I wouldn't predict the cat sound. The only prediction I could have would be an oboe. If the dog continues barking, the dog continues barking.

That just shows the melody would predict an oboe at that point, even though I've been hearing dogs. At that point, I'm not going to predict, and somehow the constancy of dog barks is going to continue. I'm going to keep expecting a dog bark. This carries over the same behavior to the second part, where we change a feature. It's going to continue to predict with that new feature using the same behavior. I don't see where the hierarchy comes in here. I have to have a hierarchy because the thing I'm predicting is an SDR. It's a complex thing. I can't represent it in a single form. I have no representation of saxophone, cat, dog, or oboe in the parent object. I need a hierarchy to play out that subsequence. That's why I need a hierarchy, and I'm just walking through the consequences.

If the rule was, "I've learned this melody using one instrument," I would now predict that instrument, but I didn't get that instrument. I got a dog barking, but that's good enough because I'm still recognizing it. Until I expect a change, I'm just going to keep going with dog. When I expect a change, I'll think about what I learned before, but that's all I can do. I can go from dog to oboe, but I can't—yeah.

The change from violin to oboe: while it's the first instrument, you're not actually encoding anything in the behavior model because nothing is changing. It's constantly that instrument every note. The notes are changing at a high level, but the behavior model does not include anything about the voice; it's just the melody. The behavior model in the parent column models the melody, but the child doesn't know anything about the note. It just knows that it's constantly the same instrument. No, it's not even that. It's saying, "I have a sequence, a pattern I recognize, an object I recognize. I've now got that object." Then someone tells me to do it again. What am I supposed to do? I don't know. It's either going to tell me it's a saxophone, or—but I'm here. I'm going to continue doing the thing I did because the lower-level column recognizes whether it's a violin or an oboe.

Couldn't that column learn a behavior model that's just changing from violin to oboe? Just that change, and that behavior model was involved. When would it do that? Under what time frame? Would it be a long period of time and then all of a sudden it changes? No, it would only encode the change. It wouldn't encode everything. When would that change occur? When the instrument changes. If I want to learn a behavioral model, I have to have a sequence. I have to say, "Starting now, this is when it occurs." Otherwise, it seems random.

So, it's a very short behavior. When we keep hearing saxophone, we're not laying any points down in the behavior model because there are no changes happening. That behavior model of the child can't predict anything because it seems like I'm going along and I get saxophone, saxophone, saxophone, and then at some random point in time it changes to oboe. I can't predict that. The child object can, but when would I predict it? When would I say it's supposed to change? It's random. Wouldn't the parent object be able to invoke that behavior in the child? It tells the child, "You should be doing oboe now," but the child itself doesn't have any knowledge about why.

The child can't learn it as a behavior. The child doesn't learn when that behavior happens in the melody. It just knows it's not a repeatable pattern under some circumstances. There's nothing to learn. It would be totally random from the child's point of view. It doesn't know what melody it's in. It doesn't know anything. If every day I knew a song began and three seconds into it I switched, I could learn that, but it doesn't know that.It's just object, object, object, new object, new object, new object. When would I predict that change? I can't—it's not an actual model if I can't predict anything. I think we need to predict it. This is coming from what a model is about: prediction. The model learns the behavior, but if midway through we started listening and I started hearing violins, this is coming from the sensory input. The model knows how to apply that behavior—the parent model knows it, but the child doesn't. The parent only knows what's going on in the child through the voting system, but it's just saying, "I'm going through the sequence." The parent doesn't know because we're not passing up the unique idea of that child. So the parent is going through this melody, and the child is associating, "I just heard a saxophone," with that parent SDR. "I heard another saxophone," association with that parent SDR. "I heard dog," associating with that parent SDR. When the parent goes through its melody again, it'll tell the child: saxophone, saxophone, saxophone, dog. But there isn't enough information for the child to learn that behavior. It will observe the transition, but it has no idea when it occurred or why. It seems random to it. It's just a random change. The child can apply that behavior to a new feature, which is the violin—the change from saxophone to violin—but it doesn't need to learn that there's a feature change happening here. You could argue that it's seen that transition, but it's not a very useful model if that transition occurs rarely and randomly. If every other note was a saxophone and a dog, then you could say, "Oh, yeah, I'm going to go A B A B A B." Maybe that's the pattern I learn. Otherwise, it just looks like some random change in time that I don't know why. I could learn it, but I can't really make a prediction about it. I don't know when it's going to occur.

The idea here is feature consistency, right? We want to keep the same behavior. We're trying to solve a set of problems, with consistency being one of them. If you have a stapler and we open it, and then halfway it changes to pink, we are able to know that if it changes to pink, I can predict it's going to continue pink. We haven't done the constancy part yet, but that's part of this. That would be easy. I observe this the first time, and I see it goes from black to pink. The model learns that. In fact, it could be learned in the parent; it doesn't even need a child. There is no sequence of black to pink—it's just at this point it's black, and at this point it's pink.

So where are we going with that one? I'm not sure. I'm just saying that the behavior can be applied to a different feature. Let's just talk about now. Now I see a green stapler. What should it change color to halfway? Do I expect it to change color halfway? No. Why not? Because we're applying a behavior, abstracting that away from the features. I don't care what the feature is. If I see it green, I'm going to say stapler is black, black, pink, pink, pink. Now I see green, but I recognize it's the same sequence. Halfway through this, I might expect it to turn pink. I'm not expecting it to turn any other color. I might think it could stay green, but if anything, the only expectation I could have about a change is it's turning pink.

This is like saying, I'm going through: dog, dog, dog, dog, dog, and now it's violin, violin, violin. If I know where I am in the sequence, I make a specific prediction, but that's coming from the parent. If I'm just a child object, I'm hearing dog, dog. I have no expectation that it's going to turn into violin right now. None. How would I know? So is a transition between saxophone and violin in the behavior model? It's in the parent behavioral model. It's not even there. The parent behavioral model has a set of states, and I associate some with black and some with pink. Actually, the color one we should separate, because I think color can be handled in the single—color is not a child object. There are things that can be handled in the parent, and anytime I have a child component that requires an ID, it has to be handled in the child. If it's something I can handle in the parent, like edges and color, I can do it all above. However, if it was a logo on the stapler that changes color or changes somehow, then it would require a child object, because it's not just color—it's an object.

I think this all works, but I don't think you're convinced. Well, okay. I thought you were pushing back on this. I still don't—we still—the hypothesis right now, which is a little weird, is that if I'm a column and I observe the behavior, and now it's like some location in some parent object I don't know, and now I'm being told from the parent object to do a behavior again, but I have nothing new to learn from. I haven't learned to associate that new location of the parent object. I'm just going to play the old one again. If I haven't learned a transition and you're saying, "Okay, I got a dog bark at some location, dog bark at some location, dog bark at some location, dog bark at some location, and now it says do another—here's another location, at this time. I'm going to do another dog bark." Even if I never learned that, I'm just going to say, "Okay, without further instruction, I'll do the same thing." So, it's like a hysteresis. I don't like this, but that's what this implies. And yet, if I've been through the song before, I could have learned that on a unique SDR here, or any of these SDRs as I'm going through the melody, I can associate the dog bark with that location, or associate this location with a violin. Then I would predict violin because I've learned that. But until I learn, I have to just assume it's going to be the same thing. Yes. So that's a little weird. I don't really like that, but that's what it is.Somehow we want to transition back to the stapler, where the stapler top is like the dog bark. If I haven't learned that the stapler top changes, I'll just assume it's the same stapler top in a new orientation. Previously, when learning it's a dog bark, I assumed—by the way, it's not just a dog bark, it's a dog bark at a particular note, so it's not just the stapler top, it's the stapler top at a specific orientation. Those are parallels; that's a clue.

I was wondering if the notes could be thought of as orientation. I don't know what that means. It almost seems like it, but it's fuzzy—what does that mean? Do you have a clear picture of it? I don't know. You don't get back to the first note if you go full circle, but I don't even know what it means—orientation for notes. I don't know.

So, to summarize, the melody or the parent object, other than the voting (which we can ignore for now), doesn't know anything about what's actually comprising it. If there are actual individual feature IDs as part of this object, it doesn't know them. But it can associate its location with a particular ID at a particular note. The child object doesn't know anything about the behavior exhibited here unless it somehow surfaces this very repeatable pattern down here, which could be, but the general rule would be no. Sometimes it could learn, sometimes it could say, "Yeah, I'm only looking as a child object, and as a child object, I can see this behavior will change. I can learn how this thing is changing over time." Other times, like changing from cat to a dog voice to a saxophone, it's not going to learn that.

Now, since the analogy is between the dog bark and the stapler top, the stapler is somehow analogous. For color constancy, you would use the same mechanism. I think color constancy can be handled in the parent column; I don't think I need anything else. I would definitely argue you don't need hierarchy for it. If there's no change in the behavior model, we would expect the same feature to be invoked in layer 4. I don't know why, but you can just imagine it. The same location in layer 6. I could say the colors—I wouldn't really learn anything until it changes. Why would I learn anything new? I don't want to learn that it's still black as I'm moving from space to space. You wouldn't learn it; you would just expect it. Even though the location is changing, the color isn't changing.

One could argue I could learn the color of the new location, or for all those locations, I'm just going to be black. I think that's what you'd have to do, because you can see a stapler in a new color you've never seen before and predict it to stay the same color, even though you never learned that association. You just have this implicit knowledge that it stays the same if there's no change in code. It's implicit in the sense that nobody knows this; it just says until I hear a change, it'll be the same.

Imagine you've got these two models—this is like four and three—and these shared mini columns. The mini columns have the same definition on the behavioral model and the object model. If I don't have any change up here, I don't force a new change here; I'm just going to keep the same mini complex. It's the same color, the same edge, the same feature—the feature hasn't changed. But if I get a change up here, if the actual encoding of features changes, then it forces these guys into the new set down here. This layer has temporal persistence.

How would it know which new columns to invoke in layer 4 given a change? When a change occurs, it's like changing color—the color appeared, the edge appeared, something appeared here that wasn't there before, that entire column.

For locations, here's an interesting question. Basically, if nothing has changed, keep constancy here. As soon as something changes, force this to be new. My location can be changing right now—the location not in this space, but the location in this space. The location in this space could be changed at any time, so at some future time point, it's changing there too, but here I don't have time.

If my center patch is moving around and nothing has changed up here—like I'm moving my eye across the stapler and nothing's changed up here—then I'm going to assume that all those locations I'm looking at are going to have the same pattern here. Whether I associate this pattern with all those different locations is an interesting question. It might take more synapses. That's a nice mechanism for color constancy: you can have the stapler moving, but since there's no color change encoded, you just keep the same minicolumn active in layer 4 for the color.

The remaining question is, if there's a movement change detected in layer three, how is that getting translated to expected location change in layer six? I think it's a separate problem. If I'm observing the location where the change occurred, then by definition, I know where the change occurred. The problem is, what if I'm not looking at the location where the change occurred? Then I have no idea that anything's supposed to change elsewhere.

So, you have a model of a stapler openingHow do you know that if you go to a location on the stapler—the new location of the top—it will be there and not somewhere else? That's the problem; that's the occlusion problem. How do I know if I'm not tracking it continuously? There are two very related problems here. One is the occlusion: even if I can see the whole thing, if it gets occluded, I can't really see what's supposed to come out the other end. The other problem is that if I'm a single column, I can only observe one point in time and observe the change at that point, but the other changes that are occurring—even if they're in the model—I don't observe them.

So, imagine this: imagine your behavior is space-time slices. Each of these is a two-dimensional sheet. I'm moving through time, and I have two-dimensional space.

And on these surfaces, this is where changes occur at that point in time. As a sensor patch, I can only move through here at one point. I can move here, up to here, down to here, and so on, but I can only observe that one small point. I can observe if I happen to be there when the change occurs—I know the change occurs. But what if a change occurred elsewhere? I'm not observing it. Now I go back here; a change occurred here, and I'm not observing it at that point. The change occurred at this point in time right here, but I'm not looking at that. The model tells me it changed, but now I go up to that point. I need to be able to predict what's there even though I wasn't present when the change happened. For example, let's say the model says this chain is green—it appeared. At this point, green appears. At this point here, I don't say that anymore because green is already there; it's not a change anymore. It appeared. I come up to that point now, and I need to know it's green even though I wasn't there when it changed. How do I know that it's green there? This is an expression of this problem in the general sense. How do I know that?

is not with color, I feel like it works because this assumes we've already learned the behavior model. So, the time sequence or signal we're getting is already invoking the changes we expect in the behavior model, but I'm not observing this. How do I know how to change if I'm a sensory patch match? The sequence of changes invoked in layer three—basically, we've learned the behavior, and now we want to replay it. It would only replay the locations we're looking at at any point. The behavioral state of the column will go through its entire behavioral state over time, but I can't observe all the spaces in that. I can observe every point in time, but I can't observe every part of the object at any point in time. I can only observe one part of the object; I'm only sensing one part. 

Can the column represent—at any point in time, it only represents one point where the sensor patch is. I can make predictions about where I'm looking at that location. Unless I invoke another location, I won't know what is stored at that location. The neurons don't become active until I'm at that location in space and time. Even though it's in the model, in the connections, there's nothing dynamic happening here. It's in the connections saying, "If I was at that location, I would see this change," but if I'm not at the location, I don't see anything. I see what's down here; I don't see what's up there. Even though the model says, "Oh, yeah, there's supposed to be a change here," if I'm not there—I'm down here—I wouldn't know. It's like I have to somehow go back in time to find out where the last change was. How else would I know? This would require all the columns to be voting on something like a global state ID, but we can't do that. 

All the columns or these patches need to be agreeing on the whole state of the object at the same time, because right now I'm just talking about one patch. Right now, one patch has this problem. Imagine I'm looking at something and I've learned the behavior of this object. I have the entire behavioral model, and now I'm watching the stapler arm go up, but I'm looking down at the bottom of the stapler. While I'm doing that, the behavioral model says, "Oh, the top should turn green." Not the part I'm looking at, but the top up here turns green. But I'm not attending to that; I'm looking here and not seeing that. The part of the model that tells it turns green is not even invoked at this moment. There's no dynamic representation of that—it's just in the model, but the model is not invoked. 

Now I zoom up. I'm looking down here, and now I go up to here. I should see green, and I should know I should see green even though I wasn't attending to where the change occurred. Could we invoke—like, we have many columns and maybe one of them represents the change to green. Green just appears; let's say green appears. Green appears is what this column represents, and we have this temporal sequence coming in. Maybe at first it predicts that red appears at some point in time at the same or a different location. We have the reference frame here, and then this temporal sequence would predict red appears up here at some point in the sequence, and then at a later point in the sequence it would predict that green appears at this location, like a traffic light. 

But we're not attending to this location when green actually appears. Let's say we're still looking at this light; we just saw it turn off. We didn't actually see this light turn on. Still, wouldn't this sequence have invoked this, even though it's invoked in the sense that at least depolarized? Or no, because I have to pair it with the location. I can't imagine green appears here, orange appears here, and purple appears here all at the same time. That information is in the model, but it doesn't get invoked until I go to that location. Then I would say, "Oh, I'm at the location, it's supposed to turn green. Oh, I'm at this place right now, it's supposed to turn purple, supposed to turn orange." But if I'm not there, could this temporal signal somehow say, "Oh, I am expecting a change at this location even though currently my sensor is not at that location in the reference frame"? 

Well, it can invoke a union, I suppose. Remember, there could be lots of changes, like I just suggested. It's not just one change; a bunch of changes could happen at the same time. For that to occur, at this point in time, I'm going to depolarize the union of changes. I'm predicting it changing purple, I'm predicting it changing green, I'm predicting it changing orange. But until I go to the specific location, I wouldn't know which one it is. It's just sort of saying all these things are happening, but I can't invoke the specific one; it has to be a union because I can't invoke them individually. 

Is there another column that's looking at that change at the same time, or are we assuming only if you said there's another column? At one point last week, I said, "This has got me thinking about lots of columns working together," and the reason I got there is because I was trying to imagine just learning the behavior of the stapler. It's going to be really hard for a single column to do that, because it can't get to all those locations. You have the same problem here—a single column can't make predictions very well because it can't get to all the locations. If I had a whole bunch of columns that all had the same model and they're all looking at different parts of the stapler individually, those columns—somebody would observe every change. They'd say, "I'm expecting this to turn purple over here, I'm expecting green over here," and they'd all invoke that. 

But that still doesn't solve our problem, because if the column that was up here said, "Oh, it turned green," and another column said it turned orange, when the green column goes down here, how would it know it's supposed to be orange? It doesn't know anymore. Unless this guy—how would I know that? It's almost like you have to have some sort of ability to go back in time to find the last change and then bring it forward.I don't know how. I don't know the answer to this question, but that would be one possible answer. I get to some point, and I'm looking at it, and I don't know what to expect. I can just go back in time until I see the last thing that changed. The last thing that changed is this became green. It must be green because that's the last thing I saw. 

It seems like this sequence signal must make some prediction, must activate something here, because often we predict a change and anticipate looking there before the change even happens, or we can directly move to the change we expect in the behavioral sequence. Imagine if I had only one change in my slice of time and we're predicting that this part of the model is going to change. I would have information. I could, in theory, say, "Okay, direct your attention to that one location," and you should see it turn green. I do that with the traffic light. I'm anticipating it's going to turn green right here. I'm waiting for it.

But if I have lots of changes occurring at once, like the edge of the stapler, you can't attend to all of them, but you could still make predictions. I can't make predictions about the ones I didn't attend to because I didn't know how they're changing. If you've learned a model of the state, the model tells me when a change occurred and where it occurred, but it doesn't tell me that it's consistently there. That's the problem we're trying to solve. I'm looking at some future time after the change occurred. How do I know? It's already happened. It's like I'm looking for a straw, and I'm looking at the green light on the stoplight, waiting for it to turn green. How do I know that when I get up to the top, the red is no longer there? I do know it; it's part of the behavior model, but how do I know it? There's another column that's looking at that part you were looking at before. 

We can try to use multiple columns to do this. I just gave an example where I didn't have multiple columns; I was looking through a straw. But that column has learned the entire behavior of the traffic light. When it goes up there, there's no change there. The change occurred earlier in time. So how does it know? Why couldn't the learned sequence in the behavior model make a prediction of how it's going to change at a certain location, even though the sensor is not at that location right now? But I can't make these predictions at all these locations at once through time and keep track of them. How would I do that? It seems like it must be happening. What I'll say is we are able to make the prediction that the red light's no longer red. I don't have a proposal for how that works yet that I think makes sense. I can't keep track of all the changes that occurred in this thing.

The problem may come about because I've said that the behavioral model is only based on the appearance of a feature, and so it goes away. If I said it turned red and now it goes away, or if I said it turns red and it stays red, then I can go back there and know it's red. But that doesn't seem like a behavior model; that seems like a static thing. Even then, the behavior model has all these possible changes occurring at once at different locations.

Let me propose an alternative. I just yesterday suggested that we think of the behavioral model as a thing where a feature appeared and it's only representing change. What if the behavioral model didn't work that way?

As I'm going through time, these are spaces at different time slices. In that space, I could mark the beginning of a turn as green, and what if I kept it persistent, saying it didn't turn green, but it is green? Somehow, I could just persist that. Don't we get that from the static model—that whatever is not encoded in the behavior model persists?

Yeah, but the problem is, if I'm not attending to that location, nobody knows it happened. The object model doesn't know. I wasn't looking at a location, so how can I change anything? It's like a part of the model that's hidden. All kinds of things could be happening throughout time, and I'm not observing them. Tons of things in my model could reflect things that could happen, and I'm not observing them. Yet, it seems like when I do observe it at some point in time, I know what it is.

Imagine your house. You have a house with six rooms, and every hour someone rearranges the furniture in those rooms in a different way. At one o'clock, it looks one way; at two o'clock, another; at three o'clock, another. Your house has different parts that are changing over time. I'm in room A, and I can see things changing in front of me, but I can't see any of those other things changing. Until I think about those other rooms, there's no invocation of those rooms, no instantiation of those rooms.

If it's 3:00 and I'm thinking about the room now, or I go there, I expect it to be in some configuration, even though maybe it was changed at two. It's like saying, "Yeah, it was changed at two, but I know it hasn't changed again since then, so it should be in the configuration I got at 2:00." I'm going over here, the change occurred back there, and somehow I have to propagate it forward, knowing that nothing has happened since 2:00. Now I can imagine this, but I didn't actually observe that room at all. It was only when I get to the room or think about the room at some future time that I realize what had happened up to that point. Did anything occur? What should the state be? It's 2:30, last time it was changed was two, so it should be in this state.

Could it be that the change is actually occurring at two? Could it be like a mental simulation of going through that sequence and paying attention to where you want to? It could be as much as saying, but it feels more like going back to the last time I know a change occurred, mentally simulating the change. I don't go further back; I don't say what happened at noon or 11 a.m. I just go back to the point where the last change occurred and say, "Okay, when was the last change?" Oh, it changed to green. Okay, it's got to be green. I could do that.

It's weird. It would be like going to some point and then traversing back through time, but you could do it very fast, much faster than the behavior happens. I'm trying to imagine a neural mechanism for doing that—backing up, basically replaying the behavioral sequence that comes from layer one, faster or at the same speed. You would mentally pay attention to a certain location in that object's reference frame. Just by looking at it, you're observing it.

It's a little weird. It's like saying, "Let's imagine what the staple does and doesn't stop in between." Now I'm saying, "Oh, it's going to stop in between. What should it look like?" I can imagine, at any point in time, reversing in time until the last change. The last change was really nearby, so that wouldn't be hard. Halfway through the movement of the staple, the logo changes from Starbucks to Pets. Now I'm not observing that, but three-quarters of the way through the movement, I ask, "What should I be observing—Starbucks or Pets?" I'm not observing the point where it changed, so I have to go back in time to the last time it changed. What did it change to? Now I'm going to predict that. That's a bit hairy.

Related to this is the fact that when you look at grid cells, they're always playing sequences. During the theta cycle, they're at a point in space, but on every cycle of the theta rhythm, they go through a sequence of points in space. It actually goes from further away to where you are to further back. It's like playing the trajectory of your movement over some short distance. In theory, it would invoke experiences that you saw or expect to see very rapidly every theta cycle. You're not aware of it.

I have some speculations about why it's doing this. In some sense, you have a little window where you could go back in time and ask, "What would this thing be?" But I may need to go very far back in time. I need to be able to say, "At 2:30, what happened in this room?" There's a mechanism of going back in time that exists already, but it's usually over very short periods. If you have to do it over long periods, you can't do it at very high resolution; you wouldn't be able to mentally simulate complex long-term behavior.

I gave the rooms example, but that's just one change per hour, so you can speed that up very fast. But what if there were people dancing in the room, doing a jig to a very rigid time frame? I know that jig, but now I exit the room, come back, and even if I know the time, it would be very difficult to predict where they are because there have been so many changes, and I haven't learned the intermediate states. I have to go back to some point. That's possible.

There's research about rodents—when they're sleeping, the hippocampus plays back things that happened in their day, playing them rapidly. But things over a longer period of time...So it's clear that the neurons, at least during sleep, are able to say, "Let's go back and play it over again."

By the way, in the case of the stapler, imagine I'm trying to predict what I'm going to see. The idea of the logo changing halfway through means I might have to go back a fair amount of time to see when the logo changed. If I'm trying to predict where the edge of the logo is, I only have to go back a little bit in time because the last change was just a moment before in a different location. If I'm looking at this thing and I ask, when did the last change occur, I have to go back—maybe the edge appeared here, or the color would require going all the way back in time. It's a wonky mechanism. If you think through it, that's kind of what you're doing. If I think about the state changing, like the logo on it while it's occluded, I would mentally simulate the logo changing while it's behind whatever is occluding it.

Let's look at the lid of the stapler opening. Now it stops, and I'm looking at some part of it. What should I be seeing here right now? I have to go back in time to when things last changed. If it stopped and I go to another location, do I have to go back again? Maybe not, because maybe we're going to solve the top of the stapler. You have the top of the stapler rotation, and that gives you all of the predictions. So we're really just predicting morphology, or what if there was another behavior that's changing some part of the stapler? I wouldn't be able to learn that. I would have to split that again. Did you learn the stapler? I learned the stapler and it's moving this way, but in that case, the solution is to break out the whole top of the stapler as a subcomponent. There could be another change happening here. I'm not sure. Then I'd have multiple subcomponents, like the door, the top of the stapler. In all cases, I still have to back up a bit in time. I have to back up in time to predict what's going to happen.

On a very abstract level, one could argue you have to back up in time because the behavior is complex. If the behavior is complex, there is no other way of knowing the current state unless you play through the complex sequence of behaviors. It's like a logo danced around a little square, and now I'm going to look at the logo. Where is it? I have to go back in time and say, "Oh, yeah, it should be here," as opposed to just knowing.

It's easier with multiple columns. We can talk about multiple columns. It's easy, but it also has its own set of issues, which I'd like to get to. If we want to attack that next after our break, or whatever we're going to do next, we can talk about multiple columns. That's going to be really hairy. When do you want to go for lunch? Our board meeting is at one. What time is it now? Twelve. Maybe we don't have time to continue. We should get lunch at some point. We'll have to leave the multicolumn discussion for later. Maybe it's a good stopping point now, before we get too deep.

Where we left it is this weird idea that to make a prediction with the behavior model—only detecting when things change, when new things appear or disappear—to make a correct prediction, you have to go back in time for a particular location on the object to the last time it changed, and move forward to see what it is. The last time it changed tells you what it is; if it changed to green, then you know it's green. How that would work, I have no idea, but we do know there are mechanisms for grid cells that go back in time.

One other random thought I had earlier is that, evolutionarily, it seems like the behavior model is actually a lot more important than the morphology model. If you imagine animals that have a motion-based vision system, it almost seems like they only have a behavior model, and that's enough for most things you need to do—just recognizing behaviors. I agree with you, but I'm not sure it's more important, but it's definitely right up there. Many things you need to know are just behaviors. It doesn't really matter what the current morphology is. I wouldn't say morphology doesn't matter—obviously it does, because otherwise we wouldn't be able to model it. It's nice to see that there's a tiger there even if it's not moving, and especially for things where color matters, for example, that's important. But if you're just a dinosaur catching flies, you just need to recognize movement and go where that movement is happening.

Frogs, for example—there's a famous paper, "What Does the Frog's Eye Tell the Frog's Brain?" Have you read that? No. It's basically an argument that the frog's eye detects motion that is likely flies. It doesn't say anything else. It just says there's something moving in this trajectory, and it's probably something you want to eat. So it just has a behavioral model; the frog doesn't see anything else. In that case, it only needs the behavioral model and doesn't need the morphology model to catch flies—just fine-tuned to the behavior, like the trajectories of the fly. It's very simple. Frogs don't have cortex, but it's the same idea. It's like an early Barlow paper, the one I just mentioned. I don't know who the author was, but you can look it up. That's literally the title: "What the Frog's Eye Tells the Frog's Brain." I'm pretty sure that's the paper. Do you think that was some of the early Barlow work on the fly detector in the frog? I'm not sure. If you find it, it goes back to the super old guys, McCulloch and Pitts, so it would have been around the same era, 1941. I don't know the details of the paper, but the summary is that significations and everything else can cause little change of response. It's not light intensity, but rather the local pattern and variation of intensity as the exciting factor. I thought it was about flies. They don't mention that there. Maybe I'm confounding two papers. There's this one, and maybe there's another one that talks about flies. I thought this was the fly one, but there is a fly one. This isn't it.

No, it doesn't sound like it's only a motion-based system. It doesn't do that here, although there is another paper that does. I'm not sure which animals actually have motion-based vision. Our paper is based on the fly, which says it detects the flies and that's all it cared about. I thought that was a paper, but apparently not. Anyway, that was your observation, and I would agree with it. Behaviors are very important to recognize. Even if you're seeing an animal in a modeled image of trees and leaves, you want to see the behavior of the animal pop out if it moves, because the behavior model recognizes it as a particular animal. If the animal's not moving, you don't see it. So it's really important. Okay, we can stop here.