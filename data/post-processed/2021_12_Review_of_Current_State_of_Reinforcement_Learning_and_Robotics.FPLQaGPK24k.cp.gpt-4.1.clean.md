All right. This is a high-level overview. As I said, I don't have experience with real robots, only with simulated robots.

I can reference papers or work groups that work with actual robots, but since we don't have robots either, it's not that applicable.

If you want me to go into more details on a specific topic, I can do that at a later time. This is really high level.

Just as a short overview, so we're on the same page with all the terms: for sensory motor learning, there's an environment and an agent. The environment is in a certain state, which the agent receives, and then the agent decides on an action and performs this action in the environment, which leads to a new state. There's a closed loop between action and perception. The environment can also emit rewards or punishments, depending on how you define it, and the agent can use that to learn to perform optimal actions. In many applied situations, or in the real world, you don't have access to the true underlying state of the world. You only have a partial observation, which may be noisy, wrong, or missing information. For example, with your eyes, you can only see in front of you, not the state of the room behind you.

This can be described as a Markov decision process, defined as a set of all possible states of the world, a set of actions, all the transition probabilities of going from state S to state S' by performing action A, and all the rewards associated with these transitions.

The goal of optimal control is to find a good policy π, which is often probabilistic—so it would be the probability of action A given state S. This policy should optimize the future reward. Often, people use a discounted return, which means immediate rewards are valued higher than rewards far in the future. That's the general setup. Do you have any questions?

One question: it seems like this formulation is already limiting what's possible. If everyone in the field uses this formalism, you're already limiting the types of possible solutions and things you might do. What are you thinking of when you say that?

It's a Markov decision process, which means you're only looking at the current state and not considering the set of states you were looking at before. We used to call these high-order sequences and things like that. Markov decision process implies that? Yes, it basically says the Markov property, which means the future is independent of the past, given the present.

There are approaches in RL that try to relax this property since it doesn't always apply. Techniques like using recurrency or memory try to relax this property a bit. There are also other formulations like POMDPs—Partially Observable Markov Decision Processes—and some extensions of this as well. That would be a big restriction. Much of the state of the world would be hidden from you, because the past is some sort of state, or your attentions or actions are some sort of state you can't even observe at this point. That would be very limiting. A common trick in reinforcement learning is to give some of the past observations as part of the current state. For example, with Atari, people often give the last four frames stacked together as the current observation to get around this a bit. These are analogous to sequence learning and other approaches; you can give the last few states as input. But with long-range temporal dependencies, it becomes hard. Complexity explodes as you add more and more time.

That's just one of my observations. One more note: we don't always have to think of it this way, but this is a common way people look at it. Current solutions have broken that Markov property more often than not. It's very usual to use things like LSTMs to preprocess the state, and LSTMs will keep a memory. So it's a restriction of the mathematical framework, but I wouldn't say it's a restriction of reinforcement learning, since most people are going to break that in one way or another—even replay buffers break the property.

In general, you can usually learn long-term dependencies in the value function that is learned. After learning, it's not like you always have this extreme short-term memory and can never really learn long-range dependencies. It just takes a while to do that, because you have to propagate the reward through the whole timeline of the value function.

One other thing: it's phrased as finding a good action based on the state, but you could include the agent state as part of it too. Here, it's phrased just as the external state, but there's an internal state as well that you could base the action on. True. Here, the state is coming from the environment—literally just what's on the screen.

The state could also include an internal state of the agent. You don't really see that in this picture.

Solving these optimal control problems—this is a really high-level overview. If you want me to go into these details, it's like an entire textbook.

Roughly, you can divide it into value-based and policy-based methods, and at the intersection, there are methods that use both, like actor-critic methods. In essence, value-based methods calculate or estimate a value function, which is how good it is to be in the current state—it's the expected future return given the current state, under the policy of the agent. Then they use this value function to define the policy.

If I'm in the current state and look at all the states I can reach from here, I choose the state with the highest value function that I can reach, and then perform the action to get there. This is the policy. In policy-based learning, the policy is optimized directly by parameterizing it with parameters θ, and then performing gradient descent on the policy parameters with respect to some objective function, usually some form of the reward.

So that's the general distinction between the two. Within these, there are many more subclasses. For value-based methods, there are two more dimensions: width of update and depth of update. On the right side, width of updates includes dynamic programming and exhaustive search, both of which require a model of the environment. Exhaustive search means going through all the state-action transitions until reaching a terminal state in every branch, calculating the accumulated reward over each branch, and then selecting the highest one to perform the action in the branch with the highest return. To look ahead to the terminal state, you need a model of the world with all the transition probabilities. For example, in chess, the rules are simple enough to formalize the states of the world.

In many real-world applications, you usually don't have a model of the world and instead learn from experience by performing actions and collecting data. You can't go through all the different options, only one. Dynamic programming is a weaker version of exhaustive search, doing a one-step lookahead for all options. Experience-based learning methods include Monte Carlo methods, where you run an episode until reaching a terminal state, calculate the returns for all states, and update the values for only the states actually seen in that episode. In TD learning, you update your value function after every step: you take a step, check the expected reward versus the actual reward, calculate the TD error, and use this to update the value function. There are also blending methods like TD lambda, where you take more than one step but less than to the terminal state, and you can weigh these states in the value function update so the most recent states have the highest influence.

TD stands for temporal difference.

Your description of value-based methods was intuitive—how good it is to be in the current state. The policy-based one seemed more abstract. I was trying to figure out how they differ in principle and implementation. Let me get to the policy-based methods.

When we move to deep reinforcement learning, it can be applied to all these methods. The basic difference is that instead of using a lookup table for values for each possible state, we use a function approximator, such as a deep neural network, which takes the state as input and outputs a value. For policy-based methods, the policy is parameterized, which can also be a deep neural network or other parameters.

A classic example for policy-based learning is REINFORCE by Williams. Here, you have the gradient of the current policy, which is the eligibility vector, divided by the current probability of the action under the policy. This ensures the size of the update is not dependent on how often the action is picked in general. If it's a common action, you don't want it to influence how much you update the parameters. It is then multiplied by the return that was collected. You collect experiences, calculate the gradient of the policy parameters, and use that directly. There is no value function being estimated; you just calculate the discounted return, GT, which is the sum of all rewards received, discounted by how far it was from the time step t.

Does that make sense? It sounds like the underlying metric here is rewards rather than value. I'm trying to tease apart the difference. I understand you're optimizing, and there's some control over how things are updated, but it's hard to move away from the notion of how good it is to be in a particular state. Is it just the horizon it's looking at, or is it about understanding what a policy would look like rather than an abstraction of it?

In this case, you don't estimate how good it is to be in a certain state. Instead, you have a function: you input the current state and parameters, and it outputs an action. Imagine this as a deep neural network. It has advantages; in some cases, it's easier to estimate a policy directly rather than a value function. For example, in Breakout or Pong, to know which way to move the paddle, it's easier to look at where the ball is and learn a policy that moves the paddle relative to the ball, instead of learning how good it is to be in a state and inferring the policy from that. It's possible to do it, but it's more straightforward to optimize the actions directly instead of having a value function in between. You can also learn a stochastic policy, while in value-based methods it is deterministic. Sometimes it's important to have a stochastic policy, such as in games like poker or rock, paper, scissors. If you have a deterministic policy there, it can be easily exploited.

It seems like you could do stochastic in value-based methods too. The value doesn't force you to pick the highest value; you could add noise, saying with probability epsilon, you'll take a random action instead of the best one according to the value function. That is used to encourage more exploration.

But it can't actually assign probabilities to different actions. It can't say, "These two actions are 50-50 and all the others are never selected." That can only be modeled by this approach.

Back on the policy side of things, in the examples you cited, it almost sounds like you were learning rules for processing a particular situation. You gave the example of Pong, where there is some kind of association between moving the paddle relative to where the ball is. Is that where the policy is situated?

Yeah, pretty much. It's always difficult to interpret if you're working with visual input, but for a lot of robotic tasks, you just get a few numbers, like the angles of the joints or distance to the ground. It learns straightforward rules for when to increase or decrease velocity.

So, in some sense, it's a higher-level process where it's trying to learn the operant rules of the situation rather than just optimizing based on past successes. With policy-based methods, there is the possibility of learning general principles, as opposed to value-based methods, which focus on becoming an expert at a specific task.

Both approaches can generalize if you use function approximators, like deep learning or other functional approximators. But if you use lookup tables, as in the original methods, you can't really generalize to unseen states.

The last one I'll talk about is the actor-critic method, which combines both approaches. There's an actor and a critic. The critic uses value-based learning, gets the state from the environment, and produces a value estimate using parameters like w. The value estimate is compared with the actual reward, and the TD error is calculated. The critic tries to minimize this TD error to make better predictions of the reward. The actor also receives the state as input, has its own parameters, produces actions as output, and uses policy-based learning to update the policy. It tries to maximize the TD error by producing actions that lead to states better than expected.

The critic does impact the actor. The actor needs to use actions that are better than the critic expected them to be.

For example, PPO, which is a popular method, falls into this category, as do A2C, A3C, and trust region policy optimization. Many state-of-the-art methods are in this category.

There are also other methods, especially for offline learning situations where the data isn't generated by an agent but was collected before, for example by a human, or when you don't have much data. There's imitation learning, behavioral cloning, and inverse reinforcement learning, where you try to infer the reward function from state and action examples.

That was a rough overview. Are there any more questions? No, that was very clear, thanks.

If you'd like more information, I made a reinforcement learning crash course at the beginning of this year, and it's on YouTube if you want to watch anything in detail. I also have a short chapter on that in my thesis, which I posted in a Slack channel.

Now, to the challenges.

This list isn't exhaustive, but some of the bigger ones are included. The first paper, "How to Train Your Robot with Deep Reinforcement Learning," is recommended if you want to read more about challenges with real-life robots. There's also a blog post that's well cited and well written, with ongoing discussion on Reddit. The critiques raised are still valid in 2021. Things are incrementally getting better, but no serious breakthrough has happened. Challenging fields like robotics manipulation are still far from satisfactory.

Are these part of the blog post? No, that's from a Reddit thread about the blog post. The issues discussed are still prevalent. Vivian, who wrote the blog post? Alex Yopam?

I can share the link. I'll share it in Slack.

Okay, just to start, here's a quote from Andrej Karpathy, who I think is Head of AI Research at Tesla: "Supervised learning wants to work. Even if you screw something up, you'll usually get something non-random back. Reinforcement learning must be forced to work. If you screw something up or don't tune something well enough, you're exceedingly likely to get a policy that is even worse than random. And even if it's all well tuned, you'll get a bad policy 30 percent of the time just because." I've had a similar experience over the past years.

Why would supervised learning be like that, and why would reinforcement learning be like that? I don't have an intuition for why that's the case. Is there an intuition for it? I'll go over some of the challenges, but one basic problem is that in reinforcement learning, you don't get the true label. In supervised learning, it tells you what would have been the correct answer, but in reinforcement learning, it doesn't say you should have performed this action. It doesn't even tell you if the action you performed was good or bad. You might just get a reward after a thousand more steps because you reach the goal, but you don't really know.

Isn't that just restating the difference between them, as opposed to why one is so much harder and you can focus on the wrong things and do much worse than random in reinforcement learning? Would you have expected that up front? I think people thought TV learning was done in the 70s or 80s—Sutton and Barto talked about this even back then. Another big challenge is that in supervised learning, the whole dataset is typically fixed, whereas in reinforcement learning, as you change yourself, you take different actions, so the training data itself changes based on how you do it. There's this double loop, which makes it even more complicated.

You don't generally hear this critique of reinforcement learning. It's often said that reinforcement learning is all you need, but this really jumped out at me. Is this how everyone thinks? Most people don't think this way. It sounds very much like how humans go on: you can get rewarded for the wrong thing and continue down that path, but you generally don't screw up worse than random. Maybe it's just an uninformed impression, but reinforcement learning is often seen as the gold standard of techniques. This says, "Oh no, this is terrible." 

Reinforcement learning has a lot of advantages and unique applications. For example, you don't need a huge labeled dataset; usually, you just need to define a reward function, which may be a lot easier than labeling a big dataset. But this all implies it doesn't work. So what's the advantage of something that requires so much effort? DeepMind can do it because they have tons of resources, but a random lab might have trouble with it. That's why there are few applications out in the world—it's really hard to deploy a system that doesn't work reliably.

Now it's really hard to deploy a reinforcement learning system, so there are few applications. The nuance of the last sentence, at least for me, is that it's less about training a bad policy that ends up being very bad at the end. It's that the system you train has no idea where it went wrong. Vivian will probably get into the credit assignment problem later. We really have no idea how to fix it. You don't know where you want to go. I can get into a lot of these things and show some examples.

As a short overview of some of the bigger challenges: sample inefficiency, learning from dynamically changing non-IID data, reproducibility and hyperparameters, specifying a reward function, the credit assignment problem (where did I actually go wrong, which action contributed to the reward), and dealing with delayed and sparse rewards. Then there's the classic exploration versus exploitation dilemma, generalization, transferring from simulation to the real world, and in the real world, safety.

First, for sample efficiency, there's a classic 2015 paper from DeepMind that got Deep Q-learning on the map, solving Atari games with human-level performance. But human-level performance in terms of score, not in terms of how much training is needed. They trained for 50 million frames, which is around 38 days of game experience, while the human performance they measured was after two hours of practice, which is quite a difference. They also wrote that they did not perform a systematic grid search owing to the high computational cost, which, coming from DeepMind, must be a pretty high computational cost.

It is pretty sample inefficient. The experiments I ran with the Obstacle Tower Challenge could run for multiple weeks and still be learning and improving.

It takes a lot of samples. Here's another example from Unity: a simple game called Bubble Shoot or something like that. They shoot the bubbles, and when you get more of a color, that disappears. But still, it takes over 60 hours to solve with one simulation, and if you use more parallel simulations, it takes less time, but it's still quite a long time for such a simple game.

This long training time is due in part to the number of samples needed, and the rewards can be very uninformative. It takes a while until the rewards are actually helpful for learning. If you only get one at the end of a whole episode, whether you reach the goal or not, for example, there are weak inductive biases, which is the same for all deep neural networks. You also have to consider that you need to simulate all of this to collect experiences. You don't have a huge dataset that you can just throw onto a GPU in a big batch; you have to collect these experiences one step at a time, which takes time and is often not optimized for GPUs. Are these inefficiencies related to catastrophic forgetting, or is that a separate issue?

I wouldn't say so, because usually the general setup doesn't change enough to lead to catastrophic forgetting.

With our paper, we're doing multi-task reinforcement learning. This is like learning one task, and then imagine doing this for two very different tasks. All these problems compound because each task is telling it to do something completely different, and that's where you get interference and catastrophic forgetting.

There are a few solutions proposed to this, and some things work pretty well. For example, "Reinforcement Learning: Fast and Slow" proposes two solutions: episodic memory and meta-learning. Episodic memory means using memory of previous experiences to solve the current problem—looking back at how you solved the problem in the past and using that information. Meta-learning is basically learning how to learn, so you learn how to learn in order to solve each individual task faster in the future. This is partially why humans can learn these games so quickly compared to AI, plus model-based reinforcement learning. Humans already have a model of the world they can utilize. We know how balls move and how physics work, while the network has no idea about this. If you do model-based reinforcement learning, you don't need to collect experiences; you can use your model and think about the options and compare them. The way I see it, brains use model-based everything—we build models and solve every problem. Calling it model-based reinforcement learning seems odd; you could just call it model-based learning.

There is a difference between having to learn the model and being given the model. For example, in chess, you are given the model—you have the rules, you just need to evolve. Chess is a weird example; it's such a constrained system. In general, for problems where you're trying to move things, navigate, or solve a puzzle, model-based learning seems to be the way to go, not just model-based biases. It's interesting that this seems to be the answer to how humans do all these things: build models of the world, understand how they work, and reapply them. Some recent breakthroughs have come from actually learning models of the world and utilizing them in the reinforcement learning setup, and there are already results showing this is very useful. Is it wrong to think that all of our work is basically about model-based modeling? It's about learning models, and then you would add some kind of reinforcement learning to use this model to decide on actions.

Some would argue, and I agree, that if you have a large enough replay buffer, that's very similar to having a model—you're just storing that model in the form of samples. When you want to know a particular transition, you get the sample closest to it. That's like episodic memory. There's a lot of overlap between memory and model-based approaches. If you have a large enough memory, you just have a model—just remember everything. That's brute force and not very efficient. In that case, it would also be more difficult to generalize if you just remember. It would be harder to generalize, but if you can do it in a way that generalizes, like with a cluster-based retrieval approach, you get whatever is closest to your experience. There's a blurry line, and I would never call DQN a model-free method; I think it's in between.

In those terms, it distinguishes itself by not learning the transition probabilities of the world. When it needs to decide an action, it doesn't look ahead multiple transitions to make the decision.

In a sense, it has a model of the world in the weights, but it doesn't have an explicit transition model of the world.

Does that make sense? Now that you mention it, are there models that use the replay buffer to roll out?

You would need a pretty big replay buffer for that. I'm just wondering if someone has tried that path.

I'm not sure. Thinking about one of these game-based things, like Breakout, right?

What am I learning when I'm learning how to play Breakout? I can learn to play the game in about 10 seconds, but I'm not fast enough, or someone explains the rules of the game at the plate. If the game was very slow, I think I could play a perfect game right away. I wouldn't have to practice. Understanding how to solve the game seems trivial to me. What I'm really learning is how to do it quickly, and that's because it's running at a rate I can't keep up with. I can't think too much about it. I have to practice to get faster, but the problem-solving itself is instant. I position the paddle under the ball before it hits the bottom, and I'm done.

We have to be careful about mixing up what we're learning. We're not learning how to play the game when we practice for two hours; we're learning how to get good at it, how to do it fast enough, which is a different type of learning. It's about dealing with the slowness of brains and how we can get around that. It's not fair to compare deep learning Atari with human Atari because, for humans, it's about speed, but for the computer, speed is not a factor. The whole thing stops until the network makes a decision, and then you get the next state of the game. I don't have to practice to learn to play that game. Once I know what the game is, that's it. You already have a pretty good model of how trajectories of balls work. The model is already in my head: a ball is going to hit something, it's going to bounce, and when it hits the bricks, they blow up or do whatever they do. The only learning is just getting fast. The game is designed to push the limits of human ability in terms of speed. That's the trick—you're trying to get faster and faster, but it's not a problem-solving game; it's a speed reaction game.

There are a lot of games like that. That's what you're really learning—how to do it quickly. There are other games where you're actually solving problems or don't know how to solve them, like chess, which has a different agenda. Chess is more complex. Tetris sits in between; you can position the pieces, but not always well enough. Reinforcement is a key part of learning in humans, but many things I do, like robotic actions, don't require a strong reinforcement signal. Learning how to pick up an object or manipulate something isn't gamified. I didn't have to learn these tools through reinforcement. Relying on reinforcement learning as the key strategy for everything seems off base. To me, it's all about model-based learning. You need to learn to model the world. Once you have a model of the world, you can solve any problem if your model is good enough. That's not to deny that reward doesn't play a role, but the ratio between reward learning and model-based learning has always been skewed. Brains are basically all about learning models.

That's why I find curiosity-based approaches so interesting. You learn a general model of the world and then use it quickly to perform different tasks. It's a promising research direction to try to learn very good models and then utilize them with very few reward signals, or none at all. If we're going to look at what the mind does uniquely, we have to focus on model-based learning because that's our strength. We need to be careful when looking at robotics challenges and tease out the ones suited for model-based learning, not the ones that require speed or constant reward functions. That's the perspective I'm going to take with all this. And sample efficiency relates to that—if you have a great model, you don't need many more samples. You've already learned a lot.

They reference the book "Thinking Fast and Slow," which talks about fast and slow learning. Learning the model would be the slow learning, and fast learning would be taking the model and applying it to a task and solving it immediately. I didn't like the basic thesis of that book because it made a distinction I felt was unnecessary. It suggested there are two separate worlds we live in, but really, we have a model of the world that takes time to learn, and then we can act quickly based on that model. It's not that there are two different worlds or two ways of solving problems.

Okay. Let me go on to the next challenge: learning from dynamically changing non-IID data. Generally, when training deep neural networks, we assume independent identically distributed data, meaning if I pick a sample now and then pick the next sample randomly, it's independent of the previous sample. That's clearly not the case when you're acting in a continuous world. This violates a basic assumption of deep neural networks, which adds extra complication when using deep reinforcement learning and can lead to unstable training.

As Subutai already mentioned, the quality of the policy determines the quality of the data being collected. If you have a bad policy, it's really difficult to recover because you don't get good quality input data to learn from. You can get stuck in a bad policy. One common solution for the non-IID problem is experience replay: you have a policy, use it to collect a bunch of experiences, and then take random batches of samples out of this buffer and replay them. You don't replay in order; you take a random sampling of the episodes and mix them up. Each episode might have its own order—you might have the last thousand games stored, but each game has a sequence. You pick a random game, play the game in sequence, calculate the discounted rewards in sequence, and associate the discounted reward with this experience sample. Then you can mix it up, and it doesn't matter anymore.

This is one of the key tricks that got the 2015 Q-learning paper off the ground. The method was already around for a while, but people couldn't get good performance in combination with deep neural networks. Using this trick was one of the keys. Still, even with that, it's difficult, and you can get stuck in a bad policy. But we live in a dynamically changing world, and we can learn, so there must be a way.

Next, there's reproducibility and hyperparameters, which is related to the previous issues. Small changes in the hyperparameters can have a large effect on performance, and because of sample inefficiency, it can take a lot of computation to tune the hyperparameters. Even a small difference in random seeds can cause large variance. On the left, this is from a blog post where he ran ten runs of the same parameters—everything is the same except for the random seed. Three of them never got off the ground, and the others still have some variance. This is from a 2019 paper comparing different state-of-the-art algorithms, and you can see some have huge variance. If you try to tune hyperparameters with this, it's really difficult because if you have the same hyperparameters in two runs, one is at this performance and the other is here. It's hard to say which parameter was actually better without running multiple runs of the same parameters, and this takes a lot of computation. These problems compound.

Does this kind of sensitivity to initial conditions also result in susceptibility to random small perturbations in the end result—adversarial attacks? Yes, adversarial attacks. Or, once you get a good model, is it robust, or does this susceptibility continue? Inherently, everything is susceptible to adversarial attacks. Brains aren't really susceptible, not in the same way. Deep learning networks, no matter how good they are, can be susceptible to adversarial attacks. I knew that was true with deep learning networks, but I didn't know if it was true for reinforcement learning. But they're using deep learning, so I guess it is. The brain is susceptible in some senses, though. For example, when we were looking at a three-dimensional shape on a whiteboard, that's in some sense a perceptual attack. I wouldn't call it the same thing. It's hard to see it as flat, though. That's actually the beauty of the whole thing—having a tunnel. First of all, I think that's a plus, not a minus. Second, by putting it on the board, I don't see it as a school bus. It's not the kind of thing you see in there.

It seems like the solution is to pick the best random seed and only publish with that. There's a startup idea: choose the best random seeds and sell them—magic seeds. We've heard the story about magic seeds before; that's basically what the world of finance is.

We've seen in our experiments that the seed only works if you see the whole thing, but even if you set the initial conditions—same model, same weights—the problem is that with the interdependence between data generation and modeling, even after a few steps, they're going to be widely different. We'll still have that variation, even if you set the initial conditions to be exactly the same. I think there are three seeds: the seed of the environment, the seed of the network weights, and one other. There's a lot of stochasticity, so you have to set the seed for everything. If you're running on GPUs, there's inherent variability in the GPUs that you can't set unless you use some deterministic mode, and then things are slow. There's always some indeterminacy.

This again seems related to the issue of model-based learning versus non-model-based learning. Model-based learning seems to inherently get around many of these problems.

The model improves over time, but model-based learning solves these problems by assuming the world matches the model. If the world differs from the model, it doesn't work at all, but if the world is similar, it works very well. Without model-based learning, you can learn anything, but it's very fragile. This seems inherently true. For model-based reinforcement learning, you have less variance, but if you need to learn the model from experience, you still face this problem because you need to sample experience in some way.

I disagree. Consider the recent work we've discussed, where the model isn't just learned from experience. There's an inherent structure in the brain's modeling system that makes assumptions about the world. The brain already has built-in assumptions about what models of the world will look like—the structure, dimensionality, how movement relates, and how information is stored. The brain essentially says the world must fit into this model. It's not learning the model from scratch, just filling in the details. That's why there's a big difference in learning a model. If you add more inductive bias to the model and don't just use a neural network with random initial weights, it solves some of the problem. You still have to learn a policy, which requires a starting point. They also show the policy.

Given our recent research on how brains model things, where does policy fit in? What is the policy? I don't have a clear sense of what policy means. It's just choosing the motor command, and we haven't addressed that yet. We haven't determined how to select the next motor command—that's what policy means. This is why I wanted this presentation: to understand how to bring motor behavior into our recent modeling work and what problems we should solve by introducing policy. If our work doesn't include policy yet, what problems should we be solving? This is a fundamental question about why we're discussing policy and what problems we should address. Defining the policies allows us to judge system performance or introduce behavior into our models, which is a challenge. You mentioned that all the models are in the brain and all the information is already there to solve robotics problems, so we just need to know what problems to solve.

Vivian, I want you to respond to Jeff, but I also have a follow-up question. A few weeks ago, I mentioned Judea Pearl's three-tier notion of reasoning: associational, intervention, and causal. He criticized machine learning for being stuck in the associational phase, except for reinforcement learning, which enters the intervention phase by allowing you to choose what data you get next.

What problems can you solve with reinforcement learning that you can't solve with just machine learning? One obvious direction is learning causal relationships. Is there work on that in reinforcement learning? I know this is a topic of discussion, but I'm not sure what's happening in reinforcement learning regarding this issue. Wouldn't causal relationships essentially be model-based? Isn't that the same thing—having a model of the world to determine what leads to what, what causes what? Aren't they almost the same thing—model-based learning and causal reasoning?

If you have a model, you have the transition probabilities and can determine what causes what, not just correlations between experiences and actions. The difficult question is how to obtain this model if it's not already given. We know that children perform very directed, scientific experiments to figure out causality, but this behavior isn't really seen in reinforcement learning agents yet. Even those with curiosity tend to seek novelty rather than perform directed experiments. There are attempts to instill this in reinforcement learning agents. For example, an environment called Alchemy was proposed, where the agent can mix different rocks or chemical compounds and experiment to discover regularities and causal effects. However, I'm not sure if there are really good solutions yet.

Can I add something, Vivian, that relates to Ben and Jeff's question? Model-based reinforcement learning has been proposed for a while. The first example is from around 1989 or 1990 with Dyna, but it never really worked well because, at the beginning of training, everything depends on the quality of the model you have. If you're not relying on your experience but on a model instead, that introduces a huge variance in your system when you do rollouts into the future. At the beginning of training, your model is really bad, so your rollouts are going to be catastrophic. You get stuck and can't move away from this initial phase. For 20 years, model-based RL never worked well because of that and still doesn't work that well. The difference, I think, in Jeff's point of view, is that for humans, or in our model, we introduce very heavy inductive biases into the model. We already assume the model works a certain way, and that's it. If you have a good model from the start, it might be able to do that, but otherwise, if you're just learning from experience, it becomes really hard to do, at least in this framework. That would be for the causal reasoning aspect.

If you're doing something like Atari games, you're not learning from scratch. You've learned throughout your lifetime about how balls bounce, how things hit each other, and how they move. You're able to immediately transfer that experience into a new environment, which is totally different. You've learned a lot of relevant things, even if it's not exactly this particular set of experiences, so you can transfer this knowledge very quickly. This ability to transfer knowledge so fast from one setup to another is a huge aspect of what we're talking about. I'm taking it one step further and saying that the brain is designed under the assumption that there are physical objects in the world, they have presence, they move in certain ways, and it's three-dimensional. The brain assumes the world already has a whole bunch of constraints. Even though you have to learn the details, all these mechanisms—how much of this is predetermined? What's predetermined is all the things we've been talking about: constraints, transforms, and all this stuff. There's an assumption in the brain that you have a body, it's articulated, it has sensors, and it has to learn the physical structure of objects and how they behave. Those objects are contiguous in space and have certain abilities to move in some ways but not others. All that's predetermined; you just don't know all the details of what objects will occur.

In the language of machine learning, that's inductive bias. We have a lot of those. This is why we have trouble playing something like Go. You look at the board and can't really see the structure of the game. It's really hard; you have to spend years trying to figure out how to interpret it in a way that you can look at a board and say, "Oh, I know," otherwise it's just a bunch of dots on a square.

Games play a role here as well because they're designed for our inductive bias—they're designed for humans to be able to play. That's why we're good at them, but we could easily design a game that doesn't attend to our inductive bias, and a machine could solve it much more easily than humans. Go is an example of a game where our inductive biases don't work well. It's like looking at Greek letters or another language. I look at a Go board and can't see the structure, but someone who's been playing for years has learned a new way of thinking. It takes a long time, whereas a game like Breakout is immediately understandable: there's a ball, there's a paddle, and you know what to do.

That's why deep learning networks have excelled in places where humans have difficulty. Even with things like protein folding, we can't look at that and understand it, but with enough data, people can. I think we've covered this. AlphaZero, which is super successful with Go and Chess, uses model-based learning and Monte Carlo Tree Search. The difficulty is how to do it when the model is not given to you and you have to learn the model.

In AlphaZero, I remember the first AlphaGo learned via human policy. It's not exactly reinforcement learning; they were relying on replayed games. How did they start with a good model to work with?

They have all the transition probabilities because of the rules of the game and the other player, whose actions you don't know.

I just know the general Monte Carlo Tree Search. You simulate possible future trajectories, their probabilities, and possible future rewards, then pick the best with some probability.

We should keep going; we have about 15 minutes left. The other points are also not that long and more fun because they are pictures.

Getting stuck in a local optimum can also be related to the reward function. For instance, here's the half cheetah. It's supposed to learn how to run as fast as possible, and this is the half cheetah that found a strange local optimum by flipping itself on its backside.

As you can imagine, it will never learn how to run because, once it's on its backside, it's not going to figure out that it's actually better not to flip over, and it will just try to go faster.

That encapsulates everything that's wrong with machine learning. There are some really funny YouTube videos with reinforcement learning gone wrong.

Another popular example is that specifying the reward function can be very difficult. It's like the Greek myth of King Midas, who wished that everything he touched would turn to gold. The god Dionysus took him literally, so everything Midas touched turned to gold, including his wife and his food, leaving him unable to eat and his family as gold statues. He was very unhappy. Reinforcement learning agents are similar—they take the reward function literally, which can lead to unintended behaviors. For example, a boat in a simulation was supposed to run on a racetrack, but someone thought it would be good for the boat to pick up turbo packs for a small reward. The boat figured out it could just run in loops, waiting for the turbo packs to respawn and picking them up repeatedly to maximize its reward. The reinforcement learning agent will exploit any flaw in the environment or reward function if possible. We encountered this with a robot arm: I thought it would be good to reward the agent for touching objects to encourage interaction, but this just led to the arm pressing down as hard as possible on the touch sensors.

You might have good intentions, but it almost always leads to unintended consequences. The reward function strongly influences what is learned, and it's difficult to specify exactly what you want.

If you only specify the final goal, it may never be learned if the goal is difficult and doesn't happen randomly, leaving the agent with nothing to learn from. In such cases, reward shaping can be used, where you first reward approximations of the behavior and then gradually reward the actual behavior. Other interesting directions in reward function design include multitask learning, open-ended learning, and curiosity.

Credit assignment is another challenge. How do you know which actions caused a delayed reward? Many tasks have sparse reward structures, making them difficult to learn, especially for long action sequences. Current solutions include discounted rewards, where more credit is given to actions performed shortly before a reward arrives, and eligibility traces, such as in TD lambda learning, which weigh states closer to the current state more than those further in the past. Hierarchical reinforcement learning can also help.

Having causal models of the world is also beneficial, as credit assignment is not just about temporal co-occurrence. We solve problems by having a model and using it to guide our actions, rather than just trying different things to see what works. However, we're not always good at seeing solutions that don't fit our model. Sometimes someone else can show you a better way to do something, but you rarely figure it out on your own. Having a model allows you to solve problems right away, but there are exceptions. For example, in music, no matter how good your model is, you can't learn to play piano just by thinking it through; you have to practice. You can learn to read notes and play them slowly, but playing at tempo with the right nuances takes years. It's not hard to play piano slowly and poorly—believe me, I'm trying.

Regarding exploration versus exploitation, the problem is a classic one, formulated during the war, and efforts to solve it were so challenging that it was suggested as a form of intellectual sabotage. The question is whether you should explore more unknown options to find higher rewards or just exploit what you already know. It's still an open problem. Current approaches use a stochastic policy, with high stochasticity in the beginning to encourage exploration—similar to childhood in humans—and then decreasing stochasticity over time to favor exploitation.

There are also curiosity objectives that encourage more directed exploration in unknown areas, rather than just random exploration.

Generalization and sim-to-real transfer are also important. Reinforcement learning agents often overfit to strange features of the environment. For example, in Atari games, agents sometimes memorize a suboptimal sequence tied to the game score, so if the high score is covered, the agent performs poorly. In real-world robotics, there are additional complications like wear and tear, latency, and less precision than in simulations. Solutions include training on a wider variety of inputs, adding more randomness to prevent overfitting, and incorporating latency models or recurrency and memory in real-world robotics.

And then safety is pretty obvious. If you have a random policy, it can be dangerous and may break the robot with random jerky movements. You can restrict the action space, regularize behaviors, or learn to recognize unsafe actions.

As a final slide, are there too many challenges? Is reinforcement learning broken? I trained this obstacle tower network with 2,500 neurons, and it learned a complex navigation and vision task with a small network. Even cats and dogs have many more neurons and sometimes still behave strangely and act non-optimally.

We can't always expect optimal behavior. I wanted to give an overview of what is actually out there and what can be done at the moment, but that would make the talk too long. The one thing I still need to know, from a practical or commercial point of view, is what important and unsolved problems remain. You went through the machine learning theoretical perspective, which is useful, but that other question is also important. Most of the situations discussed aren't really practically valuable, especially for robotics. What are the things people would like to do in the commercial world of robotics or practical applications that are just not doable today? For that, we'd need someone from the industry. 

As for ambitious goal proposals, a home care robot would be really useful because it would need to pick up many different new skills quickly. Let's break that down: what are the primitives a home care robot has to do? I don't think we should focus on understanding the disease of the person or their needs, but rather on what the robot physically has to do. Does it have trouble navigating? Does it have to pick things up? Does it have to dispense medicines? Does it have to unscrew caps? What are the things it needs to do, like retrieving items from the refrigerator—practical, low-level tasks that robots don't do very well yet? 

Maybe even things like recognizing when the person being cared for is acting strangely or is at risk of falling. Preventing that isn't really robotics, but it's a good task. Right now, I'm thinking about the robotics challenges. Many of the things mentioned would be subcortical, maybe. Unscrewing things, maybe not. The cortex has to learn what a screw top is and how different bottles open. Maybe subcortical networks assist to make smooth actions and prevent using too much force. But those are cognitive functions—you have to learn how to open a refrigerator door, take lids off things, pour liquids, and so on. All of these require a model of the world and the ability to manipulate it.

Our latest work suggests we might be able to build very articulate robotic systems that physically manipulate things because we're building very detailed models. We're showing how we can learn detailed models of objects and their articulations. What would be a robotic task related to this? Building a better robotic arm, picking things up, manipulating, grabbing, and turning things. I don't know what all those tasks are. Maybe some have already been solved, or they use a vacuum, or something else.

Taking your example, the specific task of transferring an invalid from a bed into a wheelchair requires just that. It requires perceiving the person's position, determining where to move them, and how to do it. That's a good example—a complex version of the kind of task I'm looking for.

We're starting to talk about dealing with a human who's frail. You don't want to start there, especially with tasks that require multiple arms or people. That's a tough problem. Let's pick something more manageable, like what a single robotic arm can do that's not going to harm someone. I mentioned the Japanese are investing a huge amount to develop robotic elder assistance. Viviane suggested that too, but cracking an egg is another example. These are all good examples, but I'm trying to get at more basic primitives. We don't want to create an egg-cracking system or a human-lifting system. We want to understand the basic functional primitives required to solve a set of problems.

Cracking an egg could represent many different manipulations. Taking the lid off a jar is, to me, no different than cracking an egg. It's about picking up an object with the right approach, getting the right orientation, and manipulating it very precisely. You need to observe the results to know whether you've achieved the goal. I think you mentioned it's important to hold on to it sufficiently, but not so much that you damage it. What I don't know is whether that's a valuable problem commercially. We can say it's a hard problem, but is it viable? You'd have to talk to someone in the field of robotics to know about selling or building practical systems. We can identify these tasks, but until you talk to someone who's actually in the field and dealing with these problems, you don't really know what they want. At some point, we'll need that information, because we could spend a lot of time solving a problem that's already been solved or isn't the real problem.

Most robots handle things like bolts, where it doesn't matter if you put a slight scratch on it. For anything more fragile, you need to be able to pick up an object without damaging it. But then, what do you do with it? You have to position it, screw it in, or manipulate it in a certain way. Just the act of grasping is a challenge. These are all good examples. We need people who know the field to help with this. The grasping problem was famously solved in the Amazon challenge, where, once they identified the object, they just used a vacuum to pick it up. It didn't damage the object, and it worked. That was the solution, so we have to be careful.

Can I give my opinion on this? I think the issue isn't solving a problem. It's actually very easy for these systems, given enough simulations, to solve something. If you design a good reward function and give it enough time, it can solve anything, because it will exhaustively search the space of solutions. The problem is dealing with variances in the world and transfer learning. It's not hard to solve one thing. The challenge is that you find a very specific solution to a very specific problem with RL-based approaches. It's not hard to solve one thing; it's hard to make a robot that solves many things. That's probably the main challenge in applying reinforcement learning to robotics.

A big deal would be solving a way of continual learning—a robot learning things in sequence—or transfer learning, where it learns a good enough model of the world to apply to different problems. You could plug that part of the world in, and it could easily extend to another activity. Let's take an example: you want to learn how to pick up and manipulate a certain set of objects. You build a model-based system, and what you really want is to say, "Here's a new object, add it to your list, here are some good views of it, go for it." That's appealing and the kind of thing we've been discussing.

But then the question is, what's commercially valuable? I'm trying to get to the point where someone says, "I need that in my business; that's going to solve this problem in this commercial field." We're researchers, so we might not know this. For example, Google just added a new feature to avoid Christmas trees. That's something you just add by upgrading the model. Google is not a very smart system. It's fun, but it's quick to do.

It's interesting to think about. This conversation will have to continue.