so this is the, ultrasound that we've been using. we got this online from butterfly ultrasound. so this is it in my hand, for scale. and on the right is an actual, ultrasound at the top. You can see the bit of the probe. and then down here you can see something that's definitely not a tumor. This is the cross section of a. French is mustard bottle. So that's your primer for what ultrasounds look like. Good luck. You weren't able to tell. It's pretty obvious. It's pretty obvious.

Yeah. And what's exciting about ultrasound is obviously it has a huge number of applications, medical being one of the main ones, and it's an inherently a very sensorimotor, way of clinicians understanding, what they're seeing. and it's very challenging to use AI for this because in the medical field you generally don't have large data sets. There's lots of reasons that Monty would be perfect for this kind of use case. but ultrasound is not a very intuitive, modality, at least if you're first dealing with it. this is for example, what, the Thousand Brains Project Mug. Looks like in, ultrasound or part of it, rather. and what you need to bear in mind is that ultrasound, the probe is producing a two dimensional beam like a fan. but that two dimensional fan is moving in three dimensional space, going through, a structure. And as the name implies, the probe is emitting, sound waves, ultrasonic sound waves. Basically the time it takes, for that to bounce back tells you how far away something is. And the intensity tells you how, white that is. So something like bone or metal, or ceramic like we were dealing with tends to show up as really intense, really white, whereas kind of soft tissue and things like that appears, more gray. and, and then water would just generally appear like black. And so that can give you a huge amount of information, but it is challenging to work with. So you know, for example, the TBP mug, as I of saying, this is an image of it, and in particular it's an image, a 2D plane going through the side of the mug and side of the handle. And so what you're seeing here, for example, this is the top of the cross section of the handle. If you can imagine, you had a very sharp saw that went straight through here. This would be what the top of that would look like, and this would be the top of the, side of the mug, but you don't necessarily see everything you would expect because, for example, you get what are called acoustic shadows where the sound, cannot continue to propagate. And so it's black behind this, edge. and you also get lots of artifacts. So these can be caused by various reasons, if sound wave gets delayed, if it gets reflected multiple times, all this kind of stuff, you can basically, the image kind of hallucinates, as far as the probe is concerned, it thinks there's objects out there in the world. And so this is, these were some of the things we had to, deal with when, when interpreting these images. Yeah, so it also turned out to be quite a lot of moving parts to setting this up. So I'm just gonna talk you through the architecture diagram really quickly. so over on the left we have the ultrasound device to it. We strapped an HTC Vive location controller, with just a piece of Velcro. When you click the button on the probe, it captures what the probe is currently looking at and sends it to Monty with some metadata about the depth, the scale that we were looking at on the image. At the same time, the HTC vi controller is streaming its location to, two. Two sensors on the floor, which then send them to, not pictured here because it was even more complicated, which is then streamed to the HT V Vive service where it stores the current orientation and three dimensional location of the probe. as Monty processes it, gets that image and then it asks the VI service for the data. And then, it visualizes that over here so we can see what we're actually looking at. It will also suggest, where you should move to next. So Monty can't move us, but it could tell us where the next best view to get the next best inference would be from. and then also, and visualization of, how Monty is doing what it's learning as we go. And so you'll see all of this in the demo. We wanna just give you a quick overview.

Yeah. And so one thing just to give you a bit of a sense of, what the whole thing looks like. So we had what was called a phantom. So phantom is when you're testing ultrasound or someone's learning how to use ultrasound. And so you have an artificial structure, that can be scanned. might have an obstetric phantom. But since we were dealing with objects that Monte has learned in kind of simulation like the YCB objects, we created this setup with essentially a plastic bag filled with water. So you don't, you can imagine it's something like an amniotic sack with objects suspended inside of it, and then all of this kind of set up with some tripods so that we can, kind move around it. And then, oh, sorry. We're basically putting the probe on the surface of this, bag with some, ultrasound gel, so that it gets a consistent, propagation of the signal. and of course the task is to determine what object is inside the bag. and as Will mentioned, we were tracking the position of the probe. The way we did this was with these, What are called kind of base stations, these are actually designed for virtual reality gaming. And so people have already spent a lot of time engineering a good solution to this. And basically this tracker, it actually is, look, this tracker, looks fairly what do you call it? Static, like it's not doing much, but it's actually looking for, invisible lasers that are coming out of these two base stations on either side of it. And that tells where it is, and as well as its orientation in the world. and. Quite a good amount of, sensitivity. so that was really useful for, this task. And so after we basically get the ultrasound images plus the tracking data of where the probe is in the world, we now need to extract some more information from the image. And so this is the processing pipeline. First, we have the environment class from Monty that we customized so that it, gets both of these types of information and synchronizes them. then in the data loader, it actually looks at the, we look at the full image and try to find a small patch in there, which should be on the surface of the object. So how we do that is we basically start in the middle, and move downwards until we detect a significant edge, and then we extract a patch around that. And then from that patch we detect, some point normal and curvature, and we combine that information plus the distance from the top to the patch plus the tracker post to get a CMP message to send to the learning module.

so in the demo you're gonna see this iPhone, iPad app, just to orientate each to it. So we have, the image here and then we have the depth that we're shooting at and the gain is how white this image is. and then we have the number of, images we've captured so far. It starts at minus two 'cause obviously the first two don't count. as we all found out. and I dunno, like a manual capture button here. We are set to the default, type of musculoskeletal, which was, we found was like the best for identifying the kind of hard objects that we have to in, in the demo. so that's gone. The iPad app, just so you've seen it.

yeah, so a bit more detail on extracting the patch. So there were several, difficulties. so like the acoustic shadows and artifacts were really giving us some trouble. for example, sometimes the highest peak in the kind of edge detection is not actually the object. So in this case, we actually want the patch here and not here. 'cause this is a, an artifact. So we had to do quite a bit of fiddling to figure out the right settings and right kind of algorithm to get the patch of the surface. And the next, we basically, figure out a bunch of points on that surface, and try to fit a circle through those points. And that circle tells us about the curvature at that point. and then we also try to estimate a point normal at that center location. Which can tell us about the pose of the patch. And that also took a bit of fiddling because, again, there are all these little artifacts that, that sometimes get these blue dots to show up in the wrong locations. we had to sometimes limit the range in which we try to fit, the circle because, like surfaces, like on the brain are very, bent. but yeah, eventually, I think now it works reasonably well. And then we have to combine the depth, the sensorimotor and the agent location. So if we just look at the agent location, so just what the HTC wife returns, we basically get the shape of the bag. And then after we add the, depth that we extracted from the patch and the. Kind of, offset between the probe and the tracker. We get more something that's, shaped like a mug. if you have seen it many times, you see the mug, but this is the first time you see this. You might not recognize it. it's not a tumor. Yeah. And then. Yeah, but it could, it, it could be a lot of other things besides a mug. yeah. So if you, visualize it at 3D, so you can kinda rotate it. You can't see it on the slide, but if you rotate it, that this is the body of the mug and then down here is the handle, it's a bit hard to get a good screenshot of it.

Yeah, it does look like a mug. and then we added a bit more, vis live visualization that we can run during an experiment so we can see what's going on. So we have the input image and we have the extracted patch. You see how we fitted a circle through that patch and, point normal there that we extracted. Then that gets combined into locations and orientations in a common reference frame. So like relative to the world. And over time while we move the probes. So each of these blue dots here is basically, Neil's moving the probe to a new location and taking new another picture. So over time you have a bunch of points and they update our hypothesis space and then eventually. Like you have a most likely hypothesis and a couple of other possible objects, and eventually you hopefully recognize the object, which in this case we actually did even recognized some symmetry, in the potted meat. Can, yeah. Yeah. And another thing that we wanted was to be able to show to the user, the person who's operating this, where the probe is, relative to this bag. This that's suspending the objects. And the reason we wanna be able to show this is because, as you'll see in a moment, we also want to be able to direct the user, where to move the probe. And, going into the farther future, if Monty was ever used. You can imagine, someone who isn't, doesn't have a huge amount of training in sonography, if they can just figure out how to move the probe, and they can get some direction about where the probe should move, then that could enable them to acquire some quite high quality images with guidance from Monty. but the first step in, in this is actually showing it, okay, where the user, where's the probe right now? and so that's what you're seeing here, how we have this, visualization of the. Kind of bag as this, cereal box, white rectangle, and then the probe as this blue rectangle. And, you'll see this kinda live soon, but as the probe moves in space, the position of the probe in the, visualization gets updated.

but in addition to this, we can, occasionally Monty reaches a state at which it, thinks there might be a goal state that if it moved to it could reduce uncertainty about what it's seen. and this is what we call a goal state. And so that's what's, the visualization is showing here, is that, at that point, this new blue arrow comes in and that's basically telling the user, you should move the probe, into this location. And basically that blue arrow will continue to be there until the user has, moved the probe there. It's still up to the user to decide whether they acquire an image there or not. Monte doesn't force them to do that, but it's at least giving some kind of guidance about where an interesting view, of the object might be to disambiguate, what's being observed.

yeah. Issues encountered, as Neil's already showed, it's quite hard to work with ultrasound data when Will and I first got the first images. We were not very optimistic about this week, going well for us. but with Neil's expertise, we got something working. then second one, coordinate transforms are well as usual a huge pain, to figure out. yeah, Tristan and Rami you mentioned like having good visualizations help but still spend a good amount of frustration and time on that. And then the data streaming was a bit tedious. in the Airbnb. The IP addresses would randomly change sometimes and. Internet would drop out and things would go to sleep. the trackers kept falling asleep or batteries dying. Yeah. Forgot to, so just all the communication, was a bit shaky between the four different components. we also tried to do a bit more AI assisted coding, which sometimes helped to get hacky solutions, but also sometimes really did not help. it tried to import a module called Monty Python. And would just hallucinate some funny things. not, we could not resist that temptation to eat. Some of the dataset tried to make a good case for why it doesn't matter if the hot sauce is filled or not. So it was open. Doesn't affect the ultrasound image. Yeah.

We also had a magnitude 6.4, 6.2 earthquake at 6:00 AM in the morning that woke us up And, for a couple of hours we were debating whether we should go into the mountains 'cause there might be a tsunami. Luckily, there was not.

and some fa family emergencies of, kids falling down the stairs. Downstairs. Downstairs. Oh my, she's got a black eye now.

Yeah. And, I guess there's a lot of kind of interesting things you could do in the future in terms of shorter term things that we'd love to look into. a big thing is, the challenge of getting kind of reliable features extracted from those images. And as kinda Viviane was alluding to, if it picks up on an artifact and it thinks that's the edge of the object, then in general the representation's totally off. and there's a variety of ways we could handle that better. One thing we don't have at all is, we're just looking at the ultrasound as these kinds of images in 3D space, or, sorry, edges in 3D space. And that is important for how humans understand ultrasound images, but it's also. Only part of it. We also look a lot at what is the kind of actual texture of what you're seeing.

whether it's a, lung or gallbladder or whatever, all these things will look very different. and it's true of the objects we were scanning as well, but that's not accounted for. curvature was something we didn't get to being fully integrated. We measure it, but it, wasn't in a way that we can compare it to the simulated or the, models learned in simulation. and then. Yeah, we had some ideas for how we could make sure we get this thing called the kind of maximum curvature dimension. which again was working in principle, but, just needs to be made more robust. and then some of the things that we're actually working on the research side, like for example, Rami's working on with kind of resetting, re-anchoring hypotheses, would really help with the kind of robustness of the system. So one of the things we're excited about is we've collected a lot of the images along with the positioning information, and save that, which would be the start of a new data set where as Monty improves on the research side, we can actually rerun it on this kind of real world data set, and see how it, how it improves.

yeah, it's a tuna. Yeah. But, yeah, there was a tuna can in the dataset. that was, I guess one of the things we were practicing on, this is 20, having, you can't see it on the screen, but it's most likely hypothesis is the tuna can, which is what's in the bag. And here you can see on the iPad screen, you see the depth, the ultrasound image. Niels is holding the probe against the bag, and then the data gets streamed to the Windows laptop and to the Mac and Monty runs here. So that's the, whole setup plus a bucket to collect, water potentially leaking out of this, what's actually a urinary catheter bag.

So is Monty actually running on the Windows machine or on the Mac machine? On the Mac. The windows is only for, so open vr, which is what, is used to track the, the, marker, the tracker, doesn't really support, Mac or definitely doesn't support Mac and doesn't really support Linux either. especially like a headless Raspberry Pi without a GPU and all that stuff. Okay, good. Okay, so how about answering the two questions?

can you please how it shows Monte's, capabilities is that it's like a, very kinda shape based approach to recognizing objects. It's not just like low level features that it picks up, which it couldn't do much with. in this ultrasound data, it's really about moving a sensorimotor in space and getting, locations relative to each other in space. Which Monte is uniquely made for. then also this kind of gold state generator that Neil showed with the point that tells the user where to move to the probe. Next is Montes Intelligent Action Policy, where it uses its internal models to tell you which view would help most recognize the object. So it's not just like a random policy or like a. Methodical scan as, any current, ultrasound AI solutions are. but it's like very methodical about acquiring the information it needs. Yeah. And, then in terms of the kind of the long-term future applications, so of course it's not ready to be used now, with improvements and with improvements to Monty, this could be a huge, help for kind of. Bringing ultrasound to parts of the world where any kind of me medical imaging is hard to do, ultrasound's amazing because you can hold it in your hand, you can connect it to a smart device. something you cannot do at all. Of course, with something like a CT scanner, an MRI, and it's generally safe. It, there's no eye radiation and all these things, so it's an amazing technology in that sense. The real hurdle is that you need someone who is had years of training to know how to use it and interpret it. and There have been efforts to use kind of current deep learning systems to, help with that, but they require huge amounts of data. They don't have this kind of inherently sensorimotor or spatial understanding that enables them to suggest, acquiring data the way we've, shown here. and so yeah, this would basically be a way to bring, the kinda benefits of ultrasound to, many parts of the world, that currently, don't have it. I would, echo that like the cost of the ultrasound was like a thousand dollars for an ultrasound. Cost of training a doctor is like 200,000 per year. And then hiring and da, That's a lot of money. And so the, ultrasound device is gonna get cheaper and it's very easy to get them out to places where they need them. And I think highlight a point that Neil's made about the lack of training data as well, is that. medical data isn't very available, but especially abnormal medical data where there is a problem, like if there's a tumor on the heart, like we don't have any scans of those 'cause they're so rare, but you need a device or a system that'll be able to say, Hey, this looks like a heart apart from that bit there. so while we didn't get to that in, in our, hackathon, like it was part of the stretch goals and we think it would be enormously beneficial for being able to scan a healthy thing and then be able to tell the difference between an unhealthy thing without having any additional training data would be really cool.

okay. Does that answer your questions? Yeah, that answer my questions? All right. Yeah. Then it's time for demos.

should we start and we go backwards in the water or? Sure. Okay. Since we're already on mute, maybe we can show the, the Benny Hill music. Yeah. So I'll show around the setup while Will and Niels get everything ready. yeah, here's our, windows laptop that, communicates with the vibe trackers. The bike trackers are set up down here. There's one I hope you can see, and then there's another one over there. This is just wrapping the tracker to the ultrasound device.

so you can, this is indicating it's on, and then this is, connected to the iPad over here. It's a, it was sufficiently complex setup that we ended up creating like a surgical checklist that we would go through every time to make sure that everything is, is running. because yeah, we also have a few. We have the windows basically running the server providing the, location information. So yeah, so panel blah, blah, blah. iPad. While we're setting up, Terry, do you have an object that you would like us to test on? Okay, what about first? What are my choices? so we have the TBP mug, we have the tuna. Can we have a mustard bottle? The spam can we have a package of Rat Ramen noodles. We have hot sauce. We have a heart and the little menta brain stress ball.

yeah, I think those are the object. Oh, okay. So why don't we, and these are all in your data set sets. You already have these Yes. These are the ones you can use. You would do us a big favor if you picked an object that doesn't float. Oh, okay. Okay. the heart probably floats. Yes. The brain probably floats. Yes. does the mustard float a bit? Yeah, a little bit. But we can try it. what does, what, let me ask you this. What doesn't flow? The spam? Can the mug, the tuna.

Oh yeah, we also have some Issa sauce.

Oh, let's go tomato soup. Can, let's go with the tomato soup. Can. All right. We have never tested on that. So let's see. It's, no, I'm just, I was just gonna say the tomato soup can is it doesn't have a distinguishing feature. Oh, that's true. And it looks like the cup. So let's go with something. Yeah, exactly. the cup has the advantage that has the handle. That kind of, okay. How does the hot sauce float? no, the hot sauce might be a good one actually. Let's do the hot sauce 'cause it has a different shape.

And how, empty is the sauce? Hot sauce. Nails, no comment. You may need to hold it, but I think that's a good one actually. Let me just fill out water. Yeah. Neil said that he wanted to be like a normal person for once and travel without hot sauce, and then he ended up eating our dataset.

Not much control there, Neil, but I was on holiday for two weeks in America as well, so it was a long time without, America has hot sauce. Everywhere when he told us that it doesn't matter for ultrasound, it was a bit like an addict trying to convince us that yes, it, yeah, you still have the visualization. so yeah, as things are shaping up here, you can see. So while you guys get set up, I guess it seems to me all three demo, all three projects are rely heavily on networking network. Yeah, I think that's like a robotics inherent kind of problem. Yeah. Okay. Because that kind of eliminates you being able to go to different places in the world with your, if you don't have good network 'cause of all the data coming in.

yeah, if you would Okay. Design like a full fledged product solution. I'm sure you could also work with just cables to connect stuff. so yeah, this is more like the quick hacky version.

Okay. So we're gonna start, inference, are you sharing your screen? Yes, I am. Okay, cool. Okay. And then, so did you calibrate the track? Yeah. Okay. We've been through the text. So you'll, right now, you'll be seeing the, you can see the probe. Sometimes it gets a bit laggy when we're on Zoom. Unfortunately, there's just. Too many devices running, but the can probe moving relative. And then maybe if you switch over will to the other view. What we have here is a view of try and make it full screen.

What do you call it? The, yeah, on the top left you see the full ultrasound image, and then next to it you see the extracted patch and the features that we extracted, and then to the side of that. You saw, the relative locations in the world that it estimated.

and then on the bottom row you can see Monty's current hypothesis of what it thinks it is sensing. so they're still, almost all of the objects are still possible, but the most likely hypothesis right now is Monte's heart. what did we actually put in change that? The hot sauce. Hot sauce. Okay.

So we have, in our kind of experiments set up, we have 30 steps to try and, correctly. A, b, c. You have a gold state? Gold state up? Huge. Oh yeah, not yet. Okay. Sorry. the ultrasound just needs a lot of jelly and fluid to, work properly, any air, and it doesn't say anything. We're not a hundred percent sure yet that all of the coordinate transforms are correct. Yeah, there's definitely a, so there might be some room for improvement.

can you use Vaseline?

Sorry. Sorry. nevermind.

What top hypothesis do we have at the moment? Look interesting.

That's surprising.

Yeah. It probably is the case that the coordinate transforms aren't totally right. 'cause this should be doable for Monty. Yeah. we just have a few more, like a few more observations. Oh yeah. Did we show the goal state? Sorry. If you go back, see one. Yeah. Okay. Am. Seems like the tracker prob is not calibrated, right? It's oh. Yeah. Let's, for what it's worth, that's just for the human, it doesn't affect the data collection. No. there is a, suggested pose up here. You wanna have a look at it? Okay. Yeah. If you rotate. yeah, I guess that was before calibrating, so maybe it's, yeah. Really off. But anyways, you can see this.

You can see the principles operating as well as the coordinate transforms that are clearly still in need of debugging. how many steps are we on now? Okay, we're all try and be a bit quicker.

You can see on the ultrasound image, like at the top, these like zebra stripes. that's just one of the kind of artifacts that comes in. And we have to are those reflections or something. There we go. And we finished. yeah, it's like bouncing back and forth and creating like a false image at a certain depth. So what did it categorize?

Monte's heart. Auntie's heart. Oh. Oh, that's a shame. Oh, next we can try multiple times, right? We can try another object, but maybe you first demo yours. Yeah.

If you also get it wrong, then we can have a tiebreaker.