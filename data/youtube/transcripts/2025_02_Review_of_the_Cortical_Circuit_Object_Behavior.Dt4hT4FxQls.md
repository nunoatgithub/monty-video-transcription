 And I started thinking about um uh that the the little movement vectors are not really movement vectors at all. They're just a feature. It's like it's just saying like uh nature said here's a feature. It's instead of just saying it's an edge, it's an edge that had moved in this direction to get here. But there's no sort of beyond that. There's no internal knowledge that this is movement. It's just um it's just like a static feature. Like at this point in time, there was an edge moving this way. Not that it's there's going to be treated differently than the color columns. That was the idea. Um I like that idea. But now we're recording. So yeah. Do you want to start with No, we can start with the review. I don't care. Whatever you want to do. Yeah. Um up to you guys. Yeah. I feel like that already kind of ties a bit into the review. Like Well, that's that's new setup. So yeah, um I can just draw a picture of that if that wasn't clear or or or you want to just Yeah. Do you want to maybe just at least the big concept? Um sorry, I can't be more helpful here, but um but just of the You want me to do a review of the whole thing? I thought someone else was going to do that. I said, "Oh, I thought someone was going to do the review and I just was sitting here listening." Uh okay. This is a review of what we're working on. Um, up to this point in time in our story, we had um we've got we have a a theory and biological theory and an implementation of learning um and inferring the uh the structure of um of uh of physical objects, you know, and um I'll just that physical objects And uh and so that's been working pretty well. Uh we can do that with you know remote sensors or non- remote sensors. And then we've been working on what what how do we handle when the world is changing the the the structure of objects changing. And so we're still we're still thinking about like okay things that are not like gushy mushy things like t-shirts but like a stapler opening and closing or traffic light lights changing and so on. So we wanted to add um uh we wanted to add the ability to learn a model of of behaviors um and the idea that this would be in addition to the model of structure physic object column could learn the behaviors of those objects and then we wanted to apply those behaviors to other things. So we had to figure out how to learn the different morphologies and the changes of an object and so uh we first start out with the idea there might be some sort of single um uh state variable that represented the state of like the state. Then we said no that's not going to work because the states of objects are very complex and the multiple states at once and multiple behaviors going to be happening. Then we said oh maybe we we could do um like a we could learn the the behavior or the state at any individual at every at different points on the object. So instead of having a single uh variable represents the state of the stapler which doesn't work we could then some some do something like we did with compositional objects and say oh maybe on a point by point basis that made a lot of sense but it didn't work like originally we said oh maybe just like compositional objects and it's a hierarchy or something like that and and that didn't work. So uh then we ended up with an idea which is uh fairly complicated but very elegant um which is um that uh there are in a column we've always said here's the summary in a column we've always said okay uh here's a column uh we've always said okay there's a there's a a reference frame which has a location and that's in layer six and then there's a feature coming in um uh to uh layer four and uh there's a connection between these. So we pair the feature with the location and um and that's how we learn the structure of the object and then we said let's what if we did the following. We said there were actually um there actually two location there was two reference frames and there was sort of two models at once in a column. Sorry. And if I'm not explaining this clearly, you just jump right in and make it more clear because I wasn't prepared for this this one. So, uh there's two models here. The one we've already talked about which is sort of static features uh at locations and then the other one would be uh instead of representing static features at some location, we would represent when things changed at one location. So this is this is a change meaning we only store something in this model when something has changed. So the other one only stores things when it's static and the other one has changed. In vision you might think of this as the as the uh this would be the the power cellular pathway. This would be the magnet side of the channel. These these are bits that only become on when there's something not moving. These bits only become on when they're move something's changing. And so uh and we have two reference frames. Now why we have two reference frames um is because we're going to learn models of the change or the behavior. This is a behavioral model. Um and this is a sort of physical morphology. Well, morphology is it's a little bit we'll call it morphology, but we're still struggling how to do more than morphology. Um like color. Anyway, we have a behavior model and a morphology model and they're both being learned at the same time. So the sensors moving through space, they're both moving through locations in space. Um, and uh, but when something changes, we enter that's an end up here. But when something is static, we enter it here. It's exact same mechanism. They're tracking the same way. The difference though is that um, these two models, even though they have two reference frames, they anchor separately. So uh what that means is we think about anchoring represents an object, right? So the our old way would anchor on the object and say okay this is a coffee cup or this is the stapler. The new this new model this new model is basically says I'm anchoring on the behavior. And so it's only the parts I'm going to represent all the things that are changing um in this reference frame. Um and and the advantage even though they're co-learn the advantage is in the future in another situation I can observe changes and recognize the behavior um and independently of the the new object. So I can take a behavior I've learned from one object and now I can I can observe it on some portion of another object and and infer that behavior. And so I oh yeah this other object is doing the same behavior as something I've learned before. So you they co-learn on one object or and then they are but they're separate because they anchor separately. That's the only that's really the issue. And so and you can think about that this change model it may only be occurring on some subset of the main model. So if I have a bigger object like a traffic light well the only parts are changing are the three lights. And so the the model of the behavior of the traffic light only involves those three locations. It doesn't involve anything else. It doesn't know about any other morphology about it. It's just that I see three things changing and I can place them on top of a traffic light or I can place them some new environment in some other way. Um so this idea that there's two of these things working to learn together two different models uh and that then they could be separated out um was a really great idea. Um and and we've we've speculated something like this might be occurring in the past. just the physiology and the anatomy of the cortical comp suggests this but I never really understood why it would be like that. Um so this this idea here is we have these two models going together and um that was I would say is a summary of the major accomplishment we had last week and I guess one more detail is that there's the timing. I forgot. Yes, as soon as you said there's one more detail I said oh yes I forgot the one more detail. Um, there is time and I've always speculated and we'll just write time up here that I've always said time is a signal we need for sequences and I've I've written and speculated and hypothesized that this it the time signal comes from the matrix cells of the thalmus and it's projected broadly across the cortex onto layer one and it presents a time signal so that you can learn um you could learn the timing of sequences and so the change model often almost it always involves time at least at least there's one change in time, but it might be multiple steps. So, when a stapler's open, there's a whole sequence of of of changes in the location space of the stapler as it's as it's opening or closing. And there's a there's a or the traffic light, the lights go in a sequence of certain timing that so the way to think about this uh this model of behaviors, the behavioral model is it's it's really like a four-dimensional model. It's a it's a it's it's a model of space as it's changing through time. Um and uh whereas there really is no concept of time in the morphology modelology model here's a set of features that are static in time and then the behavior model says well these are set of changes at this point there's this set of changes and the next moment of time there's this set of changes next moment of time there's this set of changes and um this combination by by adding time and this representing it like like this fourdimensional model of behaviors allows you to learn any behavior at all possible really if you think about it anything that can be represented um any combination of of changing features um in space can be represented this way in any order. Do we think that the uh the melody would be represented as a because it has time would it be represented as a behavior or Yeah. Well, we were speculating just the end of the day yesterday. I didn't want to go there. Um because you we our melody we've had this idea of a melody that was the very first thing we did in terms of really neural mechanisms. how to learn sequences and that's the temple memory algorithm and it was the derma paper that was like what 10 years ago or something like that um uh maybe longer I don't know but anyway 10 years ago and um I mean and one of the things so I'm assuming everyone knows that so you got the layer cells it's forming these SDRs and it's learning a sequence the problem with the melody one was it never it was it was never dealt with the fact that you um the invariance of pitch it could it basically would learn a melody in one key and it wouldn't be able to recognize it in a different key. And so this has always been very bothersome. Um and so I never really resolved it. Um wasn't too worried about it. She kept saying, "Oh, we'll just figure out the intervals, something like that. I don't want to dismiss mystery comments." But then I realized this is this in some sense is suggests the solution to the melody problem because you're learning you learn the melody in a particular key but the behav but the beh the melody is basically a series of sort of snapshots but it's the change that really matters the melody is almost all behavior. So you learn this behavioral model even though it was it was aligned in one particular key then um you can play the behavioral model back which is the melody in a different key and it this works um I realize this works because um uh because music is represented as the notes are on a log scale right it's always like a doubling doubling doubling and so you can take basically shift shift the melody someplace else in in the range of frequencies and it works Um, so this this suggests a solution to the melody invariance problem, which I never ever had, but I realized there are still issues with this. There there are lots of issues with this basic idea. We haven't worked out all the details, but it's such a nice idea that it makes us feel like it's it's likely correct and lots of suggested. So I would say it's really really likely this basic idea is correct even if we don't understand all the details of it yet. Um, so that's a summary of what we agreed to. I think it's not a summary of the things we are still struggling with. So the the magnesellular cells these are going only to V1 or are they also going No I'm just pointing out that we I'm pointing out that from the from the retina there's two basic types of input to the cortex. I'm not I'm just saying and that one these are center surround cells. The parcellular cells are center surround. They only become active uh when when something stops moving when it gets stationary. And the manual cellular cells are centered surround which only become active when they're changing. And so basically they just represent change and static regardless of what you want to call them but whether the color or anything doesn't really matter. It's just on the retina something has changed at this location or something has stopped changing and is static at the location. And so that that basically it's it's not even the idea of movement or edges is determined in the cortex. You know what features that becomes in in these mini columns is is is determined in the in the cortical column itself. So the the the basic idea is that change anything that's changing in this we could be modeling anything here. Anything that's changing is representing here. Anything is static representing here. And I want to use the parvis magnus to point out that that's exactly what we see from the retina. And um and we've been using that anyway. We've been using the magnell cells for representing movement of the sensor. But this is now representing movement of the object. Remember the the upper columns have the the cells are responsive to movement. Um most of the cells that people are able to identify in a cortical column even B1 are responsible responding to movement of some sort. But the upper ones are responding to movement to small objects that would be movement relative to the object. And then down here the movement would be rel you know the eye movement relative to the world. So and in the higher regions there will also be movements of uh parent objects. We I didn't say anything about hierarchy. I didn't say anything about parent trying to visualize what what that change would look like in the hierarchy. Okay. Well, we could go there. I think last week we basically just were focusing on a single column for the most part. And the idea that there's these two models in there, a behavioral model and a non-behavioral model, and we call it morphology, but it because it could be applied to concepts anywhere else in the world. I think the right way to think about it is here's a model of things that aren't changing at the moment. There's a static arrangement of things and then there's a model of here's things that are changing um and they're kind of overlaid one another. And if we could call it a morphology model, but that may be that sort of implies I'm looking at physical objects, whereas it may not be physical objects. We also talked about, for example, some of the columns could be color columns. Um, and they could either be like a mini column. We were talking about the blobs in V1. Um, so some of the mini columns would represent color. That's not really morphology. It sort of it sort of is. It says, oh, there's color at this location. Or you could say, oh, the no, there's a new color at this location. It changed to it was, you know, something. Now it's green. So it's I think the word morphology is helps us think about it from a a lowle sensory like a primary sensory cortex but I think the real way to think about it is it's static and static and changing that's the more generic um way to think about it and then we didn't really we were struggling with lots of issues we could talk about the issues we're struggling with I thought this is a summary of what we did do I miss anything else no yeah yeah and I think it's a really beautiful solution because it takes the mechanism that we already had and it basically does a little tweak. Instead of storing features, we store changes right at locations and it solves the issue of being able to apply any behavior to any object without having to see it before. It also allows you to learn any type of possible behavior that you can incredibly imagine can be stored in this way. Uh so this it's uh very very flexible and you can apply this to anything, right? So, it's really beautiful and the beauty of it makes us feel like it's right. Um, which is a dangerous thing sometimes, but in this case, I think it's it's a good thing. Um, super elegant. Um, so that's that's the fun exciting part. I mean, and we haven't talked about the um the challenging difficult parts. Okay. So, there's the summary. I'm done. Unless I missed anything else. Yeah, I I guess Misha, you're the only one who who hasn't heard this before. So, how does this sound to someone who hasn't been in all the brainstorming meetings the past week? Uh, this sounds great. Um, I'm curious if there are any more details about like how change is actually modeled on a lower level. Um, I didn't understand that question. H how do you model the ch the the the the object changes? Well, this is the model. The model is uh it's it's it's changes at locations in space, right? There are changes occurring on an object. So the model is a model of changes. It's it's it's um uh it's it's a reference. It's it's basically the model is very much like the morphology model. It's very it's the same idea. You you can look at different locations on the object and the change model that the behavioral model is only going to store points that have changed. Um is it a delta between features? That's it's just I think I think the right way to look at it's just a change and not and you don't really know. It doesn't record what it was. It just says this this is my thinking now. It's like I'm saying there is now first of all we're going to detect a change and now what I want to store is what is now there what is what is now at that location. So for example, if it changed from red to green, I would just say at time it say one at one second and a point changed from red to green. I would say at one second this thing changed to green. I don't say it was red. I just says it changed to green. And if I'm looking at u if there's an edge that appears into this point, I would say I don't know what it was before, but right now there's an edge here uh that got here by moving in this direction. Um that kind of thing. It's it's it's not it's not actually recording any kind of delta at this point. U that information could be inferred. You could assume that if it's not changing it would be the same, right? So if I say that something changed at time one, um if it it change then whatever it was before it would was what it was before. So if it changed it if it changed at time one to green and time two to yellow um the time two I don't record that it was green but it is green because the last thing it changed it was green. So you know this constantly in time uh for that um right it it's sort of like the same object but you one is masking for changes and one is masking for static features. Um yeah, like I feel a useful visualization is the um having the two kind of reference frames above each other where one is kind of features at locations and one is is the the kind of changes at locations. Um which we drawn a few times over the Yeah, maybe that's I'm not sure. I was I was trying to do it on a virtual whiteboard, but I think it's just more distracting than I saw that topology model when we Yeah, sorry. We have the stapler which is features at locations. So that would be like all right we have an edge here and here and here and here and there might be a color at all of these locations too. And those are the static features and once the stapler starts moving you would like before the stapler moves there's nothing actually stored in the behavior model. This is a different reference frame than this one but they're colllocated. Yeah. If what as you're moving your sensor, it moves like at this like it moves in sync through those two references. As soon as the top of the statement starts moving, then the upper model says, "Hey, things are moving at this location." Yeah. And then it would lay down. All right. Now there's an edge. Now here's an edge. Now here, right? And then at the next time point, you would say, "All right, now the edge is here and here." So yeah, and not just an edge, but an actual moving Well, I think this Neil, this is the thing that I came up with yesterday. I'm I'm going to try exploring more today. I think it was very intuitive for me and everyone to think of it like, oh, it's a moving edge. Um, and of course it is a moving edge, but I think the feature I'm I think the way to think about this, at least I'm going to push for it for a while until you convince me otherwise. Um, the way to think about it is it's really just a feature and there's no imp the the feature itself could encode in it like oh I got to this edge by coming from the left or coming from the right or something like that but that's not what but there's no the column doesn't know that the column just says it is a feature that appeared here and there's no no ability I mean isn't that kind of going back to the snapshot view we were kind of discussing last week instead of uh where where you have a bunch of more static snapshots over time rather than but but we have a have a change at a location but this is we're not talking about the the morphology model we're talking about the the behavioral model right now so okay but it's I guess what was described there was uh features um at a location at a point in time right so so imagine in the stapler example um you could think that as a series of snapshots, but it's really it's a flow through time, right? It's there's really the beginning and the end of the sequence and everything there have sort of um the changes are where it began and where it ended in between it's just flow of time and so I'm not sure it's really it's not really snapshots. In fact, you know, this idea that I just suggested that the the behavioral model only records behaviors and the the static model only records things that aren't changing would imply the following. If this if I have a stapler and the stapler always moves continuously and then it gets to the top, that would imply that I learn what the stapler looks like when it's closed. I learn what the stapler looks like when it's open because it's stopped at that time. I can look at it, but I actually don't learn what the stapler looks like in between. which is kind of weird. I don't really learn what the staple looks like. All these in between positions. It's basically like it was looks like this. Now I have a move. Then I have this behavioral model that says all these things are moving and now it looks like this. That seems a little weird, but I like it if I if I can make it work. Um it's like I don't really want Yeah. Is it possible maybe to to revisit the issues uh with the um storing feature changes at locations because I feel like that's at least until yesterday the the view had. And so that might be both a useful recap and also a segue into what challenges are you referring to? Well, that's my question to you because you said this, you know, like you're presumably this alternative is to address some issues with that. Um, well, here I think it's it's a very very small tweak on how we talked about it before, but it does change the way you think about it. Yeah, but it's still like the small tweak that kind of changes applies that here for theology model many columns would represent like oriented bars or color or Yeah. So like Oh, they represent the orientation of a feature at this location. Yeah. Oh, or color. Um well or well then we extended at the color and that's where the problem started occurring. Yeah. Okay. and and and then here for the behavior model what actually comes in or what is being detected from the magnellar pathway is changes. So there we have a lot of activity if there's a bar moving in a certain direction. Um so we would have like the minicoms would represent for instance movement into one direction. Um, and I think if I understand right, what you're saying is that this column, it doesn't store anything about where it came from and where it moved to. It would just store Well, the feature means that, but the column doesn't know anything about it. Yeah. Here, here's the problem I saw. We we start off by thinking about these moving edges. Oh, that makes a lot of sense. The staple is moving. The morphology is changing, right? So, maybe now we how we know how the edges are moving. Then we added color. We went and looked at the color blobs and we said, "Oh, there's these columns that represent color. They don't really have movement. They just either change one way or the other. They can be green or they could be not green. You know, it could it could change to yellow. It could whatever. So, there wasn't this concept of movement wasn't really that um it's something just appeared, right? Or something could just disappear. Yeah. It's basically the minicoms there detect change, represent change. So then I asked myself, well, would these min columns be treated differently? Why would why would the ones that represent moving edges, we would somehow say, oh, we know that's moving. We can we can do some sort of path integration or something like that. And the other ones don't represent movement. Well, that's possible. Um, but I said the more generic case would be to think about that the mini column, the feature could be an edge moving in this direction, but it really just means there was an edge that was moving in this direction and now it's here. Um, and it's and it's and the the cells don't know that. They don't know that this is movement. It's just a thing. I could have done the behavior of just there's an edge that appeared at this location and so like it wasn't there before, now it's there. And that would still be describing the behavior. But if nature said, "Hey, let's add the little motion part to it." So this feature is there's two separate features. An edge at this location that had been going this way and edge at this location been going this way. There's just two different features. Then it's much easier to infer um infer behaviors. Um but it's but the neurons aren't taking advantage of the fact that it was moving. It it's just it's just dividing up the feature space into more refined features. I don't have a better way to say it than that, but the point is that all the minicoms could be treated the same. It's just like an object, this feature appeared at this point in time or this feature disappeared. There was a feature and it's gone or there was something but now something new is there. Uh and and I don't need to make a distinction between color and moving edges. Um the movement will be captured in time. Yeah. So we don't have to represent like movement in the model itself and then apply the movement to the locations of theology model. It's we just represent right now that that feature change and and and that's nice because like we can imagine learning a behavior like you said where we actually don't have movement. We just press a button and a feature appears over here. It just appears. It doesn't move there. And we can still you would still store that in the behavior model without detecting. And then the color the color example was like that. you know there were there was no green dot here now there is a green dot you know didn't move didn't it just appeared um and um you know and so it's the edges are like that too I suppose if an edge I don't know what would happen if if an edge just appeared in in in like instantaneously something appeared with an edge in it um how would that be represented in the behavioral model it really couldn't say which direction it was going from maybe it would maybe it would encode you know, maybe be too, you know. Yes. Yeah, that would be my next question if like the direction of movement is encoded by just like the sequence in time or if we can actually distinguish direction of movement at once. You and I can distinguish the direction of movement by looking at the cell response. But but I would say that the model doesn't know that. So basically the model only knows what time tells it. Here we wouldn't store the direction of this. we were just or all right at this point in time a feature appeared here. Well, no, it's the behavioral model says the the feature itself is an edge that was moving in this direction. It's I could just call it imagine when edges move to the right they were green and edges moving to the left they're red. Yeah. It's like saying that it's like oh this is a green edge. This is a red edge. So it is part of the feature that's being stored but but there's no there's no way to take advantage of it. The column the neurons don't know how to take advantage of that movement idea. Um they're just it's just a feature appeared and it turns out but they can distinguish the two directions, right? But they don't know that it's a direction. Yeah. It's not like they take it and apply it to the they don't know. Nobody knows it's a direction. It's just two. They know it's different. Right. Right. Well, they know I don't know that yet either. All I know is that that I'm going to say when something appears. Why wouldn't they know it's a direction if if if those mini columns can be sent because what I'm saying they don't know no there's no there's no mech no mechanism that's taking advantage of that knowledge there's no mechanism that says oh these are moving let's path integrate in this direction or something like that it's it's just a feature yeah I mean there's no yeah I guess imagine if if you're trying to infer where in the state space you are it's helpful to know that like oh okay in that part of the state space edges were moving in this direction It's very very helpful to differentiate two um it differentiate different behaviors. It's really helpful but we're not so we're taking advantage in that regard just like okay I could take advantage of color but there's nobody take no one's taking that knowledge that it's moving and doing something with it. It's not like it's not like oh we are going to p we're going to like project where this edge is going to be next. It's it's whatever whatever appears next is appears next. Yes. So we do detect the direction in the mini columns. We we can associate the directional change with the location in this reference frame, but it's not like we take the direction apply it to this. Right. Right. We're not Yeah. Okay. No, I I'm that makes sense that we're not applying it. And maybe everyone else was too. I don't I was thinking before like, oh, these movement vectors, we're going to use them somehow. We're going to like path integration forward and predict where things are going to be based on these movements and how. No, it's just it's basically saying to the feature and here's another feature. Yeah, I think the path integrate stuff it would be cool if it worked, but it's just hard to it feels like if that's ever going to happen, it's going to be a very slow deliberate process. It's not like Right. Right. It's it's hard to even imagine how you'd apply it to all these points and everything. Right. What got me to this idea though was was just thinking about the color example and saying like, well, why would color be treated differently than edges? And I don't like that. I like I just feel like I want to be able to put any attributes I want in the mini column and they're all treated the same. You know, could be some other color. I mean, if if a color is just changing like, you know, red to green, is that almost the color is this is not red to green. This is basically it's just now it's green. It's not like I Okay. But but I I guess I'm just wondering is that a little bit like the open and the closed state of the stapler that you learn those two extremes because they because because they they are essentially encoded by static features. They're not encoded by change. Right. Right. I think so. Um you know um so so I guess if we did have something where like color was slowly changing that would be the color slowly changes. If it changes slowly, you actually don't notice it. This is a well, this is a visual illusion. Have you seen this? Yeah. You look at something and it's it's blue and they come back and you're just looking at it and slowly slowly and it turned to turned to green and you didn't you have no idea that it turned to green because it didn't didn't change enough to record it as it changed and so it just is lost, you know. Um can't see us if we don't move. Right. Right. So, so, uh, anyway, I I if we all if we all understand this idea, we could get off of it. It's just that the everything we know about how things move through space have to be has to be it's mostly encoded in the time slices of the behavioral model. Uh, which is greatly assisted uh our inference of what behavior is going on. If we're trying to recognize behavior is greatly assisted by knowing which way the actual pieces are moving, right? Um, right. It's very it's like if I saw if I saw someone dancing and I I maybe I have like a motion capture dots, right? And there's a body. I don't recognize this body, but there motion capture dots. Um, if I just showed you one one set of dots and then a moment and then it blank the screen and I show you another set of dots and then I blank the screen and I show you another set of dots, you'd have much more trouble understanding the behavior than if the dots moved and in which case I'm going from a feature which is just has no movement information. So maybe I maybe both features the left and right movement go to get you both going rapid but as soon as I know oh the movement of each of those dots meaning I know it because the feature encoded it then it's like oh pops right out it just comes right out you um so it's very very helpful to have that knowledge um to to infer something but it's not actually take we're not taking advantage of in terms of any kind of path you know like like applying motion to the object anyway that's the idea and um um of that one. So I like you know it might be it may not be right um but we can go with it for a while. This just changes my thinking a little bit about the models. Now I can think about now we can think about any attribute regardless anything at all. It could be high level concepts. It doesn't really matter. Many columns represent the presence of an attribute and um and we the behavior model just says okay now there's an attribute here that wasn't here before. That's simple. May not be right but simple. I'm imagining a scenario where you you learn the stapler, you learn his behavior. Um later in life you walk into a room and the stapler is in the intermediate position. How did you recognize it? Yeah. Yeah. I don't know. It's a good question. Um, I don't know the answer to that question. I I also don't like the idea that I have to learn every position of the stapler. Yeah. Right. So, that seems impossible. In fact, if all I have to sell the stapler opening and closing, one of the problems we had is you can't learn every position of, you know, it it's moving and so the morphology is changing. You don't really have the opportunity to learn what its morphology is in those in between points. That was one of the things we were struggling with, right? So, so we got rid of one problem. Now we introduce another problem which is like well how do I recognize it when it's static in between right actually I mean I think you would actually take a bit longer to recognize it and what you could be doing is you just recognize the top of the stapler in one orientation and the bottom of it in another orientation right and then you have to sort of you have to yourself move follow the path of it using the behavioral model and which takes some time. Yeah. So we can we can try some extremes here. Imagine if the staple open very quickly like right. There's no possible way you could ever learn in between points, right? You just can't. And it might and I think but you could imagine it. I think Vivven probably got the right idea. It would probably take longer to do that. Um uh it would probably take longer. Um it's also interesting. You know, we saw these examples when when you can have a change and a change happens very quickly and and there actually no intermediate states. The brain actually perceives that there are intermediate states. It perceived it moved. We did we look at some examples like this last week? I forget, but there are examples where you can show there's there's two things. We could look it up perceived motion or something. But you you have something appears it goes away and something else appears. your brain act thinks it moved there like Christmas lights when they light up or or literally uh movies like 24 frames per second right I guess that's an example but that's right but it seems a little bit I mean but it works even with like fairly slow animation like right these are things that you few frames per second these are things that you wouldn't expect to know to movement it's like it's not like I show you my hand here and then my hand here it's like they show you some you know letter A and then there's letter A. Well, we don't normally see letters move, right? We don't normally imply, you know, maybe if I saw in a movie, I might say, oh yeah, well, I know their hand is moving. I'm going to expect it to continue moving. This is like, no, this is an object. You don't expect it to move. You show it again. And the brain says, oh, it moved from A to B. It moved from here to here when it didn't. But you perceive the motion. And this is just a data point. So maybe um uh you know, it I could actually imagine in between states that I never saw, right? I could say, "Oh, well, can you show me what the what it looked like between these two points?" "Oh, yeah, sure, I can." So, it would be right here. You never saw it there. That was never anything you saw. That's that's because you're anchoring to a behavior of the movement. This you saw these two features and you're you think that this is the behavior that happened, the movement, right? But what what does it mean? I didn't actually observe it. Right? So, all I observed is there was a a state of the object here and then there was another state of the object there. some feature moved from point A to point B and the behavior that was actually occurred is this feature disappeared and this feature appeared and yet we perceive it moved. Okay, which and then also we could perceive then I could ask you what would it look like in between you'd be able to tell me I mean it seems so trivial but you anchor to the wrong behavior. No, I'm not sure anchor to the wrong behavior it got to behavior. It's just I'm just using as an example of I can predict what it would look like between two states that I actually never saw. Right? So this is the question about would I can how do I predict what the state will look like if the staple goes whoop and whoop and whoop and whoop. How do I predict what it will look like here? I actually you might say well I saw it moving through that position. I would somehow I learned what it looked like there. And the example I just gave you says no you never saw it in between. You can you can infer what it would look like but actually was never there. You never saw it here. I mean it' be like the statement never actually appeared in the middle. It's just in fact I'm what I'm suggesting if this is right. I'm not saying this is right but if this is right and we only record static things and changing things then the only time you learn is when the staple is closed you learn when the staple is open. Then when you're moving in between there is absolutely no morphology model. It's just movement models. And somehow that's like the two dots moving and you can infer as said you would infer take longer you'd have to infer what it looks like in the in between state. Um somehow using obviously using the behavioral model right somehow the behavioral model says um um I did the behavioral model would encode all those inter intermediate states because they were changing those features were changing throughout their cycle. So the behavior model doesn't code it, but the the So now this gets us back to the basic problem we were struggling with, which was how do I predict um if I had I've learned this model uh with a black stapler of a certain shape. Now I'm looking at a pink stapler with a slightly different shape. When it opens up, I can predict. I said, what would it look like when it's open? I will be able to predict what the pink staple top looks like when it's in this open position, even though I've never seen it like that. How do I do that? That was Yeah, I guess in general it feels like that kind of um path integration or whatever that like slow thinking to imagine that is is definitely easier with some sort of child like representation of the or or like breaking off of the subobject even if that's happening in one column because then then we're not trying to predict point by point how everything's changing but it's kind of like well I know in the behavioral model this the stapler top rotates so I'm just going to do a kind of coarse approximation of that rotation. So just review I think we looked at we looked at two ideas there related to that. I proposed the idea that this was could be done by having two columns in a hierarchical arrangement where the parent object is the entire stapler stapler but as a as part of the stapler starts moving we invoke um the model of the stapler in the lower column saying but only attending to like it's like the rest of the stapler's oluded. So it's like the lower columns watching the entire stapler rotate um but the rest of the stapler is included but it would make good predictions about the top. And then uh Vivian suggested that maybe this could all happen in a single column that uh within a single column um you could alternate between the full stapler and the top of the stapler and and and and somehow keep both of those models the two the two two stapler models are in the same column but they're we're including part of it and rotating it. Yeah. Yeah. And I I wrote some uh thoughts on Slack which I can go through on on maybe how uh that could work. I'd like to do that. Sure, why not? I didn't see. Okay. Yeah. Um yeah, I mean it's not super coherent, but which one do you are you looking at the the the So it's kind of a combination of of those two. Um where it's Yeah. accepting that there's kind of this parent child hierarchy, but yeah, trying to put it in one object. And I guess one way to view it is that you kind of have um the the input into L3 like the movement and and all that that's kind of that low-level information is telling you kind of where you are in some uh behavioral uh state space. Um but that we kind of in in that kind of like attending to just a part of an object that that gives you kind of like a region level state. So it's it's not a it's not a location by location state. It's not a global state for the whole object, but it's basically a state for a location region, right? And what whatever. So it's basically it's almost like having two behavioral states. Um and and but that high level one which doesn't cover the whole object that's what the what you condition on when you try and predict like okay with the morphological model based on its rotation and things like that if I move what am I going to observe it's it's sort of saying can I can I using the change detection um can I isolate a region of the parent object that is by by sort of enclosing of the the change I say okay that is a separate item right and then yeah we like we kind of talked about on Saturday but in order for that to work like when you go from one sub region to another then you're going to have to kind of shift that attentional window right but um but then but then that you know you could be attending at one point to the top of the stapler and then you could be attending to like the base plate and like the that kind of region level state is what's necessary to predict what you're going to see following a secade. But the very lowle like changes coming in like at L3 that's necessary to infer like where where are we in that like uh okay I'm I'm in the top of the uh stapler and it's moving in X direction. Well, I I I I like that. Although, um that doesn't it that didn't address the question we were asking. Is it in the single column or multiple columns? Right. It's it just says No, I guess I was thinking as a single column. Well, what you're saying is I think we can we have the information to break off a part of the stapler that we can consider as a as a as a movable subunit, right? Isn't that what you were saying? Yeah. Okay. But but I guess that could be sort of uh ascribed it feels like at least in a in a single column at a given point in time. It could be that I think you know the so so so like the the whole object like it's always like stapler but the the behavioral state is more like is more specific to whatever region is changing basically uh and which is being attended to. Well, I think and you can only really make predictions about that that wouldn't the behavioral state include I'm what I'm thinking what you're talking I'm thinking about the staple the stapler opening with the with the weird behavior I talked about which instead of the whole top of the staple moving at once that yeah part of it they opened like a little door as it went up. Yeah. So there's multiple moving parts in that example I laid out there were three moving parts. It was the the bottom of the top and then the two little doors that flipped up from the top. And I would think that the behavioral model includes all three of those things. Yeah. Behavioral model of the stapler. So it would include all I'm think that's what I'm thinking, but maybe I'm wrong. Um and then then but but somehow because of the because those individual pieces are moving that are there's like a every piece has its own little surrounding um sort of moving edges that sort of is you could use it somehow to isolate particular um the three components like okay okay I know that because because there's a a boundary around these um that's one component there's another component and another component and and I and I think you're saying I could attend to those three I could attend to the one door opening. I could tend to the other door opening. I could tend to the base of the the two doors. Um um in that case, in that case, yeah, you probably couldn't understand the whole movement with a single. In that case, I you know, ultimately I would I would think, oh, I'm going to learn that this is a compositional object where there's the a stapler has the the foundation stapler and then it has a a plate that rotates up and then there's two more plates on top of it that open in different ways. And so there would be four components. Well, there'd be three child objects to the stapler um at that and those three child objects are moving u can move independently. But they're moving in sync too, right? Yeah. But but I they are moving in sync, but I might still view them as independent objects. I might Yeah. I might say uh but like Yeah. So you want to apply the two like a general hinge behavior to both doors I guess but you also want to represent somehow that they are movement of one implies movement of the other. So I think the par so the parent object one way you might think about this you know ultimately you might say a model of the stapler says oh I have a stapler with three child objects and the three child objects are the the the moving plate which is the bottom of the top and these other two parts and so now I'm I'm described it as a a parent with three children and the children are moving a certain way u that's not how I started with because I didn't know that until these things start moving so this gets back to the idea like when do we break off a child object it seems really nice that you could break off a child object if part of something moves independently, right? That's that would be a good reason to call it a child object. Um, and so you'd want this could then be say, oh yeah, staplers were one thing and now they're four things or you know there one thing with three child objects and I have new subobjects, something like that. Um, yeah. And I also wonder maybe it's something in between where like depending on how complex a behavior is, we're more likely to rely on u hierarchy with kind of location by location binding. But that with some behaviors, we can it's sufficient to do more that kind of um sort of localized attention within a column. Um yeah it seems like you have to do the localized attention before you've learned that it's child to before you have learned separate child objects but then eventually you could learn to represent the top of the stapler separate from the bottom and then solve it with hierarchy. So, can I can I talk about an adjacent issue that that we've Sure. By the way, is it possible to put on another laptop screen just to see you guys? Uh because I feel like sometimes, you know, people naturally use hand motions and stuff and it'd be useful. Um thanks. Uh this is an idea we already we talked about, but I think it might be essential to solving all these problems. Um this is the idea that um where do certain attributes get represented? Uh so I was thinking about I I can I can talk about it in terms of melodies. Let me let me start there because it's an easy example to make it clear and but the same problem exists in physical objects. So in the melody example, the melody consists of a series of intervals. That's the definition of a melody. And that would be in the behavioral model, right? It just says there's a series of intervals. That's it. And at certain timing. Um, by the way, the behavioral model can track multi-art voices and things like that. It all works. Um so uh but when you hear a particular melody it's in a particular voice like meaning like there's an instrument or a person singing or there it's a dog barking the melody right and that voice is a complex thing. It's like an attack. It's overtones and how those overtones decrease over time. That's what make a dog sound different than a saxophone versus something else. And um that in some sense is is the feature that's implemented at that point in the melody. And without any additional knowledge, I would assume that that feature appears at each note in the melody. Like if I'm hearing a saxophone, I'm going to unless I I know I've learned that it changes from a saxophone to to a dog barking, I'm going to assume there's a continuity of the saxophone. Yet the saxophone sound isn't part of the melody, right? Um, but it's also itself a timebased dynamic thing. It's not a static feature. I can't represent it in the morphology model because itself has time and I I can't recognize a saxophone note without having its own time signature and changing its own behavioral model. It's a much faster one. It occurs very quickly in time versus the note to note. It's faster than that, but it's still so so I'm asking myself, well, where is the feature that is, this is a feature which in some sense is not part of the behavioral model of the melody. It's just a feature that occurs at each point in this melody, but that feature itself requires a timebased model. Do you think it's u just for recognizing like the temporal signature of a saxophone that could be done more subcortically or like pre-processing of the audio? Well, there would be pre-processing of the audio, but I don't I don't think it could be handled subcortically. I think this is the more general problem. Um, and it's it relates to the problem we were dealing with how to represent features on an object. So, imagine our stapler um uh as it was opening there was a little symbol on the side and that symbol was the Starbucks symbol and then it changes to the and when it's halfway open changes to the Pete symbol and when it's halfway open it changes to the Phil symbol for right well that's not something like that's not like like black or something like that that's like I had to say this new feature appeared and it's a complex thing and it's like a child object. So I can say well there's a child object at that location that appeared but those are very complex child objects. I can't represent them in a set of mini columns unlike edge or color. Um I don't I can't represent coffee logos or logos in general or images in general by a set of like you know 100 minicoms. It's just not enough information there. Um, and so that that's that tells me that I'm relying on a child object recognized somewhere below. Yet, I don't know how to represent it in I don't know how to represent in the model in the parent object in the stapler object. I don't know how to um say there was a Pete's logo at this location. I don't know how. Yeah, this was I guess another thing I was posting on um Slack about because Yeah, it feels like if you use multiple mini columns like essentially an S str across many columns rather than assuming one mini column is dedicated to a particular type of child object that at least makes it more tractable and like right but I don't think I appreciate that like in V1 we look at many columns and they tend to correlate with like a very simple feature or an orientation Right. But it may be that in general it's like features are represented by active minoms. It could be although it doesn't feel right. I don't see evidence of that anywhere. You minicoms tend to have sort of well understood receptive fields. Um they tend to be things that representing things that can be uh there isn't too many of them. Um uh everywhere I look it's that's only really in V1, right? Well well I don't know. Maybe I it's it's if I then I I think a lot of the mechanism wouldn't work. It's like um uh how does this and then and then you would still you would still use neurons within many columns to define like a unique feature at a location, right? Um, so it' be kind of like within an even in the best scenarios like even if I have, you know, a millimeter wide column, I only have several hundred minicols. I don't have thousands. I've got several hundred. And minicoms tend to be active not as an SDRs but as sort of uh you know if I look at like the edges it's like oh there might be 10 or 12 minicoms representing edge orientations and in any point in time three would be active because it's like you know think three or four that might be in that range. It's not like I have this big vector and I'm really sparsely selecting from it. Um, and even if I use all the many columns in a in a column, I don't think there's enough be, you know, a few hundred at most. And um, that's not really enough. Um, to represent really meaningful STRs. Um, I don't think it's enough. I would have to have, you know, a minimum of uh, 20 active mini columns, let's say, out of 200. So that's a 10 sparity. It's it just doesn't I don't I you might be right Neils or that may be correct Neils but I don't think that's right. It feels like something else is going on. And and again remember think about the melody. It's a it's a the thing that I'm looking at actually has its own time basis to it. You know it's like I can't represent the saxophone note with a set of mini columns in the static model. It just I don't know how to do that. I it seems like I have I almost have to be using a hierarchy of some sort and even when I have hierarchy I just don't know how to make it work. I I just don't know how to yeah go back. I just it almost feels like there's a separate representation of saxophone note that's stored someplace and I'm saying okay just keep using that in every every attack. Just assume it's the same thing over and over and over again. Um but it's stored separately. It's not actually stored as part of the morphology model or the behavioral model. It's it feels like um it's like in the melody example I have I have the the the change model that is the the actual melody and then um the morphology model the equivalent of that would be the actual note I'm hearing at this moment in time. Um but somewhere else is saying oh what should that note sound like? It just feels that way to me. I I might be wrong, but feel me. I don't know if this helps any, but it seems like part of the solution is the fact that the behavior model only encodes changes. So even if I get input for a completely new instrument I've never heard before uh playing a melody that I recognize that feature input to layer 4 I would just assume that that feature feature remains constant if there's no change for it encoded in my behavior model. Right. I guess that's what maybe you're saying if it's a novel thing like I never heard dogs singing a song before. Yeah. Or like a a voice of someone I've never heard before. Right. Um it I'm not Well, I'm not sure where you're going with this. That that would solve like a melody that alternated between piano and saxophone notes, I guess. But you can learn this that would you would learn that transition. It's like Yeah. So then if there is a change in the feature, it is stored in the behavior model, right? But I know that if if it's not in the behavior model, I keep predicting the same features. What I'm struggling with is how do I even apply the saxophone to each note, how do I how is my brain predicting the saxophone note if I haven't seen a change? Where where is that playing out of the saxophone note occurring? Where did I store it? Where is it temporarily held? It's like, okay, yeah, it would just be temporarily. It wouldn't be stored, but where is it? You Well, yeah, unless it was I only I learned this melody the very first time. is really famous saxophone solo then I would know there's supposed to be a saxophone for that particular you know yeah but it can't be it can't be stored because you you can hear it in a new voice or a new instrument and you can you predict it will keep the same kind of signature sound so so you would just h have like a memory or constancy in layer four of like whatever feature you get as input in layer four you would well I would say I would say first of all it could be just a random set stuff but um but I if I don't if I don't represent it in many columns I can't represent as an SDR because the whole point of the minicoms that I need the minicom is is the unit of change. It's not the cell. It's the the min column. If something's going to change in the in this object, it's going to be recoded as the the unit of the mini column because I have to do these changes in high order sequences. So I can't So um I c it's basically the mini columns have to define everything. I can't rely on an S str to do that and I can't transition SDR to SDR. It's basically I have to I have to represent the saxophone as a set of minicoms and and Neil is saying oh I can have a lot of minicoms. Yeah, but I don't think we have enough. Um and and each I think I think in my mind the minicoms actually have to represent something physical. They they have to be a real thing that could be detected, not something that is inferred from a sequence. It's got to be it's got to be like an edge or a color or you know a texture or a sound starting. Um it can't be um it's anyway I don't I haven't heard an answer yet to this. I think it's possible there could be some analogy to the way we're talking about color on Monday. So the center of the column would be representing sort of like morph or color I guess and the the outer like inter blob areas representing morphology, right? Some kind of separation like that between the the model of the intervals in the melody with the texture it was about. Right. The problem with that is I still have to represent the saxophone with a set of mini columns and the same set of mini columns would have to represent every other instrument that I ever might hear. Yeah. And there isn't enough mini columns to do that and it just wouldn't work. I mean there is no point there's no single thing I could detect that says it's a saxophone. It has to be a combination of things over time. And so um I mean if you had a low-level column that like if saxop if the sound of saxophone almost was like an object itself, right? That's what I'm that's okay. But then I guess it um like I feel like taking a step back about like the number of mini columns and stuff, it feels like this is a general issue with like anything with hierarchy. Like if we're going to pass the object ID from L3 to L4 in the next layer, like we somehow need to be able to uniquely encode that, right? I agree. This is a general problem. We we we didn't really deal with it in the past. We just waved our hands. We said, "Oh, we're at we're at some edge on the object and or some point on the object. We're going to sort there's a feature there. Oh, the logo is there. Okay. Uh we didn't ask ourselves how does the logo ID get encoded in many columns. I just assumed oh the logo is going to be some high order state of the minicoms that are active like oh there's an edge the individual cells that I pick in there are going to represent the logo. So, um, but that is weird because then if I was at a different point or as a different point in the object, I wouldn't pick the same cells. Um, it didn't work. We never we never tried to get it to work. So, I think there's a general problem for our morphology model. We don't know how to encode logo in layer 4, something like that. And this the what we're struggling here with these behavior models is the same problem. And so ju just to clarify on the saxophone example, the issue you're pointing out is more about that actually detecting the saxophone is in itself like a temporal behavior. Um and not the problem you're talking about is not about like constancy like if it was color it would be easier to do. The reason I bought the saxophone is because it proves that you can't do that in layer 4. Layer 4 cannot learn the sound of a saxophone over time. I can't, right? I can't do that. So, it proves that I can't just treat it as some static SDR that I pass in. I mean, it I can't I can't represent it in layer 4. It just it just proves that I have to have another place that's doing inference over time. But that's so that's just because the saxophone is like an overtime kind of detection. If it were just color, you could represent it. Well, I think here's what I'm getting at. A column, take a prime, take a column, the the mini columns in my mind always have to represent something you can actually name. It's a thing. They're not they're not part of a distribution. Mini columns are always like it could be it could be move the the center is moving in this direction. That's a thing. or the there's an edge at this location. That's the thing. There's a color at this location. That's the thing. And I can enumerate all the colors. I can enumerate all the orientations. I can enumerate all the different directions I can move. So each column can mean each mini column can mean something. It's not a distributed representation. All yes, there are multiple ones active because when I'm moving in particular direction, multiple minoms will be active because some are moving like this, some like this and this and like this. Um but it's not they there are not distributed representations. Each minicom means do we know that for sure? because I I mean probably most the experiments that have evaluated that have used like very simple uh you know a gavor filter moving we don't know that for sure but the entire our our mechanism for sequence memory assumes it if you throw if you say the mini columns are distributed then um can't can't the sequence memory can't like couldn't a set of mini columns predict a set of mini columns rather than one to one you the sequence memory has this basic property where you take you represent something in many columns and then you represent it uniquely in the cells right right and so the thing I'm representing always has to be represented in many columns and then I can say okay that thing I can now say I can represent different things I can say there's an edge to the object and now I can say oh that edge I have many different ways of that edge is occurring at the left end of the tiger it's it's occurring at the front end of a stapler Sorry. Those are like the um uh what was the name of that guy? What was the name of those cells? Um um those cells. Yeah, border ownership. Border ownership sales. Thank you. Um so we know it feels like that it would still work. And in some of the HDM papers, in some of the HDM papers, you do have multiple columns active, but but like with with only some cells active. We did this in the um when we first wrote the temple memory algorithm. We didn't understand a lot of this stuff and so we basically said oh there's this mechanism we learn high order sequences and um but always when an input came in like an element in the sequence it always invoke the same set of mini columns always it it didn't invoke a set of cells invoked the same set of mini columns the mechanism yeah why wouldn't that happen here so like let's say the saxophone to represent all I have no way of representing saxophone versus obo versus dog versus xylophone, you know. Um, I don't I don't I guess Okay, but so okay, I accept that maybe the the numbers just don't work out, but it seems in principle the sequence memory would still work. Like if you Yeah, I could learn Yeah, I could learn an arbitrary any arbitrary sequence of SDRs and that's what sequence memory does. But the whole point of the sequence memory is you have to come in with a particular item that you're now going to represent in mult many many different contexts. That's the whole point of it wi-i which is like if if it was a child object whether that's saxophone or logo or whatever let's say that activates mini columns 4 22 and 109 right like every time you see logo it's going to be four 22 and 109 and then we but I'm saying I don't have enough minicoms to do that I don't it's not going to work I can't a minom can't be I I just don't have enough minicoms I it's like it seems to me like I can define certain things like in an object I can say it's got it its edges and its colors and maybe a textures or something like that. But when I assign the Phil's logo, I can't represent Phil's logo in a small set of minicoms. I you know from every other possible feature I can there are thousands and thousands of objects that could be a feature on the edge of the stapler. I can't represent them all minis and minicoms. It's just not enough minicoms to do that. So it I just don't see a way around it. I'd like it to work, but I don't see a way around it. Yeah. Um Yeah. I mean, it feels like for this reason, like not every column could learn about every object like No, it's going to somehow rely on on kind of it somehow separation. It somehow says that until a further notice, keep assuming the same thing that happened before. It's the same logo or it's the same sound of an instrument or whatever, but that is not going to be detected in the parent ch the the parent column. It's like it's going to be detected in the in the child column. And so the child comp somehow we have to you know just think about our um let me just aside from imagine our compositional uh our compositional model right how we do composite objects. There's a back connection between the parent and the child and that back connection is very specific. It says on this location on the parent you should be on a particular location on a particular child object. The parent I don't this doesn't take any extra storage in the parent object. I the parent object doesn't know what that is. It just says I'm at a location you learn to associate with whatever you are doing down there. we have the right bandwidth going backwards meaning I can associate a very specific object with a specific location on the parent object as long as I'm going backwards but I but when we go forward now we're talking about forward like okay I'm going to associate a particular child object to a location on the parent object that's the one we're struggling with um so there is a connection between the parent and the child it's just that the parent I'm having struggle how the where do I store how do I store that child going into the parent. Maybe I don't need to um maybe you're not storing the full child. You're storing something that guides the lower level to integrate the sensory input that's that's going into it and the the higher level guidance that's coming, right? But that that is isn't that what I just described, the back projection. That's like the parent can tell the child um the parent doesn't know what it is. I'm a coffee cup. I don't actually know what's on my location here. I just know that at this location you've somebody else may have learned that there's a logo there at a particular orientation, a particular thing. So I'm a I'm the coffee cup with a logo. I said I don't I don't know what that is, but you know what it is because you you've already learned what you're doing at this location. So parent the child, you take care of it. That works in the back direction. I think that's what you were just describing. There is an association between the parent and the child where the parent doesn't really know anything. It just says I'm I'm telling you my locations. You figure out what was there. Yeah. knows just enough to to guide the it's very specific. It's like this is a location on the on the cup and um exactly no place else in the universe. We're right here on this point on the cup. You can associate well you were you were a logo at some point on the logo. Um so the backward connection works. It's like it's like if I was in the melody I could tell the child I like I don't know what voice this was but you did so you keep going you know. Yeah. I think we talked about this like maybe two years ago when we talked about hierarchy whether we actually need to learn the connection both ways. So we did talk about this. Wow. Okay. I think um I think at some point I was arguing that there's it's a bit inefficient to actually store the location of the child on the parent and the parent to the child both ways. Yeah. To do it both ways because you're kind of encoding some redundant information there. So what if the parent doesn't actually it we just have the connection uh from the parent associated with what the child should be perceiving but the forward connection actually only tells the parent that a feature was observed at that location or maybe something general about this feature like there's a a round feature there or there's like elongated let's follow this a bit here Layer three is the quote input to layer four in the next region, right? It's that's the output of the child. It's a layer four. Right now, we're talking about layer three representing the moment. I'm talking about representing maybe behaviors. Yeah, that's I don't Yeah, separate. Well, my point is what if we said, oh, we're not telling we're not passing up the object ID. Yeah, just the orientation and or we're I don't know what it is. Maybe you know, but you you just pass the orientation of the child object. Well, I'd have to know the orientation child. Um, and maybe I'm actually passing up something about behavior. Maybe it's important to tell the maybe I don't know. Um, I don't know about that. Um, right. Can we make this work if we don't tell if the parent child doesn't know what it is? It's just you're saying just the orientation. Yeah, that'd be clean. because you could still make predictions about the features of the child object because the parent has learned the backwards association, right? And you can still recognize the highle object just from the relative look orientation of the child object. Like if you have the the brute face, the banana mouth still needs to have the correct orientation, right? But it it doesn't matter for the parent object whether it's a banana or um a carrot or whatever. Yeah. So this is a pretty sort of radical idea, but not not in a bad way. It's just means that we were thinking about it all a lot wrong. This idea says, "Oh, the child object doesn't actually tell the parent object what it's viewing." Yeah. It just says, "I'm viewing something at this orientation." And and then um then the parent objects back and says, "Um, well, well, whatever you're viewing, this is here's where I am. you do it as you will. Um, I mean, yeah, I I like that. It definitely sounds possible, but it's also kind of it's it's weird because it's like just thinking about, you know, how you conceptualize a compositional object. um like at some point you do kind of cognitively know that the low-level pieces are part of the high level piece but um you know it's funny I'm I'm thinking here like do I actually perceive the child and the parent at the same time? Um I mean do I perceive them as a whole or is it like oh I'm at the coffee cup. Oh, look. There's the logo. Now I'm looking at the logo. Oh, now I'm back at the coffee cup type of thing. You know, it's like um there's lots of examples where there's child object and parent objects where you don't actually notice any of the details of the child object until you attend to it. Um you can get the rough information about the child object from like if you have a skip connections from the sensor to the like the larger lower resolution receptive field projected directly to the Right. Right. Level telling it some color information. That's another Are we debating here if the hierarchy is represented in one column or uh no uh just whether the the child the lower level column actually whether it actually sends an object ID to the higher level column or if it just sends orientation of the child object and then the parent object learns an association a backwards projection. So that's tells the child what features to expect. Let's let's think about the melody example. The melody is uh the voice is a a mini sequence. Okay. So let's say child object learns this mini sequence and it can play it back. At the beginning of the mini sequence when the attack of the note occurs, that's when I want to tell the parent that hey something happened here. There's a new thing here. And the parent says okay there's a new new note occurring. I don't know what it is, but there's a new note occurring. Um, I'm going to look I'm just that's I'm in my my sequence here. And then but it it's but it's associated back at that point um with um the location. I don't have to think through this. I'm just saying there's a there's an association back to the child which is essentially says you should be at if if I'm a predicting the beginning of a new node, you should play back um um a sequence that that was last you last played back. So like the child object says I'm just going to assume I don't know why why the child object would do this, but it's assuming it's going to play the same thing over and over again. And so that every time you get a new note in the M in the sequence, it tells the child object, you know, yeah, it's a dog barking, you know, something like that. Um, I'd have to work through the details. We we'd have to work through the details on this, but um, yeah, it would be reinstating more of like a behavior in the low-level, right? It would it would it would kick off that behavior. You'd want it to start at the beginning of the sequence of the behavior because I want to play the whole dog bark or the saxophone note. So, it'd be like I'm telling I'm telling the parent at this point something started. The parent doesn't even know what it is. like I maybe I haven't even the saxophone town the sound of a saxophone takes some time to play out but I want to what the melody is important to note that it started now so you want to like say okay the note started now what the voice started now what actually um um it's going to play out in in the in the child object but the parent object say oh something started and then later on the parent object can say you know play that sequence again at the beginning it's like saying oh I'm expecting the note to start here and the child goes, "Oh, I'll play back the saxophone note." Um, something like that. Um, there's a lot of questions about this, but um, I think it's a good idea.

Um, by the way, this doesn't mean we still don't do temple pooling in the child object and the parent object. We still don't we still might label these things. I think we have to which things you know like we we're now we're now no longer setting up the name of the item item to the parent right that was we we hypothesized we have to do temper pooling to come up with a name for this object and then pass it to the parent and now we're not doing that we're not we're not sending up we're just saying an item occurred and we have to calculate its orientation or something like that um but we're not telling the parent what it was I'm not telling the parent, oh this was Phil's logo or this was a Starbucks logo or this was a saxophone or you still do temporal pooling and vote on that. Well, I would think I'm saying we don't have to throw away temple pooling completely because one of my tenants is the brain has to do temple pooling in every column otherwise you could never name anything. Yeah. So, we'd still have to do temple pooling, but that's the output of that is not being sent. It's being sent laterally, right, for it's being sent laterally. It's in some ways I like that there was some there's a lot of issues about sending names up the hierarchy. But so, I'm just I'm just walking through the consequences of this. I no for from a compositional hierarchical point of view, I no longer need to have temporal pulled representations from a child, but I still want that for voting and um other reasons. Yeah. Right. So, we're not getting rid of temporal pooling. We're just not passing it to the next column. I'm just walking through the consequences. So, what exactly are we sending up? Just orientation. Well, I think I the hypothesis right now uh is we're sending up um orientation Whatever. Yes. Something. I don't know what orientation orientation of the first part of a sequence like how how no of the whole object there's like a you're basically saying a child is saying um of the frame something occurred here. A child says um um well it's just basically sending the child basically sending up an orientation. It says at this point, whatever we're looking at, here's the orientation of of my my object. And that's all it's sending. Yeah. I'm thinking of a message, right? And the parent is basically sending back. Okay. Well, uh, we're now going to connect between you and me. And and so now when I when I'm at this location, you're going to invoke the exact right point in your object, but no one knows what no one knows. They're not they're in separate rooms. They don't really know what the other one's doing. Could a could a compromise be like it sends something up but it's not like it sends a super unique fingerprint or at least L4 is in is insufficiently well what are you trying to prove why why go there I think I guess uh to to like bias because like yeah we can recognize a face made of fruit but I'm pretty sure we would recognize a normal face uh much quicker um like that that information can be useful to to narrow down the hypothesis. Right. Right. One thing you could do um or like even the new mug like it's it's almost like how how do you recognize the numen mug unless you pass at least some of that information up because uh otherwise it's like you just have an orientation of a logo. Uh and sure you have the skip connections but the whole point is that those are kind of too coarse to learn another. Let's assume the only way SDRs work is by association. So I could associate the logo ID with something in the cup. I I I could associate it with the lo a location on the cup. Um I guess but that's a lot of memory. Um I mean I I you can associate two SDRs. That's what you can do. You can say okay two two people have two representations and they're kind of coincident and I can I can say my bias is yours. It's just like voting but you know we're now I'm saying something like you could the logo could vote on cup with logo something like that. Um we just can't represent in layer four. It it has to be just um it has to be an SDR to SDR. So I I'm with you. I think you're right, Neil. I think we have to still somehow bias the cup somehow. Yeah. Or it's just like it's it's like a highly redundant like I cannot decode logo from that SDR in L4. But if I have the representation mug in the highle one, then the bit pattern is sufficiently like like that tells me, oh, it's probably the logo with the mug. that exact same bit pattern may also appear for guitar or um you know Phil's fills or whatever like some some other thing. So it would only get confused if you were trying to kind of reuse the safe bit pattern in a similar context which maybe is unlikely. I don't know. Um uh and and then I guess the more familiar you are with an object, maybe the more neural resources you can dedicate to actually having a unique L4 representation. But but yeah, maybe like the default is it starts at like just orientation and then gets more specific or let's let's let's walk this a little bit and taking your your idea and walking back some of the stuff we said. Child objects has a child object ID. I'm looking at the Nmenta logo. I could associate that with any kind of SDR in the parent object. For example, every SDR in um layer 4, which is let's say the let's for the moment just assume that's like the the edges of the object. every SDR in layer 4 or edges of anything really or just any location just basically I could I could associate the logo object ID with any location specific location in the parent object that's has the logo on it just going back to what we were trying to get rid of. I can associate it with that. Um, so I could say, um, I know that doesn't work. I don't know. I'm sorry. I'm trying I'm trying to make what you I'm just trying to think what are the options there. Um, does it make sense what I was trying to say that like so I accept that like L4 probably doesn't have the capacity to be like oh these this set of mini columns active means new mental logo and this set of mini columns active means Bill's coffee cup or whatever. Yeah. But if they are if there's some like overlap in them. So like, oh, this set of mini columns means nomento logo and it also means chair and it also means basket or something. No, I'm not going there. I I don't I don't I don't want to push back on. I I don't I don't think pursuing the mini column idea. You could you might be right, but that doesn't that doesn't strike me right. I in my mind I don't I guess I'm just thinking that it would bias the high level representation. I just I think let's go back to the thing I just said. I said you can associate SDRs with SDRs. So I have an SDR someplace and I have an SDR someplace else. This is a we can associate it one way or we can associate it both ways on the map. This is a really good mechanism. It's high capacity. We can do we can do millions of these things and I got well hundreds and thousands of them between two populations of cells. It's great. And so this this pattern can invoke a pattern over here. What I think you can't do is go from an SDR to a set of many columns. because the set of mini columns is not an S str and I don't think you can do that. Well, it's it's a low dimensional SDR. Yeah, but it's not really because the the mini columns themselves have to have a specific meaning. They have to they have to be have a constant meaning. That's in my mind that has to occur. You can't say this mini column is now part of one SDR. Now it's in a moment later it's part of another SDR because then you as long as it as long as it reoccurs when that feature or object occurs in space. Why why does it have to be like that is specific? You lose the ability to do high order sequences. I need to be able to do high order sequences in the in with many columns and high order sequences. Do we need to be able to do it just in the mini columns? If we have the behavioral reference frame and the struct and the structured reference frame that's where the sequences are existing like it's it doesn't need the htm where it's only in L4. In my mind, many column a layer of minicoms is a way of taking a representation that's not unique and making it unique in context. Yeah. Right. And and so I can't make the minicoms be unique. They have to be non-unique. The set of minicoms have to represent something that's that's repeated over and over and over again. And now I want to represent a unique context. So I can't encode other I can't use the minicoms as as an SDR. Yes, I could still learn sequences of it, but it would throw away the idea that there's a um that you're learning they're learning something in different you're representing something differently in context. You know, um this is true for the grid cells. Basically, you're you're representing locations, but only when you look at the set of components, it's unique in context. Um you know, you get there's a feature in layer four, but in in context, I have a unique way of representing. So, I I just don't want to go there where I I keep I'm trying to make it work, but I don't think I go there where many columns. Yeah, maybe maybe uh I can try and write what I'm thinking. Okay. But but but before Right. Okay. I Yeah, you could try to write it down, but I don't think it's going to work. But I do think the idea that we could take an SDR in the child object and associated with an SDR in the parent object makes sense. Um, and this is the, you know, the bias of like, oh, it's, you know, a banana doesn't suggest a face, but, you know, a nose does. Um, uh, I can still recognize the face with just a bunch of orientations, but if I see a nose, it's more likely to suggest a face. Um, so then I'm asking myself, okay, if I have a child column and a parent column, and I have in the child column, I have an object ID, which I say is an S str, what could I associate it with in the parent column? Um, that's also an SDR. Oh, okay. So, um, I don't know if that's where you're going. Well, I'm thinking where are there what are SDRs that would be associated with this? One would be actual the location on the object. The the the location signal is a unique SDR that could be suggested by this. But that's not very good because it would suggest all the it would suggest all the locations that have well it could. I don't know. But anyway, what if it's kind of like a lateral voting connection and it's basically I mean we don't always have to vote on the same object ID. It could just be biasing. So what I was wondering, what I was thinking is a problem is if the higher level column has learned a model of a mug, how does it know to predict the the logo down the lmenta logo down here? I think we solved that problem. Yeah. But if it has learned mocks with different logos on it and it doesn't get the logo ID as input, how does it know this is the luma mark versus mug or whatever? and and the voting if we could vote that that could solve it because we this one could I don't know what you're voting on voting object ID we would learn an associative connections connection between na logo so you're voting you're saying you're saying uh the logo is a is a is voting on the meta cup yeah the logo would bias that the high level column would detect you know I think correctly point Now, we do need to send something up here, right? You need to send something about the the logo to go up to this guy. That's the assumption. Now, the problem with this is um well, it's not a problem, but it's basically saying I'm associating the logo with with the entire object here. Okay, maybe that's right. It's certainly not specific to where it is in the object. Yeah, it wouldn't say where it is. It would just bias that this is the Nmenta cup versus Pete's cup. And that would then tell the backward projection what features to expect at that Well, that would be you know what's funny about this is well it's not funny this is this is suggestion layer 3es they say oh layer three cells predict to layer four in the next region but they also layer three vote cells vote with layer three in the next region yeah so it would be that layer three to layer three I don't understand but you're saying this is the voting one you're saying so basically we could just assume we're voting but we're not voting on logo we're voting on uh this child is voting on public logo if If the lower level is detecting an eye, it makes it more likely that this one would detect a face, right? But it's not telling it, oh, the eye is at this location. It's just saying I gives you some context for the potential objects you might be. You know, it's funny because if you look at like a Picasso picture where all the facial features are mixed up. It's kind of what's disturbing about it. You keep thinking it's going to be a face, but then you can't make it work. It's like, oh, like you see the no, oh, that's a face. No it doesn't work. Oh, that's eye. Oh, no Doesn't work. It's like it's suggesting what you're saying. It's like every time I see those components, I say, "Oh, I I'm voting. It's a face." And then and then it doesn't work. Where if you see the banana, it's less It's less annoying. You see the banana for a nose. It's less annoying. It's okay. Fine. Anyway, um Okay. So, the suggestion here is is a simple one. We're voting on object ID between parent and child. Yeah. Because voting doesn't say that they need to like we we when we vote we never make the assumption that they use the same str to express the same object. Like if we vote laterally on cup this one has a totally different S str representing cup than this one. So there's no issue of there is no it's just whether there's there's a good association. Yeah. Um, obviously, um, you know, in the in the hierarchy paper, I was just editing it and there was language in there. I don't know who wrote it, but there's language in there that says, um, in fact, I might have just removed it, this language, so I might have just commented it out because it was long and complicated, but it was saying, why is there a discrepancy between parent and child object? And I said, well, what what can the child object tell the parent object? The child object can say like I'm seeing a logo. You should be looking for something with a logo. That's all it's saying. I didn't say I didn't say anything else. It didn't say like you should be looking for something with a logo at this location. It was a the language I wrote was you know there's a there's a logo down here and because I was thinking like there could be lots of objects logos like it can't tell where where it is on that these other objects. You can't say what objects it is. It just says you should be looking for something with a logo. Yeah. So this could also bias it to detect the NATO t-shirt. Uh for instance, if if the child you just introduced a whole Oh, yeah. Yeah. Yeah. Right. Right. Right. Like if the child object detected the NATO logo, right? You should be looking at you should be looking for anything that has a logo on it. Right. Right. And that's how I wrote it in the paper. Although I wasn't thinking along these lines. Um I thought I didn't think it through. I just wrote it down like, "Oh, that makes sense." Um or somebody wrote it. I think it was me. Um uh but I just deleted it I think. Um so all right. So this in some sense does this work? Does this solve all of all the issues here for at least for the for the parent child thing we're voting? It it kind of works. I feel like, yeah, I mean, in theory, it it could work, but it it also feels like um yeah, it's not great either that we're not passing up kind of feature that's that's more unique to a location because like one of the advantages of hierarchy that we've talked about is like we'll converge much faster because you could say like, "Oh, now you should be seeing a logo because of where you are or now you should be seeing." Well, we we we do have that if we're on the logo. If we're on the cup, the cup model will tell the child model, you should be viewing a logo exactly at this point. And we can make a very specific prediction. Um, so we're telling it that it's so the backward projection is right. It's just the forward projection, which is the weird one we're dealing with here, right? Right. Yeah. Yeah. No, and I guess that's what I was referring to. But um Okay. Yeah. And I guess just the other thing is I just know when we looked at these lateral connections across hierarchy, they weren't as prominent as we kind of thought. So I feel like if this is if this is going to be like integral to hierarchical processing. Well, we could we should go back and do that. uh you know um I guess this is one of those situations where I really want to understand the details of the experiments because everyone's reports that layer three projects to layer four um and that would be their bias going forward and what and so they're going to report something different what is the um it would be sort of contrary to that and what is their you know what's their methodology how are they looking what was you know I'm not saying it's the that the imperial data is wrong, but it's one of those cases I just don't accept it. I won't accept it right on the face value. Um I want to dig into it. By the way, on the topic of layer three, um we now kind of put the behavior model there as well. Could we also say that like the behavior model is in the upper part of layer four and the morphology is in the lower part of layer 4. It seems odd to have the output layer be the behavior model, right? Right. Um well no the reason that there wouldn't be upper and layer lower layer four I'd rather do upper and lower layer three and the reason is the behavioral model requires apical dendrites to layer one it requires time so the general rule is that layer four stellate cells it's mostly stellate cells they don't have apical dendrites so they wouldn't get any time information now that used to be like oh all layer threes are cells are stellate cells now people go oh no some are some are parameal cells you know find some there but in general rule the stellate cells which are primary layer four don't get time and any the behavioral model would be all cells that get time they all have to have enable dendrite so that could be either upper and lower layer three it could be upper and lower layer two could be any one of those some people have made that distinction between upper and lower layer two and upper and lower layer three and other people say I don't see that um so but I would think that we could pretty hardly say stellate cells non non not apical dendrites are can't be part of the behavioral model and the behavioral model has to be um has to have apical dendrites. Yeah. Okay. And within layer three we can definitely have two layers there. Um you know some people really the layer 3 A and 3 B distinction has been made many times even though you know it's very vague. um you know there's subtle differences and I don't know if anyone knows about um so I wouldn't I I don't do upper and layer lower upper and lower layer four I would do upper and I got twos and threes like behavioral models up there someplace yeah I guess yeah layer three and four works too if we can say that there's different subopul of layer three that projects it could be it could be again they already have been proposed upper and lower layer three upper and layer really true so any one of those could behavior month. As far as I'm concerned, we got lots of cells up there that are unaccounted for. Okay. Then we'd have to look at we'd have to look at um what it what it does require is it requires that there are two sets of cell populations representing a location that can anchor differently. So there'd have to be two like like here's something that's weird. um layer 6A and layer 6B look like they are they're so parallel that you would I would think oh those are the two reference frames one for behaviors and one for the objects and we've thrown in layer 6B as orientation um orientation is something that does not have to be represented unique to the object you don't need many columns to represent orientation head direction cells are not are never unique they're at least general idea of head direction is also not unique. So it's not like, oh, I'm facing this direction in this object. I'm just facing this direction. So it's possible that layer 6A and layer 6B are the two um um reference frames we need for objects and behaviors, they would look like it. And then maybe the layer 6B cells that represent orientation or a small subset of layer 6B and maybe not all of them, something like that. I don't know yet. We have to work through that. But on the surface, if I just had to say, where are the other two locations? I would say 6A and 6B. And then I say, well, where are the two um what's the layer equivalent to layer four? I would say it's in layer 3 A or 3 B or 2 A or 2B, something like that. Um that would be the suggestion. Shall we take a short break, by the way? Yeah, I need to take a break. Get some coffee and stuff in the bathroom. Yeah, I might um I might sign up for the evening. Okay. Yeah, we're recording so you can But yeah, please uh please record it. Well, good contributions, Neil. So, we'll miss you. Catch up catch you next day. That's good. Yeah, have fun. It's a really cool discussion. Yeah, nice nice evening. Thanks. Feels kind of like we're doing a puzzle and when you put Sometimes you're kind of stuck and then you suddenly put one piece in place and you can put in a bunch of other pieces afterwards, but then you notice that you put another piece in the wrong place. In um intelligence, I I describe how the brain works like a puzzle. It's like a jigsaw puzzle. And um I said the problem is there's like a million pieces and you're missing a lot of the pieces many. And many of the pieces have um images on both sides. You can interpret them differently. Like this piece can be interpreted A or it can be attributed B. So you're trying to solve this puzzle with all these missing pieces. You don't know if you have even sufficient number of pieces of to solve the puzzle and the pieces can be interpreted multiple ways. That's I made that analogy. I was like this is like crazy, you know? It's like so hard. But in theory, we have more enough as you have more pieces. So what you end up doing is you end up putting a bunch of pieces together to solve a some subset of the puzzle. That was like the temple memory. So a small subset of the puzzle, but we got that one figured out. And then we're like, oh, now the morphology of objects. Oh, we think we got some of that pieces in there. Now we're like, okay, we have some of these pieces. Now let's connect them together. Now see if we can figure out the ones that are missing in between. And sometimes we have to we have to make up a piece because no one's actually observed it yet in biology. They're like, "Oh, that piece this this piece should be here. We don't have it yet. Some pieces have fallen under the couch." Yeah, you lost them. Oh, and the other the other thing I said was I said, "You have some pieces." And then someone comes to you later and says, "Oh, you see all these pieces? They were wrong. Let me take them away. I'll give you some new ones." You know, that's when when the scientists come back and say, "Well, that wasn't the right observation. This is the right observation." It's like, "Wow, I've been working trying to figure all those pieces. Now you take them away from me." Oh, it's so hard. Um, you think you'll end up writing another book? I don't know. You know, only if it's necessary. I think, um, I don't, you know, it's not, it's not fun writing a book. I just thought it would work. Um, but, you know, I thought about different books I could write, very different types of books I could write. I could talk I could write a book about future humanity. expanding on the parts I did in the last part of my last book, I could write a book like quarter of this, like just like here's the book on brain theory, everything we've learned in a nice cohesive format. Um, sort of more of a documentation than it wouldn't be a big it wouldn't be a big seller. It would be more like documenting a different form of documentation. Then I'm saying, well, do we need that? Because we got videos, we got all this stuff and you know, maybe a books are old school. Old school. We need a book. I don't know. Yeah, my bias would be towards, you know, the brain theory book, you know, like once the give it a year or two, whatever gets worked out here, it's taking the concrete learned aspects basically augmenting the last book. Yeah. Well, you could really just break, you know, but then you that book would go into lots of details that would turn off most readers. True. Right. It would be a very it's a small it would not be a a big seller unless you know unless it was somehow got popular like like um um brief history of time by um what's his name? Stephen Hawkings. Steph Hawkings uh he that was a huge bestseller but if you read it it's a damn hard book to read. It's like oh my I just got lost there over and over again. like well why was why was such a huge sell because the great title was short became famous so everyone felt they should buy and try to read it but very very few people book could make it through that book so that's the exception like you can't have a really dense book and still sell a lot well it probably didn't sit on a lot of bookshelves well I'm sure it did because you know look I was really motivated I couldn't get all the way through it yeah I still have it on my bookshelf and did you start reading it no um see I had it on my to read pile for like 10 years So if you do read it, it's hard. I found it hard and I I love that topic, you know. I found it hard. It's too difficult to read and dense and I can't even say it was easy. Well written. It's always so tempting because it's pretty small. Like it's right. Oh, it's so small. But get it is what a great topic. We learn all about time and it's a little fun. Just really really hard. And also he was a very famous guy because he has a disease you know it's like this guy doing all this great science he can barely even speak and you know wins all these prizes. So yeah let's let's read his book. Um so uh where do we end up here? I think we're going around circles a little bit. Um I think we have a clearer uh idea of at least what how we project forward and I think we also maybe concluded that um and Hills is going to push back on the mini column thing. I don't want to go there, but for the moment I'm going to go for the voting, right? SDR is uh the the child object basically just votes on what the parent object would be. And um that's probably one directional. I don't think I would need it. And by the way, some of these layer two and three connections are one directional. So I don't think I need to go the other way. I don't think that, you know, the cup has to pred predict that there's a logo because you can't be certain there's a logo, right? probably would use the same mechanism for behaviors. If you detect a behavior that can also kind of in bias what objects you're expecting to see. Um, yeah, I'm thinking about that. Yeah, maybe. this. Yeah, I was just wondering if we would pass up the behavior ID, but I guess question, right? When we when we talk about compositional behaviors, um, as a whole field we haven't really dealt with is hierarchy where the the fixed there's no fixed relationship between the child and the parent objects. Like, you know, I want to do some behavior and I can do it in various ways using different parts of my body and different things. So there's not like a a clear from a behavior point of view, it's not like you do this and then this breaks down to this breaks down to this. It could be it could branch off in different places. Um and my point is we have to deal with hierarchy in general in a bigger context of behaviors that we haven't really thought about. So that that voting between layers is that uh voting or associations between layer three and layer three in different two different regions like hierarchy right here. this this idea that the child to the parent that's that's the same voting mechanisms we've had elsewhere it's just SDR to SDR they don't even need to know that they're in different hierarchical layers okay but is there a difference between this and just associative connections between the STRs is it or is it the same thing that is it is associative connections with with um the way it works it has to be associated between the two different regions and also associated within the parent within in each uh each column like like um the the cells the the the many columns themselves or the many the unique representation is it's just like yeah you want to lock into something right you you don't want to just say oh here's a here's an SDR it's more like no we want to settle on this one or this one we're going to it's either a cat and a dog it can't be cat dog so pick one you know it's a it's a vase or a face not a bond pick them. So that has to have that locking in effect which also has to occur within the column. So but that's pretty simple. Basically all the cells just make associative connections to other cells other cells nearby other cells far away. It doesn't make any difference. Um a little bit off topic here at one time I was thinking about the voting and I said oh we're voting over all these columns all these columns. all these columns, right? And so you think that might require huge numbers of connections between these columns and say I have like 5,000 cells in column A and I'm trying to vote with, you know, thousands of other columns. If it turns out that any particular cell, you take any particular cell in column A, it can associate a link literally to sub some sampling of all of cells in all the other columns. It doesn't have to it doesn't have to go all columns. It just has to be some subsampling and it and we just pick random and another cell just sub connects to some other subsample of all these other columns. So in particular cell doesn't have to connect to even half the columns out there or even but if you do that it all works. It still works. They still all settle. So you don't have to have these massive connections. Um it's I don't even have a name for that. Yeah. the propagation between all of them at the same time. It's just going to keep selling. It's just going to settle regardless. This gets to the point. A cell doesn't know if it's connecting something nearby or far away or other column. It really doesn't know. It doesn't know anything. Says I'm just trying to associate my patterns with some somebody else out there. And they don't have to be coming off. Um used to be like, oh, I have two cell populations like this cell would have to connect to at least 20 of these cells. But it really just has to connect with 20 of cells anywhere that representing the same thing. Okay, off topic, but um this I think where we're going with this, we got down this path because we were talking about like um in the notes of the melody that a child object um would be the only thing that knows that it's a saxophone. A saxophone may vote on the song because maybe the saxophone is associated with particular, you know, odly or whatever. I don't know some song. Um but um but maybe not. So if I'm hearing the song with a dog barking, I wouldn't have any association with dog bark with the melody. So that wouldn't work. Um but doesn't matter because what happens is the lower the child object recognizes the dog bark. 8 basic it says I start here's it has this you know note here uh we're calling an orientation. I don't know what it means in songs but something happened here. a change occurred. Uh the parent object doesn't know what it was, but the parent object basically um uh there's an assumption somewhere in the system that it's continuing to be the same thing unless someone said otherwise. Um and I don't know. Um um that's an interesting question. If halfway through the song goes from dogs to violins, um, who's storing that transition, I think that transition somehow be stored in the parent object. Be in the behavior model object, but somehow it has to tell the child object you're you should do something different now. Yeah. Oh, it would it um it would, right? It would it would basically say, oh, at this point you you said violin, another point you said bark. So it would just naturally whatever the the parent object doesn't the parent object actually doesn't know the parent object just says I'm going through the melody and um oh wait I don't know it has to oh yeah and then at some at this point in the melody you're associated with something and at that point melody could be associated viol that point melo could be associated dog um I don't even know if the parent object even needs to know the voice change does it I don't think it That's what um who else would know that? It's in the backwards. It would it would make the correct prediction because the parent object says at this location I heard a dog voice and another location I heard a saxophone and and you learn that association. Yeah, it would just be stored in the in the layer six backward projection. Right. Right. It's just Okay. I was thinking part of the model. It's not actually part of the model. That's that would be an implication of this. It's not actually part of the model of the cup or the model of the saw that the interesting. So, the equivalent on on a on the stapler, imagine I have the logo of Pete's and um and coffee cup, the Starbucks, whatever. And um the model stapler doesn't know uh well, it it would have the the voting connection, but it doesn't know at this location there's it it doesn't know what location there's a a logo. It just knows that at this location, the child object isn't told there should be the child object knows to be a logo. the parent object doesn't know that. And this also means that if the if the logo changed from pizza to Starbucks, um the parent object wouldn't know that. It would it the two know it because the two together the the lo the safe says, "Oh, at this location you you should see something." And then at this location, here I am. And the child object go, "Oh, at that location I have the Starbucks logo. At that location, I have the pizza logo." Um, but the cup wouldn't know that. The cup would have no knowledge of that. Um, is that what that implies? Would it lay down? So the the associative the feedback connections from layer six to the logo in the ch child model, would that be the reference frame of of the static model then or of the behavior model? Can you say that again? Um I I just didn't hear the words. So the backward projection from layer six telling the the lower level module which um which logo to expect at what point in time would that be coming from the reference frame? I don't know. That's a good question. We could we now we have the option of sending back both reference frames. Yeah. Wow. Well, I would suggest this might be the clue to our hierarchical behaviors. because you could have hierarchy of behaviors and hierarchy of models. Yeah. And and so if I pass back the specific location in a behavior that could invoke a child behavior at a right this would be like I am right I don't I need an example but that essentially hierarchical nested behaviors could work the same way as hierarchal nested objects is that a parent behavior wouldn't know the details of a particular point in the sequence of behaviors but the child But yeah, so then in the case of the logo changing that seems like a behavior, right? So would that be would the lower level module learn the behavior of the logo transition and then the higher level module would just tell the child to expect that logo change behavior? Now what if um okay, really weird here. Um, I have a Starbucks logo and as a Staples opening, a Starbucks logo changes. It changes from green to red or the little lady whose hair goes up or something. um um at some point in the sequence that that occurs and um I'm I'm I'm I'm make trying to make up examples. I don't know. I think the idea that you might pass back the I'm trying to come up with examples where the behavior there's a hierarchical behavior like there's a change in the in the stapler which is which a change in the subobject but the stapler doesn't know the change in the subobject. It's like um I think that needs further enumeration. But we now have the ability to say we can vote on behaviors. We can vote on objects. We can pass down unique locations behaviors. We can pass back unique locations and objects. Yeah. What do we want to do about that? You know, more tools in the toolkit here. That's a good idea though. I mean, it's Yeah, why not? Unless it seems like it's a useful thing to do at some point. Yeah. Yeah. Where where am I in this behavior would tell the child object what where it should be in its behavior or something or something like that. Yeah. Um or start a behavior or I don't know something like that. All right. I think kind of like the traffic light might encode the be like the global behavior of the sequence of which lights turn on and off and then the child object would encode the actual feature change of each of the lights like turning from like a dark green to a bright green or well maybe although I think the change there is the same if I represent color in the parent I could do that couldn't I could But maybe it it would be more useful to abstract it away to have the like the general sequence of a traffic light behavior encoded separately from how exactly the traffic light looks if it's right. I was thinking like oh I sometimes see I know certain traffic lights that most of them are moved to LED but not all of them. I can tell the difference. And when the when the ones are LED, some of them have broken bulbs in them. And so you see like broken little patterns in the in the So like that's a specific one. I know like oh where I turn near my house it's always missing these little dots there or something. Something like that. Maybe you're thinking Yeah. Just Yeah. To have like a more general representation of that red, orange, green sequence versus the actual change in color and brightness happening on each location of the lights. So, um, you might also have the lights at slightly different locations relative to each other, different distances. So, um, do we want to summarize what we think we just learned this morning? So, I try to write that on the board. Yeah, I'll try to write on the board.

 Um so what have been we decided that um um when it comes to behavioral models Uh we often need um okay often need two two levels of hierarchy minimum often I'm not saying no I'm starting off wrong what would you do first. Okay, that wasn't my main takeaway from this one, but I'm just I'm just I I I haven't organized my thoughts yet. So, I'm just making a list of things. Okay, because I was thinking the problem that I was dealing with is how do I know constancy of a feature or um how do I project that this is going to be the same? And so, we're going to use a child object to make constency. Okay. Yeah. All right. Um um object is needed to um uh essentially predict constancy of features predict um I don't I I don't have better words for it. So constant constant scene constant scene like features um um like it's still black or it's maybe a better way to write this if you do say so. Um uh the child object um can also be I thought no. Okay, go ahead. What you want to say? I thought the whole child parent hierarchy discussion was solving the problem of um when the lower level feature like the saxophone is actually like a temporal thing itself. That was the next but I didn't think it actually solved the constancy problem. Well, because it does because the parent object doesn't know what the feature is, right? The parent object doesn't know. It doesn't know there's a saxophone and I should hear a saxophone and and it doesn't know in the case of the staper I think it wouldn't I might I might I would say it knows it's continue to be black or or I might even go to say that um the child it doesn't know what the what the subcomponent is supposed to look like in in its new oriented form. Um but this was the saxophone problem. the child object itself has its own sequences that the features are sequences themselves that need to be played back. If the feature is something less temporarily complex something like just a color or an edge would you say you still need hierarchy? No, I think only for um Okay. So then I would say those are two separate problems but Okay. Do you want to take a Why don't you take a separate No, really. I'm I'm happy. I'm not I'm not saying that out of like okay you do it like maybe helpful you could do it like I'm struggling to I'm writing down a bunch of things we just thought about and then I'm going to organize them later. Okay. Yeah. No, go ahead. First I also assume that the behavior model doesn't know what the feature is. Right. Right. Uh uh so maybe it's not a how about that? Um uh because uh parent behavioral model par models in general uh doesn't know the feature doesn't know specific feature. It just doesn't know the feature. It being voted with the feature but it doesn't know the feature. Uh no features um feature ids. Isn't that also true for a behavior model in the same column? Does it need to know the features? I'm saying in general a parent object cannot know the feature ID of a child object that's projected to it. It does not voting is not the same as knowing it, right? It's not stored. The parent object doesn't know it. It they're connected via the back predictions, right? But nowhere in the model of the stapler than the mug do I actually store logo and um nowhere in the model of the the melody do I store saxophone and um in the stapler um I'm trying to come up something if there was an ID like the coffee logo there's nowhere is that coffee logo stored in the model of the stapler so is it a requirement to have uh uh two hierarchical levels to apply a behavior to a new object. No, there we didn't we didn't do that yet. I didn't say that. Is that required? Feels to me that this is trying to solve the problem of applying a behavior to a new object. No, do we we I thought we had a solution to that within one column. So, how I understood it was the hierarchical problem is independent of the behavior problem. the we we just started talking about hierarchy because we started talking about objects that are made up out of complex subobjects. So we we started talking about compositional objects where we have right the saxophone which is itself its own little temporal object. But if we I don't think there's a a requirement to learn behaviors to have two hierarchical columns if if you just No, I Right. Right. I I when I wrote when I was writing this I said it's not always required. I didn't write it down. Okay. So, one one high level thing for me is that in a parent child relationship, we never pass up the parent doesn't know the idea of the child. That was like a a new idea today. It wasn't related to behaviors, but it's a new idea today. So, parents don't know the idea. That's different than I was thinking about it. I was thinking about it that somehow that unique idea was being memorized, but not really. It's just being associated. So, not the same. Yeah. So that that was a big idea separate from object behaviors. This is a big idea in general. This is a new idea. Yeah, for sure. Okay. So it helps with our object behaviors. Um and then in behaviors if if there are two things you can represent sort of base things that can be observed like edges and colors and things that can be represented in a set of many columns those um we don't need any extra help. But if the behavior is something that's an object itself like a logo or a um a saxophone note then that requires a hierarchy. Yeah. But that's basically saying if we have if we need to model compositional objects or if we need to model compositional behaviors we need hierarchy. But we don't need hierarchy for behaviors themselves. I didn't say these are behaviors. I just said if I want to make a prediction based on a behavior, I might need a hier. If I want to predict a particular logo, yeah, if you have a if you have a compositional object, you need hierarchy. But if it's if you if you just want to predict a color, right, you don't need hierarchy. Right. Right. Um Right. I I wasn't try I'm trying to make a list here that sort of incorporates behavior. It wasn't specifically about behavior. just there's a okay in my mind so in my mind this idea that that the subobject idea is not stored is related to behaviors because we were we were struggling with like a saxophone versus color versus you know trying to represent a saxophone note with an SDR as Neil was suggesting we're we're bouncing around ideas about that and so I'm I'm I'm trying to just state what we concluded about that can I take maybe I'll take a different approach here um yeah in my mind that discussion was separate from behaviors. It was about compositional, right? But it's kind of play out. It was we got there by thinking about behaviors. So it's important to know for behaviors. I shouldn't wrote behavior models. I just should have just Yeah. things we learned. Um a child object ID just in general is only used for voting. Yeah. used for voting. That's a new idea. We didn't really have that before. Maybe we thought of it. I don't know. And that can happen uh independent between hierarchical levels and between actual different objects like the child object can vote on the parent object ID. Right. Also, I used to think like there is um you know there's a sound of a ceramic something hitting the table and I recognize that that could apply to lots of different objects. Yeah. So they're voting they're just voting, right? This sound could apply to these objects to this object different sounds and they vote together. So So is that okay to start with this one? Yeah. Okay. Um, I I wanted to state something we I'm going to state as a conclusion just to make sure that um I Neil's going to think otherwise, but I'm going to say um I wanted to say something about many columns. You know, many columns can only represent things that are easily enumerated and observed. Um do I need to say that? I don't know. Say what would you what would be your next point on here? Um only parent objects can make predictions about the child object ID through the backward connection. Okay. So right so this is the child object is only used for voting. Yeah. It's not a new thing. I guess it's just right. And then this is the uh back projection um can still is the only way to connect uh with certain um a parent and child relationship. um you know that this that this child is actually on this object. That's that's the only place that's actually res recorded. There's no forward thing like that. Okay. And then we're going to say um in uh behavioral sequences, how about this? in behavioral sequences. Um sometimes um the uh change is in a child object with its own um behavior model behavior yeah of its own um yeah behavioral model time based right yeah so basically you can compose object behaviors the same way you can compose object models like we would use the same mechanism to have a child behavior as we would use to have a child object Uh yeah, you're you're essentially well what you'd actually be doing your backward projection could be viewed as following. Uh I'm going to at this moment in time you observing an object in some behavioral state and I'm going to invoke that object in that behavioral state. Right? Imagine layer 6A there was the two grid cell locations back. Yeah. Yeah. And you're you're basically associating, you know, the current parent state with whatever object and whatever behavioral state of that object at that point in time. Yeah. So it could be like, oh, I'm going to it's supposed to be associated with at this point in the melody, it's a saxophone and therefore it starts like this and progresses like that. I'm at the beginning of the saxophone. Something like that.

Can you start a child sequence like midsequence? I was thinking about that and well imagine um the I have the logo and the logo is changing as I raise this the thing. So there's a sequence of changes to the logo. Maybe this is an animated logo. The lady, the Starbucks lady goes, "Okay." Um, so as you raise the thing, she goes, "Hi." And so if if I stop the stapler halfway, I would say I'm halfway through the sequence and she's halfway through her sequence. So if I if I had a chance to say I'm in this I'm here in the sequence in the stapler, it could be associated with both Starbucks logo and where she is in her sequence. something like that. And I can stop anywhere, even place I haven't stopped before. Somehow I'm going be able to figure this out. I'm figure out where her hand should be in that behavior. So there's a an example. I guess what would be required to be able to do that because you'd have to kind of replay part of the sequence. We we haven't yet actually solved the problem of how do we know what something's going to look like in the middle of a sequence if we never actually saw it. Yes. Right. If I'm right about the that during the sweep of the logo, a sweep of the stapler, there was no morphology model learning only behavioral model. How do we know how are we able to predict what it will look like if I did stop? Somehow we can do that. We haven't solved that problem yet. Yeah. Um in behavior sequence sometimes change uh in behavior sequence sometimes to ch the change or the change um the change is in the child of it own behavioral model. So this could be like saxophone um type thing. Um uh okay. Can we also talk about feature consistency and well that's maybe that's the um that's the part we haven't really completely addressed yet. Yeah. Um I would say like um you know we have a guess I don't think it's a complete answer yet because we haven't really dealt with it completely yet. I guess part of the solution is that um a child object replays replays um same behavioral um object sequent object plus uh you know same object behavior. I'm sorry. Um as just you know as recent you know this just as as as before as recently done without um any instruction otherwise. So, so that would imply that we do need hierarchy for a check like No, it doesn't. It just means like um all it means is that uh I assume I do the same thing if you could just use the hysteresus. It could be just saying like, okay, I heard a saxophone and and um and we're in the melody and he says I get back to him, you know, play play the same. Uh yeah, it's I I feel like now we get back to the same to the problem where now we're treating object ID differently than features, but we want to have the same mechanism to deal with constancy like the color of the stapler doesn't change, but also the logo on the stapler doesn't change. Yeah, I think the color is easy again. Um, yeah, in my mind this is like taking the behavior of the stapler, applying it to a new object and making sure that the new object features is consistent and it does not change. Foil again, fade again, taking a behavior, some behavior of like the stapler opening and closing the hinge behavior, applying it to a novel object that we haven't seen and making sure that the features of that new object is consistent over time. But how do we make sure of that? Well, I'm not I'm not suggesting a solution. I'm saying it's the same problem. Oh, yeah. That's the same problem of applying behavior to a new object. Yeah, I would agree with that. And we never said that it would require a hierarchy to solve it. Yeah, I just Yeah, I I would also say we shouldn't require hierarchy. It just seems now that we have the we don't have the child object ID in a mini column anymore. We can't use the same mechanism anymore. Well, the parent can't tell the child you're supposed to be hearing a saxophone. Right? It can't tell the child that. So the child itself has to say, "I'm assuming it's a saxophone until I'm told until I've learned otherwise." Right. Yeah. And what how about color? If we if we just have one hierarchy, I don't need hierarchy for color just like I don't need hierarchy for uh edges. There the constancy is in layer four features, right? Well, I think I think you could both of the parent child columns could work in the same way in terms of this. Somehow they could both assume constancy until told otherwise. Um and um let me just just talk out loud slowly so maybe I can get some ideas correct. I don't know. Um, imagine I'm a child object and I am now we're learning a song and I hear it being played with a saxophone and uh and halfway through the song saxophone changes to a obo whatever. Um, I would I be I'd be hearing saxophone saxophone saxophone saxophone then I hearing obo obo obo obo. I don't know about the sequence. I don't know about the the I haven't learned a sequence of those. Uh my sequences are saxophone sound saxophone sound saxophone sound oas sound o sound o sound uh the parent object is basically going through a melody and and and it says okay at this point you know we're associating these were saxophone notes and these are obo notes. So and that's the backwards projection. So, and when we play the sequence again in the future, the lower level doesn't know about the sequence, but it would say, "Oh, I'm supposed to play saxophone, saxophone, no sax, now I'm supposed to open." So, I've memorized that song that way. Um, okay. Um, and if I now hear the song with a dog barking, um, I the backwards projection wouldn't predict I should be hearing a dog, right? Doesn't predict a change. It just says you should you should hear you should start another note now. Yeah. And so my lower cal say, okay, what should I do? I, you know, one thing we could do is just say, oh yeah, I'll just keep barking. You know, it's like that's what I did. I'm just keep barking. There's no other instruction. Otherwise, wouldn't it tell the lower column that something will change? Would it? That's interesting because I can't predict the change. What's the equivalent of a change from a saxophone to an obo? From a dog to what? A cat. I mean, it doesn't Yeah, you wouldn't be able to predict what feature to see next, but you seems like the behavior model would say that there will be a change. Well, you know what? I might imagine if I imagine listening to this, right? I've learned this beautiful melody and it's, you know, Beethoven and something or other and it's got saxophone. He didn't know saxopones but anyway violin violin violin obo obo obo and it's alternating back and forth between and now I'm hearing the dog you know okay the dog is singing this song right that's not be but it's baby's melody at the point where the transition would occur from violin to obo I might say oh is it going to be an obo next right I mean that's the only thing I could predict I probably would I would think that I would literally think in my head oh is the next note going to gonna be an obo because you know that's what normally occurs here. I wouldn't say the next note's going to be the oboness of a cat of a dog. I mean I wouldn't predict the cat sound. I don't the only prediction I could have would be it's either you know it would be an obo and if I start hearing is if the dog continues barking okay the dog continues barking. So um I think that just shows that the the melody would predict an obo at that point even though I've been hearing dogs. Um, but at that point I'm not going to predict the no and somehow the constancy of dog barks is going to continue, right? I'm going to keep expecting a dog bark. This is carrying over the same behavior to the second part of where we change a feature. It's like it's it's going to continue to predict with that new feature using that same behavior. I don't see where the hierarchy comes in here. I'm just saying I I have to have a hierarchy because the thing I'm predicting is an SDR. It's a complex thing. I can't represent it in the single. I have no representation of saxophone or cat or dog or obo in in the in the parent object. So, I need a hierarchy to play out that subsequence. Um, that's why I need a hierarchy and I'm just walking through the consequences. If the rule was, okay, I've learned this melody using one instrument, I would now predict that instrument, but I didn't get that instrument. I got a dog barking, but that's good enough because I can still hear the, you know, I'm still recognizing it and yet and yet until I until I I'm expecting a change, I'm just going to keep going with dog. And when I expect a change, I'll think about what I learned before. Um, but I'm not that's that's all I can do. I can go from dog to obo, but I can't. Yeah. What I would say is the change from uh violin to obo you like while it's the first instrument you're not actually encoding anything in the behavior model because nothing is changing it's constantly that instrument every note no the notes are changing in the high level but the the behavior model does not include anything about the voice it's just the melody there is no the behavior model in the high in the parent column incl models the melody but in the child child doesn't know anything about the note. Yeah, exactly. It just knows that it's constantly the same instrument. No, it's not even that. It's not even that. It's saying I have a sequence. I've rec here's a sequence a pattern I recognize an object I recognize. I've I've now got that object. I And then um someone's telling me to to do it again. What am I supposed to do? I don't know. I I it's I can't it's either either it's going to tell me it's a saxophone or you know but I I I'm here. It's like I'm going to continue doing the thing I did because I I just lower level column that recognizes whether it's a violin or an obo. Couldn't that column learn a behavior model which is basically just changing from violin to obo? Just that change and that behavior model was involved. When in what cont when would it do that? What? Under what time frame? Would it be like this long period of time and then all of a sudden it changes? No, it would only encode the change. It wouldn't encode everything. When would that change occur? When the instrument changes, but but it doesn't it's got to if I want to learn a behavioral model like I'm I'm I'm listening to saxophone, saxophone, saxophone, saxophone. I don't know anything about melodies. Yeah. And now I hear violin violin violin. Okay. There's a transition. I When would I predict that transition to occur? I have no idea when to predict. It's not really Yeah, that's what the parent of Right. But I can't really learn that as a behavioral model because I don't know where it begins. I It's just randomly happened. It's like I don't have enough data to say at I can't predict when it would happen. It just happened. I all I know is that I don't know anything about the melody. All I know is I've been I've been hearing a saxophone saxophone sax. Now I hear dog dog. It's like okay. Couldn't the parent object tell the child object to expect that behavior? It can but it won't. You can tell it, but all it's telling is like right now you should be you should be doing this, but it can't. There's no knowledge about the timing of it or where you are in the sequence, anything like that. But the parent object knows where, right? But the child object doesn't know that. So the child object can't learn it. But the child object can learn the behavior of switching from saxophone to violin, which is just one time change. Okay. When does that occur? If I'm going to learn it as a behavior, I have to have a sequence of like I have to say starting now, this is when it occurs. Otherwise, it seems random. Yeah. So, it's a it's a very short behavior. At the time when when we keep hearing saxophone, we're not laying any points down in the behavior model because there are no changes happen. So, I so that behavior model of the child can't predict anything because it seems like I'm going along and I got saxophone, saxophone, saxophone, saxophone, saxophone, and then some random point in time it changes the elbow. Well, I can't predict that. Yeah, the child object can. But no, but when I when would I predict it? When would I say it's supposed to change? It's like random. It's like it's like I wouldn't the parent object be able to to invoke the that behavior in the child? Right. But it it's just it's telling the child you should be doing obo now, but the child itself doesn't have any knowledge about why. Yeah, it's fine. Right. Well, no, but so the child can't learn it as a behavior. You're saying the child's going to learn chance of acting. doesn't learn when that behavior happens in the melody. It just knows if it's not repeatable pattern under some circumstances. There's nothing to learn. It'd be like it'd be totally random from the from the child obvious point of view, it doesn't know what melody it's in. It doesn't know anything. So, it's just like, okay, this if if every day I somehow I knew a song began and three seconds into it I switched, I could learn that. But it doesn't know that. It's just these it's just this like p you know object object object new object new object new object when would I predict that change I can't it's not an actual model if I can't predict anything I think we need to predict it I think this is coming from what a model is about prediction if no so the model learns the behavior but then if midway through we started listening I started hearing violins so this is coming from the sensory input then I'm the model knows how to apply that behavior the uh the parents model the parent model knows it but the ch the child doesn't the parent here here's what the parent knows actually the parent doesn't know anything what's going on in the child other than the voting system but it's just saying I'm going through the sequence I actually don't know because we we've said we're not passing up the unique idea of that child so parents going I'm going through this melody blah blah blah blah blah and then the child is learn is associated hey um I just heard a saxophone and I'm associating with that parent HDR I heard another saxophone association with that parent STR. I heard another I heard dog. I'm associating with that parent STR. So when the parent goes through its melody again that it'll tell the child like saxophone saxophone saxophone dog but there isn't enough information on there for the child to learn that behavior. It will observe the transition but it has no idea when it occurred or why it occurred. It will seems to be random to it. It's just a random change. The child can apply that behavior to a new feature which is the violin. The change from the saxophone to violin, but it doesn't need to learn that there's a feature change that's happening here. Well, you could argue that it's that it's seen that transition, but it's not a very useful model if that transition occurs rarely and randomly. If every other note was a saxophone and a dog or something like that, then he could say, "Oh, yeah. I'm gonna go a b a b a b." Maybe that's the pattern I learn. Otherwise, it will just look some random in time rando change that I don't know why. I could learn it, but it's not I can't really make a prediction about it. I don't know when it's going to occur. The idea here is feature consistency, right? We want to keep the same behavior. Well, we're trying to solve not just Yeah, we're trying to solve a set of problems of consistency being one of them, right? It's like if the if you have a stapler and we open it and then at halfway it changes to pink, we are able to know that if if it changes to pink, I can predict that it's going to continue pink. We haven't done the constancy part yet, but but that's part of this. Yes, but that would be easy. I observe this the first time, right? I observe this and I see it goes from black to pink. The model learns that, right? Just like the it learns that. In fact, it could be learned in the parent. doesn't even mean a child. Um um and um there is no sequence of black to pink. It's just at this point is black and this point is pink. So that's that's that. Um so where are we going with that one? I'm not sure where we're going with that one. I'm just saying that the behavior can be applied to a different feature. It I'm talking about let's just talk about now. So now I now I see a green stapler. What should it change color to halfway? Do I expect it to change color halfway? No. Why not? Because we're applying a behavior abstracting that away from the features. I don't care what the feature is. If I see it green, I'm going to say stapler is black, black, pink, pink, pink. Now I see green, but I recognize it's the same sequence. Halfway through this, I might expect it to turn pink. I'm not expecting to turn any other color. I might think like, oh, it could, you know, I don't know. It could stay green. But if anything, I expect the only expectation I possibly could have it about a change. It's turning pink. This is like saying, you know, um the same thing I'm going through, it's dog, dog, dog, dog, dog, and now it's violin, violin, and violin. Right? If I know where I am in the sequence, I make a specific prediction, but that's coming from the parent. Um if if I'm just a child object, I'm hearing dog dog. I have no expectation that it's going to turn into violin right now. None. I have no how would I know? So is a is a transition between sex, phone, and violin. Is that in the behavior model? It's in the parent behavioral model. It's not even that. It's not even there. It's just that the the parent behavioral model has a set of states and I associate some with black and some with pink. Um well actually the color one we should separate which you know because I think color can be handled in the single. Color is not a child object. So there are things that can be handled in the parent and there's anytime I have a child component that requires an ID it has to be handled in the child you know uh if it's something I can handle in the parent like edges and color I don't have to do that I can do it all up above um however if the if it was a logo on the stapler that changes color or or change somehow then require a child object to do that right because there's it's it's not just color. It's It's an object. I think this all works, but I don't think you're convinced. I don't think you're convinced. Well, I'm okay. Okay. I thought you were pushing back on this. Yeah. I still don't we still I mean the hypothesis right now which is a little weird is that um if I'm a column and I and I'm observed the behavior and um uh and now um it's like some location in some parent object I don't know and now um I'm being told from the parent object u do a behavior again but I have nothing new to learn from I haven't learned associate that that new location of the parent object. I'm just going to play the old one again. If I haven't learned a transition and you're saying, "Okay, I you know, I got like a dog bark at some location, dog bark at some location, you know, a dog bark at some location, dog bark some location, dog bark some location, and now it says do another here's another location, you know, at this time. I'm going to do another dog bark. Even if I never learned that, just going to say, okay, without further instruction, I'll do the same thing." So, it's like a hysteresis. I mean, I don't like this, but that's that's what this implies. And yet I could have learned if I've been through the song before, I could have learned that uh on a unique SDR here or any any of these SDRs as I'm going through the through the melody that I can associate the dog bark with that location so the dog or associate this location with a violin. Then I would predict violin because I've learned that. But until I learn I have to just assume it's going to be the same thing. Yes. So that's a little weird. I don't really like that. Um um but that's right. That's what it that's what it is. Somehow we want to transition back to the stapler where the stapler top is like the dog bark. Okay. And if I don't know any other if I haven't learned that the stapler top changes somehow, I'm just going to assume that it's going to be the same staple top in some new orientation. Yes. Whereas before I was learning it's a dog bark. I assume that some by the way we have to it's not just a dog bark it's a dog bark at a particular note so we didn't talk about that but so it's not just the staple top it's staple at top but a different orient specific orientation so those are parallels there that's a clue yes um I was wondering if could the notes be thought of as orientation I don't know what that means um almost seems like it I know but I don't know what it means like it's fuzzy right you're like what does that mean you can Do you have a clear picture of it? I don't know. You don't get back to the first note if you go full circle, but I don't even know what it means. Orientation for notes. I don't know. Um Yeah. Okay. I don't know. All right. So, so, so we summarize here the the melody or the the parent object actually doesn't other than the voting which we can ignore for now. It doesn't know anything about what's actually comprising if there if there are actual individual feature ids as part of this object. It doesn't know them. But it can associate its location with a particular ID at a particular note. Um and um the child object doesn't know anything about the behavior exhibited here unless it's it it somehow surfaces this very repeatable down here, right? Which could be Um but the general rule would be no. Um so but it could be sometimes it could learn sometimes it could say yeah I'm only looking as a child object and uh as a child object I can see this behavior will change I can learn this this how this thing is changing over time. Other times I'm arguing like like changing from cat to a dog voice to a saxophone is not going to learn that. Um um okay. And now since the analogy is between um the dog bark is like stapler top something like that. The stapler somehow the yeah I don't know something like that. So for the color constancy you would use the same mechanism I guess just I think color constantly can be handled in the in the parent column. I don't think I need anything else. Oh yeah, I I I would definitely argue you don't need hierarchy for it. Just like it would be the same thing that we would if there's no change in the behavior model, we would expect the same feature to be invoked in in layer 4. Well, yeah. And I don't know how that I don't know why, but you can just imagine it, I guess. Um and the same location in layer six. Um I don't know. I could all I could do is say the colors uh I sim I wouldn't really learn anything until it changes. I wouldn't learn why would I learn anything new? I don't want to learn that it's still black black as I'm moving from space. Oh, you wouldn't learn it. You would just expect it. Right. Right. Even though the location is changing, the color isn't changing. Yeah. So, one could argue I could learn the color of the new location, the color of the new location, the color of the new location. Or one could argue like, oh, well, for all those locations, I'm just going to be black. Yeah, I think that's what you'd have to do because you can see a stapler in a new color that you've never seen before and you can predict it to stay the same color even though you never learned that association of the right new color, right? patients. Um, so you just kind of have this u like um prior like what do you call it implicit knowledge that it stays the same if there's no change in code. It's implicit in the sense that nobody knows this. It just says until I hear a change that it'll be the same. You could think of it like the following. Imagine you've got these two um got the two models. This is like four and three and I have this these shared mini columns. So the mini columns have the same definition uh on the behavioral model and the and the um the object model and um and it's sort of like if I don't have any change up here I don't force a new change here I'm just going to keep the same mini complex like it's the same color it's the same edge I don't know it's the same feature the feature hasn't changed but if I get if I get a change up here if if the the actual encoding of features changes then it forces these guys into the new set down here. So this is the this is the um the you know this layer has temporal persist temporal persistence. So how how would it know which new columns to invoke in layer 4 given a change? Because when a change occurs it it's those it's like it's changing color that the color appeared the edge appeared something appeared here that wasn't there before that entire column. Yeah. Okay. Okay. Yeah sense I think right for and then for locations. Well all right so now here's an interesting question. So basically I'm saying okay um nothing has changed keep constancy here soon as something changes here force force this to be new my location can be changing right now right the location not in this space but the location in this space um well I guess the location in this space could be changed at any time so I know it's some future time point so it's changing there too but here I don't have time so basically um Um, if I'm just if my if my center patch is sitting in well, if my center patch is moving around and nothing has changed up here, so like I'm I'm moving my eye across the stapler or something like that and then nothing's changed up here, then I'm going to I'm going to assume that all those locations that I'm looking at are going to have the same pattern here. Whether I learn associate this pattern with those all those different locations is an interesting question. Um I might take more synapses. Yeah, that's a that's a nice nice mechanism for the color constancy that yeah, you can have the stapler moving but still since there's no color change encoded, you still you just keep the same minicom active in layer 4 for the color. I guess the remaining question is if there's a movement change detected in layer three, how is that getting translated to expected location change in layer six? Um, like if you Well, no, I think it's a separate problem. That one's not a problem. If I'm observing the location, if I'm observing the change, if I'm looking at the location where the change occurred, then uh by definition, I know where the change occurred. I'm looking if I'm The problem here is if what if I'm not looking at the location where the change occurred. Yeah. Then I have no idea that anything's supposed to change elsewhere. Um Yeah. So, you have a model of a stapler opening. How do you know that if you sake to a location on the staple new location of the staple the top that it will be there right and not somewhere else that's the problem that's the occlusion problem like how do I know if if I if I'm not tracking it continuously there was there was two there was two very related problems here one I expressed as an occlusion like even if I can see the whole thing it gets oluded I can't really see what I can't what's supposed to come out the other end the other problem is if I'm a single column I can only observe one point in time and I can observe the change at that point, but the other changes that are occurring, even if they're in the model, I don't observe them. I just So, so imagine this. Imagine your behavior is this is this um space time slices, right? Each of these is is a is a two-dimensional sheet, right? So, I'm moving through time and I have two dimensional space.

And on these on these surfaces here are where changes occur at that point in time. And as a sensor patch, I can only move through here at one point. I can move here. I can move up to here. I can move down to here and so on. But I can only observe that one little point. So I can observe if I happen to be there when the change occurs. Oh, I know the change occurs. But what if a change occurred elsewhere? I'm not observing. And um and now I go back here. Change occurred here and I'm not observing it at that point. The change occurred at at this point in time right here. I'm not looking at that. Um the model tells me it changed, but now I go up to that point. I need to be able to predict what's there even though I wasn't um it's like it's let's say it said this this chain is green appeared. Okay, at this point green appears at this point here. I don't say that anymore because green is already there. It's not a change anymore. It appeared. I come up to that point now. I need to know it's green even though I didn't I wasn't there when it changed. How do I know that it's green there? This is an expression of this problem in the general sense. Um how do I know that?

is not with color I feel like it works because um if we are this is assuming we've already learned the behavior model right so yes um so the the time sequence that the time signal that we're getting in is already invoking the changes that we expect in the behavior model but I'm not observing this so how do I know how to change if I'm I'm a patch a sensory patch match. So, okay. So, the sequence of changes invoked in layer three like basically we've learned the behavior. Now, we want to replay the behavior. Um, it it would only replay the locations that we're look looking at at any point. The behavioral state of the column will go through all of its it'll go through the entire behavioral state over time. Yeah. But I can't observe all the spaces in that. I can observe every point in time, but I can't observe every part of the of the object at that any point in time. I can only observe one part of the object. I'm only sensing one part, right? But can can the column represent the column can only represent at any point in time. It only represents one point where the sensor patch is. I can make predictions about where I'm looking at that location. I I I unless I invoke another location, I can't I won't know what is stored at that location. There's no way. It doesn't the neurons don't become active until I'm at that location in time in space and time. So even though it's in the model in the connections, there's nothing dynamic happening here. It's in the connections saying if I was at that location, I would see this change. But if I'm not at the location, I don't see anything. I mean, I see what's down here. I don't see what's up there. Even though the model, if I was looking there and the model said, "Oh, yeah, there's supposed to be a change here, but I'm not there. I'm down here." And I wouldn't, how would I It's like I have to car It's like somehow I get to this point, I have to go back in time to find out where the last change was, you know? It's like how else would I know? This would require all the columns to be voting on something like a global state ID. But then we we can't do that. uh all the columns you or so these patches they need to be agreeing on the the whole state of the object at the same time because right now I'm just talking about one patch right right right right now one patch has this problem like I'm looking uh imagine I'm I'm looking at something and I've learned the behavior of this object. This I have the the entire behavioral model. And now I'm watching the stapler arm go up, but I'm looking down at the bottom towards the bottom of the stapler. And and while I'm doing that, the behavioral model says, "Oh, the top should turn green." Not the part I'm looking at, but the top up here turn green. But I'm not attending to that. I'm looking here and I'm not seeing that. And the part of the model that tells it turns green is not even invoked at this moment. So there's no dynamic representation of that. It's just in the model, but the model is not invoked. So now I zoom up. Now I I'm looking down here and now I go up to here. I should see green and I should know I should see green even though I wasn't attending to where the change occurred. So um could we invoke like we have the many columns and maybe one of them represents the change to green. Yeah. Um green just green appears. Let's say green appears, right? Yeah. Green appears is what this column represents. And we have this temporal sequence coming in. Yeah. Um and it maybe at first it predicts that red appears at some point in time at the same location or different location. All right. So we have the reference frame here and then this temporal sequence would predict red appears up here at some point in the sequence and then at a later point in the sequence it would predict that green appears at this location like a traffic light. Right. Right. Um so but we're not attending to the this location when green actually appears. Let's say we're still looking at this light. We just saw it turn off. We didn't actually see this light turn on. But still, wouldn't this sequence here have still kind of invoked uh this even though it's invoked in the sense that at least deolarized or no because I I have to pair it with the location like I can't imagine green appears here and orange appears here and purple appears here all at the same time. Yeah. Right. it it it get that information is in the model but it doesn't get invoked until I go to that location then I would say oh I'm at the location it's supposed to turn green oh I'm at those place right now supposed to turn purple supposed to orange but if I'm not there but could this signal like this temporal signal somehow say oh I am expecting a change at this location even though currently my sensor is not at that location in the reference frame could it still kind of Okay. Well, it can invoke a union, I suppose. I mean, remember there could be lots of changes like I just suggested here. It's not just one change. Bunch of changes could happen at the same time. So, for that to occur, I said at this point in time, I'm going to invoke the polarized the union of of um uh changes. So, I I'm predicting it changing purple. I'm predicting it changing green. I'm predicting it changing orange. But until I go to the specific location, I wouldn't know which one it is. Um, it would be it's just sort of saying all these things are happening, but I can't invoke this specific have to be union because I can't invoke them individually. Yeah, I guess. Um, so is there another column that's looking at that change at the same time or are we assuming only only if you said there's another column? At one point last week, I said, "Oh, this has got me thinking about lots of columns working together." And the reason I got there is because I was trying to imagine just learning the behavior of the stapler. And I'm saying, "Oh, it's going to be really hard for a single column to do that, right? Because it can't get to all those locations." You have the same problem here. A single column can't make predictions very well because it can't get to all the locations. If I had a whole bunch of columns that all had the same model and they're all looking at different parts of the stapler individually those um columns somebody would observe every change, right? They'd say, "Oh yeah, I'm expecting this to turn purple over here. I'm expecting green over here." And they'd all invoke that. Um but that still doesn't solve our problem because if the column if the column that was up here and said, "Oh, it turned green." Another column said it turned orange when the green columns go down here. How would it know? It's supposed to be orange. It doesn't know anymore. Then this unless this guy doesn't How would I know that? This it's almost like it's almost like you have to have some sort of uh ability to like go back in time to find the last change and then bring it up forward or something like that. You know, I I don't know how I just I don't know. I don't know the answer to this question, but that that would be that would be one possible answer. Um, like I get to some point and I'm looking at some point and I don't know what to expect. So I can just say let's go back in time until they see the last thing that changed. Last thing that changes this became green. Okay, it must be green because that's the last thing I saw. But it kind of seems like this sequence signal must make some prediction, must activate something here because often we predict a change and we kind of anticipate to look there before the change even happens or well or we can directly move to that change that we expected in the behavioral sequence. So imagine if I had only one change in my in my my slice of time and we're predicting that this part of this model is going to change. Well, I I would have information. I could in theory say, "Okay, direct your attention to that one location." Yeah. And you should see it turn green. I do that with the traffic light. Yeah. You know, I'm okay. I'm looking I'm anticipating it's going to turn green right here. I'm waiting for it. Right. Um, but if I have lots of changes occurring at once, like the edge of the stapler, you can't attend to all of them, but you could still kind of make predictions about But I can't make predictions about the ones I didn't attend to because I didn't know how they're changing. Um, and yeah, if you've learned a model of the state, but I still the model doesn't tell me the model tells me when a change occurred, where it occurred, but it doesn't tell me that it's consistently there. Right? That's the problem we're trying to happen. I'm looking at some future time after the change occurred. How do I know? And it's already happened. It'd be like I'm looking for the It's like I'm I'm looking for a straw and I'm looking at the green light on the stoplight and I'm waiting for it to turn green, right? How do I know that when I get up to the top, the red is no longer there? I do know it. It's part of the behavior model, but how do I know it? What? How the how am I? There's another column that's looking at that part that you were looking at before. Well, okay, we can try to use multiple columns to do this. I just gave an example where I didn't have multiple columns. I just I was looking through a straw. Yeah, but that column has learned the entire behavior of the traffic light. But when it goes up there, there's no change there. The change occurred earlier in time. Yeah. So, so how does it know? How does it know that earlier? Why couldn't the learned sequence in the behavior model make a prediction like a make a prediction of how it's going to change at a certain location even even though the sensor is not at that location right now? But I can't make these predictions at all these locations at once through time and keep track of them, right? How would I do that? Like I seems like it must be happening like it is. I I what I'll say is we are able to make the prediction that the red light's no longer red. I don't have a proposal how that works yet that I think works. That makes sense. I can't I can't keep track of all the changes that occurred in this thing. If if it's this the problem may come about because I've said that the behavioral model is only based on like the appearance of a feature, right? And so it goes away. Yeah. If I said it turned red and now it goes away. If somehow I said it turns red and it stays red, well then I can go back there and I know it's red. But that doesn't seem like it's a behavior model. That seems like a static thing. And even then it's like um the behavior model has all these possible changes that are occurring at once at different locations. And um Well, let me let me just let me just propose an alternate. I just yesterday suggested that we think of the behavioral model as this thing, a feature appeared and it's only representing change. Um, what if the behavioral model um didn't work that way?

Um it's like as I'm going through time here um these are these are spaces at different time slices and so in that space I could say um I mark the beginning of a this turn green and what if I kept it persistent saying it's it it didn't turn green but it is green didn't turn green but it is green you know somehow I could just persist that don't we get get that from the the static model that whatever is not encoded in the behavior model persists um yeah but the problem is I it's if I'm not attending to that location nobody knows it happened the behavior the the The object model doesn't know. I didn't I wasn't looking at a location. How can I change anything? It's like it's a part of the model that's hidden. All kinds of stuff could be happening in throughout time and I'm not observing it. Tons and tons of stuff in my model could reflect things that could happen and I'm not observing it. Yet it seems like when I do observe it at some point in time I know what it is. Um, imagine your house. Imagine you have a house with six rooms in it and every hour someone rearranges the future the furniture in those rooms in a different way. one o'clock it looks like one at two o'clock look like the other three o'clock like this there's a your your house is got different parts of it that are changing and um they change over time and I'm in room A I I can see things that are changing in front of me let's say but I can't see any of those other things changing and and there's until I think about those other rooms there's no invocation of those other rooms there's no instant iation of those rooms. But it's more like what I if okay it's 3:00. Um I'm thinking about the room now. I'm going imagining being there or I go there and at 3:00 I expect it to be in some configuration even though maybe it was changed at two. Um it's sort of like saying yeah it was changed at two but I know it hasn't changed again since then. So um it should be in the configuration that I got at 2:00. It's it's sort of like saying I'm going over here. The change occurred back here. Somehow I have to propagate it forward knowing that nothing has happened since 2:00. Therefore, I I now can imagine this. But I didn't actually observe it that room at all. So nothing. It was only when I get to the room or think about the room at 3 some other future time then I realize oh what had happened up to that point. It's like I'm oh yeah did anything should have anything occurred where what should the state of it be? It's 2:30. Last time was changes two. there should be in this state. Um, could it be like the change is actually occurring at two3 at two? Could it be like a mental sim like a fast mental simulation of going through that sequence and paying attention to where you want to could be it could be like as much as saying uh but it feels more like go back to the last time I know a change occurred. Mentally simulate the change. I don't go further back. I don't say what happened at noon or 11 a.m. or something like that. I just go back to the point where the last change occurred and say, "Okay, when was the last change occur?" Oh, it changed to green. Okay, it's got to be green. Um, yeah, I could do that. It's uh it's weird. It would be like like you go to some point and then you have to go traverse back through time backwards, but you could do it very fast, much faster than the behavior happens. All right. I'm just trying to imagine a neural mechanism for doing that. Like back up. you basically replay the sequ behavioral sequence from that comes from layer one uh faster or same speed. Um and you would mentally pay attention to a certain location in that object's reference frame. Well, you are you just by the fact you're looking at it, you're observing it is always and it's a little weird. It's like saying, "Okay, let's go. Um, what's let's imagine what the staple the staple goes and doesn't stop in between." And now I'm saying, "Oh, it's going to stop in between. What should it look like?" So, I have to I can imagine, you know, at any point in time, I reversed in time until the last the last change. Well, the last change was really nearby. So, that wouldn't be hard. Um, it'd be more like, okay, halfway through the halfway through the movement of the staple, the logo changes from Starbucks to Pets. And now I'm three now I'm I I'm not observing that, but threequarters way through the movement, I say, what should I observing lo, you know, Starbucks or pets? And I'm not observing the point where it changed. So I have to say, okay, in that location, let's go back in time to the last time it changed. What did it change to? Now I'm going to predict that. H that's a bit hairy. Yeah. Uh you know related to this is the fact when you look at grid cells um they're always like playing sequences. Do you know this during the during the theta cycle they're like on a at a point in in like the grid cells represent a point in space but they go actually on every cycle of the theta rhythm they actually go through a sequence of points in space. It's actually it's like it actually goes from further away to where you are to further back. It's like it's repro it's like playing the trajectory of your of your movement over some short distance. And so it would actually in theory it would invoke experiences that you saw or expect to see very rapidly every theta cycle. You know you're not aware of it. Yeah. Um, and I have some speculations why it's doing this, but so in some sense you have a little bit of window which you could go back in time and say, well, oh, back in time, what would this thing occur, right? But I'm I may need to go very far back in time, right? I need to be able to say, oh, 2:30, what was the thing at, you know, 2:30, what happened in this room? Um, I'm just saying there's a mechanism of going back in time that exists already, but it's usually over very short periods of time. Yeah. I mean, it seems like if you have to do it over long periods of time, you you can't do it at at a very high resolution, like you wouldn't be able to like mentally simulate. It's complex longterm behavior. Well, I don't know. I gave the the rooms the example the the room, but it's just like one change per hour. So, you can speed that up very fast and go through it very fast. But what if there were like people dancing in the room and Right. Right. and they were doing a jig to a very rigid time frame. I know that jig, but now I'm I'm exit the room, I come back and and even if I know the time, it' be very difficult to predict Yeah. where they are because there's been so many changes. Yeah. And I haven't learned the intermediate states. I have to go back to some I don't know. Yeah. Yeah. That's that's possible. Um yeah. Um you know uh there's this research about rodents uh when they're sleeping that the hippocample plays hippocampus plays back uh things that happened in their day like plays them rapidly. Yeah. But things over a longer period of time. So it's clear that the neurons at least during sleep are able to say, "Oh, let's go back and play it over again." Yeah.

 By the way, in the case of the stapler, imagine I'm trying to predict um what I'm going to see. Um, and the idea of maybe the logo changing halfway through means I might have to go back a fair amount of time to see when the logo changed. If I'm trying to predict where the edge of the logo is, well, I only have to go back a teeny bit of time because the last change was just a moment before in a different location. But there's um well, if I'm looking at this thing here at some point and I say, when's the last change occurred? I'm going have to go back it changed the edge appeared here. uh or well the color would have to go all the way back in time like or or I don't know uh right it's a wonky it's a wonky mechan yeah that yeah if you think through it seems like that's kind of what you're doing um if I if if I think about the state changing the logo on it while it's oluded. I would kind of mentally simulate the logo changing while it's behind uh whatever it's including. Well, let's look at the lid of the staple opening and now it stops and I have my um I'm looking at some part of it and I Okay, what should I be seeing here right now? I I have to like go back in time to like when things last changed. But now if I go, it stopped. And now I go to another location. Do I have to go back again? Another uh or maybe not because maybe we're going to solve the top of the stapler. Yeah, you have the top of the stapler rotation and that gives you all of the predictions. Right. Right. So we're really just predicting like morphology or what if there was another behavior that's changing to some like some part of the what if there's like another behavior changing to some part of the to a stapler? I wouldn't be able to learn that. Like I would have to split that again. Whatever. Did you learn the stapler? I learned the stapler and it's moving this way. But we're saying in that case we're going to the solution there. We're going to break out the whole top of the stapler sub component. And so there could be another change happening here. I'm not sure. Right. But then I'd have multiple sub components like the door the door the top of the stapler. Um in all cases I still have to back up a bit in time. I don't know. I I have to back up in time to predict what's going to happen. On a very abstract level, one could argue you have to back up in time because um the behavior is complex. If the behavior is complex, then there is no other way of knowing the current state unless you play through the complex sequence of behaviors. It's like, you know, it's a logo danced around a little square that goes, you know, and I now I'm going to look at the glogo. Where is it? It's almost I have to like go back in time and say, "Oh, yeah, okay. It should be here." As opposed to just like, you know, I don't know.

It's weird. It's easier with multiple columns. Well, we can talk about multiple columns. It's easy, but it's also got its own set of issues which I'd like to get to. If we want to attack that next after our break or whatever we're going to do next, you want to talk about multiple columns. That's going to be really hairy. Um Um Yeah. When do you want to go for lunch at some point? Our board meeting at one. Yeah. What time is it now? 12. Oh, maybe we we don't have time to continue. Yeah. We should get lunch something at some point. Okay. And then we'll have to leave the multicolumn discussion too. Yeah, maybe it's a good stopping point now. I don't know before we get to too too much. Then where we left it is we got this weird idea that to make a prediction with the behavior model only detecting when things change when new things appear or disappear that to make a correct prediction you have to go back in for a particular location in time at a particular location of the object you might have to you'll have to go back in time to the last time it changed and move forward to see what it is. Well, what to last time it changed tells you what it is because that's it changed to green then you know it's green. How that would work I have no idea but we do know that there are mechanism for grid cells that go back in time. One other random thought I had earlier is evolution narrowly it seems like the behavior model is is actually a lot more important than the morphology model. If you imagine animals that have a motion based vision system, it almost seems like they only have a behavior model. And that's enough for most things that you need to do to just see like just recognizing behaviors. I one could argue I agree with you, but I'm not sure it's more important, but it's definitely right up there. And many things you need to know are just behaviors. Yeah. It doesn't really matter. Um the current morphology, right? I agree with you. I I wouldn't say morphology doesn't matter. No, yeah, it obviously matters because otherwise we wouldn't be able to model it. Be nice to see that there's a tiger there even if it's not moving. Yeah. And especially like things where color matters, for example, it's important. But if you're just like a dinosaur catching flies or whatever, you just need to recognize that movement and go where that movement is happening. Well, frogs, we know like the there's that um there's a famous paper, what does the frog's eye tell the frog's brain? Have you read that? No. It's basically argument that the frog's eye detects motion that are likely flies. It doesn't say anything else. It just says there's something moving in in this trajectory and it's probably something you want to eat. Yeah. So it just has a behavioral model of the frog doesn't see it. It Yeah. So it it in that case it only needs the behavioral model, right? And it doesn't actually need the morphology model in order to catch flies just like fine-tune to the the behavior. So like the trajectories of the fly. It's very it's a very very simple. I mean, frogs don't have cortex, but you know, it's the same idea. It's like a early Barlo paper, that one I just mentioned. I don't know who the author was. You can look up. That's literally the title. What the frog's eye tell the frog's brain. I'm pretty sure I thought that you know the paper and is that you think that was some of the early barlo work on the the flight like basically fly detector stuff in the frog but I'm not sure if you find it oh it goes back is like the super super old guy mccull and pits you remember so it would have been around the same era 41 says I don't know the details of the paper but that way the summary uh significations in everything else can cause little change of response. Um it's not light intensity but rather the local pattern and variation of intensity as exciting factor. Um I thought it was about flies. Um they don't mention that there. Maybe I'm confounding two papers. There's this one and then maybe there's another one that talks about flies. I thought that I thought this was the fly one, but there is a fly one. This isn't it.

No, it doesn't sound like it's only a motion based system. It Yes, it doesn't. It doesn't do that here. Although there is another paper that does. So, I don't know. Yeah, I'm confusing. Um maybe I'm not sure which animals actually have motion based vision. I don't know. So our paper is based on the fly that says it detects the flies and that's all it cared about. Um I thought that was a paper but a parent. Um okay. Uh anyway that was your observation. I would agree with it. Behaviors are very very important to recognize them. Um, and even recognizable like if you're seeing an animal in a modeled image of a uh of a trees and and leaves, you know, you want to see the if the animal moves, you want to see it the the behavior of the animal pop out, right? Because it's moving and the behavior model recognizes as a as a particular animal. If the animal's not moving, you don't see it. Um, so it's really important. Okay, we can stop here. 