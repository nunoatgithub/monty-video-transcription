so I started off with a very simple experiment, so please pay attention to the screen.

So what did you notice right?

It looks identical. Is it? Yeah, it looks identical and it obviously moved, but these are the, two images. So you see the quite difference in size and, I guess if a whistle would come into at night, go to a cupboard and suddenly, magically reduce the, sizes of all cups for half an inch, you wouldn't even notice the next step.

I think I would, that I would notice because you have this, physical memory of what your hands feel like, right? this is not because you see objects at different scales all the time, so neither one of these is correct scale, right? one is just one bigger than the other. And, so now it's one sort change. Yeah. Kind of change, change quite, but, what I, the point I wanna make here is on one hand. It's very obvious to compare the two, and we can clearly represent the dimensions of those in comparing the two. On the other hand, we can recognize it independent of those specific.

So, how, does it come about? So that, that's what I wanna address. Those two different things, and I shared this last time. We have, in terms of recognizing the cup, we have quite some flexibility to recognize the distortion and it's shared tilted in a certain way, but there's also limits. The last one, the, what is the, upper ring on the cup looks more like a, face.

A fingernail.

Need a manicure. Yeah. So suddenly it looks So the recognition, even the recognition is different.

so that's so our, the point I wanna make here, our recognition is flexible to some extent, but that there's limits. Yeah. So take into the relative location. These, this is different than the scale component, right? Don't you think that, the scale difference is really quite qualitatively different than this. Yeah.

Distortion.

So, we, seem to have two kinds of capabilities. We recognize the objects and variants to position post scale information. The other hand recognize the specific position post.

Guess I'm gonna take, I'm gonna object to the, scale thing in the second one again because I'm not, it's not clear to me. We actually do recognize scale. clearly I can know where it is relative to my body. I know the pose relative my body. I can know if it's deformed. The example you gave of the two cup scale? I didn't see 'em as different. that's just because I don't, I think scale is like an uber factor that's applied on that, When, especially when it comes to vision, because we see things at different angles, at different, distances. It's not, like I recognize the scale. It's like I recognize independent of the scale. So I think with touch, I would definitely say yes. I recognize the object scale. Like it gave me a small version of this cup. I would feel it and I would know it. But with the vision, I'm, not, I'm challenging. I don't think I do recognize it. I think scale is fine. I, feel like there's two different uses of scale here. This one is, the object itself smaller or larger? And the second is, the image on our retina smaller or larger? Those are two very different things. yeah. 'cause if we're doing completely in 3D, the, size on the retina doesn't make a difference. we're inva. But if it's physically smaller, physically larger, we do know like a, tiny version of this, I would immediately know it's, smaller. But if it was further away, I would still think so if there was context, if I see this cup in the context of other things around it, I can tell if it's smaller or larger. But if you just show me the cup independent of context. Agree. It depends on the context and get, depends what you mean by scale here in a sense that, yeah.

Compared to the same kind of cup and a smaller scale extension. Yeah. a smaller cup. Yeah. Yeah. But again, I don't think I can tell the difference between those two if I don't have context, Context. Because without context, I don't know if the far away cup or closed cup or two, a couple of smaller, bigger, meaningless, like we've shown pictures of the coffee cup on the screen a thousand times. It's never the size of coffee cup.

we did look at those two pictures and go, wow, that's really big, Yeah. I think context is important. again, I'm just, I'm gonna put, I wanna say that I don't think scale, I think scale is a different problem than position and deformation. Okay. Yeah.

two weeks ago, a percentage of the exercise with trying to reproduce, three objects from memory without looking at it. and here I tried something similar. But instead of just reproducing the whole object, I, placed a smaller object on the other object, and I found it, it seems fairly easy to reproduce the position on the object.

So you simply have a good sense to position of one object on the other.

And then even though I didn't get the whole dimensions right, or the keyboard, the perspective is again, wrong. And, those, the, box with the elk on the bottom is a bit distorted, but, the location of the object is fairly accurate.

That's one observation. So then, so with this comes to my, proposal. So, there's two pathways, and on one hand we represent. Object in a reference frame on, on a of another object that is supporting object. For example, the cup on, a book, on, on a table. We, have a representation within the coordinate system of, the supporting structure where's the cup located, where are features on the cup, for example, shapes where they're located in the reference frame of the supporting structure. That's one thing. And, the second path would be we have this in variant representation that's like in, in a local reference level of the object itself this time, but where we don't take exact locations into account. It's more like I have this curvature feature here and somewhere relative. To each other, another feature, and it represented in a way that is inva to the rotation. But why do you say those are different? I'm confused by that. It seems to me, the, logo on the cup is just another example of an object on object. Yes. And, and so cop on the book, how, why are these, why do you say these are fundamentally different? It seemed to me, it's a very similar process, but what's different about it?

D one, the one there, essentially two differences. One is we use a different reference frame for the first one, but is from the supporting. What is different about the first one? Why do you, why are you making that argument? What, why do you say there are two different, I look at this and say, okay, I can take any object and show it's positioned, altered to another object. in some cases, that's a permanent thing, like the logo on the cup. Some cases, you might think it's a temporary thing, like the cup on the book, but if that's the way my book came and cup on the surface, then I'd say that's the object. So I don't, I'm not sure I'm understanding the fundamental difference between these two and not, I don't In terms of, don't say in terms of what the reference frame is, like what structurally what's different about it, Yeah. Apart from the reference frame, is the position, right? The first we, decode the position of the object in reference frame. But isn't that what the, isn't that what the, logo is doing on the cup too? Isn't the, Yeah, but, that's the same, the logo on the cup is the same case as the platform. Okay. So one is different about sample here. What is different is Path two the morphology argument, I imagine. What is Path two? Path two is, this, is, the argument related to discussed before about the displacement and the displacement relative to the, local post? Is completely in, almost completely variant to the rotation. And But couldn't you make the same argument on the left? I could just rotate the whole thing. And to me, I felt there's always two, there's two things that I didn't strike with. One is you've got things which you can't say object A and object B, which is like the circle or the curvature, the morphology of an object where there's no points where I can say there's a reference frame relative to a reference frame. And then, there's this idea that maybe there's this, maybe there are other times when you can say there's specifically this thing is located at this location for that thing. So I've, I'm certain with this morphology versus features displacement, but they're, I'm trying to unite them. here, I'm not sure if you're saying the same, like the same thing if it's like you're talking about local shapes, that's like the morphology argument. I just wanna make sure if that's the same as what the morphology argument. Everything is about morphology. the local descriptors are like local, let's say local shape descriptors. But here it's represent in a way that, that this not a specific correlate. This is just so, this feature is located relative to this feature and just, to the, let's say, yeah, to the, no northwest of it, northeast of it. But isn't that similar to the copper on the box? No. In this case, we would've a precise, location representation. So there's a representation in this coordinate system and this, XY coordinate or maybe XY coordinate. There's this.

We, don't represent in this content system. Where is the handle? In the book coordinate system? In the book system, whereas in this case, yeah, I guess I don't know. I guess I'm gonna keep pushing back. I, the way I think about how I recognize this top on the book, I don't recognize the features of the cup relative to the features of the book. I, can say, here's this cup. I already know it. Here's a book. I already know it. And very rapidly I can associate the book in the cup together. And I don't do that by associating all the features of the cup relative to the book. that's the whole point of the compositional object is I don't need to study all the components of the cut relative to the A book's reference frame. At least I don't have to do that to learn it. I can just do it in an instant. I can just, in one sense, I can say, oh, there's the cut relative to. Maybe I can calculate those positions, as you shown my words a handle to the book. But I don't think I memorized that. I don't think I would even have the opportunity to do that.

though yeah, I, surely wouldn't do it in a way that's very detailed. But it would be very representation. But why would I even, why would I even do it at all? it just seems like I can, I don't even have to, I don't even have to see all the parts of the cup. I just, it somehow I'm able to say if the cup is relative to the book, then all the, then, I, that's, it's, that's it. It's very rapid. I don't have to look at the different parts and calculate the differences between, anyway, if you store it this way, then it seems like you just have this compounding problem of, then you get this, explosion of things. You have to memorize the relative items relative to other items at all points. It can get very complicated. it seems like we just, we do this very rapidly. We just said, okay, there's this existing object, like the Dement logo. I don't have to specify all the components, memorize all the components of the logo relative to the reference name of the cup. It just seems unable to do the whole thing at once. I'm saying take the, mental logo and apply it to the cup. And, I see the advantage this way. so for example, this way, this representation allows us to quickly recognize the object independent of the post. and this would allow us to represent what is actually the post of this thing in the world that, for example, the, handle and, I just need a few key features, right? Maybe the bottom of the cup is a dislocation and the, and this piece of the handle is a dislocation. And, through this pathway I recognized, oh, this is a cup and this representation will tell us. Okay. There's something like a handle feature here and there's something like a button of the cover. This is still, I'm still computing. I think we just have digital languages, didn things, on the one hand we talked about the what and where Pathways in the back, right? And the, what, where Pathway basically shows where things are relative to a body centric reference. but you don't know what it's, So it just says are things out there, some the disposition, and then you've got the web pathway. It says, oh, here's this thing called the cup, but you actually don't know where it's anymore. It's it's just in its own reference bank. So that's what you're talking about here. That's kind, but I always use those as basically the same mechanism. You're just using different reference frame. Like one is a reference of the cup and one's a reference frame of your body. but when you said the book, that's, that puts it back in the what pathway. It's okay, I have a book and a cup and they're relative to each other. I could have that in any position, any post. I still see the book relative to the book, A cup relative to the book.

So I, feel like we're mixing, ideas here.

and you also then we're talking also to reference frames and displacement, right? That's, I think you said on the left you have a reference frame and Right, like a X YZ reference frame. But on the right you have a displacement. But you also show a reference frame there. I'm confused by that also.

okay. Now we just have a look at some other examples. So, this way you could also think about the def recognizing the deformation would've been in this reference frame, would different coordinates this way, could represent the defamation that way. This will be, so the out here was stretched, so we have a different location as opposed to just the object on its own. But, so why wouldn't, but couldn't you represent the, form cup on its own? Why are you trying it to the book? I'm confused by that. Because we struggled to find a representation for the object just on own.

And but that was your path too, right? Yeah. But a path too. We could do it in a way that completely inva to the deformation. There's, no optimization process involved. What would be the appropriate, deformation rotation, transformation to good? I had, we have a problem recogniz a deformed cup, right?

problem. We can do that. You can see the same structure in different shapes and different sizes and different def.

That seems to me basically, an object recognition problem, representation problem, which to me, puts in a path too. It's okay, it doesn't matter. Limitation, it doesn't matter if it's relative, can be different shakes. so I'm confused why, what, it seems like that's a fundamental problem with just cups, not cups and bolt. That make sense? Or am I just think I'm missing something.

It's, yeah, it, I think that, there's two separate question thing. How do I recognize the completely in to those? The other thing is how would this deformation be directly represented and a straightforward way used to be within the context of another structure, because I can imagine it's independent. Structure I can recognize it as a different shape, different form, different, can dot of in principle, yes. But then you may have an explosion of the possibilities. Seems obviously we don't, it seems like I even do it. I can recognize cups of different shakes independent, of their context. and I don't have, my brain does it, so it must be a solution.

To me, it feels like I've gotta solve that problem, the deformation problem. one way or the other, independent of its context. Again, I think that the size scale change of the skies you started with was not a, that was a different example. I was trying, that's why I'm trying to separate that out. but here. I would see this. If you didn't show me the book, I'd still see it as a deformed cup. I don't need the book to show me that. Or not deformed. It's just, but if it's hard to present something without any context, and the context could be that it's self context. It's just self context. It's a different thing. An old and a circle, right? It's a self context. There's, I don't need a context to know that circle's different than a noble, I can just see it. It's, there's a representation of that. Just like I can say I have representation as a cup that's independent of context that says it's shorter or taller, whatever.

context can help, like we were talking about the absolute size of something. The context would provide that information, right? So if I, I see the cup and it looks bigger on my retina or smaller than my retina. how do we know it's a bigger cup or smaller cup? I have to look at the context.

also how information, the shadows and the like and stuff.

Yeah, but Right, but, still, I think seems to me this problem can be solved independent of this context at all. it has to be solved.

I guess my argument KO, is that I see this problem independent whether there's any context. If you showed me a shortcut and a tall cup, I see it and that's all there is. I know one's short and one's tall, or I know they're relative sizes, they're internally consistent sizes or morphology. It's independent of any context. It maybe if there's no other contexts out there to the body reference, why can't it?

It self consistent. I don't need anything else to tell me that, that it's, this cup is wider than tall and the other one is taller than it's obvious. it's self consistent reference and you trying to draw analogies with the what and where pathways and yeah, that's one, one thing I was thinking about, what and where on the other hand when looking at the literature, it seems quite messy. So the experiments, the thing that's clear is like, where is stuff that's relative to the body and how the body might interact with the object as opposed to the what type, which is just about the object identity.

I think that's pretty clear.

every neuroscience thing, there's no study.

but I think that's pretty clear to extent. It's, there's, the people push around the edges saying, I found some blah blah, so respond this way, that way. But generally it shows this some separate processing.

Yeah, I think it's very, easily explained in some way. I'm not, sure this is right or not, but you, if you think about a reference ring relative to the body, then the where pathways basically says these are things relative, this is, where things are relative to the body. And I can't, we're referencing relative to the body, I can't define the object, right? The, object's not defined in reference. It has, the body has to be defined. I can't find the object. It has to have its own reference. So the what pathway is like, oh, I'm gonna use the same mechanisms, but now I'm using a reference room that's attached to that object. And so I'll see the, I'll see the shape of the object, but I don't know what it is. And here I see different where something is, but I dunno what, it's, that's very, simple and copacetic, with, with the mechanisms and with the observations pathways. but I think I see the advantage of having also reference with cutting out other objects.

Can build up the whole world in this hierarchical world. Yeah. It's, that's just a compositional object, right? Or hierarchical object. that's, the coffee cup itself is that way, right? The coffee cup has a logo and the logo has letters and so that you can build that structured, compositional, model in the what pathway.

I can suppose I can do it in the web pathway perhaps, but I don't think I need to have a body or world centric reference frame to, to learn the structure of the object. again, to me the book is just another, it's just a composition composite object. The book is just another thing. I don't see how it's different than the logo on the cup versus the book in the cup. The book could be tilted in change in this direction, right? Could do all that as well, but I have to figure can represent. Let's say object A very well in the reference frame of object B. And then for example, the key on the keyboard has a very specific location relative to the computer. And I, yeah, changed my own perspective. Keeps How's the logo on the cup? I don't get it. The logo. It's the same. Okay, so then I'm missing what you introduced here. I'm just not, I'm just not following your argument. It's, the thing with the logo is it's, it's, represented within this, curved coordinate system on the cup. Yeah. So the cap is not supporting structure. Yeah. And then the logo is represented in this reference frame and the locations of each, letter is represented in this reference frame of the cup. Okay. It's not the reference frame of the logo, it's the reference frame of the cup.

but that seems exactly what you're saying with come to the book. Yeah. So then I don't see them as two separate mechanisms. This is this part of the one, it's the same. What is separate is the ability to recognize, like a specific shape maybe of the letters independent of this deformation. And in this case, they wouldn't have a specific location based, coordinate representation. So, we, I, the way I've been doing this is this, there's two possible ways of represent. you've got a coordinate reference frame, and then you've got the splices, which, and, these are two possible representational schemes, right? The coordinate reference frame. It's very difficult to handle. certain types of, deformation and so on, right? it's but the, this, and it's very difficult actually to infer what the object is in the court reference because you don't know where, you don't know where you are until you, how do you even assign it to the right reference frame? It's who knows? But when you deal with it, automatically handles those things. Displacement says, oh, it doesn't matter where this thing is. I'm just looking at the displacement for the features. and then we talked about how the displacement themselves could be modified and more, as long as the relative positions of features are preserved, but the distances between it could be stretched and you still see, as you see it.

That are possible reference frames and displacement. And if that's what you're talking about. I agree. There's these two possible representation scheme, but I, don't know if they're both being, it seems like they're both being used in representation of an object. Yes. and it's not like there's one pathway and another pathway. It's like object representation has both these mechanisms. And they serve different purposes. so to me it's not like Path one and Path two. It's oh, how do I represent an object? I have to, come up with some mix of placements and reference frames to do this. yeah, I agree. It might not be Path one and Path Two and it could be very, everything is in one column and it has, but essentially talk about these two type types of representations. One is this specific location based representation, and then it needs some supporting structure like the, and the logo. On the other hand, I have this displacement type of representation, which is in a certain amount of deformation. certainly variative, pose, you can change your pose of things, right.

and so I would, yeah. So, this is, sorry.

so this was, I was thinking according to the, yeah, maybe path two is not the right term, but it's according to the other way to, and 'cause we have this ability to recognize things. I think just based on rough relative displacement from each other. For example, if I hold this cup on, the corner here and I now blindly would feel the rim over there, and it was still, it would still feel like the cup.

And, so it's like it's this relative displacement.

Like core screen, and it would be relative to the, to this part. This is just the, and this, would be just relative to, so it doesn't matter how the whole object is oriented, right? It's strictly variant. This is a, displacement representation. Exactly.

and, and regarding the encoding, so what I call path one. We didn't use the location as a context, and for each location we would expect a local shipment. This is all we, again, like in the, columns paper.

But maybe the only difference is I call it as like some supporting structure. It would.

Or the cup itself.

And, whereas the other one is uses as context to displacement. And I expected a certain displacement and, certain option. One of the, you say here, what was it you said? What was it? I thought I said what was it for?

Yeah.

one of the possibilities is that the Marcus had proposed was that good cells, which are what we think of, we think about reference.

that we know that they do path integration. They know that they allow you to know where you are when you move. And, they're also, more ideally suited for, determining the, Basically the distance between how you might move, as you said here, like playing, actions. And so this idea that the great cells might just play a supplementary role, they're not really in the definition of the object itself. The object is defined by its displacement, which is a more copacetic idea. It makes sense. but then you might have this reference frame because you actually have to move relative to the object, and you have to calculate movements and you have, so it's more like you can link to that and use that to define how to move, but as opposed to actually being the integral part of the representation of the object itself. So we, in the columns and columns plus paper, we say, oh, the object that's.

because it doesn't work really very well, but maybe it's all about displacement and the reference frame is there as the supplement role that you need when you wanna move relative the, as your finger moves, where will it be relative to the object? So that's as this reacting, what is this encoding for? and I guess there's under possible planning actions, but that is that is hypothesis that, and I'm working with right at the moment because it seems like if we can define the structure of an object completely independently of these reference frames, it is all about displacement. That would be, that would probably how it's gonna be done.

because every time I think about the using reference range for defining the structure of an object, it doesn't work really. it doesn't work well.

Yeah. For example, in this case, if I represent, Like the shape and this, let's say the keyboard shape and the reference frame of this computer. And I, could use this to guide my hand actually. you have to know when your hand is moving, how far did it move relative to the object? And, we know grid cells do that, right? so that's a suggestion, right? Grid cells seems to be the, seems to be the way that movement is, converted into, displacement in some sense. Alright, so as the, I think as my one last, so as the last piece here, so I'm sorry, that is kind technical. Yeah. I've been discussing with Vivian, other, invariant representations. So we already went, so Vivian already showed with she can have a post invariant scale in, representation. And the code, the can be recognized independent of the scale and post, then we'll discussing other kind of transformations and for example, sheer or generally you could say an ine transformation. what's a sheer transformation? Sheer is, for example, this, box is, yeah, not flexible. Twist, not twisted. Imagine the top part. Okay. Yeah. Alright, alright. That's a very odd, that's not a normal, that's not a very normal type of, I know what you, I mean it's interesting you show me that and I would say that's fundamentally not a regular coffee cup. No, I'm serious. I think that distinguishes things right? There are, you can change the size and the relative, dimensions and. but I can see that as clearly a cup that's been, if you wanna call it shield transformation, I can see that it's not like I said, oh, that's a new type of coffee cup. I say, oh, that's, I, unless I guess if I saw a lot of those, that's an odd one, you also see the rotation and the scale, just because you see it doesn't mean you can't recognize it as the same object anymore. I, didn't follow, I, the words made sense, but I dunno what the point you were making Billy. So also when the cup is rotated, that it was rotated, or when it's been scaled, you can see that as it was scaled, under all the transformations, you still recognize it at the cup, but you're also able to determine that there was a transformation applied to it. maybe, I don't, when I see the rotated coffee cup, I don't say to myself, in fact, this is what the argument I was making last week. I don't say to myself, oh, that's the coffee cup rotated. Remember I was making the argument about the all views of the coffee cup might be laid out in a sheet. I don't say, oh, here's the coffee cup, and now Oh, I'm looking at, I see it now. Rotate. It just looks, if you said, what is that? I say, that's my rotated coffee cup. I I don't show me this. I don't say, oh, that's my coffee cup looking down from above. I, see that as a coffee cup, but it's not like I had to do a transform to get it into a, some canonical view in my head. And that's where I was coming from with that. Exactly. That's what exactly I was coming from. the sheer one is not like that. I look, the sheer one is no, that's wrong.

My coffee cup doesn't look like that. So I say there's a fundamental difference between, distortion of, You changing the morphology of the cop distorting in some way. The sheer distortion might be just the same as making it squishy or short or something like that. But I see that as different. I don't see that's coffee cup. I immediately know it's not my coffee cup. But when I look at a different views of this topic, I don't see it as a different object. I don't see it as my object rotated or under some defamation. Rotation does not seem like a defamation. But Jeff, what about, what if it's upside down? Then you would see it as, no, if I see coffee, if I see my coffee cup upside down, I'm like, it's in the closet, it's upside down. I don't have a problem with that. If I see that and it's normal, I first of all, even if it, I would say it's the same cup, it's, the relative displacement haven't changed at all. So it's the same cup. As soon as you start changing the displacement, then it's like a circle that comes, an oval like this cup comes something else. But's still related.

all rotations seem to me I, or just. They're not like I, got my object and I see it under some deformation. It's, no, that's, the beauty of the idea was presenting this week, is that I gets rid of that idea. There's no concept of there's a left and right to this coffee cutter. and it's just that somehow I see 'em all is the same. where if you, do some real deformation now I don't see that. Then I say, oh no, that's shorter fat. Or different color. I dunno. Obviously it's different. but any of these, pose change, don't, do not strike me as it's a different color, but I'd say it's the same. Even upside down. Yeah. Yeah. This relates to the information we actually consider would be the same. that's why that's something become a different object.

we were talking, there's two things. One is when does something, a specific instance become a different object? So this particular coffee cup, when, do I see it's different? And there's another question that's what's a normal coffee cup generally?

that's an interesting question. I would argue under the deformation category, there's, we just have to see what we observed in the world. If I never saw a tall, if I never saw a cup that had a rounded bottom, like a teacup for and every cup I ever saw in my life was straight sided like this. And the first time you showed me rounded bottom one, I said, that's not normal. That's odd. But since I've seen, I said, no, that's the different, they're both called cups straight sided, the curb cup side. So, I, obviously it's experiments based, but how we learn models of things, right? And it gets messy. How do we categorize Yeah. Category. Yeah. Yeah. So, coming back to the transformations, and, I agree that the, total space of those transformations go beyond what we consider normal. But for, let's say for a machine sy system, it might make sense too.

This is just a way of doing deformation, is that correct? So, find transformation is like what you see here. So there's any kind of linear transformation and one characteristic of those transformation is if you have two parallel lines. Any transform it is too parallel. Yeah. And certain other things stay in variant. for example, volumes. If you take the volume of, you mentioned those three, volumetric shapes here, the volume of, each of those under the transformation stays still the same Relative audience. Oh yeah. Relative. I was gonna say really? Sorry. The ratio Ratio. The ratio, yeah. I mean it's a, are you saying it's a linear transformation? Is that basically what it is? Yeah, it includes, Translation scale rotation. Yeah. and the shared all included in this, all included. So the translation you get here, the rotation you get with a very specific, a metrics that has, determinant of one properties. maybe this is how we want to implement this. I'm not gonna say no. But I don't think this is what's going on. The, yeah. Okay. There's what's going on. My example of the cup being rotated and not really being rotated is, counter this idea. there's no transformation between these two in my head. It's just not, they're both valid due to the, I just see them, what they.

where this would say, no, there's a, normal crop and then there's a rotated cup under some transformation. so that doesn't mean we don't wanna implement it this way because maybe we understand this better and we go that way. I just wanna point out that I don't think it's capturing what the neurons are doing. Yeah. I agree. so there's, one way to get, a recognition. It's completely in very, so it wouldn't cognize at all the pose. It wouldn't, but could you could recover it. Wait, I thought, that is the poses are not part of this transformation. Yeah. But it, is. If I, let's say 3D rotation and if this is three DI find it, I find 3D, the 3D rotations, 3D scale, 3D movements, and 3D share. I thought you shoulda win more. What, I mean is it would work in the sense it would recognize it independent of the post. Okay. So the post Gets removed, the recognition system, it's, you can't extract a pose after that. Yeah. Once you recognize the object and once you ation of the object, you can recover. Yeah. I used to think, okay, so that's good. It works, right? Yeah. I used to think that, think about that in the brain too. I say to myself, I'm able to say what this is. It's a cup. It's my A cup. But I'm also aware of its poses, right? So when I classify it, like what is it? I'm clearly not including the pose. The pose doesn't seem to be part of that. and so I said, there must be some neurons that are representing, The object just independent of his pose. And then, there's some other ends of representing the pose of the object. So there must be two classic, two parts of these neuro population. My point, but, there I'm working on now is that may not be true, right? I used to think that, and it was always bothersome to make, it doesn't fit, but that's a lot of things. It doesn't fit within the brain. And so I was thinking like, so this idea that you've got this sort of displacement sheet, which all of it is the object and putting a part of the sheet, you're, you are, you're actually working in now is the state of the object, whether that's the suppose or the state of the state or something like that. It's is saying, I've got this sheet that represents a seat of displacement that represent the object. I'm only able to observe part of it right now, but I still classify the object. And the part I'm observing is the post.

It's there. it's not like I have two separate representations. It's like there'd be one representation for the two. So I think it's a very nice idea, but it's true. And I don't know, it's a very nice idea. I agree. It's nice idea. I thought about it too, but my concern is that it's gonna enough explosion of possibilities. No, I don't think so. I think that's, I don't think it, I think that, I think it's actually quite efficient. So at least the representation of the, I, haven't worked all the angles out yet, but so far as I thought about it, if you take the cup and you just unwrap it, you're saying you're storing the same number of feature displacement as you would've otherwise, you're just storing 'em in a, you're, not storing 'em in a 3D structure. You're storing 'em in a 2D structure and you're playing it out, but it's the same number. It's I'm still just figuring out the displacement on, on, a local. I'm thinking of the, of these things. I'm not trying to figure out the between distant points.

so I think it would work, but I haven't proven it, so I think we should, I'm gonna keep working on it and I think if you, if the argument here is we wanna be working on these transformations, I think that's fine. we might run into the problems. but so what We might run problems. Anything, I'm just pointing out that I'm not, I'm not satisfied with yet. I wanna, how, I think there's some fundamental things. There's so many weird things going on and how we recognize things in the, that I'm afraid.

it's one of these situations where I'm worried that we might really need to understand neurons do this because other, some.

just a side note on what you, what they showed. Those are what, at least in, in my field, what were called very centric coordinates. And they can actually work in higher dimensions as long as you decompose the object into simplicities. So there's the two, two-dimensional form. There's a three-dimensional form. So they have the properties that you're talking about during, in variant, what was the term? Was it bar centric? Is that what you said? Bar centric? Yeah. Weight, heavy is what it, how do you spell it? B-A-R-Y-I think E-A-R-Y-S-B-A-O-Y. Centric coordinates. What does that mean? It, means it's weighted by the, if you think of it as a triangle where you have a point in the center and you form three triangles from that, it's the area of the triangle weights how much you would wanna contribute, to the, coordinate. In other words, the distance from the face. So we use it in graphics for interpolation, for instance.

when you where in instead of, thinking about how interpolating, on a square, if you're trying to interpolate a triangle, very center coordinates is what you use to kinda weight the contribution from, say the color of the vertices to the interior color. So it's, something that works tally, when you pass it through, cordon systems and still produces good results. So what they were showing there was, rather than breaking a triangle into three triangles, centered at the point, they're breaking a tetrahedron into four tetrahedrons. So that's a, it's a four tuple rather than a three tuple, but that still can go up to higher dimensions. So it is a way of making something that's a variant, to those, kinds of, aine transformations and higher dimensions.

Thanks for the link reference, Kevin. Yeah, So let's move on. Next presentation.

Okay. last week we talked about the sensor, which was but then Jeff pointed out that it's not really how it works because we have, we move a finger to the side, so we hit something, or we have multiple sensations in different sides of your or your finger. So the same group, the same researcher. When, together with university created this version that was quick. So this is basically the same technology. So the one that we talked yesterday is in the left there. The left there I see is the left one. So it basically has a camera on the bottom and the gel on the top. In long, what do the LEDs do in this case? They, pull light. Oh, they put light because the, there is a, coating on the top of the gel that doesn't let outside light come in. So they only see the deformation. On the gel. They have different color LEDs to, they have different colors, so know where you are on the sheet, something like that. And they probably alternate 'em, or, yeah. Okay. It's a little bit like how you might make a touch the way we used to do, figure out where your, finger or your pen was touching on the screen. You had to have different, sources and alternate. Okay. So on the left side, that's why we, that represent last week. And they like, they're usually, this gel, these LEDs, and they have a little casing and they have this webcam. At the bottom, on the right side is the new version, which uses five cameras, and there are, endoscope cameras because they need a lower focal point, small focal point, and there is no casing. It's just they put the gel on top of the cameras. So distance is basically the size of the thing, the side of the pump. So there's no inside completely. There's no air. Completely. Oh, it's completely gel. It's completely gel. So the, thing on the right has a physical sensor and they rack that in a gel. Wow. So the whole thing is gel 360 and, 270 at the top. So this, if I was think this, I keep going. This is pretty cool.

so that's, and then, in the five cameras say there's somewhere blind spots, as you can see in the top right. So those are the, this is the field of view, the cameras, all the five cameras. So the, B is the top view. So you mean this is one field of view? This is another field, yes. So right here is the blind spot. Yeah. And, you can see the, in the diagram on the.

It may seemly small. Those are endoscope cameras, so that has medical grade.

So they can use this for, some material exam instead of the doctor's figure. Yeah. They go off the nose or something. Stick a finger up your nose. Other places that we don't wanna talk about nose robot now exam, but those are the s that, because they have a, because this, because they put the jail, there's no room. Yeah. The camera in the gel. So they need a much smaller focal point to think it's fine. What, was, why do they do this? Same reason? The same reason you mentioned. Okay. So same conclusion that, on they going sideways. Look at the. You, cannot see Yeah. What's going on once you move the finger. So they came to the 10 conclusion. So in this sense, like you see the top one, it's just the top. Yeah. And the bottom is as they moving the finger. Yeah. And then you get in the different camera, they got different camera. Right now they're seeing the, left, the side camera and the top camera. Those are the different two. you could make it, you could, basically, and so within the camera's field of view, even then they have resolution within that, right? Correct. I can tell there's on the left or the right. Yeah, they have one. So even the old one, it's more than just a single. if you think about it like a century patch from a patch of skins I mentioned before, it's not resolution like this. You can't feel the entire, to feel something like that. you have to feel multiple parts of your skin. And, so. this is actually, this, is an array of sensors. We could, break it up to five, you could break it up into 25. You could take each of those and break it up into, for the pieces. That too gives you a lot of flexibility here. Yeah. It's just a different configuration of the same Yeah. Technology basically. Yeah. So you can do that. Do they have a simulator for this? It's, they have the same simulator that we have before that I presented. They have a version. So you see like the, marble goes to the sides and the bottom. Yeah. Yeah. And you go to the top is on the top camera. They didn't really simulate like physically, right? It's just No, they actually, they have a mesh for the gel and they have they apply the noise from the, camera. They have different camera fields. Yeah. All continuous thing. No, there's five different cameras.

There's five different cameras. And they apply the same noise they get from, They got from the real sensor, so they couldn't compute the level of noise there was and apply that. Wow. So I have a, I can show in the paper where they get the different, the real versus simulator ones, but if you were to run the simulator, would it be like super slow and we would say, oh my gosh. Yeah, that's what I'm reading right now. So it is, not as fast as the other one because there is a physical physics component. Yeah. Because the pressure of the, marble on the gel is simulated by physics. We may have that in the old one. The other one we didn't have physics, but they still have the ball on the. Oh yeah, the old one. Yeah. Those others, that's the same, similar as the old one. Uhhuh. It is the same as action simulator. Just they just implemented this configuration into that. This is more like you've got five of 'em instead of one. Correct. And maybe there's a little, bit more difficult because you have to figure the angle. So I'm trying to figure out what is now the, frame per sec that I can get out of this. Do they provide the source code for this? Yes. It's all open source. So you could, make a simplified version of this that didn't correspond to the physical sensor. Correct. So what if we wanted, what if we said, Hey, for our purposes right now, a lower resolution, system would be okay. Because right now the resolution they have is 4 0 4 for each camera. For each camera. That's a lot. Yeah.

I, don't know. I don't Yeah, we could, like that the data is probably out there about, if I think about the, my fingertip, how many different sort of columns are representing my fingertip? I don't know that number. Yeah. This one, one touch. You can, could read a paragraph.

So, to me, this is not like a, this is not like a, this is, superhuman, but it's also, it's really more the equivalent for a human might be, I don't know, 50 columns of, that are much fairly low resolution type. each one's not capturing an image.

if, when someone reads braille, it'd be interesting to know there is a, there's a very simple test. You do, you on the skin, you poke, the skin with two pro, two pins. And then at some point when they get closer together, the humans can't fill 'em apart. There's two, at some point they do fill 'em apart. So that, that in some sense is probably the scale of your court of a column. In some sense, maybe a good guess. and so I think on the tip of your finger, it's pretty close, right? the back of your finger, it's pretty wide apart, you do it on your back of your back. It's surprising. You have two pokes, like instrument, two inches, apart. You can't tell the difference.

so I'm just saying, I'm just trying to figure out what would be equivalent for human figure here. But that's, it's pretty cool. But I think we have flexibility to, my point is if it became slow, we will use this and it came slow. We might have options of saying we don't need all this resolution. Correct. and it open source, the.

Because this seems would be higher resolution than a human finger. If you got 400.40 by 400, yeah. That's a little patch of your, the camera they got. Okay. That is far greater resolution. You're gonna get a human finger. And we, might want to be just like touching, like basic deformation as opposed to Reading the, micro cracks in like ceramics here now. Oh yeah. Yeah.

look at that. The screwdriver there. Yeah.

Is this the same team? It's the same researcher, but like university is not Facebook. This is Berkeley. Yeah. So ly same team. They built this, not the same thing, it's a collaboration. Collaboration between the main and the first one and Berkeley, but the main office is from Facebook. Oh, I see. I understand that. I forgot. The main office is the guy who built the physical system. He built the first one and the simulator. This is on Facebook. Berkeley took that and built this. Wait, careful. Oh, I see. So, Facebook did the first one. Yeah. And then Berkeley did the second one. Correct. But collaborating with the guy who did the first one. Correct.

Okay. Interesting. What are they doing with it? That's a good question. Metaverse, maybe. Oh, so tomorrow there a Facebook. Talk at line of how they're using AI in the metaverse. Yeah. If you wanna in to see what they're doing and all that. I don't, I mean, how would you use it in, some sort of virtual reality? Because it's, like you'd have to have a remote hand that's remotely sensing something physical. Or I guess it could be just a simulator. It's remotely sensing something through the simulator.

but would I be, would it be like my hand, is that the idea that my hand would be moving and this thing would be in some remote world? if I, if my, if there was a representation of my hand in the metaverse Yeah. And I touch something uhhuh, it would need to know the exact sensation to be able to then transmit it back to me. But would then you would try to activate your hand. You, there's then you need something corresponding to, this should activate your hand. Okay. So then, all this high resolution really wouldn't be advantageous in that sense. 'cause your finger can't tell, I can't, I don't have this resolution there. It also has applications in telepresence. Yeah. Now you can pass the sensor back. Isn't that the same? Yeah. How's that different than virtual reality? the difference is you actually got a remote effector, like a robot sitting there that's got a hand touches back. That's what I started thinking, I was thinking like, oh, your hand, a, physical robotic cancellation, you were sensing it. But then I said, the metaverse wouldn't be like that. Yeah. So yeah, I can see that like you're, trying to fix the space telescope from Earth and then you've got your hands up there manipulating the space telescope or something like that, or a anything remotely if you're a surgeon or something like that. yes. Yeah, it's, yeah, I got it. Although again, it's a mismatch between the capabilities of this and what your finger would communicated.

this is cool because.

in some sense, if you had a robot, if you were building a physical robot with a brain, then that robot would have, it could have super human tactile senses, right? Yeah. I think that would be the exciting, maybe we could do some manipulations, otherwise, I've said multiple times. I thought another cool, super superhuman capability is the, is to be able to sense at a distance using your fingers. So as you approach something before you actually touch it, you would be able to sense. Yeah, but it'd be more like a little camera's vision on each finger. And, it could, in some sense, you could start seeing it or feeling it before you actually touched it. and so you'd be able to see an object just by doing, you, you'd be able to feel the object just by doing this. You already be able to perceive it or learn it, just in the sense like, move our eyes around, just like moving five little eyes, But then you could also grab it and do it.

The sort of the, sensing at a distance was sensing on the surface with robotic manipulation. So you have this super, hand. It's a little surprising. There's no organism I can think of that. Has anything remotely like that evolution seemingly just didn't cook that recipe? It's an interesting question. may maybe like a location or sonar. Yeah, a little bit like sonar. The whiskers are a little bit like that. They do touch it, but it in your field before you actually, but it's not exactly the same. I think there are some crabs or maybe something that have eyes that are actually extended on things. Yeah. But the question, combining it with the actual physical manipulation is the interesting thing. It's not just like a bunch of eyes on stalk. It's but I can also grab this thing and is combining sensitive distance with.

Surface for with physical manipulation, which is unique. I, your question, you also have to see what the, limits of, human beings, especially trained ones. the claimant, I remember seeing, some pride to this one. Japanese machin is claiming that he could basically sense things down to, a 10th of a mill. so surface discontinuities still require a huge amount of precision, and somehow we're able to do that. I'm sorry, that was used touch, or I didn't, what was that? That was touch. Yeah.

you can, feel, a human hair if it's on the surface. that's, the level that we have. Yeah.

Is they can feel the presence of walls as they walk around. And I forget what the research says about this, how they do it, whether it's too sound or through it's, like radi temperature. I'm not sure. I can frankly just tell you. I can sense walls around you. You would My ears like I just the sound of it. Yeah. Yeah. It's because of sound engineering. Now I just walk into a room and I'm like, oh wow. The sound is so different. Yeah. Yeah. That's interest observation. Animals that have a grasp, that also has sensing in the distance combined. Example a little bit. I think evolution would against it because why? Because a predator or you could lose it, from just dangerous circumstances. But why isn't it, why would it be worse than having, it seems like an extra capability can only be advantageous. How would it, get worse? Because you don't wanna experiment with the part of yourself. It's just I'm giving you the ability or something. So you're putting this like maybe, but all I'm saying is that you're getting an additional capability. You don't have to use it. So it's not forced you to do something dangerous. imagine if, I imagine if I was just able to, really sense this object before I touch it, I don't wanna see how that would make it more dangerous for me. It just seems like it would be better. almost oh, don't touch that. 'cause I now know from a distance it's, gonna be, I dunno, something bad actually. I guess the tongue is maybe the closest. 'cause you have, they're not cameras, but they are different types of receptors. Yeah. Yeah. Anyway, but they have something has to be on the surface. Anyway. Okay. Very interesting. Very interesting. I like this. It's good. I'm glad this happened because yeah, you brought up the same date. They also bought the same issue. Yeah.

Did they, did you read it? Their description of why they did this? There anything addition that we didn't think about what's in there. They say that's, sometimes the task. Exactly. We said like sometimes we don't have the actual sensation. They have move. Sometimes the object goes around. Okay. So yeah. So yeah. I didn't know if there's any additional stuff in Yeah, like you like. This gets back to we were talking a while back, Vivian, about like, how do you, when you move your finger along the edge of an object, you get to the edge. How do you know that? one reason you don't just go off the edge because you can actually sense. There's some senses that are kicking in saying that. No, it curves around. I think the, experiment, experiment experiments, they, they didn't have the same budget as Facebook. So they have a 3D printer instead of a robot to experiments. They A 3D, these 3D printing instead of a robot to do experiments.

Oh, they're actually you saying they're moving this finger on a 3D printer? Yeah. Have a C machine. I see.

It's cheaper than a robot. Yeah, they're very creative for sure. Lower budget Facebook budget grad students. Alright, this is encouraging. Oh yeah, it's very fluky seeing those little lights blowing in there wells. I know, but it's definitely gonna give you the heebie. Gee, if you saw someone fingers glow like that and had a little bubble, they, it's I'm coming. You can see the little bubbles. They didn't got a perfect seal. Oh, okay. That's distort bubbles. Bubbles visible.

A little bit, see a little bit the bright spots. Yeah. if, but if that, if you, if this was learning from the beginning, it wouldn't matter. just like you have deformation, you have problems with your retina, you know the blood, you don't notice that. See the bubbles. Some a little wide do. People's skin is not the same. Alright, very cool. Cool. Yeah, thanks. Alright Ben. Cool. we don't have a ton of time, so I think I'll just try and the shorter of my presentations.

Okay. basically there are, just a couple small things in our current diagrams for the Monte architecture that are a little ambiguous. So I wanted to raise those and try and get some answers or at least start thinking about them. And, also update you with some changes we made to the code and try and make sure that those things that are ambiguous and the decisions we made in the code are gonna be compatible with the overall picture.

So here's the diagram. The people to the right. Yeah.

Oh, look at that.

so yeah, this is from our documents. on, on the screen share. It still shows the first slide. It's not sharing that, but that's complicated. Okay. Can you, on the right screen here, just click on the second image. We'll do this manually. How about this video? Yeah, that's perfect.

And just move both of these. So when you move the mouse now, I have to keep in mind they don't see your mouse. Yeah. okay. So just verbally, let's just do it all on this. Yeah. So you won't see the animation. We see his mouse. the first question was, where do sensor modules live? Like, where do they begin? And then, so is it it's a sensor module? Is like the sensor that's in the body, is it something in the brain that processes the sensory input, or is it both? it would be like the retina as well as something like early processing?

you're talking about biology or in, in Monty? Monty, I don't know where a sensor module actually begins and ends. In Monte right now, everything's code, right? we, don't have any physical sensors yet. So Monty, like conceptual speaking, I can take a stab at this, but I'm not sure how other people think about it. to me the sensor includes in, the real world, it includes the physical sensor, plus any amount of subcortical processing that has to occur. Present in a format that's now can be, fed to a learning module. So in the brain it would be like, oh, I've got my eye, I have my retina. and that's that in the ears. It, you've got, it goes from cochlea to, this two subcortical structures. They're inferior and something else, and it gets processed and then it gets passed. The cortical that works, it's the sensor plus some subcortical process. It would be anything that, it's not cortical processing. It's anything that's specific to the modality of the sensor that needs to be processed to get in a form that can be used by the learning model. Perfect. Oh, that was easy. Works great for me.

okay, the next little thing I wanted to bring up is, there's no attention on this diagram and, there's just not enough dots and arrows here, sorry, I, guess there's the motor command which could move the, so that's, what I'm getting to is, I think I've been tending to think about the motor command and the attention as intermixed and I want to piece them apart. 'cause we talked at some point about the idea that you could give an attention signal in the form of a location in space to attend to. Yeah. but if that's the case, who decodes that into actual motor commands so that move the sensor to attend that location? I guess it would be the sub-cortical motor area. Yeah. And then that leads to the final related question here. yeah, that's right. That would be, so that would be, again, very modality specific, right? Yeah. So the ulu directs eye movements, and I think it works in coordinate systems like this. And so you would tell Theus attempt to this location and it figures out how to move the muscles in your eyes and do all that weird stuff. So on our Monte diagram, that would be part of the, that would be part of the sensor. Oh, so that's that's what I'm getting at. Is it, so now the sensor has to be able to do that. Yeah. So here's why I disagree. Oh, okay.

so as a rock climber, I've had to become very acquainted with the anatomy of the hand. And what's surprising is there are no actual muscles in the fingers. they're remote controlled by tendons. you can see that yourself by pushing down in your forearm. Yeah. The muscles in the forearm control the fingers. and also the muscles in the forearm don't move the forearm. Yeah. So when you think about motor commands, some action potentials are coming down from probably the motor cortex to nerves in the cervical region of your spine. And from there they go out to the nerves in your forearm. Yeah. And then the muscles in the forearm contract. And that's gonna move the finger and that's gonna change the position of the sensor on the tip of the finger. So what's the problem? But I think, here you're using a different depth meaning of sensor than what Jeff used. So there sensor was, or sensor module. sensor was, all of the stuff you need, including the subcortical processing to deal with in this case. somatosensory input. But that's included. And so the subcortical areas would also be the same ones that would translate stuff to motor commands. So the one in the eye does include the superior COR part or not? Yeah, The superior ulu is included in the sensor module of the eye. that's according to this definition. it, this could be distributed the body, but to think of it logically, you have a signal that says, move the sensor to tend to this location. And how it happens is in, we don't, that's a very specific thing on the relative to the body or the sensor. So the fact that the muscles are physically located in your arm makes no difference whatsoever. There's a signal that says, move your finger to this location, and somebody figures out how to do that, right? Yes. That's not Monty's problem. That's the sensor problem. Okay. I guess I'm slightly confused 'cause.

The, it still seems conceptually like the actuator is not part of the sensor.

the muscles can move multiple sensors at once together, or, and they, are constrained. You can't move different patterns on the same finger into different directions. But, how that, that exactly how that happens is very dependent on the particular sensor and the particular morphology of the body and the particular instantiation that we create. And so that is outside of Monty's concern, right? That becomes a very specific concern of the builder of a system, to do that. I just feel like it complicates things. If we wanna put this into the same bucket, sensors and actuaries, I feel like it's, conceptually much simpler to figure out if those are two distinct systems.

I thought it was, I thought it'd be simpler to make them one system. So you think it's simpler to make two systems? Yeah. I think traditional, maybe the word is confusing. The sensor is typically thought of as an input Yeah. Thing, not an output thing. But you are thinking of it like the modality, I think of a sensor system. Yeah. Like the all somatic sensory stuff is you do things that's specific to, touch and movement with, of fingers and stuff. Yeah. And visual modality would be retina plus the motor commands that are, so maybe you think of it as a modality maybe, or just more of a complex system that does these things. think about in our, if I wanna direct my gaze someplace, it may not just be moving my eyes. I may have to turn my head, I may have to turn my body, but we just do this automatically. if I hear something back here and I, I need to look, my brain says, look at that thing. I go like this, I move my body and my neck and my, all this complex stuff that's happening. so to me, I just wanna abstract that away and say someone took care of that. now I think you bring up a separate issue, whether actuators, who there's an advantage to having an actuator separate from the sensory system actuator. is there an advantage to be able to say, I'm, I, I'm, I don't know. I was trying, I thought it was a wonderful idea to say dis unite them and say, okay, when I wanna grab this object, maybe the right way to think about it is to move my sensors into these positions. Because then it unifies 'em, it makes it somehow seems more simpler in some sense. but you.

they are already unified because we have a sensory motor system, so they already learn together anyways. And, that's why it all works so well, and it is so easy and effortless. But I, still feel like conceptually those are two distinct modules, sensing and acting. They just learn together and learn to coordinate together. But, I, but why do you feel that way? Why, to me, as a, what, what's wrong with the idea of saying, let's consider as one thing, maybe not the way you've thought about in the past, but what's, the advantage of separating 'em out or, I just feel like it complicates things a lot because motor actions, they can have a lot of different. Goals, like you can move your hand in order to sense stuff. You can move your hand in order to do stuff in the world. You can move your hand in order to, move an object in, into your visual field. And in order to see it, you can do the same action for a lot of different reasons. And, same with sensing, you can sense a lot of different, sensations while performing the same action. Yeah, I got it. I, guess I was hoping that, that, would actually be, we could unify all those things into one thing. It's, when I, thought about manipulating things with your hand, for example, or your arm, you, it's hard to separate the fact that you're sensing why you're moving the object or it's hard to separate out. Oh, I'm, touching the a, an icon on my screen from the fact that I feel it, right? I feel it and I see it, and, it's actually doing something at the same time. So to me, it was separating out of the sensation from the actual action. Seems like that's not, that doesn't seem more what we do. I can't separate 'em out. My body is, every time I do something physically, I have to sense it. So, I thought that was a very, simplifying concept.

but maybe we don't have to resolve this right now. You can have them separate. But the point is, if I, wanna attend to something, let's say physically by moving my finger, I have to give it a location and somebody's gonna have to figure out how to move the finger, right? And, and if you wanna say, oh, now I wanna grab something by moving my finger to that location, I, you wanna call it a separate operation, that's fine. But it's seems I guess, in some sense it's outside of the, the, learning module that's more into this, subcortical stuff and how we do that could be done differently. Does that make sense?

yeah, definitely. I'm, not trying to, have action independent of sensation or anything. I'm totally on board with them being a completely combined system, and it's probably just a very conceptual distinction. but in some sense it becomes, as long as that distinction is kept outside of Monty itself, that's the guess. The question is, in the brain, the, parts of the brain that control the movement, your limbs, the same parts that get input from limb. how that's implemented outside Subc. Cortically, I don't really care. if, it starts affecting how we think about the learning albums, then it makes a bigger difference. so I guess I'm saying I'm, indifferent to how we do it outside, outside of the, to the left of the learning models. But if you think it actually change, it's, it, we're, approaching that point where we may have to start thinking about it. 'cause this is what we have in software right now, on the right there. Yeah. The actions that you can specify in software are like, move this thing like this is part of the body, 30 degrees left. I don't get to say, the software's not gonna take care of, gimme this location in space. It's not just gonna do the motion planning to handle all of that. We do that. The outside. Yeah. As long as it's not in the, okay. Because when, how you do that depends on what your physical manifestation of this thing is. if it was a body, like with tendon is one thing. If it's a robot with six joints that have, stepping motors or something else could do matic stuff, why? And do that. That's one way to do that. There would be no learning module would be just a technique. Yeah. The module would just say move, put your finger tip in this location and body. And then this other system would figure out independent of how many joints you have or what system you have to get it there. So that, that's like a solved problem. We can just, that's problem. There's no learning there, there's just location there.

okay, so again, I think this is a general rule. We wanna think of things like that essential module, things, excuse me. Learning modules have to be as close to generic as possible. We may not be able to there, but we wanna be as close generic as possible. And when it comes to sensor modules and the ability to move the sensors and to manipulate things, that's all that could be very specific to the implementation. There could be multiple ways of implementing it, multiple different ways doing based on what the, system looks like. again, I mentioned that you could have a camera in the corner of your room that doesn't physically move at all, but you can still give it directions to say, attend to this location. And it can do that without physically moving anything. Or you can have another camera that turns and looks, or you can have another camera that, that has a little robot and runs over there and looks. I, me, it doesn't really matter how you do that. Okay. But I can listen to the rest. Thank you.

so are you okay with that answer? Is it, yeah, I think I, first off, I learned a couple things and I think I'm okay with the answer. I just wanna make the. Diagram, clear. So is there, this diagram doesn't show any, anything besides sensor patches. It just shows them pointing directly to the learning module. So it doesn't show any way it's moving the sensors, or it just doesn't show the entire system. Doesn't the system, Yeah. Actually I didn't notice that till now. I, I grabbed this and I thought it was a different image. Oh, on what, this gets to the point we're making earlier, that the, a sensor of modality has complexities of what it does, and that's not reflected in this particular picture. This just sounds like you're just taking something from a image patch, putting into the learning model.

okay. So in, in the way you're thinking about it, sensor module includes it's the sensor, it is sub-cortical modality specific. Processing, but it also includes, access to a motor system of some kind. Yeah. think of it this way. The sensor has to tell you, where in space it's sensing, right? That's one of the things we define. It's not just like what's on the tip of your finger. It's where's your finger relative to the body that has to be provided, right? The eyes have to tell you where it is relative to the retina, right? What distance and so on. So we're already inputting location information and then now from the sensor that doesn't come from the retina itself, that comes from a whole bunch of other body parts that are somehow informing the cortex. Where's this eye pointing to, right? The moment. And where's the finger relative to the body, which is a very complex calculation, but somebody's taking care of it. and then yet all we do is now is say, and then the reverse goes. I can tell the sensor I want you to attend to this location, and then it's gonna do that. So it can either say, here's the thing, I'm, here's the location I'm attending to, or I can say, here's the location I want you to tend to. And so there has to be some sort of processing stuff that handles that. So a, cortical column, which is now gets to a learning module, doesn't do that. Each of those have their own motor output though, so Yes. So what I'm trying to understand is the map of those motor outputs, sensor module, motor outputs, and then who gets the final say so, so we know in the cor cortex there's these layer five cells of a motor output. No one knows what they represent. They know the motor output 'cause they project someplace that makes things move, but they don't know what the encoding that they represent is. And this is always this, I don't know of anyone who's ever, maybe it's someone studied it. I'm not aware of it.

so if I have a bunch of cell active, what are they saying? Are they saying contractive muscle? They don't look like that. 'cause they don't project to muscles, they never project to muscle. Nothing in the cortex muscle. So the cortex doesn't contract muscles. The cortex sends a signal to someone else who figures out what muscles of the contract. And so what occurred to me recently, a possibility is that what these layer five cells are doing is they're passing the location information, not a muscular information. And because we know that the cortex is gonna operate in location spaces, right? So says, so that's possibility we, but we do know it's not sending out.

So it's not saying these are the muscle sequences to do them, it's not doing that. Yeah. but we think the subcritical areas are doing that we know they're because someone's kind. Yes. We absolutely know that. So the per callus has, neurons that project to the muscles that move the oxygen. It's just they do.

and is there any interplay between those? are the outputs those layer five neurons in cortex, those going to the superior? Yeah, yeah. Yeah, they do. Exactly where they're, yeah, that's the point. The point is, you look at these layer five cells, they all point, they all project some place, that ultimately results in movement. Like even the ones, that in your, what we call used to call motor cortex, which is really just somatic cortex. They may project, if they're gonna move your foot, they'll project down to some part of your spinal cord. Where there's a, plexus of neurons and that group of neurons then somehow sends a signal to move muscles. but those are coordinated activities, right? Those, sets of neurons in your spinal cord are able to do complete reflux reactions, like how to move it on the weight from something like that. So in theory, what you're sending to that stuff is, we don't know, but I, lately I've been thinking like, oh, the, simplest thing to do is pass it in, vocation information and let it figure out what to do. So I'm thinking maybe in this diagram there are arrows there, motor arrows coming out of learning modules. Yeah. Back to sensor modules. They're, yes. Call motor. they're, but there might be location signals, right? That generate behaviors. Yeah. Good distinction. so location arrows coming out of learning modules, back to sensor modules, and then like actual motor arrows from sensor modules down to whatever moves Yeah. The actual body. Yeah. And who knows how that's happened, We, wanna let that be the up to the specific system with design. Okay. And then is there a single, right now we have this one subcortical motor area who's responsible for all moving parts? Do we have really one of those or do we have independent sensor modules that each, say, move my left foot here, my right hand? I'm not clearly I need to be able to move my independ independently, move my sensors independently. You could say you have, for each sensor you have a processing block that computes the appropriate with a command. look, I can, touch it with my left hand. I.

yeah, so just the question is just I'm consciously making that decision. There's a difference between look at the mug and touch the mug as far as, mo Yeah, but I'm saying even touch the mo there's differences between use my left hand and my right hand. So you can think on the spectrum. You have eyes and ears and different parts. Your skin can all attend to that spot. So we're gonna attend to that spot. Who does it actually, right? who's actually gonna do it? Is it my left hand or my right hand? or just looking at it or just looking at it, or both? I, so my point is there is some sort of cortical, it's not just, there's a signal that's thrown out there and say, tend to the spot and go at it. Somebody figures it out because I can consciously decide to do it with my fingers or my, eye. I have some control over this, so I obviously it in the columns in V one, projected as a per which control eye movements, they don't control your hand.

it's not like there's just one processing point that says everything goes through here.

Okay. Yeah. So we might, we could implement it that way, but maybe that would be effective, but certain not the way, again, I'm seeing my back of my head saying, do we have to do it that way? that's way seems to be brain do it. Maybe we can do it a different way. I don't know. On a simple robot, you might have a single subcortical section that does figure that is all out for you.

Yeah, I would still, pretty strongly be for not doing, for separating it from the sensor patches, the motor movement, because I, still feel like motor commands are not necessarily modality specific and not necessarily patch specific. You can say, I want to attend to this location, but then the, motor area can figure out. What to do to put a sensor to that location. and I feel like it should be separated from the individual sensor patches. Yeah. But just do you, think that you should separate out vision and touch, or should they all be part of the same system too?

I, the motor area could decide what would be the mo the best way to sense location. Well, neuro neuroscience tells us that's, that, that's not right. in neuroscience, every column that we know of has its own motor output. It goes someplace subc quarterly. And so the visual regions projected the spray ulu and the touch areas project to, I forget what it's called.

anyway, so, it, this division of labor is. Is largely taken care of in the cortex, in the brain. yeah, but also sensors are not the same as actuators in the body. just, I guess we, we already had that discussion today and I, that's not, I don't think that's the way, think about it from a neuroscience point of view, but I'm okay if you wanna think about it that way. But, there, here's already a, problem with that, right? If, I can't specifically have, decide to use my left hand for something versus my right hand for something, if I, if that is not part of the money system to make that decision, then you're gonna be limiting yourself. it feels no, you, can still have this part of the Monte system. It would just not be part of the sensor patch. Oh.

So the learning module can, still output information about this. It would just be processed by a motor area instead of a sensory area.

all outside of module. Yeah. Alright. I guess that's, I guess that's a distinction we can let live for now. Just bear in mind that, it, it might lead to problems future. but I don't, really care. it not, I don't care, don't see a problem if commonality always fuse it later. yeah, it just, if you look corticals.

So the tactile columns are sending information back to moving the fingers and the, eye columns of sending information back to moving the eyes.

and if you wanna say, okay, it goes through a separate channel for actuators, do you consider the eyes an actuator? Ah, they're moving them. Yeah. The muscles of those would be the x-rays. As long as you come up with a system of works, I guess that's fine. this, again, I think anything until outside of money per se, in the, that's modality specific and implementation specific. We can try lots of different things and, see which works and what doesn't work, what the limitation. Yeah. I guess one reason to separate why I feel like separating is important is because motor commands are not just for sensing. They are also for changing the state of the world. They are for communicating. You can do all kinds of things with motor commands that are outside of the sensing. But one could argue that when I move my hands, I'm doing sign language, I'm moving my hand. it's the same. I'm moving my senses even not sensing anything. It's the same things as I'm moving. So whether you're moving it to touch something or whether you're moving to just put 'em in right positions, it's the same things.

I, but I think we're, I'm fine with this. It's alright. I guess I'm not gonna object to that. It just, my prediction will come back later and decide that there's some problems with it. But I'm not certain about that. A quick question, do you, the prop, what's the word, prop sense of, nervous system where you get feedback as to the actual position you actually achieved. Where would you place that?

Good question.

I think we have an answer to that in the current Monte architecture. 'cause there's, there's always like prediction happening. So you take, you decide an action, you predict what's gonna happen, and then you actually take the action and then you update. So that's, in the elevator. You're updating something else other than the sensor you moved into position. There's something else that senses that, oh, I've achieved that goal. And then you have correction to that saying, oh, I really meant to move it a little bit further. But you, are able to put your arm out at a particular angle. You have feedback that in fact you've achieved that, that angle to some degree of fidelity independent, whether you actually touch an object to give you the feedback that you actually got there. There's something else sensing whatever the, whether the goal of whatever, muscular action you sent actually was, achieved. And in fact, it's all in dynamic tension too. So I'm, just curious, Kevin. Yeah. The way I've been trying to, I've been trying to isolate these things. So one thing we just assume is that we know where our senses are, the pose of sensors to the body, which is, that's an assumption we're making. We know, and the prop system, of course, plays a big role in.

But if we were just saying, all we need to know is where the sensor is, it's POed relative to the body, then we can just offload that to someone else to figure that out. It is not, it's not part of the what's going on in. Okay. That's, but there's problems with that. Lemme just say there's problems with that. I wanna move my fingers from one location to another. I have to know if my elbow's gonna hit something along the way, and if I do, I have to then turn my hand a little different to get my finger over there. there's a lot of weird stuff going on, so I'm not arguing that sufficient. but the moment, that would be a, placeholder to say, let's, just assume we know where our sensors are relative to the body. We have the ability to move, we have the ability to attend to a location, via movement of the hands and the eyes. and therefore we don't have to worry about the appropriate sive system per se. There has to be something equivalent to it that's providing this information. But, I'm not sure I agree. I wouldn't say that's the end all because I see problems with that, that somehow our bodies know how to avoid all, if I'm moving my hand, it, knows how to avoid all the obstacles I'm gonna do to get there. I don't, you know what I'm saying? it's not just where my finger is, it's also where my elbow is and where my arm is and my body and all these other things. No, I, agree. And, that's gets further along the line of putting that motor system as a separate thing. Yeah. That would be the, that would be the easiest thing for us. Let someone else worry about it, right? Yeah. Yeah. Yeah. It depends on the specification, right?

1218. Yeah. Thanks everyone for attending some of the discussion.