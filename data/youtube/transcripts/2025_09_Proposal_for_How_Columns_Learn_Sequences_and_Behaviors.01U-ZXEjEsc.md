I'll just jump in real quick and yeah, I don't mind any in depth discussions, or want to get this over with quickly. So definitely feel free to interrupt if you have questions. so yeah, basically try to make some visualization of how I, what I took away from last week's research meeting about representing time. it's a mix of in the brain and in Monty. Hopefully it works. and yeah, maybe you can just let me know if you took away the same broad concepts or other things. So as a first example, I took a melody, So let me just explain this, figure, there's, can you see my cursor? Okay. this is what's being experienced. There's time and I'm gonna move this kinda gray bar towards the right and it goes through that melody. and then up here, this is the kind of shot clock or a stopwatch kind of mechanism that we talked about. And I think I'll refer to it as a stopwatch mechanism now because it I visualize it like a stopwatch where basically he started this arrow moves through it, and then you can reset the stopwatch lap to lap or reset and it, and, thus arrow goes back to the top and starts moving around again. And you can reset it again. It goes back up and moves again, reset it, it goes back up and so on. And basically, whichever point the error is currently pointing at is the input to layer one. And that basically represents the timing since the last event and what, I mentioned is this clock gets reset with every significant event that was sensed. for example, an example of the melody, I'm basically saying each of those nodes is a significant event. So whenever it hears any node, doesn't matter. The, which note it is, but any note adheres it resets this clock, and then as soon as it resets, it starts moving again clockwise around. And the, where it points at is the kind of neural or context representation. It goes into layer one. and then the specific, is it, I'm sorry. Yeah, go ahead. You say, and is it a circular representation of time or would it, because that I guess implies that if enough time passes it goes back to zero. But I'm assuming it's more like a, it just trails off and you, get slower and slower time cells. Yeah. So I haven't really, I've in this example, avoided this issue of what happens if there's no event by the time it gets back to the top. I think some, one thing we talked about last week was that as it moves forward in time, it, the threshold for a significant event gets lower and lower. So maybe something really minor gets detected as a, as an event and resets the clock. Yeah. or just something that at, if it gets to the end, it just resets the clock. but basically it's not really circular in the sense that once it gets here, it keeps, it, it starts over again. It's really like to get back to zero, it's, this is a reset. Yeah. If it gets back to zero, it, it shouldn't really get to zero from this last point here. Cool. okay, so let me just step through that example. So we move, we hear the first node and which node it is, basically the feature input to layer four. and then timing since last event in this case is just the very first one, since this is our very first event we're sensing. And so I added this other thing down here, which is which element in the sequence we're currently in. And right now we're in the first element. We would associate the time since last event. And the feature at that first element, we move forward, no node detected yet. We're here. This is currently the input to layer one. move to the next note. Not, this is shortly before the note. this is the input to, to layer one. And then we hear the note clock gets reset. we get the feature input again, and we have, this kind of reset signal triggers that we move to the next element in the sequence down here. So now we're at second element in the sequence. And then we associate the timing representation that we got just before that event happened, and the feature with that element in that location in the sequence. so then again, we move in time, move forward. This is the input to layer one move forward. Now this one is the input to layer one move forward. Okay. We hear the node, we reset signal. Reset signal means we move one more forward. In the element in the sequence, we associate the last kind of temporal input we got. and the feature with that element in the sequence, same thing happens. Next note. it's really the same thing in this case, we are sensing a different feature and, associating that different feature with the same element in the sequence. Now, this one is the only other interesting case 'cause it's, still the same mechanism, but just, we are moving a bit further down the, stopwatch. And as you see, as time passes, the in input representation to layer one changes, it gets like brighter. It's a bit hard to see 'cause I, want to make it look ex aesthetically pleasing, but it's, not the best I guess to distinguish, but it cha it changes at every step until we get to the next event, which is, yeah, sensing this note triggers a reset, which means again, we move one further in the sequence and then associate again, the feature we get as input and the time since the last event. with that element in the sequence and that goes on. Does that kind of make sense? Is that kind of how you were thinking of it last week as well? Or, yeah. Yeah. It's nice animation. Yeah, I think that generally fits. I think it was still an open question in my head, how we associate like what exactly is as being associated that time thing with the, what you call it, with the feature. I think this is also to me the most intuitive, but, but I don't know if this is what Jeff had in mind, just in terms of things like if we start speeding it up or slowing it down. yeah, so it's speeding it up and slowing it down. It's basically this blue arrow. I shouldn't have made it blue 'cause it's not really real time. It's not this error. It's this one can speed, can turn at different speeds on the clock and it turns at different speeds, but it still goes through same sequence of neural activations. So whatever was active at this point predicts that feature in the sequence. And you can speed up. Or slow down this blue arrow on the clock, basically. But it, it doesn't change anything in, the representation here. It just changes like how, when, which of these purple features are active in layer one. Yeah, that makes sense.

So just to make sure I'm clear on this, scheme, the clock gets reset at every note touch, right? Yeah.

At every significant event. So I have another example with the stapler that maybe goes into a couple more different cases. So yeah, that might help too with understanding in a moment, but yeah. do you wanna ask some questions first? Do we send back, do, we send back the tempo to the thalamus to do stuff or, is this, it looks like this would figure out how to predict based on like basically infer the pace, or the temple and predict the next step when it will come. Yeah. So basic, yeah, go ahead. I was just gonna say, does it send something back to the thalamus so that the next prediction is like, doesn't have to infer every time or, I'm not just a little confused about the template. Yeah, so basically the idea would be that this is in the matrix cells, in et thalamus it projects to layer one. and then the column would recognize if it gets a feature input, like basically the next feature it expects in the sequence, if that appears slightly before the expected neural representation or after that tells it, whether it needs to be slowed down or sped up. And since, if we think of these as SDRs, it would be smoothly shifting. So it, I think it should be able possible to tell like whether it was before or afterwards. And then that would be a signal that gets sent back to the matrix cells to that kind of clock and makes it go faster or slower. and this is a global clock, so this would then change the neural input, representation inputs to layer one of all the other columns that projects through. So it's enough for one of them to say, oh, you should be faster or slower, or you should be reset. and that kind of gets, changes that global signal everywhere and synchronizes everyone. So like for example, also detecting a significant event could be anywhere, like with the stapler example, I'm saying actually like the sound of the stapler stapling can be an event. that's not something that's represented in the. Morphological model of the stapler at all, but could be detected somewhere completely else, but it resets the global clock. Okay. another question is the element in sequence, what kind of representations Is that like a, 1D grid cell or, yeah, I was thinking of this as like basically 1D space and you move through that space, you can only move through that space in one direction. So always move forward in time. And the kind of displacement that moves you forward through that is the reset signal from the clock, like getting the first in sequence again. So that would be the movement. I would just like a, movement.

Yeah. I would basically be one movement. And I guess if we, this is with time, but if we have actions, then you might be able to move in different there in two directions through sequence. but, haven't thought through that too much yet.

It's really hard to try to do a melody backwards or something in, in your mind or move through it. Yeah. So here we don't have actions, but if you have a hinge, you can imagine opening and closing it in both directions, and maybe it's like a double sequence. You need to learn both directions or you can go through it and. Those directions, I'm not sure.

But yeah, in terms of things that we learn as discreet sequences, if they are long and complex, it is really hard to do them backwards, like as a sequence. opening and closing a stapler is extremely simple. It's it's literally just doing that. Whereas yeah, saying the alphabet is really hard to do backwards. Or even being like, okay, how do I cook lasagna backwards? I don't know. It would just be really weird. you'd really have to Yeah. Almost think forward and then do it backwards step by step. Yeah. So yeah, it might be that this is just something you can move through in one direction. And I guess, yeah, one thing to note is that is also discreet. So it's not like continuous 1D string, it's like discreet elements in that sequence, but then within each interval on there, you can have different temporal representations that, that tile, that interval space. And yeah, I think maybe I'll move to the stapler example now 'cause that shows a little bit better.

so this is a, an object behavior with events and also continuous changes between those events. So before, like there was no feature change between events, every feature corresponded to a discreet event. With moving objects, there's often like a lot of continuous change. So I think this mechanism would be able to deal with that. And as I started making this animation, I realized that using different shades of purple wasn't very, useful. And so I added some kind of colored rings around the first ones at least. So you can distinguish them a bit better, of what's the current input to layer one? So here's a stapler and the red, yellow circle represents the sensorimotor patch, of where it is currently sensing in the world. And here it shows where it things it, it is in the objects reference frame. So this is basically the kind of morphological model. it has learned of where the stapler exists and what features exists at those locations. 'cause the layer six A locations, and so here we're, at the first time step again, we got the first representation as input and we're detecting kinda a flow pattern in this direction, this change. And so we're associating that change with, this time since last event at this element in the sequence at this location. then the stapler moves a bit. We detect this change. We have a different time since the last event. We have not detected a, significant event, so no time reset yet, but since there is a different representation up here already, we can still make a new association with this, slightly different flow pattern we detected. And then we move further, it, moves down again and again. We have a different representation up here in layer one. So we can associate that, different change pattern again with this location in the behavior model. then we staple, which makes click, we detect a significant event and it resets the clock. And this reset clock signal makes us move one element further in the sequence. So now we have a new reference frame at that element, at the sequence. Yeah. Kinda like the new state. Yeah, exactly. This is new state of the object. everything can be at different locations again, and we assume that we stay in the same location within that reference frame. Like it's still, it's, it's still the same reference frame, it's just conditioning what features we would expect at the, locations can change. and so the stapler opens, time passes, we get the orange representation and all the flow pattern is moving upwards. We associate that upwards pattern with this location at this time. Event time passes. Again, we associate this, in this, with this location and the staplers reference frame.

and yeah, that, that would be basically it. That's the general idea. And then the learned model for this kind of behavior I just showed would be that there are two states or two intervals, and the one state is, has three different durations since the last event associated with it. There's, at the event, short duration afterwards, in the longer duration afterwards. those are the representations in layer one that continue to change as time passes. And each of those has a different flow pattern associated with that location in space. Then significant event where in a new state, and that one again has, two different, features associated with it. Both had slightly different durations since the last event. And yeah, during inference we would basically. In addition to testing different orientations of the object, we would test different states. Like we would start hypothesis of being in a bunch of different states and then try and match, see, which of those match the features we're currently sensing. but we don't have to test the different durations since last event, since those are externally provided by the GLO global clock. Also during inference, so the global clock also ticks during inference. So during inference we already get this kind of red, orange, yellow signal. So it tells us like, which one we should expect right now, do we just do inference on these discreet, intervals or states?

Yeah. Yeah, that makes sense. And, is there a reason the arrows are shown superimposed rather than kinda like at different positions in the reference frame? Because it's all still at the same location, so the sensorimotor hasn't moved.

it's, always at the same location in the reference frame. It's just at that location we're observing different changes at different times.

Okay, interesting. So the sensor's staying totally still? Yeah, if the sensorimotor would move down here, then this yellow one would move down here and we would associate the changes. Detect there with this location in the reference frame? Yeah. Yeah. I was just thinking because like sometimes we talked about with the smooth pursuit and stuff, that it would more create a path, but then yeah, I guess to get the input feature to change in that case. And yeah, I tried to leave Smooth Pursuit out of this example because LA last week we said that we wouldn't kinda start with that. Yeah. And it's complicated because there, the sensorimotor is moving and the feature is static, but we want to interpret it as the feature moving and the sensorimotor not, so yeah, this, I thought this was the simplest example to illustrate it because it shows how like at the same location you can associate different changes at different times. Yeah. One thought is just whether, I'm just trying to see what my eye is doing. I think to a certain degree, maybe when you're not doing smooth pursuit, what you probably do is yeah, a bunch of secs that are very short and, or so you would you'd be moving and then, but you'd be pausing at each one, so you'd catch a glimpse of movement and then you'd move. it's not that important. it's obviously not that relevant to the time thing. It's just. Trying to think through how we represent the, the behavior reference frame, but yeah. Yeah. So basically I wouldn't, if I implement this now, I wouldn't change anything about how we represent space or the reference frame here. It's still like you're at a location on the, in the staplers reference frame and you're associating changes at that location. Now there's just an additional conditioning on what changes you expect there based on the time since the last event and where in the sequence you're at. And then if we were to do like supervised learning, for example, we could move the sensorimotor as much as we want and just tell it like what is currently this, the element in sequence in time since last event. It, would learn it correctly. It's, just gonna get complicated once you don't tell it that. And for it to consistently infer, but it shouldn't be that hard, it just needs to be able to detect the significant events somehow, or someone needs to detect them.

Yeah. I'm trying to think of what really makes a significant feature. It's, the way you described it here is, like a. a sound of the click or, so I've been yeah, I've been thinking about pendulum, pendulums, pen, pen, pendulums. And thinking like in that case, a significant event is when there's, it basically goes to the edge and there's a moment where it gets, it's still, and then it goes into motion again. Yeah. And I guess this is the same kind of deal where not only is there a period where, a point where it's at rest, but then it also reverses direction. So maybe it's like the fact that you go, you, are, you do have some sort of expected motion, but then you hit some limit and then you have to risk reverse direction. Is that something we could use to like, qualify something as a significant state? Yeah. Yeah. I was thinking about, saying that, the significant event is just the, change of direction. but then I thought it's maybe a more interesting example if the event is detected somewhere else, just to illustrate that this clock is global and can be reset by a bunch of stuff. And the sensorimotor itself doesn't even necessarily need to observe that, that event.

but yeah, I like the idea of almost yeah, the smoothness of the prediction. So it doesn't necessarily need to be like reversing direction, but if it's just like suddenly it gets hard. Or like suddenly the nature of change is just different and predicting is you can't just predict based on almost like low level, first order stuff. Then you would chunk it and be like, okay, this is now a different kind of sequence of behavior or something. I think Ramy you were gonna say something else? No, I was gonna say, yeah, I noticed the same thing as, Scott, that maybe it's a, it is the key frame where, the object is, is just staying stable. Like it's not moving. and I was thinking whether, that key frame is basically feeding into the behavior, basically went to Oh yeah. Time. Like basically the morphology model of the stapler recognizes the closed state. Yeah. I think a bunch of things like that could be a, significant event. it, could, for example, if the clock moves much farther and we decrease the threshold for significant events, I could imagine events like the orientation aligns somehow, like in a straight or 90 degree orientation as significant events. Kind of if you see, an object just smoothly spinning like this, you might just artificially set events when it's like upright or sideways, and That would, yeah, it could be really minor things, that, that define these events at some point. it would just be important that it's consistent events that you see every time you see that behavior and that can then reset the clock. But I also like what Neil said about the prediction error being a, a change, like a boundary, a change in, between moving to the next element in the sequence.

it seems very plausible.

Yeah. And it does seem like to the other point, like when humans see any kind of repetitive behavior, you do develop, I feel a pulsing representation of it, where it's like, it doesn't feel truly continuous. It's not if it was truly continuous, then you would just, you wouldn't even really perceive it as repeating. You would almost perceive it as constantly changing, but it's the fact that you notice that it's cycling and it's the beat of that cycling that kind of naturally emerges in your head. Yeah, it's like with the thought that spins on the circle when you know, when it laps, you know when it starts repeating again. Whereas if you look at like a fire or the ocean or something. There isn't really like a point where you say, oh, now it's, it starts over again. Unless you watch like one of those YouTube videos that have like smooth jazz in the background. But yeah, fire is a good example of something that doesn't feel cyclical at all. Yeah. Yeah. I guess going back to the time cell thing, when I was thinking about the pendulum during this last present presentation, if we imagined the ball started here at one end, you start the clock and it goes like this. Let's just forget the right side for now. And then, the time cell is clicking down. And then, but eventually when, we get back to the beginning, we, if this thing had learned the association with that early time cell, the state is now saying, oh, I was al I was already associated with the time cell and it was at the beginning of the sequence. Oh yeah. And so now we can re now we can restart the sequence because I've already got, I've developed an association between what the time cell is and what this morphological state is and that Yeah. And that time cell was like an early sequence time cell. That's what I was thinking about for detecting cyclical, yeah. Yeah. That's a really great point. actually basically like just. This cell down, this location down here. if it sees a free feature at the beginning of the interval, it changes and then it sees the same feature again. That, that's the signal that it's back at the beginning, right? Yeah, That's your, re like reset, like a cyclical reset signal. Yeah. I like that. That might be the easiest mechanism actually.

is that, events signal, reset a vote?

no. I don't think voting would play too much into this because basically this, since this is a global clock, it's already broadcasting widely everywhere. So I don't think we would need voting anymore for, a significant event detection. we would still do voting on which object we're sensing or maybe also where we are in the sequence, but we wouldn't vote about whether the event has reset or not.

when we see, a sequence like a, B, C. DAD or Abhi, like a sequence that has a repetition of a, for example, but it's not really cyclic. It doesn't repeat the same thing.

the second, A should not be bound to the first element in the sequence. It should not know that it needs to repeat it need, it basically needs to, because the notice sequence is A, B, C, D, A, D, not a, B, C, D, A, B, C, D, A, B, C, D. You get what I'm saying? It's, like it's a higher order. Yeah. I think, yeah, in that case, like basically each letter would be one of the elements in the sequence. And I guess the reset I was thinking of right now was like, what triggers moving one forward in the sequence, in your example, like each letter would trigger moving one forward, whereas, yeah, I think that's a good point. Like it doesn't solve us knowing when this whole sequence restarts. It just solves if we only have continuous change within one of those intervals, how do we know when that kind of starts over? Yeah. I think the event stuff plays into this quite a lot.

if we do have a continuous stream of A, B, C, A, D, F, or whatever, so it's a higher order sequence. There's no rest period in between that sequence, then it is quite hard to learn that like higher order sequence. But if there was like A, B, C, A, D, F break, A, B, C, A, D, F, then that break gives us an indication that first a is the beginning of an order sequence. It's a significant event. And so that second, A, when we come across to it, maybe we would associate it with a, eh, I feel like it helps somehow, but I'm not sure. Yeah, even with music, it's like you often play the first note a bit louder, bit more of an attack on the first note, or you have some kind of beat in the background, that tells you that, what do you call it, the four by four rhythm starts over again. yeah. Yeah. And I could imagine you can almost imagine like a threshold for kinda like you were saying Viviane earlier, like maybe that gets lower as the sequence gets longer, but you have some threshold for saying, okay, the sequence is repeated. And yeah, with a kind of challenging sequence like that, like it might almost be a bit of kind of trial and error where you're trying to predict what's happening and you might not initially know where that repetition happens.

I don't know. I can almost imagine this, if you were like reading like genetic sequence or something, if you were just doing that and you would see like a bunch and you generally, you probably wouldn't initially reset it at all, but maybe at certain points you'd be like, hang on, is that a reset? And then you'd update your hypothesis about where that reset point is, and then you'd keep going and then you'd be like, oh yeah, maybe that is the reset. And you could almost like tune that point to align with how, yeah, and like pendulum, like when you first see the pendulum swing, you don't really know when it's gonna stop and change direction. But then you s yeah, I don't know. Yeah, like HTM solved the higher order sequence thing. So like it could correctly deal with like A, B, C, A, D, F. I don't think it does. it would treat that second A is different than the first A because of the It was, yeah, it does, that part. It doesn't resolve the issue of how do you in an unsupervised way detect the start of a sequence versus the continuation of the longer sequence. So it seems like we could just augment that logic as it is. Instead of thinking about as, completely independent identities, there's an SDR R for that first, A and a SDR R for the second. A and maybe they may be similar, but when we're talking about resetting a clock, the similarity of that, I don't know. I think similar, different, it's tough. I think that representing the same feature differently solved by re by having the reference frame down here and moving through the sequence here as although the same letter A might be coming in it, it would be at D we would be in a different element of the sequence and then predict a different next element in the sequence. But then, yeah, like Neil said, I think it, it wouldn't solve the, like figuring out whether the a was in the middle or, if it's restarting again. for, what it's worth, like revisiting the warning paper that you brought up last week, there were, when I was in the lab and was involved in studies where we did have sequences like A, B, C, D separated by a gray screen versus just let them run without a gray screen in between. And there were like, is, it's actually quite hard for the mice. To show like the kind of prediction errors that you would get when you don't have that gray screen in between. It's possible, but having that sort of reset teaching signal or some clear event indicating the beginning of the event has like a, quite a large impact on whether or not like prediction error, like signals emerge afterwards. interesting. That's, great. That fits perfectly with, this idea. Let's see if I can find the paper.

Oh, by the way, just one side note. the pa, the time cells that Scott out last week had final resolution at shorter time intervals, so that could be easily integrated here by basically putting more neurons here at the beginning and then making them less and less towards the end. and there wouldn't be need to be any change in the learning module, just have a, of course our resolution, in the beginning, a final resolution in it, would change more frequently here. So it could, represent, more detailed differences in the changes.

all right, let me just, I'm gonna put this in the chat here. if you go to figure two there, it shows, oh, if you wanna share your screen, I, stopped sharing. Okay.

Screen.

So it's this, one, oddball evoked responses, yada yada. B one. Oh, hey, look who the first author is. Yeah. Yeah, that's cool. so here's one, showing the, with and without Gray essentially. So you have A, B, C, D, A, C, D, A, B, C, D with gray, A, B, c with gray. And the, these are the comparison of the responses. you don't have to worry about OSG or RPG, too much the different stimulus types, but essentially, yeah, that's cool. The kinds of deviations that you were able to get from that 2014, I think it was 2014, good morning paper are, pretty different when you don't have, intermediate gray screens to in emulate, indicate stimulus onset or sequence onset. I, I think this matches what, Niels has been talking about the prediction error, because now every time gray, nevermind, it's, you're always learning the prediction from D to A or D to gray to a, so it, it's not about prediction error. Does it, the gray, period change or is it always the same period time period, like a second or something like that? I think, but yeah, you, were saying with the gray, it's better prediction right? they understand the sequence better. Yeah, definitely. if you don't have that gray period, you're essentially just doing like markoff chain learning where you're, you're just learning transition probabilities between sequences and it's just a harder task to do in an automatic way than if you have something to set the clock at the beginning, which is actually why I was thinking when you're originally going over the clock and there's a new event being set at every single note intuitively, it feels like the clock is set at the first note. And then, we tick along and learn the few notes until maybe there is that rest period or something to reset the entire event clock. And I, kind of wonder on that, whether it could be a bit of both that like, 'cause it feels like both are useful and yeah, whether you'd almost have in the same way that we have different spaces for different, for objects that we have like different time starts, but But yeah. But this is really interesting 'cause yeah, basically. The issue with the top is like the animal needs to infer that d is the end of the sequence. It has very little, signal for that.

but, and sorry. And so Blue, just to, to interpret it, blue is, high firing rate because of a high prediction error in the continuous case or talking about here? Yeah. Or, I'm just trying to see of the traces, what's compared with what, or like what should I be looking at? different days. So there's a five day learning regime, so we're actually not looking at prediction here as here we're actually learning, looking at the amount of like adaptation over days in this particular case. So when we're saying day in the continuous case, it says here blue green is day one, blue is day five. So it's, it is saying that there isn't that much of a difference across days when you don't get a teaching signal on the gray. But when you get a do, get a gr, a gray signal in between then, like the day one responses are small, but they grow quite significantly after. Yeah. Okay. Multiple days of exposure. And it's thought that the, this is the kind of adaptation I have to dig into this paper 'cause it's been a while. We did so many experiments for this paper. I don't actually remember what, what made it into the final version. so I have to review it. But it's thought that, the, kind of adaptation, the kind of learning you get from having this gray signal is what helps to drive the dictionary signals that you see in the sort of 2014 paper. and that they just don't seem to emerge, quite as robustly when you don't have that gray signal when you try, to learn four days of non, continuous sequences. So anyhow.

Yeah. That's cool. yeah, that's, really fitting.

I have to remember what went outta this. Yeah, there's a lot of, I, I can just put this on the chat or something also, but there's a lot of timing related stuff in this paper as well, if you're interested in, that kind of thing. Yeah. okay. That, yeah, that was everything.