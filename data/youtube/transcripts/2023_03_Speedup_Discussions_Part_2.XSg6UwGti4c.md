Yeah. So this is a part two of the discussion we had a few weeks ago about, how we might speed up the, some of the slower computations in, Monty.

and yeah, just as a short reminder, last time I showed this kind of analysis of which functions take about how long, to run at the moment. And we talked a lot about, KDTreeSearch and kind of possible alternatives for that. and then I also went over some of the matrix multiplications and other matrix operations that we are using. but yeah, we didn't have that much time to talk about that and, so I thought I'll start off with those. So it looks like the KD tree search is The biggest thing to look at but all these matrix operations actually add up to quite a bit as well. And the thing is that we can probably apply if there is some way to speed them up we can probably apply it to all of these.

I just thought it could be worth it to look at that a bit more.

How did we end up on the tree search? we had the suggestion to do a table lookup, but then it didn't sound like it was going to work. Where did we end up on it? Yeah. So the table lookup, does speed up the search, during inference. The problem is, it slows down, like the building of the table takes a long time. So if we are just focusing on speeding up inference, It's definitely a viable option, but if we are, if we want to keep the option of being able to learn continuously fast, and being able to update our models often, then it's probably not the best solution. So Does it provide any advantage in there, or is it actually worse?

Yeah, for updating it's actually worse. so overall it's not a good idea. Okay, that's unfortunate. It's all that didn't get anywhere. Okay, all right, next, now we're looking at the next best option. Yeah, at some point it might be good to go back again to the KD tree and dive into it, at some point, but yeah, the table lookup is really just an inference thing, I think. Yeah, so I found another paper that looked promising. They use a special way to search architraves, I think they use, that is especially optimized for searching 3D point clouds, which is exactly what we're doing. Oh, I know. I might have a closer look at that one, but unfortunately they don't have any code online, so it would be a bit of a longer time commitment to try that one out.

yeah, there might be some other alternatives to the KD tree search.

So spatial queries, I think it's like R trees or something?

their implementation, you mean? Yeah, I think that's something used for a spatial database, in R3. I think I've seen this R3 stuff.

Yeah, I'm not a hundred percent sure what, they do. They somehow, sort it in a specific way that it's really quick to search through. which is also done in KDTree, but apparently it's optimized for 3D point clouds.

Yeah, there are some, bounding value hierarchy, like they, they chunk them in terms of bounding boxes and then they divide amongst those. That's what I remember as well, like something like that, but that was used for databases. I don't know if that's also used for that. Yeah.

But yeah, sorry. I've seen that using GIS systems, like in spatial databases. Yeah, that's what I've seen as well.

So there must be some open source. Absolutely. There is, Postgres has it. I guess it would be good if there was Python bindings. There is Python. There is a Python version of that. How's that? Can you share the link of that? Oh.

I'll look this before, I'll see if I can find it by my new pen. That's a long time ago. Maybe you know the key words to search for. Yeah. R three A trees. There was a, that's, it was a keyword that brought it. R three For the GS systems, they has A-R-C-R-R dash three oh r. Think that what the name, and this is, related to the GIS system, right? Yeah, R 3G IS. But that's a company, I think. But I think there's a few, I'm sorry. Is this related to the matrix operation? That's the KD tree. That's for, okay. We can look this up. So it's a JS, that's what it's for, searching. It's exactly as Eric was describing. It's just a tree of spaces that you can find within the space.

Okay. All right. Yeah. yeah, I'll have a look at that. But, Yeah, at least for now the conclusion here was that we'll wait with implementing something because we don't want to over optimize for inference right now, and keep it flexible to be fast, also for model updating. And then, but the matrix operations. Might still be a viable thing to add speed up, and that would not be the biggest, computation to be removed, but still, a significant chunk of computations that could be sped up.

so I'm not gonna go into the exact applications of these again and where we use these, operations, but generally just a collection of. here's again the collection of operations that we apply, and the main point is that, we, right now, we always do the full matrix operation on, all of the hypotheses. We update every hypothesis at every step, but, In practice, we don't really need to test all hypotheses at every time step. We could just test the most likely ones. And that way, we could mask a lot of the rows in the first axis, so along, like, where I put H as the shape, which is, which can be, between 1, 000 and 40, 000, rows. And then the second one, some entries in K are also already masked, since some locations in space have less than K neighbors in the allowed radius. So those could also be masks. So overall, we have, for example, here, when we do the dot product, we have a big matrix of size H times K times three, but almost all of the rows could be ignored. Some of the columns could be ignored in those rows. And then, yeah, obviously the third dimension is also ignored if a row is ignored. So The question would just be if there are some efficient way we could make use of this fact, that we can actually ignore a lot of the rows, and in every step. So if one thing that comes to mind is if you can somehow sort these hypotheses so they're all together, then you can just do brute force. on that subset, but then, of course, you have the cost of the sorting operation, but do you, so that would be one possibility is if you could just stack them up so all the white ones at the top, and then just do those, but do you think there's a lot of change in the top few hypotheses, if you were to sort it, because if it's compute, it's, intensive to sort it and then, but then updating it could be pretty easy. Thank you. Yeah, there definitely is change in the top K hypotheses, but, after a few steps, it gets stable, depending on at how many, hypotheses you look at, so if it's just the top 10 or something, then it gets pretty stable after several steps, but it can still change, depending on, how you move over the object and when you see significant features. All right, and if it changes and you didn't have that in your top ten, then would the whole thing fail then? it's possible it just might not right now. It would just mean that a different row is now grayed out and a different row is now, needs to be updated now. If it's stable, then you don't have to worry about doing that sort and un sort again. Or it can be much faster to update it, much faster to re sort the whole thing. but you definitely want to do all of the white ones. Yeah, no, but I'm saying, but if some of, if the set of winners is changing over, time, then if you don't update it, then you're going to miss that, right? So you want a fast update. That's what I was saying. It's faster to sort it into a, or re sorting list than it is to sort from a, And yeah, if we sort it, we still want to keep like a reference indexing between which row used to correspond to what hypothesis basically. Yeah, I know like our old sparse matrix stuff, I think every row was like a pointer. So you could, reorder them very easily.

Mo, I think, in the documentation I remember correctly the calls. Marcus did something by Torch.

That was Abby. That was Abby, yeah. Yeah. I was able to do something similar with PyTorch, so maybe we can, just move PyTorch from the brothers up. We just moved PyTorch from Monty, so I don't know if we can come back to that. But I think Marcus one probably, I don't know if it used pointers for each one. It used pointers. It used pointers. Yeah. Then it's very easy to re sort. Yeah. Use pointers. I would, roll as a pointer.

yeah, I guess the question is, it faster to sort this thing and then take only the top ones to multiply, or would it actually be better to just try and move the implementation onto GPUs and do the large matrix multiplications? that's, yeah, we could, so, two questions there. One is, if you could sort it, sorting 10, 000 should be super fast.

then once you sort it, then, that matrix structure, you can use all of these operations on that. So it's not like you have to re sort it for each one. You just have to re sort it when your hypotheses change, which is the bigger loop. So internally, once you sort it, you can just do everything here, with, that structure. Yeah. In terms of GPU, I don't know, GPUs will be very fast at it, but then you have to worry about data going back and forth between the data, between GPU and CPU constantly, so I don't Yeah. Thoughts on that? Usually that's a bad idea. Yeah. if once you get on the GPU, you wanna do a lot on the GPU to amortize the cost of that native movement. just so I understand is we have h is that 10,000 that's supposed to be the length there? yeah. It can be up to between 1,040 thousand usually. Okay. And what's the, the ranking of these things? In other words, what's, the figure of merit? What's the range on that?

what do you, mean by what we would sort it or? Yes. Yeah.

so that's, that would be the amount of evidence for each of the rows, basically, and the evidence can, It can either be bound to, for example, to be between minus one and one, or it can be arbitrary, so it can, grow infinitely. Okay, but I'm, let's see if I can answer this in a different way. The, if the, let's say it was quantized, let's say arbitrarily you only have ten values that the hypothesis validity, can fall into. Would that severely compromise what you're doing here, or do you need to define level precision so that these things actually filter out to a very fine cream on the top sort of thing?

Ten might be pushing it a bit, but it should be still okay. The bigger issue is if we bind, if we bound it to a range, say minus 101 or like 10 bins, it basically means that we have a time horizon of what we can remember. so we need a very efficient policy to see all the significant features in that time horizon to recognize the object. That makes any sense? Okay, so when you're, when you were talking about sorting, are you talking, are you sorting on a scaler, or are you sorting on a, each of the dot products individually?

right now we don't do any sorting, we just, basically do the If you would sort, if you would sort, what's the Yeah, how would you, what's the algorithm to figure out the top, ones that you want to keep for this operation? Yeah, so for example, the dot product is to calculate the angular difference between hypothesis and observation. And then from that difference, we get an error, which is added to the evidence. And then we have a separate array, which is the evidence for each of these rows. And we just take the maximum of that.

Yeah, so normally it would be a scalar for basically the evidence value for each. Hypothesis. Okay. is there a, is, the bulk of the time in doing the operation afterwards or is it computing the evidence?

it's, all of it together, so Einsam, this is the top one, this is computing the dot product between the, like the angle difference. but then there are also some, other operations like compute, computing difference between observed features, getting distances and yeah, just general, like weighing the evidence. By however much we want to weigh each feature. Okay, let me be more specific. You have some amount of calculation to come up with that scalar, which hypothetically if you were to sort on and then only operate on those rows, which had high enough evidence or, all of them that had some, greater than threshold evidence. I'm trying to figure out the, how much of the processing is split between arriving at that. estimate of the evidence, and then actually processing the rows once you have the evidence. yeah, I would say, basically all of these are used for arriving at the evidence values, except for the, this one, if I remember right, corresponds to the maximum, so getting the most likely hypothesis. Out of there. Okay, so then it wouldn't help too much. Yeah, exactly. Because I was about to say, if you get down to an evidence number, and let's, make it a, say, 256 evidence levels, let's suppose you mapped into that, then you can use a bin sorter. And those are really fast, because you basically index on that evidence and just put it into a list. But if you're spending the bulk of your time coming to that point, then it's moot. no, that would be done. But it would be a lot quicker, yeah. So we would have the evidence from the previous time step, and then we only want to do all these operations on the top k ones.

Okay. Oh, okay.

Yeah, so let me At the very first time step, we would have to do all of these, but then after that, we can just take the top k evidence values from the previous time step and update those with the new observation. I see. Okay, so if, that were the Case, you can do bin sort in linear time. So that, that would allow you to at least bucket them. Will we actually need to sort the entire array? if we just need the top K, could we just stop? that's, why, that's why I was looking at what's, what makes it white, what makes it gray.

If you know ahead of time where the white ones are, then it's moot. If you're trying, if you're trying, if those, gray ones are potentials that just didn't fall into a large enough, quartile or, quintile or whatever of the things, then that's a different question.

Yeah.

So yeah, which might do is think about have a threshold, like a dividing line between, what is good evidence and what is bad evidence, and just sort them into two bins, so that after like the first iteration, you're maybe at 50, maybe after the second iteration, you're at 75, and then 80, or whatever, and you can just continue to close that gap, and you should see less and less of them showing up with The high enough, thing so that you can just concentrate on those that are exceeding the threshold amount of evidence. the problem is, that their threshold is not going to be constant. So what we did in the K Winners of the FPGA, what we did in the K Winners of the FPGA was that we did one pass through to form a histogram, process the histogram to find where the threshold was, and then did a second pass through picking off the ones that are greater than threshold.

Yeah, it's similar to what we do with the voting at the moment. Basically we scale the votes to be in a range of minus one to one by using the maximum and minimum value, and then we just set like a fixed threshold it has to be like, oh, above 0. 8 so like in the top 10 percent percentile so We could do something similar here. It doesn't have to be like a fixed number of n rows that we look at. Yeah, there are advantages in getting away from a bounded floating point range to a bounded, integer range. Because then you can do addressing operations based upon the integer values. Or lookups, or a variety of other things. So the floating point value, then you're stuck with doing comparisons all the way through.

so if you're just doing binning, like you're just like storing the index of the hypothesis in one bin being greater than threshold, and one bin being less than threshold, then you don't have to do any sorting. You're just basically, you're just binning them into one or the other.

And how do you create, how do you create this big matrix? Is this, you create it once, and then, I'm just wondering how this matrix evolves, does it always get smaller? no. Is it, you recreate it every time? Yeah, it's, always the same, this is, constant. this is all the hypotheses that, that we have. And then, we, use the, sorry, this is constant. This is all the hypotheses we have. And then this is, using the observation and the Kenya's neighbors of this location, and comparing, that basically to, the stored, the stored features in the graph. So with every new sample, with every new sensation, you have to recreate that hk3 matrix?

yes. Okay.

But h is at least fixed?

Yeah. So yeah, h is a fixed size. It's always the same from the start of an episode. and then we basically have to find the k nearest neighbors of the location in the graph and, take the features stored there, which are unit vectors. In 3D space, so three. And then, we have to take the observed vector and rotate it by the hypothesized poses, which might be different for each of the rows. And then this is that.

Is there any way you could just, do some algorithm, either sort or what Kevin was saying, just to pick the top, the white hypotheses, and then only create the matrices for those white rows? Then you don't have any gray elements. Yeah, we could.

that's true. We could, only basically h would be variable then here. And it would be much smaller. Yeah. Yeah.

Yeah. That would actually be the, easiest, do I don't think I can help, so I'm gonna go do so you can optimize your time.

Yeah. Sorry about that. no. Apologize.

Yeah. if that works then that's, then you could just keep everything else the same. It's just. You have an H prime, which is the top set that's right now, and then That works for the second and third, not for the step, we still have the tail column.

No, everything would be with H prime, once you sort it. No, but the first step, you don't know what to white sort. The first step, yeah. Yeah.

and then we would just have to keep some kind of reference to which, hypotheses the, white rows then refer to basically. Yeah. And the numbers should go down with, each iteration. You should have less and less. Yeah. Yeah. You should have less and it should go super fast. the only time this will cause problems is when you've got noise in the data and you might be discounting hypotheses prematurely, because of the, data was noisy, but then that's something that could be an optimization on a later pass. Yeah, we will, we can always come back to any of the gray rows if, the white ones become less likely because the evidence we see is inconsistent with it, then, we'll come back to older hypotheses. It would just take longer to get there than it would if we update the whole matrix in theory. We've also been talking about adding some sort of resampling process to reconsider less likely ones. and if we have this kind of sparse subset of most likely ones, we could also use that extra budget to add in like low probability ones every now and then, resample them. Boosting

Okay, yeah, I'll have to think, and go through the code to see if, there will be an easy change to make it. Right now it sounds like it should be possible to just, Before even creating these matrices, we select by the highest evidence values and then just keep a reference to which they correspond to. Yeah, in terms of what you were saying, Kevin, so would the fastest be firstly like a hard threshold? Just because yeah, we could have a variable threshold based on how much evidence we'd accumulated or whatever.

but then an arbitrary number of K, specifically the sorting bit, would that be faster or would, having, sorting the top K, presumably that would be slower? I'm sorry, what was the second one again? specifically looking for, say, the top hundred, or specifically looking for, Any that are above a fixed threshold. I would say because you don't know, if you wish, ahead of time what your confidence in all this stuff is going to be, what the range is, you might want to basically pick your strongest ones. you could use heuristics on this thing. If you're getting a bunch of hypotheses and then it's all of a sudden a drop off in what I would call confidence, then you might threshold at that point. But the point of it is, to bring all these things into a metric, if you wish, that allows you to, make that choice. typically, if you're, if anything where you can, you, something becomes a fixed K, Makes it easier because in all your subsequent array allocations and everything else, all of them become fixed. But it really depends upon how, your data, works, how it distributes, how it evolves as you get each additional, sensation. I don't know ahead of time, what the dynamics of that would be, but I think, what you were talking about earlier, which is, I was actually thinking about recommending is that if you do come up with some threshold that's derived by whatever heuristic, that the ones that fail by a small amount, you might want to reserve off to the side, as, filtering in, just like you were talking about, because that's, essentially how we work with K winners, right? is when, during training, we don't want things to fall off all the way off if they happen to just once. during training, fall below threshold, because then they're forever lost, so part of the boosting algorithm is to resurface those guys, give them, you say, okay, here's what your figure of merit was, but we'll give you an extra 10, and does that bring you above threshold? Okay, we'll try you again, randomly sort through there. So it gives you the ability to, if you wish, have a memory and re examine these things. The context changes. there could be something that's way out of band that would be, that was just deselected in the beginning. that for, if, this thing is highly non Lipschitz, there's something wild that could come out, then nothing's going to work on that. But I think the hope is, that you have, you're in a space that is, relatively smooth and relatively progressive toward converging toward a solution. And if can establish, some criteria. or some heuristic to say, hey, if we, work with, up to a certain threshold based upon some heuristic, then we reserve 10 percent more off on the side to, on the off chance that things are getting worse to filter them in or filter them in, randomly. So you're not, so you, have K plus some Delta of that reserved pool to play with. If you have an assorted list. Then you basically can play those games and assume that, the degree of the confidence you have there is also part of the likelihood that they could be a viable candidate, then play all those games.

Okay. Thanks. And then, and the thing about the kind of integer values rather than floating points. So do you think, yeah, would that still apply if we were doing the, threshold and I guess. Would, Python, I wasn't sure the reason you gave for, it depends upon what you're doing. If you're not actually sorting the list or like specifically calling.

So, if you're, not specifically sorting it, then it might not be as interesting. However, if, you basically have a threshold and you wanted someplace to say, okay, where do I put these rows? I've got these rows. What's the data structure that I put them into? and I would still like them to have be in priority order, I would think. I would think that you would want to try the most likely ones first. Am I incorrect or are you going to try them all equally?

at the moment we try them all equally, but yeah. Yeah, even if we threshold, we want to update all of the ones that are above the threshold or in the top K. Okay. so the advantage of, quantizing that value, it doesn't have to be as fine as I mentioned, I was just using that as a, as a straw man. But if you want to deal with these things, as groups, where, you store, them as, If you wish, clusters. physically they can then reside, at some point in near values in memory so that you're not, so that if you're on a CPU, you're not, randomly accessing memory all over the place. You might want to, migrate those rows into physically adjacent storage just for the purposes of the fact when you bring in one of these guys you could drag in the memory for the next couple of and your memory of cash efficiency goes up. Yeah, that's actually that's a second order effect, but basically having a quantized Address or tag or where to look for these things makes it easier to just use a lookup table and says, oh, okay, I'm looking for cluster five, lookup table, okay, memory location, this bounded by this, go ahead and do those things. I'm just saying that, it really depends if you're going to look at all of them equally and do the same thing as well. But if they're scattered all across memory, You're going to pay costs for that. There are advantages to bringing things close together in memory. And would we have that flexibility within Python or is there a way to you basically, you have a maximum size of what the row could be. That's your k is equal to 10, right? So you allocate an array of n cells. Okay. And then, once you process it the first time, you might, allocate some subsidiary arrays, K is equal to 10, H equal to, I don't know, 20, something. I could do the computations for you to figure out what would fit into L1, L2 cache, that kind of stuff. But the thing is, that if you know you're going to bring these, guys in and maybe reaccess them multiple times in quick succession, then there's an advantage in keeping them in. Close memory, if you wish. Nearby memory, again, that's, this, is a, This all assumes a C implementation, not a Python. You can still do it that way. it's, just an array. Python will blow any cache coherence that you have. the whole interpreter running for every line of code.

In the memory access pattern, though, it's still going to be coherent. the instruction cache, yes. But the data cache, if you're basically accessing coherent rows, that can't help but be benefited. that's the underlying machine architecture. no, I understand. I'm just you may be right. I'm so skeptical of Python.

how many clock cycles does it take to add two variables together? A plus B, I think. If they are not using SIMD, I understand what you're saying. Over a hundred. Yeah. It has to do all these up, it doesn't, the plus operator might be overwritten. A is a class, and everything, it's just interpreted so dynamic and flexible that it's doing so much stuff. Yeah. It's not like Z So even if the underlying arrays were NumPy, they still have all these characteristics. No NumPy, NumPy operation is optimized. But in between them, in between lines of Python, mpi, it could just completely blow the cache. Okay. that's what I'm, I don't know. That's my suspicion. Okay, so what? Subutai is right. I'm basically mapping this onto a C model that where the operations you want to, pare them down to the most efficient things possible.

you might be right too. They might, yeah, they do try to optimize that. We would have to profile to see whether you are compute intensive or whether you're memory intensive on this. And if Subutai's hypothesis is correct, if you're compute intensive, what I'm asking, what I'm talking about would be a second order effect on top of everything else. Yeah, So yeah, that sounds interesting though. Yeah, thanks. I definitely think one day we'll hopefully have a lot of this re implemented into it anyways. I didn't know if you guys were there yet with this meeting.

Not, yet. Okay. All right. I have a quick question. just to refresh my memory, it's like the things that you're using to judge whether an observation matches with a hypothesis. what does that feature vector look like? Just real quick. Is it just normals and tangents or something, and what is the features that you're comparing, observations against hypotheses? you have two different types of, so we have features like color and, yeah, just features that don't change when the object rotates, and that's just an array of numbers. And then we have. The curvature directions, which are like these, matrices that have threes in here, that are, yeah, basically, unit vectors. So which ones are you using to compare in this, this dot product Einstein matrix? What, are the values that the three values, the dimension that you're collapsing in these, products? What is, that? What are the values that are being multiplied there? here we take the dot product basically between the. Observed, rotated, I think he's asking is it X, Y, Z, he's asking if it's X, Y, Z. What are those values? What do they correspond to? Are they rotations? Are they positions? What are they? Oh yeah, it's X, Y, Z coordinates of a unit vector. Okay, so it's a normal. Okay, all So then there's a very easy way basically to speed this up and that is basically sort your entries in that matrix, the H matrix, sort them along the lines of their orientation, basically, so that when you get, when you bring in an observation, and it's obviously pointing in this direction, you have, it are, the list is already sorted on those hypotheses, like those orientations, then you can go directly to the part in the list where you've got everything that's oriented in that direction, or, do you have to do another transformation to, to, to, see They can't sort because they have multiple, that's, a matrix there, that's a three by K matrix. So you can't say you have multiple orientations you're going against. Okay, so, they have to reform the entire dot product there, and then Do whatever processing they do to say, okay, this is representative of a good fit. Alright, so for each hypothesis, you've got ten possible, neighbors, so that's like points that you could move to nearby, and, their, orientations, right? Yeah. is there any way that you can sort those based on what the observation is?

I guess we could, but we still have to calculate them all because we also want to get the negative evidence if it doesn't match. Okay, all All right, nevermind. I was just thinking that there might be a way that you could, orient or organize the hypotheses and those k nearest neighbors in such a way that you could very quickly go to the ones that were closestly or closely aligned with the observation that you're bringing in, given that they're both in these three dimensional spaces. Yeah. All right, never mind, it was just a thought.

Yeah, so I think I'll try doing, yeah, what Subutai suggested, to just threshold before even building these matrices, see if that, if there's some, That's something I'm not thinking about right now, why it doesn't work. And then, yeah, I can look into, that some more as well. But if that works, that would save us a lot of time also with the KD tree search, because we wouldn't have to even search for a lot of the hypotheses, nearest neighbors. So yeah. Let's see. yeah, that would be a cool side effect. So the big question for you is deciding how to arrive at that threshold, whether it's a fixed one or a dynamic one. If it's dynamic, is it based upon, you're falling off of a level of evidence or is it something else? So that's the real trick in any of those things is, the heuristic to figure out what's a.

Yeah, because we could save a lot of computation if we ended up essentially ignoring objects that are, extremely low evidence and like those were having almost no point updates.

yeah, that would be in a second threshold to just not do any update for a whole object. If it just doesn't, slow, yeah. yeah, I have another topic, I don't know if Yeah, we can, yeah, we can do that. By the way, these are all sort of these lazy person optimizations, which are rather than doing something faster, avoid doing it at all. Yeah.

Try it through a true optimization technique. Yeah. Fastest computation is the one you don't have to do. Exactly. You just have to prioritize here. Yeah.

yeah. So the second topic, I don't know if there is a good solution to it in Python or not, but I was just wondering if anyone knows about better alternatives to multithreading. So I don't know if you remember from last time, but we do a multiprocessing across episodes. So we.

we do that, but then within these, sub, within these processes, we also do multithreading over objects because we can update each object's evidence independently of all the other objects. and for that we use Python's threading. thread. but it gives us only about a two to three X speed up and, theoretically we would want the speed up to be proportional to the number of objects in memory. So a 77 X speed up for the YCB data set, for example, but it isn't. So for example, So you're saying, so you're doing multiprocessing over episodes. So each episode is in a separate process? Yeah. And there's two different things. Yeah, so that's, so every episode is in its own process, and then for each episode you want to use threads to do that, right? Yeah. Yeah. So basically, yeah, your episodes are parallelized. During evaluation, during training, we can't do that because they have to be in order. but then, yeah, within the episode, basically for this for loop for object in memory, we run each update evidence call in a step using Python multithreading.

but yeah, it still doesn't seem to give as much speed up as it should. And yeah, for example, during the demo, you saw it seemed quite fast, but that was because we only had nine objects in memory, and then when we have 77 objects in memory, it basically, yeah, it's basically a linear multiplier, however many objects you have in memory. Yeah, it's not going to help much because if you're already, if your processes are all busy, assuming they're using up the computer, Multithreading will only help if there's spare CPUs lying around. Yeah, and you're also thrashing your cache, with all those objects, you're in memory. Yeah, you're swapping back out from memory, but you're also ejecting from cache quite frequently. Yeah. It's sharing the same processor, So I'm not sure how, I'll just say I'm not sure how Python implements threading. Because I always, I used to He told that it was a single threaded process and you had to do something extraordinary to get it to multithreading, so I'm not sure what type of threading is going on here. But, having said that, if I wasn't in Python, there's this notion of affinity, where you basically lock a process, to a particular CPU, and then it doesn't help you to do more than two threads on that CPU because there's only hardware support for two threads. And the only time it switches is either you yield it or it has to do an I O operation that kicks off to something else. Now, the reason why you want to segment it like that, is because each processor, each CPU, has its own L1 and L2 cache, but they all mutually share a L3 cache.

if there's, a thousand threads waiting to be run, there's no coherency there. They're basically, you're going to bring in an object and do a little bit of processing on it, kicks over to another thread because it, by whatever thing, timeout or whatever, then it's got to load that data in and do a little bit of processing on it. So you can, really easily You know, without thinking about it, as, Eric was saying, thrash the cache, because you haven't really controlled how much work you're doing. What's the, for every unit of memory that you read in, are you getting a good benefit for doing that? Are you getting enough work on that? Or is it just doing a little bit and then having to be checked? And there's, a double cost for that. Not only do you have to, bring in the new memory, but it's got to write out the old memory right then, before it does that, effectively. some of it could be in parallel. like I said, I'm not sure about what the principled way of using threading on other Python is. But, what I described is pretty much an optimal way of doing it. You typically don't want more than two threads per processor, if they're all doing the same thing. I know how to do this under MPI, but I don't think I've used Python threads, so I have no idea how to do it, but I know it's complicated for using it in other parallelization modes, especially under C. I think it was the width of the threads being created. when are the thread objects being created?

within the matching step calls. So basically.

yeah, I can actually show you. The problem to me is like the amount, the unit of work that we're trying to thread on is very small, right? It's less than a second or so, right? And if you're creating the threading objects repeatedly, spinning up new threads and then tearing them down, that's going to limit, that's a very heavyweight operation, creating and destroying threads. Which is why people create thread pools. Yeah, So I think that might be an issue there. It's not so bad if you're going to create them and they're going to run for 20 seconds, but The reason I think you're not seeing 77x here is that it's just too small a unit of work, so what will happen is the cost of creating the thread is too high. Oh, okay. So I don't know whether you can hoist the threads out, out of, to the outer loop, so you can establish the random work.

Okay, yeah. This is only within the episode, right? You are still seeing a 77x improvement because the episodes are paralyzed. It's just within that loop you're not seeing. Is that, I'm just, I don't know, I joined late and I apologize. It took me like an hour to get there, an hour longer than necessary to get here because of roads and trees and, yeah, let me just, I think so, yeah, definitely do what Lauren said, but I just want to be, I do want to understand if you're running, if you're paralyzing the episodes across processes, if you have 77 objects, Does it actually run roughly 77 times faster than if you ran it all on one process?

not, unfortunately not. I think one of the reasons for that is that episodes can have very different lengths, so it'll have the length of the longest episode. Oh, okay. But, yeah, it is definitely, faster than just a two to three time speed up. So when you, spawn these processes. Are you spawning them and holding on to them, or are they being also dynamically created as you need them?

yeah, so these are basically being created and, destroyed every time this, matching step is called. So I guess if I understand you right, Lawrence, it would be better to just create them once at the beginning of an episode. Have one thread for each object and then keep reusing it in every step and then only destroy them at the end. Both the processes and the threads. creating, forking or creating a process is an extremely heavyweight process. So typically what you want to do is create a resource of these things and if you have the ability to know how many processors you're going to use is if you can set the affinity to them to say, run only on this processor, because otherwise the scheduler will time, it's trying to do round robin, even when you're not asking it to, and it will basically swap between processors just because. And so if you look at, looking at top K, where you can see the threads and where the processor, what's running the, which process processor is running is, jumping up and down, going over the place because it, has this aesthetic. That you run for a while and then to be fair, let somebody else run in unless you grab hold of it and tell it not to. So again, at the Python level, I'm not sure where that is. And it certainly, it's easier to do under Linux than it, Python running and Linux than it is under say, macros for sure. But the whole concept of allocating these resources to particular CPUs. And then mindfully subdividing that, among some subset of threads. it, it takes more work, it takes more thought, but you don't wind up accidentally sabotaging yourself by thrashing. Okay.

All right. Yeah. So that sounds, definitely like something to look into then for us. Yeah. you have to manage your own work queues in essence, you allocate the resources and then dole them out, under your say so rather than letting the scheduler just flip a coin. Okay. Yeah, okay.