I guess like I posted, we thought we could talk a bit about depth. I have a few slides I can go through. and yeah, Hojae, also put some stuff together on, structure from motion and then yeah, but we, I thought we can start by talking about the thing I found most confusing was just how parallax works and how it relates to Monty and really just to think through it in concrete terms. And then, yeah, eventually this discussion could maybe get into like, where's this happening in the brain and things like that. And, but yeah, unless, any objections, I'll just get right into it.

so yeah, I posted some notes, in the group about kind of various things I'd, I've been reading about depth perception and, that was like a broad view of, everything from monocular to. Binocular cues and things like that. but I thought the kind of key thing or like one of the most important things for like actual, fine grained depth perception and, also I, I felt like one of the more kind of, less clear kind of computational things is, the role of parallax. So I just put together some slides discussing that and then, some stuff discussing how it relates to active perception. of the, and inference of depth.

but yeah, definitely stop me if anything's unclear. Sounds great. I feel like I'm, I feel like I'm back in college. I'm like, yeah, basic, lecture here. It's great. Yeah.

so parallax basically just refers to when you have, two different views. here it's, in space, but it could be in time with kind of motion, but two different viewpoints. and they're viewing two different, images of approximately the same thing. Any kind of displacement or, kind of change in that image. is generally what's referred to as parallax, so in this example, if you imagine this, is just some kind of telescope, or it could be whatever, camera looking out, and then there's this other one, these are the kind of viewpoints, these are the kind of the views that would be perceived at these two, locations. And if you imagine these objects are very far away, at infinity, they're going to be coming into, these two different, locations basically in parallel, there won't be any difference, and so the same image is perceived, whereas this much closer object, it's going to be perceived on the right hand side, for this, viewpoint, and on the left hand side for this viewpoint, and so that's the kind of shift that's happening, this kind of shift. Okay. of the, star.

that's the basic idea. I guess it's worth saying that if you had a bunch of objects at different depths, they would shift different amounts. So they would sort themselves in order, in some sense, distance. Yeah. And the other thing is we tend to, because we have mobile eyes that move, we tend to fixate on, the object of interest. and so to generally think about it the way we view the world. is, imagine we have our kind of mobile eyes, we're fixating on an apple, and this apple then, or at least the center of the apple, is going to fall on the fovea of both of these, both of our, eyes, and so there won't be a disparity for the apple, because of this. So this is different from this kind of situation where you're fixating that infinity, and disparity, which is a term that's often used, is I don't think anyone's super consistent with the terminology, but I think the best way to define it is essentially like a computed estimate of the parallax. So parallax is a property of the world, a property of optics. Disparity is what is the retina picking up in terms of, the relative difference between, where an object is falling in these two images.

and as I said, because we can move our eyes, there's no disparity, at the, fovea for the item we're fixating there. but there is disparity for the orange, and that can then tell us something about its, relative depth to, the apple.

but then what's interesting is that, generally that's not sufficient. So this kind of gives you some sense of relative depth, but you can see how in different instances, instance A, where both objects are far away, and instance B, where they're both closer, the relative disparity sensed for the orange, is the same, based on these angles here. And in general, this is combined with some additional depth cues, such as like how turned in your eyes are, here they're turned in much more, so that implies your point of fixation is closer in space, here they're more parallel, so that implies This only makes a difference if you're trying to figure out the actual distance, right? it's still Yeah. It's still figuring It still is accurately replying the relative distance. It's still saying which ones are further away than others, which for a lot of tasks, that's really all you need.

Yeah. If you're not like reaching to grab something. If you're just trying to, decide where the boundaries of an object are or something like that.

but one kind of important thing for this to work is the eyes, the retina or the eye or the brain kind of whatever level it's happening. the kind of know the commonality between what's being perceived in the two eyes. in order to sense that the orange has shifted, across these images, you actually need to understand that it's the same object that's there. and this is what's known as the correspondence problem. how do the kind of features that are present in one image correspond to the features in another?

and I think it's important to realize that the brain, in general, is only solving this at, kind of fine levels of detail. and the way it does this, it's believed, is with these kinds of disparity sensitive neurons. So you can imagine, at the point at which you're fixating, let's say on the apple, even there, there's going to be disparity, in the images in terms of what each apple, what parts of each apple are falling where on the retina.

and you can imagine having some neurons that have binocular, receptive fields, so they're receiving inputs from both eyes. And then they, are looking for kind of the same feature, but with some displacement. make that more concrete. Imagine these kind of simple stimuli. These are going into both eyes. And most of these, stimuli are the exact same for both eyes. that's what's shown outside of the red box. And then within the red box, the stimulus has been shifted, so everything outside of the red box, your eyes can easily fuse and perceive as a single stimulus because they're identical, both eyes perceive the exact same thing.

But within this box, this kind of stimulus that's being shown is simulating some disparity. Essentially, all these black, points here have been shifted over by a small amount. And so a disparity neuron would Basically have a receptive field that responds to this stimulus, say, of course this is just a toy example, but a white box with a black box next to it, but on, in this kind of retinal space, on the retina, it responds at this location, but on this other one, it responds at this location. So it will be sensitive to this precise kind of disparity of that feature.

And that basically gives these, neurons are known to exist. Is this or is this a hypothesis? Yeah, The, so throughout definitely at the stage of V one and later they, exist. Whether they exist even earlier than that. It's funny because in this example, the thing that really matters is this, if, this is as if you're fixating in something at a distance and there's something that's closer to you that you're not, that you're not really paying attention to. It exists at two different locations on the two different retinas, but that's not really the thing you want to attend to, right? that's, the thing in the little rectangle is not the thing you want to attend to. That's just something that's in the way. And you don't want to attend to it. So here, yeah, so a more realistic portrayal, maybe this is what the, yeah, a more realistic portrayal is there are some things that would be the exact same within the box, that's the point of fixation, but there's going to be stuff nearby around it that's going to shift if there's any depth. Okay, okay, so yeah, wherever you're fixated, wherever you're fixated. There should be commonality, and then where you're not fixated, it should be displaced, or have disparity. And and I'll get back to this. This example was a little bit odd, because the thing in the center was not the thing you're fixating on, because it was moving. Yeah. Yeah, let's maybe assume that this is what you're fixating on, and so that's the same for everything else. So while, we were sitting here, you can do this real experiment right here while we're sitting here. Just look at a wall away from you. and attend to the wall and then hold your finger in front of your face. Yeah. And, you'll see two fingers, right? Yeah. Unless you focus on the fingers. So that's, the example of what you're just doing there is that, but it's I'm attending to the wall and therefore the fingers. no, so, I'm trying to show the more fine grained example where this is happening at a small enough level that you have neurons selective to this disparity. So I'll talk about the finger later. The finger is where the disparity is so large that your eyes aren't able to fuse it. Okay. And so you see double. But why would I want to fuse it if it's at a different distance? There, because then if you just, if you have a small object like this, and you're focusing on this point, Which one? You see this? No. Oh, right there, okay. You have a small object. A small object like this. You're fixating on here, on this corner. There's going to be zero disparity there. I don't see the corner. which corner? Let me, sorry, I can't actually just, use the words like the red rectangle is the object, right? that's what I was trying to say. the within the red rectangle is an object, and there's some parts of it that there's the point of fixation. Let's say that's four E, so that's gonna be the exact same across the images. Yeah, but everything else is shifting by small mouth. You think because the object itself has depth, is that what you're saying? Exactly. So that's why, it's like the box. So I'm looking at a curved surface of an apple. And one point the apple is going to be and the other point is going to be. So is it, do we know that the brain actually wants to fuse those other parts or is it? It does. Yeah. How do we know that? but that's how you perceive. That's how you perceive fine grained, depth detail, like, the curvature of the surface. so let me just play devil's advocate here. So I'm thinking like a single column looking at this. That single column is a patch. I don't, nobody's looking at the entire apple. I'm looking at, that column is looking at a patch at a time, so for that particular one column, I would, I guess the different columns could be looking at different patches of the apple and they'd be at different depths. So for, some, I wouldn't want the one right in the center of my fixation, maybe I want this, but maybe the ones that are off to the side or. Other columns are looking at, the side of the apple, which is a little further away. And you're saying for that column, wants to fuse the two together. Yeah. okay. It's maybe, I'm not 100 percent certain of that, but okay, I'll go with that, at the moment. and then that, provides the signal for the, kind of relative depth. And what this means is that with these kinds of fine, shifts, I You perceive depth. So the classic way to test this is with this random dot stereogram, which is something like what is being shown here, but just a much higher resolution. And basically each eye sees a, to one eye, a totally random stimulus, but because there's correlations across the images that Fire up these disparity detecting neurons, you end up perceiving that as a consistent stimulus like a banana, or a star, or an elephant, or a I programmed some of these when I was young. but this also works with one eye. I don't need two eyes to see this brute. Yeah, so that's across time. I'm showing it here with, time, so that we can actually perceive it, because I don't have a way to show you two images. okay, so, I'm testing this because, okay, yeah, I agree across time, it makes sense. I'm, I was, yeah, I trust you, but I, it wasn't obvious to me that this would actually occur, without time on two, two, a column looking to the side would somehow fuse these two together somehow, route them, route the, in general, what we're asking for a column to do, but imagine a column that's looking at the.

And we're asking it to say, Oh, I'm looking at a patch of retina, but there's two retinas. And so somehow I have to take the input from those two retinas, which are now the, those inputs aren't the same. And somehow I have to shift it to make it the same. I don't know how, I don't know how that would happen. That's what I'm saying. There seemed to be, or at least one way the brain seems to solve it. It's probably multiple solutions. is to have neurons that are essentially hard coded for a particular kind of subtle feature at a particular disparity. So this kind of green neuron here receives inputs from both the fovea and this point which is displaced from the fovea. and it's detecting the same feature, but with that offset. So that's what I was trying to show here. These cells are known to exist in the cortex. Yeah. Yeah. You can measure them. I have a question. Yeah. any idea how, so like in the case where there's static images, you can imagine there's some kind of difference being taken, locally between, we've got input from both eyes. There's some local difference being computed to, to, create these disparity cells. Any idea how you're using motion to artificially drive those?

Yeah, as in what's happening here? Yeah. I think it's just something where, yeah, I haven't thought about it too much, but I think it's just something where Kind of the same complications happening, but you are, there's a buffer, so that comparison can happen as long as these kind of stimuli are shown close enough, together in time. Kind of a similar, comparison is done. Maybe I could, speculate on this a bit, right? So the only thing that could possibly fire here, there's no features, the only thing that fires is motion detection, right? So there's a series of points in this image where the motion is being detected and that is typically would be your magnocellular cells. in the retina that detect motion, right? The parvicellular cells do not detect motion. They do not fire when things are moving, but so you have a set of magnocellular cells that are firing, at the borders of this thing. And, I'm just going to start with that. And then for the columns that are, receiving Input from those magnocellular cells would say, oh, there's a feature here, there's something going on here. And obviously the feature really is just, it's almost there's no features here, but there's a set of columns that are active, that surround these objects. And and it's just that literally it's like saying, oh, there's features that there's some feature at this location. I, there is no feature, but there's something at this location and that is apparently sufficient to recognize some of these things. Now, I don't know what the image on the right is, I can't even tell what that is, but one of the things I learned when I did this earlier is that your limit to recognize a particular object is very limited in this regard. That is, if you start making something that's a little bit more complicated, you just can't tell what they are at all, and I was surprised when I did that. Here I could see, oh, it looks like a pepper and a star and a, a star is a different thing, but the elephant, I don't know what that other thing is on the right. I think it's meant to be like a jeep, like a car. Oh, I didn't get it. See, I couldn't tell that. so there's a very limited ability to recognize things in this regard, but anyway, I would argue the mechanism is magnocellular cells are foreign because they detect change. And, so a bunch of columns being activated at those, and that's it. All you have is the idea that there's something here, and maybe you can get the orientation of that thing. Yeah. Yeah.

I don't think this is necessarily, contradictory, but I, guess Scott, like just in terms of this kind of view of it where it's not necessarily that dependent on motion. It's more that kind of like at any given. point, the, these cells are going to be getting a stimulus, I guess one of the stimuli, the first one is going to come here. And then the one that's shown like a quarter of a second later or whatever is firing up that point. And then at the next time point, you get another shift. And so that's why It gets fused together so that it feels like it's both 3D, but also that it's wiggling. it doesn't feel like it's a 3D image that's, static in space. Isn't it true? Isn't it true that there really is no disparity in those images? There's no disparities. But that's the thing. I think the brain perceives disparity because of how closely in time they're presented. I see. I disagree. I think it's gotta be, then it's, a change, right? The brain is, this only works because the cells are detecting change. There is never a point where there's any possible thing to look at to say there's a disparity between these two things. it's just edges of change. I'm not saying disparity cells don't exist, but I don't think that they're playing, I'm guessing, I don't think they play a role. Yeah, in a special one. Maybe. I guess the question is like, Is this triggering a depth percept for you or not? it's not a very strong depth percept for me. I get this more of a surface texture segmentation, slight depth, like it may be driving the depth a little bit. I don't, think this is a depth perception example, is it? This is just a figure of separation example. Yeah, this is a more pronounced example where there's more cues. But I would argue I perceive this as being almost like a 2D, sorry, 3D like paper world, like it looks like they're like paper cutouts that are inserted into a 3D scene. So it's like any individual person looks kind of 2D. But the arrangement of them within this, street looks 3D to me. but I think it's an artifact of just the way this is presented.

first of all, you can, all these perceptions, I don't know about the disparity neurons, but almost all these effects can occur with one eye. And and typically when you're walking down the street, if you're walking down that street, your head's constantly just jiggling a little bit one way. You don't have to try to do it, your eyes don't have to move, you just, that's just natural. And so there will be that kind of, the thing you just illustrated in that street scene will occur in your head. And, I think the, fact that it feels like two, sheets of paper is, probably because it's a very unnatural way of presenting it to us. The world doesn't jiggle like that, without our motion. Like we, we typically move our bodies, we sense it, but it's, really no different than you just moving your head back and forth a little bit. There's, technically no difference between those two things. Yeah. and so I, I don't think you should read too much into it, but other, that it feels like sheets of paper. But that effect is what's happening, and it doesn't take two eyes to do it. I guess it could be disparity over time. but I It's the same, I think it's the same idea ultimately. yeah, disparity over time, you still, whether it's over time, and I guess Hojae's presentation might go into this more as well. you still need to solve this correspondence problem, and so the way the brain seems to solve it, at least for static inputs, is to have neurons selectively tuned for these features. but, I, it's interesting because if you think about the street scene, you don't have to solve a disparity problem. You, fixate at some point, and then everything that's not at that depth, you fixate at some depth, and everything that's not at that depth is changing, so you can just ignore it in some sense. It becomes blurry, it doesn't, you're not trying to solve a desired problem. This is when I thought, I was surprised you mentioned the disparity neurons because the only way I can imagine that is like you're trying to You're trying to, focus on multiple depths at the same time, and that was the example of the apple which is curving away from you. that, it was like, oh, that's, like a different problem. That's I'm actually trying to perceive multiple things at different depths at the same time. Where normally I've been thinking about this problem, it's oh no, I just want to ignore all the stuff that's not there. Not that, I'm looking at the woman and the child, and everything that moves relative to them is not part of the woman and the child, it's something else, and I can tell how far away they are. The disparity neurons, if they exist, they would say, oh, this is this far away, or that's that far away. as opposed to attending, like fusing them. That's the part that's weird to me, is we're trying to fuse those. Yeah, because I guess the way I'm thinking about it is say the, I think, the receptive field of a V1 neuron is something like your two thumbs held out at arm's length, so it's not, nothing. It is a reasonably, decent amount of information, and so it's like within that sort of receptive field, you have a perception of depth all at once. It's not You only, it's not like you always perceive everything that you attend to as just being a single plane, relative to everything else. so like we got, so So it's within that kind of local but let's say I have, I have two, let me just write it this way, I have two eyes, and they are trying to focus on something that's in that, in the receptive field of that neuron, but maybe the thing that they're focusing on, or the things they're trying to attend to. it's smaller than the receptive field of that neuron, and, and it's just smaller. And so I would, normally, I wouldn't want to pay attention to things that are further away. I just want to say, Yeah, the only features I'm really interested in are the ones that are in focus, the ones that come from both eyes at once, that are aligned, and anything that's not aligned would not be a feature that I'm attending to. and so that would be a, that would be a quick way of saying, yeah, there's, a bunch of input coming from the retina over this area, some size of a little circle or something. But actually the thing that's of interest to me is smaller than that. And, only, the axons from both eyes that are co aligned will, will be, recognized. And the axons that are non co aligned won't, they'll just be ignored because I don't really want to tend to them. I don't want to see more, I just don't want to, I just don't want to see the stuff that's beyond it or closer. so you're, it's funny because you're trying to unite them and I'm trying to separate them. I'm trying to say, yeah, the things that are further closer away, I don't want to, I don't want to. So I'm, agreeing with that for, like larger differences. I guess I'm just trying to say that within a small narrow field, there are often cases that we want to perceive true depth there. that, that is how primates like are able to like break camouflage, for example, that they can see even if something in a 2d input looks. The exact same. It's that disparity at that kind of fine grain that makes stuff pop out. I'm questioning whether that's, that's true. I don't, the hidden animal pops out because it moves. If the animal doesn't move, you don't see it. it's just, you can though, yeah, like that's one of the, I guess the innovations or whatever of depth perception is that you can, something that, an animal that doesn't have depth perception will be unable to see, against the background, something like a primate can, because of the 3D depth popping out, because there's this disparity, I'd like to see examples of that, because everything I've ever noticed and thought about in this regard, It's just the opposite. You can't see these things, until they move.

so I guess there's, I guess all I'm going to say is there's different ways to think about these problems and I'm, and I've been thinking about it differently than you're thinking about them. I've been thinking about how it is I eliminate information that isn't at the right depth and you're trying to somehow fuse information at different depths, which is the exact opposite of what I would normally want to do, right? yeah, maybe if I, Move on, because then I'll talk about, I think, more what you're describing, which is this is just within a particular area of focus, But then your eyes are constantly moving, constantly looking at different things, and then so that means that, as you say, most things that you're not attending to are, not in focus. Okay, I'm, gonna still lodge my, my, concern about this idea that a single columns, retinal patches are trying to unite different depths on it. I'm not aligned with that yet, maybe I'm wrong, but then we can go on. Okay, yeah.

so anyway, so in theory this kind of having these disparity neurons helps you, can help you have this. Fused percept of some depth in a small area, but this depends on having neurons that are tuned to particular offsets across the two retinas, and also particular features, whatever those kind of very simple low level features are. Of course, you can't have this for all possible disparities and, for all possible features, so the reality is Can I, ask I'm sorry to interrupt again, Niels, but this is research, this is what we do, right?

I would have If I read about this, I would assume the disparity neurons are doing something completely different, something that I need to do, and I say, oh, wow, they could solve that problem. It's very difficult, for a single eye to determine absolute depth.

two eyes can. And one of the problems we have with vision is we have to know, where are, where are we observing things in the reference frame of an object? And so this, if I had a, a column that was, and it had some disparity neurons in it, I could say, oh, or there's, I could, somehow I could use these disparity neurons to determine the absolute depth of the distance to the thing to me. and that would be very useful not because I want to recognize a feature or unite these things, just to know the actual distance. Just to say, hey, this feature I'm observing, which is, at certain depth for me. and therefore the disparity therein wouldn't be to unite or fuse images, but it would just be to determine the distance to them. I wonder if that idea has been discussed. That's at least how I understood it from this picture, that this is the purpose of it to detect the absolute depth, but the way that it does that is it has to recognize where, which features correspond to each other, and then when it knows, okay, this feature on the left eye retina corresponds to this feature on the right eye, and this is the This disparity between them, and that tells it the absolute depth. Okay, in this case, all we need is the absolute minimal feature that will satisfy that requirement. It doesn't have to be a feature that is relative to the object itself, right? it literally could be a progression of, dot differences or something, you know what I'm saying? It's not like a feature that's on the object that I'm trying to It's just, add part of my object representation. It's just the minimal thing that could be done that would achieve the disparity. Yeah, it doesn't require any models or anything, it would really just be to figure out correspondence between the two images. So that could be all hardwired and, or not, but it could be hardwired and, then, but that makes it not useful for being part of a model that says, oh, you It's not like this is a feature of the object, it's just, no, it's just a feature I'm using to determine depth. And therefore, I thought Niels was saying Yeah, I, no, sorry, I'm not trying to imply that this is going to be a feature that then becomes part of the object, if that was what it sounded like I was saying. I I guess what I was talking about the apple and seeing around the side of the apple and the depth of the, it's like again, no, it's, more just to get the sort of features we're getting right now. So a, basically a feature at a location. No, but it's not even a feature at a location. It's just, all we want to do is calculate distance. That's all we're trying to do is calculate distance. Yeah, that's what I mean by location, but there's no feature, it's just distance, it's not great. How I'm putting it into context also of Monty is that, it seems like what we know about these neurons and vision, that there is a way to extract depth from binocular input very early in visual processing. maybe like the first input into V1, you can extract depth by taking these disparity neurons, seeing how, like what the difference is and taking, getting the absolute depth from that. And this depth is not, it's. It's like, Monty gets depth input as like it's sensory input features, and it's needed to determine the locations where the actual feature on the model is at.

So this is like what we are doing in modeling to extract that. I didn't follow all that. Oh, sorry. Yeah. Yeah. I know that was a bit convenient. Monty, we've been assuming a camera with a depth. Yeah, exactly. So yeah, we assume a depth camera. And I guess now we've been going back into the literature to figure out whether it's reasonable that depth information is available and that this can be extracted without too much knowledge of a model or too much pre processing. And I guess this would be, at least for me, the context for why we're talking about this stuff.

And on the point of it being hardwired, I posted a summary earlier today about actually how babies learn depth perception and it seems like at birth, they don't have binocular depth perception, but it is learned very quickly in the first months. So essentially they first have to learn, have to separate the input from the right and left eye, in V1. So you actually see these stripes in V1. If you put in some dye and after they have learned that separation, then they learn, then they get, binocular depth perception within a couple of weeks, like very quickly. but it seems like after that, it's pretty much, It's quite fixed because if for some reason a child or a monkey or whatever's, deprived sensory input during that formative period, they have like lifelong, impairments. if you are cross eyed during that period, or if a cat has the eye, eye patch during the first few months that eye is functionally blind for the rest. I guess, I was, I slightly misspoke there when I said it could be fixed or not. What I meant by that is it's not something that you would learn throughout life. It's not like a features, we learn new things throughout life, but I was thinking this would be something that once it's set up, it doesn't have to change. And yes, I can see that it could be part of this developmental period, certainly. when I think in some sense, it's quote, some sense learned, but to me, when I say learned, it's oh, I see something new, I'm going to learn it. No, it's not like that. It's this is a part of the development. You just set this up and then it goes. the thing is, it's interesting about, and I've talked about this before, but in primates, V1 has these extra layers, and only V1, and I think it's only primates. And I've always said, what the hell is going on there? And it always felt the difference there is that we're trying to perceive at a distance. their fingers are right on the, the, the, object. You have other ways of knowing where your fingers are, but I were seeing a distance where some sound, and so I figured, this, that's really tricky, and so I always felt maybe those extra layers in V4 are calculating. And, doing that depth and, and look, there's a bunch of calculations that have to be accomplished because, it's not only the distance, but then when your eyes move, how far they're moving on the object depends on the distance. So you have to know the distance, and then you have to use the distance to calculate the displacement on the object. And so I said, that's something that only applies to eyes, and therefore maybe that's what's going on in those extra layers, those input layers. So I still think it's a good hypothesis. I don't know whether the disparity neurons exist in B1. Yeah, so I think there's evidence that could be exactly corresponding to these, the processing in layer 4. There's at least evidence that's yeah, that's where the first binocular inputs, Really come together. And then, and the way I've ignored that for all these years, because we have, we have this common cortical algorithm, is we could assume we have a camera with depth, or we can say, look, an animal with one eye is pretty damn good at vision.

you can do 95 percent of what you can do with two eyes with one eye. And, and so let's, not focus on that too much. that was my excuse. Yeah, and yeah, the, core principles of this, of like basically parallax and all this stuff, apply whether it's the two images are across space or they're across time. but Right, but I think there's a big, there's a big The difference between the point of this is to fuse features, or the point of this is to determine depth. And I guess I was reacting to the suggestion that they're trying to fuse features together. Yeah, I guess it's just the fusing of the features is a way of perceiving depth. Okay, as long as it's not the features that we're going to apply to an object, like it's not, the input to an object that's, what is the feature at this location? That is definitely going to be, learned, throughout life. You could change what features you, a column can, assign to it. Yeah, and you could totally do that in this case, like you, whatever features are being detected, That could change, if there is, I'm, no, I'm saying, I'm, saying, I'm trying to go the opposite direction. I'm trying to say, we're using the features for two different things. There's a retinal feature, which is just whatever the minimum that's required to figure out depth. Then there's features of our object models. And that's something completely different in my mind. and it's a common use of that word. I'm, pushing back on the idea that there's a point of saying, Hey, let's fuse these features together and then feed them into our model, as opposed to, oh, we're fusing these features and we're using disparity neurons just to determine the depth, of the neurons that are not disparate. The ones that I want to pay attention to, the ones that are actually in plane, and anything that's not in plane, is the one I want to ignore. No, I, think that's, yeah, I guess a good way of emphasizing that, as you say, it's what you are fixating on that is the most important. So really, this is just giving you a bit of depth information around that. That makes sense to me. So it's the kind of language we're using and it just confused me a bit. But yeah, so, that's, yeah, this is giving you, so you can almost imagine it's this is, Basically saying, it would be reasonable for us to assume something like a depth map we currently have, at least near to the point of fixation, just around it, from this, from these disparity neurons. But, as said, they can't cover the full space. and okay, what happens when you, when they don't work, when, basically disparity neurons aren't doing what they're meant to do? And this is exactly what we were talking about earlier, Jeff, where if you hold your finger at arm's length, so very easy, to, if you fixate on it, to, to perceive it as a single object, but then you, fixate on in the distance, you will perceive your fingers as, your finger as, being double. It's not confused. I don't know where you're going with this, but I don't think, I think in this goal, the purpose of the whole system is not to, see the two fingers. Yeah, I'm not arguing that, it's meant to, no, so my point is just, this shows that it only works in a very kind of narrow, you can almost imagine if you're familiar with cameras, like a very narrow depth of field is where this is around where you're focusing. is where that part of depth estimation is going to work. But then, in addition to that, then you have all this active perception of depth. And in particular, you need to figure out where to actually fixate, where your eyes should converge, and how much the lenses should accommodate. So that you have the kind of stimulus of interest in, in your kind of fixation. And initially, if you're, again, just fixating at infinity, then any kind of objects near you are going to be doubled, and all this kind of stuff. somehow you need to estimate where should my eyes actually converge to, to focus on this tree trunk. How far away is it so that I can focus on it? and then on that tree trunk, let's say there's a beetle, this is literally the example they give as a paper, a beetle with camouflage, then you could use those disparity neurons to perceive that beetle standing out from the bark of the tree. and yeah, so there's different ways that you might do this. This was a, this paper talked about this kind of contour stereopsis. It doesn't really matter too much, but basically you can imagine you can do correspondence problem for very big, obvious things like, the tree trunk, things like that, so that you don't have these kinds of hardwired, which call it disparity neurons for things like this, but it's pretty obvious to each of the eyes that it's seen, something large like this. And using that kind of information, I had this basic idea, it's funny. I'm thinking about the beetle on the bark, right? Yeah. and if you, I'm literally thinking about situations like this where you see something and you're not really sure if it's something different. my first reaction is to move my head. I move my head to the side to look at that edge from the side to see if the beetle is protruding from the surface of the, or if it sees it, see if it moves separately from the bark. I think my, my, I'm just, personal observation is that my ability to determine depth using these disparity neurons is very limited. it's It's pretty hard to determine if it's a well camouflaged beetle. The whole point is it's camouflaged means that there are no local features which can be, maybe the disparity neurons can work with.

and so then you've been, I'm just making an observation. The way that I would really solve the problems, I would move my head around to the side and look at it from the side and see. I guess that will improve the parallax, but yeah, maybe for by the bay I can bring, I don't know if I can find some camouflaging. it's a big thing they talk about in the literature that, one of the main reasons for, depth of this, fine grainedness is how it breaks camouflage, and I think the, person who discovered this in, the 50s or 60s, he was actually a military, we call it technician who would look at tanks and they knew in the military that in order to spot tanks that are camouflaged, you need to make sure you give two images, one to each eye of the technician, because then using depth, it'll pop out. but, at the, time, that wasn't really appreciated and, perceptual, but the whole point of the whole point of camouflage painting. is that it doesn't give you, it makes it very hard to find edges where you can do the disparity. It's like there are, that's the whole point of it. It's like that the, boundaries of the, there's lots of little boundaries and they're matching lots of little boundaries in the background. And, it makes it very hard to, that's the whole point of camel straw is coloring, is it makes it hard to pick out those edges that are moving relative to each other. Anyway, I'm just thinking out loud. that's interesting. I'm glad, I'm sure there's people who study this a lot. I'm just thinking, in the back of my head, thinking, what do we need to do for Monty? what are these things, which of these are appropriate? I already in my mindset, we don't need to have two eyes. we don't need to do this converging thing. We could, but, it isn't necessary, not absolutely necessary, or maybe the bigger thing we could do is to have two cameras and just use them for parallax. You bounce back and forth between the two. Yeah, I was going to keep interrupting. No, it's fine. Sorry, was that Will? Or Ramy? That was me. are these disparity neurons also useful for calculating how to fixate on an object? so we basically want to minimize the disparity on, some objects. So can we use those? I think so. Yeah. I think for like fine perception, it would make sense that you would use it, but like you firstly, you coarsely fixate on the tree trunk, but then again, say, something in the middle of it is catching your eye. Okay, that's when you fixate on the beetle or whatever, the tree branch, and then, yeah. Other than depth, it would be, yeah, so it would be something else other than just calculating depth but also how to guide your eyes into fixating on an object just to minimize disparity. I don't know if it guides your eyes or it would tell you when you're, when you have fixated on it, it doesn't tell you, clearly you, when you're reading like a small text, it you want to keep the letters in focus, I'm not sure these disparity neurons would tell you they might, but I, think, to think of them as part of an active perception I think is a little weird for me, it might be Ramy, but to me it's more like saying, hey, when I, when my two eyes are converging, there will be some point where there, there's some depth where these things will co align, and yeah, I guess it could the way you want, you think about it, but I was, It's just, I don't think it's part of a, like it's not guiding your eyes like saccade over here. It could be like, it could be tweaking the eyes slightly, saying oh, you're a little bit out of focus, move it this way, a little bit out of focus, move it this way, that kind of guiding. That's what I was reacting to. It's not oh, move over here. It's more you're trying to look at this thing, and it could give fine level control to the eyes to say, just tweak it a little bit, tweak it a little bit, keep it in focus. Yeah, if that's what you meant, then I'm with you. Yeah, I agree. The move over here is, something probably, maybe high level, but Right, it would be. So if that's going to be model based, or motion based, unexpected motion, or model based. So I was reacting to the word guide. It's okay, yeah, it's a limited word for guide. It's just it's like a local feedback loop to keep things in focus. yeah. Thanks. Yeah, and I guess on the model based point, I think one of the nice things about this and, how we build Monty is, obviously, because it has these internal models, that's another, very natural cue for having a sense of oh, where should I fixate, I'm at my desk, where do I think my computer is going to be relative to me? I know where it is in the world. that already gives your visual system some sense of where the eyes should be converging. And then, just in general, based on what they do focus on, that gives you some sense of, okay, what I thought I was going to focus on is in front of that. And so you can have, an iterative process, Improving the actual accommodation, but all this, is just to say that there's these two, this two stage process, it seems, where you have an initial course kind of sense of where this object is so that you can fixate on it, and then the kind of disparity neurons are really just doing this small amount around that. Point of fixation. Sorry, all my noisy comments were really just trying to, I didn't understand that's where you're going. If it's a disparity just for this fine tuning of it, then I get it. Okay. But using the word features and all this stuff is confusing. Yeah, sorry. Fine tuning mechanism. But yeah, on a high level, I think this fits in really nicely with what we're doing in Monty and to me, puts a lot of things into place and kind of validates that it's reasonable for us to assume patches can estimate depth at some level. yeah. Yeah. I think, I'm sure someone's done the math on these things, but if you know the granularity of retinal ganglion cells, and you know the physics, You can determine, the ranges of which, the accuracy of which these two mechanisms can work.

how much can these disparity neurons actually correct for? There'd be a small amount, they could calculate that probably, and then, What accuracy did you get just from, parallax and so on? These are all things that I think are estimable, mathematically. I'm sure they've done that. Yeah, I didn't come across any specific figures, but in general it did seem that people were consistent, I think, with our intuition talking about small, very small differences for this. Yeah, it only, it can only work for, I can only imagine how it works for small differentials. And, again, you can test, we can test it ourselves with this finger example, like it, it clearly doesn't work for, and that's not particularly, I don't know, hard stimulus. So I know Hojae is going to talk more about motion parallax, but I just thought it was useful to at a high level or kind of like bigger picture, just touch on the concept that, or the fact that essentially the same concepts, same kind of mathematics of parallax. Apply to images gathered over time as they do to binocular or some animal has even more eyes or a Monty system has more eyes. It's really the same ideas about the perception from different points in space. And you can see how, again, something like smooth pursuit makes sense in terms of you're essentially foveating at a point in space that's associated with zero disparity. And then everything else is relative to that, and then, but zero disparity, but you have also, some absolute sense of depth of that location.

and I was going to confuse when looking through the literature by just the sheer kind of variety of the terminology used. So if this is helpful to anyone, in general, you can think of motion parallax as, It's broadly equivalent to structure from motion. Sometimes people refer to structure of motion as when the eye is static and other things are moving. Whereas if the eye is moving, that's motion parallax.

There's a kind of computer vision term of stereophotogrammetry. It's essentially the, again, the kind of same disparity matching correspondence problem that's happening in stereopsis that I was just describing. But done through a kind of computer vision process. And then as mentioned, the structure for motion and or like whether it's motion parallax or it's stereoscopic vision, again, it's all kind of the same type of thing. So that also means that structure for motion and stereo photogrammetry, it's all using similar algorithms. So these, things aren't as distinct as I think they might seem on the surface.

And just a last note, what I going back to what I think's nice about how this all ties in with Monty. When you are solving this correspondence problem to do this kind of processing in computer vision, it's really computationally expensive and really challenging because they try and do it over the entire image. take something like this, two images of Notre Dame, and you try every, find every single point in the image that corresponds to Point in the other image, and that gives you a depth map of the entire image. But, we are concerned with a fovea or, a narrow, point of perception. And so that would make the problem much easier, much less competitionally expensive. in general, I couldn't find much kind of active research on this. maybe there's stuff in robotics that I missed, but I think it makes sense because, of course, there's not any Monty like systems that are just perceiving the world through these kinds of, narrow, apertures, everything tends to be trying to view the whole world all at once. it's funny, just showing the picture of Notre Dame there, it reminds us, this is why disparity neurons can only work locally, because the computational requirement to do this over the whole image, It's requiring disparity detection over very long distances. yeah, exactly. That's exactly what we said. That's exactly what we said. Disparity neurons can't do that. They're going to, neurons can't do that. They're going to have to do something very local. so that's just a reflection of the fact that they're, it's just a different way of looking at the same thing to say, yeah, that's disparity neurons can't have to be simple. They're small. No, thank you. Yeah, exactly. Yeah, and also just integrating over neurons that come from the entire visual field into V1 would be, crazy.

yeah, I can, I don't know. I found this image very helpful for myself, at least. I don't know if you can see the second screenshot, yeah, basically. Stripes that form in V1, after the first learning period where like they correspond to input from the left eye versus the right eye. these are ocular dominance columns, right? Yeah, exactly. Stripes, yeah. Yeah, the ocular dominance columns. and Yeah. So if we have a little cortical column, it can integrate over a small, difference in left and right field between these two inputs. But if we would look at the entire picture and find correspondence, we would have to look from over here to all the way over here. That's another way of putting it. Exactly. It's you can imagine during a neuron doing, maybe one or two stripes, but beyond that it gets a combinatorial problem. Yeah. Yeah, Nice. Yeah. So I just had one last slide was, is just that to summarize, in general, extracting depth from either binocular moving images is not easy, even for biology. And as I was just showing you, it's a really hard problem in computer vision, but fortunately people have worked out the algorithms that we need to do, given two kind of 2D images to solve that correspondence problem. But it's much more tractable if you have something like Monty, where you're moving in the world, so you can get kind of crude estimates of depth, fixate, and then you just have this fine estimate, and that fine estimate is just over a narrow view of the world. and yeah, basically the kind of tenets of, what we're doing, which I thought was a nice, a happy accident, or I guess in, in terms of what sort of depth information we need.

Yeah, very nice. That was great. That was a nice summary. I'm glad we discussed it and debated it. I have a better understanding now. Cool, thanks. Yeah, and it definitely helped me because Yeah, I read snippets of parallax, but it never really made actual sense, but it was nice to see that it fit in with what we want to do. Of course, I'm gonna, I've concluded a long ago, maybe erroneously, that most parallax occurs because of motion. maybe Hojae's going to talk about that. Is that what you said? I, yeah, so I didn't really touch on that, that much, but, one of the cool I thought you said Hojae was going to talk about that. yeah, But I guess I was just gonna say one of the cool papers, that, that I came across was how basically mice, they detected almost like this death map already in V1, precisely in the setting where the mice aren't moving around. Of course, it's just in the, the field of one of their eyes, but through motion parallax. So I think there's good, there's almost better biological evidence that depth from motion parallax emerges really early than depth from binocular fusion. and of course rats and rodents don't have binocular vision. I'm afraid. Yeah, I think they have a little bit just in front of them. It's a little overlap, but I wouldn't call it binocular vision. My perception is that they have no binocular vision because they're, the center of their visual fields are just not, can't align. So there's a touch overlap on the, in those, on the edges. Maybe that works, I don't know. But my assumption has always been like, they're there more for detecting predators all around them as opposed to discerning things that prey that they might find in front of them. Very.

I have some thoughts I want to mention about this, but we'll wait to Hojae's time. We have time. Yeah. Yeah, mine's pretty short. So the only thing that I'm saying is that there, all this. In computer vision, there is some kind of ways that we can estimate some depth, let's see, that I think has a lot of parallel to what Niels talked about, Yeah, so there's many cues that allows us to perceive death. so Niels mentioned, convergence and like accommodation. I'm still like learning these terms, but there are many ways that we can perceive death. the one that we've been focusing on in this presentation was parallax. parallax can happen. I think we talked about this a lot, but can happen because we have, when you have parallax. Binocular vision, because we have some inter pupilary distance, that is one way paralympic can happen. And then the other, way that it can happen is through motion panel ax, which, can be monoculars. If, you have one eye, we can still estimate death. I'm not sure how good, but I also agree with, Jeff, that, with some like physics and math, there's probably a way to estimate, like how. Accurate it can be, though that's not the focus of this presentation. structure from motion, so is a way to perceive death using, I guess I like to call it one eyeball, and a movement, which is what I think Monty's distant agent is already, that's the setting that we have. Again, is this movement of the object or movement of the eye? Movement of the eye, movement of camera or eye. Okay. All right. So how's that? Is that different than motion parallax? I don't think so. Okay. All right. Cause you had motion parallax and then a separate bullet is structured for motion. So I didn't understand. Yeah. I was trying to get it. Yeah, exactly. It's like the biological term versus the computer vision term. Okay. Yeah.

We can come back to a six second video later.

yeah, so I, I think that, so I guess the main argument that I'm making is that, in Monty, we can, just with the current settings that, without any extra cameras or changing the, settings, we can remove the, information coming from depth and still be able to estimate depth with what we have currently.

So yeah, but I guess the caveat is that I'm not sure if that depth map is going to be as good as like the ground truth one.

yeah. Yeah, so this is like the classical kind of output from, structured from motion. Is this the cool video? No, it's not the cool video, but, yeah. Oh, I see what you mean. but this is, I think, another cool video. So basically, there was a research where, people took a bunch of images from Flickr. So all these kind of black, Triangular cone things are cameras. So when you up, apparently, when you upload pictures to Flickr, you can, some like phone information, location information is stored and, you have the corresponding picture and people try to use those pictures, from Flickr to try to reconstruct this coliseum in 3D. and there's a bunch of other examples and this website called Building Rome in a Day. So what you saw was the coliseum here, but there's a tray fountain. This one's pretty cool too. Yeah, just because a lot of people take, pictures of these kind of, landmarks. But nowadays, structure for motion, is used in, a lot for, like mapping, terrains. they would send out drones, take pictures, and try to, map out. So they're actually, in these videos, they're actually creating a 3D model, because you could make the same video just by linking together images. but they're actually, yeah, they're creating, are they creating a real 3D model or are they just looking, they're creating point clouds, they're, creating point clouds, the output are these points in 3D space, yes, and then they figure out what it looks like from different, so they could view the system from a novel direction. Yes. Okay. Yeah. And I guess just to hammer home the kind of parallels between the stereopsis and, structure from motion, like in, in this instance, you, could argue that this is stereopsis, but where you, instead of binocular vision, you have a thousand cameras all viewing the same thing at the same time. I think it's just helpful to recognize, I think that, yeah, the sort of calculations that you need to do to, work out the, depth of what you're seeing is basically the exact same. Yeah, so I guess already from this kind of point cloud, it reminded me a lot about Monty because we do make graphs slash point clouds of objects. so these are very detailed because there's thousands of cameras, but, the minimum requirement, let me go back to this, is just, two images. So theoretically, it's po. it's possible to, reconstruct three structure from as, I guess it starts from two, and then we can refine it as, we take more images or as we do more steps in Monty. Okay, so, SM is the technique to, which utilizes series minimum of two of 2D images to reconstruct 3D structure. And the final output is point cloud, similar to 3D models that output by LiDAR sensors, which are More expensive, and the minimum requirement is that the two pictures, the, there should be, again, correspondence, the features need to be visible across those two or three pictures, more is better, and the idea is, a mathematical idea is that if, in this 3D world, the, a camera is capturing lights from different parts of the 3D world. And we're taking an image, so in this case, point 1 ends up here in this first image, point 2 ends up a little bit to the right in this image. But if we move the camera, now the point in 3D is projected to this location in 2D, etc. So that's the setup for this. And does the camera actually have to be in motion for this or can it just use a lot of different points of view? the camera, so you can have two people with two different cameras taking the big picture. it doesn't need to be like that one camera need to be like moving. Oftentimes this is extracted also from like video where one person has the same camera and is moving across time. But like in the Flickr example, those are all probably people's pictures. iPhones, I don't know, or Samsungs, taken at different time. Yeah, very little. most, because most cameras have zero depth ability, right? but if we're doing this with eyes, we just talked about all the different ways we can determine depth. And in Monty and, in eyes, you don't have to move, right? You can just be in one spot, as long as you can see the points. Yeah, so the overall algorithm is fairly simple and I think a lot of has a lot of correspondence to what we talked about earlier in Niels presentation. So just go through each one, try to put a picture of what that is. that algorithm means without using math, but so the first one, extracting feature, so the feature in this case is not featuring location, but the retinal features, that Jeff mentioned, features from image that the sensor gets. okay. if you're going to do it, if you're going to do it this way, do the whole image at once. Okay. Then you need more than just a retinal feature. Yeah. Oh yeah. So I'm not, if the retinal feature only works on that very small distance, you can say, oh, there's an edge and it's moved over a little bit here. I have to say there's a gazillion edges that it could correspond to. I have to say, no, this is, this particular cornes that is, is over here, is this particular corner over there. So it has to be a much more complicated features in this case. Yeah. Yeah. I, think that this, so in Monty, obviously the example is a picture 'cause nobody is. not nobody, but not a lot of people upload pictures of, the mug, very, very close by. for example, this is, the whole building, but I think this would work, I'm not sure how well, with, the 64 by 64 small images that we see in patches. For Monty, you can imagine those instead of these two pictures. But anyways, it's the point I'm making is that it's not yet, this step can happen in the sensor modules without having gone to the learning module, so we haven't learned those features yet.

anyway, so it can't be done on the big scale like this, right? No, it cannot be done. It can only be done very, locally. Yes, yeah, Which doesn't tell you, it doesn't tell you where that feature is in the overall object. It just tells you, Yes.

Yeah. yeah, our pictures, or the Monty pictures are, we have usually very small, 64 by 64. Usually, it's like a, very, nommed in picture of a mug, right? Like the kind of images that Scott has showed me.

but still, but, so whatever the underlying images, there, let's say there is a way to extract some kind of feature, like feature from computer vision such as shift or something. deep learning is like another thing, but we don't need to use that, but anyway, first step involves identifying some features, and then next step is the correspondence problem, so when we have features from one step, and, we take, we move a little bit, and then, we have a new image, and then we have a new image. that we need to be able to match those features. So this is a corresponding problem that Niels talked about. it, mathematically or like computationally, this comes out, the features comes down to like in computers as a vector, for this particular location. Basically there'll be some vectors and basically we're trying to find, similar vector values. we can do like a, Euclidean distance between these vector values to see how close they are. I'm just trying to make this like more concrete, how this might be done on Monty or, But, Hojae, didn't we agree that this is exactly not the thing we want to do in Monty? This is the computer vision way of doing things, but Monty doesn't look at images like this and try to figure out the different parts.

I just want to make sure that what I'm understanding is Yeah, I think this is useful to see how it's done in computer vision, and maybe we would take approximations of this, like Yeah, to what you were saying earlier, Jeff, like we can use much simpler features and just in a local area and all this kind of stuff. this is exactly what we don't want to, this is the kind of things we want to avoid in Monty. we want, Monty does a, thousand brains basically does a patch at a time and integrates them in space, as opposed to trying to do images at a time. And so the only important thing is to understand for that patch, where is it in space? And the next patch, where is it in space? And the next patch, where is it in space? Yes, I guess in Monty it would be maybe slightly useful in the case where we have a small patch moving but it's not being moved by the agent itself or we have an object moving in the patch, but so it could be an alternative to using optical flow or something like that. A similar approach to that, track how, the sensor is moving. I don't understand that. it, there's no place in mind you should ever be looking at the entire image and trying to correlate across the entire image. Yeah, no, it's, I'm thinking of this more as what does the approach do? So replacing the image of the cathedral with a small patch on the object. And then the idea is basically just, okay, we have a small patch. Now that small patch moved over a little bit. How do we find out how much it has moved? We do it by finding specific pop, like low level features. so doing it very locally again. Yeah, exactly. Yeah, It's a complex because how much you move depends on how much you move depends on how far away it is, and the orientation. Imagine also you're looking at the facade of Notre Dame from the side, from a skewed angle. And now you have even more difficulty because you're moving in depth as well as moving, laterally. It's, really complicated.

Yeah. So again, for example, there's this nice cathedral, for practical purposes, think of these, replace these images as patches that Monty sees, the 64 by 64 patches, the tiny little, dots. so anyway, between the two patches, we'll, If we go this route, we can extract some features, we'll try to match some features between two patches, and then, I'm skipping a lot of the math here, but basically, if we have the corresponding features, and there is some epipolar geometry that we can do to put these, images in 3D space in the same kind of, same coordinate system. Unfortunately, I used common reference frame speculation overloaded, the term a little bit, but but basically this process, overall is called, I hadn't gone to the math here, but there's a process called triangulation that we are able to, once we have these two images with corresponding points, then we can estimate what the 3D point is, and I know that's the whole point of knowing this, but, and then just for the algorithm it's, SIG, the final bundle adjustment is when you have, so we do this starting with two images, so this algorithm is iterative in a sense that okay, once we're done with the two images, we'll try to add another one, fourth one, yadda and that's what the final kind of point is. I don't have a better visualization of this. I think it starts falling apart. That idea starts falling apart when we're talking about patches.

just, again, typically what we'll do is we'll fixate someplace and then we'll move some distance and we'll fix it again and we'll move some distance and fix it again. we don't move our eyes smoothly over an object doing this, this integration smoothly. It doesn't happen. and we don't want it to happen that way. Yeah. Maybe if something else is moving, if there is a moving object, like a car driving past or something. I don't know. Maybe. I think we should analyze that problem separately. It's like a different task. Or just, it's just an object, even if it's not moving, it's just like behaving like a bird is flapping its wings in front of you or bobbing its head or something like that.

That's a separate problem. Yeah.

Yeah, so that's the end of my structure of what structural motion is. It's one of the ways to, one of many ways to estimate depth, with I think probably the least minimal requirements, like one eyeball is fine, one movement is fine, and then we don't need, two eyeballs like changes in Monty. I think we're agreeing, Jeff, I don't know how, well this will work in patches. in computer vision y terms, it's because I think the patches are quite featureless. there's not like interesting, a lot of interesting edges, interesting features, like in the cathedral example. and so one, there's not that many features. And two, because when we see one patch, there's a lot of edges there. There's a lot of edges on the cathedra, right? in some sense, it's almost defined by a bunch of edges. Yeah. But it would only work locally, right? We've agreed this, we're not going to try to do this the way they did it in these computer vision systems, right? It's just, it's something that can be done locally. in the bottom line, just jump to the conclusion here. it's really interesting there how biology does this. but if we had a perfect depth camera, we wouldn't need any of it, right? Is that correct? Yeah, it's just interesting to know that we can make that assumption that we can have depth. And if we needed something else, we could ask ourselves, oh, maybe we'll use LiDAR, which is a different depth. That's another depth camera. Yeah.

so it's, really, it's, and it's also useful to say, why is this extra machinery in the neocortex with V1, because maybe it has to do these computations, but we don't have to do those computations. Yeah. Yeah, I guess one of the reasons we were talking about it was, there was a few things, yeah, in terms of applications of Monty, like depth cameras are expensive, a lot of people don't have them. So it was just interesting, like what we can do with 2D cameras, since that seems to be sufficient for people. there was, yeah, to Viviane's point, the biological matching, at the moment, we're assuming this depth information is available basically in the first cortical column. Does that mean that we are somehow making incorrect assumptions about what cortical columns can do. what if the first cortical column wasn't, what if V1 was entirely responsible for getting the depth information in the first place? then we'd have to rethink what the columns are doing. Unfortunately, that doesn't seem to be the case. and then, yeah, I guess those were the main two, but, that was what led us down this, vein of thought. It seems, if we wanted to, it's an interesting question. if we really had to do some sort of cost reduction or a different type of camera, I suppose you could just, You could go to just having two cameras, like two eyes, just, separated by some space, of course, the larger the distance, the better parallax you'll get. and, and build a system that works like that. where you can just alternate back and forth and determine depth. But that would be all outside of the cortical column, it would be all in the sensor module. the sensor module would have, would get input from both these cameras and, figure what to do with it. so that would be one thing we could do. Yeah. I think one other thing I'd come across that I thought was useful to remember or whatever is just that, like depth cameras are generally quite limited in that, so they tend to work by projecting some form of light. So for example, they use this time of flight algorithm or this kind of structured light space. Thanks. Where they kind of output that and basically, but basically in both cases, they then measure that light when it comes back to them, but that fails very quickly, for example, in daylight. So depth cameras are very difficult to use. except for LIDAR LIDAR is popular. Because it's lighter, so yeah, I guess also expensive. Anyways, I guess it's just, yeah, it's nice that what the brain does seems to be robust in a lot of different settings.

I, don't know the current state of the art of depth cameras. I thought they were pretty cheap these days because they have them in game consoles and things like that. maybe they're not very good. Yeah, I'm not sure the one in, yeah, I think ones like, I think decent ones like the Intel RealSense one or whatever can cost nearly 500. is it, there's a depth, there's an infrared depth camera in my phone, right? Don't they use infrared depth information to do face recognition on a phone?

yeah, that's why we did the Monty meets world with the, iPhone because it has this camera in it. But to get a standalone one is Not that easy. I think actually buying a gaming console is the cheapest option. All right, we don't have to worry about, what it costs us to get one and what it costs to build one. Obviously, they're not very expensive. They're putting them in every phone. they're going to be really cheap. those are going to be, pennies, dollars, max, I imagine, on a phone, something like that. So they can be built cheaply. We don't have to worry about that right now. Unless we need to go out and buy one, then we can worry about it.

But in terms of, the limitations of Monty or limitations of sensorimotor learning, it's not really a concern. There's lots of ways of getting around it. Yeah, I feel like a nice maybe takeaway from all the reading that we've done is that if we need to revisit this and build depth perception from 2D images, There's lots of ways we can do it that fit really nicely with Monty and stuff like that. But yeah, to your point that we can probably just keep using the depth camera like we are for the moment. think about the ultrasound system that we've talked with the Gage Foundation about. We talked a long time ago. I don't know if you guys talked since. There's a, that's an interesting, there's like a camera that is, I, don't know, I assume they get, they, they must get depth perception from that, but they also, you really can't interpret those images at all until you start moving the sensor, and that is not like a saccade, that's like a continuous motion, or maybe, I don't know, I have to think about that some more. I'm just saying, there's a practical system that you might want to, it's like a camera, and it's moving, but it doesn't move like eyes. something to think about. Yeah, actually, so that reminds me of the third point that I forgot about, of why we wanted to talk about this, was that, we have some of these challenges like, How do we recognize, after a lifetime of exposure of 3D, the 3D world, how do we recognize images, in a 2D picture or a painting? And it was just thinking about, what kind of information from depth perception and how that emerges could inform that. And I think what was interesting from that is, is, as you I think everyone would have guessed, there's lots of monocular cues for depth perception that don't have anything to do with movement. And that's things like shadow and lighting. Or even just relative size, if I have a set of features, and let's say a set of features are the letters in a word, and now all of a sudden the letters on one side are smaller and the letters on the other side are smaller, and it has the right sort of skew to the whole thing, you'd say, oh, that's, this word is facing away, it's moving away from me, it's like it's in depth, it's in, it's moved, it's rotated in depth and plane. So there's size cues too, That's an interesting one because this one actually relies on model, having learned models.

We only have a couple of minutes here. Can I just throw out something to think about? Yeah. Something that's been, something that's been, something I've worried about. I'm not worried, but I've always, it's puzzling and I haven't really thought about it too much, but it's still, and it's related to this. Think about an animal learning in a model of its environment. And the grid cells in the enthorhinal cortex. The reference frame is a plane, in the plane of the environment. it's like a map on the surface of the environment, right? but we never see that view. we don't look down upon our environment from above. We don't look and look down like we're looking at a map. We're seeing it skewed from the, side. And, the views we see are never the model we end up with is not what we observe. We observe these different skewed things and yet we end up with this model which is like a map. And, and you can see this too, it's very difficult to, it's very easy to draw a map. View if I ask me to draw a map of your house, but if I said, draw what your living room looks like, looking through the door to the kitchen, which is something you see all day long, it's gonna be really hard to draw that. And if you've, unless you're a practice artist and you've learned to, to practice on perce on depth perception issues. So there's this sort of gap between what we actually observe and then what, we actually, the model we actually present, which is not. It's, there, there's a gap there, and, the same thing, I think, is true for, We assume right now there's three dimensional models in Monty, and if you assume you have a finger touch up sensor, it's not a problem, but visually it would be a problem. It's if I'm looking at an object visually, like a face, and I'm seeing faces at skewed angles to me all the time, the model I have of a face is more 2D. It's oh, there's a nose in the middle and two eyes and a mouth. But that's not usually what I see. Usually I see these very odd looking things. and so it's the same problem exists in cortex, that there is this, I don't know if you understand the problem, why it's so puzzling, but how do we convert from what we see to what we actually map? Maybe part of this is, maybe a little confounding factor in there is like the, decoding problem, the way the, basically that we're not very good at drawing in 3D, but if you would. And I think that if you could actually walk into a room and it would look different from that perspective, you would be surprised, like you would make the correct predictions for how the 3D face would look like. The room would look like. So the model must be 3D and have it. But it, I feel like even mental imagery, like if I imagine my kitchen and things, or if I imagine someone's face from the side, I feel like I can have a pretty good. It's certainly way better than I can draw, image of what it looks like. that's the thing. It's interesting because, first of all, how do you learn that model from your observations? And clearly, when I see a face from an angle, I will recognize if something's wrong about it. Literally, right away. But it's not a janitor model. I can't say, oh, let's draw that. It is extremely hard to draw a face that's even slightly realistic. if you've tried it, try it sometimes. It's really hard. And, it just can't do it. And so it's like you have this model, it's more like to me, it's like you have a model of the environment that's, reference frame is, anchored on the floor. It's like a map of the floor in some sense, and yet somehow we're able to on the fly generate 3D images, not that we can create them, but we would know if they're wrong, and we can infer from those skewed views and skewed angles, no problem whatsoever. in fact, that's how we do it all, that's almost all we do, and yet the model itself is not that view. I have this other meeting here now in a couple minutes, so I have to run to it. I'm in the office today, by the way, I'm at the office, I haven't been here, haven't been here in months. Yeah, basically we have the group, because I think we've talked about some stuff around the bottleneck of goal states and how, which I think is what Viviane was also getting into earlier. And how that Yeah. Yeah. Yeah. Yeah, basically the model is in the column, but then when we actually want to produce actions to draw it or, yeah, to draw it, we can't just take the model and put it on paper. We have to send action commands to our hand to draw it and, Because we can't communicate the entire model, we recognize mistakes when features come in, but we can't send it out. But think of it this way. there's a huge amount of evidence in the enthorhinal cortex that the models are two, basically two dimensional, and somehow, we're able to tack on three dimensional objects onto that two dimensional model and then generate views of it.

I'll leave it as a, to me, it's a puzzle. It is, we're not doing this in money, we're just doing direct. oh, we have a X, Y, Z location and rec, write it down. And our models are three dimensional and we could create, we could in theory. anyway, this goes back to the question in the brain, are all these models really two dimensional and with. 3D appendages. Or are they 3D? And, I'm not so certain. this is, because grid cells are basically two dimensional and, yet somehow they work to do this three dimensional modeling. Anyway, I just wanted to throw that out as, we're talking about these issues about vision, this problem really surfaces in vision and, and that's all I want to talk about, it doesn't surface with touch. And so that's another sort of mystery of vision, in my mind, how does that work.