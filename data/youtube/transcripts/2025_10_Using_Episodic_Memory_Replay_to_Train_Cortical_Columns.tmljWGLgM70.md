JHawkins: Alright, there was a bunch of things that's been bothering me recently.

And, I've been... I've been just trying... Put them together in some sort of cohesive form so we can think about them. And, if you have a whole bunch of things that seem to look crazy, don't make sense, then you try to, okay, what's the common theme here, and how do we... how do we go about it? that's what I've been doing. And... It has to do with how multiple columns work together. In... in the cortex, so it's really... I'm focusing on the biology here, not necessarily Monty. But, these... some of these themes will be important for Monty, this slide is just a review of the things we do know. We learned a lot about how columns work... work. You know how a column models static objects? We've shown how the same basic column can model dynamic behaviors. Those little figures, the ones that, Pivine came up with for the modeling behaviors document. We've also learned how two columns Who are co-located, meaning looking at the same point in space, can model compositional objects, The parent passes down the child says at this location, here's the child ID, and the parent says, at this location, you should be seeing this child object at this location. So that's all good. I assume there's no questions about that, but if there are, speak up

Why is my screen not progressing? Okay, so here's some things we don't understand yet. when it comes to a single column. We know how it can represent behavior models, but we really haven't figured out how we can learn a behavior model. It just doesn't have time to go around and sample all the points, which we could do in a static object. It just doesn't seem possible that a single column can learn behaviors, yet we know how it could represent behaviors. So this suggests somehow that multiple columns are active in participating in learning. Somehow, you can't learn this with one column like you could a morphological object. that's an unresolved problem, unless someone knows a solution to that.

There's another problem which has been bothering me for a long time. And that 1,000... that's the second picture here. The Thousand Brain Series says that all columns learn complete models, so a region has a whole bunch of models that are complete in some sense. But... Is it really reasonable to expect every column to train on every location, on every object? We can imagine doing that with a single column, your finger, you're moving over the entire object as you build up a model, as we do today in Monty. But can we really expect that to happen to all columns in a region, and multiple regions? And it's never felt reasonable to me, although technically it could be done. And again, now when we add learning behaviors, it's... we know that there's a problem. but even just the, the idea that every column has to have the same model. It suggests there's something else going on, that you... somehow learning has to be shared among one columns. It's not... it doesn't seem like we can expect every, every part of your... your skin to touch every part of every object, something's happening there. And then, there's been some observations about learning, which suggest it involves the hippocampus.

For one, is when we learn new models, we do it very quickly. if I stick my hand in a black box and I feel an object.

It doesn't take a lot of repetitions, I can do that very quickly, so there's fast memories involved. Which suggests the place we know that fast memory exists is the hippocampus. Another thing is that learning new models is always multimodal, if I stick my finger in the box, I'm feeling an object for the first time, I can visualize what it looks like while I'm touching it. In fact, it's almost automatic. It's hard not to do that. It's somehow, it's not like my visual system is clueless what my hand is doing. And I can learn an object by touching it, and then later recognize the object by looking at them. how does that happen? And, another one that's a little bit more subtle, less obvious, perhaps, is that When we learn new models, it's always associated with some sort of episodic memory.

if I were to do a little test, and we're going, okay, we're all gonna stick our finger in the black box, and we're gonna learn some objects, and so on. You'll have a memory of that, at least an episodic memory of that, and it may not last forever. But you'll have a memory of it. So you'll say, yeah, we did this on this time of the day, and then you might even remember, you would almost certainly remember even, when you were learning, trying to touch a new object, you would remember how you touched it, where you started, or where your confusion was. At least you'd have that for some short period of time. So you have episodic memories of learning events. Which, again, these are all suggesting that the hippocampus plays an important role here.

this suggests that learning new models at least involves the hippocampus.

And, at least starts there, or it involves it, if it's happening multiple places. And so when you try to think about that, there's lots of questions that come up. How could this be? And so on.

Thousand Brains Project: And maybe... maybe with starts, to clarify, you mean happens there.

JHawkins: That's true.

Thousand Brains Project: But not necessarily... Proceeds, or not necessarily has to proceed otherwise.

JHawkins: I don't know, I'm not sure about that. I've been trying to think of... situations where I'm learning something new. It could be anything, a new word, a new cup, a new, an arrangement of plates on the dinner table, cars at the intersection. It's hard to imagine that occurring without forming some sort of episodic memory.

Thousand Brains Project: Sure, but I guess I just mean, that would happen in parallel, because.

JHawkins: for someone.

Thousand Brains Project: who doesn't happen... who just happens not to have a hippocampus with enough repetition, he can still learn.

JHawkins: With enough repetition, he could learn some things, but not many things, right? He had a very limited ability, and it took lots of repetition. it wasn't like, oh, he could learn new words, he could learn new, stuff, and he just doesn't remember that he learned it. He had trouble learning anything, and then they... I believe they found that he could learn a few things, and some things were learned, and... but it was... it was pretty minimal, unless you have... you might have more... better information than me on that.

Thousand Brains Project: No, I don't know about, yeah, stuff like if they... if they tried showing him, a word every day, if... if he... if that was.

JHawkins: he would...

Thousand Brains Project: If you...

JHawkins: He would meet the same people every day, and he wouldn't remember them from day to day.

Thousand Brains Project: Yeah. I think he met the same... he met the same people for years, he didn't remember them.

JHawkins: Every time they walked in the room, it'd be like, oh, who are you? he had... he had a really bad deficit. I think it was more like... I'd have to go back and look at the research, but it was more like, oh, look, we found something he did remember It's look at this But the vast... he just really was a stock. that was... for many years, they thought he didn't learn anything, and then... then they found a few things he couldn't learn.

I'm gonna go with the assumption that, even... even was basic, without hippocampus, you really can't learn Stone. And there might be some, exceptions and so on, but... I'm gonna go with that.

Thousand Brains Project: Interesting, okay. But yeah, so that is quite different from... In the past, I feel.

JHawkins: In the paths of our thinking? Or the past.

Thousand Brains Project: Yeah, or... yeah, how you've often described.

JHawkins: everything that we've described, I don't think is wrong. In fact, obviously, models have to be learned in other parts of the cortex, right? Models have to be learned in V1 and V2. All I'm saying right here is that it seems that hippocampus is involved. You can't... it's and And by the way, I think all three of these problems on this page are related.

This is... this is a set of just, observations that... things that don't make sense to me.

It doesn't throw away anything about how... We assume models are learned in a cortical column. It just says, hey, that's not the complete picture. There's other stuff going on.

That, that, doesn't quite fit, right? But I'm glad you're.

Thousand Brains Project: Yeah,

JHawkins: today.

Thousand Brains Project: Yeah, I feel like maybe it'd be interesting to discuss more after the next slide, because, but yeah, but I feel like It feels like learning almost has to happen at the same time in the columns, or as in, it would...

JHawkins: I didn't say it didn't happen at the same time. I just said that.

Thousand Brains Project: Okay.

JHawkins: Hippocampus has to be involved. that hippocampus is participating in... and I say new models, not reinforcing existing models necessarily, but learning something new. a new thing you've never observed before. A new arrangement, right? and I'm not saying learning... you can't reinforce existing models about the hippocampus. I think you can. But I'm just trying to think, okay, I'm going to give you a new word, a new this, a new mug, the first time you see a mug with the... with a logo on it, it gets your attention. You look at it and go, oh, that's new, And I... if I asked you later in the day, did you see a new mug today? You go, oh yeah, I did. I was in the... I was at my, Neil's desk, and I looked at it, I saw he had this interesting mug. So I'd have this episodic memory of it. It's not like I said, oh, yeah, I learned a new mug, but I have no idea where it was.

Episodic memory goes away. at some point, and you might still have a model of the mug, so I don't remember the first time I saw a mug with the logo on it. the Nimento logo initially. I don't remember that, but I do remember the mug, so it's the episodic memory fades, but the model persists.

Thousand Brains Project: Yeah. Yeah, I guess I just mean that, as in with, Hebion... updates. you can imagine that each column if we're attending to something. There are neurons becoming co-active together. there's a reasonable chance that they're going to, adjust themselves a bit. And that's where, over sufficiently long timescales. you could imagine... I guess I'm just... that's why I'm saying maybe after the next slide, because I... you were going to talk maybe more about, replay than.

JHawkins: Yeah, But I feel...

Thousand Brains Project: But that, even without sleep and replay. that maybe that's helpful, but not necessarily... Entirely necessary for learning.

JHawkins: that's quite possible.

In fact. I just... I can't imagine a scenario where I learn something new for the first time. That hippocampus isn't involved. I just can't... I tried to think of examples, I couldn't think of one. Maybe that's because my hippocampus didn't remember it, but... but I'm just trying to imagine new things.

Thousand Brains Project: Yeah, or at least not, structured knowledge. yeah, maybe some motor reflex or something.

JHawkins: I'm...

Thousand Brains Project: I'm just, I'll notice.

JHawkins: The kind of models that we're talking about, like models of behaviors and models of morphology. those... those kind of models. Yeah.

Thousand Brains Project: No, I agree.

JHawkins: We're talking about critical columns, clearly models have to be learned in all these cortical columns throughout the hierarchy. all I'm pointing out here, and the learning is what we've talked about. There has to be repetition of, Exposure to, features at locations and, our standard methods here. But somehow, that's not the only thing that's going on. And it's not... it doesn't seem to be sufficient by my, my... By my second point on this slide, which is, there seems to be We don't seem to be able to train all of them, all of the columns on all the models and all the locations just by experience. It just doesn't seem like I have enough experience. I don't have enough time to do that, I just don't seem to be able to expose every... and this multimodal learning, like, how do I... once... if I've only learned something in one modality, but I'm able to recognize in another, how is that?

Thousand Brains Project: And also how fast it is, that seems hard to imagine anywhere, but in the.

JHawkins: so there's... there's some holes here. In... not in the theory of the column, necessarily, but in the sort of our understanding of how hierarchy works, where multiple columns interacting together work. these things were weighing heavily on me. For a long time, and it got worse more recently when we started thinking about, behavioral models, because that was, like, so clear that you can't... a single column can't seem to own behavioral models, so how could it possibly do that, It's just not able to look at everything, so what's going on? So that... that... That's... that was maybe the genesis of this, thinking, it must involve multiple columns observing the object at once, and somehow knowledge of the That the behavioral learning has to be shared among among columns. not all columns can experience everything, therefore they have... but if they know everything, they have to be shared somehow. Anyway, that's got me thinking about this, and then many of these items were stuff I've noodled on for years, but I tried taking it seriously, and I said, let's see if I can make some progress on this. I don't have an answer. Yeah, go ahead.

Ramy Mounir: Naive question here about the, what does the circuitry look like for the hippocampus, how it's connected to, is it just like the neocortex connected to the thalamus, and is it getting the same? Sensory input.

JHawkins: I don't think it does get rejected from the thalamus.

I think... unless someone else knows otherwise. we don't... we've talked about cortical columns all getting, sensory input. And that's true from V1 and V2, but I don't think it's true for higher-up reasons, and it's not true for the hippocampus. So that's a good observation, Rami, that, that's another... we never really addressed that issue. What's the equivalent?

Ramy Mounir: hippocampus is connected hierarchically, exactly V2 is connected to V1.

JHawkins: it's connected hierarchically as it's shown in the Vanessen diagrams, Feldman-Vanessen diagrams, as I've shown it here. The actual connections are... nothing is clean in the brain. And of course, the hippocampus has a different structure than the cortex. It's... it's got the, the hippocampal complex has got the entorhinal cortex, and the cubiculum, and the hippocampus proper. I didn't spell that out here. It has, the architecture is reminiscent of cortical architecture, but it's it's different. if you recall, one theory of what has been positive, which I like. Is that the hippocampus And the entorhinal cortex are separate organs, but they folded on top of one another, so the hippocampus is folded... it's like a sheet. So try and imagine a sheet of cortex. When it gets to the edge of the sheet, you're at the edge of the sheet, then it starts getting different looking. And, the enthorinal cortex is a three-layer structure, and then after that is the hippocampus, which is another three-layer structure. But the hippocampus is physically folded on top of the entorhinal cortex, the edge of the sheet is bent over on top of each other, and there's connections Between the hippocampus and cortex. And one theory was that basic folding of a three-layer structure on top of another three-layer structure led to the evolution of the cortical column. And there's...

Ramy Mounir: Honestly.

JHawkins: it's not the... it's very different, but it has some... it has some similarities to Cortec. But it's not clean, it's not like prototypical Cortex at all. It's messy. But it is... connected in a generic sense, like I've shown here. If you look at the Feldman-Vanessa... Vanessen diagrams, they show the top box in their hierarchy as the HC. It's just a block diagram, not showing what the details are.

Thousand Brains Project: Ian...

JHawkins: Excellent.

Thousand Brains Project: I could be wrong, but I think often the entorhinal cortex As well as, what's it called?

LEC are... I'm just trying to look up online, see if there's a good diagram, are intermediary Areas for the rest of the cortex.

JHawkins: What do you mean by that?

Thousand Brains Project: So as in, I think a lot of the cortex doesn't connect directly to the hippocampus. But, connects to those... Enterital cortex and LEC, I think...

JHawkins: I think LEC in particular gets a lot of.

Thousand Brains Project: sensory and association area inputs and stuff, so that's... that's often considered, the what. Of, what's being encoded.

JHawkins: So you're saying there's a lot of convergence on the LEC? Is that what you're saying, from a lot of different regions?

Thousand Brains Project: I think so, yeah, I... That's the least... With these diagrams.

JHawkins: Two pieces of j.

Thousand Brains Project: testing as well.

JHawkins: Yeah, there has to be convergence Somewhere, because, you have these separate sensory regions that All funnel down into one anterinal cortex and, and one hippocampus, and of course, some of the higher regions, there's a lot of convergence in these higher regions as well. so that is suggested in this little toy diagram.

Thousand Brains Project: yeah, yeah, exactly, so that would be the block just below the campus.

JHawkins: so you could.

Thousand Brains Project: Rather than all-to-all.

JHawkins: It could have been entorhinal cortex there. I think there's a lot of... weirdness. If you look at these actual filament vanessen diagrams, it's all kinds of regions that are all kind of converging and diverging, and it's messy.

but none of that scares us, we just need general principles, how they communicate with each other. But the exception is the andoronto cortex and hippocampus, because they look different, and they perform differently. we have a... I have a theory of how the hippocampus, how it's able to learn very rapidly using, silent synapses. And so that's the one place that it's known to have lots of silent synapses, and that's the one place that can learn very quickly that we know, synapses can be grown they've shown the growth of synapses can occur pretty quickly, but I think the fastest I've ever seen is, a minute, so that's not enough for... the kind of instantaneous learning we can do with the hippocampus. And that may be rare anyway. Any more... any more discussion on this picture before I jump into a bunch of text, trying to resolve how this might work?

Thousand Brains Project: No, I just thought I'd mention that, yeah, I think it may be wrong, but I think in general, the consensus view is, yeah, medial enteralhinal cortex is where, grid cells are, and where the what is represented going into the hippocampus, and lateral enterhinal cortex is generally the, what. Sorry, I meant... I don't know if.

JHawkins: everywhere.

Thousand Brains Project: Good cell is where? And then lateral intralinal cortex is what? Huh.

JHawkins: So there's... I think what you're saying is there's this general belief that there's these two parallel what and where pathways in the different sensory modalities, and that's... and that seems to be preserved in the entorhinal cortex as well.

Thousand Brains Project: Yum.

Ramy Mounir: Is that...

JHawkins: And...

Ramy Mounir: Analogous to layer 4 and Layer 6 in the cortical column, the what and where.

JHawkins: It's huh.

Ramy Mounir: blend.

JHawkins: No, they're more like.

Thousand Brains Project: Or layer 4 and 6,

JHawkins: It's, the what and where pathways are just two separate regions.

Thousand Brains Project: But you're saying within a column, Rami?

Ramy Mounir: Yeah, within a cargo, so in the... in the....

JHawkins: I know.

Ramy Mounir: hippocampal complex.

Thousand Brains Project: I feel like the input... it's like the input... From lateral intrarinal cortex is the, input to L4. L6 is medial entorhinal cortex, the where. For representation, and then L4 is the hippocampus, the actual place cells that are binding the two together.

Ramy Mounir: Okay.

Thousand Brains Project: Yeah, maybe.

Ramy Mounir: I was just wondering about the associations between the what and where in the intranial cortex, and.

Thousand Brains Project: Yeah.

Ramy Mounir: It would be similar.

Thousand Brains Project: Yeah, I feel like the what and where here is more similar to, which I think is what you're getting at, the what and where within our columns, rather than the what and where in the ventral versus the dorsal strings.

JHawkins: Oh, I thought it was more like the dental versus the dorsal stream.

Ramy Mounir: No, not at that.

Thousand Brains Project: I guess it's similar in that it's like a macroscopic anatomical thing, but... But the one where... generally isn't about reference frames, I feel. It's more....

JHawkins: It's more like...

Thousand Brains Project: Egocentric versus allocentric.

JHawkins: I always thought that the two divisions of the anterior cortex were more like dorsal and ventral streams. I think that's what most people think.

and, Rami.

Thousand Brains Project: Maybe it is, but what was...

JHawkins: to where... where are the... where, the Layer 6 location and the Layer 4 feature... you can think of those as what and where, but that's a different... I think that's a different what and where. That's... that's just basically saying this feature's at some location, whereas, as Neil's just said, the what and where pathways are more like, oh, egocentric models or allocentric models.

Thousand Brains Project: Yeah, in the dorsal and menstrual stream, but I do feel like with the hippocampal complex. the hippocampus is learning, a structured representation of, what is the sensory experience at a location. That feels what our columns are doing.

JHawkins: but I see.

Ramy Mounir: the division.

JHawkins: between medial and vent... the lateral and the medial intraminal cortex are more like dorsal ventral, I don't think... because they both have grid cells, right? Don't grid cells exist in both the medial and.

Thousand Brains Project: I'm not sure, I thought they were.

JHawkins: they wouldn't... okay, they wouldn't... okay, so here's an interesting thought. The classic grid cells representing locations in a rat's environment would only be on one side. but there would be equivalent grid cells, In the same way that... You know what I'm saying? It's that's an Alice... that's a... That's a, That's like a what pathway? Classic grid cells are... Only... you might have grid cells on both sides, but they wouldn't see them unless they knew what to look for. Because they're not going to be modeling the same thing, just like the what and where pathways don't model the same thing. I think... I think we can put this aside for the moment, perhaps? Yeah, no, that's fine. I... I'm not... I'm not going to suggest anything as detailed as what we're talking about here. I'm gonna... I'm gonna suggest, or at least propose some things to talk about that are more very large block-and-tackle ideas. okay, what... what... what could be going on here? And... And in the end. it's consistent with a lot of things people think about the hippocampus already. I just... I haven't... I just have a... it's so more like it's... it's... I've gotten it in a different direction, and it seems more clear to me. What might be happening. And what I'm about to present, I'm certainly not 100 certain about it all. But at least it's a way I've started thinking about this problem. How it is that columns can learn when they're not exposed to stuff. And how does the hippocampal come into play?

Anyone want to ask... discuss any more on this page before I do that?

Viviane Clay: I guess just maybe to throw out one alternative view on the... on the second problem. I'm not sure if... I don't think it's as much supported with anatomical evidence, but... An alternative to all columns needing to learn all the different models so we can recognize an object with any patch on the skin. An alternative to that could be that the movement information of whatever patch on the skin is currently sensing is somehow broadcast or pooled in a common place that has that model. Because basically. The movement information is universal, and wherever that model exists, even if it is in visual cortex, it should be able to recognize the shape of the object, or the object based on shape, just from the movement vectors.

JHawkins: No, you'd have to... but you'd have to pair the movement vector with the sense feature at that location, right? So you'd have to broadcast... I'm agreeing.

Viviane Clay: Yeah, I guess movement and orientations, maybe.

JHawkins: You'd have to also broadcast the features that are detected there, right?

Viviane Clay: if you learn something with vision, and then you're inferring with touch, or the other way around, you can't really share the features. I'm just saying, the shame...

JHawkins: God.

Viviane Clay: So it should be, like, the morphology.

JHawkins: Okay, alright, yeah, so the orientation. It would be the location and the orientation have to be broadcast.

Viviane Clay: Yeah. I agree. broadcast or, learning associations between, movement of a, A patch of skin, a movement of the eye or something. Or having, a common place higher up in the hierarchy that kind of gets pulled information off that and has a... More multimodal or, general morphology model.

JHawkins: These are all things that I thought about, too, but... and they're consistent with what I'm about to present, so it's not really an alternate view here. When I say that... I say, Learning is shared, and the second thing, it suggests that learning is shared somehow. That means that somehow things are broadcast.

That's what I mean by that. I think it's the same thing you're saying, that a column.

Viviane Clay: Yeah, I guess what I'm saying is, if we have that, then we wouldn't need... I don't need a model of a cup in my... whatever patch my wrist connects to in the neocortex to recognize the cup with my wrist.

JHawkins: you don't have to have a... you'd have to have a model to recognize it there.

Viviane Clay: Not if that... the movement of that patch is broadcast to a different column that's learned in Europe.

JHawkins: that's clearly the case, for example, when we when we, learn an object with touch, and then I can ask you to identify it for vision, right? I'm not proposing that, the visual columns actually formed a model of the cup.

I'm not saying, V1, after I, after I touch an object with my finger in the black box, I don't think anyone has models of it, but somehow I have a visual imagination of it, and I'm able to imagine what it's going to look like.

Viviane Clay: Yeah.

JHawkins: I guess I'm not... I'm not disagreeing about anything you said there.

Viviane Clay: Okay, yeah, Yeah, I think, yeah, it must be part of the solution, but yeah, I wouldn't know which connections would actually work for that kind of sharing of information. But yeah, maybe you go ahead with your next slide first.

JHawkins: That's the problem. on the second point, I... for a while, I used to think about not all columns are getting information. I've shown these... these blue arrows, those are representing the columns that get information. And the other columns are not getting anything, or they're, they're not... because they're... if it's vision, this is V1, those columns, the... the... there's... the center surround receptive fields are... are quiet.

And, so one thing I thought was, like, oh, even in V1, training could spread horizontally within V1, take the blue arrow on the right. the columns next to it are not getting input, and so could it say, oh, I'm learning something, you guys, my neighbors, should learn too? And we talked about a paper once, we looked at this paper where it showed that activity in a column does spread to comms nearby that are not being driven. So I thought about that possibility, that there could be this horizontal spread within a region. But it didn't seem sufficient, and it didn't... and it may actually still be happening, but it's not sufficient to deal with all these issues.

there's just... stupid thing.

Viviane Clay: Anything... Sorry, go ahead.

JHawkins: No, I'm just saying there's... there's multiple... Possible ways that bet.

columns can learn something without being exposed to it.

Viviane Clay: Yeah, I wasn't even referring to learning, I was just thinking of inference, we could do inference in a column that isn't actually getting sensory input, just based on motor information from a different sensor.

Thousand Brains Project: But when you have to... you'd have to somehow associate the input feature across modality Which is, where it feels like the hippocampus would be quite good, or some sort of, association area would be quite good. It feels hard to do that in, primary cortex.

Viviane Clay: Yeah, higher up in the hierarchy, or... But yeah, we would have to associate... Movement and orientation information. Between...

JHawkins: But not all features, right? You can't learn...

Viviane Clay: Yeah, not the features.

JHawkins: I can see the mug with the logo, but I can't feel the log with the muggle.

Viviane Clay: Yeah.

Thousand Brains Project: But you also need to somehow like, when you feel something like a hard edge. That kind of awakens something visually in you.

And And yeah, and it feels like an association area, whether that's Super Campus and or others, would be the best place to have that learned.

JHawkins: So this is... this... these are... this conversation, it's just the kind of conversations I had in my own head. Which... which... but they're all fuzzy, right? It's, yeah, it has to do this, it has to do that, but how? What the hell's going on, That's what bothered me about it. we need... we need concrete mechanisms here. To solve these. All right, let me go on to the next slide. The next slide, unfortunately, is just a bunch of text. by organizing them in numbered paragraphs, we can talk about one at a time, and I may want to come back and look at this picture of the hierarchy as we're doing this, or just try to keep this little picture of the Bottom left here in your head when you talk about this. And again, I'll say it again, this is not a concrete proposal, but it's got some ideas in it that are interesting. and this is all hypothetical, Let's just start with number one. I've already stated this, but I'm just gonna state it and say, okay, this is part of the solution. But the hippocampus is always learning. It never really stops. It's always getting... when you're awake, it's getting information. And it's going to play a role in learning all new compositional models, that anytime you're learning new structure in the world, the hippocampus is going to be part of that, at least in the beginning.

Then I start at number 2 here.

I have this vision in my head, I'm gonna... this paragraph tries to describe this little vision. But you can imagine the world is just things composed of things right? And... and these models have varying levels of depth, meaning, some things are arguably structured in many layers, and some things are shallower. But there's always a limit to our knowledge. There's always a limit to what we've modeled. And... and there may be more structure out there. There is always more structure out there that we haven't learned to model yet.

And, so there's this sort of boundary to our knowledge, which is, oh yeah, I've learned... I've learned all these models of structured things, but perhaps there's new structure built on the previous things I've learned. That I haven't learned yet. And I'm studying, I'm gonna learn those things. So the brain is continually trying to extend its models of the world by forming new compositional structures. And some of these will last, and some of them won't. Some of them will be more permanent, and some of them will be less permanent. It's it's just like heavier on learning type of thing. It's oh, I see arrangement of cars in the intersection, I'm going to learn that, but it goes away, it doesn't come again, so I can't learn that particular arrangement of cars. But I see an arrangement of letters and a new word, and that repeats again and again, so I can learn that, because it statistically repeats over and over again. So it's just the brain is just trying to continually Deep in its hierarchical structure of composition of the things it knows, and it's always forming new compositions and seeing if they stick, seeing if they're... if they come and repeat again over and over again. So it's constantly trying to learn. And so the hippocampus, now we'll move on to point number 3, and this is a question in number 2. Okay. I think with the hippocampus is a staging area for attempting to extend its models.

It needs... this is where you have to You have to be able to build new models here very quickly, because you may only be exposed to them briefly. And it's at the top of the hierarchy because, presumably everything that's coming in... let me just read it here. It's a staging error to extend models. It's where new compositional arrangements are first learned. And this is important, it's not just for very complex things. It can be new arrangements of simple objects, such as, I'm learning a new word, and there's letters, and if I see a new word of new letters, a different arrangement of letters, I will focus on and tend to that. Or a new shape, even. But it's also new arrangements of complex objects, such as the mug on the logo or cars at an intersection.

anytime it sees some new arrangement of anything it's learned before, the hippocampus gets involved and says, oh, I'm going to try to learn this. And it's a staging ground, because it's going to try to learn it, but it may not persist. Like the intersection, it may go away, or that this is on the table, it may not happen again.

It's okay, I'm gonna try, I'm gonna learn this, and let's see if it sticks around for a while. And to understand how we can deal with little objects like letters and big objects like... like cars. you're running... there's gonna be an intentional mechanism, and I'm gonna talk about attention in a moment. But essentially, the hippocampus is saying, here's a new arrangement of things I haven't seen before. Let's attend to these items one at a time, and see if we can learn a new compositional structure.

The hippocampus, number 4, is fast, we know that, so it can learn a new arrangement in a single presentation. Now, how does it learn a new presence, a new arrangement? You might think it works like our current columns, like a reference frame with features. And the... the entorhinal... the... the grid cells and the play cells are reminiscent of that. But it's also... we know that the epicampus forms episodic memories, which are different. We don't assume that's happening in cortical columns. And episodic memories are just... it's a memory of where and when in the order in which things occurred.

So, we know that it does that. It records the order in which the features were attended to, not just their location. if I were... if I were putting my finger in the black box and feeling a new shape, trying to learn this new shape with my finger. After I do that, I would remember the order in which I touched that object.

that 100, but it's not oh, I have no memory of where my finger was, or, I would have this sort of episodic memory, oh, I first felt this, then I felt this, and then I realized it was rotated, and things like that. And what the hippocampus can do, which most cortex can't do, it can recall specific sequences of observations. The order in which they were experienced. Where in a current, in a cortical column, that doesn't happen. The order in which things are learned is not important, it's not saved, it's not remembered, it doesn't matter. We used a, the reference frame to make that so it's the case. did.

Ramy Mounir: Sorry.

JHawkins: Go ahead, Ron. No, go ahead.

Ramy Mounir: Unless it's a behavior, like, a sequence where the order of the... Sensory input was important to form that sequence.

JHawkins: Oh, you're... that's correct, you're right. I wasn't thinking about behavior.

Ramy Mounir: It's only for, objects, learning objects, it doesn't.

JHawkins: Absolutely right, that's a good point.

and it's funny, in the behavioral one, It's still a little different than in the hippocampus, because the hippocampus, episodic memories are placed in time and space.

it's... it goes even beyond just the sequence. It's like, where was I when I did this? What was the, the time of the day? And, these kind of things. Our store.

Thousand Brains Project: It's interesting to think about. Yeah, how, It does feel a little bit like how time is represented is different in hippocampus, and Behaviors like a song, or... a sequence of movements of something. they just feel... Different, whatever that's worth. it feels time and.

JHawkins: It's

Thousand Brains Project: Hippocampus feels much more, biographical, and like, when did this happen relative to me, and all this kind of stuff. Whereas, I guess what we think would be learned in columns feels much more like Oh, this is a pattern that plays out, and it repeats, or whatever.

JHawkins: The sequence in a column is a high-order sequence. It is the structure. We're in the hippocampus, I think you said it well, Neil, so the hippocampus is... it's not really representing the structure of things, it's just representing the order in which you did it. It's there's nothing in the world that says, I touch... I should touch an object in this order, but that's the way I did it, so that's what it's going to remember.

Ramy Mounir: So in the hippocampus is more egocentric, and the cortical column is more, object-centric, or, with respect to the actual structure of the sequence that we're learning, but not really with respect to me. It's not egocentric representation of what happened.

JHawkins: I'm not sure... I'm sorry I've used that word, Rami. It's more like... it's a melody is a structure in the world. It is... it is something that exists in the world through time, and But the order in which I touch an object is not part of the world. I'm not sure it's egocentric, it's... It's just... It's just a history,

Thousand Brains Project: Yeah, it feels like there's a sense of, like, when something happened. And of course, you can have a episodic memory of a sequence as well.

Ramy Mounir: But it feels... it feels very...

Thousand Brains Project: different somehow. It's oh yeah, that happened at that time. I remember that event. It's like a point in time, which is very different from just, oh, I'm at this point in the song, or whatever. like you say, Jeff, it's... that's just an object that exists in the world. Or a structure that exists in the world, and you can be at some point in that structure.

JHawkins: Think about it this way. We all hear melodies. We're all going to learn the same sequence of the melody. No one's going to learn it differently. We're all going to have the exact same... because it's in the world, it's part of the... it's a real thing in the world, where if we... each of us were to feel the object with our hand in the black box, we would move our fingers differently, and we would remember how we moved them, but it's not... it doesn't really matter. yours would be different than mine. It's not really part of the world, it's just... it's just a memory of what I did. It's... it's a... It's... it's not any kind of structure in the world. It's just a history of my behavior.

Alright. Go on the number 5 here. what was really striking to me was, like, how is it that I'm able to learn a new word based on very low-level objects like... like letters, but at the same time, I can learn a new phrase based on mid-level objects like words. Or I could even learn a whole, a new, I don't know, speech, a sequence of words. It's I can append my models, I can add new models at different levels of composition in the world.

and... and how is it I'm learning a new model of an intersection between cars and bicycles, and the moment later, I'm learning a new shape, which is just a bunch of edges, it's what it feels to me, if the hippocampus is going to be involved in all these things, it has to be a way of narrowing down And saying, let's all talk about this right now, because I'm only interested in this thing. And this is what attention is. I'm just going to define attention here, or at least a part of it.

I think I've said this before, so you might have heard me say this, but I think attention... when I first started thinking about attention, I used to think of, oh, I'm going to attend to part of my retina, part of my visual scene, but I don't think that's right. I think attention is defined as specifying a region of space. that we're interested in. It's a region in real space, out there, And I don't know how this is specified, but this feels like what it is. It's I tend to something... it's... it's not like just restrict what's coming in on the parts of my retina. It's more there's a... there's a location in space relative to my body. And that's what we're attending to. And any column That is getting input from that space. I'm interested in. And if a column's not getting in front of the space, I'm not interested in it. It's and I don't care if that column's a visual column, or a touch column, an auditory column. It's anybody who's sensing in that space, that... you can think of some volume out there, or some area. on an object.

is what we're attending to. That's the definition of attention. It's saying only comms that are sensing within that region get to vote, meaning the other comms may be getting input, but they don't get to vote. We're only going to vote on the things that are being, tended to. And this relates very much to what you're working on, Rami, about, figuring out where the child objects are. In... in a compositional structure. and I don't... I'm not going to suggest your work and how to modify it yet, but I'm just pointing... I think this is what's going on in the brain. And there's always this idea of attention.

That, if you're sensing the world. And... and I... I don't know where this... who's... who's in charge of this. I'm not sure... I'm not saying it's the hippocampus or someone else, but usually, you would think it's in something that was unexpected, that is, something that was someplace that we didn't expect to see it. That's what we're going to attend to, because that's an area that I could learn, where there is a... unexpected, Arrangement or unexpected item. And I don't know who specifies that, whether it's the hippocampus, or Collins can do it, I don't know. But let's accept the fact... I like to accept the fact that Somebody has to find a region of interest.

and that region of interest is... is defined in space, and that any... anybody can... Any kind of column that can get info from that space is valuable. If I define the region of space off to the side of... to the left of my gaze, my... my eyes might move to attend to that region, so I have more columns in that region of attention. Or if I... if I felt something unusual. On the back of my hand, I might rotate my hand around to feel it with my finger, so that, so I get, put my hand on this table I'm sitting at, and the back of my hand felt something a little unusual, I might turn my hand over and look at it. And so I might be, by attending some space on this table, I'm also... I'm moving my eyes there and my fingers there, and so all the input I can get coming from that region is useful. And what ends up getting to the hippocampus. Since we've restricted all inference to be what's in that region. We're not gonna infer anything else that's not in that region. That is what's going to rise to the top of the hierarchy.

And the input to the hippocampus is the largest object, Recognized within the attended region. If, in the attended region, we do not recognize as a previously learned object. It's... somehow the structure is not known, and the voting doesn't reach a consensus. Then the attendant region will narrow down until we do recognize something. maybe I don't recognize a word, and then I zoom in on the letters, and then I see one of these letters is a letter I've never seen before, then I can zoom in on the features of the letter. Or if I'm at an intersection, I'm looking at the different cars, and all of a sudden, one of the cars is unusual, I tend to that, and it doesn't match, I don't recognize it as a car. something else there, so then I would zoom in on and say, oh, it's a three-wheeled vehicle, or, I don't... I would keep narrowing down my attention until I find something I do recognize and start learning structure again. So let me just, walk through this again. You're the hippocampus. We're trying to learn new structure. Somebody, me or somebody else, has defined something which doesn't fit within a model. we all then, the whole cortex, including the hippocampus, says, okay, let's attend to that region and see what's there, and if I recognize it, then I say, okay, that's a new item in some location in the reference frame, or if I don't recognize it, we zoom even further down and say, okay, there's... that's not a recognizable object, let's go even deeper. Until we find something we recognize, and then we can start building up compositional structure again.

There's a lot in that.

Thousand Brains Project: Yeah, no, I like that, though, the idea of attention With that as a heuristic.

JHawkins: I think that's what we do, it's... it... Yeah, I observed myself doing this, Tristan Slominski: Alright,

JHawkins: Yeah. Tristan Slominski: I have a question, because a counter example to this, specifically to the phrase, region of space, comes to mind, is if I'm listening to you, closing my eyes and looking away, I am attending to... Like, how does listening to a podcast while you're doing a whole bunch of other things, how does attending to a podcast or audio. seem to fit specifically the phrase, a region of space.

JHawkins: So yeah, you're right. Absolutely right, Tristan. And I don't know.

Thousand Brains Project: normally, doesn't it, actually? I think that's what, the cocktail party problem is.

JHawkins: You could focus on a region of space where the input's coming from, But.

Thousand Brains Project: And that's why it's difficult for people with hearing aids, for example, to... they struggle a lot with parties, and... Crowds of people.

JHawkins: Oh, I see.

Thousand Brains Project: Is they... it's harder to localize sound.

JHawkins: And

Thousand Brains Project: they just can't really attend to it in the same way. It feels like if anything that's in headphones, you're probably just attending to, your head, and because there's no other sound going on.

Viviane Clay: This seems more about...

Thousand Brains Project: It's simple.

Viviane Clay: Attending to something in conceptual space versus, having trouble actually getting the auditory input.

Thousand Brains Project: Yeah, because I guess there is no issue with getting auditory input, but... But I don't know about you guys, but if I'm multitasking, and I'm sufficiently focused on something in front of me, then whatever I'm hearing. I will stop. hearing. at any significant level. again, it feels like you can't attend away from it.

JHawkins: I think there's two things we're talking about here, and maybe Vivian just touched on it. One is, Neil's... you rank a great point. We're tending... we do, when we're listening, we attend to some point in space. And, that... and we're trying to restrict input from that space, that point. So that's correct. I think that's a good... but I think there's this other issue, which is there can be another problem. Let's say, often when I'm listening to, a podcast or something, and I'm listening to it really fast.

And then I couldn't understand something someone said, right? So I back up, and I slow it down. And I listen to it again. And maybe even then it's too fast, and I back up, I slow it down even further, until I get to hear what they said. I think that's a different type of... I think that's more what Tristan was talking about, I don't know, but... or maybe Vivian was talking about. It's There was a secret... there was a series of patterns which didn't fit in my models of... of patterns of... temporal patterns. It's not a spatial pattern, it's a temporal pattern, but it didn't fit. It was... it was... it was a model that didn't work. I didn't... it didn't sound like a word I knew, or a phrase I knew. And so I had to go back and then attend to the components of it slowly to make sure that I did understand it, or maybe I learned something new, something like that.

Viviane Clay: I feel like both of those are still about the sensory input and getting the right sensory input, but... at least what I was thinking of when Tristan brought this up was, like, actually... being in conceptual space, if I'm driving and listening to a podcast, I feel like I'm not even... obviously I'm paying enough attention to drive, but my brain isn't really registering it, I'm not really remembering... what I'm seeing. My brain is, having a bunch of mental imagery and moving through conceptual space of what the person is talking about in the podcast. the words themselves elicit, a space in my head that I'm moving through and imagining and paying attention to.

JHawkins: I see what you're saying. Okay, so that... that makes a lot of sense. I guess what you're... the point there is we don't really have a good understanding of the conceptual space yet. We talk about it a lot. It's gonna be based on grid cells, but... we don't really have a deep understanding of what it is, but I... if we accept that it exists, then I see what your point, then... then, we could have a mismatch in conceptual space.

Ramy Mounir: There's the example of also two people talking at the same time, and it feels like I could filter, I could... actively switch between one or the other, and basically pay attention to one, and completely disregard everything the other one's saying, and it's really... there's no location there, but it's more that... it feels more like model-based attention, or I'm... I want to listen to something, and I'm basically not attending to everything else that's happening at the same time.

JHawkins: Maybe, I could, to a cheat here.

Vivian brought up conceptual space, which we talk about a lot, and we don't really understand it deeply, but we say it's gonna work, it's gonna be built on grid cells.

My... my argument here that we're specifying a region of space could include conceptual space, I think that's what Vivian was suggesting. And... and... but I can't really think about it too clearly. It's easier... much easier for me to think about physical space. that's how I'm thinking about it right now. But if... if... it should apply to conceptual spaces, too. I think that's Vivian's point. So even if we don't understand it completely right now, it's okay, we can go forward and keep focusing on physical space, because in theory, whatever we do, physical space will work for conceptual space.

Scott Knudstrup: So I'm trying... I'm trying to think about something you want to attend to that is moving... that is itself moving around in space. a dog walks into your apartment or something. You're going to continue attending to where that dog is in your physical space.

Something is informing that, so something is helping you update the location in space that you want to attend to.

And it's still spatially bound to begin with, right? because the animal is still moving around in your house.

JHawkins: Goodbye.

Scott Knudstrup: What's up.

JHawkins: I think... I don't think these are counterexamples. The dog walks in your house, maybe you weren't expecting that. That's like a car showing up at an intersection. And... and you can attend to it. You may not attend to it forever, because, eventually you just give up, and you start going back to work, and you don't remember... you're not attending to the dog, and you don't see where it goes, right? But I think the dog walking into your house is very similar to a car at an intersection. It's a... Yeah, It may or may not be repeating. Maybe the dog walks in the house every day using the same pattern, then you can learn that as a behavior. Other... it might be just random, and you can't learn it,

Scott Knudstrup: So I'm just thinking about what... what is it that initializes that location and space that we want to attend to? What is it that defines that?

JHawkins: Don't think...

Scott Knudstrup: Like a model-free sort of stimulus.

Thousand Brains Project: Yeah, it feels like a Model 3 and... a mixture of Model 3 and Model Base, As soon as anything is moving in your apartment, probably you get model-free attention, but then you recognize And then you're like, oh, that's

JHawkins: Not necessarily.

Thousand Brains Project: to that.

JHawkins: There might be something moving in my apartment all the time. And it's always there, and then I don't notice it anymore.

Thousand Brains Project: yeah, but then that would probably be model-based, like... I'm just saying it's... it's a... it's probably a mixture, but, it... It almost always has to start with some amount of Model Free.

JHawkins: I think when we say Model 3...

Thousand Brains Project: you

JHawkins: I would say a model... my model doesn't predict it.

you could call that model-free, or you could say. I could say, look, I could learn my dog walks in the door every time I pick up my coffee cup, and he goes and sits down in the corner, right? And at some point, I'll stop noticing that, because it becomes a learned behavior. It's just, it's just I can give examples of things like that.

it's... is it model-free? It's... it's basically... it's outside of an existing model, but it could become part of a model. How about phrasing it that way? It's... it's not purely... you could think of it different ways. You could say, something... something unexpected happened, but it's only unexpected because it wasn't incorporated in the model. You remember the, example sometimes I use? The kid's in a classroom, and the teacher says, focus on the board, and then someone opens the door on the side of the room, and everybody turns their head to it.

If that door opened all the time in a consistent, persistent way, people would stop turning their head to it. So it's not purely model-free, it's... It's it's unexpected in my current model, but it could become part of a model if it was repeated consistently. It would suggest that it's not... Completely based on, outside of models. It's it can become part of a model, and then I won't notice it anymore.

Never.

Viviane Clay: Yeah, I think that makes sense. I think Neil's having headphone issues,

JHawkins: Who is he? Niels, can you hear us? I've got all my people turned off here.

Viviane Clay: Yeah, I guess in terms of conceptual space, coming back to that while we're waiting on Niels. it almost seems like it's also using the same kind of hippocampus mechanism of quickly laying down points. if I'm listening to someone talk about Monty, and sensor modules, and learning modules, and how they connect, it's like I'm forming a quick model of, Putting these things in a relative arrangement. connecting them with lines or something in my head, and then I can also focus my attention on different parts of that kind of conceptual space, even though, learning modules and sensor modules are not really something that physically exists, or that I've ever sensed with my sensors.

JHawkins: Yeah.

Viviane Clay: But still, conceptually, I'm putting them in some kind of space, and then attending to different elements as you talk about them or describe them.

JHawkins: And you typically have an episodic memory of that sequence of thoughts.

Viviane Clay: Yeah.

JHawkins: Just like we'll say, oh, what... didn't we just talk about this a minute ago? We were bringing up this, or, Romney said something, whatever.

Viviane Clay: Yeah, actually, like, when you listen to stuff while driving or walking somewhere, sometimes you associate the information with the place you were at when you heard about it. And you go back to that place, you're like, oh, this is where we talked about that, or...

JHawkins: That is true, that... I see it all the time when I... when I... I can't remember faces, as or names, and... And, but if I see the person in the right context. The name will come to me. but if I see the image out of that context, I just don't get it. Yeah, no...

Thousand Brains Project: I guess clearly the hippocampus has evolved to specialize in remembering, environments and where we are in an environment, which is, I think, the whole concept of a memory palace. That it's much easier to... remember large amounts of information if you place them in a hypothetical physical space. Rather than, yeah, just

JHawkins: But it's interesting here, though, it's funny, because I think about the hippocampus. neural tissue, they don't... neural tissue doesn't know anything, right? it's It's just dealing with locations and regions, and it doesn't know if those are physical spaces or if there's conceptual spaces. If it's just grid cells, it doesn't know that. So it seems like the mechanism should work, universally. I'm just guessing.

Thousand Brains Project: If I had to guess, When you're learning something in conceptual space. the grid cells in your entorhinal cortex aren't moving that much, and so there's more kind of interference, where.

JHawkins: Yeah.

Thousand Brains Project: you're moving... through... Physical space, or even, mental... or, a virtual or mental physical space, like an environment. Or, a memory palace, then the grid cell code is going to be more distinct when you're laying down information, and there's, less interference, or something like that.

JHawkins: Okay, let's... maybe we should just try to keep going here. I think that's a good observation. I'm trying to paint this picture in your head. The hippocampus is basically somebody saying. There's things here we don't... we don't anticipate. Let's attend to them. And the hippocamp is laying down a memory of this... of these observations. Now, I'm gonna... those... those episodic memories we were just talking about, they are located in some place. I remember where I saw this thing. Or, what letter on... in the new word, what letter was the first letter, the second letter, the third letter, and so on. point number 6 says, while we're awake. The hippocampus learns these episodic memories. And episodic memory, briefly, is a sequence of attended spatial regions, and the objects that were observed in those spatial regions. So that's what we were just describing in the previous point. You're just attending to regions in space, whether it's conceptual space or physical space, we'll leave that out, but same idea. And... and... but imagine physical space, it's a little bit easier.

So you're imagining you attended these things in space, and the order in which you observed them, and what was observed at each... at each location. That's what the episodic memory is. It's just... it's just a recall of what you did, and what you observed. Now... Next part is, it's... it's well-known belief theory, but I don't really know... I don't know any of the details of it. Number 7. it's... it's certainly... we know that during sleep. memory consolidation occurs, that if you don't get the right sleep, you don't remember things. there's a lot of research about this. How... during sleep, your memories are consolidated, whatever that means. and there's also... A lot of evidence that some of these episodic memories are replayed during sleep. cycles, and I think sometimes they're replayed very quickly, if I recall. They're not replayed in real time, they're replayed very rapidly. But they can see the neurons recalling sequences of events of the episodic memories. In a rat, for example, what the rat did, very rapidly. Now, during sleep, we can say that no comms are getting sensor input. We're gonna say that's... that's the rule. They're not gonna get sensory input. And as the episodic memories are replayed, Somehow, the hippocampus could enforce The region and object pairing That are... that... these are the sequences that were learned in the episodic memory. It can enforce that over all cortical columns. I have a lot of questions, I don't understand this. But the idea would be, like, okay. let's replay this thing I heard. I'm going to broadcast it to everybody. Everybody, you're all gonna pretend you're attending to this region, and you're all gonna pretend you're... you observed some object. Now, some... some cortical columns won't make... this won't make any sense to them, but some columns would make sense to them. But you're basically telling everybody, let's all do the same thing. We're all going to relearn what I just did earlier. You're going to be exposed to this. And... I don't know how often this occurs, how rapidly it... I know it happens rapidly, I don't know if it repeats over and over again, but this is a means by which the The model... the cortical columns and their models Could learn models that they never directly experienced. We're just telling them, you're not getting sensory input, I'm telling you where... pretend you're looking at this region. to this space, and pretend you saw this, and now you're working at this region, and you saw this, and you saw this. This is the first time I've ever had a sense for how How we could form memories over broad areas of the cortex that never actually experienced The behavior directly, or the input directly.

It's not inconsistent with existing hippocampal literature, but it's a bit more detailed and specific, and it makes more sense in this concept.

Thousand Brains Project: Yeah, I... I like if we can get this to work as, a... Yeah, that's a way, and one thing I like is because, And maybe you know, Jeff, I think there's a fair amount of computational theories out there about the role for dreaming and, hippocampal replay, that it's like. But it's focused a lot on, how neural networks and deep learning Relies on this kind of independent and identically sampled distribution. And dreaming is maybe, a way of randomly sampling from almost, an infinite buffer of experience, so that you don't have catastrophic forgetting and stuff like that.

JHawkins: This is interesting, because I'm not aware of this.

Thousand Brains Project: Okay, yeah.

JHawkins: It seems to be opposite of what I'm suggesting.

Thousand Brains Project: Yeah,

JHawkins: you don't want IDD stuff here, you want it to be... you want to do the exact detail of learning,

Thousand Brains Project: Yeah, or at least, Yeah, but that, it doesn't actually work as a solution to, unless you do literally sample from, an infinite distribution and stuff like that, and it also... it doesn't explain how does online learning happen, during the day? it's... you don't have to sleep in order to learn something. You can learn something during the day, and then you just remember it better after sleeping. This feels much closer to, reality, which is, The solution to continual learning and all that stuff isn't going to lie in hippocampal replay. Or stable learning or whatever, it's going to... this is just a way of, as you say, Columns which take a while to learn, or maybe didn't get the full sensory input, can be a bit more exposed.

JHawkins: we don't have the problem that biology has, which is, learning is slow in synapses. Monty can do... we can do quick learning, but it... but it does suggest a way that when learning, we could be training lots of columns that are not getting input at this point in time. I've even asked the question. one of them awake? Okay. What if I'm awake?

Could this mechanism still be at play in the following way? One, I'm away, some of the columns are getting direct input, either from sensors or from other columns, and they're being driven by sensory data. But other columns are not.

And, could this... and the intention mechanism is working, right? let's say the cortex, or let's say the hippocampus is saying, we're attending this, we're attending this. Could it be that this same broadcast mechanism Works, while you're awake on the columns that aren't actually being driven by anything. you often notice that when I'm talking to this group, or I'm listening to someone else speak, I often close my eyes. I... and I always... I feel bad about it, because it's rude. But I do that, it helps me think, and... and it's... it's maybe a way of saying, let's turn off the input to a bunch of my cortex, so that it can be Involved in this, invoked as part of this attentional mechanism, And, I could be training things in my visual cortex, because I'm not looking at anything right now. Something along those lines. My point is, I've described something here that would work during sleep, But, Monty doesn't need to sleep. Monty doesn't need to, it can run quickly. But it's also... the basic sharing could occur During wake periods, too, but it wouldn't be for the entire cortex. It was only for parts of the cortex that maybe aren't receiving anything. At this point in time. I don't know about...

Viviane Clay: Or inference, then, to generalize inference?

JHawkins: No, I... it would be more... just learning. It's again, Medra, I put my finger in the black box, and I'm feeling an object. In some sense, I visualize what I'm feeling. I have an image in my head. But what it looks like as I'm touching it.

And... and so it could be that I'm actually training my visual cortex imagine my visual cortex has no input coming in, and then... or, I'm not attending to it, and so now... so I'm attending to a region in space, but potentially I could be training my visual cortex on that same thing. at that moment, from a sort of top-down mechanism, Obviously, I can only train on the morphology, but... All I'm pointing out, this mechanism I propose that somehow works during sleep might actually be partially operating during awake periods, too. That's... that's all I'm saying. the training records.

Viviane Clay: Do you know what kind of connectivity people propose? To be used for the replay.

JHawkins: Nope, I know nothing about it. All I know is that they... I think it's... they look at either grid cells or place cells. And they see the same... they see a sequence of patterns that they know Was experienced by the animal earlier.

Thousand Brains Project: Yeah, and then... again, it could be wrong, but I think from what I was... reminding myself of earlier, any, reactivation of cortex would probably happen via... Lateral entorhinal cortex and medial entorhinal cortex.

JHawkins: Projecting back the cortex.

Thousand Brains Project: Projecting back to Cortex.

That's like the way station.

JHawkins: I don't, it's this idea suggests that There needs to be a broadcast signal That represents a region of... a region of space That is the attended region. And this has to be in some sort of universal reference frame. that then individual columns can say, I'm in that zone or not, I'm in that region or not. I don't know how that would work. I haven't tried to walk through that at all. I don't know.

Scott Knudstrup: Yeah, I don't know about the replay. Stuff, but... A big part of sleep, at least during a slow wave. If I'm not mistaken, which is distinct, from... if you're talking about replay, that's gonna have to be something that happens during REM, probably, because otherwise the branch is too synchronized.

His synaptic renormalization, most of your synapses strengthen throughout the day, just during waking life. And at some point, you've gotta... bring those synapses back down. In terms of strength. Otherwise, your brain will just be teasing all the time. So during... the idea with sleep is that you take the small handful of synapses that have strengthened the most. And you strengthen them? And everything below that threshold gets weakened. you've got this, mechanism by which the most important things get committed. And everything below that, you just, basically make room Or more memories and synapses to strain the next day.

JHawkins: is this empirical results, or are these theoretical ideas?

Scott Knudstrup: I think it's both.

JHawkins: the re-larminization doesn't make sense to me, because I think about synapses, for the most part, in cortical synapses. they have very little weight. they're binary more than anything else. Either they exist or they don't exist, but the idea that there's any kind of, Any degree of fidelity in the weight is really minimal or non-existent. So there's a lot of theories that people speculate about neural networks. Including deep learning networks, where they assign, synaptic weights to some level of precision, which just doesn't exist in real brains. I'm just... the idea that you're renormalizing weights seems odd to me. It doesn't fit my... my understanding of, Of how synapses really work in the cortex.

So that's why I was asking, is this, proven empirical results, or is this people just,

Scott Knudstrup: Yeah, that's a good question. I'll post about it later, because it's been a while. I'm seeing that there's some empirical stuff now, but, obviously I can't...

JHawkins: It's just

Scott Knudstrup: In the short term.

JHawkins: I think so much of what's... People believe about synapses is wrong.

Scott Knudstrup: And...

JHawkins: And... It's, by the way, it's confusing, too, because if you look at other parts of the brain and other animals. Then there really is the strengthening and weakening of synapses. You go to, the Eplicia. Sea snail, right? They can see that. But in Cortex, it doesn't look like that. And it looks in implicit, all the synapses are there, you just don't... you don't add them and subtract them, they're just changing their weights a bit. But in cortex, we know that synapses are forming and unforming all the time, and the whole theory about sparse representations all suggests that a synaptic weight is really not an issue... it's not something we need to think about. It's more... they're more binary. And you have to... when we go look at literature on this stuff, you have to be very careful what... what they talk about, what animals, what, what is the paradigm, what is the assumptions I make.

But I, just bear that in mind when you're reading through that stuff.

I should just at least mention the last point here. I didn't think about this theory at all in terms of behavioral models.

I want to, I didn't have time. So I don't really know, because we have this... this... this helps me see a path as a solution of, oh, all columns can't experience the same thing, but they all need to learn models. how does that happen? This gives me a path to do that. But it's not clear how it would work for behavior models. And maybe it's easy, I just haven't thought about it.

I just throw that out there, it's like, it's a total void in my thinking right at the moment. But it... hopefully, if you think about it, maybe this basic mechanism would also suggest how a column can learn complete behavioral models, even though it doesn't experience everything. It doesn't jump right out at me, though.

that's a... Something to think about.

Thousand Brains Project: Yeah. That's it. Yeah, I think it'd be interesting to think through, like, how the... how the replay... Actually works in terms of, yeah.

JHawkins: I think, in fact, I don't think... I think mining, we can do all kinds of fun stuff in mining that doesn't have to work like this at all. I think the idea that you might broadcast or train all columns That aren't currently involved in something, have them all learn the same thing in some sort of, basic form. And using attentional regions, could be important, and it could be important for the work that Rami's trying to do now, too.

As much as I understand it.

I just want to make sure... I don't think we have to go and have, Monty doesn't have to sleep. Doesn't have to do anything like that.

Thousand Brains Project: Yeah, it can be a bit more like dolphins, maybe? I think they.

JHawkins: switch off.

Thousand Brains Project: Half their brain at a time.

JHawkins: And.

Thousand Brains Project: That half-brain sleeps.

JHawkins: While the other half is awake.

Thousand Brains Project: And learns.

JHawkins: it's interesting. I remember reading it now, yeah.

That's pretty funny. Does that mean... I don't want that.

Thousand Brains Project: So Monty can asynchronously...

JHawkins: but we can do... we can do it, we can do whatever we want,

Thousand Brains Project: We're just coffee muddles over, yeah.

JHawkins: Yeah, we might have copied Miles, and Monty could, do this stuff in 100 milliseconds, and then get back to the main event, I don't know. But, I've always felt... I don't... never had a comfortable feeling about how can we train models about them experiencing something.

Thousand Brains Project: And, yes, we can copy it, but I wanted to understand how brains do this.

JHawkins: And that can help us think about it. And unless it works across modalities, as Vivian was saying earlier, obviously I can't learn complete models across modalities, but I can learn morphology models across modalities. so maybe, that's something to think about, too.

Thousand Brains Project: Yeah, and then one... for what it's worth, I was just looking now at, V1. at least from some quick Googling, both MEC and LEC have pretty minimal projections, too, there. they exist, but it's 0.5 of the projections to LEC and MEC come from, to V1, sorry, come from... MEC and LEC.

JHawkins: Doesn't have any projections.

Thousand Brains Project: from Entoronto Cortex would be one, but... What's that?

JHawkins: I'm surprised they have any, but...

Thousand Brains Project: But I guess, yeah, cause... presumably we want to project back in order to reawaken, but anyways, but this... I feel like, to me, this suggests maybe that, again. This could be a way to help learning, maybe particularly in higher-level cortical regions, because I think... I think more... higher level cortical regions do get more projections from Entorhinal cortex. but that, something like V1, like you've often said in the past, Jeff, it will learn models, but it just takes a long time.

And it is.

JHawkins: Yeah.

Thousand Brains Project: Maybe that doesn't need hippocampus, but it just needs a lot of time.

JHawkins: it might be hippocus, but here's another...

Thousand Brains Project: But I guess... I guess my suggestion is how would it even get help from the hippocampus based on that connectivity?

JHawkins: I... I can imagine... I didn't know if I needed that connectivity. I was thinking more like. There's some sort of, the cortex... the hippocampal complex. Has to tell all cortex during the sleep periods. That says, okay. here's a region in space, you translate, that... you translate that into your reference frame somehow, I don't know what that, and here's... here's the basic thing you're supposed to be observing there, And...

Thousand Brains Project: So is it projecting back to, almost like LGN, or,

JHawkins: It could be some... it could be something else. It could be... you're right. It could be the, the reticular formation, or, it could... there's a bunch of things that could be playing a role here. I don't know what the mechanism is, I have no idea. I don't think it would be a direct connection from entorhonocortex to every region. I would think it'd be more global somehow, or it would go through some intermediary somehow. there's a whole bunch of people who could... things that could come into play here to make this happen, because the hippocamp complex is going to be broadcasting some sort of egocentric region space, and it has to be converted into I don't know, models. I haven't even thought through this. By the way, I want to come back to one thing about V1 and V2.

Again, I think of the... think of the models in the cortex as this sort of These sort of hierarchical compositions. And when... when something unexpected happens, it can happen at any level in that... in the cortical hierarchy. if I have... if an unusual car shows up at the intersection, like something that doesn't look like a car. That's gonna be pretty high up. in the cortex where that... I don't think it's V1, it's going to be saying, hey, there's something wrong here.

But if I... if I see, but it... some things could actually show up in1, depending on what it is. So my point is, what I'm imagining happens here is that when we're training during this sort of sleep cycle. it's not going... not everybody in the cortex is learning. It almost feels like it would be starting at the hippocampus and going down some levels, and then it's going to stop. This is a very fuzzy idea, sorry about this. is gonna stop, but sometimes you go a little further. Maybe sometimes it goes all the way to V1, but that's pretty rare. Something down in V1 doesn't make sense, It's it's like some basic... structure in the world is weird.

but you mostly... you can imagine this... this... this sort of mass training event The things they're actually learning are, it starts at the hippocampus replaying stuff, and then it's just dynamically going down as much as it needs to. to... to train these regions that... that... but... because... because most of the learning will be of complex objects in complex environments, but... but if it needs to, it keeps going further down, and you tend to smaller and smaller regions of space, and it might get all the way down to V1, say there's a new letter here, it doesn't make any sense to me. and maybe that letter was on the logo on the side of the car in the intersection. So first you look at the car, then you look at the side of the car, then you look at the logo, then you look at the word, then you look at the letter, and you go, holy shit, what is that little thing there?

Thousand Brains Project: Maybe the TikTok logo? I don't know if you've seen it, but it's those two colors, where when they're really close to each other, it looks 3D. I feel like that's something that... that maybe V1 would be troubled by.

JHawkins: you look at it in detail, what's going on there? It's red and green at the same time, Anyway, so these are just fuzzy ideas, but I just not counter what you said about V1, but I'm just pointing out that even in the normal learning process. most learning wouldn't affect V1 because the representations of V1 are not incorrect. We only want to change models or add, extend models where the model isn't sufficient, and so those will mostly be towards the top of the hierarchy.

we're learning new arrangements of complex things we haven't seen before, is mostly what we do. But not always, sometimes there's new, basic little structures That we might learn.

Scott Knudstrup: I don't know how to square these two things, but I've got these two things in my head. One is that... The hippocampus during this short-term this, really rapid learning memory. Let's say you look at a table, plate, fork. Mug, cup, things like that. Those are super... coarse-grained. you're just getting, basically, category tags for each of those things, like plate, that, you're not really learning a super detailed Special representation.

JHawkins: Why do you say that? these are... these are known objects, right? I'm looking at the things on the table?

Scott Knudstrup: Yeah, they're known objects, but I think... I think when you just really quickly learn that arrangement. It seems like you're really learning that's a cup. you're not... unless you really focus on it and try to learn that cup. In detail, this really rapid learning seems to be, like. I'm just going to learn the coarsest. Most general version of this kind of object.

Thousand Brains Project: Exists in that location. It feels like it's some sort of, neural signature that's on a continuum. Because I feel like if then someone was to swap in an object, that's similar... your chance of noticing the discrepancy would probably be proportional to how similar it generally is. if someone puts in something that's technically a cup, and you would label a cup, but it's just, a totally different cup. I think there's a reasonable chance you'd notice it. If it's slightly different. who knows? But... so it feels a bit like... almost like the distance in representational space.

JHawkins: Yeah. Here's another way to look at the same thing you just said. In my house, we set the table every night, and either my wife or I set the table. And... We have a set of dishes, a set of cups and things, and it's... if... if... and I walk in there, and I'll say, oh, did Janet put out the spoons, or this, whatever, whatever. If... I... if one of those cups was not one of the five glass styles we have in our house, I would notice it right away, I would say, hey, that's not ours, that doesn't belong here, right? If I walked into your house, Niels, and you had something, and you had a glass, I wouldn't think twice about it. I don't know what to make of this. My point is. my... my prediction is... can vary. I think there's a... on a scale between, oh, any cup would do, maybe Neil's likes, these kind...

Thousand Brains Project: I'd be surprised if it was the exact same cup that you have.

JHawkins: It would be, I would notice that. You're right. I would say, hey, you bought these IKEA, too. And But it's interesting, it just... it goes back to your point, Scott, about, like, how detailed it is. it's... I don't understand it, I'm just... I'm just making an observation that it can be quite detailed, and I'll notice something's wrong. Or it could be not, if I'm Neil's house. But I think the general rule is, I would... I don't know the answer. Sometimes it can be quite detailed. And the exact arrangement can change all the time. it's not there's a set arrangement, has to be every time we sit down at the table.

Ramy Mounir: No, but if you do the same arrangement every night for years, then...

JHawkins: There is this scale between things that are repeated accurately, and things that are repeated, and things that aren't repeated.

This is similar to the idea that, cups can look differently, that they have different basic shapes, but they're all cups. Or my cup, which has a very specific shape. The same basic thing. Anyway, I'm not sure what your original point was, Scott.

Scott Knudstrup: Yeah, okay, so I have sort of two...

JHawkins: By the way, before you're.

Scott Knudstrup: Figure out how to...

JHawkins: I want to know what your background is, because it's very distracting, I keep stopping...

Scott Knudstrup: Sorry.

JHawkins: Looking at your offset.

Scott Knudstrup: I'll switch it. I don't know, I was just... I checked the stocked backgrounds last time I was open on Zoom, and I just... Thank you.

Thousand Brains Project: I'm wondering this, is it meant to be, like, a... a scene? the inside of, a...

JHawkins: Cardboard buildings with shadows or something?

Scott Knudstrup: I don't know, it reminds me of Inception, when they're, like, bending the city up and stuff like that, but...

Thousand Brains Project: less disorienting if it didn't have that... At the thing at the bottom where it.

JHawkins: you're like a, and so I'm getting distracted because I'm attending to your background, trying to figure what it is, and then I don't hear you.

Thousand Brains Project: Or, a cardboard forest,

Scott Knudstrup: Here we go.

Thousand Brains Project: Okay.

JHawkins: Alright, there we go, there's your home.

Scott Knudstrup: So, the idea of, tagging locations with the most general, object type, or really rapid learning. Really rapid picking up of models. Fits in, I think, nicely with the compositional idea. we're... we'll build a compositional Objective. As well as we can. Which means tagging locations with IDs, essentially. But also, I was thinking about, that's a pretty sparse way to represent a cup, if you can manage it, and, there's nothing special to learn about it, you've just got an idea at this location for a cup, and plate, and knife and fork. So it's pretty sparse, which is nice. Then I was thinking about, model sharing throughout the rest of the cortex. And... It's a lot easier to share model information if it's a really sparse, basic, general model.

it's a challenge to imagine how really detailed special information is going to get relayed between different columns, right?

JHawkins: Let me just... let me just stop you just a moment. Imagine the hippocampus, Has a representation of location, meaning, Region of space. And, it's not a point, it's region, it's different. And it also has the ID of the object that was in that thing. That ID could be very specific. Or it could be generic. The location could be very specific, or it could be fuzzy. the difficulty of transmitting that to other people is the same. I don't think it's any harder if the SDR represents a very unique object, or the SDR represents a generic object. All I'm doing is... it's not like I... it's not like I'm... Remember, all we're doing is specifying objects at locations, and the object ID can unfold into a very detailed object. I'm not trying to pass all the details. if it's my cup on the table, then I don't have to say, here's all the facets of the glass. It's just no, this is the ID for that glass, and that's it. So I don't... I'm pushing back on the idea that transmitting or broadcasting a detailed model, meaning objects that are detailed versus objects that are fuzzy, really doesn't take any more effort.

Scott Knudstrup: needs.

JHawkins: It's just an... it's just location and ID. It's just...

Thousand Brains Project: storing... storing them seems like it would be similar effort.

JHawkins: But...

Thousand Brains Project: I think maybe what you're saying, Scott, is more like, if... let's say this is... The hippocampus knows it's learned about a new object today. And now I want to transfer that over sleep to other columns. Presumably, the more points that there are in that model. The more, at least, time-consuming that would be to visit all those locations and share all that information. Is that kind of what you're saying?

Scott Knudstrup: Yeah. It'll be a lot easier to share, information, like if it's a cup or a mug or something that I'm trying to share between columns.

Thousand Brains Project: Be a lot easier to...

Scott Knudstrup: do that in a compositional setting, where I share a vague cylindrical-looking thing connected to vague handle-looking thing. It's.

Thousand Brains Project: As long as those columns also know about those,

Scott Knudstrup: Yes.

Thousand Brains Project: Stop objects, or whatever.

JHawkins: Yeah. I'm gonna push back on this a little bit. To me, it's... Neil, as you said, look, there could be more points to transmit. if I've learned about the mug with the logo, and now the logo has a bend in it. that takes more information. I had to make more observations of the logo, had to tend to the point where it bent. Otherwise, I can extrapolate between.

yeah, there's a few more observations. the number of observations that has to be trended would change, but I think the quality of the observation doesn't change at all. It's... it's... it's still this location ID, same thing, it doesn't matter what the ID is, if it's a fuzzy object or a specific object. I think the difference between what you call a detailed model and an undetailed model might be the number of points you have to transmit, the number of items that you're... or the number of observations that you made. That would be true.

Thousand Brains Project: But yeah, I think your point, Scott, is just that, compositionality enables us to have sparser models, which is good for this. in your example, Jeff, of the, logo with the mug. If... or the mug with the logo. If you didn't have a model for the logo. You would have to transfer a model Where you had learned all the, colors at specific locations, and that...

JHawkins: But we're not doing that.

Thousand Brains Project: Yeah, exactly. I think we're all in agreement. Scott, was that... It's nice that we get sparsity from, compositionality, and maybe just Kind of a reminder to lean into that, because it's useful for transferring models.

Scott Knudstrup: Yeah, and the final point was, sleep as a potential mechanism for helping with the sparsification process slash building up of categories. So starting to take synthesized information throughout the day, in many kinds of cups. Perhaps that offline period is a period when you start building these sort of general models.

JHawkins: But that's not what I'm arguing. That may be true, Scott, but that's not what my point today was. My point was, you're... you're not doing some sort of data management here. You're just replaying experiences. That's it. whatever action you experience, it's going to be replayed, and the receiving columns that are getting this location and ID information, they can learn what they're going to learn from it. But I don't like the idea of... or maybe I just don't understand it, the idea that somehow we're, like, using this to come up with conceptual things, or to massage the data, or build different types of models. The mechanism I propose here doesn't do that at all. It just says, I observed some ID at some location, I have a sequence of them, I can play them back, and everybody can be told... pretend you're seeing the same thing.

It's... it's not doing more than that. Maybe something else is going on more than that, but I... I'm not... I'm not proposing that. Yeah.

Scott Knudstrup: Fair enough. Yeah, I was just pointing with the idea that the detailed model information is learned. to some degree, in cortex, and the hippocampus is... The reason why it's a fast... Memory is, because it's not storing that much information.

JHawkins: No, I just...

Scott Knudstrup: If it's just storing,

JHawkins: But it is going...

Scott Knudstrup: That are, with the detail information being lost. sucks.

JHawkins: but it might be... the ID that it's storing could be a very detailed object. it's just still an SDR, there's no more extra memory involved, it's just... but it could be a detailed object, It's not like it's... the hippocampus is... is... it never has to trans... all it has to ever do is transmit the IDs of the existing known objects That are childs, or children, of a new composition. So it's only transmitting ideas of already learned things.

And, where it was absorbed in the world.

That's... it could be very detailed, or it could be a super comp... it could be... this is the, the space shuttle, that's the ID for the space shuttle, or it could be, a letter A.

So I'm just trying to get away from this idea that that I keep... I keep hearing the saying is that somehow, we're trying to solve a, information bottleneck problem, or something like that. It's not like that at all. I don't... I don't think it's... it's just a... It's a way of... It's a way of getting all the columns to act as if they actually experienced this thing directly.

Scott Knudstrup: Meaning?

JHawkins: That's all it is. And the information sent is very minimal. It's just ID and location. These are just SDRs.

Hojae Lee: I'll... can I try to share something that might support Scott's idea?

I just started drawing this, I'm not sure if it's gonna work out. Everybody see the Excalibur?

JHawkins: Yeah.

Hojae Lee: Okay, I was making this earlier, and then I gave up on it, but so let's say, we have multiple columns, and I'm ignoring any of the intermediates. And these three are getting input, so let's say this is, finger 1, Not being a one... Finger 2, and, yeah, finger 3, and we're learning, let's say, finger 1 is, on the, handle, we haven't learned a mug, a handle of mug, and this F2 is... I don't know, body... And this is, the rim or something.

JHawkins: Are we learning the mug for the first time?

Hojae Lee: Yes.

JHawkins: Okay, so let me... Let me just...

Hojae Lee: It's like putting, three figures into a black box. And we're touching around the mug.

JHawkins: Okay, so let me... let me stop you right there, because I think you might be coming down a path I think it's... I disagree with.

Hojae Lee: Okay.

JHawkins: When you're learning, you attend to one region at a time. I can't learn the mug By putting my hand in there and grasping all the... I can infer it this way, but I can't learn it this way. Imagine the mug is a...

Thousand Brains Project: Or maybe at a higher level column, you could. it feels The column that's almost getting inputs from all of those fingers could say, Oh, I'm sensing, But then you're attending to a region, and there's one thing you're sensing at that region.

JHawkins: the output, during learning, the output has to be a single object at some location. And... and so if the three fingers do not recognize a single object, because I haven't learned this mug before.

Hojae Lee: Dan...

JHawkins: I'm then... I'm going to attend to some region, and maybe only finger 2 or finger 3 is the finger that's getting the input in the attended region. it's not like we can learn all these together. it's you have to attend to a region at a time. Learning is... it's always, attend to... a region where a single object can be identified. if finger 3 is attending to a region that says, oh, I recognize a rim. Great, but at that time, during learning, the other two guys aren't really... they're not being used.

During inference, they can all vote, but during learning, only one... only one region can be attended to at a time. And if the object can't be recognized within that region, then the region gets smaller. So, these three...

Thousand Brains Project: And then, Maybe, an example where it doesn't need to get that small is, I don't know, again, you put your hand into a box, and someone's arranged, a board game, and so there's some dice on it, and you touch a dice with your three fingers, and you instantly recognize the dice. That is an... now you've... you're learning that there's a dice at that location, and then you move to a new location. But like you say, Jeff, there's, one object. But if it was, like, a totally weird thing, Then you would.

JHawkins: let's say you touch...

Thousand Brains Project: focusing on.

JHawkins: So you grabbed it with your three fingers, and it was, like. the old shoe in the Monopoly game, or something like that, a little... If you're familiar with that.

Hojae Lee: Go ahead.

JHawkins: these,

Hojae Lee: It's fine.

JHawkins: They used to have these really old ones from, 1800s or something, so these old leather shoe, little metal shoe. Anyway, it had a weird shape. if you touch it with your three fingers, and you didn't know that thing, it's not like the die, where you knew it, you'd say, I don't know what this is, and then what you'd have to do, then you'd have to use one finger, and move it slowly around the shoe, trying to figure out what the shape of the finger is. You can't learn the shape of that object, this totally new shape. The whole point is, you're a tender region, and you have to recognize something in that region that you already know. And if you don't recognize the thing in that region, I don't care how many sensor modules are looking at that region, if you can't recognize it, then you have to narrow it down to a smaller region. And you can use multiple modules, multiple learning modules can be attending to the smaller region, but they have to all agree on something. So if my three fingers are going to work together, those are the ones that are going to vote. They have to recognize an object that gets passed up to the top of the hierarchy. If they don't recognize the object, then we don't learn it right then. We have to then narrow it down to a smaller subset that we do recognize. At some point, you get down to the smallest subset, it's just an edge at some location. that's or something like that.

Hojae Lee: is RIM... can that be considered an object ID, or...

JHawkins: Could be, if it's something you recognized.

Hojae Lee: Go ahead.

JHawkins: But if it's something you've learned, all I... I'm just... generically, all we can say is. If you... when you're learning a new object. What you're doing is you're finding a child object. That you can place within the... in the parent. And if the input to all these columns is not recognized as a known child object, then you have to narrow down the area of attention. Until you get to a known child object. So I can't say up front whether those three fingers are recognizing something or not, but if there are different parts of a mug and haven't learned the mug yet, I would say that they're not going to work together.

Hojae Lee: So let's say that they're, like, close together, 3 sensors, just, sensing, okay, this is... I recognize this is a mug. the idea, that I... or the way that I understood today's conversation was that, you know. that ID will get passed up eventually to the hippocampus. And the hippocampus will somehow, broadcast all of these. Back down to all the vortex, not just the figures I've learned. And since the fingers only learned... it didn't learn the color, even... let's say this... this is all the, visual...

JHawkins: No.

Hojae Lee: let's say the rest are visual. The visual, they will still learn something because it's coming from the hippocampus, but they don't have any of the... they only have the morphology of the rim and not the color of the rim. Is that... Oh my god.

JHawkins: Yeah, this is details that I didn't try to figure out.

Hojae Lee: Okay.

JHawkins: But clearly, you can't learn the color of an object by touching it.

Hojae Lee: Yes.

JHawkins: But I can learn the morphology of the object by touching it. when the hippocampal replays. I don't know what the mechanisms there are, and what it's actually replaying. for example, if, for example, Let's try to come up with a good example.

let's say you're learning the mug on the coffee cup, right? And so you're learning it visually. And so then, there's a region in the visual space that says, oh, I recognize this whole logo. So that's the tended regions, the entire logo. And so the logo ID gets passed off. And now, at some location. And let's say the logo ID, orientation, and location is known by the hippocampus. Now, the hippocampus plays that back.

there's no representation for the logo in the somatosensory columns. So they wouldn't even know what to interpret somehow there. I... I don't... you know what I'm saying? It's

Hojae Lee: Yeah.

JHawkins: There is no... There's no model of logos in the somatosensory cortex. but... but we could say there's a logo at this orientation location, and they say, I don't know what the hell that logo is, but the orientation location makes sense to me, so I can just learn a model of this an orientation, of course, a logo wouldn't... Built on part of this model, but the point is that...

Viviane Clay: the... would it ever work for features? even if it is also a visual column that knows about logos, its SDR of the logo will be different.

How would you broadcast that?

JHawkins: I'm saying if... presumably, I didn't catch all the words, but presumably, the, the FDR ID is something that can be broadcast, that is in some sense shared among... when we vote. There is a pattern of activation that.

Thousand Brains Project: But yeah, Because that's...

JHawkins: He's

Thousand Brains Project: about that object. It does feel like the projection almost needs to be, like, basic sensory input, like you were saying before, that it would... Rather than projecting to V1.

JHawkins: no, what we want to project back is a recognized object, the recognized object at some location. But if a column doesn't know about that object yet.

Thousand Brains Project: there's no...

JHawkins: Whoa.

Thousand Brains Project: meaningful SDR to act.

JHawkins: okay. Some columns know about that. some columns know about that object. Because, actually, it's coming a little clearer in my head how this might work, sitting here. Some columns, Look, at some point, the object was recognized. There was a... there was a stable pattern in layer 3 or whatever, layer 2, representing that object. And that stable pattern can be re-invoked by the hippocampus, and all the comms that know how to vote on that object. would, in theory, know what the object is. If a column did not ever know that object, there's a touch column that doesn't know what the logo is. Then it can't... it can't invoke the ID. It can't just... it just can't do that. It says, I don't recognize this. It's something... I've never seen this before. But... but if we're... if we're passing down three things, location, orientation, and ID, the location, orientation would still work. And so it would just learn that... it would learn the morphology of this object, but not know that there was a... what the particular feature was. I'm not sure if I'm expressing this well.

Thousand Brains Project: Yeah, I guess it... to me, it feels like it fits with that if we are going to get this to work, it makes more sense to try and Reawaken the... almost like the raw sensory and movement input, rather than... Because that's the only thing that's universal.

JHawkins: A thing that's universal The thing is, okay, the thing is...

Thousand Brains Project: In order to learn new models in columns that have.

JHawkins: We don't want to... we don't wanna... we don't want to specify, we don't want to specify the movement. But I don't think that's... that's what we know. We know... we know the region in which... the area in which the object was.

Thousand Brains Project: Or location, but I guess...

JHawkins: Okay, location. Location.

Thousand Brains Project: model.

JHawkins: You could specify location, orientation, and feature ID. The location orientation works for everybody, the feature ID only works for people who know what that feature is.

But if you do know what that feature is, then you're golden, right?

Viviane Clay: But, Wouldn't the feature have different representations in each column?

JHawkins: Yes and no.

Okay, a side... side thing here, okay? When we talk about voting, Every column has its unique ID. And then they send their projections broadly, and people then associate with them. And I've mentioned this several times. That the projections can be very sparse. That is, I don't have to associate the ID in column 1 with the SDR in column 2, also associated with the SDR in column 3, and so on. As long as you make... you can make a... you could... Put it this way, if I looked at all the activity going across all these neurons. And it's rice bars. If I sample from this activity. Just even 20, 30 neurons in any set of columns, it doesn't really matter.

It could be one from every column. it'll still... if I sample from that, and I get a new SDR, which is those 30... Out of million things. That is sufficient to specify the ID for everybody. I know it doesn't sound like it. But I worked this out once.

It's As long as you just have enough connections to all of them, you're good.

30 connections, and as long as you just randomly distribute them.

Viviane Clay: But wouldn't you have to do it the other way around? if you project back, you'd just activate one in.

JHawkins: No, because that...

Viviane Clay: One might be used in different multiple records.

JHawkins: Because, I believe the voting mechanism would work. You're essentially... all the columns are trying to reach a consensus, and if you... if you... if you... Specify individual bits on... distributed around this region.

It'll force it to get to the right consensus. I'd have to work through it again.

Thousand Brains Project: Yeah, as your point, Vivian, that, it's coming from one region, so you don't have a bunch of different places you're sampling from. You only have the one?

Viviane Clay: And so it almost needs to.

Thousand Brains Project: Bull's signature.

JHawkins: Imagine...

Viviane Clay: You would elicit a partial representation in each column, and then use voting to resolve the rest of it?

JHawkins: That's how voting works anyway. voting works, I may know my ideas. And I broadcast it, but I could do a very sparse broadcasting, and it'll force other people to come to the same conclusion.

It's a bit, counterintuitive. So imagine this, imagine you...

Thousand Brains Project: I was just gonna say, because I had a separate question, which was, like, devil's advocate, why does, why are dreams so random, rather than basically just the sequence of the day?

JHawkins: I'm not sure dreams are... dreams are not necessarily...

Thousand Brains Project: Not random, but there's certainly a lot of... Strange things that happen.

JHawkins: I'm not sure... I'm not sure dreaming is the recall that we're talking about.

Thousand Brains Project: Okay, I thought there was some evidence that there's...

JHawkins: It might be.

Thousand Brains Project: 4AM sleep is when the recall happens.

JHawkins: All I know is.

Thousand Brains Project: Or the replay.

JHawkins: First of all, these.

Thousand Brains Project: These replays occur very rapidly, remember? They occur extremely much faster than real time.

JHawkins: I don't know if dreams are like that. I didn't make that assumption. I didn't assume that dreams are the recall that we're talking about. I do know that recall of sequences occurs in the... in the rat hippocampal complex, and it's very rapid, and it's very precise for the exact things that the rat did. It can retrace what the rat did in the maze, or something like that. Which I don't know is the same as dreaming, because dreaming seems to be, Weirdo stuff. So we should look that up.

Thousand Brains Project: During REM sleep, where dreams occur in humans, replay events also... okay, replay events occur both during slow-wave sleep and rapid eye movement.

Suggesting a possible role for play cells in dreams. Anyways, I was just... the reason I brought it up was because in those computational models I was talking about before. The sort of randomness of dreams has a computational role, which is basically shuffling the sample data and making it more statistically independent. But I was just thinking, in your response, just now to Vivian, about how do you, get back the representation with such sparse inputs, I was thinking, you would... that would go wrong a lot. you would basically hallucinate on the law, it feels, if all you need is a bit of top-down input, and then voting will... And submit.

JHawkins: So maybe that it's...

Thousand Brains Project: often just, triggering, a random incorrect object, which is why in dreams, suddenly, something will appear out of nowhere that you weren't expecting.

JHawkins: I'm not... it's not clear to me, you'd get... you'd have a lot of errors. Think of the... think of the hippocampus as just being one more column that's voting.

Thousand Brains Project: Okay? Or a bunch of columns, it's... it's more than... it's bigger, it's a big structure. I guess one... one column isn't normally enough to trigger everyone else to...

JHawkins: It might be enough to... it might be enough to force everyone to reach the same consensus. Let's... let's assume that for the moment. I actually think it is, but assume that for the moment. It's... I know it doesn't seem like it, but I think... I can't remember how I worked this out once. But anyway, imagine... imagine it's what's going on. I'm the hippocampal complex. And there's a bunch of activity in layer 3 or something, 2 that's being sped all over the cortex, somewhere, a whole bunch of activity, and I get to sample that. And at any point... and the only thing that's going on in that set of neurons is what's being attended to at that moment. So we're attending to some region, there's some activity on some of these neurons. And, what if I'm able to sample from that set of activities. and I'm in the hippocampal complex, and I say, okay, I'm going to remember that's what the intended object was. It's not... I don't have to look at any... I don't have to do anything else. I just have to just form synapses, or turn on synapses, that capture a sufficient number of bits in the pattern that was On some sections of the cortex. So maybe I'm looking at all the cortex, so millions and millions of these bits are coming in, but I'm only gonna, I'm gonna quickly say, okay, here's 30 of them that were active. I'm going to remember that as the ID. Now, later, I'm gonna play that back. It's the same ID. And, my argument, that is sufficient to force those other columns to reach the right conclusion. I can't prove it to you right now. But I worked it out to convince myself that it worked.

Thousand Brains Project: Yeah, okay.

JHawkins: Maybe Super Tai would remember how we did that. Maybe, I don't know. I could work on it again if you want. I could try to figure it... I had some good arguments for it. It was a surprising conclusion. It's yeah, this... because voting requires this. Voting means I... if I have millions of columns, I can't be sending connections to millions of columns, or not millions of columns, if I have a thousand columns that are active, I can't be trying to activate every one of them. All I could do is send out a broad, broadcast of what my neurons are active, and And, it'll randomly associate with some other neurons that are active elsewhere, and it's the collective effect that's important. It's not any major visual column. Yeah, collectively, they're gonna settle.

we can just take that for the moment as a working assumption. So I think it would work. I know it's... it's... it's just... all this stuff is amazingly complex.

Viviane Clay: Yeah, but then if it's the voting that settles to an object ID, are you saying then that would be the input to the next column, and that's the one that's learning?

JHawkins: No.

Viviane Clay: Because if you just have... you just get the, object ID re-invoked, that doesn't really help you learn more about that object.

JHawkins: remember, we're going to send back... we're also going to broadcast the location and the orientation, right?

we were just focusing on the object ID, because Scott brought up the issue about, We talked about... not everybody knows the object ID.

Viviane Clay: So is this the ID of the incoming feature at that location?

JHawkins: Whatever feature was... whatever feature was determined at the attended-to location. attended to a region. So we attended to a region. The neurons are restricted to anybody... anybody who is getting input from that region gets to vote. And they reached the consensus of what's in that region.

That consensus is then stored in the hippocampus. The hippocampus can play it back later, saying to everybody, not just the region... not just the people who observed it, to all the columns. I observed this... we observed this ID, this object ID. Some columns will be able to recognize it, some won't. But it would be a lot more than the ones that actually sensed it, to begin with. That way, huh? I hope... I hope I... I hope this is not super confusing.

Viviane Clay: Yeah, I guess I'm getting a bit confused with, we're sending back the object ID, which would go to layer 4, but then also resolving it through voting, which would be happening in.

JHawkins: No, I didn't say it goes to Layer 4.

No.

Viviane Clay: If we're using this...

JHawkins: yeah.

Viviane Clay: learn models.

JHawkins: I guess, I guess you're saying, yeah, it would be... let's work out the details. Okay. it... it would invoke... the feedback wouldn't go directly to the phone. It would go to the regions that are able to recognize this object. these would be, And then those regions would then project to Layer 4 in the next hierarchically higher regions, so that's where training would occur. So basically, I'm invoking the features In some columns. And then we're going to try to learn the arrangement of those features In the next region up, right?

Viviane Clay: Okay, yeah.

JHawkins: you wouldn't... you wouldn't...

Viviane Clay: Okay, yeah, that makes, sense.

JHawkins: There's the boundary of knowledge, right? The boundary of knowledge is that you're going from the bottom of the cortex, you're recognizing and recognizing the compositional structure, and at some point, the compositional structure is no longer recognized. It's there are features that I recognize, but now the arrangement of them isn't... is not recognized. at that point, to train, I want to invoke the features at the child regions. Yeah. And I want to invoke the locations at the parent region.

And, I haven't thought about how to do that.

Viviane Clay: Yeah.

JHawkins: so you wouldn't be... you wouldn't be projecting the layer 4, you'd be ejecting some... some place, probably Layer 1 someplace, so you'd be invoking the... the correct model ID. Yeah, you'd be booked into layer 1, then to layer 3 or 2, and then you're... And book the correct model ID. I don't... it just all seems crazily complicated, but I can't reach any other conclusion. And in some sense, it fits a lot of data that we know, so I'm reasonably confident of the basic Ideas.

now just thinking through it here, I can see how the ID would work. Pretty clearly, I don't know how you would transmit a region of interest, or a location, I don't... I don't hate to do that. I don't know how you transmit the... Or broadcast the.

Thousand Brains Project: Yeah, one... one way I'm... not saying this is how it's done, but I guess one way it could be done, is if you... reactivate the sensory input. And that just gets filtered through whatever sensory processing exists, and gives you the... The kind of orientation. of the... The lower level input. And then... and then...

JHawkins: I'm not re...

Thousand Brains Project: Go ahead. Yeah, and then have a movement And although it's locations that are stored in the hippocampus. Whatever movement takes place as it moves from one of those locations to another. is transferred as movement to the columns. And basically, as far as they're concerned, it's like they are... Just awake, and...

JHawkins: See ya.

Thousand Brains Project: And that would at least fit with, the phenomenological experience of dreaming, which is, you feel like you're just moving around in a virtual world. it doesn't... it doesn't feel like you're just getting, flashes of what you have seen when awake. It's it's... you're actually moving and your sensory motor still.

JHawkins: I'm gonna... at least a stake a point of contention here. I think we should separate dreaming out from...

Thousand Brains Project: Okay.

JHawkins: the mechanism I talked about. Because I don't think this replay mechanism is dreaming. It is a precise recall of experiences of an animal. And it happened really, quickly.

Bye, Vivian.

Viviane Clay: Yeah, sorry.

JHawkins: It happens really, quickly, so let's just... let's not... let's not conflate dreaming with this, because maybe it has nothing to do with it. I don't like the idea of trying to go back to fooling the comms to think it's actually moving and bringing about movement data. That's... that's really problematic, because I, I wanna... I'm trying to... every column... if it... it somehow... that's a detail that can't be transmitted from the hippocampus. It's just... it doesn't know, the hippocampus doesn't know how things moved, and it just says, hey, at some location, I observed this. And then... and later, another location, I observed this. And, it just seems like we're trying to get away from representing the movement. We're just trying to say, this is... I'd rather say, somehow you have to... I'm going to tell you a location in, in space.

I don't know.

Thousand Brains Project: It's weird, because, yeah, normally we don't communicate location directly to columns, unless it's...

JHawkins: No, I know.

Thousand Brains Project: Unless there's a learned association, because it's always in its own reference frame.

JHawkins: Alright, so I don't... I don't know how it works. Okay, I'll just admit, I don't.

Thousand Brains Project: Oh my goodness.

JHawkins: I don't think the solution is going to be converting into movements.

Thousand Brains Project: Okay, and just, a virtual world.

JHawkins: Somehow it's gonna be... yeah, I don't think so. Again, remember, we're not... we're talking about, recalling movements of your finger over an object, or, recalling...

Thousand Brains Project: it's not... this is not, dreamlike things, necessarily. They could be.

JHawkins: at the intersection, I look at and see different cars and stuff, but a lot of what we learn the logo on the mug is, anyway, that's the dream pink, put that aside. It seems to me that what you wanted to transmit is a language that could be understood by every column. And what the column needs to know is. There's two things. To learn, for the cortex to learn, you have to have... child columns need to learn the... invoke the idea of the child object. I think I have a good sense of how that happens. And then the location information It has to somehow be invoked into the parent columns, language, and I don't... I don't have... I don't know how to do that. But I bet you we could think of a way of doing it.

Thousand Brains Project: Yeah.

JHawkins: Yeah, I feel like that would be an issue.

Thousand Brains Project: interesting. Follow-up exercises to try and think through some of these details.

JHawkins: in fact, I think...

Thousand Brains Project: Yeah.

JHawkins: Yeah. In my notes, I said, lots of questions, and I didn't understand it,