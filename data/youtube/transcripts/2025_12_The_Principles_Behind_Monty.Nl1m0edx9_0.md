Scott Knudstrup: what I'd like to start with here is... This question... about cortical columns. if you've been following Thousand Brains Project, and you meant it before that, you've certainly heard some talk about cortical columns, and... Because it might not be obvious why they're so fundamental to Thousand Brains Theory, I'd like to spend a few minutes talking about what they are, why we care about them, and how they inspire Monty.

Cortical columns are found in the neocortex, which is a thin sheet of tissue wrapped around the old brain. The neocortex taps into the more ancient structures below, sometimes augmenting the old brain's sensory systems, and other times granting entirely new capabilities, such as language. Structures in the old brain come hardwired, custom-made to serve some narrow but system-critical purpose. Neocortex, on the other hand, it shows up with few concrete skills, but it's eager to learn. A flexible, general-purpose substrate that learns on the fly. It sounds like a good idea, but how do we know it works?

Because nature's go-to strategy has been to just keep printing it, and it seems to be working. Even as our brains and our ancestry, in our lineage, they've been getting bigger as a whole. But neocortex does take up a greater and greater portion of it as time goes on. we know that it does work, but how? One important clue came in the 1950s from legendary neuroscientist Vernon Mountcastle and patron saint of the Thousand Brains Project. While cataloging responses in the somatosensory cortex, he noticed that cells that lay in the same narrow, vertical band always respond to the same kind of stimulus. For example, if you were to perform this experiment on me, you might stick an electrode in a region of my brain to respond specifically to sensations of the fingertip. You're labeled in blue. While applying various kinds of stimuli to my finger, you might find a narrow, vertical region whose cells respond only to light pressure against the skin. But if your electrode crosses into the boundary of a near into a nearby column, all the cells there might respond only to flexing of the nearby knuckle. Or further, again, heavier, pressure, like deep tissue pressure. This was a remarkable discovery. And it didn't take long for others to look for the same kinds of themes in other brain areas.

So within a few years, David Hubel and Torsten Wiesel famously discovered the same pattern in the primary visual cortex. This was good enough for the Nobel Prize. And there, columns identified visual features, such as a specific orientation of an edge. Here, I'm showing a handful of orientations here.

There in the visual cortex, they also found that columns were bundled into bigger columns, hypercolumns, they were called. Which forms a larger structure that binds together features belonging to the same region of the visual field. Now, columns, or mini-columns, as we might say, to distinguish them from hypercolumns in primary visual cortex, they turned out to have real meaning for the brain. And their discovery suggested a new kind of building block for modular design. And this caused a lasting shift in focus, towards understanding the form and the function of cortical columns.

In the words of Mountcastle himself. Columns are the basic functional unit in the neocortex. Each performing the same essential computation. But what computation is that, exactly? You're probably aware that Thousand Brains Project is rooted in the notion that cortical columns are fundamentally spatial creatures. Placing their knowledge of the world into reference frames, which it also uses to guide decision-making behavior in a continuous sensorimotor loop. And while column-to-column interactions are fairly important, enormously important. We begin by taking a more maximalist view of what columns are capable of on their own. And, as a motivating example, Considering the following situation. Imagine you've been blindfolded and instructed to identify an object placed in front of you using only your fingertip. Which is something we do for fun at the Thousand Brains Project. Since your fingertip projects only to some small region of the somatosensory cortex, say, a column or so wide, this task rests on the intelligence of just one column.

We take our first observation, which feels like maybe a rim of some sort. And then another. So you're starting to think maybe it's a cup or a ball or something with something with kind of a rim. Let me move over here, we feel some some kind of protrusion. And then, move back over to the body, and feel this sort of cylindrical area. And you start to think, oh, could this be a mug? Okay, yeah, maybe it is a mug. That's... that seems fitting, that seems, consistent with all the observations I've taken thus far.

Now, think for a moment about what needs to be happening in order for the column to arrive at this conclusion. First, You need to know where in space its sensations came from. Second, It needed to be able to assemble all these locations into a coherent 3D model. And finally, It needs to compare that model with all the objects in its memory.

So let's run through a similar example, now pivoting over to Monty and its architecture, and how it's inspired by this example.

We'll substitute in Monty's components here, so instead of a finger this time, we'll have a camera, which captures a small field of view image. It's analogous to a small patch of retina, only observing a small portion of the object at a time, like our finger did in the previous example. And then we'll have a sensor module. Which performs location and feature extraction on the image patch. You can think of a camera and sensor module, like the retina and the thalamus. We'll also have one learning module, which again stands in for the cortical column. And finally, we'll have a motor system, which drives the camera. Controls his movements.

At T equals 0, the camera takes in a small part of the object's surface. Which it passes directly to the sensor module.

The sensor module determines where the patch is located in space, and it extracts features from the image, such as curvature or color, and it sends this information down to the learning module.

There, the learning module, compares the observation with points it's seen before, looking for potential matches. At this point, the curvy red patch strongly reminds Monty of the location seen on the mug and the cup. And less so of points on other objects, like the blue bowl. Right now, the mug and the cup hypotheses are top contenders, but the learning module doesn't yet have enough data to decide between the two. where should it look next?

The smartest thing to do would be to look for what distinguishes them, in this case, a handle.

And so this is exactly what Monty does. And it sends, this location down to the motor system. And the motor system's job is to translate the target location into sequence of commands that will get the camera pointed at the target, such as turn right by 10 degrees and up by 5.

Now, after moving the camera, we repeat the process, and the small patch is processed by the sensor module, which, again, extracts the location and a set of features, and it sends that information back to the learning module. And now the learning module's hypotheses are updated in light of the new information. For each hypothesis, the learning module thinks something like, if I started out looking at this specific point on this specific object. And I then looked over to the right, then I'd expect to see X. If what I'm actually seeing matches that expectation, like it does for the mug, then I'll increase my confidence in that hypothesis. Otherwise, if it doesn't match, like for the cup, then I ought to adjust my confidence in that hypothesis downward.

Now, when one hypothesis stands well above the rest, the learning module reports its final verdict.

Just a quick clarifying point here, which is that the illustration on the left might suggest that there's one hypothesis per object, but in reality, there are many. On the right here, you can see that each point there is a different hypothesis, many associated with the mug, many associated with the bowl. And this is what allows Monty to perform accurate pose estimation, in addition to object identities. very briefly, there are many learning modules that can work, or they... you can... make, Monty have many learning modules, just as there are in the many cortical columns in the brain. They can work together, they can communicate with each other to perform faster inference, and they can be communicating laterally, or you can arrange them in the hierarchy to learn and recognize compositional objects. to recap everything, Brain, and Monty, share a lot of common, features by design. Modularity, modality agnostic functions, so learning modules aren't specifically designed for vision or touch. They've got this narrow input locus, cortical columns and learning modules. Cortical columns and learning modules also have precise spatial mapping capabilities. They function alone or in an ensemble. And, also, motor output. Didn't get into the biology there of cortical columns, but they contribute to the motor side, so that's how we have these continuous sensorimotor loops. And thank you for your time. I'm gonna pass this over to my colleague, Hojae. Who will talk about how to get started with Monty and run your own experiments.