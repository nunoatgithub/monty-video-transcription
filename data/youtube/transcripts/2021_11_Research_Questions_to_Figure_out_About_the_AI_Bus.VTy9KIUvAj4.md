Okay, so then, I'm just, this is like a very cut and dry way of looking at things, so I don't know if this is the right way, but I split it up into three different categories. One is, the first one is figuring out what the information is that's transmitted across the AI bus and how that's, what is that information? The second one is each module has to operate on that information. There's various things that have to happen. So what, these are questions along those lines. No, wouldn't that be like some of the modules could do different things, right? It's not Or are you thinking, like, how do they all interface to the bus? yeah, this, they're very, they're strongly related, but things, I'll walk through the details of it, but Because my mind is, different modules can do different things, but the AI bus operations would be common. Yeah, so it says operations modules perform on the, oh, I see. Yeah, yeah. okay, you can look at two ways. Like, how do they work with that information? But also, they could do different things with it, some may. Yeah, and then here I'm trying to get at the generic stuff, but, vision module might do something very different. I was thinking also like the, the episodic memory type of module is different than, the standard, Yes, I don't even have that.

And then the third category was stuff inside the modules that we've talked about, that's common and unclear whether the episodic memory would do this stuff or not but things on the graph, updating based on motion and stuff like that so I can go through each of these in more detail but this is how I split it up. And I'm not saying this is the right way to do it but. The first question is stuff that we talked about last time when Marcus presented. So how are object ID location in the external relative to the body and orientation? How is how are they represented? so in the last week, there's tons of, options that have been mentioned, SDRs lists, vectors, particle filters.

this thing DDC is, I think it's like distributional distributed coding or something like that. It's a lot like SDRs, but with probabilities.

it's an interesting, I just threw out there is that thing. It's a small community of people working on, they can talk about stuff that's similar to SDRs. But there are, they don't really talk about it in the context of high dimensionality and sparsity, but some of them, there's relationships there. So I think there's actually a nice match between SDRs and you can see it combining those two. Anyway, then there's the question of how much of the representation is learned. and Marcus presented on the extreme view last week where the entire thing is learned and, by backpropagation. then there's the issue. We again, we discussed last time is how much information is represented jointly. these three completely separated or on the other extremes, everything completely joined into one, representation. so those are the questions to go through. A separate thing is, how is uncertainty represented? I think we've talked about a couple of different ways of doing that. It's clearly tied into this representation question, although the idea the, there might be different answers to these for location and orientation than for object. Yeah, that's why I've separated. Oh, I see. but, if we do them all joint, then they would be the same thing. But I also think it might be slightly different, but How our location, our orientation, uncertainty represented, and then how does the voting happen? how is uncertainty resolved and ambiguity resolved?

this is one where you could imagine that it might actually go down, belong in number two, but perhaps there's something just in the AI bus that does this. the representation scheme would might dictate how that goes. Yeah, it is this, yeah, these are all really tightly related. Yes. Is object similarity represented? So this is a new one they put in after the discussion last week. for are all objects distinct? I think we all really like the idea of having similarity represented in the representation.

this question came up last Tuesday. Do we need to represent and communicate the common, the current common location? For example, if it's represented in relation to my body, what happens when the body moves? what happens if it's related to something else? We need to communicate that common point. This is one where you could really deviate from biology a lot if you wanted to. I think the only important point at this point is that at any point in time, every participant on the bus knows how to, generate, their object to body information.

And, and therefore everyone needs to know where they are relative to that common point.

but that's something that could change. like in human, like if you think about your body, our body moves steadily through the world, right? But in a system like this, you could abruptly change everything. So the one, the question is But then would you need to communicate it somehow? yeah, you'd have to, right? The question is, you'd have to. Because everybody needs to know, their new, if I was a censor, I had to know the sense of my relationship to the common point. Yeah. And, I'm just saying, in a physical body like a human, it just, you could even imagine, I'm just saying, I'm imagining scenarios where this is abruptly done. And yes, you'd have to communicate it. I guess the question is, do you communicate? Everyone has to know where they are relative to this common point.

I'm just thinking out loud here. we don't all have to consider this, but I'm keeping myself open to the possibility that the common point changes with every attention shift and that goes back to that, that, that's a very valid, I'm trying to at least hold that possibility in mind because that might be interesting. It doesn't seem that way introspectively, right? if I'm sitting here. And attending to things in the world, and I say, where things are, they all feel like I'm still perceiving them relative to my, body, I, when I turn over here, I don't feel like, oh, the cup has moved relative to my body. My sense of where's the cup, if you ask me where the cup is. I would say, it's, I feel like it's in some position relative to my body. that's from suites, it feels like some position relative to my body. It doesn't feel that, that's changing every time. But you could, as long as everybody knows, it has to be communicated somehow. I was even thinking of something like, right now our senses are, they all, body and your senses move, They all have some movement and continuous flow, even though Isaac Highton is like continuous, like I know I am on the cathode ray, but I can imagine a system where you have a bunch of sensors and they, imagine they learned how to model movements at a factory floor or something like that. And so they all have some common point. And then you, now you want to put those sensors in a completely different arrangement, and it may be in another room, another point in the factory, right? And now there's three other cameras in the other point in the factory. And you want to switch to those three other cameras, as if you hadn't switched, as if they moved there. those three other cameras would be in different positions relative to each other. So you could instantly switch to these three other cameras that are in different positions relative to the first three cameras, in the sense that it would continue to work. It would just say, yep, okay, now my three eyeballs are in different positions over here. They're more than wrapping. and so it would be this very discontinuous flow. So somehow that has to be communicated to everybody. This could almost be like a preset thing where you've set up a system. You can say, Oh, here are the different positions. If the sensors actually weren't moving, if they were just in different positions. I'm just riffing on this question. Yeah, that's a related thing there where, for location of my sensors and my fingertips, I can only move so far from, Something that we call in the location of my body, but I had a system with like robots just running around. there's no longer just an epsilon ball you can run around the body and say, I'm always here. what does it mean if the robot moves? Yeah, your body can move too, so how's that different? because the distance of, a sensor of, my finger to my body, that's fixed. I can only go so far. What if you're a robot? Is your finger fixed to your body or fixed to something? I guess I'm thinking of a distributed robotics system where you have a bunch of different robots and the bus is more, online. Okay, does that sort of go into the camera situation I just said? It's more like the camera situation, but now the camera's gonna loop. Yeah, or I guess the equivalent would be imagine you had some robotic arm that was manipulating something and it's going down the conveyor belt and it's working on something and it doesn't finish and it gets out of range, but the next arm down there could pick up exactly where this one left off as if it was the same arm. I'll continue working down here or something like that.

that's a good idea. I guess I was more just bringing up a challenge to keep in mind, it would possibly be more powerful if we could have you know, many robots using the same bus to communicate. you relax the constraints of having them all be in the same spatial location, but it introduces an extra challenge of mapping where they are relative to. Yeah. I think the thing is one of the restrictions of the system. Is that it's really only useful if multiple sensors can be sensing the same thing at the same time, in some sense, because what you're communicating is observe things relative to some point, right? And if there are some sensors in room A and some sensors in room B, And you couldn't easily explain how, the ones in A and B can't really observe the same things at any point in time. So you could switch from A to B in B to A, but you couldn't be observing the same things in A and B at the same time because you're in different rooms. So at that point, it's a little harder to, they could know where they are in all seriousness, but they really can't. At any point in time, the sensors in A and sensors in B can't really talk to one another in a useful way, because they can't observe this.

But they can't be observed in the same logic. my point is, it's only useful that two sensors, like my eyes and my ears and my fingers, can at some point be observing the same thing or tend to the same thing. Then they have a way of communicating with each other. but if they can't do that, there's other ways to communicate, not directly on the bus. You know what I'm saying? it's like language. Yeah, that's what I was thinking. Somebody could just teleport this. Yeah, but the major benefit, one of the big benefits of this is that the reason these things can work together is because they can communicate. They're sharing, they're observing the same thing, different positions, different orientations, but they're observing the same thing, and so it's, just part of the benefit of all that. I have a question about this object similarity, all object instincts. How does that work? I'm looking at the whiteboard here, there's like E, different E's in this whiteboard, they're all different. I think there's a different issue, there's like, how do you take E's of different shapes and classify it as an E, that's like animus, but then there's, it's an E and an F, they have similar representations, they're different objects, they're not the same object, yeah, different scales, but it doesn't matter, correct, yeah. there's an issue we haven't really addressed at all, and I'm not sure how much we have to address it up front, which is the state of objects, right? And, so we've talked about the state of objects like, laptops opening and closing, or what's going on in a smart home, or a safe loop, and so on. and once you argue that, like an e and a italicized e and a bold e are different states of that object. And I'm not saying that those differences are all going to be understood as states, but there are lots of objects in the world which don't have a very rigid morphology, and in that we see them as the same, and we don't, we know they're different. So I'm throwing into that same bucket, this issue of state, which might be the same solution as the issue of but that's not similarity. That's, there is similarity. Mean a stapler is a stapler. I can say it's similar. I know those two things are the similar, in some sense they're different in, they're different instantiation of the same thing. Just like the letter is.

my handwriting might be different than your handwriting. You might be able to look at my handwriting and say, this is Jeff's.

I'm just saying there's a dissection of different things that are different in the world that some of them haven't addressed at all, and it might be a mistake. So I put that in here as g. I think we can do some very useful things without solving that problem initially, but we do have to solve it eventually. Yeah, maybe like both of these, it may be something we can figure out later. Yeah, there's a lot of advantages this architecture that comes from just having to plug and play architecture of the bus. And you look at the number of applications you can do, but this still could be quite good.

so I think the point of this is, this could help structure our meetings. we would focus on these things, in terms of what operations module, modules perform. So one is, the thing that Abhi asked, how do we do associative learning of object ID, location, and orientation, so how would the different modules learn. To do, the same very related. Maybe the same question is how do we implement many to many mappings between modules? So it includes one too many and many to one. So one, if you have an audio module, it has a certain sound, but that could be associated with many different physical, visually different objects. And so from the audio standpoint, it's just one thing, but you'd want to associate with many different physical objects.

this one is you just mentioned this idea of sticking in a module and having it work. And what does that mean when a module inserts itself into the bus? Again, maybe this is something we can hunt on for a while, but how did, what happens when, I can see how that could work, in a system that uses the, the sparse representations with associated memory. I can explain that. How you could design this, I think in the real brain, for example, if I added some new, if I could add some new columns to a cortex, they would have to learn how to, Yeah, and then there'd be new wires and new accent. Yeah. But I think in an engineered solution, there's, there are ways you could make it so that you could instantly plug in another camera and it would be useful. So I'm happy to go through that. I can understand that from the, some of the sort of the model we did in the columns paper. So I could explain that, but I don't know how to do it in other systems. Thanks. I think it would be a really nice attribute to have. I don't think brains do this, but Right. Would be great.

Yeah. And then this is a big question, obviously, but how do we transfer knowledge from one module to the other? How does that happen? I have some additional thoughts on that share. So I think each of these could be like a topic that we dive into.

they're, and then within a module, this is what a module needs to be able to do in order to participate in the AI bus.

how are internal locations relative to the sensor and orientation? How is that represented? Then we don't need to specify that generically. How is the pose calculated? The thing that you're going to actually transmit on the bus. How is the graph represented and constructed? How do you query the graph? How do you, when you, move, how do you update this external sense of the pose? and then how our internal reference frames represented and updated again. These are all sort of very overlapping questions. they may basically be the same thing, but I split them out here just for purpose of discussion.

so that's pretty much, would you, would one of these, so I think one of the core things that core, areas that's going to need research is unsupervised learning of a bunch of reusable parts. Would you say that falls under any of these ABCDEF? Or like, how do you learn a good set of parts, a good set of objects that you can now construct other objects from? I did not include that in here, so that would be a separate question. I take it unless you Yeah, one of the nice things about attacking this from a bus structure is that if you think about one of the modules that's on the bus, like a column, you have a lot of flexibility how you implement that as long as it works into the protocol, right? Thank you. And in some sense, it's that's why, I've said, hey, I don't care how you do it initially, because as long as it works, the protocols, as long as it works, then the internals don't matter.

and that way we wouldn't get stuck up on saying, we don't quite know how the neurons do this, or how we do that, or something like that. you could do, you could come up with partially engineered solutions that maybe don't learn as well as you want. I don't have all the kind of representations you want internal to these modules, but they at least whatever they do that communicate properly and they adhere to the protocol. So then they're still useful.

Think about it from a biological point of view, there's all kinds of eyes, right? And there's some that are very primitive, some that have color, some that don't have color, some that just don't focus on their multi, lens eyes, all different types. Some are better than others, but they're all useful. And so you could think of it that way too. You could say, okay, there might be the ideal way of doing some, bottling with a particular sensor. But we could get started with one that's no less than ideal. and just to put that in its own little bus, you can imagine teams working on that on their own independently, upgrading inside of that bus, inside of the module. I guess one question is, in terms of what's transmitted on the bus, is there a difference between object IDs and how objects are represented and object parts are transmitted? Or is it basically just different objects as far as, it's just, this is the, sorry, this is, the operations within modules. But it's trying to get at the generic operations within the modules that every module would have to do. I don't know, again, I think. So if it impacts the bus or things that it would be in here otherwise. I could imagine, easily imagine there's a module that has no learning whatsoever, but it works on the bus. You could have, they could have different kinds of graphs. Some of them might represent the graphs in different ways. Yeah. Yeah. So maybe, these are not generic. Like we don't really, it might be important, but they're not going to have to sell some of these. so we have to, we'll have to decide how we want to build some of them. So that from that point of view, they're generic, but not no one has to think about them. So the representation could be completely different.

Inside the module. That's why I said I don't mind if people want to use standard, CNNs or ANNs inside the module, because that may not be the way the brain does it, but it doesn't really matter at the moment. You want to build something that works, for as long as the module performs the right functions. But how do you communicate from module A to module B? No, look, if you adhere to the bus protocol, you have to query something in the graph from module A to module B. So it has to be able to do that. So it has to know about the graph from module B to module A. It has to know something. It doesn't, it just, no, it just needs to know that it can ask what is at this location or what is at this orientation. At this location, if I can, the query is in the form of, so the query has to be global. Yeah, the queries have to move over the bus and have, but they're all location based queries, right? Or object based queries. Or object based queries. But not all modules would be able to answer the object based queries. If I asked, which is, point to the red cup, the somatosensory system can't do that. I put my finger down my hand, there's two cups here, but I can't tell you which one is the red one. So not everybody can answer every question. So maybe all of these are, could be different in modules. Any of these, that's, we don't really care as long as it outputs the, I think that's right. yeah.

I'd be able to do some communication on the bus that's useful. Okay. Cool.

Does that make sense? Yeah.

You can imagine, I imagine this as a, an established, platform. And you have lots of different companies and people working on the platform. Different people will build different modules. They'll say, I'm an expert in infrared sensing, or I'm an expert in this type of sensor manipulator or something. And they can build that module any way they want. Yeah. As long as it communicates properly. coming from the enterprise world, that logic sometimes has some kind of bridge interface that translates a message from one to the other. In a document, for example, a new voice, from one content to the other. The message is, that's the figure, that's number one up there, right? But it's It's going to be, it's going to be in this language. But the modulate would know about module B, in that sense, when they communicate module A to module B. The module translates from an object to module A, to object to module B. There's no internal knowledge. That's the question. the module that's Relating the sound of something to the visual aspect of the something, the auditory module does not know what the visual module knows. It doesn't know anything about it. They've learned to associate with one another, so the auditory module said, I'm sensing this at this location, and then the visual module says, oh, that corresponds to things I know, and I can, I know it's at that location. Maybe it's the thing making that sound, but it doesn't have to know anything about the internal structure of what it knows about it. It shouldn't know what can query. It queries again, or in this format of, I'm looking for this, something that's associated with this ID, but it doesn't even have to be the same ID. It just has to say, my ID corresponds to some of your IDs. And, it doesn't need to know anything about it. That's the beauty of it. One module really doesn't need to know what's even going on inside the other module. The language of the query is in the, at least in the first set of objects we have to decide, object ID, location, orientation. There might be other things that we transmit. We talked about state, as an issue, but at a minimum this is the language. So all queries have to be in that language. We're trying to avoid that idea of any kind of internal I know what's inside of your mind. that's the part that becomes a bit hard. Yeah, but the beauty of this is you don't need that. You just don't do that. But this is the language of the bus and therefore everything has to fit into this language or some superset of this language. But you're trying to purposely avoid any knowledge about what the particular sensors know and how they work and, their internal machinations.

I have a question. What binds the associations together? What learning process binds associations together? we haven't decided that. And that's really the obvious question that we're going to get to, is the bullet point on the upper right. Yeah, I can see how it works in a neural system, I think. we have a particular way that we did in the columns paper. Yeah, that works. So that's one method. It's a starting point. It's an association of sparse representations. You have a set of sparse representations. So that yours and mine are pretty similar.

That's a pretty, before we, Marcus has proposed something different last week. Yeah. but we should all at least understand what we did in the columns paper. That's a very simple version of how it works.

we must We walked through the voting presentation, yeah. It doesn't mean everyone is going to watch it. So in that case, there's no common language to be used. No, there's no common language. the common language is orientations. It's not even a language. It's not even a common representation. There's no, format which is passed over the bus. It's everybody has, their format, and then you associate and link them.

And then it looks in the color scheme. Which is, I haven't seen it in most things in computers while I've done it, but that seems to be what it might do, And it's a lot of power, whether you're doing a playmark as a pretender or a play that did it. Do you want to dive into one of these now? I don't know. You had something you wanted to talk about. I do, but I think we should. We have to start tackling these things.

But to me, this is still the first place to start. Yeah.

as far as this question, we have these three variables there.

at the moment, I think they're sufficient to get started. I don't think they're sufficient ultimately. But, we want to debate that. Talk about it. Does anyone see an obvious hole? Things it doesn't represent? It doesn't represent state. It doesn't represent any way of, commonality. I'm not sure that's going to happen. It doesn't represent any kind of actions. I'm not sure you need that. You can just, actions could be defined by a series of these over time, but it doesn't represent actions. It doesn't matter any of that. I don't know if there's anything like that. But this would let you do inference, of static things.

So you're looking at sequences being a problem that comes later? No, sequences could be handled two ways. They could, they certainly have to be handled in each model. Each module can learn sequences. Each module can predict what's going to happen in this little world. over time. The question is that knowledge shared in any way over, over the bus? Or is the bus really just a representation of its current state? imagine I model how the stapler will open first. I'm a model.

I could just put out a series of, objects and orientations over time. and that could be it. I could just put out a series of those, a constantly changing representation and therefore not. I'm not actually representing any, there's no actual representation of the sequence, it's just encoded patterns over time. Or, there could be a separate, representational concept of, oh, like state, oh, this is the open stapler, and this is the stapler's closing, or something like that. In which case, you, would pass something in points like that over time. Usually, I was thinking about this. Imagine doing a language module. And the language module is looking at the bus and wants to describe what's on the bus in language. you could easily say, Oh, there's this thing over here with this orientation to me. There's a couple over here, and it's facing this way, and there's this thing over there. You can do all that. But language also includes actions like, Oh, the water is flowing, or, the car is going down the street. So how is it getting that from the visual system? Is the visual system just reporting a series of, a car here, Or is the visual system saying, no, this is a car moving in this is an object moving in this direction relative to the body? It has a trajectory relative. We can pass that information as a, like a trajectory as opposed to just a location relative. So both being able to recognize external sequences, but also being able to generate internal ones? it's going to definitely generate internal ones. The modules have to do that. And the question is, what goes over the bus? Does the language area say, oh, I'm seeing a sequence of car, if we're going from this point to this point, I will describe the car as moving, or is it the actual something else translated on the bus where you associate that thing with the verb driving.

I don't know the answer to that question.

I'm willing to go forward just with these three things, but, the real limit what the system can do, I think over time as we build these things out, we'll learn so much more and we'll figure out things as we go. One thing I see as missing is a coordination mechanism, abstract that away and say, yeah, things are right with you too. You talking about in time? I'm talking, yeah, in time, but also just being able to. Oh, a bus synchronization thing? Yeah. Ah, that's interesting.

can we, can, that's interesting. So that's more like a, a substrate question. it is. It's a question of whether it can be abstracted away or whether there's some sort of little property so that they can be leveraged.

Maybe. But let's just talk about a substrate question. one substrate, you have neurons. And then neurons are active all the time, and they go through, maybe they're cyclical, at some frequency. but they're active all the time, and there's really, so maybe they're all cycling at the same time because they're attending to the same thing.

you can imagine these, the transmission times have to be short enough so that they arrive at the same timeframes. But now if you're going to build this in software, for example, how are you going to do that? What would be the substrate for building this stuff? Yeah, it's, the two ways you normally think of it when you break it apart is either you are event based or you're time based. Yeah. but in this particular case, it's interesting because we know that waves propagate across. there's a, it's not only just the, point to point communications delay, but then there's a purposeful rolling wave of synchronization that goes across the brain. And again, that could be extracted. Yeah, I think that you, I would see, my first guess would be to extract that away, right? we don't want to model all these biophysical properties. So the question is, how would you end it? You could do like a clock based system, you say there's a system running on some sort of clock, everybody posted, Or is it self timed, and it just emerges that you have a wave coming through? why don't we take the simplest approach possible, which, to me, the simplest approach possible is you just assume there's a clock and everybody's working on the same clock. non biological solution, perhaps, but it could work just fine. this is pretty much what PyTorch does on a distributor. It's just a discrete, right? is there any harm? Is there any downsides? I don't know enough about these. I know a lot of advantages. Yeah. So what are the downsides to it? the downside, not the downside, but if the brain has adapted to a finite propagation delay, it might Treat long connections phenomenologically different than local connections that could, get within a certain but let's again I could argue that could be abstracted away. In the individual module. So let's say I have an auditory module that's doing speech recognition. One could then say, oh, I'm going to be, I'm going to be posting certain, recognized spoken sounds. and I'm going to be putting them on the bus, and at any moment in time, I'll have to say what my current one is. although the actual temple structure, I guess the distinction I have is whether you see the coordination as a defect to be overcome, or whether there are properties emerging from it. For instance, if you have an exponential decay of the signal, you naturally have locality. And things are based around that. How's that locality?

if You mean how far away I am or something? Yeah, you get a stimulus here. And let's say the myelin only is I would think myelin, I would, at the moment, I don't know, I would, I'll take a, I'll take a position and see if someone will take it off. I would take a position, this is the problem that the brain has to get around, and it does all kinds of tricks to get around it. But, for the purpose of the neocortex communications layer, not within a column, but in space layers, that it's not useful, and it's more of a problem. So that would be my first take on that.

that's just, I'll take a stance. I'm going to be always shooting for the simplest Possible thing to do right now. Okay. And we'll see if that's too simple. Okay. in terms of distance and things like that, those each module has to figure that out on its own.

I don't think it's no advantage the brain that my You know, that my, the area representing my left hand is pretty far from there except for doing my right hand, that's, I just don't think that's an advantage. I don't think it takes advantage of that, if it could be everything would be tight together probably do it. So anyway, that would be my first stance if I say, okay, you've got a bus structure here that we would like to act as if there's no delays on the bus. That would be my first guess. I think for the goals you put down here, that's a good assumption. in the back of my head is, how the brain kind of constructs itself, during the growth and stuff like that. It was featuring back to why the brain does the way it does, but that's not true. to what you're trying to solve right here. Now we're trying to capture the essence of what the problem, the information processing does. Yeah. And not, we're not trying to recreate a brain. We're trying to recreate what the brain does. And there's always this, questions that we're going to be all asking over and over again is what can we abstract away from? Yeah. So I would abstract the way, the timing to share. Okay. So I don't dunno enough about PyTorch or any other, whether that's important to know about this. it seems like a very basic, soft, synchronize, Yeah. My point is all the basic substrate stuff of communicating and all that's just easy to, yeah. Okay.

these kind of questions you, that's why it's easy. Unless Kevin's talking about, so I'm trying to keep it easy.

it's only important if it actually fundamentally impacts any algorithms. So I think the purpose of this meeting is to just discuss this. It's going to take a whole bunch of assumptions and we're going to find which ones are correct and which ones are wrong. So I think we can pick that one. So the underlying communications protocols should be pretty simple.

because I missed the first meetings. I just want to understand Something very fundamental. When you, where are we trying to, with this AI bus idea, where are we trying to strike an aesthetic boundary between neuromorphic, driven by neuromorphic, and machine language architecture, trying to pick something in between? No, I'm going to try to write anything no more, anything. Okay. I, that would be, I want something we can build today, right? Okay. And the nomorphic hardware is not really, today. Okay, the, AI bus is really It's a, yeah, it's catching, no, I understand. Yeah, no, yeah, the, idea is a goal here, this is something we can build today. if she's been useful today, Okay, so it's useful as opposed to say, Yeah, we're trying to explain the brain. No, this is, just be really clear. This is, this is moving down the path of creating an architecture for AI that is viable today. Like I said, I missed the first couple of meetings, so I didn't necessarily hear where that was laid out. So not last time. This is a technical, a technological platform.

It's something that's with our second goal of the company. So that's just, honestly, it's not a change.

just a question. people online when two people in the back corners of the room, they're talking at that volume. Can you hear them very well?

Someone? Yep. Yep.

What I found when I'm online is that people in the corners can't hear. So then it's surprising. That's what I've noticed. so we can put that aside. We, we have these three ideas, these three variables, I think. I'm nervous that there might not be missing something really big we'll have to do right away, but we don't have to. We can start with just those three, like solution like, Marcus present. There was an other space that was, what was that, mark? I thought that was as a placeholder. I was letting everything, just let fall into one thing. It could be object, state id, location, orientation. let it choose how to represent it though. Yeah, but we don't how to do object state at all. We have no theories about that. I don't care about it. I don't think we've discussed one here, so that's oh boy, so let's not, deal with that right now.

I think, Marcus, I think, is proposing, you can just add things willy nilly and they'll figure it out, right? There's something like that. Yeah. Yeah. and I think that might be true no matter how you represent these things, so that's a possibility as well. And if I'm not mistaken, you tried to represent a stapler open versus closing grid cells, right? We were, no, we were representing, the, how a module, one of these modules, could learn what a stapler does. How could it learn the staples opening closing and do a series of displacements, it can do that, sequence displacement. These modules here can still do that. The question, the problem is that we're dealing with what is on the front end. Remember, everything goes back to, we're going to find this bus, and the bus is going to be retained here. internally, a system can, oh, the state will open and close, and the state will close out. It does that magically, and that's how we do that, we're using displacement cells. So you can use the displacement cells for your, state, right? It's just a question of how does that get, that was what you said. We don't know how to represent state right now. Oh, state. Yeah, sure, we don't. Jeff just said that two minutes ago also. So then, I guess within the module, the displacement cells can be used to represent state, but it's just a question of how do you convey that information. I don't, I take away, we had one idea of how displacement cells work in the columns. Okay. Marcus proposed that actually displacement should be viewed in terms of vector cells as opposed to grid cells. So it's the same idea but in a different representational space. And I think that was really clever, and I don't think That kind of displacement, I have to think about this. I'm not sure. Can that be something that can be shared on the bus in a meaningful way? it has to be something that can be shared that other people don't know what it means, and they have to learn how to associate with that. I don't know yet. We have to start thinking a lot along these lines. what could be shared on the bus that's useful?

for example, the displacement If it's all in the language of local knowledge that really doesn't translate to other people, then maybe it could be, I don't know.

But it's good, the system's going to have to deal with this one way or another. Either, either that kind of state information is transferred on the bus, which probably has to be some way, or in the, movement information is transferred on the bus, or it has to be recreated on the other end through a sequence of, static, temporal sequence of static things.

Like you said earlier, you have these different sparse representations and you learn to associate them together. So with the state, because you're, using displacement cells, you can get a sparse representation that way. Then, whether you're seeing the stapler open and moving your finger along it, you get, you get associations between different sparse representations. Yeah. Imagine you have something like this. You have a series of, you have a series of a static, impressions. like you're seeing something and something's moving like the staples is moving, but I don't, think it's static, it's quite as, it's just something's changing. But any moment in time, any moment in time you just have some representation of its current position. This is a sequence of states. Yeah, and then you have a temporal memory, that learns this sequence, right? So now you can, and now you can play it back. You can say, given if I'm in one of these sequences, like a melody, I can predict the next statement, and then I can predict the next statement. And so that's the way we thought about this working. And so this could go back and forth, this sequence learned here, and this sequence can predict what's going to happen next.

now the question is, what do you put on the bus up here?

option A is you put these things on the bus, and the bus, there's no information about this here. and then anybody else who's extracting information off the bus would have to recreate this in a temporal sequence. That's one option. I'm not advocating that. So this would go into another module over here, and it's feeding a series of states, and then this guy over here says, oh, I can learn that sequence The other option would be to say, no, we want to send something, we want to send something about this up on the bus, and then that can come up here and perform that. I don't know how to do that, but maybe it's possible. So that's just the two options we're talking about there. the bus itself would know something about this information, or it wouldn't, it would just know this. I was thinking about like, where does it want to do the bus? Sometimes I'm aware of all this data, for example, what's burning? What's what? What's burning? What's burning in the kitchen? That's, is that a state? Something's burning.

okay, that's not, that's less querying the model, is too.

but that's the query, so would you query something like that in the bus?

I think in some sense you could. You can imagine two models on the bus, right? Let's go to sound and, The burning is an exception state, like you don't normally have these burnings in the kitchen. but what's making the noise? Okay, but let's say, let's, I'm going to first address the question, not the unexpected noise, we don't know, we'll come back to that right now. One, I hear, like we talked about the train before, right? I hear a sound, open that direction, it sounds A train, but it just is a sound, and the auditory column says, I hear the sound, I know what the sound is, I recognized it before, and it says, here's my ID, here's where I think it is, and then somebody, the other column says, I know that sound, I know that sound, I've associated that sound whenever I see the train. And you say it's over there, so I should expect to see the train over there, right? Or, the auditory column, the visual column is looking and seeing a train, and it would say, and then the auditory column would say, given trains, I often hear the sound, I associate your train with the sound, so I can maybe predict I'll hear the sound. So that's a kind of query. It's like saying, I don't know if it's a query, but it's a, it's an association. I could then, but I could ask. A question like, a query that would be like, you want to say, what is that this location, or where would I find this thing? But you're limited to questions to locations and objects, not to state that at all. the right, that we don't have a state representation. But that was the first question, because then, those three objects are there. I think it's enough for certain applications. This is why I wanted to get eventually to this thing, but we don't have the right net. I think, let's say we, we go build this system, and let's say it takes a fair amount of time to build it, we're going to want to have a set of, capabilities we want to demonstrate at the time we're done.

It will be like the first set of features that the system supports, and so we're going to want to at least have that as a minimum set in our mind so we know what we're completing it and it's working, if we're achieving our goals. it won't be everything. it won't be saying, I smell smoke, they'll tell me what food is burning on the stove. I don't think we'll probably have that right away.

so I have ideas for this, but that's why I think it's important. We can't build everything initially.

usable subset. I think this is a usable subset, but we won't really know until we start laying out the capabilities we want.

So maybe we can, I don't think any one of us knows right now, I'm not hearing it, a proposed additional thing for this list of three that, that someone can make an argument like this, I believe is really going to have to happen in the distribution. Someone does have that case. please say so, I think these other questions, I'm prepared to talk to them from like a normal point of view, I'm not sure, Marcus, you proposed a way of thinking about this, but I'm not sure these other questions are even important anymore, somehow magically they also get resolved. I don't know, like, how you represent uncertainty and how you represent location and orientation. I can come at it from a biological point of view.

I don't know how we resolve that.

One thing I'll throw in, I don't want to dominate the conversation, I took up so much time last week. The terminology I wish I'd used, In addition to what I used, I used very deep learning terminology. I compared it to autoencoders a little bit. I should have also translated it into our language. in some ways what I was proposing last week was like a spatial pooler, but for space, because, like spatial pooler for describing, what's out there. For basically everything you're saying, the object relative to the body, but doing it in a spatial pooler with where, the conventional, the traditional spatial pooler takes the sensory array and describes that in a way that has like the overlap property. I think that would really confuse me with. Not the mixing of the orientation or location, but it was the mixing of the object ID. I thought that was where I was like, oh my god, that doesn't seem right. It seems like it's going to introduce this real difficulty to the system and I couldn't see the benefit. Because a lot of the queries will be one of those two categories. I'm looking for this, where is it? Or what's at this location? and, and so it's when you mix those up, then you have this very, of course, combinatorial problem of potential number of things you could have. and it just seems like a lot of it I know that, I can represent location. pretty precisely, and we know how we can represent object ID, pretty precisely, but it's a combination of the two, it seems like a combinatorial explosion.

that's what was tripping me up about. if we could separate out those two basic ideas, then I might be able to understand it better. the combinatorial explosion part, I feel like I can solve, but you just brought up queries, I need to think about that one. yeah. How do you does that break the idea of queries? That's a good question. I was assuming somehow it's going to work, but maybe it won't. No, my point is right now, that's the question. I think it's really important. It's really important to be able to, for the system to ask itself queries and for an external user of the system to ask queries as well.

and even if you're like, you have this episodic memory and you want to query the episodic memory. what happened? where did you observe over time? Imagine it's observing robots running around a factory and it's I saw this one go over here, and then it did this, and then it went over here.

you need, the, a lot of the queries, internally and externally, would be like, where was this, or what was that? And, what is the sequence of events that led to this? And the, building up this episodic memory seems to be, an important part of this, the capability of the system. and you have to be able to query that on those different, capabilities. Or just in playback, just playback, here's the sequence of things that happened, in a way that, could be understood. So maybe I'll have to think about that one. It does bring up, if you don't mind, I'll throw in an idea I had that is Sorry, but it's a little bit tangential to this, but maybe related to this. I think we can be done with this. I don't know. I can leave it up. We can leave it up. one of the things we talked about graphs, not the Marcus conversation, but prior to before, we talked about in one of these modules, there's a graph and, and then there's, it's inputs from sensor and motor, and it attends to some component. And that component added to the graph and the more layers of those, that kind of thing.

and he talks about how is it that, that when you learn new graphs, they become these components, I'm sure Marcus, it's like they become base units, and then I recall in the brain, the classic story isn't like that. The classic story is that fast learning only occurs in the hippocampus, and the famous case of H. M., they removed the hippocampus, didn't learn anything new, and that it's very well documented that model building in the cortex is much slower. It takes time. It takes months, actually. we talk about it often as it's very rapid. It could be theoretically rapid, but actually in the physical range, it's not as rapid as we'd like it to be. and certainly without the hippocampus, it seems to be extremely impaired. I never could understand how the hippocampus could transfer memory, what it knows, to the cortex. And I've always dismissed that idea. I thought, oh, no one can't believe that would work. But now with the bus idea, I see it actually is possible. so let me just explain what I mean by that. imagine you have the bus and you have a set of modules on it. different sensors and different actuators, but they're all modeling some of them, right? they're all simple. These are cortical column equivalents. And then over here, you have your episodic memory, which is in the brain, that would be the hippocampal complex. And this, we know, is the fast, Very fast, episodic memory, it's a sequence of things happening in time and it knows what happens in time and it remembers exactly what occurred and it is, as I pointed out, it is this sort of first person memory. it is the memory where things, where you were relative to things, in some sense, in body centric coordinates. and when we go about our day, almost all the memory that we're storing for today is in this thing. what you did so far, this meeting we did earlier today, it's all over here on the hippocampus. now clearly these things learn too, and that thing they don't learn. we have to train this, but this is much slower. In the brain, these are slower. And I've always felt we don't need to be slow, because, the brain's limited by neurons, synapses, and various other things. but, if I adopted this architecture, just accept it for what it is right now, it means that I can have a system where these are relatively static. They're not learning. I have to train them sometimes, yeah, they have to be trained, but once I get the system, they're working state. Then it'd be like me going about my day. As I go about today and I solve problems, almost all the memory acquisitions is happening up here. And almost none of the time. Aren't those also learning during the day, slowly? we don't know, actually. There, there's, the example of said that actually almost nothing is learned over time without the hippocampus. Now that doesn't mean that, that we don't know. and then of course it's very much associated, there's a lot of theories that say only these get updated during sleep. Because, a lot of these memories are, seem to be formed during sleep, and so there's some There's a lot of research about how the hippocampus interacts with the cortex and sleep. I think it was like one of the not Focus on that stuff. And I used to say, look, I can understand how one of these things learns on its own. I don't need the hippocampus to do this because it has to learn on its own. So let's not worry about that. but all my pointing out is a practical system. If I wanted to deploy a system that does something useful, I'm building a factory robot and they're, solving widgets all day long.

I believe it has to have this component, but I'm not clear to me that you just have to learn it all, that it'd be like, it'd be like saying, yeah, I'm going to go about my day, do what I'm going to do, but tomorrow is going to be a groundhog day, and I'm going to start all over again, and I'm not going to remember what I did yesterday, or I'll remember what I did for a month. Our memories here are about a month long, like the maximum.

when you forget things your whole time, you forget things you already did this morning. but things can last for up to maybe a month here. and but then somehow it has to get transferred into this. So the only point I'm, the point I'm trying to make here is that We have this issue that these internal modules have to learn and they have to build their graphs. We have this question about how, how it is that the graph that's learned becomes the basic thing. And I'm working on that. There's issues about attention and all this stuff. But I'm saying from a practical point of view, you don't have to solve that problem right away, because we can pretend it's like a brain that is, it's just going about its day and it's all the model building is in one location. So this is where the graph. Would be constantly updated.

and yet, and these would have to be pre trained. But once the pre trained a useful system doesn't have to continue training if I want to create, an AI Einstein. shouldn't these things are going to be updated all the time. But if I wanted to build a practical system that does certain things and still is aware of what is going on and you can see things can tell you what happened. you wouldn't have to have, these could be static. but the graph now is global.

it's not global in a sense. There's one graph. no. it's not, it's, these all have a graph too perhaps, but it's not being updated. so imagine, these are just like I said before, they're just, I'm just saying we could fix these modules and they don't, they're not learning in that particular application. and, they might, they're going to have some sort of structured graph as well. But my point is that you're, as you, the one that's updating rapidly, the one that's going around and saying, Oh, no, this is where the food is on the table. This one's over here. This is the chair here. And I have to, and now I'm going to turn around and remember, there's a chair behind me. So don't walk into it, that kind of stuff. That's all happening over here. and it doesn't have to happen down here. So all I'm pointing out is that this, learning problem that I've referred to, of like, when does the graph become an element of the graph, or a node of the graph, like the compositional structure, we don't have to address that. We can have a system that works and fix it, and then the graph building becomes here. The graph, this graph doesn't actually ever get transmitted. We don't have to have it transmitted to other people. But again, this one could say, where was this? No, it's not. Someone else could tell you where that was. And this is where was this? node, something to tell you where that was. So again, you pass around the, node information, not the graph itself.

So this was a simplification that will allow the system to be much, just simpler, and avoid some of the learning problems that you ultimately have to deal with, but it will actually still be quite useful. and this gets us back to the sort of more of the plug and play architecture, that, you really could imagine a set of pre trained modules, and again, they might not be trained for a particular application, maybe not, I don't know, but you can imagine, but imagine these were just built on some sort of CNN with some glue logic around them. And, and they can output, the components of what they're seeing, this here, I'm seeing this here, I'm seeing this here, I'm seeing this here, however, they do that.

then you could just plug these guys in and they'd all be the same. They could be different modalities, right? Then, and then, all the learning would occur here, during the operation of the system. so it makes things a lot easier. simplifies a lot of the development effort.

In this model, would you, theoretically, something happens and another module, some consolidation of whatever that is, appears as a new static module and that's how you learn additional tasks? I didn't follow that one. You're saying operationally we don't need these guys if it's not relevant to, We don't need which guy? The static guys. No, we need those guys. I'm just saying they don't have to be, they don't have to be. Whatever's represented here. I said these guys, all I'm saying is that these guys don't have to be continually learn. That's my point. No, I understand that. But we still need them. What I meant was we can operate off of this until one of these guys appears, right? If there's a particular behavior, learn, whatever. No, this guy doesn't do anything on its own. See, all behaviors are going to come out by it putting things on the bus thing.

I saw, imagine you're looking at a bunch of things on a table and it's building a graph of them and now its goal is to, pick up object A. where is object A, right? It has to, it will know where object A is, but it has to tell someone to go pick it up. this episodic memory doesn't do that. It doesn't have its own actuator. or, I could tell, push the button. You push the button. You push the button. But this is just, I guess the way I view this can't do anything on its own, right? this is the ears, arms, legs and so on of the system. And this is the thing that's building up its working model of what's going on right at the moment. And, and so when it wants to do something, go find the coffee cup and say, I think I know where the coffee cup is. in space, but it doesn't have, it doesn't have an eye or it doesn't have, it says, this location, you should see your coffee cup and then it puts that information on the bus and the eyes will turn and go, yeah, there it is or not, something like that. so I guess I'm not understanding what this subsumes, you're just saying it's, it knows what the configuration of things are around it. This is an episodic memory. So it basically, it just, it recalls the activity on the, it stores the activity on the bus, for whatever reason, it just stores the activity on the bus, like this is what actually happened, that's what episodic memory would be, it's the same, this was here, and then this was here, and then we moved over here, and so on. Okay, there's a time. Yeah, that's how the, that's, this is all well researched stuff in the hippocampal episodic memory literature. There is a time sequence to it. it's literally why it's called episodic memory. You can remember these episodes in time, and what happened in those episodes in time. So when you recall what you did and you say, I did this before that and then I did this and then I did that's all in your hippocampus. That's where it's happening. Okay. So I. You're describing a recording mechanism. Yes. What triggers it to actually output something on the bus? that's a good question. I don't know the answer to that question. I'm just assuming every iteration just happens. no, I think maybe Kevin's asking what's the system going to do, right? In some sense, you're asking, how does the system do something? How do I get it to make something happen? Something like that? it's, if it's a, if this is a, I'm not saying you said this, but I'm trying to take it to, to the extreme. This is a pure recording mechanism, and this is a pure action mechanism. no, it goes both ways. It goes both ways, right? This is a recording mechanism.

But also it can put information on the bus as well, but what's going to trigger it to put up? All right, that's a good question. So this is a some sense a modeling system with sensors and actuators if you will and Now what's it going to do? What's the purpose of it? what's the goal, right? Yeah, that's not shown in this picture. That's a good question. One could imagine, you could imagine now that this is the temporary model you built of the world as it is now, right?

you can think of that as everything you know about today so far, where you are, what's happened so far here. we could then query the system and say, where is something you saw today? And it would be, it should be able to say, I last saw the mashed potatoes at this corner of the table. and, we could some, voice of God comes in here and says, where is X, that would be on a language model if you will, something like that. Or, what is at this location? Or when did this happen? And then you'd be pretty good thing. but if you tell this thing, for example, if you could say, pick up the coffee cup, if the coffee cup was just temporarily placed on the place. That would only be in this model. It wouldn't be down here. Where the coffee cup is in your world right now is not down here. This guy knows what coffee cups are, but doesn't know where it is. So this guy can then output where the coffee cup is, and if he said pick up the coffee cup, it basically would tell the information about where the coffee cup is relative to the body, right? And then we'd have to figure out who is going to actually go pick up the coffee cup. but the information, it would say, yeah, I know where it is, because I just saw you, I just recall we put it over here, therefore, that's where it is, and anybody can go get it, because now you all can translate where it is known to the body, you can now where it is known to the human. So just for audience's sake, this thing coming in from the external, that could be a separate communication mechanism, or it could somehow flow over the same bus? Yeah, it could go over the same bus. So we can talk about that. I suppose we could try to give it.

this really is not an issue of what capabilities we want to demonstrate. what I can see here is a system that can do inference in prediction, using, prior, so static knowledge of the world and dynamic knowledge of the world, as if, the static knowledge of the world is like here, the dynamic knowledge of the world is up there, and, You, it can do inference and prediction from different positions, it can keep track of things, it will know things where things are, even if it can't sense them anymore, it would have a sort of situational awareness of its world, but what do you do with that? But we also have a sense of time, but that's when it's you could have that, in that, up there, you'd have that information. So yes, so when does the question, but what do we do with this system? It's okay. We got this system. What's good to value?

at some point, the abstract thing is, where's the volition, what initiates this thing? so yeah, yeah, how do you apply it to something? So I'm just saying, what is the system capable of doing? before we ask the volition question, right? I don't think that's the word we want to use. like what we want the system to do.

there has to be some goals for this system, what is it trying to achieve? inference itself is quite useful, and, and has all these really neat properties in between different sensors, and, so when you're missing pieces of data, they won't get confused by various things.

that's, but, so how would I apply this to something? it counts the predictions because of the time you have. the time is more of the history, right? No, but you, if you have a, if you know what something was at a certain time, you can make a prediction of something. Yeah, imagine my graph in the episodic memory just, the last time I looked at it, and just said, okay, the last time I saw it, I liked the food on the table. You sit down at the table, you look at all the different plates, and instantly you find this memory of where the foods are. And, and so that just on its own saying, okay, where was that thing over there? I know where it is. I can't even see it anymore, but I know where it is. Or I'm like, this is why I don't know where it is.

so I'm not sure. I probably didn't answer the question. I didn't understand the comment.

we're sliding into this capabilities thing, right? So imagine that we have this system, what do we want to do with it? Now I keep coming back to surveillance issues. I hate that. That's why I always imagine watching something in a factory, as opposed to people walking down the street. I was thinking agriculture would be a nice alternative to, you could surveil crops instead of surveilling humans. We looked at agriculture issues in the past here and one of the problems with agriculture is that the data is very slow changing. You can build a model, you can look down and say, Oh, I see patterns here. You can build a graph, you can look at the satellite data and look at the picture and try to figure out things, but there's not much of a time component to it. Very slow.

I think it'd be better for sort of dynamic systems.

it might be too much to ask this group to figure this out at the moment, but I think Kevin's question really gets to the heart of what we meant by my second point, which is we want to figure out what we want to demonstrate that's valuable and surprising and quite, remarkable.

and if I just think about it broadly, like inferences of everything that people want to do, they're like, what is this? but now you can do it in a multi sensory way or different configuration using a lot of nice capabilities. You can do it, you can do it not just with static things but with dynamic and changing things, so that you could see, it's not oh, we have this training set and now we're just going to know the thousand categories. It's more new arrangements of them constantly.

Another question is, we really want to introduce anything that's more of a robotic nature that actually does something physical. That seems to open up a big can of worms, although this architecture should support it. it opens up a big can of worms about, complex, applications.

I don't know about that yet.

so maybe this is, the two ways of thinking about this is, Subutai had this very list, good list of technical questions, and that's why I thought it'd be nice to have them present them. And then, but then I do think just the next big thing we have to decide is what we actually want to demonstrate, what we want to show that people will go, holy shit, that's amazing.

and we may not be able to solve that today.

I think we want to get started on implementing this stuff. Even if I don't know that yet, I'll get that soon enough. I think we need to resolve these things though. how are we going to resolve them?

And what's the time we want to spend debating? Can we like try a few of those and just get things going? What was the first sentence Lucas? Can we just try a few of those things and get things going just because when you put it into words and just ideas, it's hard, at least for me to come to conceptualize and actually, see it working. Maybe we can try all those ideas, make small projects like a week project, see how it evolves and come back with feedback. What do other people think of that?

I think it's really nice to have. Same goal, or at least a low start to say, we want to be able to demonstrate this, and which of these things will happen. We will. We'll have to do that.

I've been through this before. It's very similar to me. You sometimes just have to get started with uncertainty. It's just a way to, when you're doing big things, you just cannot resolve all the issues. You just have to pick something and go. And learn then spend some amount of time up front just trying to figure out, okay, let's take our best shot. This can't resolve everything. that's why I'm partial to Lucas's suggestion, until Marcus brought up the, his suggestion about these, autoencoders ideas, I said, okay, we can build this, we can build this using the mechanism we had in the Columns paper, and how to do that. We can get started on that today.

walk through that, but now we're gonna seriously consider other ones. Either we do multiple ones, but I don't want, I don't want this to go on for months of research here. we need to pick something, we need to get going on it. I do know how to, I think I how to do the neural one, the positive one. Does anyone else feel comfortable with that too? I assume you were. Yeah, I know how that works. Yeah. So we know how to get started on that. I just, I don't know how to get started on the idea that Marcus was proposing. Marcus, you'd have to tell us the answer to that question.

I've essentially known how to get started on it, yeah. But, it's Jeff, what's the idea that you're going to, that you want to get started on? It's just, it's just the mechanism that exists in the in the voting mechanism, it's just an associative learning between representations in each column, and the, the new union was used primarily as a method for representing uncertainty, and they, since I went through that, we can do it again if it's not in your guide. And to me, that mechanism is easy to implement. The hard thing to me is how do you calculate? Location relative to the body and orientation relative to that seems like that would be an easy one to do. Interesting. That's again, it's doing this reference frame transformation, going from SB to OB. What if you represent them in polar coordinates. I thought you were going to say computing the S, Sb is the hard part. to the body? Yeah, That's just a forward, Okay, I was thinking like, currently, somehow taking a convolutional neural network, taking an object and outputting both id and pose. I thought it's doable, but that's something we haven't done yet. Do you even have to convert it from Okay. Because what if you just have, different representations for, based on how you're perceiving it, and then different sensors will have different, those representations link up based on what's firing it. No, I don't think it's possible. Because then it would be changing constantly. The whole trick works because, the whole thing works because there is a common reference frame. You didn't have the common reference frame. I mean, that's why it works. Okay. So if you didn't have it and you just have this commentary mess, you can't figure it out. As simply, I said, your fingers are constantly moving. The rest of the brain can't figure out where your finger is, I didn't say that. no, I think the brain always knows exactly where the finger is, though. Your body. Yes. I think SB is no. My point, yes. That's, no. I didn't say, sorry. Did I say sb? I meant you said, I meant, I think you're thinking how is it that the individual modules know the orient, know the pose and orientation of the object. Yeah. Yeah. Yeah. That's the b. No. Oh, okay. Okay. it started, starts that's it, starts with then becomes ob Yeah. Or so to Marcus. The hard part is, so I think we actually agree. We were just checking. Yeah. if I use a convolution neur network to recognize the par glasses. How do I extract out the distance and the orientation of that, glasses so that I can represent it. I think that's what you're saying. The next time if your glasses are in a different way, the OB's should be the same. The OB's should be the same, but the SO's are different. Yeah. But you have to know the SO first.

Yeah, so to me, figuring those things out is really the hard part. Doing the voting, the columns paperwork is easy. So it's not an issue at all. So as I said in the very first time I presented this, that the thing we have to do first is define the bus parameters and the bus architecture. I didn't think it was going to be hard. It can be really easy. But we have to define it, we have to pick one, and go with it. And so if you say, that's trivial, I know, I'd be like, great, that's trivial enough, let's do it. And then, secondly, we say, okay, now we want to start putting things on that bus. They have to output the correct information, and that makes them harder, right? That, I think you're saying, is harder. we know how convolutional neural network can tell me this is a pair of glasses, but how do I get the orientation of that? How do I get this distance? To me, one is like a couple of days implementation. The other is like an open source research project that I just don't know how to solve. So it's it's two very, dramatically different I think that's great.

I'm under the impression, and Marcus has hinted at it, maybe I'm wrong, but I think we could solve this problem. There's lots of ways you could solve this problem. maybe we don't know the best way, maybe we'll have to do, some, crazy stuff. the distance from an object to, something could be easily determined, and you could have multiple sensors, you could have a camera, you could have one of those cameras that detect differences in the distance. You can, but then how do I get the glasses in different orientations? Maybe I just overly train the system and I have a representation for 20 different orientations. And, I represent it that way. I don't know. You can do all kinds of, lackadaisical things to make it get started. we talked about doing some of these unit tests properties and listing those out. I didn't have that on my list, but that's something I've talked to Lucas about. I think that would like, just, we just talked about one. Those would be useful to list out. to me, the first thing I would do is I would get the bus defined, and I don't care if it's trivial, that's good, right? It's simple, but at least get it built. One set of unit tests would say, I'll just, hard code the output for one of these modules just to make sure I can do the right queries if I had if this module were working. how do I send information back and forth? Do I need an attention mechanism? Do I need, there's a bunch of stuff. It's not, I don't think it's driven. So you can build a set of unit tests on the bus itself. then you can build, you can start building modules, and you can say, okay, I'm going to do a visual module. How am I going to work? And that's a whole separate development exercise. it might be much harder, and it may have its own set of unit tests. I don't know. I guess I had one question that was at the level of, posed at the beginning. Was that question okay? Yeah, one question at that level, which is, what is the relationship between modules and cortical columns? That part is not clear to me yet. it's not clear to me either because we thought that maybe, recently we started thinking that what we think of as a cortical column is really a what column and a where column. There's all the functions that have to be performed in this bus. are what column and where column function. but is it even, yeah, so that's one question. Another one is, but I don't know if that's important right now, is it? But is a module, like all of vision could be one module or do we need thousands of modules similar to cortical columns where each one is looking at a small patch of an image has pretty big implications.

I guess to me, the, it's the output of the bus which matters and the input of the bus that matters. And, using a convolutional neural network for vision doesn't rely at all on the many columns global, voting stuff. And so that could be like an eyeball, if you will, one eyeball. And you could have many of those, multiple eyeballs and in brain that might be a thousand columns, but we don't need to do that.

but we might in the future just decide to implement this more brain like and divide it up into, sub components or columns or something like that.

Again, I think if some of these questions are, if we can, if I might just say what does the input and output of the thing have to be, then you can, then you just accept a little problem and you have to say, okay, what's the best way to implement it? I'm imagining right now we can build some of these subdivision ones with a CNN and some glue around it and some other components and get it to work. at the moment, I don't care if that's not biological. It doesn't matter to me at this point. because I wanted to communicate properly.

So that would be, again, if we define the bus architecture, then we can tell you what this bus has to do, and, and then you can start thinking about how you would build those things.

one take is, I think two extremes are one is we could do everything with SDRs, maybe we do these are binary SDRs, we do O valued SDRs, and everything here I know how to do pretty much. the other extreme is Are you talking about the location, the orientation, location orientation? Those would just be SDRs. But, how are you internally representing them?

I'm thinking polar coordinates, but yeah, you could just pick some coordinates. Assuming you want to do real value SDRs. We're going to use that work with the associated memory kind of linkings. I think we're doing deep learning that's going to integrate with deep learning stuff, then we'd have to have your audience stuff right now in some way, I don't worry about deep learning, it's the output of the bus here that matters. so you need to have some representation of orientation and. And location. And at this point, it's nothing to do with it, but it shouldn't matter how I implemented this in here. There's no difference. they did this with, the rounds are people.

So the question I can say would be, we might eliminate if we use your buyer, not sure if that's true or not, but that's what my concern.

.

Are there alternatives that don't require, that prop, that would be available to support research? No, I understand that. are there, it's a literature search at this point, right? Can I just No, that's an unsolved problem. So, It's unsolved? Yeah.

Leapfragging comes out and gives you, top best, vision system and gives me a top five, right? classified. Why can't I represent those with SDRs?

if the system is going to learn, I have a system here. It's got some topics. You have some classifier on it, right?

And now I have some output from the classifier, put those in SDRs. Also, I need other, I need to go the other way. I need to take an SDR and turn it in. One of these, one of these other, yeah, we could do that. We could create encoders. Those encoders, we could create encoders of arbitrary resolution.

I'm a little bit neutral right now, whether the bus should be binary or not. I don't, I don't have a strong opinion about that. But if you start going to non binary SDRs, then I have to question, does the associated memory system continue to work or not? I don't know. Okay, we could do binary SDRs. I'm not pushing for that. no, I'm just saying, I'm just thinking through it. We could create encoders to do, polar coordinates and location, orientation and all that stuff. I think, think about the I'm just saying something that's Close to the columns that I can articulate. Yeah. Then the other end of the thing is what you talked about is the other extreme, and we could do that one too. the, let's take the polar coordinates one. You don't want to just have some random word SDR for every position and polar coordinates. There you really do need to have similarity. No, yeah, no, I'm assuming that. Okay, the object, we don't have similarity, so there's a question there. at the moment we don't know how to do that, and that's a natural output of the. Okay, the columns, okay, fine. The columns paper did not have any kind of concept of similarity. You have to output an SDR that does have that, and I think about like how brains represent, vector, like vector cells or head direction cells and things like that. There's, it's not a big SDR, it's really small and it's not that sparse even, there's a bunch of cells that represent the positions and there's like a bump that moves around so it's not like this big high dimensional vector. but we have encoders for that already. The only thing is we have location encoders and we can do any dimension. oh, yeah, we did all that. Oh, my God, that was really clever. I forgot about that. It was like some kind of random. We did it with, yes, we did it. We were able to encode infinite spaces. yeah, no, that's pretty cool. so to me, that's, if we want to do binary SDRs, we have all of that available. OK, I think that's an important property that we can do it at arbitrary resolution. I think that's something to agree with. This is something that we can do much better than a brain. The point being, though, if I have a very precise Location I can tell you exactly where that is, but I also have to be able to say it's somewhere in this general area, or even a bigger general area, I need to represent all those. So that's where the uncertainty comes in. I'm not sure, yeah, that's this question I'm not sure if that's it, the way I think about in the brain, like it's this bulk of activity where all the, cells around that bump. Okay. have similar representational, more grandmother cells with a bunch of active type. I think those are things we've figured out, but the point being, I need to be able, you can have high precision, but I also need to be able to represent ambiguity in, location. And ambiguity in location is not like ambiguity in ID, object ID. And, that could be a, a coffee cup or it could be a, a farm. We don't have to do this stuff. but here I have to be able to say, it's a rough area or we're narrowing it down. So could that just be a union? I don't know if it could be a union because you just, it might be. unlike, again, I guess you might say, oh, it could be over here, it could be over here, it doesn't seem like you have that kind of ambiguity. It seems like you have more ambiguity along the same lines. Could it just be, all the SDRs? as long as it works.

imagine you had a very high I think voting would work. Imagine you have a vector representation, you have a polar representation space in this room from here. So I'm in the corner, and from my position, what's all the points in this room? I can come up with a very high resolution polar map of this room, and, that's great. But I also have to use the same bits. To represent a much fuzzier one, and I can't be a pleasure. I can't be. Oh, the million bits in that around Iran's head. that's the ones we want. That's too much, right? You can't do that. That's a somewhat familiar question of attraction. one of the key parts of assembling something from a group of parts is doing where this is connected to this. That's something that a convolutional network doesn't do a very good job of. Basically, it learns a whole bunch of relationships that eventually comes down to the fact that implicitly they must be connected, but it goes through a whole Connected? What do you mean connected? Physically connected? Visually adjacent to each other. So So, what do you want to say? We need to know where things are relative to each other. I know, but different. Why? Topology as opposed to geometry. You're saying that I need to know polar coordinates of this, and this, and by inference, they must be close to each other. I'm saying, is there a way of actually parsing the thing and saying, this is connected to this and this. It could be a squashed representation. I think you're mixing two things up, Kevin. I'm just talking right now, purely representing locations in space. It says nothing about what's there. absolutely nothing. It's just This is a location in space. it's a separate question how does the whole system know what things are combined or not combined. so you're saying I only need to know the position space of the entire object. Of the entire object, the level being recognized down here. Okay, so the problem. It might be just a subset of the bigger object. The problem you're facing right now is how you can do that, but how do you compose. And one of the ways to disambiguate, you don't need necessarily have the pose if you recognize that the thing is an ensemble of parts and then you have an invariant, which then you could represent as a graph. no matter what the position is, so Do you understand what I'm trying to say? It's You're talking about joint angles that are constrained by one another?

if I have a crab version of a present interpretation of a bicycle, And I'm not talking about mega parts where it would be, this disconnected from this, but the critical notion is that this is connected to this. I would argue that's not the critical notion. The critical notion is that they're in relative positions. But if I did This, even though the joint angles are quite different, you'd still say, okay, that's like a bicycle. Yeah. Okay. So you want to be able to abstract away the geometry and just have the, I guess I think we're looking at two different ways. The answer here is you're, you're learning the relative position of parts, and that's the key item that's being learned in the system. That's the problem when you say position, that implies a metric. Yeah. I'm saying that you can still recognize it even if things Yeah, but, so you have a graph which says where things are relative to each other. Okay? Hang on. But now as that graph gets distorted, You can still say, it's not exactly, but it's still things that are similar in relationship to other, and at some point it starts breaking, Right.

I don't think of it as the connectivity is the issue. It's really the same, all the arguments you're making here about the role of positions of things.

after all, what if the part of the bicycle was obscured, it was a railing, cut across, you couldn't see part of the frame. You'd still see it as a bicycle, there'd be no questions about it. there'd be a, there'd be a subgraph that would match. No, it would be almost everything in a graph would, yes, to be a sub, which part of the graph is missing, maybe a few components. The point is, it's not that I, determine connectivity, it's I'm looking at the relative positions of things, And, there's an assumption that it holds. It's funny, that reminds me of the Stretchy Bird. The what? The Stretchy Bird experiment. Oh, yeah. you're saying that in, in your ski, as long as they're in the relative positions, even if they're not physically connected, if the bicycle requires it having a frame, and I saw all the components of the bicycle without a frame, I would say, that's like a bicycle, but it's missing a piece. The frame itself is one of the objects that's in that thing. It's not just an edge of a graph, it's actually a thing.

I'm just trying to I mean it's a type of, I'm trying to get at the type of similarity between objects that are, Topologically the same. Topologically the same. It doesn't necessarily have to be connected, there just have to be, so you could have relative positions between those things that are very different. But still the same underlying topology. I'm hypothesizing that, The ability to associate things together by connecting this representation. I think that this idea will fit in at some point, building topological maps of objects topological graphs, being able to see that this is this example I've used before as a cat, and it always has this arm that's connected to a body and.

cats in different poses, but there's some similarity between them. It's the topology. I think, at some point, I've been talking about the morphology problem. Yes. We think it's related to that. So my point is there's this cluster of ideas that are important. this, and this is really hard for a CNN to say this is the same unless you're, unless you train it. So what we would do in this case, the CNN, we would have it attend to different components of this. It tends to those three different components. Not, it says, I don't recognize this object. So let's now attend to the different components of the image. Okay. And now we build it up temporarily, build up a graph, while we have these three things like this. and, I, think Marcus a hit on the right thing. There's a, we, he and I agree that there's a missing component of this. I was calling morphology, you might, want to call it topology. just that this thing's the topology you were describing. I think there's another component besides the relative position of the object, and I talked about that recently and it bothers me because I don't want there to be no more things we have to learn, but there seems to be. the moment we're thinking about this, it doesn't include that. so again, this is version alpha we're talking about, Right. So the question is, can we make a version alpha that's really cool and useful? I think we can. and then as we develop it, we'll develop a lot more ideas and second generation and all these other things. the bet here is if we can come up with an alpha version of this system that is actually works and does something very useful. and it may not solve all these problems.

so that's, and so we know that. This is another problem. It's not like I solved the behavioral problems. learning behaviors.

I guess I'm just coming back to the issue that super time mentioned trying to discern pose. And I'm trying to abstract it down to something that might be easier to solve in a certain fashion rather than, you can throw something at CNN and show a rigid object or its orientations and eventually, slowly, it will learn. You know that these all comes these are all the object ID, but it's a brute force thing. Yeah. And I'm trying to see that if we could break it down to a Much smaller things say that this is connected to this these things somehow together adjacent to each other. No You don't have to look at the entire thing at once as you see you can attend But the notion of you know, you want to know it's physically connected and I'm already know I don't do that now I'm saying if it's visually connected, then you can, start building up for visitor connected. So all you can see is visually, unless you touch it, you could put these things in the arrangement. So visually it looks like that, but if you go to this thing, oh, they're disconnected, it's an illusion, but that's the basic thing when you want to associate these things. It's and that will work no matter what the scale is and what the orientation is, you can say something about that, that you could maybe recognize for what it's worth, Kevin, I agree with you. I think there's a problem with the brute force approach of training vision models to do it this way I think it's a pretty widely accepted criticism in the field, and some of the stuff that I've been trying to get a handle on is the narrow symbolic. And what you're talking about is closely related because it involves a step of abstracting out individual parts. So there's an abstraction step and then a composition step. So you break it into parts and you figure out ways to combine them. But we know in V2 or V3 that there's a notion of end stopped online segments, which kind of relates to maybe, I have a whole nother theory about what those are doing. All right. I can't go there.

I think what here's again, what goes on this bus is going to be tricky, right? It's going to be really hard to do. So we want to build something that works today, even if it's even if it's brute force, but something that's going to do, it's going to meet my requirements up here. And that can be brute force for starters. As we go along, we can improve upon it, we can do all kinds of variations on this. The beauty of this architecture is that You can get started today with something useful and not wait for us to figure out all those problems. that's the challenge, right? That's what I was confronting when we were looking at our roadmaps. How are we going to make reference frames and so on? Then I realized that we can build a whole architecture and get it working on a subset of what we'd like it to do eventually. It'd still be very useful, not a toy problem, not a toy system. And then over time, often, other clever people can figure out how to build these things, how to improve upon them. the ideas that you have, maybe those on the right, maybe it's different, maybe it depends, I don't know. I guess what I'm looking at is, I'm worried about the contents of that bus being fragile. you can build all this stuff around it. one of the beauties of this is, what if it was fragile? I now have, give you ten sensors that are all doing similar things that are equally fragile. But together they won't be bad. Okay. That's the beauty of it. that's the thing. It's I'm looking at it from three different directions, from three different sensors, and in the end we're going to get the right answer. I have a maxim that goes back to when Subutai together. is never put computation space where a sensor would give you a direct answer. So in that sense, you can simplify things tremendously by doing that. Yeah, that in some sense shows, highlights the advantage of the bus. It shows that I could, I might be better off having a hundred really trivial sensors here. Yes. that might be the right answer. Just like little stupid things like a fly's eye, a fly's, a frog, a fly's eye, little things all over the place, but just stupid little sensors that are really fragile or just don't know much. Cut them all over the place. And they all try to, erase them as well. Yeah, have to infer depth. I don't know about inferring depth. No, our eyes have to infer depth. You could actually directly sense it as you pointed out. yeah, I think determining depth is, you could build sensors that literally measure depth. it's No, I understand, but there's a lot of just looking at a two dimensional set of pixels that so much has to process to do that. As you say, if you add more sensors, modalities, whatever, to the thing, the problem becomes easier if you have a way of integrating them. Yeah, and actually the bus does that. And so that's a powerful thing. that brings up the interesting question we should consider. is think more about this idea of building really simple versions of this, really simple, that's like a horrible problem. Yeah.

not so simple, right? We know that they're pretty damn, they're pretty damn sophisticated. But what if you, that's a very interesting question about, I, I don't have to think about that a lot, we can all think about that a lot. what if I give you a hundred trivial sensors versus one sophisticated sensor, or ten sophisticated sensors, a thousand trivial sensors versus ten sophisticated sensors. What are the implications for learning and modeling? Yeah, in some sense, you've got to build up a graph over here in the episodic memory. and nobody knows more about the literature of real things than your graph can do. it just, you're restarting a world of concepts. all right, so that's an interesting dimension of going multiple and small versus, fewer and complex. And I can stop, sorry, I was trying to think about how we actually build one of these using multiple columns, just like we were just like we said, and what kind of local voting would you have so there might be what I was really thinking about at one point, you might have a system bus here, but then you might have local buses on the different sensor modalities. and so you have different columns playing to these, and then you have these playing to these, and so that, that may be really the way the cortex is built. and I was trying to avoid that. I was saying, if we could put a lot of smarts in one of these modules, then I could avoid thinking about this hierarchical construction. It's not relevant. it's more than that, it's more of a physical, these would always vote together before they vote up here, that kind of thing.

I wouldn't route this low level column to this low level column. I might, but I don't think so. Anyway, the brain's used to have this sort of tiered hierarchical structure like this. and when I was trying to think about attention, and I was saying, how do I attend to something? I narrowed the area of the world I'm going to receive input from, but I'm still going to get multiple columns. So I have a set of columns that at one point represent a small feature, and at another point represent a bigger feature. And how does that all work? And, I, was going down a rabbit hole in that. So I said, oh, can we just get away from that? Can we just build a system that has a Maybe a fragile but fairly sophisticated thing that, these modules know can recognize all the things that the ultimate application cares about, right? Imagine I'm a, doing the bin picking problem, parts all over the place and you're trying to figure out, what are the parts you're looking for and something like that. And could you tell, okay, systems know that 50 different parts it has to find. And there's a bunch of sensors and they're all looking at different directions and together they decide to have the, there it is, that orientation exactly, whereas a single camera can't do that. That could be a good solution. Yeah, it is a bifurcation in architecture. Which way? Oh, the one that you made between a few sophisticated, many and small. Yeah. And, If we had, sufficient amount of resources, you would send one team down one path, one team down the other path, and see what, and then synthesize, what the good parts of both might be. just to make sure, I want to make sure everyone understands, the ultimate way I believe this is going to play out, goes 20 years from now. There's going to be a huge industry of people building components and deploying systems. it's a classic sort of technology play. Where there'll be lots of people come out and say, Oh, I have a special sensor technology or I have a special sensor motor system that I want to build and make it do certain things. And so my point of saying this is, we don't have to do all these things. It's impossible. We're not going to do any of it. In the end, we're going to, we're going to be just finding the bus and maintaining the bus. And there'll be a thousand other people doing all these things. This may seem high footed to you, but give me a moment. It's up there for the moment. and, so we just need to get the whole system started. We need to show how it can be done, how you can build these things, here's how it works, then you open it up to other people, and, and there'll be many people trying many things, and they'll be cleverer than we are, because they know their domain really well, and they know their application space very well.