So I volunteered to give a presentation today about Monty's current capabilities. I think it will be a good reminder to all of us and a nice overview to everyone new who joined just to get an overview of what Monty can currently do and also what it can currently not do. So we have realistic expectations and know what we're talking about and know what are the next things we're thinking about and planning on and where we have. solutions and where we don't have solutions yet.

I just put these slides together today and I hope they won't be too confusing. there's a lot of content, so there's a lot, there's three years of content and I tried to put them into a one hour presentation. so just interrupt me if you're missing context or. Not following what I'm saying, but yeah, hopefully this should give a nice high level overview. I'm not going to go into the details of how the algorithm works. so if you want more details on that, we have some meeting recordings and, the new documentation about that. So this is really just about the capabilities.

Let me share my screen. All right. Can you see that first slide? All right. Awesome. So what Monty currently can and cannot do.

first of all, very briefly, what is Monty? it's not some magical wizard system, but it is like this wizard. It's a sensory motor modeling system. So you have a motor system actuators, you have sensors that can move through the world, and then. A brain that kind of processes all the sensory information, if you like to think of it that way, and we can have like different types of sensors and different types of movement through space. So for example, in our current implementation, we have something we often refer to as a vision agent, which is fixed in space. And then it can, the camera can tilt up and down and left and right and explore the space like that. And then we have something we referred to as the touch agent, which is like a finger touching an object and it always moves perpendicular to the surface of an object. And it can move along the surface of a cup and explore it that way.

Monty has a modular structure. and no sensor sees everything. So we have sensor modules and learning modules. And it's not like how you usually think of it in classical computer vision, where you get like one picture of the entire object, and then you recognize the object in there, but instead we have like small sensor patches that move. along the object and recognize the object through successive movements in space. So this is, the system never sees this part. This is just for visualization. It's, it gets this small sensor patch as input. and movement information and you can think of the sensor patch as like a small patch on your retina or a small patch on your fingers, skin, Good analogy is the thing about looking through a straw. Yeah. And that's quite a shift of thinking if you come from classical computer vision, that's something you have to get used to thinking about it in those terms.

Also models use reference frames. So for one, we get motor input and have motor output since it's a sensory motor system. but also the models are optimized for that kind of data. So they use reference frames, to structure that incoming data. And they are loosely modeled after cortical columns. So you can roughly think of a learning module as a cortical column, where sensory input and motor input. And we have some kind of way to maintain reference frame and store how features are located relative to each other in space. And then we have a cortical messaging protocol, which is a kind of a common agreement of how messages are sent between the different modules. So when learning modules vote with each other, that's all in a common language, no matter What kind of sensory input the learning modules get. So a column that gets touch input can talk to one that gets vision input. it can also talk to a higher level, column or learning module. and it's all in that unified protocol, which makes it very easy to plug and play different modules and have a cross, modality information exchange. Does that make sense so far? That's the only slide I have on the basic principles.

Okay, then let's get to the capabilities.

we have this project overview spreadsheet. I'll show it here briefly, where we have a high level view of the project and bigger tasks and ideas that we want to work on. and we have Monty, which is split into these different components like sensor modules, learning modules, motor system, voting, hierarchy, environment, and then we also have capabilities and that's the part I'm going to look at here.

Whenever we get a task done over here and check it off, it will add progress to these capabilities. So we can see where we put a lot of work in already, where we haven't done much yet, or we have a lot planned to do. and this presentation, I structured along these lines. So I'm going to go through each of these one by one and just tell you where we're at, what we've done there so far, And what we can do on that capability. you might notice that none of the capabilities are at a hundred percent yet. So there, everything can still be improved. This is still like a, this is a huge project and there's, it's not like we're anywhere close to having a general system that can do all these things perfectly, just as a front note.

and. Yeah, I'll go through them one by one. Now, if you don't have, yes, I am.

all So object detection, just again, as a brief disclaimer, how object detection works at the moment. So this is the evidence based learning. If you look back into the code later on, it basically tracks how much evidence we have for each hypothesis. so here we have movement through space and with every, after every movement, we take the sensation that we're sensing with the sensor patch and use it to update the evidence for different locations on different objects. And you can see, as we're moving along this cup, we're narrowing down the possibilities. So blue is low evidence, red is high evidence, and it's already. quite clear that we're not on the banana or on the dice or on the bowl. but there's still quite high evidence for anything around the cup here. We're not sure where yet because the cup is quite symmetrical. and through more and more movements, we can narrow down also the location on the cup. Once we actually get on the handle, we can be quite certain where we are on the cup. And over here, you can see like how much evidence we accumulated and what, rotation, we also think that the cup is in, because that's another part of the system. It doesn't only recognize what object we're on, it also recognizes where on the object we are and how the object is rotated.

yeah. Yeah. So it says current most likely hypothesis is one with rotation. Are the correct options? Does it learn the correct option or is it? Is it trying to choose from a set of correct options? Yeah, so it has learned about the coffee mug. It has an internal model of the coffee mug, but it could infer any rotation of this coffee mug. So when it first senses the coffee mug, it's, it initializes hypotheses of what could be the rotation of the coffee mug, given that sensation. And then the more it moves, the more it narrows down those possibilities. I couldn't learn instead of a mug, just cup or cylindrical, I guess my question is it like in the classification? Is it trying to choose from one of the correct 20 answers? actually thinking? So in a lot of experiments, we use this YCB data set. So the, it has models of the 77 objects in memory. So it has seen all of these objects before it has learned models of them. And then when it senses a new object and tries to recognize it, it basically has all of these 77 objects, plus any rotation that these objects could be in. And then it, narrows down which object it might be in what rotation it might be in.

Yeah, maybe it's a, I was just gonna say, yeah, maybe it'd be helpful to clarify that. Yeah. So at the moment there's this discrete list of 77 objects. And some of the stuff we've recently implemented is, starting to capture more that okay, some of these objects are obviously more similar, like the different cups are more similar. but we have some ideas for how to also consolidate that knowledge so that you could imagine it Yeah, developing like a cylinder representation, that it would that would come to mind. and so that it's not always oh, it's this exact cup.

doing actually, it's gotten, pretty complicated because their teams seems to be the ability to, recognize objects using different sets of attributes. You can recognize, sometimes we call it the morphology of an object. and, but that could be independent of the actual features of the object. classic examples, you can have a face made of shroom, and you associate it with a face, even though the features aren't face features. and so there's, so then this gets into the problem of, classification of categories, right? what's a face, right? Doesn't have to have eyes, nose, and a set of things in the right position. we've, spent a lot of time thinking about these issues about object modeling, and it's not just a unified thing. It's not just one model. It turns out that When, the system's working, it, it can, it says, okay, I have evidence for the, or, the orientation of some feature separate evidence, what the actual feature is, and, and the models have to work. It's in this gray zone between them, and we also haven't really quite figured out how to do that. how we represent different classes of objects. we have a lot of ideas on this. We've spent a lot of time on it, but it's still an unknown, a challenge because we can clearly recognize things that are distorted and different and still know what they are. and the system has to be able to do that. I guess the reason why I was asking if there's like a set of 77 or Just thinking on its own, it's, if we have a Monty robot, an actual Monty robot, there's more than 77 objects in the world, it just learned them, right? It needs to learn these objects. Okay. Viviane didn't talk about that, but you learn these objects through the same method of sensation movement. so it can learn more than 77. Oh, yeah, it definitely can fix about 77. Okay. Yeah, it can learn as many objects as you want. There is an issue of capacity. There's an issue of capacity of individual cortical column or learning module. and so that's another topic, complicated topic we could spend time talking about. But yeah, maybe two quick asides on learning is. Like two kind of nice things about it is that it's, does well with continual learning and it, it just learns very, in a very kind of sample efficient way. So it's not like you need to look into classic deep learning where you need to revisit an object potentially thousands or millions of times, and in a batch, otherwise you forget about all the other objects. you can just add a 78th object. It could learn it after a few kind of passes over it. Similar to yeah, a human and then start using that knowledge, right? It's a good point. Really good point. There was no, there's no training phase versus inference phase. You can mix them up. And as you can continue to learn as you go, don't forget. Yeah, and that's, yeah, a very good point because all the measures we have here, they don't really talk about, the learning and how sample efficient it is, but that's one of the big strengths of the system that you can learn a new object very quickly without forgetting anything about the, already known objects.

all right, now some numbers, object detection. So here, those are, we have the 77 objects in memory. We already learned about them and now we're trying to recognize them. And system is fairly well on it. Not perfect, but, here we, in blue is the distant agent. That's like the eye that is fixed in position and moves, tilts up and down and left and right. it's a little bit worse than the surface agent, which is like the finger that moves along the surface. and the main reason for that is that with this fixed. agent, there can be quite ambiguous views of objects. the surface agent has a bit more flexibility of going completely around the object and getting the whole size and shape of the object quicker. and then when we add noise and test these objects and random previously unseen rotations. performance gets a bit worse. it gets less worse with the surface agent again, cause it can just move around the whole object quickly. and then. with using, so all of these numbers are with one learning module and one sensor. And then if we use five sensors and five learning modules and let them vote with each other, also again, noise and random rotations, performance is a little better than with just one. And I'll show some numbers. Later, because voting is really a lot about cutting down on the number of steps. That's mostly what voting does is let you infer to your vote. Yeah, just to give a bit more insight into these numbers. So, we don't just have right and wrong as a potential result of an episode. We can also have the correct most likely hypothesis. That would mean we've moved along the object, but we have multiple potential hypotheses and we didn't get enough evidence to make a certain decision of which object it is or what rotation it is in the given maximum number of steps. So just to keep experiments in a reasonable timeframe, we have a maximum number of steps and after that we stop and we just look at the most likely hypothesis that the system had. And this is the count of where the most likely hypothesis was correct. And this is where it was the wrong object. That was the most likely hypothesis. And it can also happen that. We actually classified an object, but it was the wrong object, not the one we saw. And here are some examples for when it confused objects. So again, here, this is, I'm just showing these to give you an impression that like when it fails, it is a bit understandable of why it fails in a sense. so for example, here, the. Spatula for a spoon or fork for a knife, or A toy airplane for a B toy airplane. which are. Yeah. similar objects in morphology. And then when it has the most likely hypothesis confused, it confuses Phillips screwdriver for a flat screwdriver, but, they were both still in the hypothesis space. So if we hadn't moved more, it might've figured it out, corrected itself. Are you going to talk about action policies, Vivian? yeah, briefly. Okay. Yeah. It might be worth just briefly mentioning on that. Cause I was just reminding myself, Because, yeah, I think at the moment for all of these main experiments, including like the 77 objects, the distant agent can still go to other sides of the objects because they now have the, so there's a policy that, called like the hypothesis driven policy where the learning module. Kind of have its internal model of what it thinks is the most likely object. And then for example, it'll have another model of another likely object. And yeah, I guess maybe you'll talk about this, but yeah. Okay, cool. So just in case you guys are watching this video later or something in terms of that difference in accuracy versus the distant agent and the surface agent. Partly as well, it's also there's other subtle differences in how they move, even though they can go to different parts of the object. Like the distant one is a little bit more of a random walk, whereas the surface one is a bit more. of purposefully moving along the surface, and so it's that small difference is potentially just down to not getting, like if it's not converging quickly, then it's maybe more likely to just accumulate a lot of noise. and things like that.

but yeah, but sorry. No, I'm done talking. but I just wanted to clarify that in case anyone's watching this later. It started at this time. We had no, we didn't have a sort of any kind of sophisticated action policy. We just went along. of course, if you were presented with two screwdrivers. You and you wanted to know which was which you would immediately know where to look. You would look at the end of the end of the screwdriver and attend to that point, whether you do that with your finger, whether you do that visually. And so there's a more intelligent action policy to differentiate objects. And, but, in the beginning, we didn't do any of that stuff, right? And I'm not sure where we ended up on, but we do have more sophisticated action policies than we started with, which in the beginning was like, it's all action policy. But this is a big part of getting, to differentiate certain objects is to know that, oh, between these two objects, that's where I have to pretend to tell the difference. And we do that all the time. So speaking of action policies, actually, I had a. Question while reading that TBP document like a little bit. I didn't go through the whole thing yet in one day. Just the first part where I think we do a good job and distinguishing Monty that it is not your classical computer vision or supply sort of problem that we're trying to solve.

and I guess I was wondering how, maybe we need to clarify like how different it is from RL, especially because we're talking about policies as well.

are we learning policies? so will Monty learn, okay, if I have these two screwdrivers, the intelligent, will it learn on its own? Yeah, it'll be a mix. the policies we have right now can be considered innate policies and, but yeah, but we'll definitely have elements of, reinforcement learning as well. Even now we have, policies where there's learning involved. So we have the model free policies that are basically, you just get the sensory input and that decides how you move next. But then we have the model based policies, which are called the top down policies, where You learn the model of objects and then you use that learned model to decide the next action. And we're not also not Saying that reinforcement learning is like at odds with the system. So for example, you could, you use reinforcement learning as the more for the motor system and train a reinforcement learning agent for the more motor system part of it. it's just that here we are setting the framework for it, where we also have these, models that we learn using reference frames. and, we're not using deep reinforcement learning, which is not very sample efficient, for example. There's a tendency, pretty much everyone, to think about Monty in terms of what they've learned about machine learning in the past, and almost always that's wrong, okay? You just have to really start rethinking things, the sample efficiency, the whole reference frame, the integration of movement into the whole thing, it's just really, different. And, you have to be careful. That's my, that's the hardest thing to be successful, Monty, is to realize that we don't wanna be constrained by the way we think about things in the past. It doesn't mean the brain doesn't do reinforcement, but you just not gonna drop it in and say, okay, we're gonna solve this problem with that. And then, yeah. One, oh, go ahead. I had a couple questions. I know we have a lot to get through. I don't know.

We have. Sure. So when you do these object detection things, are we looking at a fixed number of steps is allowed to terminate itself once it gets to a certain confidence level. And yeah, I guess my question on that is that what if the object, there's a brand new object and it looks like 90 percent like another object, but it has a new distinguishing feature. Like how do we prevent it from terminating itself too early before it gets to that new feature. So now we're looking at a new object that has to learn. Yeah, so there is a terminal condition that is basically saying, Oh, this is an object that I don't know about. So it does not force to choose between the 77 objects. It can say, Oh, this is a new object. And then it would learn about that object and create a new model for that object. And next time it should be able to recognize it. but if it is the case, like you say, where you're moving along the object, it all matches with a known object and we'd never get to that. different distinguishing feature, then yeah, we would recognize it as that known object.

it will be difficult to prevent that. I guess we, would have, if you've never observed it, how would you know. I'm just thinking if I were to maybe grab these headphones or something. And they have been swapped out slightly differently. Maybe the very first grab, I'm satisfied that this is, these are my headphones, but maybe if I grab on the other side, I feel something that's brand new. in your old headphones, you've explored the entire object and what's back on the backside, then you'd see something's wrong. You'd have a model in the bottom. It's incorrect prediction. You go, that's not Right. but if you've never observed the backside, how would You know, it's at the moment, basically, we don't have a condition to handle that. I think there's a lot of stuff we want to improve about the kind of unsupervised learning aspect. in terms of, yeah, how you basically naturally add on more objects, without it being there's also the issue of like, how do you learn in like a multi object environment and, how do you know, okay, now I'm on a different object. and I think all of these things are related to concepts of like prediction error and surprise and okay. I thought this was the, I thought I knew what object it is. Okay. Clearly something very wrong. So now I need to take a step back. but yeah, it's not something we've got, we have implemented at the moment. But yeah. And I got into a lot of these topics in a moment. So yeah, just for the sake of getting through the presentation, maybe. Yeah. I guess just one quick thing I wanted to mention about the, policies to your question, so it was just, Yeah, although a lot of these policies that we have right now are intrinsic, the other thing, and being designed by us, the other thing to, emphasize is that they would also apply even in more kind of abstract spaces. a lot of these ones, like the following principal curvature on an object, can have kind of parallels in abstract spaces and also the hypothesis testing one. so we're not just going to try and design all policies that something might need to recognize physical objects. This is still like a long term, plan with these policies. I guess I just don't want other people to come see Monty. It's oh, I guess it's just not another RL system. It's not right. Yeah. Yeah, But to your point, yeah, we can maybe emphasize that a bit better.

Yeah, so yeah, and just to make it clear right now, all these results, like none of them use any kind of deep learning or reinforcement learning, even though we could use it in places, but we don't need to, okay, let me just show you, I just showed you the results from the best one now just from the worst one. So this is the, Distant agent with noise and random rotations. And here you can see there are just a lot more cases where we reached a time out condition. So it's just not confident enough to terminate the episode and say, I'm on this object. most of the time it still has the correct, most likely hypothesis. And then again, there are a couple. objects that is confused about, but again, you can see that the, all the ones where it's confused are like very similar objects, either morphology wise or color wise or both. Yeah, we shouldn't view this as a failure of the system to recognize, you've got your count of 40 steps there. Those 40 steps are all local on the object. You're not going to figure things out. It goes back to the action policy and how do you, efficiently sample the object completely. there's a lot going on there. So yeah, these incorrect things were natural, given the way that, that, we set up the action policies in the sampling.

yeah. Definitely. And that's why I'm saying there are a lot of points where we can improve more and we have a lot of concrete ideas of how we can do it. We just need the time and resources to implement all these things. Hey Vivien, did you run an experiment where there was an unbounded number of steps to see what the correct percentage was? yeah, we, did, but I don't have the results by hand right now. but it would be pretty easy to rerun it if you're interested.

Alright, now pose detection. So this is, this happens at the same time as object detection. It's a, the same process, actually. and this is the rotation error in degrees. So just for you to have an idea of what those mean, I put these little icons here. So this basically means if that would have been the correct rotation up here, then this is what it would have detected on average.

it is again, better for the surface agent than for the distant agent and better without noise than with noise, but all of. These I think are reasonable. The five learning module condition is actually not very good, but again, there's another action item we have on the agenda voting right now is not happening on pose, but only on object ID. And I think once we incorporate that, that should also help a lot here. and then another note on that is if we actually look at the distribution of, rotation errors, it's a bit of a bimodal distribution. So there's a little spike here at one 180 degree rotation error. And that's essentially like mirror symmetry. So a lot of objects in the data set are symmetric along one or two axes. So like cups and dice and all these kind of objects. we do have a symmetry detection in the algorithm. and account for it, but doesn't always catch it. And in those cases, we get a pretty high rotation error, which kind of skews it up a bit more. Yes. One of the, one thing that's really useful when you think about these experiments, like you think about the surface agent, imagine you sticking your finger into a black box, you're touching something. If the cup is completely upside down, you might at first think it's just the cup and it takes a little bit more effort to realize the cup is inverted. Then I'll be my touch the handle. You might just decide until you actually put your finger on the top. You don't know. so that's just you can always relate this back personal experience. And it's very helpful. I'm trying to do that when you're trying to figure out why is it just imagine I thought it. and you'll be able to see what's going on. Yeah. And then, yeah, here you see another example of the symmetry case happening. So we have the target here, this dice at the, this rotation 90, 0, 180, but it actually detects 90, 0, 0. And if you look at the image, it's almost identical, there's some artifact down here that maybe distinguishes it, but it detects this rotation, plus it detects a symmetrical object, so we would count that as correct, and it's actually good that the system doesn't overfit to these little artifacts down here on the dice.

next one is number of steps. So how fast do we recognize it? And there are two approaches we take to cut down on the number of steps. One is voting and the second one is policy. so voting is we have a bunch of patches in this case, they all move together in the same way. they could be moving independently, like five fingers on the hand, for example. In our case, they don't, they just move together over the cup, like shown here. Each patch feeds into a different sensor module and each sensor module feeds into a different learning module. And since they all get different sensory input, they might have different hypotheses about what object they're on or what pose that object is in. But then they vote with each other and can, thereby narrow down those hypotheses much faster together. and if you look at the number, it cuts down the number of steps required by more than half quite significantly, and it slightly increases accuracy. Again, you're not voting on pose here, right? Yeah, And that's something we need, that's not fundamentally impossible, we just need to add it. Have to do it, it's tricky. Yeah, we don't know how it's done in the brain then, but we know it has to second way to improve number of steps is policy, like we already mentioned before. So for example, this, touch agent has a, Policy that follows principle curvature and then changes to follow the different principle curvature. And at some point it changes again to follow orthogonal to that and so on. So it is a bit more directional about where it goes and, explores the object more efficiently than it would if, you were moving randomly on the object surface. And that helps a lot, like Nils mentioned earlier. Second one is Maybe it's worth us just quickly defining the principal curvature, so if you imagine the like side of a cup or a cylinder, you'll have the, maximum magnitude of curvature, which would be like going around the, cup, and then the, orthogonal, direction to that is the second principal curvature, and in a cup that would be flat. Like it would be no curvature. Yeah. So on this cup one, the flat one would be in this direction and then the curve one would be in this direction. And principal curvatures are always like, they're always orthogonal to each other. So they span up a reference frame together with the point normal, which always points out, out of the surface. Should have made that clear. Yeah.

yeah, the second type of policy is the top down or model based policy. Those are based on the models that are learned in the learning module. And one that we use at the moment is this hypothesis driven jumps. So basically we have two hypotheses in this case, a fork and a knife at, in these rotations. And then we look at where should we move in order to distinguish these two, two objects. that would be the red point here, and then would jump with the sensor to that location and sense there. here's some example. we are first following the principal curvature and then we do a jump to the top of the cutlery since the handle is not very distinguishing. and then that should narrow down our possibilities quite a lot. Or here with five learning modules, we, in a distant agent, we can also jump, to different locations on the object.

I haven't seen that thing, it's good. Yeah. Niels made those. And again, that cuts down quite significantly on the number of steps we need to recognize an object.

So it seems like we would, you would start with a, non model driven, action policy. Do you have some hypothesis and then you equate the switch to a top down one? Exactly. Yeah. That's what's happening here. First, we follow the principle curvature, which is not model driven. It's just based on the sensory inputs. And then we, once we have some hypotheses, we do a jump. You can imagine again, the finger of the box didn't have no idea what you're touching. You follow some edges and say, Oh, that might be this. And you make a jump to hypothesis, see if it is correct.

It's also interesting on this animation that like you have your high confidence about the rotation of the object. You're not sure what it is yet, but you're very sure it's like lying along this plane. So your hypotheses are like almost overlapping, which gives you greater confidence about how to use the hypothesis to move around the object. Yeah. And again, with the symmetry, like it'll sometimes fail in that, it might suggest going to the other side of the cutlery, but it's not really a genuine fair failure. It's just it can't know until it tries to sense one of the tops, like which way it's actually oriented in space. But yeah. And another, yeah. And so that's like another thing where we have some ideas for how to do it, but like At the moment, when we go to an empty point in space, that doesn't update our hypotheses. instead, we just make sure we move back onto the object, but it would be very natural for us to integrate that into the system so that, scaling empty space does tell you a lot about, what is the orientation of the object and things like that.

Yeah, it's very good to know where the object is not.

And yeah, that's the nice thing that it just comes naturally out of the system and how we do things that these hypotheses will align with each other and you don't, the models for fork and knife might have been learned in totally different orientations. But then once we use the hypotheses about how they're oriented, they will be automatically aligned and we know where the most distinguishing features would be.

so now on speed, efficiency, obviously that's a bit related to number of steps since less steps means it takes less time to recognize the object, but this is really more about like how long does each individual step and hypothesis update take, and not going to go through these in too much detail because those are a bit older numbers. I just. Pulled together slides from the past two years. but we've already done a lot of stuff on speed ups. So including just vectorizing a lot of stuff, testing only the top K hypotheses, using different libraries that are faster on our infrastructure, using a multiprocessing and multithreading, and that kind of helped us get a lot of speed ups already. this is the current setup. So we vectorize the evidence update. so those are all matrix multiplications and matrix operations. we use multi threading for the loop over learning modules, for the loop over objects and memories. And then the loop over learning modules could be parallelized, but is not at the moment. you'll see that in the speed numbers, that's one of the bottlenecks. If we use more than one learning module and then we parallelize episodes in an experiment. During evaluation during training, we can't do that because when you're training and learning and constantly adding new information to your models, obviously the sequence matters. So we can't do that in parallel during training. Yeah. Maybe it's worth mentioning, like at the moment we tend to have a lot of objects, like up to 77 objects, whereas we mostly have one learning module, maybe five. And so that was the reason for kind of choosing to. The parallelize over objects, but of course that'll, that might start changing as we want to add learning modules and then sorting that out would be more important. Yeah, And then, yeah, we also did some analysis on what the slowest operations are now. So currently the slowest thing is a KD tree query operation, which is basically. If we have a hypothesis of where we are on the object, that location usually doesn't exactly correspond to a location where we have stored information about like in the objects model. So we have to do a nearest neighbor search to find the nearest points in the graph storage, in that search, take some, takes the most time. we have looked into alternatives. We have tried alternatives, but we have not found anything faster so far. but this is an example where it's really interesting. I think I know how neurons do this and it's not a separate step. It's very efficient. It's really clever, but it requires the neurons do it using synapses and they're locational on the dendrite and it's a completely different mechanism for how we achieve the same result. so it's the kind of challenge we face in this system, right? Here we're trying to do it, not emulate neurons exactly in their, in the details, but when we don't, sometimes you run into issues like this. Yeah. And this might be one thing that we could maybe solve with a more biological implementation at this point. But then that requires putting a whole bunch of other biological details into this, right? So it's That is one of the big challenges, about this to know where you put the dividing line between, and I hate using the word neuro inspired, so neurally constrained solutions, this is how the neurons do this, and we don't want to emulate everything, and yet, when, where do you put that line? It's challenging to know. Yeah. and then, yeah, the second slowest operation are just matrix operations combined. We already managed to cut down a lot on this time just by only testing the top most likely hypotheses or only updating the most likely hypotheses. We can make the matrices much, much smaller, and make it much faster. and then just some numbers again. So this is runtime per step with 77 objects in memory. A lot of our smaller benchmarks only use 10 objects in memory and that's a faster as well. And this was on 16 CPU cores. If you. Use more, you can also, of course, make it faster or slower if you use less or on the laptop, it's also faster. but yeah, here under the setting, just to get some relative numbers.

it takes around two seconds per step. It takes pretty much exactly five times that amount for five learning modules, because as I mentioned earlier, we don't parallelize the updates of learning modules. Which we could do somehow at some point.

but then if we take the total episode runtime, so total time taken until the object is recognized, that difference isn't five times anymore, just because five learning modules take less steps to converge. So they just recognize the object faster. and yeah, it takes about two to six minutes, seven minutes. The left chart is seconds. The right chart is minutes. Yeah. This is seconds. This is minutes. Okay. It's still not as fast as we would like it to be, but yeah, this is current numbers.

all right, noise. So there are two places, there are three, but two places that we commonly test where you could inject noise. We can inject noise right here onto the data. The raw data goes into the sensor module. Or we can inject noise here, onto the data that goes from the sensor module into the learning module. Third option is injecting noise into the models directly. basically this one tests how robust the sensor module is and this one allows us to test more controlled how robust the learning module is.

Here's an example of how noise looks like if you add it before it goes into the sensor module. So you could imagine like some noise added to the RGBD image, and then all the estimates like of curvature and point normals just get skewed quite a bit. and then if you add noise here, you can add it more refined on specific features, like just noise on the location information or just noise on the color, or make it more or less noise on different kinds of features. It's just an example of the typical amount of noise we add to the locations in all the previous experiment results that I showed, where it was the noisy conditions.

To address this raw noise, we added some better calculations, some robust calculations for the point normal. That was a work that, intern Jack did last year. so we got some improvement, quite significant improvement there just using, more, more robust point normal estimates. And then, another thing that comes out of the system is this ability to switch, to use different modalities, So I just threw it in here with noise because it seems a bit related to noise. If basically this is testing, we, learn an object with a touch sensor, put the model in here, and then we unplug that sensor module and plug in a vision sensor and try to recognize these objects. And we pretty much get the same performance, slightly higher rotation error, pretty much the same amount of steps needed.

one thing to add here is that our sensors are pretty similar here. just the action policies are a bit, the bigger difference.

that's that did someone, did you have a question? Will, it looked like you were going to say something. I was shocked and delighted by that that works. That's, crazy to me that you can plug in a different sensor and it's still recognized. Is there anything out there in deep learning world that can come close to doing that? It seems shocking. Yeah, I'm not aware of anything where you can just do it like plug and play like this. That's definitely one of the big strengths of the system for one, you can just plug and play sensors as long as they can all like estimate movements in space. you can at least generalize the morphology of objects. Obviously, if like you have a vision sensor where you have color and then you use a touch sensor, it doesn't sense color. You can't distinguish a blue from red cup anymore, but that's like impossible. but the features that are still there, you can just generalize very easily. And then the second thing is that you can also do cross modality voting and communication, with pretty much all deep learning solutions I know about. It's like there's different sub networks and you have to. coordinate them somehow and merge the features into a mixed vector or something or mixed embedding, whereas here you could, they all use the cortical messaging protocol. So it's, very straightforward. Yeah. This goes back in history and neuroscience again, where Mountcastle's had been other people say, Hey, there's this common structure throughout the cortex is repeated over and over again. Same processing is occuring everywhere, every point in the hierarchy and so on. And neuroscientists didn't believe it either. So many neuroscientists say that's how can that be doesn't make, it looks like it, but it can't be true. And that may be, Will, your, comment as well, the key to understanding all this of course, you just have to accept that it's true. And then you figure out how it works. And, and then really the key to understanding it was a whole bunch of things we've talked about. It's the fact that it's sensory motor and each system, each module knows about movement and the reference frames and then the voting, all this stuff plays together to allow that to happen. But that's one of the key elements of this. And we think about the future of Monty and Thousand Brains Project, people are going to build these modules out of, in silicon. And they're going to be identical and you can build a lot of these modules and you can hook them up to different types of sensors and all different types of ways. And that's what we see in biology. Mammals have different types of sensors. They all feed into the same cortical learning algorithm. They all seem to work. all animals seem to function. All mammals, and yet they have the same underlying thing, even though they have different sensors, different, even eyes are different, and some, animals have electric sensors and things like that. So it's, we just have to accept it's true, and we have to make sure we understand how it works. Can I ask a quick question, Vivian?

is the, when you're saying you're having difficulties in, accelerating the nearest neighbor's search, did you guys try locality sensitive hashing?

yes, I did. I'm not sure anymore what the problem was with it. I would have to go back into my notes. Okay, because effectively it creates a dimensional reduction. So that, jumps you to the most likely candidates more quickly. but sometimes you've got to do multiple hashes in order to get enough specificity. Yeah, I remember there was something, it was either like learning became massively slower or something like that, or, yeah. there, there was some issue that we had when you, when you looked into it. Yeah. I don't remember the details. Yeah. It seems if you, are using KD trees now that it could be a swap in thing, but, that's just my naive conclusion from what you presented. Yeah. I'll, have a look back in my notes what the problem was back then. Yeah. at least I'd be interested in what the problems were, because we're looking at that on our side of the thing too, as a, possible, accelerant. And if there's inherent problems with it that Monty's seeing, then it would be good to know. Yeah. Yeah. I'll have a look back.

okay. Let me move on. If that's a lot of stuff, I hope I can get through it. all right. Unsupervised learning, like we mentioned earlier. the ideas. Learning and inference is really a very intertwined process. And although right now a lot of the experiments we run are just evaluation, we still think of the system eventually just working continuously, learning, recognizing, learning, recognizing. It's to do any kind of learning, you have to do inference at the same time all the time as well. So for example, here you're moving along the cup. You recognize, oh, this is a cup. And then you move some more, explore the object some more, and add these new observations into your existing model of the cup. And so you keep learning and adding onto your models. and we want to be able to like continually. update our models of objects. here's a more concrete example. We have a model of a banana and we recognize the banana and then we exploited some more and added some new points to it. And obviously we also make sure we don't add redundant points into it. but that's the basic idea. And we have a couple of experiments in our benchmark test suit on this as well. We don't get a great performance here yet. And I think this relates mostly to what you touched on Scott. that if we have very few objects in memory, like just one or two, it's very easy for the system to confuse objects. They are not as entangled disentangled yet. so for example, it merges a model for mark and. Cup, or model for strawberry and golf ball or a fork knife spoon object usually emerges. and then once you have this kind of merged object, all the following episodes just recognize these merged objects, and add points to them, which is partly that's A symptom of the data set or like the benchmark as well. Like we want the mug and E cup to be merged, at least at some levels of the system. And, that's also something we're working on. like in some ways it's, great that it. It confuses, or thinks these are the same thing and merges them into one. Yeah. We'll be able to need the fork at some point, right? If it's important. Yeah. so what we really need is like a hierarchical data set where we can say okay, does it recognize it at the granularity of the most precise object, like fork versus spoon, or does it recognize it at the kind of granularity of like piece of cutlery? Does it recognize that the granularity of like cylindrical object? Yeah. And what this really probably boils down to is like a learning parameter of like, how high do you set the threshold for recognizing a new object? And that might be something that we, that just has to get higher, or lower over time, the more objects you have in memory. something like that. if you don't, if you only know one object so far, it's very likely that the next thing is maybe a new object and not, just that object again.

Yeah, not an unsolvable problem and actually a desirable property of the system that it merges these also correctly oriented and aligned. It's also currently, only using like bottom up signals to, detect when it's on a new object or whatever. there's no, for example, like scene level understanding or like object permanence. Like we could definitely add that with the hierarchy. but yeah, so that it understands okay, I'm moving away now. Okay. Now it's going to be a different object.

And things like that. But, yeah.

talking about, multiple objects, this is now we're getting into the newer parts and the capabilities that are a bit lower on the progress bar. so Niels implemented this, multi object environment, and that's what we've been testing on so far, where we just put a bunch of objects into an empty space. And we also have some more. Evaluation criteria measures now where we can have a primary target, the object we start on, but then once we move on to a different object, then obviously we have a new target, that we want to recognize, or we want to go back to the primary target and these are also some results from last year, that like we can use the shift in evidence. That we get when, we move from one object onto another object to detect that we are now not on that object anymore and use that to move back on the object or reset our hypothesis space. and those are some initial results and Niels is picking up that work again now, and this is not really integrated in the current code yet. So this is work in progress that we're showing here. but that's the general idea, categories and generalization. also another topic we talked about briefly now, but just some experiment we ran here is that basically I took the memory, the 77 objects, I deleted one object out of memory, so I would delete the cup out of memory and then see what other object it recognizes instead. Now, that it doesn't know about a cup anymore, and that's the object always shown on the right. So instead of a cup, it would then recognize the red cup instead of a mug, or instead of an orange, a peach, or instead of this one, or instead of a spoon, a knife. You can see that this is Pretty much always the most similar object in the data set that's remaining.

and of course there's a lot more we can do on categories and generalization, but first we will need some new data set that actually has categories of object and new measures on testing that. another one is, I'll just go through this quickly, but if you cluster the internal representations of learning modules, you get these clustered into, nice, Clusters that kind of express also the similarity of the objects, even first the morphology, but also in color space. and this is a plot from, Rami. also if you use TSNE to put all the objects on a two dimensional space, you can make out some meaningful clusters of cups and ball shaped objects.

and then another thing we are, we've implemented, but still need to do a lot of testing on this, Adding more constraint graphs and graphs that kind of update over time and where you can learn through those constraints, you can learn more generic objects with like only the features that are consistently present. so you like learning a generic cup shape versus this specific cup that has a dent in the right corner.

Then compositionality is also one we've done a lot of brainstorming on the past year and we have a lot of concrete ideas also of how we want to implement it. We have the basic routing implemented and some first Monty setups and experiments implemented, but we don't really have a good data set for it yet to test. Rami's internship work tied into this as well and coding object IDs, with some kind of similarity measure. So we can, so the higher level learning module can meaningfully interpret these features. And then this is, was our hackathon project in the summer, to put together like a scene data set where we can actually test compositional scenes where we have. Lower level objects. So fork, spoon, knife, plate, and then a higher level scene that's composed of these objects relative to each other in a certain arrangement. so now the next step here is basically turning this into a benchmark experiment, adding some measures of performance in there, and testing how well our current, implementation does on it.

Deformations is something we have no clue on right now, I would say. something that comes up every once in a while, but I don't think we have any really promising lead on how to implement this or deal with this. at the moment, all the objects we recognize are like solid objects. not like a t shirt that can be deformed in a bunch of different ways. but it's something we should, we need to think about more and figure out how to do.

different features on the same morphology. So we have the morphology of the cup, for example, it can have different color, different pattern, can have a logo, on it. or similarly, same features on different morphology. So can have the Numenta logo on a cup or this squishy brain or baseball cap, and we can recognize it on all of these. this is something we've done a lot of brainstorming on lately, and we have some ideas on this, but nothing implemented yet.

Scale. you can have a children's chair or an adult's chair, or if you live in Germany, maybe you've come across this furniture store that its trademark is that it has a giant chair standing in front of it, and you've probably never seen a chair this big. They actually, I think, have the world record for the biggest chair, but you have no issue recognizing it. At the moment, our system would not be able to do this. we have some ideas about how this is done in the brain, but we have yet to translate that into an actual implementation.

Real world sensors and agents. We've briefly dabbled into this one during a hackathon, like one and a half years ago, where we took the iPad camera and wrote a little iOS app that would basically take a picture with that. iPad depth sensor camera of whatever was in front of the camera. And then we would have a little patch out of that picture that moves over it. So it was 2. 5 D maybe. and we would then actually recognize these objects.

and that was the first real world demo of Monty. There was a recording of it as well. It's a cool moment.

and. In that project, we also made a data set, where we have this kind of set of real world objects that are all at Neil's apartment right now. and we have 3d scans of all of them so we can test them in simulation, or we can test these extra, pictures that we took under different conditions. So here's, here you can see the sim to real transfer. we have some performance drop, but it still works. above chance, like significantly above chance. there are like 12 objects in here. and then some more adversarial conditions like where it's dark or very bright with a lot of reflections, hand intrusion or multiple objects. and yeah, there's still a lot we can do to improve these numbers, but we have at least a small data set to test on.

object behaviors, again, something we've done a lot of talking and brainstorming about and reading other papers and thinking about how we might solve it, and we have some ideas of how we could implement a solution to it. And, Jad, an intern, implemented this little toy environment where we can set up some kind of very simplistic objects with behaviors. to test solution solutions on, but the current system can't deal with object behaviors yet.

Achieving goals. So this is on the policy part. So if you actually want to put the go, the world into a certain state, how do you tell that to the system and how does the system then move to achieve that state? again, we have some ideas on this. One thing we did implement is the goal state generator, which is part of the learning module, and that one can take the models and memory, the current hypotheses, and whatever's currently being sensed. And use this information to generate a target state and send this to the motor system, which can then translate that into an action. And we, for example, use the skill state generator for the top down policy to, to tell the motor system where to move to resolve ambiguity. Like what I showed earlier with the knife and the fork telling it to move to the top of the knife.

And then we also made some plans of how we might set up some more, test environments to test kind of goal policies, where we have a certain goal state that we want the environment to be in. And then we have the state the environment is currently in, and it has to figure out in various adversarial conditions of how to get to that state.

and then some plans on how this would look in the whole system. one requirement for actually implementing and testing this will be the Modeling compositional objects. So solving that. Then I said table scene and recognizing these scenes will be something we need to do first before we can tackle this, someone just started sawing outside. So I'm going to close my window.

We didn't hear it. I didn't.

All right, these are a bit quicker because we don't have actual numbers on it yet, so maybe I'll get through everything.

abstract concepts and spaces, n dimensional spaces, for one we have the temporal dimension, so object behavior, so melodies, Things like that. We have the classical 2D space, like grid cells, and we have three dimensional objects where we know a lot less about how the brain represents those reference frames. We have more abstract spaces like family trees, which might be like a subspace of 2D space with more constraints on it. very abstract space, like math, and then four dimensional space, like quaternions, even though I'm not sure our brain is actually equipped to deal with these, at least from my personal experience.

yeah, I think we're generally dealing with the assumption that brains can represent 3d reference frames plus maybe a temporal dimension, which yeah, is maybe why quaternions are impossible to understand for at least most humans. It's also possible that, yeah, I've always viewed it as. One possibility is that a cortical column or a learning module, the algorithms we think underlying it might suggest that they could learn n dimensional spaces. The neural methods suggest that. But early in our lives, we don't experience, we don't experience certain dimensional spaces. And if you don't, and then they become set. That's the part that doesn't get changed a lot in your lifetime. And because everything else is built upon that. So if you didn't experience those as a child, you wouldn't develop representations for it, you couldn't do it later. that with math, right? If you don't learn math early, it becomes very, difficult to understand math concepts later in life. a lot of things are like that. Music is like that. so it's possible that the underlying algorithms are capable of learning n dimensional spaces. But once you've learned whatever dimensions you're working with, you don't want to change it. because all the models might get really screwed up at that point. I, I don't, just because we're not good at four dimensional space, it doesn't mean that it's not, it may, be that range could do that if you expose the child early enough. I wouldn't say that's, that's out of the question. And link. Do you have any, Vivian, do you have any things for link to look at that are four dimensional? I'll teach him about the term then soon.

I was going to ask in terms of sort of things that get abstracted away. If I understand Monty at the moment, there isn't an inherent representation of time as in it's a sample data system and it doesn't actually matter. Whether it runs slowly or quickly, it's still essentially ignoring time, just processing a sample and then processing the next sample. And in that sense, synchronous. Yeah. So the models don't have any temporal dimension at the moment. they definitely will need to have it to, be able to model object behaviors, for example, or like also melodies and stuff like that, but. You're right, right now it's on the one hand the property we want. So if I move along this cup, it doesn't matter if I go this direction or that direction, it should be the same, but, yeah, it doesn't represent time in the models. This is a case where, I have a pretty solid hypothesis, how time is represented in brains and in the cortex and how it changed, how it handles variations in tempo and things like that. So it's not like we're clueless about this. I've talked about this many times, but I think I know neurons are doing this. But, but it's not in Monty, and again, would we do it the same way or not, it's an interesting question, but it is something that I thought about a long time ago, and I figured we've got to solve this problem, and I, think I know how brains do it. And I was, thinking about it, going. to the context of control system as like an engineering discipline where we're like one of the harder things in controlling something as a robot is to deal with delays. And so you, there's some like temporal problems that have to be solved, but at the same time, it's one of those nice things to abstract away and ignore. Because. Cause the moment you put time into it, then, you have to deal with that through your whole system. Like you have to simulate with the notion of time and you're thinking of time as being a problem, perhaps because you got delays and things like that. Yeah. I was thinking of time as part of the modeling as something you have to represent as well in the model. So it's clearly how you'd have to do that in, like melody and yet you can speed up and slow down the melody and still retain it. It's clear that many of our motor actions. if I sign my name, there's certain velocities and speed at which my fingers move as I write and those vary over time. object behaviors have certain velocities too. And, and some are faster than others. So it's part of the modeling system. It has to deal, it has to represent time. And delay, but you're separate question is, then how would I have to implement this? and, what would, how do you deal with the variations in the real world in the brain? Is it there's a lot of suggestions that another part of the brain, the cerebellum, it takes care of that issue of delays and. And, compensating for, variations in time and not the cortex. So again, I don't know what that means for Monty, but yeah. And this is a bit of a theme in this presentation to a lot of these things we have pretty decent hypotheses about how the brain might do it, or even ideas of how we want to implement it. But we just have this huge list of action items and things we need to do to get it into the system.

and yeah, obviously once we actually start implementing it, we'll come across a whole new set of problems too. Fortunately, I think you're at the end of the, I'm trying to think of what are the big topics that you haven't brought up yet. Oh yeah, I'll finish up. It's only three more. it'll be quick. You're still in abstract spaces here, yeah. Yeah, I, bunched language with abstract spaces. so yeah, just the, one of the things we talked about before, how we might. Also, how language would also be represented in Monty. So if we, for example, read a sentence, that'll be basic object recognition, like all the other objects are being recognized. We recognize the characters and relative locations of characters from words, which we recognize. But then how do we know what it actually means? We can learn associative connections similar to the To voting to models that we've learned in other modalities. Like we might've learned a cup with touch, or we've seen a cup, have a model of a cup and a vision learning module. And we can learn an associative connection. So when I read cup and invokes this model, over there, same with like relative displacements. If I read on, it might invoke like a relative pose and then table and also the table model until it like. invokes a complete scene in our head. That's at least, how it might be working in Monty. and same if we hear language, we would have auditory learning modules that can recognize words, given like temporal, features. I think this is the opposite of, large language models, in the sense that we start out with these physical models of the world, and we understand that scene, whether we have language for it or not, and then what you're really doing language is taking this sort of structured modeling out there, this compositional structure right now, and translating it into a way of communicating to someone else. it starts with that structured model, not with the language, and, where like a deep learning, large language model system, there's no physical relationship of anything. It's just the language itself. It's just the words. it's completely flipped around in the brain, and we shouldn't forget that. we figure out how to represent these structures. Essentially, imagine you're looking at that scene and you're attending the different things. You say, oh, there's a table, there's a cup. As you do that, you're pouring the representation of the relative position to come up to the table. that's done visually or tactically. And then you can translate that into language to communicate that to someone. and of course, you can go back the other way, but it starts with those models. and so I think I've always felt that language is a bit dangerous to focus on early, because it's really subservient to having these Representational, models of the structure of the world first, otherwise I end up with like deep learning systems, which are really good language, but they don't really understand anything. Yeah, exactly. So the nice thing about this system will be that the language is really grounded in physical reality and it will really have an understanding of what these words mean and what a cup word is, what is a cup? And, similarly also how does this word sound of course, but yeah. What you said starts at the top works his way down question. you said grounded in physical reality, but it can also be grounded in abstract reality, right? Just whatever the modeled reality is.

When I say physical reality, what I really mean is a reference frame. There's things in the reference frames at orientations and locations, and those can be physical corresponding or they can abstract spaces, but either way, it's the same thing.

So the sentence structure there, it looks like, okay, you've got the, nouns and the positions of them relative to each other, how do you handle verbs?

those will probably be like behavior models, for example, like object behave, like we might have morphology model, feature models, and behavior models, and also models of how something changes over time. That, associated with that. Your question's exactly what I was trying to address. You start by thinking, oh, with these different types of words and so on, you're going to get lost. You've got to start at the top level, as Vivian was saying, and say, start with trying to, how do we understand language to describe things here? and right in some many verbs relate to actions of objects or actions, relationships between objects.

and then verbs come from the reality versus we start with a concept of a verb and try to understand what it is. I realize there's a big gap there. But, that's the way to approach it. Otherwise you just get lost in, lost down the rabbit hole of being linguists. Okay. the notion of a behavioral model makes, sense to me. So that, that you can, segregate that functionality into that way of thinking about it. okay.

I'm, satisfied for the moment. Yeah, let me quickly go over this one. It's not the most interesting one. So deep learning networks suffer from problems with adversarial examples. So you add like a tiny bit of noise and it suddenly makes a totally wrong prediction with higher confidence or a bit more real worldly. You add a little patch on a stop sign and detect something, a different sign. it's a bit related also that this deep deep learning vision systems often over rely on texture. So this would, for example, be recognized as an elephant instead of a cat, or this would be recognized as a flamingo or a volcano or an Arctic fox instead of an elephant. whereas I think, Monty would be, inherently good at these tasks or would not be tricked by a different texture on, the morphology of an elephant. obviously adversarial examples, Are optimized to the system. So you might optimize adversarial examples for Monty, but hopefully those examples would be more similar to like visual illusions that humans also perceive as visual illusions. That's at least my hope. but yeah, that's something we can explore in the future. Hard to imagine how that wouldn't be the case.

And then lastly, recognizing objects learned in 3d and 2d. So you learn an object by touching it and looking at it, in a three dimensional world, you have a model of the object and you see a two dimensional picture of the object and you can recognize it, or even like a sketched line drawing and you can recognize the object, Yeah, this will, I don't think, be a super difficult property to add to the system, but just another generalization that we might test at some point. Does Link sign the media release? I'm his guardian, I get to sign his media release. Yeah, get some higher, some n dimensions into that baby, test that hypothesis. Yeah, I think he's actually not so good yet at recognizing the 2D version. I didn't see anything else other than a cute baby. I've seen parts of him doing very good. How old?. Almost nine months. Wow.

He's younger in that picture. Oh, yeah. This is from today, I made it for this presentation. You're not supposed to look at screens, but I didn't want to print out the picture.

Okay, a little bit.

all right. To sum it up, back to this overview. So these capabilities here, so object detection, accuracy, pose detection, number of steps, speed, noise, and learning, unsupervised or continually, those are regularly benchmarked. We have benchmarks in our code base, and you have to update them every time you make a significant change to the code.

we have benchmarks in progress for the multiple objects. We have one there already, but. I guess I put it in progress because we also still need to solve it And then for real world sensors and agents, we also have a basic benchmark there, but also still need to get a decent accuracy on there. And then for compositionality, that's the one we got started on during the hackathon that we still need to put in like proper accuracy measurements and, make it into a benchmark experiment. current work is focused on, dealing with multiple objects and compositionality. And then current brainstorming has lately mostly revolved around different features on the same morphology and object behaviors. And before that, we talked a lot about compositionality, although there we are have some more concrete ideas already on how we want to implement that. And then sometimes we also touch on abstract concepts and how to represent space in reference frames. lately, I'm going to talk about compositionality in a few weeks, right? Yeah.

and then there are a couple that are still like, some of them are not as relevant right now. And some of them are still quite untouched, like deformations.

yeah, that was the overview. Anyone have some more questions now? That was great. That was amazing. Yeah, it really was. you covered everything. It's I feel like this is my life. We would be disorganized after 40 years of being disorganized. We've worked on these problems since sort of ad hoc order, cause it's so hard to solve them. And, for so many years, it was just very confusing on these things happen, or even just what had to happen. It took a long time even just to make this list. and yet I think it's pretty complete now. so it's really great to see that.