you talk about this new idea, right? So it's actually a pretty small idea. It's a small change from my current previous thinking, but it is difficult to describe. So I'll spend a lot of time describing it and it's really difficult to understand the implications of it. but I'm really excited about it because it gives you a new way of addressing all these issues we've been doing with, it's like a new tool in the toolbox, a new sort of twist on it, which seems like it's gonna open up a lot of average. I haven't gotten out those yet, so, I'm gonna describe the basic idea and there's gonna be a lot of details, which are gonna be a bit, and, and I won't understand all the implications. but I'm still really excited about it. Okay, so that's that, just that, that's just a warning about what is about to cover. I thought I would start with the review of some of the model, building stuff just to give the basis of, how to understand this new twist. and so in this picture here, I show a column you can think of as a module in commodity, but I'm drawing more like a column. I'm layer three, layer four, layer six. this is basically the model we wrote about in the columns plus paper. It's I can just leave it at that. The basic idea about modeling that we put out in first columns, but then modifying columns plus is the following. You have, and this may be a little bit different than you've been thinking about it, but this is how I think about it. You've got essentially two inputs to model a module. You've got some sensation input and you have a movement input. And the basic idea is yet is you, you sense, you move, you sense and move and you keep track of where you moving. You can build a three dimensional model or something. That's the basic idea. It seems pretty solid. almost has to be that way. So we know so many things fit that. So that's it, but exactly how that happens. It's still, questionable. So the way we modeled it in the Comms plus paper, I guess some frameworks too, but is we have two layers of cells. One layer of cells received sort of the sensory input and the other layer of cells represent location and it received movement information. so these are the two basic inputs to this column. this is gonna represent the location of the object and this is gonna represent, but actually they're gonna represent the same thing because the way we did this is that, there's a, you ultimately end up with an SDR very sparse representation in each of these layers. And in the end that SDR is unique to the object and to the location on the object. and that's true down here too. It's unique to the object and unique to the location on the object. just a little aside, if you think about the sequence memory, is basically you feed in, a series of, observations. You use time as the sort of scale of the, instead of movement. We have time. and, but the SDRs in the sequence memory are unique to, they're unique to all things in the world. They're basically unique to the, sequence name of the melody. And it's also unique to the location in that sequence. So it's, a unique representation for this element in this location and this segment. and that's what these representations here are two that we did in column paper. we're basically saying, we have a, an input that represe will be sense, but that's not unique, but we're gonna make it unique. We're gonna make it sparse or used in the temple memory algorithm. And we also have a location that is not unique like grid cells, but we, have a couple different ways I've talked about of making that unique too. So the basic idea here is you wanna, you basically have this, you're trying to narrow down your possibilities of where you are in the world, and you have two ways of doing it, and they interact with each other. So that your location predicts what you should see next and what next tells you limits, narrows down what you might be looking at, and then, then, that tells you what your location is and so on. So I put these two little dots here because this is this basic mechanism where you have two SDRs. Each can be a union, but each is being, driven or narrowed down by a different type of, reality in the world. So in this case, we have movement and we have sensation. Those are two separate ground truths, if you will. And by using those two different elements, you can narrow down, and get to a unique representation of the input in the unique in this, object at this location. And you can argue that, okay, this is a reputational location that's new to the object, but that's also unique since it both represent unique locations in the object. that's the fundamental premise. And then we did one more thing. In those papers, we proposed that there would be a pooling layer, a temple pooling layer, where you could take these unique SDRs in, a unique meaning each one is classifiable. Like you, each one of those SDRs could, it was a sequence from any one of these SDRs would be, you could classify as this is that sequence, right? Because, they're unique to that sequence. When you hear, you could say, this is unique to some object. And so you basically do a pooling between all these very sparse SDRs, onto some stable representation. So it's a many in one mapping. And that, rep that is a representation of the object that you're sensing in different locations. the reason we know this occurs because we have stable perceptions of things. And so there has to be a stable representation. even though your eyes are moving, your fingers moving, there's a stable representation in addition to that, which is Hey, this thing is still there. It's not moving. The world seems stable. And so that means there are cells that are stable and we, said this is gonna happen in every column. So you can then say, okay, we have two representations here. One is stable and it's independent of the census location and the other is changing with each movement. And we're only generally aware of this one. We're not, able to notice our eyes and things like that generally. we then said one more thing in that paper, we said, Hey look, if other columns are doing the same thing but sensing different locations, then they're also gonna have a stable representation in their column. And we can associate that several representation with disabled representation. So you have these two very sparse, and if we learn to associate them together, then during inference, they too will narrow down. So there's three narrowing down processes here. The one that's voting between these stable representations and two that, I guess this is another narrowing down, but there's three, SDRs that were essentially, being narrowed down in different ways. So this is like saying, okay, I'm narrowing down my, my, my possibilities by my neighbors are telling me, or other parts of the brain are telling me and I'm narrowing down through sensing movement.

are there any questions about data yet? That should be all, like review. but it takes a while to sink in everybody.

alright, there's another twist to this.

and we've talked about it. We did write about this. We didn't know about at the time. Wrote there some other issues, in. There is, these models have an orientation you can think about like vertical north, we talked about that. And, they can be, tilted, your head can be tilted, and all of a sudden now you have to compensate for that. and the other thing is that these models have, they were learned at some scale. Now you might be recognizing the object at a different scale.

what we proposed, we never wrote up. And but we talked about this a bunch of times, and I think there's a lot of truth to this, is that, the brain has to compensate for both scale and, orientation. and we're just talking about vertical orientation now. Anything else? and the thalamus seems to be the right, has the right architecture to do both of those things. And it turns out this movement information and the sensorimotor information both goes through the thalamus, before they get to the brain, before they get to Cortex. And, there's a very profuse, tr projection from cortical column back to the thalamus where this is going through. And so we have this theory that says, first of all, the thalamus looks like a multiplexer. It looks like you've got a bunch of wires coming in and it can rewa reroute them to different wires coming out. It doesn't look like it's doing other types of things. It looks like it's doing that for unique. so we have a hypothesis that it can compensate for vertical orientation. I give examples of this. Remember when I was talking about like you just tilt your head, which you do all the time, and it's just magically everything seems to work right? Even though you've actually skewed everything in your brain coming in on the.

And, yet the movements are adjusted and your perception is adjusted. it's all taken care of. And one could argue that if someone wants a picture, this Yeah, I'm fine. You're good. so I'm, going into this hypothesis now. There's a lot of evidence to suggest this, that Thomas does. Both of these. The scale is, because this.

frequency that's oscillation frequency between these two elements here. And that changing that frequency would change the scale of movements and change the skin. So I'm not gonna go into that, but that seems to be what's going on there. So I just show that in here because it's gonna be important. so we got this model and, we're trying to narrow down what it is we do. Movement sense, We narrow it down by, alternating between those two limiting, possibilities. And we can do it also by voting. It's not necessary, but makes things quicker. and I believe that this is an assumption that the system has to also figure out the scale and orientation of, of, what the current presentation versus the model as it was learned. I'm ignoring displacement for now. I think displacement can be added on top of this whole discussion today without problem. And so I'm not gonna talk about displacement. I'm just gonna stick with the model we had in columns and columns plus.

Important. We all on board with this before.

Alright, wait. Okay. I have good, thank you. Small questions please. Thank you. There's no arrow going back from the layer two, three neuro layer four neurons. Oh, yes, so it's complicated. the classic anatomy does not show that this projection's going back and, I don't remember if we saw some recent, more recent papers that do say in the columns paper, we assumed there was a connection going back. I, think we found some slight support for it, but it's not a, we do say it's not a well documented, we do know there's, a profuse connections from layer two three down to layer five. And so there is this feedback and it's gonna be more complex and I'm not gonna, I don't understand it. And so I, it's a good point, Ben. There will be. This information does flow back into lower parts, the lower layers of the cor, it just doesn't go back to layer four. maybe some, there's a slight evidence for it, but we, there is a very strong connection between layer three, layer five. So it does feedback, but in a complex way. Okay. let me post these, these things to slide real fast. Gonna wait. if you don't mind, I don't mind, I'm trying to stick to high level concepts here because the, new idea is pretty simple at a high level.

yeah, I think people need to understand this before they understand the, what you wrote in the document. Yes, that's right. I think it's going over this again and again because even if it helps me, to see it again and again, but it really takes a while to sink in.

What's going on there? Okay. I, have one more kind of basic question, but I just need a brief review Yeah. In the pooling step, can I think of that as a, sum or a union?

I don't know what those words mean. Sounds good. like I have a bunch of SCRs and layer four and they're different at different points in time during the sequence. Can I think of layer three as like the union of those? Yeah. it's, not the union. No, it's not the union of them. A union, it's a, yeah, go ahead. Yeah. it's a single SDR that's gonna be stable, but the synapses on each cell of that form union, of the temple memory representations. So they, so if you have all of the different unique representations in layer four, a single cell has to be able to recognize all of those and stay stable during that. At least that's how we implemented it. Yeah. So it's a union in the synapses, but it's not a union in the, at the level of the layer two, three represent. I'll just try to show that visually. That's nicely described. You've got an SDR here. It's very sparse. In fact, sparse is good. And this does not change as you learn, as you experience more parts of the object. And so you're essentially, you're taking this SDR and you're forming synapses on these, cells to this pattern. And then you take the next SDR and you form more synapses than these cells to this pattern and so on. So it's basically mapping many SDRs to one SDR. there, there is a limit to how much you can store this way, because you, in theory, you could be struggling, wanna start a lot of these, things on here. And so there's so many synapses and so on. But, but this does not get more, this does not become a of these, it's literally.

if I rec, if I put this input in, I should get this SDR. If I put this input in, I should get this in SDR. And if I did have a backward projection, this SDR wouldn't associate with all of these. But, that doesn't occur, doesn't appear, occur directly here. Yeah. Thank you Many to one, one makes total sense now. Yeah. It, just doesn't change as you, train more, yeah, I have a question about the thalamus function. Yeah. So is the idea is, so Theus corrects all the rotation and then since at the beginning, let's say, we don't know exactly what. The rotation of the object is. And so that would be a, mismatch until then there's a feedback to the thalamus to say, okay, correct your rotation until it matches. Yeah. we've never worked out the, the details of that comb.

but yes, it, has to infer that information and, but in the same way it has to infer everything. So there's gonna be some sort of, process to do that to narrow down, say, okay, what is the right orientation and what is the right scale? but we've never worked out the details of that. Okay. but it looks like the thalamus is capable of doing those things. and there's this very large projection from the cortical column back to the thalamus. meaning many, axons. And there's many synapses. So there, there's a, it's a classic sort of associated memory looking type of thing. Somehow the cortex is gonna, that columns gonna, is gonna work with the thumb to figure out the right answer, just like it's working between other layers to figure out the right answer. So that would be a good exercise to go work out the details of that. but we haven't done that. Okay, thanks. Yeah.

But you'll see why I think it's important in a moment.

okay. Any other questions about them? So this is just the first layout layer of the hierarchy. First level of the hierarchy. This is just one region. One layer. One column. Just one column in a primary sensory region, right? Just make sure I like, yeah, the other layers. let's, go with, so now that's a good segue to number three here. we're gonna talk about two columns that are hierarchically range. And so I really labeled those R one and R two, region one, region two, something like that. Now, I've had an assumption, for a long time that I think is wrong. And the assumption was, we know that layer three and R one projects to layer four and R two, that's a well established fact. I didn't show here. R two also gets input from the eyes or the skin or whatever. But, I can put that aside for the moment. But we do know there's this, it's very clear that layer three here projects layer four. And so you say, oh, that, that's the input to this guy, right? you've learned something here, and now the object becomes the input to this, model. So we've now, we're now, instead of passing in the raw sensory data, we're passing in some processed object that we already recognized and that being sent to here. So that's how I wrote here, assumed that R one to R two was stable sdr r the object rec, recognized. Oh, that just erased something. I, it all erased an object recognized by R one. So this idea was like, okay, if I recognized, a letter here, then I'd be passing that letter into this guy. And this was stable over movements of R one. The anatomy suggests that's what it is 'cause they got this laser three to layer four projection. And I always thought that was the way, that was, the whole idea of the hierarchal temporal memory was like, oh, we're gonna, we're gonna do these, temporal pooling and then temporal pooling and temporal pools. You can get more and more stability go up.

but I think that's wrong. There are a lot of problems with that. and I was thinking about the, the logo on the coffee cup problem and many other problems we've discussed about distortions of objects and we've talked about how you can lay, displacement on top of a morphology and all these things that were just really puzzling. And the basic problem was if you just say, I. This stable representation of whatever I've learned here as an input to this. Oh gosh. I wish I, I just, how did this get erase? Did I do that? Used to tell, I think the eraser, which is right there, slid down. Did you take a picture before? actually the picture is with the eraser half of obscuring it. Oh, this, you sit down myself or I, it slid down one way or another. I don't remember moving it. I don't know. The mind of its own sliding was, that's really interesting. But it slid, maybe I bumped into it or something. I need to, what to predict fails?

I'll say with multiple, oh, no, the, picture is missing. Doesn't, many, I dunno what I wrote there. Wrote, it might be cases, curved and arced. yeah, angled logo, curved and arc. Many oh cases. You are right. Thank you. There we go. Puzzle picture.

I can't imagine how he did it by somebody.

Okay.

here's the thing. I was struggling with this. I imagine that R two is getting this input. Okay, I've recognized this letter A and now it's part of a word or something like that. But, but all the different R one columns, remember there could be multiple R one columns. Each one has its own little space absorbing this, thing at this places. And each one's gonna have the same stable out output. They're, but they're gonna have different, representations here. And so when I get this, if I'm in this R two reason, I get this input. And I wanna, send this feedback back to R one to tell 'em what to expect. I don't know how to send anything back. it's it's, there's no, I've lost my location information here. It's I can't, say, oh, you're supposed to expect this, and your column's supposed to expect that. He says, I just, every time I tried to work out how those had worked, I got totally lost. I can never make it work.

it also, it, it doesn't solve these other problems, which we just talked about. Where we had these many cases, where we had the, logo at an angle, the logo of different scales. The logo could be curved around the cup, the logo could be in an arc. There's all these examples that you come up with where I had a model here, and I'm not trying to apply it to a parent object here, but none of it worked because this, everything was distorted. I can't just take this logo in its normal form and just plop it on this, it just didn't work. And so this is where we had this idea of, taking the displacement and wrapping around the morphology or something else. So the, the, very simple solution to this problem is, the following. It's really simple. just don't assume, no longer assume that what you're passing here is a stable representation to the higher level. and by the way, I sent layer for you projection layer four. But remember there are a lot of different cell layers in layer two and layer three. Some people have identified at least four different types of cells in here.

just because someone says layer three could projections layer four, it doesn't mean there other aren lighter three cells in layer two cells are, that are not the stable representation. It's, a, simplification. So the basic idea here is I don't, R one does not wanna send that stable temple pool of representation. It wants to send, its unique, it's the unique, SDR right now. It's here's what I'm sensing right now. It's here's the instantiation of what part of the model I'm seeing in my model. So if I'm looking at the logo here, I would say, I'm looking at this letter. in the logo. and, but it's really a unique SDR for it's, this location in my model, when it's passed here, this guy has no idea what it means, right? it, there's no letters or anything in past. It's just an SDR. So it's some really sparse pattern that comes in here. And this guy says, someone told me that at this location in my model, there's this very sparse pattern, that represents what's being sensed by this guy.

and I can then associate the, this unique location here with the unique location here. This is, if you think about the unique location here, it's like a node, and now I'm associating some pattern with that node. it turns out that there happens to be a node on this model, but that doesn't, this guy doesn't know that. It just says, okay, there's something coming in that's unique, SCR I'm gonna associate with my location and therefore, and that associated you both ways. And there's, these are connected both ways. So I, when I get to this location, I can invoke that same unique SDR back over here, just by associative learning. And so when this guy's, when this guy's moving around, let's say it's the coffee cup on the logo, when this guy's moving around the coffee cup, it can tell this guy, this particular, it says at this location, this particular, you should be, if you are, if you're, if, you should be, seeing this very specific pattern right now. I'm telling you what you should be, not only if I can invoke the right pattern here, it's both the location and the object. It's saying you should be, at this point on this object and you know what to expect from a sensory input. I don't know what that means. This guy says, I dunno what that means, but you should be at that point in your model. so just so I understand there, so R two is gonna tell R one what location to predict. R one is the one that knows the details of that particular logo. It's, the unique location. It's the location that's the unique location, right? Yeah. It's, essentially this state, the state that's in these, it doesn't really matter, whatever the state is here, that's, these are both unique to the object and location.

and it's basically saying, yeah, given where I am right now, when we went through this, let's say it was a logo when we trained earlier, when I was at this location, you passed the pattern. And, I might now say, oh, that pattern's an n let's, hand wavy.

and, so I said that was a pattern. I, I'm going to associate that. So then I'll tell you when I get back to you, I'll tell you what that is again, I'll tell you. that's what you said. It was, that's what you were doing when I was here. And not only this guy can say, I'm here. you can use that information to validate any models we have over here. It's I'm seeing this part of this logo right now. and you can use that to narrow down where you think you are on this whatever model you're making. It's just another one of these SDR to SDR, nowing capabilities. But now we're going between, instead of between two columns at the same level. We're going between two columns at different levels, and we're making the assumption that they have, they're representing different spaces. This is one model of one object. This is model of another object. and we're just, but we're doing the same thing. We're just passing this information both ways. We associated with learn it. So whatever I think I'm seeing here, you, I can vote all the places. You might have seen the logo over here, and if I'm at any place that had the logo, like on the cup that had the logo and there was a, there was an end of allocation. I'll tell you though, I, you should be expecting that over here. These could both be unions, like if there's uncertainty. I'm not sure what exactly what I'm seeing, and I'm not certain exactly what I'm seeing. They can again, narrow each other down. It's just one more example of the same mechanism of, narrowing down between unions. but this is now doing it hierarchically, which narrowed down the state representation or the, no, the, state represe is not being passed between these, it's just, it's narrowing down the point here. The L six, it's, basically saying it's helping these guys resolve their, I could narrow this down to, you're right. Lewis, I'm not being more specific. All I'm saying is literally what happens. The, cells that project from the upper region to the lower region, I didn't show it here. I, I, didn't mention it. Here. They come from layer six and they project up into layer one. They essentially, cells in layer five, layer three in layer two, all have their apical bend rights in layer one. So in essence, the upper region could be enforcing, basically suggesting to all these cells simultaneously. This is the state you should be in. So it's gonna be more complex. There might be multiple things going on here. but basically, so it could be narrowing down the, it could be narrowing down the stable representation. I, didn't make that distinction. I'm just saying there is a, remember I said the implications are complex, so it's a complex, but basically I'm saying I'm using a unique location here. I'm pushing it back here to help you guys figure out where you are. But it's unique. It's not just oh, it's a logo that's not sufficient. I need to, invoke the specific state right here. It's, this is the part of the logo you should be seeing. 'cause I, when I'm at this point in the cup, that's the part I saw. That's what you told me you were seeing. So I'm gonna tell you to see that again. Something like that.

there's one more twist on this. so I said here, RR one, R two is not the stable SDR R but reference to mutation on the child. So those are parent child things. To get this to work, to solve the problems we know we have to solve you, you have to also share the orientation and the scale because the orientation of the logo in the scale of the logo can be changed independently to the orientation of the scale of a cup. I can have a little logo, I can put different angles.

so that has to be part of what's, recalled here, but we're doing it on a point by point basis. Meaning again, imagine if the logo was in an arc over the here, it was like, and then like it was in a quick curve. the model of the logo doesn't look like that, but the cuff can say, at this point you, told me you had an and at this angle is, and then you have a you at this angle and then you know, this kind of thing. Because that's what this guy would be seeing. It, may not even know that it's recognized the entire logo yet, but it basically, I'm seeing, this, you, and it may be part of the logo. So looking over there, the point is that to get this to work, you have to share the orientation and the scale between these two guys as well. You have to, say, I saw this pattern, I was looking at this pattern disorientation scale. He says, okay, I'll remember that. And then he says, okay, at that point I'm gonna tell you, you should see that pattern at this orientation, at this scale. Maybe you could figure this out, maybe it doesn't have to be passed. I don't know. but the clearest, cleanest implementation would be like that. And the question then is, is this possible? Are these kind of patterns set back and forth? there are multiple, cellular layers that go between regions like this and I have to go back and review the literature. We've read it many times, but I haven't looked at it in a few years. The Alex Thompson papers were the best a few years ago. I wanna look at 'em again because I wanna go very, carefully through what kind projections between these two guys. So maybe we don't have to send this, maybe it's inferred, but I think in the general rules you want to send this, you certainly have to send something about the orientation. I dunno if you do it, then I could try to figure it out. You could say, you could just say you should be silicon, the end on the logo and it has to say, okay, if I'm looking at the, on the logo, maybe I have to figure out the orientation scale on my own. So I'll put this as a question mark. There's only goal to the talent as well. No, not in this direction. No, backwards. yeah, not, in this direction. There is again, complications. Are these layer five cells, which this is typically layer six. So there's a layer five cells here. These are the motor output cells. they also go into the next region, but they go to the over here. so those are the ones that considered like motors. So this is like the movement. It's like the movement command. It's been going back to the, and gets modified again. It like, it goes up back. Yeah. Yeah, It's complicated. As I said, it's really complex. The idea is very simple. The idea is I'm not sending a stable representation between R one and R two. I'm sending the unique representation between R one and R two, and that solves all that looks like it will solve all the weird things we could do with child relationships on parents. it can solve curves and arcs and roundness, and it could even, any, kind of weird thing I can think of. It's because we're not, we're basically not trying to learn the whole logo at once. We're just trying to say, Hey, I saw this thing, which is part of the logo at this location. alright. And then later on, the logo looks like it's tilted at this point or whatever, and I'll associate that.

this means that R two has to learn, has to see every single. Location on the cup that the logo right. Is on. but that's not Excel. So I wrote about this and then wrote that. Yeah. And I said that's, I don't, we can't expect that.

so there would be a solution to this.

for example, I gave, I don't know the solution, but I have some ideas.

why can't the, why can't R one just send the, location in its reference frame? but the details of what's actually that's what it is sending no, it is sending, I, what I mean is that R two just has to understand the relative location and orientation of the two reference frames. That's just one quantity. Yeah. But that, that only works then it can just, that works if the, logo is. Is not deformed in any way. No, It still works. Even a deformed logo has a reference frame. So you still have location reference saying, the reference saying will change.

yeah, but that's R one's job. That's R one's job is to represent that. R two just is to say, okay, where, on this object are you at this point in time? I, and that way it doesn't need to learn that. All that really intricate mapping for every single compositional object. I, can't get that to work super tight. I think that's not gonna work. But I could be wrong. I, here's what I wrote. I, it'll work. Imagine I had a instead of a cop, I just have this flat surface and I'm putting the logo at some location and at some scale and some orientation. in that case, all I have to do is learn one point. If I associate one point, then everything else follows because. Now basically, I can just tell, this guy can tell, you know what, here's a point where I'll give you your orientation, your scale, and your i your object. And from now on you can calculate everything on your own. So I don't need to store all these points. I, sort one or few doesn't really matter, but, I think when you start having the logo do weird things like the arc, I don't know how that works. Maybe you see it. I don't, I, it seems to me like there isn't, I can't tell you what the orientation of the logo is because at any point in time it's, these weird things are going on with it. It's, there's, there isn't, I can't just, there isn't the way of assigning the entire logo to this, new space here. I, can't do that. that space still has some relative orientation and scale with respect to the cup. It may be different from the original logo, but it has something. It's still a single association, but I'm not learning a new model of the, logo here. So how, if I'm not learning a new model of the logo, I'm, using my old one. How is it that I'm able to, but, even here, it would have to learn it because R two has to learn all of those new locations now, that correspond to the current momentous. They still, in both cases, it has to learn it in a very detailed level. in one extreme, we don't have to, all we're doing is associating some part of the logo with something here, some part of the logo, some here, some part of the logo here, some part of the logo here. So probably, and from this guy's perspective. He doesn't know what the logo is. He doesn't know that it has a current, he doesn't know anything about it really. And but it's just saying, these are things you told me when, I was here. You R one, this is what you told me you were seeing. I don't know how you can solve the, basic thrust of, I'm thinking here. So is I, don't see how you can assign the entire logo in these deformed states. It feels like you have to, you have to be able to deal with the fact that, I'm going to, I'm gonna have to map parts of the logo to different, orientations. No, I, agree. I agree with that. I'm just wondering, now you have even more learning to do here because for every object that curve logo could be a part of, you have to learn all of these detail associations. whereas if you just have to learn about the curve logo once. Now you can apply to any hierarchical composition. Now you have this n squared problem of every object it could be in. Now I put the curved logo on a t-shirt. Now I have to learn all of those associations all over it again.

I think there's a solution to this and I, hinted at it. I didn't hint at it. I tried to describe it in some level of detail, in the writeup, which is, the following. If you, don't, first of all, if you did learn every location, remember I was talking about the circle? Lemme just give the example there. Circle.

how many points do you have to sample going the circle? And, there's really no salient points.

so what I was arguing that these, I believe these SDR properties, as you move, in some direction, in on any object, the SDRs represent, the locations change slowly, and at some point they changed completely. So imagine from here to here, there's a slow changing SDR, and now all the bits have changed. and imagine it here and again, I say, okay, move slowly, and now all the bits have changed. You move slowly on, all the bits have changed. This is true for both the location and the sense features and so on. it can change rapidly, but in general, they change slowly. my point was that, if I wanna learn all the points along here. I don't need to sample all the points along here. That is any point between these two points will correctly be represented anyway. if I, as long as I don't go too far, as long as I go okay, this is, an sdr r this is an SDR, and this is an SDR. But in between 'em, there's some shared bits. So they, haven't changed completely. Then all the points in here, if I learn this SDR on this sdr, all the points in here are also learned simultaneously. That is any, if you, point anywhere here, you'll get the correct SDR. There's, it's just you don't have to sample everything. And if I, did sample, if I trained myself on by at different points and learning If if I, at some point you have similar amount of memory dedicated to that, and then if you sample for more points, you're not gonna get any more memory. It's just not gonna form any more, it doesn't need any more information. All the information's already captured in that. In fact, you can infer it from new locations. You never, learned it on as long as you, you're relying on this sort of continuous SDR property.

and I think, to me, that's gonna be the solution to this. That is, I take a, temporal memory, layer here or any one of these layers. if I have a continuously changing location and I have a changing, imagine to the circle, I have a gradually changing curvature that, I can learn the curve without sampling every point. And, yet every point will look like I learned it. every point will be correct, even though I don't have to sample it. and so that it does limited the amount of memory of the store. You don't have to. You don't have to store a unique SDR for each location. 'cause as you go around here, you're only adding a few more bits to different neurons as you go. So it's not like I have to store an SDR here and an SDR here and an SDR here. It's just, it'll naturally just fill in the spaces in between. So I'm working on that hypothesis that's the way this is gonna get resolved. it's also interesting to point out that, as I said earlier, there are some situations where you can actually capture it all at once, right? You can say, oh, if it's just a low, at some orientation scale, one point will do it and then it'll be good. and you and, R one can infer in between. So if all, we have is one point R one, we will guess in between and says, okay, I'm gonna use my model. But then, but if previously it was learned in the context of the cup, then R can two say no. At this point you really have a different orientation, different scale, or, that's what it should be.

So I realize there's a memory problem here, but I think there's a way around it. and also if, another example I gave is essentially the logo goes horizontally. That's really easy to learn and then it starts going vertically. clearly you have to learn something about the logo changing at this point. you just can't use the old model of the logo.

this angle could be anything. I could learn it. It's really simple to learn. So it seems like an infinite number of, permutations on this idea that, that say okay, I can't just rely on the learn model of the logo. That's not gonna work. I have to, have someone else telling me this guy saying, you know what, it's not at this point, you should see this. At this point you should see this, and this point you should see this. The example of this is just that you have to learn something new. When you see something new like this, and you have to learn something new. When you see something new like this, there's no way you can do this without memories. You have to do it. The question is, how much memory is it gonna take, and how's it gonna be done? So do you have an alternate solution to this?

I didn't understand what you said, so I, think I was thinking the, you. Yeah. You do need to learn these continuous representations of the circle and the. The angle one, but I was thinking R one would learn it, not R two.

that would, be, it's like a new OB object or a variation of an object. The re the reason is that, say we have a new curved logo, we just, Christie decides our logo is gonna be curved now. Yeah. Now I can say, oh, I'm gonna put it on our letterhead on a notepad, and I can instantly imagine what that curved logo would look like on the letter pad. I don't need to learn all of those detailed associations. Yeah. And I can put it on a t-shirt, I can put it on, different types of coffee cups. I can put it on our door so all of those can be learned instantly. Or I can represent it and predict and reason about it instantly without learning all of these detailed pairwise. Things I, agree in think I would learn that, I think. I think I would say, oh, there's a new version of the logo, just like there's a, new version of the coffee house. Yeah. But there's many examples where that doesn't make sense. If I put the logo on a flag, for example, I can't learn all the, undulation models of the logo. I might have the morphology of the flag represented in R two, which at any point in time could be, folded, whatever. But R one can't learn, oh, now the logo's folded this way, now the logo's folded that way. Another example would be just the curvature on the cup. when I learned the logo going around the cup, I actually don't learn a new logo. I don't think of it as, oh, there's this logo now going in 3D right now we have this three dimensional curved logo. I don't even perceive it that way. it's just like the regular logo on the cup. So I would say that, I would argue that's the case where you actually didn't learn a new logo curved on the cup.

it, because if you don't perceive it that way, you think it's the same old logo, it just happens to be wrapping around the cup. So maybe there's two different examples there. One is you've got these sort of, morphings of the logo that are not permanent. They're somehow just like fitting to the form of these, of the secondary object. And then there's other things like the arc where you're making, I would learn that, and I guess I would agree with you. I could learn that. but the other example, I don't think I would learn the other example. I just have to deal with it. yeah, I'll have to think about that. Yeah. Yeah. As I said, the implications are complex.

yeah, that's it. This simple idea. Don't, send up the, stable pattern, send up the changing unique SDRs. That's what's going on between level one and level two. There's, there is, again, the unions narrowing downs. It's another union narrowing down process.

In hindsight, it almost seems obvious to me that something like this has to happen and I gave the reasons why I was fool taking otherwise earlier.

does it also send up the stable pattern? I don't, know. Does it need to, if our one R two we're both recognizing the same cup in some sense, then it would make sense. I don't know what R two would do with the stable pattern.

I don't know.

it's a good question.

I don't Maybe that's a, that's an orthogonal thing. If it's, yeah, if it to vote, if it needs to vote on it, then it, then you would need it. Yeah. it, can't vote on it. If R one and R two are modeling two different things at once, Then, there's no voting. If R one and R two are both modeling the same thing for some reason, they're both just looking at the coffee cup, the morphology, the coffee cup, and they're both trying. It is the same thing. That, is it the same thing? I give different layers of the hierarchy. They probably have different views of the same thing. Not exactly the same. in this case, I'm arguing that R one and R two invoking two different models. R two is invoking the model of the cylinder. And R one is invoking the model of the, logo. they're not the same thing at this point, R one doesn't know anything about the cup, doesn't know anything about the cylinder. It's just looking at the details of the logo. It says, oh, I see that it's a logo where R two is looking at a larger picture and it's not. I, this is a, now we're gonna get into a little weird area here. The question is, I think there's a, there's still two different representations. Remember I said I'm gonna ignore displacement for now. I think it's possible that, you still eat. Each model has a morphology model and a, which is really just the, it's by basically learned by the cont of the object. and then it can have a specific, features if you will, at, points on that, at points on the morphology. and so what, where am I getting with that? Is at this point I was just assuming that R two is just looking at the morphology and it's just saying, okay, I'm running the morphology of the cup and and now I'm at some location on the cup and by the way, I can associate with this feature over here.

and I'm assuming this guy is, is, I dunno, they're just modeling two different things.

and so in this case, I, it's hard. Its question is, could a column.

in this, regard, you might argue that a column just starts by, modeling morphology and then assigns whatever pattern it gets from somebody else.

in the, primary region, it's gonna learn a morphology of something and then it's gonna say, okay, that's not about the shape of the thing and the outline of the thing. and now there's some sensory input that are more associated with some point in that morphology. So that might be the color of something or the, the shape of the edge or something like that. I dunno, I, again, it's very confusing to me. but I'm gonna try to, and my next step here is to try to figure out, how to bring back in the idea of displacement and morphology, and build the whole model.

it's like here's your graph here's your morphology model. And then at any point here we can assign some observed feature to that point.

I have a question regarding the back projection from R two to R one. Yes. Is it the SDR of the part or also the SD of part and its location? it's both, Those SDRs in layer four and layer six, they're unique to the location and, the object. Yeah. So unique to the location, but I'm wondering, does it require the, actual location information itself, because these are two different reference frames.

you, could say, I make a prediction at this location. I expect this part, and I know in your language this is the SDR for that part, so I sent that information to our one. The part, is synonymous with the location. you can't separate out the two. remember I was arguing here that. The layer four and layer six, you think, oh, this is the location and this is the feature. But when you actually get down to the SDR, is unique to the location and the object and the SDR is unique to the location, the object, right?

It's, the SDR Pardon? Location in the R one language. R one reference frame. Yeah. In this case, I'm just talking about R one here. Yes. Yeah. So what I sent back from R two to R one is actually encoding the location in R one, in the R one reference frame. It's, basically sending back in essence this SDR and it says, you shouldn't invoke whatever you need to invoke based on this SDR.

yeah, it's the same kind of SDR one sent to R two. The same Statistically it's the same one. Yeah, it's the same one. there's a unique, SD here and there's a unique SD here, and this says, I'm passing you this. Okay. I'm gonna associate with your unique SDR with my location, with my unique SDR and, you can associate your unique SDR with, I'm gonna, I'm gonna bias you. I'm gonna basically say you should probably be, you should probably be biased to the same SDR r you had before. It doesn't, this is just, it's still a simple associated link between two first large representation. Okay. Yeah. So it's not the exact same SDR, it is just, there's a one-to-one mapping between Yeah, the arrow, arrow this way and an arrow this way. That's a great point because I think people get confused about that on the voting here. The voting between comms, the SDR R and this column, and Yes, and this column are not the same, right? They're not, they're just different SDRs, but they associate them. Just like here, this is a different SDR than this SDR R but you associate, so that's, an important point here. these, this mechanism never assumes there's the same SDR anywhere. It just, it can't be, there's no way for the cells to know that. so it's just saying, I got a unique SDR, you got a unique sdr r literally all we're doing is saying, okay, I'm going to, I'm gonna get your pattern, which will help you narrow down your pattern and I'm gonna give you a pattern to help you down your pattern. That's the same thing that's happening here, and that's the same thing that's happening here. Okay. Ko, when we were talking earlier, I think this is a, lot more data than I had to originally assumed. So it's every, Every location along that object in R one is there's an SDR associated with that's sent to R two. It's a very intricate mapping between these two. Yeah. I honestly, and so I was with you on this initially. I said, this can't be right.

But, the more I thought about it, I, couldn't see a way out of it. It's has this has to work somehow this way? So then I started thinking about, what, mechanisms would let it work this way? Now I could be wrong about that, right? I could totally be wrong. But my, thought processes said, I cornered myself to say, to handle all these cases of, objects on objects, and all the different weirdnesses about it. I had to come to this idea that there's this very in intricate mapping between these two regions. I think that 3D curved logo was a good example. 'cause it's, it, we haven't learned that 3D curved logo individually. It's just a flat logo to us, but it's wrapped around the cup. Yeah. And that we can Have to associate that somehow. Yeah. it, the way I think about that is like this looking at as this cup, and of course I can only see part of the logo right now. And so some columns like looking at part of the logo and it says, okay, I recognize that this, 'cause I recognize this part of the logo, I can't see the rest of it. I don't need to see the rest of it. I just, I can recognize it by these local displacement. And, so great. I'll just say, it's like I'm looking in the obscured logo. I say, yeah, I only see part of it, but I identified it uniquely. Then I rotated a little bit and now I see a different part of the logo. And that column, same column will say, yeah, I recognize the logo. Now I see these displacement here. the rest of it's obscure. I can't see it. It's okay, but I still recognize it. or I recognize part, I have good guess it's all, 'cause it says the mental, where it says ENA, whatever this point, curvature issue.

I don't recognize the entire logo at once on the curvature. I don't have to, I just have to be able to recognize a part of the model, and therefore the rest of it, I can't even see. I can't assume I can, I just, I, but I still narrow down and says, okay, I think I'm on the logo on this little section here. and so that's, what I, that's what this column in R one nose at this time. And so the challenge of wrapping around the cylinder is avoid, I. Right now on this surface of the lo of the cup right now, I don't have to worry about where it is over here. Metimes. I'm thinking over there. I'm just sensing this part. It's right there. and then I rotate it again. Oh, it's right there.

as far as the R one column is, it's just it's looking at an, included part of the logo and says, yeah, and I'll just, and the curvature is irrelevant to me. I don't see it, in some sense.

so it solves a problem by just because you don't see the whole thing at once to come just see a little bit and as soon as it's tangential to the, cup. And that's all needs to know at that point in time. And the entire curvature is encoded in R two. R one doesn't know that, but I agree with you. I think we would learn a logos if I saw them. So that's a good distinction. There are some more, some changes to the logo, which I would learn very rapidly.

Yeah, I've got this piece of the discussion. There's, a, request for my own understanding. I wanna go through afterwards. I'm why you do it right now. I'm gonna let that go. Continue on this, this flow here. it just a question here. so I, if we, so our two represents the, SDR for logo part in its reference frame, right? The reference frame of the curved surface of the cup, for example. that was, be careful. Be careful. R two does not know anything about the logo. It says it just knows the SDRs, right? It just says there's an sdr r I'm associating with this location. Yeah. Okay. Yeah. That's it. He knows it, it doesn't, yeah, I agree. Yeah. it doesn't know the logo, but it has the corresponding SDRs of the logo. It just says Yeah, some SDRI don't know what it means. Yeah, but I got it. Yeah. It's the corresponding SDR on the logo and it's, and it's on reference frame. And so I'm wondering is, this. R two? Is it separate from another model that would learn the cup?

is R two, I guess I, I was using the word cylinder. if I wanna, if I just assume R two is learning the morphology of the cup, then, then the morphology could include a handle. So I'm, not, sure what your question was suggesting that there's a cylinder in a cup and I'm gonna start, I'm gonna keep on the path that there's a morphology model. Okay. And then there's, associated with the morphology model are specific SDRs that represent the output of somebody else. and I guess there would be, if the, cup is defined by the morphology of the cup, then I would say R two. Is it, there's no other place. that's what, it's handling. there's no cylinder and then a cylinder with a handle. maybe I could learn that, but I wasn't assuming that here I was assuming a continuous morphology of this thing.

So, you're thinking that ART two could also learn the, cup itself recognized the cup? I'm assuming it's running the morphology of the cup in this case.

Okay. I have a Okay. Yeah, Ben, a pretty basic question that I think will just help concretize my understanding. So I just wanna go back to what you said about the circle, and you have SDRs a different points on the circle. and then there was something going on about interpolating. I have an SDR associated with this point on the circle. I have an SDR associated with another one. Let's say I now land on a point that's between the two that I've never seen. just walk me through what happens. Okay. Just imagine you've got a, dendrite.

This, I write, wrote this up. here's a section of, and I have some SDRI wanna memorize, so I'll pull in 20 synopsis, let's say, and that's gonna represent some SDR. Let's say it's a location, SDR. Okay, now it's coming out of this thing down here or something like that.

As I start moving, so I'm at some locational world, and as the thing starts moving, what's gonna happen is some of these bits are gonna go away and some new ones are gonna appear, right? Just better, because that's what's gonna happen. That's the way there's like a bump, it's like a bumping activity, and you're moving the bump in some direction. some new things are added to the bump and some things fall off the bumper. It's always a just, it's always a set of cells that are active, right? in a good cell. It's a bump of activity. So a whole bunch of cells here that are active. And as you move a little bit. And many of them are still active, but a few new ones and a few go away and a few new ones are added. It's interesting to think about this, by the way, because this bump can move in many different directions, and if the bump moves in one direction, some bit fall off, the bump moves in another direction, other bits fall off. And so essentially if there are 20 synapses here, and sometimes there's 20 dimensions, you can move in 20 directions, you can move in.

but just assume we're moving. and at some point, so at some point I'm, adding new ones and I'm forgetting old ones. Okay. So at some point here, I now have, I now replaced the last of the old ones and I have all new ones. Does that make sense so far? You're looking puzzle. Yeah. okay. So these are dendritic segments. These are, no, this is one dendritic segment. These are synapses? yes. Okay. And so there's an, SDR up here that's representing location, and there's, let's say, 200 bits are active. Ones. And as I move, and anyway, what, this mites gonna do, it's gonna form 20 synapses, subsample this thing, right? Said, okay, I'll subs sample this. I'll get 20 synapses. That's good enough to recognize that 200 bit pattern. This is the fundamental SDR method. So what they did, so this is a robust representation of that pattern. if I see these 20 bits active, I know I'm at that pattern Now as the location changes, these bits come and go, and therefore some of my synapses will become inactive. But I'll, be able to form new ones because some new cells become active. Okay? The point is, as you move the location bump, if you move it smoothly, these bits will change smoothly that you don't change them all at once.

We, they'll be changing one at a time or so. And which ones change first? Depends on which way you're moving. Okay. So as I change the SCR that's up here. So really what's happening, there's, a cascade of things. There's my location and space say of my sensorimotor as I move my sensorimotor, the SDR here representing that location, it's gonna change. And all those synapses are already, have already formed synapses under that dendritic segment. But as I change set of synapses that are active, change smoothly with the SDR, I have, again, I can form new synapses because I have new bits of some of these bits are no longer active, so some of these synapses won't be getting any input. But there'll be new bits up here that are active. And so I have the opportunity forming new synapse. Okay, but that's on a slower timescale. My question is, at inference time, basically I know the morphology of some object, but now I'm sensing a point that I've never, I'm getting there. Okay. So now I have two sets of 20 synopsis here and 20 synopsis here. These are now, my, my argument is the following. Imagine these, this represents a point in space and this represents a point in space.

anywhere in between here, between this point and this point.

I will recognize, that I will infer because it'll be some of these bits in some of these bits. In fact, what I argue is if I just form these 20 and then I just form these 20, it'll automatically infer between, as long as these two points are not too far away, it'll, recognize any pattern in between these two equally. It's actually inferring. It just says this represents the entire span from here to here. This dendrite will pick up any, anywhere along this, span. I see. So in fact, this dendrite can't tell the difference between this point, this dendrite simply turns on It just an now you gotta remember there's a lot of neurons here. This is just one neuron, right? there's gonna be other neurons that are doing the same thing, and, the, population. Imagine it's like the space, the temporal memory. So you got some minicolumns are active as you move. So I have two inputs coming in. I have the, sensory input and I have a location input as, they move. The minicolumns are gonna be changing too. So a single neuron will not represent all points. At some point this neuron gets turned off and another neuron comes on. It's 'cause this minicolumns no longer valid. And so this guy will learn some amount of distance across here. So imagine you now you're looking at the actual output of the, temporal memory layer. The sun cells are active. As these two inputs change the location of the sensory change, if you go smoothly through space, the active cells here will change smoothly too. and they, in some sense you can then represent. You'll have, an SDR that represents all points in between these two points here. uhhuh, I, need to work it out on the visual example somehow. It's a little weird. I know it's strange. Only point is if you don't sample too far apart, then all the cells will, cells will, likely fund over a range of, of, of, inputs. but collectively, they'll still be unique and I don't need to, I don't need to learn or train on all the in between points. And at any point in time, I can come back and, have some pattern. If it's one of these in between points, it'll just recognize it correctly. So I don't need to train at every point and I points Go ahead. does, this only work with spatial interpolation where we know how movement correspond to the changes in the representation? So we, so you said there are like, essentially like 20 dimensions in which we could be moving. And if you have two overlapping SDRs, you, there are many possible ways how you could interpretate them. So does this only work if I know that it's like a sliding window?

I'm a little confused by comment. First of all, it only works as I've described it here, if the SDRs are changing gradually or, if they're changing, suddenly there's an overlap, right? So if I go, if I sense from one location, I jump to some other location, then, I can't triple it in between them. so it requires smoothly trend, smoothly changing inputs. And if the inputs aren't smoothly changing, then I, it takes more memory. you can still learn it, but it takes more memory. So that's one thing. It doesn't just supply to spatial things. It doesn't apply to just movement things. It's just the property of SDRs. So if the SDR is changing smoothly, like I gave the example it, let's say my SDR represented the cur at some point here. as I move around here, the curve tool will change slowly. It'll change gradually. If, I got to some point here and all of a sudden there was like a, bump out like this, it's a coronavirus, then I get to this point, this s str R would change suddenly, and I, and it would take more memory. I wouldn't be able to interpolate at that point. but that makes sense. You have to recognize this change.

so property. I think it doesn't have to with locations. It doesn't have to do with movement. But in the, I found it very interesting in the, if I representing location, that the idea that how this SDR changes between two points is, determined by which direction you're moving.

my question is this basically the brain's way of doing spine interpolation? So if, it's, basically asking how many points do I, okay, so I'm sorry, this is gonna be a little bit more. No, I think I, I know how many points do I need to fully specify a function? If the function is pretty smoothly, varying, or even linear, then I only need one point or a few points. The function here is exactly the morphology of the object. Yeah. Then if I'm trying to learn a really complicated function with lots of wiggles and I need a large number of points. Yeah, I think that's right. Although I don't think you can represent, with a slide, I might be able to say, oh, here's one function that represents the entire circle, or something like that. Or one in this case you, still have to learn a bunch of points.

I think it's a good analogy. Okay, so then I understand it. actually, okay. the next question would be as I make small changes to where I'm sensing on this object.

the SDR, that's produced in say layer six. That's gonna be either the same as it was if I'm slightly over, or it's gonna be different because, a couple neurons fall off and a couple neurons come on. as you move, if you're able to detect the movement, if you're it's discernible, then something has to change. So some of gets will fall off. The movement is small enough that, that none of the, pits change when you can't run it. So now I just wanna walk through what happens if I make that small change And, a new SDR happens in layer six of R two. How does that percolate back and affect R one? if, R two is, if you're moving smoothly along surface cylinder, it's gonna be changing continuously here.

here's a conjecture. I'm making a conjecture. It's not proven. I'm making a conjecture. My conjecture is whenever you have two SDRs that are associated with each other.

if they're smoothly changing, it's very, it's much more efficient in memory usage than you think. And if they're not changing smoothly, then you have to start store something more. So here, if I'm moving smoothly here and I'm moving smoothly over here, then it shouldn't take a, tunnel membrane to do that. It's a general property of associated two SDRs, if they're both smooth, changing smoothly. I haven't worked this out. This is my conjecture that needs to be proven that if we're both changing smoothly, then the memory storage is fairly efficient, not zero. It's efficient enough. There are, by the way, there are a ton of synapses between R two and R one and the brain. So there are a lot of memory there. In fact, people writing, there's more memory up in, the storage up in this, up these April OIDs, it's huge. I know it's, huge. so there is a lot of memory, but we can't have it. We can't train on every point. that's just not possible.

or if I do train at every point, if I can, if I trace my finger around here, and sometimes I'm training at every point, I can't create a huge amount of memory. So my point is, if I do this, I'll sing store the same amount of memory if I go like this anyway, so that's a general property. I'm arguing, it's conjecture. There's a general property in T SDRs. If they're both smoothly changing, then it doesn't make a lot of memory. And, the, confl co conform of those two is also a smoothly change. So if I'm thinking of R one is now representing, Let's say we are in layer four of R one and now I've got an SDR associated with the description with like a feature on that logo, like the U in Menta. Yeah. so I make this, is a simple exercise, but it's worth going over it. And so I changed the position of my sensorimotor a little bit in R two. This changes the sdr r in there six in R two. So now I come back to R one and I've now got the different, stuff biasing my dendrites. So now I'm gonna have the same, what you just described. Some neurons are gonna fall off, some neurons are gonna come online. And instead of the U at eda, now I'm getting the, possibly the M or something smoothly. Interpolating between two. Yeah. I'm moving across from the U to the M to whatever. So is there a question in that or is it, I just wanted to walk through and understand that loop. I think that was right. you have a continually changing representation of your location here and you have a continually changing of your rep representation of your location here as long as you're moving smoothly.

by the way, this may not be smoothly changing if the sense feature changes dramatically. Imagines I'm going from, I gave the example like here, right where I have a curvature, it is smoothly, moves smoothly, smooth.

In some sense, the, imagine the feature is not, smoothly changing anymore. It's not just it's all of a sudden you go, then, if I'm, detecting curvature or, tangential or something, then there's a 90 degree turn here and it's sharp. So that means that the representation over here would change sharply at that point. or, yes. Or here depend, whatever. It's to be able to handle many cases. 'cause it could be that the actual morphology changes smoothly, but say the color on the object changes radically at one point. It is multiple dimensions. I agree. So the point is obviously the more, unique things that are happening, the more memory it's gonna take, to handle it. If you wanna learn it. Exactly. because, we've, when we learn a lot of objects, we don't learn intimately well. we don't learn all the details. often our, concepts are very fuzzy. Only when you study something very carefully, you actually learn exactly what it looks like. You saw that with HighCo examples of when he was trying to draw things. Yeah, I do that with bicycles. I thought a lot about bicycles. I can draw a pretty accurate picture of a bicycle. Most people can't, but you have to really learn all the details. what does it do here? How's it turn there? Where does this come out? This, is horizontal, that vertical, that kind of stuff. So my point is, we don't, necessarily form all these synapses, even though we're, if we're scanning of an object moving around like this, I may not form all the synapses.

it's just not, we don't do it. It's, we don't learn all the details. but I can, if I really work at it, I can learn more details. I can learn the exact things of how they change and curve and so on. so that's another variable on this. We don't have to store every little thing here, just even if I, scan over the whole thing. But I could, if I really focus on it and I train myself and I pay a lot of attention, go around really careful, I can learn all the details. but often we don't. anyway, the point is that we're getting at is if things are smoothly changing, then it doesn't take a lot of memory. But if things change sharply, like the feature changes sharply color or the direction, then it's gonna take more memory at that point, to learn that, this, it's logical that you have to, You can't learn the detail about learning something. Can I go through one quick example just to understand please, interation. So if we, have smooth changing of the SDR, but we take away the assumption that it's, 10 adjacent neurons. But let's say you have 10 neurons that are active in the beginning, then you turn off one randomly and turn on another random neuron, in the. In a hundred possible neurons. And then, a next step on the circle, you turn off another random one and turn on another random one. Why would it be random? I, guess the random part I'm objecting to, it would always be the same ones, but it, would just not be adjacent neurons. I, okay. I don't wanna, even adjacent neurons mean it's just, an sdr r right. yeah, so in the, drawing that you have there, you know which one will be the one that gets dropped, off, and you know which one will be the one that gets turned on. okay, so, careful here I was showing the bump activity, like in grid cells, but if I'm thinking about the location representation here, I'm working under the. This SDR is not the set of bump of activity. Here it is. Remember, it's this would be like the minicolumns that are active and then there's individual cells in the minicolumns. So at the, at one level, I can say, oh yeah, these minicolumns are, they have meaning, and one minicom will become active and the next one come off, and the next one they're the, there, they're ordered. But when you actually look at the actual, if I'm have, if I have a unique SDR where I'm individual cells in the minicolumns, then there is no order. You can't see the order anymore. It's totally lost. It's just like just some hundred bits that just happen to exist out in space, and you have no idea that they're what the relationship to each other is. yeah. Yeah. Okay. I, guess my question is if you have not, if, you don't see that relationship anymore, how, can you do the interpolation? So I. I, let's say I have 10 that are always on, and I have two points, and four of them are still overlapping, but six changed, and now I want to have the middle point. How do I know which of the three from the beginning I need to take away and which three need to be added if I don't know how it, how the changes are in, the syringe? I'm sorry. but some of 'em disappeared and some of them new ones appeared, right? Yeah. So if I, how, do I know which ones disappeared and which ones got added?

I, don't know. I'm, it's, I'm guess I'm confused by your question because if I have two SDRs and they overlap, it's obvious which ones disappeared and which ones became active. the ones added, it from the start to the end point. Those are the two points that I given. But how do you know it for a point in the middle? In between. okay, so here's, the thing that was a bit surprising that I, that would come up a number of years ago was that maybe, this is related to your question, Viviane. I don't know. This is a valid point and this is a valid point, right? What about in between? Couldn't this be invalid? Couldn't some of these, couldn't this set of bits be actually something else? It's like I'm mixing and maxing two things. we call that a mix and match error. you couldn't, this be an incorrect assumption that these are in between? Theoretically that's true, but practically it doesn't happen mathematically. It doesn't happen.

and, maybe could explain that better, but, that was the insight. It's It's couldn't this be wrong? I'm taking half the bits from here and half the bits from there. couldn't there be another point in space that's like that's not between these two? Theoretically, yes, but practically no. Mathematically it's extremely unlikely. So I'm not sure. Maybe that's getting to your question. how it is that I can be certain that this sub, this set here is actually in between this point and this point. Yeah, I guess it would be. How do I know which half the, of the points from the first one and which half of the points from the other one? I would, I don't need to know that all. I need to know that this, these set of synapses wrapped and these set of synapses wrapped. I don't need to know which is from which It's just, a bunch of synapses. Yeah. The, dendrite doesn't know anything about what these represent or which was, it's, just saying that if I, do this exercise, then, the dendrite will recognize every point in between these two points and, not get confused.

the dendrite doesn't know which points come from where and where. There's no idea. Okay. So yes, there sub sample from a bigger, I think, the confusion subsample there, sub sample from the bigger s there, from the bigger activation. It's not the whole activation.

it's true. I'm not sure if that's big. I think that's, I think she was saying that's a smooth information point to the, but it's not, it's like a sub sample from that. It's a subsample, but, yeah, I'm not sure if that was, I'm not, maybe, I don't know. Maybe are we getting close to your question or not? yeah, I, guess I, so could you say that this only works for recognizing a new pattern in between, but it couldn't give you a pattern, a definite pattern in between. So I get the, I can't predict. Okay. Okay, then it makes sense. Yes. That's your que I cannot predict this, I, I don't, all I'm doing is I'm saying I, recognize this. Okay. Okay. That makes a lot of sense. Thanks a lot. Yeah. there's no prediction going on there. As far as the dendrites concerned. These have, can all be mixed together. They, as long as they're within 40 microns of each other, the group I'm looking for, that's sufficient.

yeah, there's no ordering necessarily. It might be, but it doesn't have to be, there and there's no ability to, for the, for anybody to predict what this SDR should be.

Okay. Yeah. Now I understand.

I guess you could predict if, I was associating this point right here, which is in between point 0.1 and 0.2, if I associated it with something else over here, some like sense feature, then I could, this sense feature could predict that's what I'm gonna be, right? This sense feature says, oh yeah, you should be at, some sense. it wouldn't predict the synapses, but it would predict, it could predict as sdr. A little confusing. But the point is no, nobody understands what these bid means on the dendrite. there's no knowledge. You can't predict. It's just recognizing us.