All right, can you guys see my slide on the laser pointer? Yes, yep. All right, perfect. Okay, yeah, so I'm going to try and explain the evidence based learning module in a bit more detail and hopefully give you a bit of a better understanding what it does and also how it works within the whole Monty framework. Let me see. Yeah. I've felt like this for a long time trying to explain it and I spend a lot of time thinking about how to best explain, how everything works and fits together and how everything is connected. And I hope I, Got it halfway understandable by now, but yeah, feel free to interrupt and ask questions and also let me know if there may be better ways to explain things. but yeah, let's go ahead. So I'll start with the general framework first. It's not specific to the learning module, but. yeah, first since, Jeff doesn't really know the framework yet, and also just as a short refresher.

yeah, we basically test object recognition at the moment, object and pose recognition, and we test this on the YCB dataset, which are 77 different objects, and we test in test these objects in different rotations and locations in the environment. and the objects are basically floating in empty space and there's only one object. So we have the object in Habitat in an empty space and we have a sensor patch which always only perceives a small part of the object.

usually it's a RGBD camera and then from this little patch we can extract, three dimensional patch, which are XYZ coordinates of where each pixel in the patch would be in space using the depth image and the location of the agent and sensor. And then from this patch, we can extract a pose. So a location in space and a, an orientation, and the orientation is defined by the point normal. that's a vector that points out of the surface, basically, and the first curvature direction, which, in this case, for example, would point to the side of the cup, and then the second curvature direction would point upwards. So the largest and, direction of the largest and smallest curvature, and then we can also extract pose independent features, so features that don't change if the object rotates or moves. For example, color at a location or the magnitude of the curvature.

And this is the general format. Everything inside of Monty happens in this format poses, with features or features at poses.

we currently have two different types of action spaces. One is the vision action space, where we have the agent fixed in one location, and it can tilt up, down, left, and right. And the second one is the touch action space, where the agent moves close to the surface, always perpendicular to the surface, and it can move around the entire object. And just to visualize, so this is how the vision action would look like. So we just tilt, we never see the object from the other side, but we see it from one viewpoint and we can, yeah, explore it here. This is the viewfinder just for visualization. And then this is the patch. That would get sent to the learning module or the sensor module as well. And then this is how the touch agent would look. It basically can move around the entire object. It moves a little slower because it wants to stay, on the surface of the object and not fall off the object. But yeah, it looks like the object is rotating, but it's just the sensor moving around the object.

Okay. So then there's the distinction between agent and sensors. An agent is basically whatever moves in the environment and whatever, the policy use it, what the agent uses, the policy that we define to move in the environment. And then we can have a variable number of sensors attached to an agent and all the sensors attached to an agent move together. So for example, like. patches of skin on a finger would be multiple sensors attached to one agent, or patches on the retina would be, yeah, multiple sensors attached to one agent.

And then we can have different policies. So we can have a completely random policy, like this orange one that just randomly moves along the object. Then we can have bottom up policies. That means we use the sensed features to decide the next action. So here we use the sensed curvature directions, and we just follow these curvature directions and switch at some points, and then we can have top down policies, which means we use the models that we have learned to decide the next action. Typically the learning module suggests a motor command. And then that gets executed by the agent. So that would be the green line we move. And then the learning model could suggest to move to the handle. And then we keep moving there.

Hey Vivian, quick question with the previous slide.

you said an agent can have multiple sensors associated with it. and in, have we ever used that in any such scenario? Or is it always one sensor per agent right now? for example, all of the voting experiments use, one agent and multiple sensors. okay, got it.

So yeah, I will show some examples later where we use one agent and five sensor patches, for example. Okay, so in that case, the voting module uses sensors that are near, very close to one another. It's not using patches that are quite very different in very different locations. Yeah, exactly. Okay, so that's actually impressive that the voting works as well as it does. You would expect to get a lot more disambiguating information if they're further away than if they're right next to one another.

yeah, I mean they're not like, millimeters next to each other. They can still cover further away parts of the object depending on the perspective, but Yeah, we haven't really tested yet with multiple agents that completely independently move around the object and can see like opposite sides of the object at the same time.

Were you about to say something, Jeff? No.

Okay. All right. I'll go on then.

we have to discretize, discretize time in a way. So there are three ways in which we do that. First is steps. That's the smallest interval. So a step is taking one action and observing a new observation in the environment.

then the second one is the episode. So we can take a variable number of steps until an episode ends. that happens, for example, when we recognize the object. And then when the episode ends, we initialize the environment new, we reset all the hypotheses in the learning module and we show a different object. And then the last one is the epoch.

basically this happens once we cycle through all of the objects, a new epoch starts, and we then start again with the first object, and then usually we show it in a new rotation.

that's yeah, the testing set up at the moment.

And then. Yeah, the whole system is designed to work, from scratch. So it is designed to be able to learn starting with no knowledge about the world. So nothing is in memory yet. The sensor moves on the object. It recognizes, I don't know this object. And then it starts exploring and collect some information about this object and then builds a graph out of these observations and stores it in memory. And then when we have a new episode, the sensor moves again on the object. it might recognize the object as this graph that it previously built and saved, and then it can again explore the object further to collect some more observations about it, and then use all of these new observations to update the graph in memory. And this way it can gradually learn complete graphs of objects, complete models of objects. And it can, or should ideally even be able to do this without labels. at the moment we usually do supervised pre training where we provide labels to learn a complete model. And then we load these models into memory at the beginning of every experiment. just so we don't have to run this from scratch and we can make sure that these models are complete. I really like these slides. The way you've done the slides are very clear. Cool. Thanks. Yeah, I was trying to think about how to explain these things, in a good way. Yeah, they're very nice. I have a question. Yeah. So there's a difficult period when you're, when you're confronted with an unknown object and you have to decide whether you're learning a new object or you're updating an existing one. yeah. how do you handle that?

Yeah, so basically every, episode has two phases. One is matching and the second one is exploration. And if during the matching phase, none of the models and memory match the observations, then we create a new graph. And if during the matching phase, we recognize the object. So we do match it to a graph that we have stored in memory. Then we extend the graph with the new observations. of course it can happen, for example, if in the second episode I would have started like somewhere on the bottom of the cup or something that's not in my graph yet, then I would create a new graph for this object, even though it should be the same graph. But it's just because I've seen a completely different side of the cup that I don't have in my model yet. So there, in the future, we could do some kind of graph consolidation later on or something. But, Yeah, this is how it is right now. Or you could extend the exploration phase to actually, try and make sure you explore the entire object when you build the graph in the first place. Yeah, no, I think it's a good approach. And this was something we dealt with on sequence memory, and it was very tricky. and so I think it's very practical the way you're going about it, dividing it into two, like an exploration inference and a learning phase.

yeah. theoretically during exploration, you could also try to match, but we just leave this out to save computational time and just collect a lot of observations to be able to build a good graph.

okay. So yeah, this is the supervised learning. So that's what we use at the moment. Most of the time we basically present the object in a bunch of different orientations and we tell the model what is the object ID and pose, and then it can learn a very complete and perfect graph of the object.

Which is, yeah, a bit of an aid that we're using right now, but we do also run some experiments with learning from scratch to test it.

Okay. So now the Monty model on a very abstract level. So we basically have an environment and we have agents with sensors that perceive this environment. And every sensor patch, since it's raw sensory information to a sensor module, the sensor module. Turns this raw sensory information into the common communication protocol. And that is features at a pose. A pose is a 3D location and, rotation. Or these pose features like the point normal and the curvature directions.

this gets sent as input to the learning modules. learning modules have these, lateral connections so they can vote with each other. And they can also output their most likely hypothesis at every step, which is again, features at a pose, and features in this case can be the ID of an entire object model. And then this is, so you can see the input and the output of a learning module is the same format, so you can easily stack them on top of each other, and then those can recognize successively more complex objects. And the third output from learning modules is a suggested motor command, so a suggested location in space, that can be then translated into the action space of the agent and executed in the environment.

Does this make sense? Some things of those are not nailed in stone yet, like the hierarchy aspect. We haven't implemented yet and we're still thinking about it and brainstorming, but this is how it is set up right now.

You're going to talk more about, the motor output, right?

not too much this time. So the motor output is basically. the learning module suggests a location relative to the body and then the motor system can translate that into this agent's action space and execute it and the learning module, the location that the learning module suggests depends on if it is matching or exploring. So if it is matching, it would want to locate somewhere that, can disambiguate two hypotheses from each other. if it's exploring, it would want to go into areas where the model is not very complete yet. So high uncertainty in the model, basically. Sometimes we'll have to think about, implementing, goal oriented behavior. where the learning model is trying to achieve something. yeah. Yeah, so we haven't really implemented any goal condition policies yet, except for the goal of recognizing the object. it seems like the framework we have would work for that. A goal is basically to get this object into a particular state, or give me a particular condition or something like that. We can do it later. yeah. So yeah, all the components to do that are there. So since the learning module has a 3D model of the object, if it has like a goal pose of the object, for example, it can, yeah, pretty easily output a suggested motor command for that.

We just haven't really needed it yet for what we're trying to achieve. We didn't implement it.

but yeah, so today I'm going to focus on the learning module and the evidence based learning module specifically. Any questions about the general framework before I go to that?

Okay. so at each stage in the, hierarchy, there's an SDR that's being generated for the representing it or when you have poses and features coming from the sensory modules, but I'm assuming the learning modules are doing some kind of an encoding of the features. Right now we're not using SDRs, we're thinking about doing that at some point but right now it is like a explicit. Explicitly X, Y, Z coordinates, and then features out like a list of continuous numbers, basically. At some point you assign an object ID or when you think you've identified it, or maybe of candidate IDs, is that right? Yeah. Yeah. So here at the output level, right now, object ID is just like a label. Either if it was supervised, that's the actual name of the object, or it's just like a number, the sixth object that I learned.

but it's, yeah, we are thinking about turning this into an SDR at some point, but it doesn't right now.

Okay, so just to quickly recap, there are right now three different graph based learning modules that we've implemented. The displacement based model that basically matches, incoming displacement to the edges that are stored in the graph. the learning module that uses features at location, so it basically matches to the nodes that are stored in the graph. And then the evidence based learning module, which also matches to the nodes, but it uses evidence columns for each hypothesis. And so the displacement learning mode, all of them are location invariant, so the object can be anywhere in space, you can still detect it, it doesn't matter where it is. all of them are rotation invariant, however, the node based ones have to explicitly cycle through testing different rotations. this comes naturally with the displacement learning module. And then only the displacement module is scale invariant at the moment, the other two not yet, even though it's not logically impossible to do this.

and then the downside of the displacement module is that we have to sample the displacements that are stored in the graph as edges. We can't sample newer displacements. which we can do with the other two.

we can also not sample any new locations that are not stored in the graph. we can do this with the featured location module, but it decreases performance a bit. With the evidence module, it works quite well. with this one, we can also deal with noise quite reasonably well. And then another advantage of the evidence based module is that we have a most likely hypothesis at every step. So knowing what is the most likely object and pose helps us to, one, communicate this further up the hierarchy at every step, and two, to use the motor policy more efficiently, to test this hypothesis.

So we'll look at this one right now, and that's the, yeah? Could you just explain again the downsides, the new displacement from the new locations? I didn't quite follow that.

Why, does the displacement module can't deal with this? I don't, I'm not sure, I'm not sure what they actually mean by sampling new locations. Is it sampling joint learning, inference? I'm, confused by that. sorry. Yeah, so basically if we learn a graph and we store a discrete, number of nodes and connections between those nodes, and if we use this displacement module, in order to recognize this graph, we have to sample pretty much exactly the same edges that we've stored in the graph. I remember that from before, I just didn't connect that with the world here. Okay, thank you, that's sufficient. Cool, nice.

Okay, so this is the general structure of the learning module. We have the pose and the feature coming into the module. It has a memory of graphs that it has learned before. this memory is used to initialize its hypotheses. And then we use the displacement between the current pose and the previous pose and the current features to update the hypotheses. So each hypothesis has a evidence count assigned to it, which I try to visualize here with colors. So some of those locations on the objects are more or less likely and also rotations of the objects and then with an incoming displacement and feature we update these hypotheses. Then we can send out, votes and we can also receive votes and use the votes to update the evidence counts again. And you can see each learning module has three types of outputs. One is the vote. One is the current most likely hypothesis. So what's the most likely object and its pose? and one is an action suggestion.

A quick question when, could you remind me again, you might have already mentioned this, but what reference frame is the pose in?

Is it relative to the body or to the, it's relative to the model and to the body in the sense. So basically we have a location in the model reference frame, and together with the location of the sensor, we can transform it into a location in the world.

And then the rotation of the object is relative to the learned model. okay.

how is the best pose and feature different than the output of a vote?

Votes output all the possible, all possible locations and rotations. So all possible poses. And then this one is just one pose, the most likely pose.

And why do you have them separate? Do you need one on the right for action policy?

so the most likely pose and idea is this is the same format as the input, so this can be sent to the next higher level up learning module, but the votes they go laterally to other learning modules. And they are used to update the hypotheses. so it's a list of possible poses. here we just have one. So here, this pose, we can calculate a displacement to the previous one, same as we do here in the beginning.

but here we have a list that gets directly sent to the hypotheses, and it's a different type of, different way of updating the hypotheses. In my mind, I've always thought of the, And they vote the collection of possibilities of what's being passed off. But you're saying there's a set of possible objects, which is what's voted on, and then we only pass off the most likely, correct? Yeah, since we haven't really implemented hierarchy yet, this is not set in stone. We could also pass all the possible poses to the next learning module. But I was trying to have the output the same format as the input, so it's easily compatible. But, yeah, it's, right now this is just used for, logging, basically, for seeing if we detect the object. But, yeah. With an SDR, you could have, you could represent a union of these things. We could, yeah. Then you wouldn't have the representation problem. But it's still an interesting idea. it's still a very interesting idea that you don't pass up the union. You only pass up the one. Yeah, I think with a union it would be a bit more difficult to calculate the displacement to the previous pose.

Yeah, you start getting a little bit of a combinatorial explosion there. Yeah, that's why this is a very interesting idea that never occurred to me. This will of course require two different sets of neurons, one set representing the union, one set representing the hypothesis, which is quite believable, I just never thought of it, and it's worth thinking about some more.

yeah. And yeah, like I said, it's not do this right now because we're not, using hierarchy right now. And, yeah, actually Niels and I were thinking maybe during the, by the Bay Week, we can do some more brainstorming and, narrow down the hierarchy and all of this some more, and yeah, decide on these things. But yeah, that's just how I've been thinking about it so far. Yeah.

Yeah, and I mean we, we might pass up both to the next to level, there could be two channels passed up.

yeah. The most likely one could be through the thalamus, potentially, the layer five path and the other one, the voting one could be potentially the layer two, three, up to the layer four pathway. Yeah. Or the, or There are distinctions maybe between layer 2 cells layer three cells and subdivisions of them. And, I think that's more likely what occur. I think the five pathways, there's still gonna be a motion pathway.

anyway, it's very interesting. It's interesting to think about a new idea that I don't think I've mentioned before. Yeah. The main reason I did it like this is, because yeah, that way the learning module to keep this modularity of the learning module so we can. have the exact same structure and we can easily just stack them on top of each other because the input and output formats is exactly the same. And then the inner workings can also always be the same no matter where in the hierarchy it is.

okay, are there any more questions for now?

Okay.

yeah, real quick, what is a graph? How do we represent the models inside the learning module? So we have the 3D object that we explore with the sensor, and then we turn that into a graph where nodes in the graph represent 3D locations in space, and edges represent the displacements between these locations. And then nodes can also store features at these locations, so for example color and curvature. And then additionally we define the morphology with point normals and curvature directions. These help us with, the rotation of it. show later why. Point normals basically, they, like you see here, point out of the surface and then curvature directions point towards the minimum and maximum curvature. And in the evidence learning module, we don't make use of the edges of the graph. So theoretically, we could also just store a point cloud.

Okay, now to how the matching process works. This, might be a bit complicated. I hope this example helps. So in this example, we have a cylinder, which is mostly red, but has this little blue corner, and we start. collecting one sensation down here. We haven't moved yet, so we just collected the first observation, and we use this to initialize our hypothesis space. So this is the model of the cylinder, each point is a node in the graph, and then at each location in the graph, we stored the point normal and curvature direction, and then we sensed a location in space that doesn't help us any yet, but we also sense the point normal and a curvature direction.

since the object could be anywhere in space, the location doesn't help us. We could be anywhere on the object, but we can use the blue and the red line already to narrow down the rotations of the object.

if I would be on this specific location, the object can only be in two different rotations. The cylinder can only be standing upright or upside down. If I would be on the edge of the cylinder can only, given the point normal and curvature direction, there are only two options of how the cylinder might be rotated. And these are different for every location on the cylinder. And in some cases, like in the, on the flat top of the cylinder, we actually have to sample more than two possible rotations, because the curvature direction is meaningless. So if we're on a completely flat surface, we don't have curvature directions. So here we just sample a couple of possible rotations perpendicular to the point normal. And we do this for all objects. Here you can see it again, this is just colored by the rotation. I'm sorry, can you, on the rotation, are we dealing with just two three dimensional rotations or three dimensional rotations? three dimensional rotations. Okay, so basically any change in the orientation of the object today.

Sorry, what was the last part? It's good, three dimensional, I got it, Okay. On the object, what is the gray? What do the gray up arrow, gray arrows mean exactly? Is that, you said that's the two, oh, that's the two rotations of the object. it could be upright or upside down, right? Yeah, exactly. Okay. And then at the edge? For example, okay, so for example, here on this point here, where's my pointer, in the model, we have this point normal that points out like that. So if we align the observation with this, stored point normal in curvature direction, it means the cylinder would have to be tilted like this or like that, like it would have to be tilted for the 45 degree or minus 45 degrees. But wouldn't we have a, that's just looking at the pose, but wouldn't we normally have a feature also that would be, the feature would be very different there and we actually, we could not be there at all. Yeah, I'll get to that in a moment. This is just the pose hypothesis.

we're not looking at the feature in this step yet. yeah, so this is just possible locations and rotations initializing. And then, yeah, as you can see here, like the top of the cylinder would have exactly 180 degree rotated pose of all the points on the bottom of the cylinder. And then the sides would be 90 degrees rotated from that. along one axis. And now this is the second part what you mentioned. Now we also observe a feature. So our model also stores the color and curvature at these locations. And we observe with the finger up here, we observe a red color and a curved, convex curvature.

so now we can actually update the evidence for these poses and locations. So at points where all the features match, we have the highest evidence. So here where the cylinder is red and curved in this amount of curvature, in places where only one of the two matches, we have medium high evidence. So for example, here we have a red feature, but the curvature is different because we're on the edge or here the curvature matches, but the color doesn't match. And then wherever we neither, none of the features match, We have zero evidence. Does that make sense?

Why do you say the color doesn't, oh, because the object has a blues? patch. Yeah, exactly. So all the points here in the model store blue as a feature layer.

Now in a real vision case, the color would be, could be quite different, even though the actual color is the same and the perceived color could be quite different. So we're not addressing that here. Yeah. So for example, we usually look at the hue. So the hue isn't that much influenced by like brightness and stuff like that.

But yeah, we don't really account for that at the moment. But also the evidence value is not like a one or zero value. Like I like it seems like in this example, but it's like a continuous value. So what's the distance between the perceived observation and the one stored in the model? So if the red is just a little off, it's like a still quite high evidence, but if it's like a completely different hue, like blue, then it's low evidence. And then another thing is Features can only add evidence. They cannot subtract evidence. So even if the features do not match at all, it's still zero evidence. We don't subtract evidence like we will with the morphology. So that way we hopefully should still be able to recognize the morphology of the object, even if the features don't match. That's an important consideration. So I think it's great. I, just remind me a basic thing, the model, these are discrete nodes in the model, and then we are able to interpolate between them later, or do we have to sample the discrete nodes again? I'm sorry for that basic question. Yeah, so these are the discrete nodes, yes, and that means we will initialize a discrete number of hypotheses.

every following step will happen in continuous space. But we still initialize a discrete number of hypotheses, and that is defined by how many points are stored in the model. Just as the model itself is a set of discrete points, and then when I'm doing, when I'm sampling during inference, I'm not assuming that I'm on those discrete points, or am I assuming I'm on a discrete point?

yeah, you're assuming it, but if you're not on one of these discrete points, we have slack variables in the system so you can deal with starting somewhere that's not that point so even if i start like somewhere over here and then i move up the next observation will still be similar and close by enough to the other point to the next point that I should still be able to recognize the object. So from experiments, it seems like that isn't a big issue. I don't know how it, so our models are much more dense than the one I show in this example. So if we would have a model that's this sparse with so few points in it, that might have an effect on it, but with our models, it's not a big deal.

Is your evidence stored only on the nodes? Do you have any other evidence other than displacement stored on the edges?

yeah. So evidence is for the hypothesis, and that will move around the space. I'll show in a moment. it's not gonna be stored in the graph at any point. oh, you're not storing like color or, other in information at the nodes that's part of that. Oh, yeah. Features are stored at the nodes, yes. Okay. But you don't store like a transition, like when you're going from one node to another node. Either the color changes or the curvature changes in a specific way. No, it's only like the way we built the models if between, if in a small space features change a lot, we will store more points in the model. If features change a little, we don't store many points in that area of the model.

okay.

Any more questions before I go to the evidence update?

Okay. So now we move the finger. So we move from down here, up on the model up here. Again, we sense red and curved. so we get on displacement that is a certain length and direction. We have all these hypotheses that I showed before, and then we basically calculate search locations by taking each of these location and rotation hypotheses and We start at the possible location, we rotate the displacement by the hypothesis pose, and then wherever this rotated displacement ends up is our new search location. So for example, up here, we, let's see, yeah, let's take this point here. We start at this location, we rotate the displacement by this possible pose, and then the search location would end up over here, and then the other arrow would end up over here. Same with all of these arrows here, the search locations end up in a circle around this point. Down here, they end up like down there, and then inside the curve. It's a bit difficult to draw this in 3D, but yeah, that's the procedure. So we have one search location for each hypothesis now, and now I'm just going to take a short detour into a few details, just one hypothesis now. So again, in this example, we would have a sideways displacement, just to make it a bit more clear. We have this location and the pose hypotheses are either the cylinders upright or upside down.

We rotate the displacement and apply it to this location. So we get this search location, this one. Sorry?

Oh, okay. So we would get these two search locations. If we now look at this one, for example, this is zoomed in there now. We draw a search radius around this search location, look at the nearest neighbors in this radius.

For each of the nearest neighbors, we then calculate the evidence. So we take the observed features there, both the curvature direction. and pose independent features, so color and curvature, in this case. And we calculate the evidence, that is for the pose features, it's the angle between them. for the other features, it's just the real value difference. So how different are the colors and curvatures? And we weigh this by the distance to the search location. So if the point is far away from it, it's, less evidence. If it's really close, it's more evidence. Why is that?

just to Yeah, make sure we stay on the model surface and if it's like an, if it is a point right where we're looking, then it's more certain that this is the one we want to look at than if it is one that's like far on the outside of the search radius. So you're saying you might be on a different object?

Yeah, you might be off the object or your pose hypothesis might be wrong, like you might just have rotate the displacement in a way that's going off the object, for example. But if you went off the object, you wouldn't have an index observation. No, we didn't really go off the object, but if our hypothesis of the object rotation is wrong, then if we test this displacement with the wrong hypothesis, it will end up outside of the model, in the model space.

All right, so you're relying on the fact that if you move it just a little bit, then even if your orientation is wrong, you'll have some useful information. Yeah, it works also with large displacements. It's just generally, most of the hypotheses will be wrong. So most of the search locations will end up somewhere outside of where the object actually is. in the model space. So we don't want to consider these hypotheses anymore, or we want to at least decrease their evidence. Yeah, earlier you had mentioned that in places where things are pretty constant you store fewer points in the model. Yeah. And it seems like there you'll just, it's much more likely to just have lower weights because, just because there's The density of stored points is fewer, right? But actually it's not really, that's, it should be more dependent. I don't know. I don't know how you take that into account. Yeah. Yeah.

it might be that this weight factor doesn't actually help a lot with performance. I would have to test that. I just thought intuitively we would want to weigh that, but you're right. If we have a lot of noise, for example, or we don't store a lot of points in that area, it might just be a hindrance. So yeah, maybe I'll do an experiment without this weight factor.

Or maybe different parts of the object could have different weighting factors if you have to put it in. Yeah. But that's more complexity. Yeah, definitely. but yeah, that's a good point. Maybe I'll just try that. Maybe it's not even really necessary to weigh that, weigh this.

yeah, anyways, so We basically calculate the, how well each of the points in the radius matches the observed features, and then we take the best match to update the evidence for the hypothesis.

So even if, for example, one of these doesn't match well, like if we're here up on the corner, we do have a red color, but everything else doesn't match, the point normal doesn't match, the curvature doesn't match. so that one would have a pretty low evidence, but that doesn't matter. It's just a another point in the model. As long as there's one good match in this radius, we get a high evidence update.

and evidence update basically means we add this evidence to the accumulated evidence so far.

and then as another detail, the search radius. is not exactly circular, but it's a bit, it's spherical influenced by the point normal. So basically it's the notion that we want to search far along the surface of the object, but not far out of the surface of the object or inside the object. So basically in this example, the point normal would be pointing upwards. Search location is in the center of this. And then color represents distance, so points that go in the direction of the point normal or down into the object, there the distance increases much faster than if we go along the surface of the object in any of these directions. And that's, I'm sorry, earlier we were talking about the idea that, you have an action policy which just follows the surface, and it sounds like you're not doing that. I hear you're doing an action which is independent of following the surface, it could be off the object at some point, but wouldn't you normally be following the surface? In which case you wouldn't. I guess I'm trying to understand why is that? It wouldn't be on the object. No, so these are not actions, these are just, we sense the displacement. There was an action that was performed in the environment, but then we add this displacement to all of our hypotheses, so for every hypothesis that we have. Where did the displacement come from? This is what we, this is the action that we performed. so why wouldn't the action be constrained by the object versus just, not constrained by that?

we could only take actions which keep us on the object, like the fingers following one another. Yeah, that is the constraint. But then your points will all be on the object. You were just describing how the points could be off the object or in the object. I don't understand how that could be. No, this is just for the search inside the model space. So we have the displacement applied to the, applied inside the model space. This is not an actual movement. We look up here, we look down here. We do this for all the hypothesis in here. And then we want to search here on the model hypothesis. We don't move here. It's just looking at all the points on the surface in this area, basically.

That's a, that's not an observation in feature space and in object space. That's a prediction made by the model. Yeah. Yeah. It's a simple example. I mentioned the following, let's say this is the object we have in memory and we also have serving. Let's say I'm touching it here, but I'm actually thinking one of my hypothesis is I'm touching it over here. So now I make a displacement going from here to here. If my hypothesis was I originally touched here, then I should have expect something over there. But now we're outside the object. And then, so this is the one we're considering here, this just being outside the object because my hypothesis was wrong. I was at the wrong location. I should have thought I was here. If I think of it like, like you're making a prediction, like you're doing like counterfactuals, what, if I moved in this direction, what would I expect to see? And if that's not consistent with my current evidence, then I'm going to discard that as a counterfactual that's not likely to be happening. Is that right? Yeah. Yeah. Yeah. Let me go to the slide. Actually that. Maybe it visualizes it better, maybe I should have put that one first. So basically, yeah, so for example, if we test the hypothesis that we were down here on the bottom of the cup, and we sense this displacement, we would say, if I would have been here in this hypothesis rotation, then now I should be here. But then if we do a search radius around here, there are no nearby points in the model. So it's, we want to decrease the evidence. for this hypothesis, because we're not on the model surface anymore, if we would follow this hypothesis. Is the displacement an actual movement of the sensor in our hypothesis space? We have one displacement with the sensor. This is the actual displacement that we sensed, but then we test all our hypotheses given that displacement. I think it's like a trial, the trial displacement, it's not an actual displacement. It's a, like a mental manipulation where you're, I don't understand.

If it's not an actual displacement, then I have no data on which to say, oh, it's off the object or it's not right. How can I understand if it's just imagining?

No, there is an actual, where it says displacement, that's an actual movement. Yeah, but that's what they're saying. But then I was told that it wasn't. no, that is the actual. So there is one movement, which is that displacement. But depending on your hypothesis, that one movement can lead to very different predictions. And so that's what's being checked for every point for every possible, pose hypothesis. You try that movement out.

So make it out of predictions. In that case, back to my earlier point, which I'm saying, like, why would I try a movement that takes me off the object? I just require my action policy to keep me on the object. I never test. I have a displacement that takes me off the object. No, we stay on the object so our movement in this example is we were down here with how my laser pointer up on the top right corner. So we were down here, we moved the finger up here. That is an actual movement in the world. We stayed on the object surface. So this is the displacement we're sensing from year up here. And then we take that actual sense movement along the surface of the object. And we compare it to all the hypotheses we would have. And with a lot of the hypotheses, this is not going to match. I think, I'm sorry, it's pretty basic and that's it. Thank you. Yeah, but Vivian, it's also, but isn't it the case that you can't guarantee that the movement is going to keep you on the object because you don't know where on the object you are. You just have a whole bunch of hypotheses. You can take your best guess, but you don't, you can't guarantee that it's going to keep you on the object, can you?

So in the real world, we're going to stay on the object. The policy makes sure that we stay on the object for all the actual movements of the agent. But then for testing hypotheses, if I have a wrong hypothesis, this actual displacement that was actually on the surface is going to end up somewhere out here in model space. out here in model space, not on the model surface anymore. Yeah, no, I understand that piece of it. I'm just wondering how can you guarantee that the original movement is going to keep you on the object if you don't know where on the object you are? that could be a low level action policy, just as, keeping contact. Your finger just never comes off the surface when you're touching an object. It just, it tracks on the server. I don't know how it does it. Yeah, so for example, in, in the implementation side, we have some simple heuristics. And for example, if we do go off the object, We don't send those observations to the learning module. We just perform some corrective movements to go back on the surface. And then when we're there, we send it back to the learning module. Okay. Okay. Got it. I have a question. And how do you know that you're correctly translating this displacement from the real world objects to your graph? is this the reason why we're not, invariant to scale now?

I hope I understand the question. So we basically learned from a smaller, cup of coffee and now you're traveling through a bigger one. So the displacement, It's going to be different. Yeah. yeah. So that's not going to work if the cup now has a different size, we wouldn't recognize it only if it's a very similar size. but yeah. So the length of the displacement is fixed. And we're not going to test like different lengths of it.

I'm going to get to why I was, thinking before, if you'd also stored like the rate that the properties are changing along the edges and you can basically, you can decide, of all the nodes that you're at, it's which directions can you go in that graph that correspond to the actual change that you saw when you made your displacement, right? there's a matching. Delta in the properties that will correspond to the transition from one node to another. Yeah. Yeah. That could be an idea. Yeah. Yeah. So scale here still, an open issue, how to best do it. Yeah. We could do the change in features. Yeah. Yeah. So in a lot of places, features don't really change much. except for the point normals. But, yeah, one option, for example, is to re anchor the hypotheses and then use that to re scale how we scale the displacements. But yeah, we haven't really figured it out. Another option could just be to learn models of different scales, separate models.

And then if the scale is similar, it would still work because we have some slack, but yeah, we would have to store separate models. not necessarily, because what you do is if you're moving along the evidence that your observation says that's property changed by this much, right? Then you could basically do a rescaling in the model saying, okay, if I were to go in this direction along the, the graph, then, and I'm expecting a change this much in that direction, I could actually, I could predict how far along the graph I'm actually moving. I may not be going one node. I may be going like two or three nodes along because I have more of a change in the feature than I was expecting just going by one node. You can do some sort of auto alimentation or something like that.

Yeah, I'd have to think about that one. I guess the question would be if it also works if we don't move continuously, but if we do, saccades from one location to another, for example, that would also work with it? it's interesting, with physical objects, I'm not sure the scaling issue actually comes up. If you're always in real, coordinates, if it's a larger coffee cup, it's actually a different coffee cup. It's not the same coffee cup anymore, but it's more to the generalization that gets more to the generalization question of how can we recognize similar morphologies. or something like that. But if it's the exact same, the coffee cup never changes in size, right? with the perception on our sensor may be different, but as long as we're translating back into, real body coordinates or real world coordinates, you're never actually going to see. a scaling issue. you argued it's a generalization issue. I think I understand that, but on the other hand, if I do see a miniature coffee cup, then I can make predictions based on my previous model of a larger coffee cup. So somehow I'm able to bring in a model that was learned at one scale to help me generalize. Yes, yeah, I think it's more of a generalization question rather than just recognizing the same object. Yeah, we could, you're still able to use a model that was learned at one scale to inform a new model on a different scale. So they're not completely independent. Yeah, no, they're not. Yeah, exactly. And we have this goal of trying to recognize morphologically similar objects and that when we think about it that way, it's not just scale, it could be other, small, distortions, the, the top could be a little bit narrower, the bottom could be wider. It could be curved a little bit. There's all these other things that define similar quote unquote similarity, and scale is just one of those things. You know what, maybe it's obvious. It's not obvious to me. In vision, of course, we see things at different scales all the time, different distances. And, That's automatically, we never get confused by that. it was a small coffee shop. The coffee shop is just further away. I've never really quite understood that. So I'm just throwing that out here. That seems like it's going to be part of the solution that we have to think about. That scale invariance can be actually the actual size of the object. It's just your view behind scale, at least when it comes to the vision. for like distance, for example, we don't have that issue with the system because just from the depth image, it will just convert it to different X, Y, Z coordinates, but the scale in like 3D space will still be the same. But yeah, exactly. Actually, what I was thinking is we could also solve the scale issue with hierarchy. For example, if we learn models of multiple size cylinders, separate models, but then we encode them with similar SDRs. And then a higher level model can learn that a cylinder plus a handle is a coffee cup. It can get the SDR for small cylinder or larger cylinder. And both of them could be, if it gets that plus the SDR for a handle, it could recognize small and big cups, for example.

The problem with my argument is that after the holidays, I think my dimensions have definitely changed.

too much. My, my fail have increased okay. Should I go on with this for now? Yeah, Let's go. Okay, so now let's move again. So now we move to the right, into the blue part of the model. So we got a different displacement that points to the, right now our previous. where our previous hypothesis ended up. So at the end of all of these rotated displacements, this is now where we start. So we start at the end of the previous displacement for every hypothesis. And again, we do the same thing. We apply the displacement to, the rotation of all the hypotheses to the sense displacement. We again, get new search locations in the model space.

We compare the points in the model space that are around the search locations to the sensed rotations and features, and we update the evidence. And now we can already see that we have a most likely hypothesis, which now is this having moved from here to here, because only this sequence of starting here, moving upwards here, and then moving right here, matches with all the features that we have sensed, namely red, blue, and also the curvature staying constant, the convex. And then a lot of the other hypotheses, like all of the ones that are completely off the object already, are very, have very low evidence counts. And then some, like the ones that kind of go around the object, are still likely. It might have been that we just sensed one blue observation due to noise or whatever, so we might still be here, or the cylinder might be upside down, we might have moved down, and then here, But yeah, we basically just add the new evidence from this displacement and features to the existing hypothesis evidence counts. Does that make sense?

And you're exhaustively testing all the nodes in the graph as hypotheses for all of them. You're not like discarding like the ones that are like completely implausible. Not right now. So since we can do this all with a single matrix multiplication, we just do it for all hypotheses every time. But There is an option in the code to, say, we only want to test the n most likely ones, so only the ones that have a positive evidence count or stuff like that. And we might actually do that and try to use, some sparse matrix multiplications or something then. But, yeah, for now it's fast enough to just update all of them every time.

In this matrix, is it, is, what is, what are the rows and columns?

I mean, it seems like you get the reason I'm asking. It seems like it's not really a sparse matrix. You might be just eliminating rows and columns as you eliminate hypotheses. Or are you actually eliminating individual cells within the matrix? Because if you're eliminating full rows, it's a much easier optimization. You don't need to worry about sparse matrix optimizations.

Yeah.

let me get back on to you on that. I don't have it on, up off the top of my head what the dimensions are right now. Okay. That's all right. Yeah, figure. Yeah, I, my, my guess is you're eliminating rose or columns and in that case it's much easier to optimize that, right? Yeah.

Yeah, so basically this procedure gets repeated at every step after every movement. We look at the displacement, we apply the hypothesized rotations to it, and we check the sensed features with the features stored in the model and update the evidence.

Yeah, and this is how it looks in reality. So here we see the displacements that we sensed up here. This is the viewfinder, this is the sensor patch, so what we actually use. And then these are three, four models in memory. and how the evidence gets updated. And for example, for the dice, which is a small object, all the hypotheses are already far off the model and very unlikely. Also, after moving down the mug and then on the bottom, we also already made almost all hypotheses on the bowl and on the banana very unlikely. and then on the mug, since it's pretty symmetrical, we have a red ring of likely places, that kind of moves up, up, up the cup as we are moving up the cup.

and we have a most likely hypothesis at every step. Which right now is already the correct hypothesis. I'm just letting it run for a while to have this visualization.

and yeah, we just move and after every movement, we update these, locations and evidence counts until we recognize the object. And then another example is the dice. So we move on the dice. here's the sensor patch. That's what the sensor module sees. The model never sees this wide view that is here, it's just for visualization. And then already after a few moves, the large objects are pretty much excluded. And then the dice is quite symmetrical, so we have a lot of, positive evidence on each side of the dice. But we also already have a pretty stable, most likely hypothesis after a few steps. And you can see it's not the correct one actually, the target is 90, 0, 180, but the most likely one is, minus 90, 0, 0. And if we look how this one would look like, it's, yeah, over here, it's, it basically looks the same, it's, and, in this episode, the learning module actually detects symmetry, so it, reach the terminal condition, by detecting symmetry between those two poses. So then it, it says actually this one is the most likely one, but, yeah, notice this, it is a symmetrical object.

Okay. Does that make sense before I go to multiple patches? Yeah, can we, is there a notion of, uncertainty that if you have a couple of hypotheses that are really high and everything else is low, then it can be a little more certain, but if everything is close to equal, then the most likely hypothesis is not going to be very likely, right?

Yeah, We do use so we have a list of what is all still possible and I'm going to get to that a bit later. So in this case it would still consider we still have a lot of possible locations on this dice.

it doesn't only look at the most likely one. it's, we still look at all the ones that are, that have the highest evidence counts, and that kind of gives the notion of uncertainty in a way. Does that make sense?

yeah, I was just thinking more in a practical scenario, you have to make some decision, so you'll want to use the most likely hypothesis, but there may be cases where you're not very confident, other cases where you are more confident. Yeah, so you could just look at how many other possible hypotheses do we have. Yeah, and then if that's a lot, then it might be less confidence in the most likely hypothesis.

That's what we use as the terminal condition, that we need to have very few possible, only one possible hypothesis or very few similar ones to say, to actually classify the object and end the matching procedure.

yeah, oh yeah, let me move on for the company meeting starts. So yeah, this is a multiple sensor patches now. here we have one agent, and it has five patches attached to it, and the patches can have different locations, they can have different zoom values, so they can be larger or smaller, and they can have different resolutions, for example. And, yeah, they all move together. So the agent moves, all those sensor patches move together and sync, even though where they are in 3d space can vary radically since yeah, some can be like here on the rim and then others can be on the inside of the cup, which in 3d space can be far apart from each other.

And then, yeah, each sensor patch sends its raw observations to the sensor module, which again turns it into features and a pose that gets sent to the learning modules and learning modules output a most likely object and pose at every step and then have lateral voting connections between each other. And those voting connections can speed up object recognition. So how does the voting work?

in this example we have two sensors. One senses the rim of the cup here and the other one senses the handle of the cup. They send their features and poses to the learning modules and then have their first evidence counts. So now this one would think it's most likely that we are either on the top of the cup or on the bottom of the cup, given the sensed color and curvature. And then this one would say it's most likely that we're either on the top or on the bottom corner of the handle, given the color and curvature that we're sensing here. And now we're going to do an example of the first learning module, sending a vote to the second one.

So in order to do that, we first have to transform all the hypotheses from this learning module space into this learning module space, and for that we calculate the sensor displacement between this finger and this finger. So what's the displacement between this sensed pose and this sensed pose? And then that displacement gets applied to all of these points. and we have them in this model space. That basically says, for every hypothesis here, if we say, okay, it says, if I would be here, then given our sensor displacement, your sensor should be over here. And then it sends a vote for this location. And it does that for all of its hypothesis. And for most of them, it will be far off the model in this case. So if it says, if I would start here, then your sensor should be over here. It would expect a handle on the other side of the cup, for example. Does that make sense?

I'm a bit confused. What is actually being sent? The possible poses are being sent and the evidence. Bottom arrow. Where is the sensor displacement?

So we take all the possible poses and their evidence and we transform the poses by the sensor displacement. And who had, but the learning module on the left does not know the sensor displacement of the learning module on the right, or where is the sensor displacement, in this case it's fixed right this is like to pass it on the retina so they're fixed right displacement into the sensor. It's not. No, it's not fixed because We are looking at locations in space, so one patch might be like offset like we saw in this example here, one might be inside the cup, So you're, oh maybe you're just saying that we know, you're just giving, we assume we know the displacement between the sensors, is that? Yeah, the system knows, it's clear the system knows, but I'm wondering where is that in the learning module, Yeah, the learning modules don't know this, but the Monty model knows this. So Monty basically handles the vote, like Monty handles all the communication and it handles transforming the vote from here to here and Monty just calculates the sensor displacement from the difference between this pose and this pose. Okay. Yeah. I think that was my confusion. in the brain, it would be within the cortical column somewhere. It wouldn't be some, it wouldn't be some external system, it's interesting question about that. It's not clear. there's all kinds of evidence that there's, there might be certain intermediaries determining this stuff, but I don't know if we have to talk about it here. No, because if you have 100, 000 cortical columns, there's 100, 000 squared relative displacements. Yeah, but actually It's a huge number, but Right, but Actually, let me correct this. in the implementation, actually, this learning module, when it sends the vote, it also sends its pose. So it sends all of those Plus this, and then the receiving learning module can use, the pose of that one. It communicates to it to calculate the displacement. okay. Yeah, that makes sense. That makes sense. Yeah. Yeah. That, that would work. I'm sorry. That sense? Okay. Yeah. Sorry about that.

yeah, and so the receiving learning module is the one that then transforms all of the other cortical other learning modules, hypotheses into its own, reference in own models. Reference is not the right word, but yeah. Its own model, reference frame. Yeah. Yeah. Okay.

Yeah. Sorry. Sometimes I also forget these details.

yeah. And yeah, so this displacement is also different if it gets a vote now from a different learning module, for example. Then it has to apply a different transform to those votes to get them in the same space.

yeah, and then we have all these votes here in the models reference frame and we can use them to update the hypotheses. So basically to do that, we go through all these points that are stored in the receiving learning modules model. And we again use a search radius same way as before. We look at all the points in the search radius and then take the weighted average of them. To add evidence here. And we scaled this to range of minus one to one.

and yeah, just add that to the evidence count. And then now after this voting update, in this case, we would have the highest evidence for the top panel because only that is getting high evidence from the voting connections and had high evidence before.

Does that make sense?

Yeah.

Okay, cool. yeah. Okay. Almost done. So now, most likely hypothesis and possible matches. We already mentioned that before.

So basically we have the evidence for all the possible poses of an object, and then from this, if we take the maximum out of that is the object's evidence, and the maximum of that would be the most likely object, in this case the mug, and then the maximum of the poses within that would be the most likely pose. And then to get the possible matches we can threshold this, So we say everything, so we look at the maximum evidence here, and then everything that's the maximum evidence minus x percent of it, for example, minus 20 percent of it, is considered possible. So everything that is very similar to the highest evidence count is also considered possible. In this case, for example, we would say the mug and this larger cylinder are still possible. And then also for the poles, we can apply the same thresholding, saying everything 20 percent below the maximum is possible. And as you might see here, orange is, so both of these are orange, but in the mug, all of these points are not considered possible anymore because They're too far off from the maximum evidence count, so here we would only consider these. I don't know how well you can see the colors, those are darker red, dark red, and those are orange. While in the cylinders, all the orange points are all considered possible because they're all about equally likely. And then if there are no positive evidence values on an object, it is generally not considered possible.

does that make sense? Oh, yeah. Yeah, and this threshold is that you said is a dynamic threshold. It's based on the max evidence. Yes, based on the maximum evidence. And then we, it's like a parameter where you sent the percentage below the maximum that is allowed. So the larger you set this percentage value, the more certain you need to be an object to reach a terminal condition. Basically to say there's only one possible object because it's so much significantly more likely than all the other objects.

Seems like there are a few things there, but it seems like a bunch of choices there which potentially could investigate a little bit more. yeah, definitely. and there's also, yeah, so there are two versions. One is where the evidence value can grow infinitely, and then there's another option where we can, we have the evidence bounded by weighing the past compared to the previous evidence. Okay. compared to the current evidence. And if the evidence is bounded in a certain range, we can also just use a fixed value threshold, for example, instead of having to, make it dependent on the maximum. Yeah. Or maybe use a softmax or something. Yeah.

Okay. Lastly, real quick, we also need to define a terminal state. So when do we end an episode?

and we have three different terminal states. One is timeout. We took too many steps. one is no match and one is match. So if. We have no possible matches anymore, so all of the evidence counts are negative, we say no match, and we create a new graph in memory and learn a new object. If we do have possible matches, and we look at all of the learning modules, so this check is now done for all learning modules in the system, we check if the number of possible matches is one, so do we only have one possible object? If no, we set pose to none, I'll get back to that later. If we do have only one possible object, We look at if the evidence for this object is above the minimum required evidence. If yes, we look at all the possible poses within that object, and we see how many possible poses do we have within that object.

if all of the possible poses have a very similar location, we say the location is unique. If all the possible poses have a similar rotation, we say that the rotation is unique. And we can also, we also have a check for symmetry. I'm not going to go into too much detail right now. but basically we say we determined the pose of either location and rotation is unique, or if we detected symmetry. And then we set the pose to the most likely hypothesis pose, otherwise again pose is none.

and then after we did this for all learning modules, we concatenate all the predicted poses for all of the learning modules. And if we have actually decided the pose for enough learning modules, this is again a parameter we can set how many learning modules need to be sure of their pose, then we check if all of the learning modules agree on the object and pose. If yes, we say it's a match. If no, we say we take another step and continue matching.

That's the terminal slide. Very nice. Just in time. Thanks. Did you do all the lettering yourself. Yeah.

That's amazing. Really great. Yeah, I tried to try something new. I liked it. It's good. Great. Thank you.

Yeah. And there's a lot in there. And, I know we talked about this stuff before, but, going through the holidays, I forget things. anyway, there's a lot of big ideas. I think that was a very clear explanation. Have you run simulations of this and the performance and how slow it is and all that kind of stuff. Thank you. Yeah. Yeah. yeah, that's, basically the results that I showed before the Christmas break. So this is all just a review of the stuff you showed before the break. Yeah. Yeah. It's not. No. Okay. I don't know. It looks good because you have new diagrams or maybe new language. Yeah. Yeah. I just couldn't remember if all this was done before or not. and you're getting reasonably good performance speed up if I recall. Is that right? Yeah. Yeah. Yeah. So detecting one object takes One second or so, usually, depending on how many models we have in memory. Yeah. Nice presentation. Yeah, really nice. Yeah, thanks and thanks for all your questions. Yeah, I'm always trying to figure out how to explain these things best.