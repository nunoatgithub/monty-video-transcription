Okay. so these are, papers that Heco originally posted to the Slack. it posted the neural descriptor fields one, and it uses something called vector neurons. That's a component thing. and then we talked about this in some detail on Friday at the research meeting, and I.

it's a, it's fairly relevant, at least to the problems that we're tackling. And, at the end there's a question of do we wanna look into these in more detail or not? I think I would go through it in, obviously, in a much higher level of detail than we did on Friday. In the discussion of Friday. It was very helpful for me to understand a little bit better. but I'll go into the reverse order of what we did on Friday. So I'll do the vector neurons first. I have maybe two or three slides on that, and then two or three slides on, on this. Just describe it at a pretty high level. I'm not gonna go into any of the math or the techniques or anything like that.

okay, so vector neurons.

so the title, is, it's a general framework for, so three EVAR networks. I have no idea what, so three EVAR meant I have to look it up. so, three is basically dealing with full 3D rotations. and I'll explain EQU variant in a second. And so what they're trying to do is create neurons that work inherently with 3D data rather than 1D vectors of numbers, which is what traditional neuron network works with. So these are not biological vectors?

No, It's nothing to do with biology whatsoever. Vector. It's very confusing.

so a classical neural network looks like this, where you have a set of numbers. So you have a vector here, going to another vector here. So you do matrix multiplication here. instead what they have are neurons that are vectors, 3D vectors. So at each point here, you have three numbers instead of one number, and then they project to a layer of no vector neurons. We have three numbers again. So these are X, Y, Z initially, and then they can be whatever later. X, y, z numbers coming in. So you have a set of x, y, z points coming in that represent the shape of your object. And then it tries to work inherently with this 3D data. So it's sort of groups of three neurons, you could think of it that way. And what they're doing in this paper is creating a, toolbox for creating all sorts of different deep learning networks that are EQU variant to 3D pose and inva to identity. So I'll explain that in a, second. but the point is they're trying to create a general framework so you can create all of the standard deep learning networks that you typically create. You can now create them with, 3D closed. So that was a pretty important, point when I, so they were focused on this.

so what does this Inva Equian thing mean? So I have a little video here. Let's see if this, So here on the left is a 3D point cloud of, for an airplane. And on the right is, a representation of what you think it is. And basically when, you have in variance, what you want is global object pose should not affect the query result. Okay? as you move the pose of the object, the identity of the object should be the same, and that's what they mean by in variance. So that's fine. The property is also desired in a variety of point cloud processing tasks, including classification, segmentation, and surface reconstruction. Okay? So here they're showing classification. So that's the in variance piece. And the EQU piece, will be shown here. And what they mean is as you rotate the post, as you change the post, there's a corresponding change in the representation. So if you think about a rotation matrix, for example, as you change the 3D angle, it changes in a predictable, non predictable way. And so that's like the pose of the object. No, it could be anything that changes in a predictable way. So, that could be morphological changes or behaviors?

like here is the segmentation of the object, for example. Yeah. into wing body and all that you think about in terms of coloring the airplane space. If you rotate it, your coloring should rotate accordingly in exactly the same way. So yeah, for example, if you want the exact shape of the object, in the scene. If you wanna segment it out, that will change in a very predictable way as you rotate. I guess what, it seems very similar to pose in some sense. Oh, I just said, you said No, It's not just Pose. Pose is an example of that. Pose is an, these are both examples that Pose would qualify for, right? it's not just, no, these are, not pose, they, in my mind they are in some sense they like, they're all tied to the pose, but they're not, yeah, okay. Yeah. If I know the pose, I can generate these. Exactly. Yeah. Okay. So yeah, so the pose would be the underlying thing that is changing and then you could generate these Yeah. These objects, these two examples can be generated from Pose. I have a model of the object and I know it's pose I can generate. Yeah. Okay. I'm not sure if there's other things, like you could, one could argue, could you, if you take a, a more general case, like you said, oh, it could be any behavior or anything like, then I could say the stapler one might argue that. The, open closed stake or, EQU variant, parts of the state because of the behavior, the movement I do in between that. I don't know if you were making that argument. I don't think so. Another example of equ variance or just equ variant to changes in the stake. Yeah. Okay. So did they mean in that way? Did they go that far or they just say No, they're talking about this kind of stuff here. It's just rotation. EQU is a very generic mathematical thing. Yeah. Suppose is an example of something. The rotation is an example of something that's equ variant, but the exact pixels which belong to the wing would also change in a predictable way with 3D rotation, 3D changes. So the, these set of pixels that correspond to the left wing here. Yeah. that change is equ to the, I just try one more time. So one of the ideas I'm working on right now is that, a model of an object. These different poses is really just, it's, a, way of representing that is, it may not be any different than way of representing in its different states. And, so the question is, this trying to deal with all the states of the, they brought out to that regard or all the states of an object like we were just talking about? Or do they, they just talking about these kind of things, which is really based on color in their, the experiments that they've tried. It's all rigid, okay. There's no behaviors, objects. Okay. Alright. they, do have a robot grasping thing, which we can get to later, but that's, here it's just rigid objects, but there's no particular reason. It couldn't be generalized too. Behaviors. Okay.

this notion is very generic.

depending on how they do it, one could be like, oh no, they're just gonna do a pose. Or they could say, no, this is a really basic algorithm that could do anything. It could show how, objects change. they're. It's, one level more fundamental than that because they're creating, showing how to create layers of a network that can be, that can vary smoothly with post and, depending on what sort of network you create, you will then be able to apply it to different things. Yeah. Alright. So it's a, it's a toolbox. Yeah. If you'll, it's not any one thing. It can be used for lots of things where you want, where pose you wanna treat, pose in a kind of a smart way. Yeah. Again, imagine the airplane has, aons on the wing and they go up and down, right? Yeah. so that's a, it's still an airplane and it could still be in the same poles, but it would be in a different state. And the question is, are they, does this technique compass that? It should, yeah. It should. Yeah. I don't see why not. They don't show it in an experiment, but I don't see why it wouldn't. okay. I just wanna make sure, because it sounded like what you were saying is, if I take the airplane gear and I apply transformation or I lift the wing up or tilt the pack of the wing up, so yeah. So it, I don't think it should be equ variant to something like that. That would be like a change in state that you can't describe shipping the wing up as rotation. It's, equ variant to the angle of the el. But that's, a specific part of the object. You're not rotating the whole object. This would be Right. This is not specific to rotating the whole object. Okay. Then I misunderstood. This example is, and the experiments they tried is, but the generic fundamental technique is not specific to that. It's a generically updating a layer that's equ to any angle you have that happens to be part of the representation. Thank you. So how would I, remember the word equ? I think it's important distinction. How would you, what. That in your head when you say equ, I in variant, I had Okay. This is an airplane in to its code in to its scale or something like that. that's a word I'm familiar with. How would you, what does, the term EQU mean? It means, it varies in a predictable way with what your evar to, it's a mathematical definition. I didn't know what the word means. okay. Yeah. It varies equally. Okay. so as if, okay. It's good enough, let's keep going. So a lot of what we've been talking about would fall under EQU variance, as well.

so their basic trick, the fundamental trick they do is each neuron. represents some feature 3D feature that's like a shaped feature. and that's would be normal. the other thing it does is it learns a 3D direction that's specific to that feature. So it learns. How does the three, how do three numbers represent a feature?

or is that It is a feature at some position. What, is that? A neuro represents a feature, and these are the three attributes of the feature. it's whatever the network decides to learn and an input one of those. It's just a 3D point X, Y, z. Yeah. So X, Y, Z comes in, but it's actually getting X, y, z from lots of different points. And then it creates a three dimensional feature vector. So it could say, oh, I'm an, one way to think of it, it could say, I'm an edge at this orientation. Okay, so that could be one, like this could be an edge and this could, I can think of just classic neural networks. But now it's 3D, is that right? Yeah. So like this could be, mini column feature. We've talked about the directions of minicolumns. Yeah. And also of this could be an example of that, and then what the output of that neuron is, how close the input is to that feature in that direction. And every, little vector neuron will learn a different direction. So you'll end up with a layer of these vector neurons, each of which represent, a different direction. We have a smorgasbord of directions and is associated with a smorgasbord of features, and it's gonna learn both of those things.

the three there is arbitrary, right? Because at some point they no longer represent coordinates, so X, Y, Z anymore. yeah. Yeah. I don't know why it has to be three specifically. I guess here it's, they're dealing with point cut, so these directions have to be probably three. But that's, a question that came to my mind too, Y three, you're the, you're talking about the features design? Yeah. it's because I think that's how they check for EQU variance, right? Because they apply some rotation to the input and the option they should also apply to this, and you should, yeah, but they don't actually, that's just in the proof. They don't actually use that anywhere.

That's just to prove that it's equa variant rotation. But if this was four instead of three, it would probably still be equa variant.

And that's, I think it's just in the proof that they were shown.

and then, anyway, they create a layer and they, like car was saying to prove that these things are Arian. there's a bunch of other things like normalization, layers and other things that they have in here, but this is the, fundamental operation that's different. Is it correct to say that classic normal network is basically doing over a two dimensional field and, they do various things with them and then this is just trying to make, they're trying to use the exact same techniques, but I did, I the sense that we're dealing with three dimensional spaces, is that all the ways they use neural networks in the past, but still work except how Yeah, they're inherently three dimensional. That's their claim. That's what they're trying achieve. Yeah. That's an easy way to just. Explain it because even in two dimensional neural networks, it's very hard sometimes to tell what the neurons mean right there. Yeah. Same thing here. Here you wouldn't what they mean. Okay. the, one thing is, you can always use 3D inputs with traditional networks, but you don't get this echo variance properly. and that's the big thing that they're doing here is showing EQU variance.

which is if you wanna do anything with pose or recover pose, we need it to be equ. And I would say our Monty architecture is equate trying equate pose. It falls under the mathematical well, because we're trying to make predictions based on the pose. Yeah. Yeah.

so, if you do this using 2D images and just two neuron, would it be to translation, not rotation? Not translation, but those are to translation? No, just rotation. Doesn't, the way they do translation is they just take the center of mass of everything, shift everything, and then, so the translation piece is hacked. Oh, so everything stands mass. Yeah. Is there a convolutional equivalent, that you could then wouldn't have to do that?

Maybe that would be the way you, traditional No networks aren't, they don't take center mass center, right? they don't, they are sensitive to translation. Traditional, no. The whole convolution layer trying to do is no convolution layer. Very little. They translation equ by design.

It's a little complicated idea. I could show you the picture of the dog in different bartends. Gonna recommend as a dog, right? Yeah, but it, has to be trained translations. Really, it's only, a few pixels translation in, equip. Wow. I didn't know that. No, I'm, not totally convinced by this, the actual operation of convolution. It's like you have the same parameters, you just apply 'em at different places. Yeah. But there are a lot of edge. We can go into this later, but there are a lot of edge effects and things that happen. And so the max pooling, yes, it is to a little bit, but once you get overlapping, things and, their edge, it just isn't that, once you get up the hierarchy, it's not, I think it only works if, each filter in your max pooling has some overlap. And if that overlap is exceeded, then you, lose all track of, translation and variance. Yeah. If you look at like a partic one max pool unit and one set of filter outputs, yes. If you move it a little bit within that receptive field, it's in very, that's what I'm thinking. just a single layer. A single, not a layer, a single unit. Once you get to the lake, it does, it just isn't. and we've done these experiments before, just, you don't actually get translation in variance. Okay. I'm curious to talk about this offline then. Yeah. That was seems like image that works so well. it's like you think you, you train it on 1.2 million images and for each image you have to show lots of distance. Oh, they actually do that. You manage manually translate it. Oh. And you have to manually rotate it manually. I didn't know that. If you don't do that, it doesn't work. So that's a example. You just have to this the same problem in the sense that the Yeah, I mean you'll have to pick the center, the mask to pick the center. Yeah. But it's, I think it's a little bit less of an issue here because they're just dealing with positions to begin with. Everything is position based, not color or anything based. It's all shaped. It a little bit easier. Yeah. and you don't need to segment the object versus the background. That makes it easier. Yeah. You can just subtract this enough mouse. Yeah. Yeah. So they create these things which now vary smoothly with posts. and so you can do a lot of things that very smoothly with post and not, and don't get confused. And then for recognition for the invariant layers, they have layers that essentially learn to undo this, undo the EQU variants and, create like a more stable to think of it. That's like a pooling layer for our case. it, undoes this, I'm not going to go into the details right now, but they have a Inva version and then a Arian version. So they have two, two outputs, which is very similar to what we had in the cones thing. We had a cooling layer and we had the location layer was in. In that case e translation. Yeah. Although those, that was never good because when we perceive something, we do perceive it as some more pose. We don't perceive it as just an airplane or just a dog, And we see the details. We also, yeah. So they, so here you can read out either one. Yeah. Yeah. But they have a separate undo thing. The thing I'm working on right now is solves this, potentially solves this in a different way, whereas the model, the object is, is learned through different poses so that, you are only recognizing a portion of the model at any point in time. And that portion dictates what the pose is. So the model, it's again, you take a 3D like a cup and you display it out in a, line of, or a sheet. Then you're only recognizing a part of it and the part, it's all the cup, but the part you're recognizing tells the pose. It's, a different way of solving the same problem. Yeah.

Now there are lots of downsides to this technique too. Which, but anyway, this is, lemme move on. so here's an example of the type of results to, to do. It's a little bit, you just look at this top section here. these are theirs, these VN is vector neurons. So they apply it to these two traditional networks for those 3D networks. Those are 3D networks. The difference here is they have variance. Are those 3D networks, 3D datasets, 3D networks. The data set is this, what's model net or something? That's 40 model, not 40. it's 40 categories. these are inherently 3D networks, but they don't have natural variance property. Okay. And so what in this pink thing is, the first thing here is. All the rotations that they were trained with. And the second piece is all the rotations they were tested with. So it was trained with rotations about the Z axis. And here it is tested with all 3D rotations. So it's really, it is trying to see how well it generalizes to the other two rotations. And you can see that these other networks, they perform, their performance really gets much lower. Whereas the vector neuron one, the performance is really not affected by these other two rotations. So these other, these first two networks did well and they just went around the Z axis. That's what they were designed for. And then they say a hop, but 'em otherwise, and you fall apart, but we don't, yeah. Yeah. Unless you also train them in all sorts of rotations. Yeah. Which is the last, yeah. If you train it with all sorts, then they get better, but they're still not as good as these guys. But of course, then you have to do a massive amount of training.

So is there, I read the paper, but I don't remember, is there any indication of this method requiring fewer rotations than the other methods? For training? Yeah, for training. Yeah. Absolutely. It requires fewer rotation. Much fewer. That's what this whole middle column, you don't, you only train it on zx to train on the other, the absolute number. Maybe it's just a zx, but you need like a Yeah, I was gonna say I that, I dunno, my expectation is it still requires a lot of training. That's one of the downside. It's still a deep learning technique.

I have no idea what these other ones are. Sense if it's less, but yeah, it's still a lot. Yeah. So I don't know what these other things are. There's a long list of them. I think these. This is like another category of stuff, which I'm not sure. But anyway, this is, they show, they also show reconstruction results or prediction, I think of it as prediction. so here's an example of, the net network that was trained on these airplanes. and if you give it the pose that it was trained on similar poses, but novel instances, it can, reconstruct it just fine, but as soon as you start giving it other rotations, it just falls off a cliff. Those are the old tech old techniques, whereas the orange ones there's works just fine. It's not affected by the rotations. You can see a little bit here.

Generally they can do prediction quite well on rotation.

Okay, let, so that was the first paper. That was the first paper. Okay. let me move to this other one. Neural fields. So they, what they do is they create vector neurons, which are now EQU pose. Now they actually create object centric reference frames around objects. And they, and the second big thing they do is they generalize to reference frames of completely novel objects within the same category. And I'll show a couple examples of that. So how, what it mean to create an object into reference. Wasn't there an object referencing in the old, the previous paper, meaning you had to pick these dimensions? no, it just said classification and that, but what were the dimensions of space there? X, Y, Z, but x, Y, z Relative to what? nothing was just world coordinate. Okay. Not object coordinator. No. So now they're gonna try doing with object sensorimotor coordinate. they're gonna create a, reference frame around the object. Yeah. The same idea. But now we're gonna have object centric reference frame for X, Y, and Z. Is that, yeah. Okay. Yeah. And one that's, works for different objects from the same category, like different shaped coffee cups. Yeah. so I'll show that. So one of the big problems that we're struggling with here is figuring out what the right, when you're trying to inference what the right reference frame is, anchoring the reference frame. So it's one thing you said, oh, I got this model of a plane and it's own reference frames. but when I infer, I have to figure out what that is. do they solve that problem? Yeah. Okay. It's just given a target post find post. Yeah.

a one consequence of this is if you give, you can have poses relative to the object and even if you move the object around, you can still find that pose independent of where, how that object's related to you. And you use that for griping. That was the task to turn it off. But the core thing is these two, generalized. Okay. so here's an example. coffee cup. So here's a, yeah. So here's an example. Here's a point that's on this, that's relative to this coffee cup off of the sandal, and it gives you some representation at the end of it, what they're saying is, here's another cup, coffee cup that's smaller and slightly different in shape. You take the same point relative to the handle and you're gonna end up with the same representation.

And an example that just scale differences. Was it, there's some deformation. It's unclear. That's one of our question. Unclear how deformable you can be. here's, let's see, that gripper, that a hand. It's a gripper. Yeah, it's a hand. Why is this is a 2D gripper. Yeah. I'm just thinking we, yeah. Sorry. We have to go through, let's first assume the demonstration and test are identical.

We can attach a body frame to the object record, the grass pose in this frame. So here they're gonna grip it. So they, now they have the pose of this guy relative to the object in the objects reference frame, estimate the pose of the mug, and its new configuration.

Can be executed by moving to the recorded grasp post, expressed in the new frame.

But when the mug has a different shape, this procedure fails Since the river must align to a local geometric feature like the rim, and this feature changes location on the new shape. We must instead attach a local reference frame that moves with the task relevant feature. Moving to the demonstrated grasp post expressed in the corresponding local frame on the new shape can then align the gripper to theri. So that's part of their technique is they, figure out some key features and they can associate, a reference point in the objects reference frame relative to that feature. That feature. So all they have to do is now locate where that feature is and then they can. I think, that makes sense. In some sense, that's what we do too, right? You're gonna grab the handle of the, you, it doesn't really matter where the rest of the is. You have to figure out where the handle parts are and then for that, and the, cool thing is they can do this all relative to the objects reference frame, even though kind of the object is distorted up to a certain point.

Okay, lemme see here's an example.

here's like a, an object that they start with and they pick and point here on the handle. Now they rotate that they could find that same point on another object that's rotated differently. and this, these colors kind of show how the reference frame moves with the object. And they can do it, you This pose and this larger mug pose sim, similar thing here with a bowl. I guess it's a bowl. It's a bowl, yeah. So they can create, basically create these reference frame fields around the object.

you can't, it's hard to interpret what it is, but they're just, it's, something that's in there into that, rotation three, facing the handle. Yeah. Yeah. This same variance right before. So this whole field would rotate to the object? No, I'm just think that the example of the cup on the left, they make it see, it looks like it, this rotating in plane, but assume that says for visualization. Yeah. Through the plane as well. Now one big disadvantage here is they don't just directly compute things. In order to do this for every new object and new pose, they have to go through this like minimization process where they try to fit the. Ference frame to the object or try lots of different variations to find that the one with the best, minimum energy. And so it takes, I'm assuming a very, that sounds like some techniques that you did decades ago. reminds me of that where yeah, you're, trying to do these stretchy fitting of reference frames. Yeah. Which can be, but now they're doing it in a deep learning setting. The old ones, is it slow? Yeah, it's slow. the, older ones never work super well. No, the template ones, here because their expectation, I guess is with, since it's deep learning now, it's gonna work much better. But yes, it's slow. do inferences, like one optimization of finding the right six degrees of freedom reference frame. Yeah. Rotation translation. That itself, it's relatively fast, but, the, challenge here, okay. I dunno how fast it's the, so the sole part here you're saying is a morphing of the shape now, right? It's, it's, they don't need to morph the shape. No, it's a different cup. It's a different, yeah, it's a different cup. But all they need to find the cup shapes already represented in there. Deep learning. But that was the, new novel cut shape that you said was slow to recognize the post. Oh, the object centric thing? Yeah. Oh, I thought it, was modifying to the, different shape. Object was slow. You're saying it's post, it's slow. The post pieces slow. Sorry.

So since there are like two key things here. One is finding the key points, that's the whole encoding trend. And the other being burn rotation, that's where, what about translation? They still do center of. Yeah. But then you need to know the center of plastic. Yeah, so that, so what they do is they would, they train on 3D point clouds and they take the center of mass of the point cloud, but they train with partial point clouds. So I think the system will just learn all sorts of different subsets of the object.

Force.

Okay, so I'm just summarizing and I'm using the same format, but I change the words a little bit. so it's a toolbox for 3D, inva and Equian network layers. That's the vector neurons thing. you can represent the 3D location in full 3D pose anywhere in the reference frame of the object. represents the full morphology of complex objects. So they show like rabbits and things like that, including vocal curvatures, just learns all that stuff.

it can work with some defamation of 3D shape. We're not really sure how deformed it can be or how different, it can work with partial 3D data, like I was just saying because it's brute force strain on partial 3D data. and it generalizes to novel instances, but it's not really clear how novel it can be. but it's a general technique with 3D data. It is not specific to vision. this just re requires 3D points. There's no color, no nothing in there. presumably it can be augmented with color, but, the core thing doesn't, is not specific division.

it's really unclear how it scales to large numbers of objects. The last one I showed, I think they only tried on three different objects, right? there's how well it deal with noise and ambiguity is not clear. it's not really clear. The exact post can be recovered. It just, you get to a representation that's the same as it would be, but it's not clear. You can read it out that it's, two degrees this way, 20 degrees that way. Training is probably quite slow and, inference may be slow as well.

it's unclear to me exactly how we. Might or might not want to use it in Monty, maybe individual modules could use these tools. And these are maybe each module goes have little something like these vector neurons or something that's EQU variant, that's the base of it. we might be able to remove some of these limitations. I don't, I guess a question here, should we even investigate this or not?

as I, was about to leave this morning, the way, if you think about the Columns paper, it occurred to me that we could even do something similar to the columns paper, just replace this location signal with their signal. 'cause we never really had an object centric location signal before. and so that's one possible way to approach this. Obviously this is. To the Monty Architecture, but, this one simple way to think about it is we just changed the location signal with this representation. Again, I don't know if that any of this would work in our context or not, but, that's occurred to me as I was leading. But yeah, and another thing you could change sort the location is a sensorimotor input. If you look at it, this gives you some way to describe in way the local curve, the local shapes. Yeah. Yeah. The, yeah. So it might, be some replacement for this as well. But it doesn't have color or, those kind of things still. How, as a field, is this new and hot or is this something that's been around for a little while or What's this, paper is what? December. December last year in archive. Okay. December last year. That's new paper in the previous paper. Is that recent June? Yeah, it's last year, I think. April, March. Is this like generating huge amounts of buzz? Are lots of people working on this? Do we know that or not? Didn't it get a best paper at the conference? Presented at i paper citation?

I guess the reason I ask that question is, how many people are working on this right now? Is it 20 people? Is it two oh people, 2000 people. it's it makes a difference because, it's, a lot of people work on this idea. There'll be a lot of progress and, going forward. or it could be just, it could be an idea that, hey, this is really cool, and then it doesn't, reminds me a little bit of, the, an example of that would be some of the slam architectures, which are hot and then they disappear. take a, guess at this. there's, a, another paper a year before this called Geometric Deep Learning, which introduced this whole approach to constructing architectures with different types of, in variances and variances. And this strikes me as a special case of that kind of program. And the geometric deep learning thing did make a pretty large splash. So my guess is that this is one example of introducing those. It is a special instance of creating a network of certain types of, in variances or echo variances, that. We'll see a lot more of them in the next 10 years. No problem. That's good. one, one could say, Hey, maybe all neural networks are gonna go in this direction because, the entire field of AI is gonna have to be built on concepts of, it's hard to say, this could be, replacing, this could be infused through everything, right? Because the, world has to go to 3D century motor modeling and, this can be the basis for doing that artificial mill network. Or it could be just, a subfield. That's interesting. We don't have to know the answer. One, one area, I've been talking to Scott Purdy a lot, Zoox, and self-driving there. They have, they deal with point Cloud constantly and I, they, use a precursor to this. I could see in the self-driving world they could gravitate to these kind of networks. Slow interest with slow interest.

it needs to be sped up. But you mean just again, no one would use exactly this technique, right? This is gonna evolve. This is just, deep learning is evolving so fast. The idea that you're infusing through Ality 3D into the networks in an inherent way. Is a big deal, right? Because that's Yeah. That they're already doing in self-driving cars. They use the precursor to this called appointments. Yeah. Which deal with 3D points. the new thing here is the variance piece. Yeah. because in some sense, our thing is hey, sensorimotor learning is all about 3D space and all about modeling and, 3D structure and, how we'll structure them, model more and so on. And that really hasn't been the center of, networks for a long time. so it should be moving in this direction. one thing I have an interesting question. one thing I've always thought, if we could define what a learning module has to do in Monty and just say you what inputs have to look like exactly, what does outputs have to look like and these are the things it has to be able to accomplish, then, that would answer, maybe help answer this, your question is I have to do these things. Is this gonna help me do them or not? could I have a one team that goes off to doing these techniques? Nothing. I, don't, you think these things, techniques, we can only specify it at a very generic level. We can't say it has to be exactly this or exactly that. 'cause these things are just gonna learn these representations and it's really hard to specify ahead of time.

it may be that we can, recover the exact post, for example. like we, right now, we'd have to do some work to actually get it to correctly output the correct post. Or, but we could say representations of the pose have to be consistent for a consistent poses. We could say it at that level. I, guess maybe, you say, which might not help. Yeah, I know. Maybe I'm being too simplistic. Maybe I may, my understanding not, but I'm just thinking Hey, in my mind there should be a specification, what a learning module has to do with Monty. And and in some ways we're somewhat indifferent to how you go about doing that, right? Or, and maybe some things be better than others, but if you, I've always said you could define the interfaces then, you can have at it in between. so I don't know. But I dunno if we could do it at the level of Monty modules. it would be like the entire hierarchy would be this. Oh, that's what I mean. we could define a single learning module and say, because I'm just throwing that as an idea. Yeah. yeah, I think that's a good point. Clear about what this, what does it ex exactly. Do I, we could say you wanted to recognize a certain object and a certain post. Would expect each module on each column structure.

but you, there's still a, the train scenes, you have to manipulate these objects or turn on a rotator, right? So there's still a yeah. you don't have to rotate 'em too much because it's e rotation. You just have to show it in lots of, subsets or weird defamation. it's like anything, it's like I can't, this won't, if I can't see the bottom of the airplane, I won't know. I won't be able to recognize the bottom. Yeah. You have to do at that level, you still have to move things around. And that's true for us. And that's true for this. but you wouldn't have to do the 2D rotations and you would only have to show the other side, for example, if it's, or maybe three or four angles at it. that's okay that too. So, this relates to another question I had. Why do you say training is slow? do you have specific numbers or something?

'cause the, because it should be fast based on what you just already Yeah. It gets back to what Lucas was saying. I should verify this. I just assumed it. My intuition was similar to, I think what Lucas was saying. You just have to show a lot of partial subsets of these point clouds and a single image presentation could be. Training.

Yeah. I read this paper. They were very vague about having code trained and it seems to me that a lot of training just from the interesting, and these are complex networks, it might be two different papers, right? The second one would be much more training data intensive and, time intensive for the training. And then the, I think even for the three objects, they need a hundred thousand examples and, train on that one. The piece is, relatively simple, but the second one, they don't even talk about training code. They just assume you have a training code. Yeah. They need to open how hard it's to frame that. Yeah. They need to encoder plus then on top of training. Yeah.

Yeah, we could try. what you're saying is, each module will recognize full objects anyway. and maybe we could just try potentially small modules at the level, looking at small patches Yeah. And see if, getting a local shift features. Yeah, that's one representation. And the thing is, all this stuff we've been discussing about curvatures and smooth surfaces and displacement and all that stuff, it's already captured in here. We wouldn't need to do all that stuff. we don't need to build a graph. it sort incorporates all that, but it's an alternative. It's an alternative. We, could still go through parallel apps and Yeah, It is just, it's a very different mentality than what we've been talking about.

So that's, I don't know, should we spend time on this or not? What's your intuition say?

I think we should spend a little bit of time. It's so close to what we're trying to do, getting more, I like there's so many unknown here. Maybe it fits very well into, we could at least go that far and see does it fit well and can we specify some sort of interface, possibly some variation. Variation, yeah. A little bit. What I was thinking in terms of variation. So instead of having this encoder network that maps the whole point cloud into some major presentation take off the shelf, we have. Multiple of those local descriptors around the object and yeah, multiple, those local curvatures. And so one local curvature relative to Yeah.

Yeah. Each one could have a little reference frame associated with it, and then you could build a graph from that. I guess the key thing about Monty is not, it's it's not oh, we have to many learning modules, or we have to do this. We have, basically, it's about, it's a platform for sensorimotor learning and there's some capabilities we wanna be able to do is with sensorimotor learning and sensorimotor interaction with objects in the world. And, it's hard to know, oh, could you plug in a wholly different type of learning system that sort of achieve the sensorimotor learning aspects that you want to achieve. I think the trick is to, if you're gonna investigate this. It could become the tail that wags the dog. You don't want to like, let just run it and become, then you just become doing this. You know what I'm saying? We don't wanna decide what the bigger picture we're trying to achieve.

and that's, the issue. Yeah. Yeah. So I think that's the problem. That's always the risk of working on something like this. You we just say, oh, let's just investigate this, as opposed to thinking what were we trying to achieve overall? does it help us do that? But anyway, it sort seems, like you don't wanna discount it. It seems pretty important. A, significant, potentially significant additions in normal that works. yeah, we could, try just for feature learning local curvatures and see how this would go. It would be learn variant, curvature features. Maybe that's the start. Yeah. Yeah. That's a good, that's a good idea.