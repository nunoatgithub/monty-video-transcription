cool. So welcome everyone to the post, brainstorming week, research meeting. yeah. Are you gonna summarize the 20 hours of research in the next couple minutes? Yeah, that's the idea. Although, yeah, it's not gonna be any particularly concise overview or whatever. It's more kind of working through the notes we wrote, last week and kind of the sort of provisional conclusions we came to, about object behaviors. And then we're probably going to disagree and argue about some of those. And then, so it'll also be like a new, I'm sure there'll be new discussion points that come up because of that.

but yeah. Yeah, I guess Tristan and Will did, you guys probably haven't had a chance to watch, any of the stuff? No, I haven't. I haven't. I was ill yesterday. I didn't do anything. Yeah. Okay.

like I said, just for kind of interest, so this was kinda the first time we did, one of these brainstorming weeks that was entirely remote. We had four days and it was like a four hour session where we would, brainstorm on our own and, think through ideas. And then a four hour session where we would meet up and, discuss everything. And we had some of that, kind of discussion that happened through the medium of PowerPoint, but a lot of it was using Exela draw, which was, kinda exciting to see that working well in terms of remote whiteboarding. and yeah, Vivian's kind of added some helpful boxes showing kinda the different days, but you can see how we progressed like sort of mad scientists through these various kind of concepts here. And yeah, I think it would be too much to try and go through, all of this, again and would be trading too much of the same ground. But what we were also doing while we, were brainstorming is at the end of each kind of day we tried to, so I should say we were being guided by these kind of questions we have here in this document, these open questions. And at the end of the day, if we felt like there was some things that we discussed that sounded reasonable, we basically wrote them down in this kind of gray text to convey that this is a provisional conclusion, or kind of idea that we should remember. And so the main idea today was to go through this text, check if there's any disagreement or confusion, and then I think where relevant we can refer to the kind of diagrams or figures that kind of informed, this discussion.

But yeah. Is there any questions or comments?

I guess if, this will probably take the whole meeting already, but there's more time. I thought a bit more through the proposal I made last time about the optic distortions. okay. Nice thought of an issue with it that I could talk through briefly and, a potential solution, but, yeah, no worries if we don't get to that. Yeah, that does sound interesting. And, so for context, to those who weren't there last week, yeah, there's a few different ideas discussed. but probably the biggest, kinda most important one was when Viviane proposed in the last day for how we could have these kind of distortion maps, and that those could be, oh no, not what I wanted to click on.

Where is this?

yeah, so that's what's conveyed here. obviously there's a lot to it, but at a kind of high level it's, taking some of the concepts that we had for kind of, how a parent can influence the information coming into a child learning module and bringing that into one learning module as a way of dealing with things like, Object distortions like this?

yeah, I'm I don't know. In some ways it could be nice to start with that because I feel like we could always go through these comments offline. whereas we, we can't do the kind of discussion, that you're proposing, offline or that's harder. But we can also start at the top of the questions and then I think we'll naturally get to the, one around object distortions, and then I could go over Okay, cool. At that point. Sounds good. Yeah. I don't wanna take away from the summary. Yeah. Okay, cool.

the kind of first thing we were looking at was how we would actually learn in a kind of, reasonable amount of time, object behaviors, as we've discussed many times, it's, challenging to do that given their complexity, as well as how we quickly do things like inference and the relationships of voting. And one of the things we thought a lot about kind of, or, talked about on and off, beforehand was if there's some sort of sharing of knowledge across columns, and how that, might enable a more efficient learning. So we ended up having a fairly extensive discussion about. The role of hierarchy for learning and in particular, learning at higher highest level of, hierarchy. It wasn't actually that much related to object behaviors, but I think it was still an interesting discussion about, we don't have it here, but yeah, an interesting discussion about, learning.

But basically, yeah, so what we wrote, let's see. I think the, reason it wasn't that related is because the solution will apply to both static and behavioral objects. And it was easier to talk about the static objects, but yeah, it would still, like the problem becomes more apparent with behavior models that it's so difficult to observe everything on a behavior.

yeah.

so let's see. In typical, fashion already this first sentence, slightly confused by the way, we can use hierarchy to infer objects with a column that hasn't learned the object yet.

that doesn't make sense. as in, let's see. This means we can remove the core for all the level comes to learn model of each objects. They will only learn the mod objects through higher, that we can already make predictions based on models that will learn quickly in higher regions.

I think what this is meant to say is I think it, yeah, we can remove, the requirement for all the level columns to learn model of each object. That's true. They will only, learn the object through high repetition, but we can already make predictions based on models that were learned quickly in higher regions. I think that's, yeah. I think it relates to that figure you pulled up, before and the example of, like we learn an object through touch and then inferred through vision, right? Where we have a high level model, of the general features. And then each of the sub components have models in both touch and vision. you know how an edge looks like and you know how an edge feels like, but you don't have a model of the compositional object in the lower columns yet. You only have that in the higher column. yeah. So we have some discussion that ear, right? And yeah. Sorry, was someone else saying something? I was just thinking, I think we also reached a conclusion where we're learning a different resolutions where, it's okay to learn also in parallel, like a higher, level at a higher reso like a. At different granularity. And then basically, if you need to learn details of the lower level objects, you just do that through repetition. Yeah.

yeah, no, exactly. Yeah. And so that's basically, what stated here. So high level model, low resolution, fewer learning locations, we can be quickly learned, high resolution, low level.

LM will generally require, many observations and is, learned more slowly. and yeah, we talked about how the kind of high level LM might even be more like a hippocampal complex. In the case of biology, this was the first point we already discussed, so I've just moved that down.

one tends to attend to course features first when learning rapidly at the high level. However, you can also tend to detailed features and learn these rapidly at the high level awareness. So the example that kind of, informed this statement was again, with the hippocampal complex, for example, you can walk into a room and you can quickly see, all the pieces of furniture, large objects that might be there. but equally you can attend to something very detailed like letters on a page or the arrangement of objects on a diagram or whatever. and that because of the speed at which kind of the brain picks that up, that's clearly happening at a very high level, as well. But, yeah, the, kind of key factor is it's still not that many locations, so, there's a difference between kind of locations and the scale in which it's happening. if you're learning a diagram and there's a lot of detail to it, that's still going to take a lot of time. Or if you're learning a whole room and there's a lot of different objects that you're trying to learn about, that's still going to take a lot of time and require a lot of capacity. It doesn't really matter so much whether it's at the scale of a, yeah, an entire football field or a piece of paper in front of you. but yeah, what we tend to notice for first are these kind of course, features that jump out.

Cool. And yeah, definitely. But in, if I say anything that anyone agrees, disagrees with, otherwise I'll just kinda keep going. location space at the top could be detailed, but we may not be storing a lot of points in there. it can then interpolate between the features if we pay attention to details, we can store more features at granular locations. I think this kind of saying something similar to, the earlier one, it just also mentions that we can interpolate, which is nothing new.

Learning course models first and inter interpolating until we fill in the details should also work at lower levels, but will require more observations as we can't rely on informative, compositional inputs.

I'm not actually entirely clear what's meant here. I don't know if anyone remembers if, they wrote this, if they're able comment on this one. Yeah, this was about like the general strategy should work at any level in the hierarchy that basically we first learn a course model of the object, which allows us to make some rough predictions already just by, I interpolating between the few points that we stored.

and that can happen anywhere in the hierarchy. But then if you're at the low level in the hierarchy, you're just getting raw features as input. So each point that you're storing isn't, that informa isn't as informative as if you're at a high level in the hierarchy. Input is alright, chair at location X and then you already have a model of a chair. So you can use that existing model of a chair to make a bunch of predictions about all the locations on the chair. so like at a higher level, you, are already getting more informative inputs for each location, like entire descriptions of entire models that you know about. So Your point help you more.

Cool. Yeah. Thanks.

okay. And then attentional masking to only use information from, parts that are moving can help here.

I think then we went to thinking about this more specifically for o object behaviors and how we can be more efficient when learning object behaviors quickly as far as I remember. Yeah. Okay. Can help, I'll just say with learning object behaviors efficiently, the kind of further thoughts on tension. So yeah, we later than had a, more kind of in depth discussion on tension. Oh, what's going on with Google's cursor?

Can you guys see how it's like in, in the middle of this e here and like offset? Yeah. Very strange.

for learning behaviors and morphology, we don't need to share knowledge laterally or hierarchically. learning just happens at, all.

Of the hierarchy in parallel with learning happening fastest at the highest level. This is separate from using hierarchy to support multimodal generalization.

discuss below. So yeah, this was basically when we had entered this discussion, we had talked about how maybe there's some way to share kind of knowledge through some sort of lateral, connections or even a way to share it. Hierarchically what we described was basically a way for learning to happen really fast at the top of the hierarchy, which already enables us to do useful things while spending more time in the world. Basically gets you into kind of expert mode by learning more models at the lowest level. but there's no kind of transfer of, models between columns. and yeah, we discussed how that was problematic, for various reasons in terms of if we're constrained to CMP signals. it's not clear how we could do that. And I think, Jeff also brought up neuroscience evidence for like, where people have shown that knowledge is not transferred from the top of, from the hippo capital down the hierarchy, but instead it is relearned at every level of the hierarchy over time. So it seems to match biology. Yeah. Does anyone know what papers that might be referring to? Because I, I think that's a really interesting point to bring up. obviously like the classic, Finding or argument is that like he could never learn any new skill, but I'm assuming there's some, what Jeff is referring to there some study where like with sufficient exposure he did, or, some other patient with hippocampal lesions did learn, new things. It was just much slower. I seem to remember there was stuff he could learn motor skills, but that could be more kind of subcortical, stuff like trial and error, in the basal ganglia.

j just to make sure I understand the, problem in the solution, the problem was that we were trying to, figure out a way where, we're observing different parts of the behavior at the same time on an object. and we assumed that there would probably be some shared, some shared learning so that we can observe the different kinds of behaviors at, the same time. and the solution is that because the higher regions have a larger spatial receptor field, they should be able to, we should be able to build a, course or model of the behavior at the higher levels. And we don't necessarily need to transfer it down to the lower levels. but we would still have this course model of the behavior at the higher levels. And this basically assumes that we don't need to the lower, level columns, they don't need to have a complete model of the behavior. I don't think it was necessarily about the size of the receptive field or it not being able to see all of the behavior at once. I think the assumption is still that we'll have to re-serve the behavior multiple times and look at different locations on, on the behavior to be able to learn it. So even the lower level, model can learn that behavior. It just has to see it more times and look at all the locations on there more often to, because it learns more slowly. but I see, I guess to me the big question was just like, how is it practical to learn a behavior? Like when we think of our miles that have 2000 points or something and the optic changes after a second and it's in a different state now. Like how is that realistic? But then the answer being, all right, we just need two or three points to start with and we can start making some predictions so we can cate very quickly. And then, the next time we observe the behavior, we add another two or three points and another two or three points and, our predictions get better and better over time.

I see. So it doesn't really. Solution doesn't really solve the, problem of learning a behavior quickly in one column because we still need to do the same repetitions. yeah. So that will, I, we can never avoid having to move over the behavior to learn it. 'cause even at the highest level of the hierarchy, it's not like the input is broken down spatially, it's still just features at a location. It's not like we suddenly get, like a bunch of locations at once as input. So the other thing is we also discuss further below the effect of boating. So that does help with, some of that where, if all the columns are voting on the, both the object but also the state that it's in, then that information could be passed up and that can inform a behavioral model that can generalize, but that's only for inference, not for learning. For inference, add learning, you would learn, it's where it's like the stapler top, for example, is moving. So you would learn stapler top is moving, but stapler top is like a broad feature. So if you know where that is at a point in time because of your behavioral model, that can enable you to generalize to other parts of the stapler top. So that means you don't need to learn every single point of the stapler top that was moving. But that's more like using compositionally versus using voting. you can use voting to quickly, it, it's both prefer the stapler top pose at the lower level, but then at the higher level where we learning the model, it's more about okay, we have this compositional object already. So we again just need to store a few points to be able to make predictions about it.

Yeah. During the brainstorming, I, had the a feeling that, the idea of the horizontal spread of learning, like sharing knowledge across columns in the same region, might be promising to solve the, we don't need to do repetitions if the knowledge is being, spread horizontally across the column. because they all see different parts of the behavior at the same time.

but maybe we can discuss that later out outside of the summary. Yeah. 'cause I guess just to observations, the stuff we already discussed helps make that tractable or at least seems like it would be tractable to a reasonable amount of observations. And then, yeah, I guess the observation in humans is that we do need to observe behaviors. Many times. Like people, if it's a totally novel behavior for a child or something, they can't just zero shot or one shot, learn that in the same way that they seem to be able to learn morphologies. Just like the behavior you showed with the logo moving around on the cup, They watch it like a bunch of, yeah. It wasn't like all the columns just saw it and then, or it didn't seem like that's what happened and then they just transferred the knowledge compositional in, in some sense. but like I'm talking about the behavior, like the inflating of a balloon or something like that.

it does feel like you, you might be able to aggregate knowledge from neighboring columns to, because they all feed into one behavior, and there's no positionality there.

but yeah. Yeah, I guess the, it might be an, a mechanism to explore more if we run into limitations of what we have so far. But I feel like what we have right now seems pretty powerful already, so yeah, maybe we just have to practically test it and see if it's sufficient or not. but it would be nice if we don't need to rely on the lateral spread of information. 'cause Yeah, we also don't necessarily need to learn every object in every column.

Yeah. I have a question about the predictions. we're talking about learning quickly and being able to start generating predictions quickly. Are those predictions generated in a higher area and do they. Show up in the lower area, in which case it seems like a, there's some model sharing going on. if a higher area makes a prediction about what might show up in a particular location in a lower area, I guess is the question. So when I said predictions, I was referring to what every learning module does all the time. Like whenever you get a movement input to the learning module, it will activate a new location in the models reference frame. And at that location we have storage of certain feature. So it'll predict to sense that feature. so by that definition, it would be happening at every level in every learning module that has a reasonably confident hypothesis. predictions basically stay within that level. Yeah. Yeah. And then they, to narrow down hypothesis or have prediction errors to then attempt to, or Yeah. Learn new things.

Okay. But all within, constrained within the learning module. Gotcha.

And so when you, and when we say learning is slower and a lower level versus a higher level, is it slower in terms of is there any sense at which it could be adding observations? Like, an adjustable learning rate, or is it just because it's number of observations that sees is small over time? No, it's more the learning rate, I think. Okay. So it could be taken, and I think there's two factors, like there's probably many factors, but at least two factors we could discuss, before is that like in biology, it's expensive to be able to learn quickly. Like you need almost specialized neural hardware to be able to make rapid connections. And that seems like the hippocampus has like specifically evolved for that and, is this big structure compared to an individual column. and then I guess the other thing is, like with representational drift, you maybe don't want the lowest level representations to change as quickly as the highest ones because the highest ones are going to kind be using stuff that's lower down. And so there may be a benefit to the fact that kind of, there's less rapid plasticity at the lowest level of the hierarchy.

and yeah, those are at least two factors come up before. but it, it's possible that in Monty we could be like, okay, we don't have the biological constraints, so maybe we want more rapid learning at multiple levels. maybe that's like a way that Monty can be superhuman, but, it could be interesting, maybe. Maybe there's a reason the hippocampus seems to be at the top of the hierarchy and, generally isn't dealing with like very low level sensory input. And, yeah, I was wondering if maybe the slower learning rate might be advantageous for generalization purposes. if the learning rates are really fast, you might enter an overfitting type regime on a behavior or something, but with maintaining something that's has a certain degree of coarseness makes it easier to overlay on, novel objects or whatever. Yeah, no, I think that's a good point. I think, yeah, it what do you call it? Yeah, no, I think that's right because that, the hippocampus or whatever is good for just like rapidly learning a, like a particular instance of something. But yeah, you want diversity in the models that are learned and having these different learning rates as a way of being like, okay, some of you are gonna be specialized at learn this exact instance of something and remember it when you see it, and some of you are gonna be more kind of generalists.

cool, cool. Yeah. Any other?

Okay, yeah, I think Ramy you brought up a fair point that it, maybe it wasn't that clear how some of the stuff we discussed here mapped onto object behaviors. firstly all the stuff we discussed that seemed like we agreed on, turned into black. And then, I added, so there was already this thing about we learn these course models and interpolate between them and we can do that better at the higher level. But just written the above also relates to behaviors, either core century inputs, so flow over a relatively large region or compositional inputs, EEG, stapler, top moving enable a high level LM to quickly learn a behavior model with a few points that already enable some basic predictions.

That sound reason. Yeah, that sounds reasonable to me. Maybe could you make it a different color than black, like blue or something so we can distinguish questions from answers? Sure.

Kinda satisfying.

cool. So the summary is we rely on a system that will learn a few, points and interpretate between them later. We add more detail, helps to end the course model at a higher level because we can use compositionally or maybe just write, oh God, that's driving me nuts.

Do you try refreshing your browser window? I was just thinking that, yeah, maybe I should. don't even, let me, wonder how quickly your brain can learn that special offsite. Yeah. Yeah. 'cause I think it is consistently. Oh, nice. Okay. Thank you. Yeah. Turn it on and off again. Always, cool. So by the way, yeah, and I was thinking I'll maybe go down to here and then just to keep it interesting, we can maybe take turns kind of reading step out so you don't have to just listen to me. although Scott, feel free to bow out from that one.

cool. So for inference, yeah, this is where the kind of multimodal part comes in. and I think this was actually when it came to the hierarchy, this was where the context Jeff first brought it up and, so see if this makes sense on its own. yeah. How do we do multiple modal generalization? For example, learn with vision and info with touch. If a high level learning module has, has previously learned the association between two low level objects or sensory features from different modalities, for example, the touch in sight of something sharp or the touch in sight of a cube, then if we encounter a new setting where we observe one and learn it as part of our high level model, then we can immediately predict what we'll sense in the other modality at that location.

And so hierarchy in this way is necessarily to recognize a novel multi feature object that has been learned one modality and we are inferring another. For example, if we saw a complex object and studied it with our eyes, we would need to use a high level multimodal model to slowly to attend each part when touching it blindly in order to recognize it as the same object.

yeah, I think that still sounds good to me. And then Yeah. Yeah, that was a pretty big one, at least to me. 'cause I feel like the prob problem came up many ti times over the past year of oh, how can you learn an object with your finger on your right hand? And then you inferred with your finger on the left hand or Right. And then, yeah, maybe that's another good example to include. Yeah. but yeah, since the brain always naturally has hierarchy, even though like most things we try to solve within a column or they start there, this is like a really neat way where hierarchy can be used.

But yeah, this should also, work when generalized across specific sensory input inputs within a modality. For example, you can learn object one finger and recognize it with another.

but then yeah, we discussed that, sometimes you cannot generalize, with this mechanism. For example, a logo exploding in visual, space. And that behavior, cannot slash shouldn't be inferred by a touch column.

Oh yeah. A logo exploding.

maybe you say a logo moving. Yeah. 'cause exploding doesn sound like sound kind like a toptal.

not everyone assess intimately familiar with the exploding logo yet. Yeah, not yet because we are, we can bring it to the world, put it up in Times Square, make it famous.

how can, columns collaborate to infer behaviors? So recognize states before it has changed again. so yeah, we can maybe mentioned this earlier, but I think what we agreed was we definitely want columns to somehow, share the point in the sequence with other columns. we're just not sure if this would be done through voting or broadcast by the matrix cells.

at least in the case of biology. voting could be a reasonable approach to identify where we are in the sequence slash behavior note. There shouldn't be that many elements in a sequence. maybe say at any given level of the hierarchy such that it could be realistic to vote even on the state in the behavior. And this is where we had a discussion about how many points is there in a song or whatever and, or, words in a sentence and in a paragraph and all this kinda stuff. Yeah. And at what point do we break it up hierarchically, like breaking up a song into versus, and then two, four by four notes and, yeah. Yeah.

cool. Any questions or challenges to that?

Okay. And then last one, this I think is related to the gov. How does voting work for behaviors? Do we vote on behavior id pose and point and sequence? I might just move this. I feel like these kind of answers moved back and forth 'cause I think they were all touching on the same thing.

yeah. And then I. There was some discussion that it might be too much to ask the thalamus to encode the timing in the s in the matrix cell output, I should say, specific to a given sequence.

Yeah. But then I think Jeff was arguing, we shouldn't throw away this idea yet.

maybe, it's something to basically still have in the back of our mind. So let's say maybe this one gray might, since we haven't really decided. Yeah, that's a good idea.

Okay. I think I'll take this one out.

okay, cool.

yeah. Moving on to object segmentation. Does anyone particularly wanna do this one?

I can do it. Okay. Cool. Do you want to share your screen or, do you wanna read out here? I think I'm gonna be doing it on the other screen and not logged into the teams meeting. Okay, cool. Please scroll up and down while I type. Yeah. Or read. Yeah. And then I'll highlight it blue and stuff. You should. Okay. So object segmentation. I think, this part we're talking about, A hierarchy in, a hierarchy where we have composition objects and for example, the stapler. we, learned the morphology model of the stapler, and then the top part starts to move up and down. And, we were thinking that, attentional masking will allow us to do something like, define an object in the lower, in the lower region column. Like the only, the top of the stapler has moved. So we basically want to only attend to that part and define it as a mask.

and, there we brought up some evidence from, just that the TRN allows for, attentional masking, through the spotlight hypothesis where it basically just, defines an, attention on like part of the, part of the retina, basically because RN is also retina topic, organized or it, matches the, retina.

so I think that was the idea. so let me just read through, cool. The points. Yeah, I might just quickly, breaking this up.

Just as an overview of the neuroanatomy, basically the, TRN is just a thin sheet of inhibition that applies, inhibitory, the action potentials are like, effects on the LGN, which is, or the LGN or basically the whole palus. But, the, the TRN is divided into sections based on modality. there is part of the TRN called the PGN, the peri nucleus, and that is basically matched one-to-one with the, g, LGN. So the, these are doing, vision, inhibition. The LGN, the TRN or the, PGN receives input from two places. It receives input from, the LGN, so it, it forms a loop with the LGN. So the LGN can, excite the neurons in the TRN, and then it, the TRN will inhibit the LGN back. So this is like a closed loop. but the, PGN also receives input from the cortex from layer six in the cortex. and we were thinking that, when it receives, input from the cortex, that's basically model-based inhibition. And maybe when it receives input from, the, the LGN, that's more model free inhibition, because it still doesn't, the, doesn't really have any models of the world, yeah. By the way, Niels, we have for this one, I think we have a bunch of notes at the bottom after all the questions. Yeah. Okay. This one, Thern and stuff. So just might have to sort them in there. Oh, okay. Cool. Yeah, you're better at, note taking. Oh, no, it's okay. Yeah, I just thought, yeah, while we were talking, I'll just add this, yeah, let's maybe then, so is the behavioral able model to tend to regions model any template, either it seems very reasonable this, be, the case and would consist of model three tension, like model based tension, and then I'll add this stuff up here.

Okay. Sorry, Romy. Yeah, you go ahead. I was just, one thing I think we didn't really address is whether these. Whether this attentional mask is going to lead to a permanent model in the lower level columns or not. are we using the attentional mask so that over time it basically becomes the top of the stapler is just a separate model from at the bottom of the stapler? Or is it just like a temporary mask so that we can do some predictions? yeah, I think that's covered here. So how do we communicate outside the column? We don't communicate entire models, so it's unclear. We need to communicate kind of half model, quarter model, et cetera. But this is, it may be sufficient in the long term. We will learn a new sub child object morphology. in the short term we can attend, right to the, sorry, what were you gonna say again?

yeah, I guess it, it says that, yeah, we might learn a, model in the long term, but it doesn't really talk about how we would do that, but Oh, that's fair. Yeah. That complicated once we Yeah. Invoking a mask, it seems, right? I guess it's still open questions, does it turn into a totally different reference frame? is it somehow still related to the object reference frame? Feel there's still some open questions around that. I, could imagine if we had consistently get that this, inhibitory signal, like basically we only, we're only getting the A mask. From the thalamus then I could imagine a way where basically the model through heavy learning starts to forget about, different parts of the object that it's basically not reinforced.

yeah. So that means, one column would, wouldn't have a model of both the stapler and the stapler top because it would forget about this whole stapler. Once it keeps seeing, keeps paying attention to the stapler top only, the, model of the whole stapler will be in the higher level now in the higher region. And the parts only will be in the lower region maybe. Yeah.

Yeah. That could be, I guess it would still have to be fairly consistent. 'cause like once the stapler stops moving, we'd be paying attention to the whole thing again. So feel like we would still be able to learn the morphology of the, or re retain the morphology of the whole stapler.

Yeah, that's a good point.

Yeah.

yeah, a little bit on that. And then one thing I don't feel like we had in these notes, which I'm just adding now, is, what are the key elements of attention? I was gonna write maybe there is, what draws our attention to something and there is what narrows our potential window around left sub select the input. These aren't always the same thing. I think there are some notes guys agree with well about that. Okay. Oh no. because there was this bit.

so to me this is top down decisions about where to attend can be.

Oh yeah, I was thinking about that bullet. I was looking at the bullet point below. It says there's top down, bottom up attention, both limit, which columns are processing input based on where space they're sensing.

Did we put that somewhere already? Yes. Yeah.

Ah, okay. I see. It's the part of the, okay.

And yeah, I'll just delete it down here.

Did we get, sorry, I think that first sentence though, maybe we didn't get, oh, because, yeah, so then there is, also top down and bottom up, factors determining the attentional window.

I'm not sure I understand the distinction between drawing our attention and narrowing the attention. Okay. Yeah. So I guess to me, for example, if you, hear a sound or something that's a bottom up signal that's going to draw your attention to an area, but, once you're like, when you're actually looking at, that's different from, I guess what's determining what's actually coming into your retina.

it's more about informing your organs how to move, in the world. Versus like masking what's coming in and saying this is irrelevant. So is that basically top down versus bottom up? No, I feel like top down and bottom up are in both.

I, think. Yeah.

Okay. Maybe I'm not understanding the narrowing down part.

I guess to me the narrowing, I'm sorry, go ahead. Roi, no, I was just, I was gonna say the same thing that you're saying, narrowing down is basically that you're looking at something and then you just mask part of it, but without moving. So you don't really have to like, it's not like you see some stim like you feel some stimulus in individual field and you start looking towards it. so the narrowing down would just be masking part of the input, without movement I guess. But the other kind of attention is where really feel some kind of sim stimulus and you just move towards it and basically cade on it and then you start masking to that part. Would that make sense? So if we start with the last bullet point, which is, top down, bottom up, attention, both limit, which columns are processing and put based on wear and space they're sensing. So that I would, that includes both those, both narrowing down and. And drawing the attention, right? it's basically just about which parts of space do we want to be processing and which do we not want to be processing, right? Yeah. But I guess I feel like it's helpful to break them apart. 'cause I feel like they're, I yeah. Going to be driven by different stuff and yeah. different mechanisms. So you mean, the drawing, the attention is like using action commands to move somewhere? Yeah. Over attention. Yeah. Versus, the narrowing is basically, it's been more like the covert, basically, which columns should process input, like covert attention. Yeah. But then I guess the only confusing thing about using the term covert is generally we were, when people talk about covert attention, it's like the very behaviorally, what you call it, atypical setting of keeping all your sensory organs frozen and trying to mask out or attend to a particular region. But the reality is these tend to be overlapping so that you move your sensory organ somewhere and then you use your kind of attentional windowing at the same spot.

okay. Yeah. So yeah, this can probably be worded better. Yeah, let me just think.

So, there's what draws our attention to something, okay. Or, there's a tension that requires moving sensory organs to focus on something, a tension that narrows the sensory inputs received, given their positions.

the former is overt attention. The latter is overt. When we don't move our entry organs ready, is the two usually happen together such that we move to and to something sensory organs and then focus them to the window.

Does that kind of make sense?

and then either top down or bottom up mechanisms can drive. Both of these, it feels to me that they're both part of the same mechanism. like you, you get some stimulus. you want to attend to some part in the, outside, like in the field of you. But just you're not focused on it. If you're, not, if it's not in the focal point, then you basically go to it. And, like that would be the first step. And then narrow your attention. mask it there. if you're already looking at it, then you just mask it. But it feels like these are just two steps of tending to something that like, yeah. Yeah. I'm not trying to suggest that they're totally like unrelated or, I dunno. I guess I just think it's useful 'cause like sometimes we were using language, which to me was clearly referring to moving sensory organs. And other times it was language that was more like narrowing potential window. And at least as far as the LM is concerned, in some sense they're, or like the kind of sensory flow and stuff, they're, they are very different in that one is a movement that is causing like a totally different sensory input. And one is some way that we are processing sensory input.

but I agree they happened together and, Yeah, I think we had a similar one here, but it just changed this, in the brackets to, to say just processing input in a certain region independent of moving the sensors there.

instead of without moving the sensors, like you might be moving the sensorimotor, but it's about which area in space you're processing doesn't matter how or where you're moving the sensors. Right.

yeah, 'cause yeah, again, with vision, like I feel the moving your sensory organs is more there's a sudden flash or you, there's a suddenly large amount of movement, so you move your eye there. But then it's the fact that there's a big blob of movement that's coherent is what tells you, oh, okay, that might be one object. I'm gonna narrow my sensory input to all that.

so it starts model free and then you get more model based. like in that case, both of those are model free.

I see. Like you're just using the, some heuristic about the boundary. Yeah.

yeah.

Okay, cool. Is that Yeah, clear as mud?

let me just check, sorry, before we keep going, what other stuff is down here?

Yeah. This is I'll just say use this as an example.

May, relate a bit to what Scott is starting to on, right?

Yeah. Maybe I'll just say for the Al window maybe what are something like this?

Yep.

You have that up there.

Okay. I, think the other things we can get to when we. So I think finally this, we can just read through the questions and then the answer. Okay. This what you were just saying earlier.

Yeah. So with the thumbs being involved in intentional masking, seems reasonable, given the neuro anatomy and, seems to be, you just say the TRN seems to be able to support, both model three, model based. Yeah, that's a nice point. Yeah. You mean given the like L six inputs?

basically be just a layer six, two G.

Cool. And then the maybe one other interesting thing you talked about that would be worth mentioning is the, I'm forgetting the term right now, but like the colocalization of the receptive fields or like the, As in that the, layout is they're both Tino topic and, they're both reino topic. Okay. Organized.

Yeah. And just for clarity, the PGN is the nucleus in the TRN that corresponds to the L lgn or like the part of the rn. So yeah, this is the, I just call it the vision sector.

Okay, nice. Thanks G.

there was also some more information about how, the receptive fields of the PGN nucleus, or neurons are like basically, complex. They represent complex shapes, that are more reminiscent of receptive fields and features in, in the cortex.

Basically be very useful if you want to, just have an intentional mask over an object or some boundaries or something like that, because they're detecting more than just simple. yeah, that's a good point.

that was PGN in irregular with, complex features and something like that. And are they dynamic as well?

what do you mean? Presumably? I guess they would have to change, I dunno if it's observed that they like change given what is presented or Yeah, I, would think that because they get input from the layer six neurons, I think they change based on the input. but I'm, I don't recall seeing that. Yeah. Okay. Yeah. I feel like that's what we'd want. Or, like it would be cool if they had that.

yeah. And I would think also there would be that they would con like they can have combining effects if they're applied together or I don't know.

yeah. Yeah. Yeah. That might be the alternative is like maybe they're a single. Cells, receptive field is fixed, but you can combine them to create arbitrary masks to me.

Cool.

What can enable attentional masking, I agreements or movement based masking?

how does a column neuron represent this attentional mask?

yeah, so this is now in only in the cortex.

I guess the movement based masking could be the model free type masking.

Yeah, yeah, discuss the right ideas around how a column could use sensorimotor displacement associated with votes to predict when it'll move outside of an island of agreement. Yeah. This is, the island agreement idea.

I don't know if you wanna go over it, Niels. Yeah, I can, Yeah, just. Briefly say, yeah, it I guess it's here, we already require sensorimotor displacement to process votes. so at least Monty, that kind of information is local to each learning module. And you could use this to look at your votes and basically see if you're about to move, was one of those votes from somewhere where you're about to go. And then based on whether that vote was from a consistent object or not, decide whether to, decide whether you're, gonna basically be seeing the exact same object you think you're seeing right now or it's something else. and then that could inform prediction.

so yeah, I guess it's a kind of form of model-based attention. so could be complimentary to model figure, free signals and then it's computationally feasible given the information that's local to learning modules and Monty. But it's unclear, that's biologically plausible 'cause we've always been a bit uncomfortable with this kind of center displacement for voting. And and then we also kinda discussed, it's unclear how important it is that the column can make this prediction accurately rather than just incorrectly believing the object will continue. it gets to the whole thing of like, how important is this a temporary mask in the first place? Like it feels intuitive that's what we're doing. That, that we have the sense that, oh, the object stops. And I guess we also discussed some examples of that with like static objects, which relates to this question, but, maybe it's not actually necessary.

So if I understand this idea, you're talking about more of a prediction error between the sensors, right? So if you're trying to, you, trying to predict some feature in, the other sensorimotor and if you're not able to, you're saying this is a different object. no, I think it's simpler than that. Let me see if I can find the slides I had.

Okay, there we go.

So it's basically just the idea that, like this column with, that's getting sensory information from location A is voting with these other columns at B, C, and D. The votes coming from columns at B and C are consistent with what column A is observing and the vote, we call him d. Is inconsistent.

now if column A is about to move, what it will look for is, a vote where the sensorimotor displacement that kind of, that vote is based on, is similar to basically the inverse of the movement that's about to happen.

Okay. And then, it could basically say, okay, I'm about to move where, column B is, and I got a vote from column B. Did it agree with me? If it did, then I can basically conclude, okay, it's part of the same island of agreement, we're seeing the same object. So I can use my own internal reference frame to predict what I'll see at B. So it would still, it wouldn't try and get directly the, like sensory information from B or anything like that. It would basically just say am I likely to be still in the same object? And then I'll use my own internal kind of most likely hypothesis and kind of reference frame to predict what I'll see an I. But then if it was like moving to location D and it did that comparison between the movement and the sensory displacement between these learning modules or columns, it would see, okay, yeah, that it's, again, it's similar, so I need to look at the vote coming from D rather than code. Those come from B. See, it's different. So basically I'm, gonna predict that I'm, off my reference frame. I, don't know what I can predict, but I just know that I'm not going to be seen. Like whatever my current reference frame would predict. So it's like dynamically determining whether we are moving beyond the, island of agreement, but only for that one movement. So it doesn't try and calculate like a, this kind of boundary everywhere all at once.

Okay. So it reli more on sensorimotor displacement than on actual features. yeah. And then I, guess implicitly there's the sensorimotor features and stuff like that comes in with whether these learning modules agree in the first place, but Yeah, exactly. It's not like it's doing a direct comparison with those.

I guess it relies, also on having pretty accurate un noisy votes, to, to use for this. Yeah. I guess maybe it could help, like if you're getting lots of votes, you can maybe pull multiple of them. Like it might not have to, to someone. Have we solved how voting works in general in this scenario? we could just be like, oh, it's not a stapler anymore because our votes are not agreeing anymore. So if Oh, as in like when you have, I think it, when the object, I think it works fine in that it's like. I think we've just maybe always assumed, like there'll always be a sufficiently large subpopulation that, e even though these ones will disagree, it's like all of these would, agree on that. And, I guess also if voting is happening in different sections, then it might be like, oh, okay, yeah, we're definitely seeing a stapler, but we can't all agree on where it is.

but the, but note A would, half of it's incoming votes would say one post and half of it's incoming votes would say another post. Yeah. So like how does it, yeah, votes then. Like they can't be as important as what a itself is sensing, like pose is as important as object ID or, more as in what it's sensing is only consistent with the votes coming from here. So even if it got 50 of votes from each, as long as it's what you call, as long as its own sense of where it is, like fairly significant in that, then it's, gonna be an easy competition.

so basically like the votes are just biasing it, but, it's relying mostly on what it itself thinks.

We're using the incorrect votes to modify the evidence, which may confuse the learning module, because it's getting a lot of wrong votes.

so it might think, oh, I have the stapler, but I only, I have 50 of my votes are saying it's in this post and another 50 saying it's this vote. Vote.

Yeah. I think we definitely would have to modify our current voting algorithm. I'm not saying it's impossible, but Yeah. Like this, we need to explore a bit more. Yeah. No, yeah, I agree. I think our, current learning modules would get confused, like Rami says. Yeah. Yeah. And I think, yeah. At least I was thinking it is just at the moment, like a vote basically has the same amount of influence as like sensory movements within a learning module. I feel like that's part of the issue. all the votes combined, not each vote. Sure.

But yeah, yeah, we can figure out that this is a, this is basically a behavior because I'm getting a lot of votes from the bottom of the state without saying it's disposed. And then we can detect a behavior and then say, figure out a heuristic where we can just exclude all of these votes because we're now in, in this behavior model rather than. Still trying to detect one pose. We're, just basically gonna say, okay, these are all consistent with a different pose, which means this part is moving and we can just exclude all of these votes from modifying the evidence.

Yeah, I feel like I can imagine having a heuristic where it's I'm getting kind of two general themes in the votes. I'm gonna pick the favorite one and then ignore the other one.

but this problem I also like using, that to like model a behavior is maybe a bit complex one. Sorry, what were you saying? Will, yeah, would this problem go away with compositional objects? 'cause then, you'd be ignoring votes for the different sub objects as opposed to different poses of the same object.

Yeah. So once we put the stapler into two parts, that wouldn't be an issue anymore. So yeah, in this setting, what we're, we were hoping to get working is that like you see a stapler, in the same way that like, you see this phone, it's always been like one object. Suddenly half of it splits off and part of it starts moving or whatever. Then you can like instantly be like, oh, okay, I'm gonna segment that and treat that as its own kind of object temporarily, because I haven't had time to learn it as its own model.

But yeah, I, think you're right that once we've had the kind of luxury to, to learn it as like a child object, then it's not as much of an issue.

And like the reason why we wanna apply this mask and keep using the model is because once it starts moving for the first time, it's not like you forget everything you've already learned about the stapler or the phone. You, expect still like the colors on the stapler to remain the same on the stapler top and just move. so like you, you're still obviously using the model of the stapler you learned, but somehow only the features on the part that is moving. Yeah. And like even in your head at a some level, it's, you're thinking, oh this is, a stapler or this is a phone or whatever. It's just it's part of a phone moving for some reason, but that's kinda like almost Yeah. How you would describe it. It's what is it? It's this part of this object I know, which is normally bigger. That's kinda moving.

So maybe we leave this in gray and add another open question about to talk more about how voting works with object behaviors and what we might be able to use voting information for.

Or can it help us inform learning behaviors and masks? Yeah.

As in, so there was the bit we were just talking about, like you raised, about, voting might be problematic more generally, but then, yeah, maybe, does that make sense? Yeah. I guess the only thing with this second one, I just dunno if it's too general. Like I feel like using voting to inform recognition of behavior models, we discussed like up here and then masking is what we discuss here. So my bias would be to leave it just at learning.

Sure. Yeah, I was thinking to put the gray bullet above, underneath this point. but yeah, we can also just remove it in general. I just thought it's worth having another whole meeting, talking about voting and all this stuff. Okay. Yeah, no, I see what you're saying. yeah, maybe we, how did this start? Object segmentation.

just in, so when we revisit this and at some point when not confused, cool. Yeah. So I'll leave that gray. Sorry, what you say. like I, I wonder about whether, what model freeness might give us beyond just attentional masking. if I think about if we had movements, signals that were also available and then two points nearby on the top of the stapler, we'll have, should have the same or very similar movement vectors. From time point to time point. And beyond the intentional, I'm asking that should give you some information that these two points are likely to be viewing the same object. So when they vote on what the object idea is, it feels like that ought to be able to help window down what the object idea is. Because they're, basically priors now that like whatever we're seeing in terms of whatever the object E ID is, if our movement vectors are the same between neighboring lms, then you should wait higher, whatever that vote is for the object id. Yeah. and, but I guess that would happen at the level of the learning modules like output. So as in a particular learning module thinks, okay, I'm seeing stapler moving. 'cause they wouldn't vote on like sensory input. But yeah, I think to your point like yeah, the, there's movement happening and so they would infer that the stapler is moving, And they would be agreeing about what direction and speed or whatever the stapler is moving. And so they're gonna be much more likely to think, okay, yeah, we're seeing the same stapler than Ls. Maybe seeing the stapler not move at all or whatever. Yeah. You basically saying like maybe the. The, model free sensory input already contains enough information, like it's already informative enough to create these potential masks. And then having those potential masks would actually help us solve the voting problem by saying, alright, anything in this kind of model free defined region can vote with each other. Yeah. It's almost like a, a pre prescreening of agreement islands So I don't know, it's not totally clear in my head how they interact, but it feels like there are a lot, could be leveraged with the, with the movement signal. Yeah. And maybe various stages, but yeah. Yeah. I think it's a good idea to look for coherent optic flow or something like that. 'cause that's pretty simple feature, but can't think of an example where it wouldn't come from a, like a object that moves. Yeah. Yeah. So I'll write here maybe. So under more generally, how can we work on subject ordinary, the, model pre doing could help limit payment by ensuring only.

Relevant elements are processing in their one another, something like that.

Yeah. Should we add the coherent optic flow somewhere as a potential bottom up signal? I guess we have regions that are moving, but maybe also moving together as an additional factor. Yeah.

Cool.

yeah, maybe Rami, do you wanna Okay.

could, can, or should this information be communicated outside? the column? I assume we're talking about the mask and would we wanna communicate that outside the call? not really sure. Maybe I'll say a model based, mask. So this is like a, so irrespective of how it's derived. Okay. We don't communicate entire models, so it's unclear, we need to communicate half models. Yeah. I, because this is not talking about an object idea, it's talking about the actual model, all of the associations between features and locations and all of that. Do we want to Yeah.

it may be sufficient in the long term. we will learn a new sub child object morphology, while in the short term we can attend to a particular region.

Yeah.

I, would agree. Probably don't need to, I'm not sure. yeah. Should we make the language stronger? Yeah. I would just remove the last part of the sentence and say We don't communicate whole models, so we don't communicate half models either.

yeah.

like everything up to the dash, I think we could take out there. 'cause the second bullet covers, talks about, what does it, that's more asking how we learn the new child. Yeah. So I feel like it, at least the way I read it, it's clear with the Yeah. But then in the last part of that sentence, we should make sure that while in the short term, we can attend to a particular region that refers to this region being defined inside the column and not, shared.

Oh, I see.

yeah. So while in the short term. We can use masks. You mean kinda like that? Local attentional masks or model free?

Yeah. Maybe I'll make it clear here. We don't communicate, so we don't communicate intentional masks outside of the column.

But like implicit, if we have a model free mask that gets broadly distributed by basically telling which column should be processing the input and which shouldn't. So in a sense, that mask gets broadly applied already. And then this question is more about if there's a top down mask. Yeah. This is asking if a model-based mask should be communicated outside the column. I feel like it'll be through the hierarchy. Maybe the, I think what this is specifically getting at. Oh, you mean the sub component becomes part of a parent model? Yeah. As in so okay, we know it's a stapler top in this column. Is it telling the higher level learning module? By the way, I'm just a stapler top or I'm just part of a stapler. That's what this is asking. That really depends. Are we changing the object ID or we sending the same object id? Because we sending, that's what I was trying to imply here, is that over time you would learn like a slightly different or totally different, a different questionnaire. object id. That's that, child object. But you wouldn't develop that immediately. But then the last part doesn't make sense to me. I feel like in the short term it's just, you just don't have that information at the higher region if we can't communicate the mask.

yeah, I think that was more referring to that, particular learning module that has it. But maybe that's not necessary just to specify by.

Yeah. How would we prevent the higher model from thinking that there's actually two staplers, I guess like substantially I guess with the, with at least the way we have compositionally it doesn't really matter. 'cause that's what happens with the mento logo. the high level learning module thinks there's, if it's like a weird, like it just thinks there's a bunch of new mental logos in some sense. All it knows is there're like, is new mental logo at a particular location. yeah. Remember that the hierarchically arranged, columns or co have co-located receptive fields. So it's not The higher region gets inputs from all of the locations on the stapler. It just gets input also from the top of the stapler where the lower region is sending its input from. Yeah.

yeah. Okay. But yeah, it raises an interesting question, how do we count, how do we count child objects?

yeah. Like how would a high level learning module know, like you say, Scott, like how many ERs there are, which I guess just interesting think about 'cause yeah, we can count objects.

it's like the, why do we need to count the chat objects? Oh, I'm just saying this is now moving beyond anything we've talked about recently, but just as a ability of intelligent systems, at least humans, like we can count things. So it's if an our, if in our, compositional, models, like generally we're we keep laying down points every time we need to, capture the structure. But then it's okay, so we can't just count like how many points there are associated with a particular object at the high level, that will like over count.

so it's, almost yeah, we don't have to talk about this now, but yeah, it's like when you have a point, you're somehow saying, okay, this is now a different instance of this same object, from this other learn point.

Especially if one is, like if you have a, logo that is like bent halfway and you're storing two different poses, I think the high level LM will probably think that these are two different objects, two different child objects. Right.

yeah, we can I guess get to that another point, but Yeah, I think it's a interesting question. You brought up Scott.

was there more general? Oh yeah. I think like another point that talked about tension was, I got the point that maybe you are getting the, learning modules might not need to. store an intentional mask because what if it is already getting sensorimotor information? It is already something that is attended to like it.

so like the attentional mask doesn't need to be represented in the column at all. But it, it's, I guess my mind was more like related to sensorimotor modules and learning module.

yeah, I'm not sure if that just makes sense, but gosh, I remember thinking that okay, like this, maybe this is not like an object behavior thing, but like a generally whether we're trying to recognize an objects like static object or a behavior we are already attending in both cases.

and I think my question was like, can we assume that learning module is already, if it's processing something, it's because we are already like attending to it. so yeah, I think it's a question. I, think I asked it like, do we have attention for, I dunno, like for free in some sense because just by the nature of like we're doing something about it means that we were, but by the prerequisite of that is attending to that, maybe I should just write it down as a question. I just, yeah, I mean my understanding I think is what you described me as Is model free attention sufficient.

because the model for it to be model based, right? Has to be involving the cortical column somehow. yeah, I feel like, yeah, that's maybe one, if I'm understanding right. That's maybe one way I'm wording it, or, yeah. I think the other thing is this kind of gray out question that was here, which is it's unclear how important it is that the column can make this prediction accurately rather than just increasing the believing the object will continue. this basically gets at the question of is what you call it is model-based attention necessary?

Yeah. I guess at least in the sense that we can use the models to guide our attention to a new location, move our attention. Yeah. That's why I said, sorry. Attentional windows. Okay. yeah, yeah. Model-based masking. Yeah.

So we're assuming that any sensory information that goes into a learning module has to be processed. And this is why we're doing the attention outside of the learning module in the thalamus. But I think if I understand Jose's question correctly, if we already have that signal of what we want to, not process, why can't we do it in the learning module directly or, oh, no, what you said in the beginning is what I was saying, but much better. Okay. So yeah.

And then this is, so can we do that attention direct? can we just ignore some of the points that, like some of the input that we're getting from the thalamus, if we already know what the attention mask is and what we want to process and what we don't want to? Or is that, isn't that what we're already doing? But the idea is like either the signal for that is model three or model based. Like it can be based on simple heuristics, like things that are moving together or that there's like a large flat area with some edges and stuff like that. Or it can be model based, oh, I know what, stapler looks like or whatever. So I'm going to focus on the area that the stapler actually occupies based on my hypotheses.

Does that kind of relate to also what Scott brought up of whether already the model free attention mask is it's quite powerful and it might actually solve some of the later processing, like the voting disagreement and stuff, if we just rely on model three signals, for the most part, for this kind of masking.

Yeah. But I'm happy that we added this quick. We, can move on from now. I was just wanna remember that, how do we learn, oh, sorry, I was muted. I think we Yeah. Got to hear. Okay. how do we learn a new child model? does it have entirely new reference frame? I think we did, we covered that already. Yeah. We alluded to, I just turn it into a question because it's, yeah. Which we don't have to discuss now, or I don't know. Yeah. Unless we, I don't think we have any good answers. This is a controversial topic. Yeah. Yeah, I think the bias would be to create a new reference frame because, like Viviane was saying, also, we sometimes we need to recover the whole model and sometimes we don't. we just want to have an intentional, but if we're applying a temporary mask, then we don't need to create a new one. So yeah. I see why this is controversial. Yeah, I know. I think I agree with you about the model. I think it's the SDR r overlap thing that's maybe the, so but also for mask then it means that we're sending the same object id and now the parent object would not know that this isn't, this is just only a part of the object, not the full one.

yeah, that would be my bias as well. Maybe we can just write, having serial overlap would provide the benefit of in the commonality in the child and or child. No, what's, the break off and original feel if we use the term child, that's gonna get confusing.

Break off, make sense? it would be child object, so Yeah. But if, I don't know, At least in this case, like a single learning module might have a model both for the whole stapler and this kind of break off thing. So there's no like hierarchy within that learning module necessarily.

Yeah. There's also view of, having a stapler and then at some point taking the stapler apart, like unscrewing the hinge by unscrew the hinge and pull off the top. It's not like I have to learn a brand new model, of the top of the stapler after I completely separated it from the rest of the stapler. Yeah. Yeah. It seems like we transfer some of the knowledge. Perhaps I would make this gray 'cause I feel like we are not like certain enough about this answer. Sure.

But, yeah, Scott, I think it's, a good point.

I don't know if this is actionable and hopefully this isn't backtracking too much, but I was just thinking about the islands of disagreement and in the case and just the really restricted case of the stapler motion. the actual, if there was more than just disagreement, but disagreement was specific in some way, so whatever that angular distance was or whatever. That would be enough to encode where you are in the sequence, the behavior sequence.

in terms of, as in all of these are green on like a, kinda like sequence identifier or, yeah. Maybe this is, maybe I should just try to think, this might already fit into a model that we already have, but I'm just thinking about the particular, these agree and say this orientation, these agree, say this orientation, step two, these agree, say also a particular orientation relative, let's say relative to this orientation, knowing those the relative orientations of disagreement does in this very small example, define the location within an entire behavioral SE sequence, which would be this, maybe that's already something we already get out of the child, the parent child relationship. So yeah, that, that is probably something that already shakes out of the parent-child relationship actually, now that I think about it. No, I mean I think it's, yeah. Interesting thing about it. Maybe it's something Rodney was asking or suggesting earlier about learning based on the, kind of disagreements and the votes. It just feels to me like voting isn't a great place to represent that because it's, yeah, I think it's. Voting will be used to get the, these two poses and child object IDs, but then to represent it is better to do that in the parent column. to say at this location we have stapler ID at pose X, and at this location we have stapler ID at pose y Versus someone having to learn that out of all the messy votes coming into the child column.

So in that sense we're, it's tying it back to talking about voting on the p, the location within the sequence.

maybe that is better done at the higher level. I don't know. Might be too much at the lower level. yeah, like they could vote on where they are in the sequence, but they wouldn't try and store like Yeah.

how would you know? I don't know, maybe what's the vote on at that lower level, but Yeah, maybe if it's consistent enough, but I guess that's kind of part of like voting is like it works best when they're all consistently have the same thing. Yeah. I don't think it would work with voting because we can't do coordinate transforms and stuff. It wouldn't generalize to new. Like relative orientations and, things like that. So I think that's why we need to do it with the parent-child relationship because there we route things through the thalamus in store, relative orientations in relative locations, in the, parents' objects reference frame.

Yeah. Yeah. yeah, I guess we do coordinate transforms well, but yeah, we can probably get rid of that, question that learn between the, between the agreements. You, would have to learn that, like you can't just do like hypothesis testing between vote agreements, hypothesis between vote agreements like stapler, top post A stapler, bottom post B in the votes are very specific rotations. They're not a relative location to each other. So if you wanna learn that association, you have to learn that association for all the possible, poses the stapler could be in versus if you send both of those poses to the parent object, we store the relative orientation to the parent object.

and we route Yeah. All the, no, I agree. Yeah. We would have to store, you'd have to store for the vote, the relative offset, and then transform it. similar to how we ensure that they're all agreeing on the rotation. Yeah. What would be the point of doing that? Yeah, I think, yeah, we can move on.

oh, so can I ask my question? the, yes. the, so this section, I know it's called objects orientation, but the mostly questions about potential masking, and I realized they're like related, but, does anybody remember like, why we were trying to, model like the break off, like object in the first place?

what, was the like, this is greatly beneficial for learning, compositional objects, so that we have a stapler top and also a stapler, but we have a mechanism for already learning compositional objects. And I guess this can like speed things up. Prob, as in if the seql are top, like truly broke apart, like we don't need to necessarily, explore the entire stapler, top to create that model that it will exist because we have observed some kind of behavior. But, but I guess it doesn't seem like it's helpful, but not necessary.

yeah, it, was trying to, that question of yeah, why, yeah, so with the stapler, a lot of the w times we've discussed behaviors, we'll have the stapler model, and that's what's known. That's the morphology we know. And then suddenly part of the stapler moves. And so we talked about how okay, it would make sense if we what you call it, mask that out and it's okay, it's only part of the stapler that's moving. okay, so that's the connection to behavior. So it's basically like, how do we use a fixed morphology model we've learned in the past to adapt it to be able to learn a behavior once it starts moving.

Okay. So I think that makes sense. When we have LMS that are in the sparse and it's not looking at the entire stapler, we want to know that only the top is moving and not like the whole stapler is rotating together, but only this part is moving. But when we have. Like new example where like learning modules are covering the entire plr morphology, like in your PowerPoint, then we automatically know that only this part is moving because we're not getting, we're not throwing any behaviors from the bottom part. 'cause there's no change in locations.

sorry, you were saying high level learning module. So any, just to clarify, is any learning module in this diagram is only seen part of the stapler, right? yeah. Any learning? Yeah. oh, that's right. Yeah.

yeah. But if we can, if we were to be able to gather, okay, like where are the locations of these learning modules? Because I, I think we can know that by which sensorimotor it is attached to. I'm assuming that there's like a sensorimotor to every learning module and a sensorimotor knows it's location we consolidate. Okay. which of the sensors that were lighted up or attending, then all those points together would form an attentional mask and Right. And then that can be applied to the morphology model to learn the, break off object.

if I'm following you, it sounds yeah, you're describing a way we might do the attentional masking. Yeah, No, I think my question, but it sounds like we still morph into something else need to, but I think, or it could be beneficial if we're trying to use a fixed more model, but if we, do go with a solution that is more general to any kind of object defamations, like not the case where there's like a big chunk of the object all moving together. So that was the idea of the attentional mask, that we can just apply it to that chunk on the stapler and make the same stapler predictions, just rotate the entire stapler model. But if we have total object def affirmations, like the stapler is made out of wax and you can bend it and all this kinda stuff, then that solution, the attentional mask solution doesn't actually help much anymore. Or you would have to make very tiny attentional masks.

so maybe if we do go with a different solution, we wouldn't need them anymore. or we at least, they might still be useful in the sense that we could use the model free attention to the mask to tell us which parts are moving and then use that to inform like what is the sub-component of the object that I want to learn a separate model of. So basically. I'm only gonna pay attention to this part. So only those learning modules are gonna get input who are on those parts. So naturally they will learn a new model for this sub-component on the object. but, we wouldn't use the attentional masks anymore for making predictions about morphology.

Yeah, I guess unless it's like a more efficient, I don't know if the distortion stuff takes more time to learn and model, but I guess maybe if you have ones that you can grab that you before Yeah. if you can use, if you can match existing distortion models to a, new morphology, it would be very quick. But yeah, if you'd have to relearn it and like the, not quick, like the new logo behavior. Yeah. Or yeah. So I guess attentional mask like may not be the criteria. it may not for always breaking, for always creating a child object. otherwise if you had deforming stuff, that would be, it might be creating like infinite, many child objects. it's a good criteria. if something is moving along together, like that's a good candidate, but maybe it shouldn't be like the only necessary, requirement. yeah.

I have another, sorry, I have many questions. Suddenly pop, if we had the mug like, mug with a logo, sorry, I'm not using that right now.

and that was the only mug that ever existed. Would that be a compositional object or just one object? so all the mugs in the world are just TP mugs, like white tp.

will we still, in that case, like there's no point of making like a child object for the logo?

so let's say the logo appears in other places. yeah. I was gonna say let's say other places. Yeah. Yeah. I was gonna say maybe not the logo. Logo is not good because it can appear in other places, but like a completely unique design. I don't mind, but no, but yeah, I think it gets more towards a yeah, general, like questions about unsupervised learning and stuff. But yeah, like one example that I guess relates to this is handles, like handles on mugs are pretty much always associated with mugs, but we also seem to have some sense of that they're like their own thing. And so we maybe learn that through affordances and stuff. yeah, I think there are complex questions about like, how do you basically decompose the world based on, okay, sometimes you can rely on statistical dissociation. Like things will appear consistently on their own, separate from other things. Which your example was breaking that, but even cases where that is broken, like handles always appearing on mugs or nose is always appearing on faces. We still have some way of saying, I'm gonna think of that as an object. I don't know. It's weird. Yeah. Yeah. Okay. But yeah, some mysteries.

cool. we've got seven minutes left. Should I just try and quickly whiz through the rest of the segmentation stuff so we can at least say that's done? Yeah, sure.

Cool. is there more general, attention mechanism? Could the same mechanisms be used for computational objects?

I'll just say the voting ends of agreement mechanism would, in theory work for static objects. but see more general concerns around information required. And then, maybe we can say model three, signals for no masking would work in many situations, I think is a fair statement. And then maybe I'll keep this one gray. Yeah, sounds good.

how can object behaviors be used to we turn an intentional master into its own model? I think we've I might delete this question. I feel like we already covered it with this stuff.

Yeah. We added a question that already existed.

How do we define child objects for continuous substances? Okay. Yeah. This is a good one. like a rubber blue and a cloth or liquid. Do we do so at all? If not, how do we apply our solutions to behavioral modeling?

maybe be later. I feel like this is your proposal. yeah, maybe, you can just say, see, object distorting behavior. see object or distortion maps. Yeah. And then, or models, to make it more consistent. object distortion models. Oh, okay. Yeah. and then maybe say if we do go with a more general object distortion mechanism, we might not need attentional masks to make predictions about morphology.

Yeah.

you want me to write that down? Sure. I was just thinking Yeah. Where we write it, but yeah, no, go ahead.

yeah, and then I guess the only thing though was like the, if a behavior's totally novel that it might still be helpful.

like the, you mean the behaviors, like the exploding logo?

If the behavior's It is a new behavior morphology combination or you don't have a model, a new behavior at all. Yeah, new behavior. Because I feel like that's a totally different scenario where we first have to learn the behavior model. Yeah. But I mean it's one we've been discussing, right?

in the, in terms of applying behavior to morphology, I feel like it's always, we already have a model of the behavior and we already have a model of the morphology. We just have never applied them to each other. Like we have never seen this particular object. Yeah, like that's definitely one question, but I thought it's also a question like how we learn, like even the concept of like stapler opening and closing and stuff like that.

I thought we already have the solution for that part before we learned that.

It just takes some repetition, which is why the logo was difficult to learn because we needed to observe many times to build this new behavior model. Like we couldn't just take an existing behavior model and apply it even though we tried it for some of the components.

Yeah, I guess I'm just wondering whether attention is playing a role when that's happening.

Feels like there was definitely some, but yeah, maybe more model. free or whatever, bottom up. Yeah. I would definitely say attention plays a role there, but I'm not sure if what, where, if we would use attentional masking.

Yeah. Okay. Sorry, I'm rereading what you wrote. Yeah. We wouldn't need attentional mask for making predictions about morphology. I agree with that. Yeah. It wasn't that we don't need attention.

Yeah. Yeah. As I added in brackets at the end too, like we would still want attention for other reasons. Okay. Cool. Cool. and then we have these two other things in a new scene. Yeah. This was just something that kind of Jeff said that I paraphrased, I think could be classified under a kind of feature of attention, which is like in a new scene. Attention may be brought at first and then narrowed down. This doesn't necessarily need to be a simple circle of attention becoming narrower EGFP threshold attention regions by the movement, spatial frequency changes, these regions can be dynamically made smaller, larger as required, still remaining regularly, if that makes sense.

I think it was just an interesting point he brought up in terms of if we come develop attention in Monty, bear in mind.

and then, yeah, there was also this discussion about priming. When you have a specific task, eg find object X, you would already be, using top down attention to bias, which models to update and use the input for. But then, yeah, we discussed how attention may not be the best term for this. It's more priming column for which object to observe doesn't match, an object. It won't try to match, it doesn't match the object. It won't try to match other objects. there's about like it use, it uses Monte terminology. if that primed object Oh, okay. Then it'll not try to consider other hypotheses. It would just go on and say the object isn't here.

Yeah. Maybe say if it doesn't recognize that object, it won't try to recognize other objects or Yeah, or just even it will only try to recognize Yeah, the specific object.

only recognize specific object B essentially blind to other objects.

Okay. Cool. I feel like that was actually like, I know it was only two of those sections, but.

that was still a lot to cover. There's quite a lot of blue.

Yeah, I found that helpful. I would do that again. Okay, cool.