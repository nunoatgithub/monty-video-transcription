today I'm gonna talk about this paper sorting out Lipschitz function approximation. the paper has a couple of motivations. I wanted to start out really big picture, and, I have some slides in this presentation that, are trying to cram a semester's worth of statistical learning theory into a couple slides. And, I did that not to convey that knowledge to my audience necessarily. It was an exercise for myself to just. Actually go back to the basics and make sure I understood it. really high level, the idea is to reduce generalization error and increase adversarial robustness. this is like the high level problem they're trying to tackle. They also talk about some stuff I'm not so familiar with, like wasserstein distance and things like that. I came across this paver because I've been interested in the fusion of, or the intersection of symbolic reasoning, deep learning, solving discrete optimization problems. and up until now I had this impression that deep learning had these glaring weaknesses. Like it couldn't do something fairly straightforward, like sorting a list, despite its pretty impressive capabilities in other areas. so I didn't think that, it could do that. And a buddy of mine was like, actually, yeah, it can, you should check out this paper. So that's how I got here.

All right. time to just, like quickly do a whirlwind of statistical ml, just to get kind of notation on the board.

generally we assume there's some input Domain X, there's an output domain y frequently to like derive theorems. We assume that y the labels are, just binary zero or one. And, not all points in X are equally likely. There's a data generating distribution, DA samples points from X, and there's a labeling function F, which takes points in X and labels them with something in Y for now, the said zero one, there's a loss function and again, like in early basic theorems, you just assume it's zero one loss For simplicity, you have a training set, x s, which consists of this series of two pulls, point X, and then F of X, which is its label. Models, neural networks are just functions. The set of all possible models. It's called the hypothesis class. It's for example, if I'm using resnet 50, the hypothesis class would be the set of all parameter configurations for RESNET 50. A single model is what you try and get through optimization. It's a single lowercase h in that hypothesis class. So training is basically just trying to approximate f the labeling function with some H. So I'm searching through this big stack of hypotheses to find lowercase H and generalization error. this is the overall, error. this is the expected error, the expectation of the loss over D, so like over the whole data generating distribution minus the loss on the training set. So I'm sure that. Most of this is like extremely obvious to you all. But, again, it was just an exercise to go through and make sure I understood some stuff.

going one step deeper, a couple important notions to keep in mind. there's sample complexity. So this is a function actually, these are open intervals, but it's a function from zero one cross zero one to the natural numbers. And the sample complexity just asks, how many samples do I need to be within epsilon of the truth with probability, at least one minus delta. So going back to the generalization error, if I want my, true my LDH, this thing, if I want the expectation of the error, to be within epsilon with high probability, how many samples do I need to guarantee that? Then there's the notion of VC dimension. This is the largest size of a dataset C, which is a subset of x. such that my hypothesis class shatters it and shattering just means I can label it every which way.

to make that concrete, the size of, the restriction of H to C. So this is, all the ways H can label C. I sh I can shatter this thing if the size of the restriction of H to C is just two to the C or you can replace two with the size of the output space y. And, generally speaking, the more parameters you have in your model or your hypothesis space, the larger the VC dimension is gonna be. neural networks with tons and tons of parameters may have a very high VC dimension. It doesn't always correlate with the number of parameters. So in, understanding machine learning by Shale Schwartz and Ben David, which is like my bible for statistical ml, they give the counter example that, even something really simple like a sinusoid function, with this ceiling, non-linearity applied to it. Even this thing has an infinite VC dimension. 'cause you can basically crank up the frequency so that, you hit the training points, with perfect labeling.

Okay. I know I'm babbling a lot about statistical learning theory, but, we're, almost done. I just, I, again, I wanted to do this to contextualize the actual problem that they're trying to solve in this paper. two more ideas. There's the growth function tau with respect to a hypothesis class. so this just asks the question, how many ways can I label this small dataset c with my hypothesis space? And what happens is I change the size of C as I go from a dataset with one point to two points, 10 points. How many functions are there that my hypothesis space can represent?

so you can shuffle symbols around for quite some time. And, Prove the following textbook, generalization error bound the gap in the overall risk versus the, the risk on the training set. This gap is bounded above by this quantity here, and tau h is in the numerator, which depends in fact on the VC dimension. And the number of samples down here is in the denominator. So if I want to decrease my, generalization error and get a tighter bound, I can basically do two things. I can lower the VC dimension of my hypothesis space, perhaps by choosing a simpler model or reducing the number of parameters or, layers or something. Or I can get more samples. So these are two sides of this inequality, but generally we wanna make this as small as possible. Could I ask a question, Ben, on, on your first paragraph up there with that.

Epsilon Delta is, are we supposed to have some kind of intuitive feel for what the shape of Epsilon Delta space is?

An intuitive feel for what that space looks like? Yeah. So you have epsilon within the truth, and then you have a, with a probability of one minus Delta. So I'm just trying to get, what the relationship is between those. You've got this functional relationship. I'm trying to figure out whether that's a sim if I, plotted, M of h as a function of Epsilon and Delta. Does it, is it just some kind of smooth thing or is it, there's all sorts of gotchas in there.

Yeah, fair question. The way I would think about it is, that there's a trade off between Epsilon and Delta. If I just need my risk to be like barely above chance, I can have epsilon is my error term. So if the error can be really high, I can guarantee that with probability almost one. Delta can be really, small. On the other hand, if I want, an error to be really, small, I may have to start sacrificing my probability. So if I want to get a hundred percent accuracy, maybe possible, but with very low probability, I'm gonna find that needle in the haystack. And you can go the other way too, if I relax the probability bound well, I can have perfect accuracy. I can guarantee that as long as I'm allowed to do it, a low percentage of the time.

Okay. I guess maybe I'm anticipating the Lipschitz condition, but with the question I have, just trying to visualize that you're, couchy has a form of trade off, but, the, it's, I'm presuming it's a nonlinear trade off of some sort. it's, so I'm just wondering if there's anything interesting in the shape of that, those that trade off.

It's a good question. I've never stopped to try and plot a sample complexity, bound. So I don't know that I can give you a whole lot more, honestly. Yeah. 'cause when I, see the lift condition, there's usually some.

in other context, some, that the surface is continuous and is locally differentiable and a couple other interesting things. So I was just trying to map that philosophy to what you were showing there. But anyway, why, don't you, yeah, I'll, get there. It's, a little bit different. the lipitz thing is gonna be with respect to the function that you're learning, like the H and the small H and Big H, the actual, neural network that you learn. We're gonna look for something that has a net, like a network, a function that has looks, its property, which is a little different than sample complexity function and the smoothness or whatever of this. Okay. Fair enough.

Okay. promise, last slide on theory. the way I like to think of this to try and make things concrete is by thinking about how many functions I have to search through, or how many samples I need. So if I take some dataset C with, let's say 10 points or whatever, there are gonna be, and if the labels are binary, there's gonna be two to the 10 possible functions, between the two. so this is just a general formula to keep in mind. Now, how many functions can my hypothesis class represent from this little dataset C to zero? One, it depends on the vc, excuse me, the VC dimension of your hypothesis space. And for a neural network with a really large number parameters, the VC dimension can be high. So the question is, doesn't this mean that the generalization error bound, increases? it does. And so there's also an interesting question here of I. Yes, neural networks overfit, but shouldn't they actually do even worse than we see in practice? So just an interesting thing to keep in mind.

And I made this slide to try and tie together two notions. there's the notion that as I add more parameters to my model, the function space that I have to search through gets larger. So that hurts my generalization error. On the other hand, there's what people always refer to as the curse of dimensionality, which has to do with the dimensionality of X. And everyone just knows that as I increase the number of dimensions, I need more and more samples to cover that space better. so how are these ideas related? It's, a good exercise to sit down and walk through it. So here was my thought process. if I disco discretized my space X into bins. So for now, imagine x to just be, like the interval, zero to one on the real numbers. And I want to chop that thing into bins. So let's say I chop into 10 bins, zero to 0.1, 0.1 to 0.2, and so on. the question is, how many samples do I need to construct my dataset? C? obviously if, it's just, one dimensional and need only 10 or, n bins, 10 samples. but if I take that interval zero one and, turn it into a square, so now I've got zero one cross zero one. Now I've got the product of those two kind of. Sets of bins. And that means it's gonna be 10 squared, which is a hundred. And then if I turn that into a cube and scan back and forth, it's gonna go up to 10 to the three or n to the three. And generally speaking, with K bins and dimensionality D the size of this dataset, C, which samples, one point in each bin, this thing scales exponentially. So that means if I want to construct a data set like this, the size of C is exponential in D. And finally, to tie this back to the problem of how many functions there are, that means if I go back up here and construct such a dataset, the number of functions is exponential in the dimensionality of the inputs for a dataset of this type. So the takeaways, basically the number of functions I have to search through is gonna grow in. Two ways. First of all, it's gonna tend to grow in the input dimension and in the number of parameters. So both of these things, will tend to hurt the generalization error bound. So I started this presentation by saying the whole problem that they're trying to tackle in this paper, like one of the main ones is lowering the generalization error. And there are these two problems with generalization error. There's how many parameters you have, and then there's the dimensionality of the inputs. And I just wanted to tie them together as part of the same coin about generalization error. so that's all I have on learning theory. Does anyone want to respond or have like questions about this before I get into the actual paper there? So that's the space. It's exponential in, in, in, two fashions.

but that's, like the total space. if, are there priors that reduce the effective ity of that space that we're gonna be talking about? Or is, is that a question for a different day? I'm going to touch on it right here and I'll tie back.

you're right. So generally, like this is just two to the size of C functions. Okay. How do I restrict? So instead of having to learn every possible function here, I don't have to search through every function. So here are some strategies. One idea is to assume that the labeling function or the target f. Has certain properties. one would be to assume that it's like invariant to certain changes in the input. So we've all talked about in variance recently, obviously the idea would be if I'm trying to detect is there a dog in my image? If we want translation in variance, it shouldn't matter where the dog is in the image. It can appear anywhere. The labeling function should be inva to those types of changes. another one would be maybe the labeling function has a small lipitz constant. So the labeling function is really, from the input domain to zero one. So what's lipitz got to do with this? if we just assume there's like a continuous lipitz function and then we threshold it to get the labels, we can think about the underlying F as having a small lipitz constant. So that also restricts us down from having to learn, all possible functions to like a smaller set of functions. Another one would be to assume that the labeling function only makes use of a subset of the features in X. and then we can try and learn functions that respect these properties.

and I have noted here, remember how many functions, this will help us decrease the size of the function space we have to search through.

Maybe to put a finer point on that, if we go back to the growth function tau, this thing is gonna grow exponentially as long as my hypothesis class shatters C. So if I wanna slow down the growth function, a couple of things can happen. I can either increase m or I can decrease the size of my hypothesis class. there's, a theorem which basically shows that as soon as you exceed the VC dimension of h the growth function goes from exponential to polynomial. And if you have enough samples, then the growth function slows down. Or if I make the VC dimension smaller, the growth function slows down. Which gets back to your point, Kevin, of now I don't have to search through exponentially many functions. Now I have to search through poly mally. Many functions could, I just wanna make one comment. the reason why I was going there is that, there was a problem in computer graphics of, in ray tracing where you're trying to, there are a lot of variables when you're trying to launch rays that how many samples do you fire for anti-aliasing? How much do you do for detecting shadows? How much for detecting, depth of field and there's like end dimensions that you're trying to go across. the critical thing that made Ray tracing work so well was the fact that they found out that they didn't have to increase the number, the sample size with the dimensionality. They just simply scattered the samples across all dimensions independently of each other, and that was sufficient to sample the space. So this is giving something of a bound, more detail from that finding that you're able to actually do something like that, and still, sample that space. So that's what intrigued me about the thing is that's something that came up in the 1980s and, the math was too complex for me to understand at the time, but you've presented a nice, background for that.

It's a very nice tie in. yeah, so as I'm digesting it, it's, I'm just thinking, how many samples do I need so I can, is the question or was the problem, like, how many samples do I need to learn a function that, like shades properly and represents these different aspects of computer graphics properly? Yeah. That, that, that's essentially it. 'cause you, basically, you have to launch enough samples so that, You, simultaneously, minimize like Ailey sink, and all these other aspects, and in including, distributing samples in time so that you can get motion blur. So the worry was that you, you're gonna have to, go up, again, exponential number of samples because, you, you wanna sample each of those dimensions equally, right? And it turned out no, after a certain number of samples, it actually, I, maybe it's that cut point where you said it went from exponential to polynomial. I'm not sure. But the finding was, that no, you just have to launch a sufficient number of samples and the rest of the error converts into a form of noise, which is acceptable. So the, yeah, it somehow, so that's, one, one of the, it was, it just solved this huge dimensionality problem that they're having and made gray tracing practical.

Yeah. It's a very nice tie in. So I'm trying to relate to that, to like the properties of the target function. And there are two ways I could unpack what you're saying or two kind of guesses I have about this. one would be, of all those dimensions like. Motion blur and, like shading. And I have no idea what I'm talking about with computer graphics, but with all of those things, one possibility that would make it tractable is that, some of these dimensions matter a lot more than others. so I can sample some of 'em heavily in some of 'em. I just don't really need that many samples and it doesn't hurt me. Or another, possibility is that, like you said, if they're independent of one another, then I don't have to sample like for two dimensions, all pairs of points in a grid or like all triples of points in a cube. I just have to actually sample each dimension, end times, and then it would, the complexity would be linear in the number of dimensions instead of exponential.

That would make it polynomial, right? But they found out that it actually was more or less linear.

It's a, good tie in. I'm, glad you brought it up. 'cause, it's nice when the ideas here actually feel, tangible and they relate to a real world problem.

Okay.

yeah, the other thing that they talk about in this paper are adversarial examples. I'm not an expert. I'm not even really a beginner in adversarial machine learning. and this seems to be a pretty deep field, and the person who notified me of this paper, studies adversarial L I'm tempted to say this is, through the extremely blurry, probably largely wrong lens of generalization error. But, anyway, the sort of definition. Roughly of an adversarial example means, I can apply a small perturbation. for example, the norm of the noise that I add has to be small. I perturb, the inputs X by a small amount and it changes the decision rule. so basically the label that the model spits out from, in binary classifications zero one, but in multi-class cases, it just means it flips it to the wrong answer.

so naively, not knowing anything about adversarial ml, there, it's part of the generalization error. So all of those examples that you get wrong, some of those are gonna be due to adversarial examples, like cases that look adversarial. Then some of 'em are gonna be cases that, are not, but you just still got them wrong. So all other things equal. If I just make my model more adversarially robust, then theoretically I should decrease my risk.

Another kind of just thing to note is that, from what I can tell, adversarial vulnerability is generally observed in these really large models with a high VC dimension. and not so much in small models like an SVM or something. again, I'm not an expert in this domain, but, my naive read of this problem would be something like, adversarial vulnerability is, a symptom of the fact that you've got a really high VC dimension. And so the generalization error, has a potential to be pretty high. Hey, Ben, by the way, I worked in adversarial AI on machine learning and yeah, you characterized it well. Yeah. that's all correct. And regarding the difference between the vulnerability to the other kind of generalization errors, from I. Misclassification, those sometimes are not directly, eliminated by making it more robust adversarial attacks because those adversarial attacks tend to be more obscure, right? You change pixels and in a random way that's not even notice and you wanna do it in a way that's not noticeable, right? You have a very small absalon on the pixel. Either you do a small absalon on the pixel value, you change, or you do, a few amount of pixels you change. There have been examples with one single pixel can change it.

so those are a bit different in nature than just mistaking a banana for a cap and, so yeah. So you don't necessarily get a better performance by making it more robust actually. What, what happens in. In practice is that in, with these various techniques to making it more robust, typically you lose some classification performance, so there's some trade off, right? Making it robust, but then you lose some of the performance.

Yeah. not to derail into adversarial ml, which is gonna be pretty much speculation for me, but, I've, heard both kind of takes, I've heard that there's a trade off between, your performance and how adversarially robust you are.

but I've heard that in some cases, like surprisingly, making it more robust actually also increases just overall accuracy in some cases. Yeah. Yeah. Depends what the kind of errors you make. Yeah.

Okay. Basically the read that I have on this is that it's just relying on theory here. The number of functions that your hypothesis class contains is extremely large, and that means the number of functions that fit the training data well, but don't fit the test data well, is also extremely large. So the kind of naive interpretation is that I'm just selecting a hypothesis from this hypothesis space that, fits the data. But, it's basically a, I, my guess would've been, it's a type of overfitting. So I don't know if that rings remotely true, but that would be my naive model of what's going on. Oops. Yeah. Yeah, there's, certainly some amount of overfeeding.

Okay. So now, let's actually talk about the paper and let lipitz functions.

here's the definition of a Lipschitz function. my X and my Y both need to be metric spaces. And D is, like DY is a metric and the Y world, and DX is a metric in the X world and a function is Lipschitz's continuous. If there's some constant K, so that, over the whole kind of domain X. this difference, the, distance between F of X one and F of X two is always bounded by this K times the difference in these inputs. So mentally you start shuffling symbols around or okay, I've got DYDX on the left hand side, if I divide both sides by this. so the intuitive way to grasp this for me is just that the slope is less or equal to this constant awe at all times.

So they're trying to learn a neural network that is, that has this lip, its property. 'cause it narrows down the space of functions that you have to choose from. It makes your hypothesis class a lot smaller. And this has benefits including possibly reducing generalization error, providing guarantees about adversarial robustness, and maybe even making it more interpretable.

Okay. a little bit of math. Can I ask one quick question back on that slide?

do we know of problems where we can't guarantee that continuity? Or is it's always possible you just basically try to, construct solutions that do satisfy that property?

No, I think it's the, opposite. It's that, With generally speaking, this is not satisfied. Like when you learn a neural network. I think generally there are no guarantees about lipitz whatsoever and trying to make it satisfied. this is trying to make the function that you learn. Lipschitz's continuous is like a somewhat new line of research. Okay. That's my take. Thanks. Yeah. Okay, thanks.

By the way, Ben, what do you think makes it discontinuous since generally el ship, we're not talking about, small K, right? K could be very large and so it means if it's not lip, it's continuous. There's some discontinuity, right? Some gap, some chunk.

How do we get, how do we get the jump in? Why do you think? Yeah, I mean my, so my intuition here would be that like adversarial examples suggest that functions that are normally learned are not Lipschitz continuous, because a small change in the inputs flips the label. So it could be that you're already just, you're, it's not actually a big change in the, the logs of the network, but because it's like a one heart vector and we just choose one like the best. Yeah. yeah. Okay. Do you agree with that? yeah, that's right on. On the output. Yeah. Since we only choose one One correct sample, one correct class, and we know a small change, at some point it flips the class. yeah. Actually, yeah. Now that I'm remembering, I think there are. Are plenty of examples where, an adversarial perturbation also causes the logs to change a lot. So instead of being like very confident that there, that I'm looking at a dog, the logs can flip to being like really confident that I'm looking at a cat. So I think that's a clear example of not being Lipschitz continuous, or, it means that, K is just really, large. Yeah, K is large. Yeah. In that case it would be continuous, but K is large. But then at some point, if I do the threshold, this is this class, that's where I get the discontinuity. I see. I'm, I'm sorry. You were asking about why do I think it's discontinuous and I don't necessarily think that for like the logins, I'm more just thinking that K is large. Yeah. Okay, good. Thanks.

Okay.

so they just casually mention in this paper, one lipitz functions are closed under composition. this paper took a long time 'cause I had to stop and convince myself of a bunch of things. that also means I ran out of time and didn't get every theory in this paper. But, I've never, taken a class on functional analysis or anything, so I wanted to take this opportunity to try and think through what they mean by this.

generally, this was just my naive reasoning. Here I have two functions, F1 and F two. and let's say they have Lipschitz constant, C one and C two. that means this is just the definition of legits continuity. The left one has a constant C one, the right one has a constant C two. And, if we assume that this distance metric, excuse me, has the property that, in the limit as X one minus X two goes to zero, that, if I divide both sides of this inequality by this distance in the XS and get this in the limit as X one minus X two goes to zero, I recover the derivative of this function. And I think this would be true for some, for a distance metric like absolute value.

So if that is the case, then what that means is I can say, DDX for this function is less al to C one and it's less al to C two for this function. And that means if I compose the functions, by the chain rule, I get the derivative with respect to acts of F two times the derivative with respect to acts of F1. And these are both bounded by C one and C two. So the pro, the composition of Lipschitz's functions, has Lipschitz's constant the product of the, two functions. maybe ko if you or anyone else really, if anyone's familiar with this, you could maybe check me on this and see if this reasoning is accurate or if there's a level in it. Yeah, that's a good reasoning. Yeah. Okay, cool. so if, this bullet point is correct, then if C one and C two are both one, then the composition of one lipitz functions is automatically one lipitz.

and they just mentioned that in the paper. I didn't get it initially, so I thought I would try and I go through and actually, get it. The one lip sheets means that's the K. Is it K equal one? Yes. Is the not patient. Okay. Yes. I used C here I probably should used. Okay, I see upstairs. Yeah. In the first line it says C one Nip. Okay.

So when you mentioned this, function that includes whatever non-linearities in the layer.

everything above this last bullet point, we're not even necessarily talking about a neural network. We're just talking about functions, generally speaking.

so it's the bolded one that, where you're basically saying that you're connoting a MLP layer to being a function in the same with the same properties as you're trying to apply. in the preamble there, is that the case that you consider a, whatever the non-linearity is, you consider a, an MLP layer to be a function. Yep. That's, exactly right. And, you, nailed it. It's to make the whole thing, the whole function, the whole network start to finish. One lipitz. It means that like the weights in each layer need to be one lipitz, but then the nonlinear activation also has to be one lipitz, and they tackle both aspects of that in this paper. Okay.

yeah, so yeah, that's exactly it. So now this problem making the whole network lipitz is boiling down to, I need the weights to be lipitz and I need the activations to be lipitz.

I'll skip that slide for now and come back to it if necessary. so here's what I understood from the next section in the paper. If the weights w so not the activations, but just the weights here. If these are norm constrained, then I have a lipitz function.

the, reason is that having the norm constrained is a stronger condition than being lipitz. So if I have a function that's one lipitz, that doesn't necessarily mean, that the, the slope equals one everywhere. It just means it's less or equal to one everywhere. So the relu activation, for example, that is definitely lipitz almost everywhere. It's not at the single point where you cross zero, but that point has measured zero. So almost everywhere Relu is one lipitz, but on the left side of zero, you, have no change going on. So the derivative is actually zero, and so you're less than one Lipschitzs there. you're zero Lipschitzs on the left side of the real numbers.

if I constrain the norm so that, basically when I multiply by x, the norm of the output, Y is constrained to be one. Now I'm saying that I have to be Lipschitz one exactly one, like everywhere. I can never even be less than one. Does that make sense?

Yeah. Question regarding the definition of Lipschitz. One, does it mean it doesn't mean it, it has to be one. It says the K is one, right? Isn't it like the upper limit? Yeah, exactly. K is the upper limit. Yeah. So that's, it took me a while to parse this, but I think what's going on is that's why Relu is.

It's one lipitz. But if you just looked at the left side of the function between minus infinity and zero, it's zero lipitz on that side. On its own. Yeah. But the whole thing is still one lipitz, right? Exactly. More than one. Yeah. Okay. But if I make the norm so that I multiply X by W and the norm of Y is always fixed to be one, now I've just made it so that the slope has to be, I've made it so that the slope basically has to be exactly one everywhere, not just less than one. But by the way, I'm not quite sure how the definition is. You mentioned for the rail low around zero. It's not, but isn't it, if I take any delta, if I think about function Y or x, right?

For, any delta Y divided by delta X. Right. It's always bounded, right? It's, it doesn't have a derivative. It's not, it's, you cannot derive it at the 0.0. but it's still lip shifts, isn't it? Because it's always bounded, but the slope is discontinuous. Yeah, it is, but it is question how exactly is defined, but isn't it has to be bounded. It doesn't need to be. if, the definition where they were showing where we were trying to approximate a partial derivative with the finite derivative, and it's ambiguous at a point, then it's a singularity in that sense. Yeah. That if the function is the derivative itself, but the criteria, that they were defining there, if you're saying, so if I approach it from the right hand side, I'm gonna have the lip one. If I approach it from the left hand side, I'm Lipschitz zero. So at that point itself, you have a conundrum.

it's continuous in some, in, in some sense, but the derivative is not so what impact that has on Lipschitz continuity is, I guess what the question is.

I think, you raised a good point, Haiko. I was thinking that, it wasn't Lipschitzs at that one point, but actually going back and looking at the definition. The changes in, this and this, if I move this around. Yeah. Seems like it's, It might actually still satisfy this upper bound. So it's like Lipschitz continuous even though it's not like actually differeintial at that point. Yeah. Differential. But it's more, how do you say, stricter? Yeah. yeah, if the f is the rail low, right? Then it is continuous Lipschitz continues. Okay. I think I learned something important just now. Differentiability is a stronger condition than Lipschitz continuity.

Okay.

So back to the idea of making the weights and the activations. one lipitz a stronger condition is that their norm preserving, gradient norm preserving. But wait a minute, if their norm preserving, that means the slope is just one all the time. And doesn't that mean I just have a linear function now? so now they have a couple theorems about this. They say that if I have a whole function, a neural network from RN to R, and the weights are always too norm constrained to be less or equal to one.

then the norm of the gradient, is one almost everywhere. Then the whole function F is linear.

It's a little bit to unpack, but I think the general idea here is that, as I add these constraints that like the norm and the gradient norm, therefore, have to be not just lipitz, but actually exactly.

actually I'm not sure they use or equal to here, but for the gradient, they say the gradient has to be exactly one.

But I think the, this is not obvious they prove Ethereum about this, but I think the high level intuitive view is that, these constraints basically take your network and cause the nonlinearity to disappear.

okay. So next, this is quite dense, I'm realizing. They prove another theorem. I'll just read it to be, keep everyone on the same page. So we have this network from our end to R and it's built with matrix two norm constrained weights and the gradient is always equal to one. then without changing the actual computation of the neural network, I can replace all of my Ws, with some w till day whose singular values are all equal to one. And when I read this, I was wait a minute, if the singular values are all equal to one, that tells me it's stretching and squishing space equally in all the different directions that w is warping space. And, I guess they actually, they show something similar. in the proof. They basically say that depending on the dimensions, M and K, you either get, ortho normal rows or columns. And when they're the same, then w is, orthogonal.

and so I guess indeed that means that, the rows are all warping the, space in different directions.

Yeah. I'm not sure I really get what's going on here, but if you blur your eyes a little bit, it seems like it makes sense.

Okay.

Now on to activations. So I guess the whole point of all these theorems is that, first of all, instead of searching over WI can search over W with singular values that have this pro, the class of matrices with singular values all equal to one. And, this helps with searching over Ws. And then for activation functions, they have a couple of notes. First of all, they note that most activation functions like tan and Relu. Are already one lipitz, or if they're not, you can do some scaling to make them one lipitz.

so Relu is lipitz, but it's not norm preserving because when you're on the left side of zero, the gradient goes to zero, and so it doesn't preserve the gradient norm. So the whole function is no longer gonna satisfy that gradient, norm preservation property. Maybe a question here might be something I missed. Why do they care about this non preserving as opposed to just one Lipschitz?

Yeah. My, my guess about that is that norm preserving is a stronger condition than one litz. So yeah, it is. but why, do we care about it?

Because it means that if I'm trying to learn a lips function, if I can restrict my Ws to norm preserving Ws, I have a smaller space of Ws to search over.

is that the motivation here? You just wanna limit the, search space? and it's also, actually I don't know that's really the motivation. I think it may just be hard to guarantee that the whole function is one lipitz. So you might consider several strategies. And the strategy to ensure that it's one lipitz here is to focus on norm preserving Ws. Yeah. Okay. Interesting. Maybe the concern is you could have, Some parts, right? Of if you think about deep net right? It, collapses to almost zero w and then it, you would, so at another layer need to blow it up, right? They probably wanna avoid that. This way it's nicely if, you go deeper, right? It nicely stays the same norm.

Maybe that's the reason they do talk about that later, that because the norm is preserved, then the gradient norm is preserved. I think that follows. Yeah. But if the gradient norm is preserved, then you can stack as many layers as you like and you won't have vanishing or exploding gradients.

so this seemed like a bit of a conundrum to me. It's like you're looking for a non-linear activation function that's norm preserving. So the slope is one everywhere, but then you're just it's linear. Isn't that just a linear function? So what's going on here?

Okay. here's how I interpreted this. First of all, actually no, let's go to the next slide. so this is a nonlinear activation function that they introduce in this paper. And, each cell here is a neuron, and what they're doing is they group these neurons into clusters of, say, five in this example. So here's five neurons, here's five neurons. And in each of these windows, they just sort the neurons based on their pre activations. So this is still not intuitive because when I saw this, I just thought you're sorting. And sorting can be like for any inputs that look like this and outputs that look like this, I can do that with a permutation matrix. So am I not just multiplying by a permutation matrix, making the entire thing linear once again.

and that's where the kind of almost everywhere thing seems like it might matter. they do show that they're able to learn functions like absolute value, or, chopping it up into Ws. And it seems like the network basically has this one linear function over here and it doesn't have to worry about this one point. It can have another linear function over here. maybe now it's also a good time to. Look at the actual paper.

Let me make my participant be a little smaller.

Okay. Running short on time.

So they try and provide some intuition for like why this is a non-linearity in the appendix. Pretty interesting. Still, not really sure I get it yet, but, they show, so min max min, this is when the window size is two. So I just look at, two neurons and I sort them. That's all I'm doing. They show that this is in fact the same thing as applying this relu versus minus relu, minus x, this composition, this like mix of non-linear activation functions. So they try and demonstrate that flipping my x's around basically can recover relu functions and different variations on relu. They also show that it can do the same thing as the absolute value function.

I'll be honest, I just, I've looked at this for a little while. I didn't quite get it.

Seems like it's correct, though. They can learn the absolute value. it can be the same thing as absolute value, non-linear activation. The other way to get some intuition about what's going on is, they train the network to learn one dimensional functions. And so this just means instead of a vector, I just have a single number as input to my network, as a way of diagnosing what's happening. So here are the inputs, and if I have a network with just two hidden units, then these are what my two hidden units are doing. And if I sort them, you can see that on the right. Basically the green and the blue, the two units flip on the left hand side. So I get this, and with this network with two. With one hidden layer of only two units and a one dimensional input, they're able to read out in the output, something that looks exactly like the absolute value function.

I just make a quick observation here. it's, they jing the rail because the fact that it's zero in one half space of the inputs, have they just simply split the problem into two half spaces. One, dealing with the negative inputs and one dealing with positive inputs.

that, minus rail of negative X. So if X is negative, that makes it positive, that means it's positive, but then you give it a negative value, so you're just displacing it. And, so have they basically made a bilateral relu? is that what's happening here?

Honestly, I, wish I knew Kevin.

you're talking about this equation right here? Yeah.

So that, that will admit to both, positive and negative values of X, right? Yeah. let's just walk through it. If X is positive, then X is gonna be greater than zero, and that means X is gonna be here and zero is gonna be here. So what's happening, X is positive. This is just X minus X is negative now. So that means this part goes to zero, so that makes sense. Now, if X is negative, then relu of a negative number is zero. So now I've switched zero to be on top and relu of.

Minus. Minus. Now I basically just get X here and then I apply the minus sign to it. So I get, Hey, wait a minute. Negative value. Yeah, you're right. but X was, no, X was negative in the first place. So then it becomes positive, oh, you're right. You apply rally to it and then, you basically, but you could say it's ex again, right? Yeah, it is. Exactly.

So it's, like they, I don't know if it's the function of the sorting or what, but at least in this kind of toy example, they've split the problem into two pieces and it looks like they're dealing with them independently.

I don't think they implemented it this way. I think what they're doing here is they're trying to show that this swapping of things around and sorting a couple of elements is like equivalent to some. It's like similar to other non-linear activations, okay?

But it's, if you start out with X, now you're going to, you go to a space where you, now you have two x, you've doubled the dimensionality. So in that sense, in what? In, the, in, in the upper one, you're dealing with one half space of X and the other one you're dealing with the other half space of X. So you, you've doubled the dimensionality, excuse me. dimensionality is the wrong way of saying it, but you've, you've double, you've gone from a scaler to a two vector. In this particular case, I'm not sure I actually follow that. You've gone, you have gone exactly from a two vector to a two vector. you start with X, right? If you just had X and did ue, you'd get, a single value, right? You wouldn't get, you wouldn't get a, column vector. But in this case, you're, getting two outputs for every one in I think I see what you're saying. you don't, they're trying to show that if I apply this function to this vector, x and zero. It is the same thing as applying to non-linear activations to just X. Okay. I, misunderstood. I'm sorry. Okay. No, that's fine. it's good to pick this apart 'cause, it's, honestly, it's pretty confusing. I'm still not totally reconciled with the idea that you're basically just applying a permutation matrix, as an activation function and Yeah, it's, I still don't really get it, honestly. But, the permutation matrix seems interesting. Assume that the sort order is constant right? But you don't necessarily, for each individual thing, if you're doing a real sort, that, that permutation maker is gonna vary depending upon what, the input is in order to get things sorted. So it's not a constant function. Oh, may. Is that the idea that, basically it's a different, it's non-linear because it's basically just a different permutation matrix at every time step. Yeah. It's, it is almost like K winners. you're sorting the thing, you're, breaking it up into parts of groups and then sorting each one of those things and they have a canonical order based upon, the rank. I'm not sure what happens after that, but that's at least, so it's, if you had a static permutation matrix, you'd be exactly correct. But because it's a sort function that's not the case. You're using rank statistics on those pieces.

Thanks, Kevin. You could document it as such, but it change each time you change your input.

Yeah, I just, I don't think I quite got that until now. I'll have to go back and make sure it really makes sense when I really get into it, but I think initially that seems right. 'cause that seem right to you, Ko. Yeah, I don't have a good intuition yet about how this sorting works.

Okay. we're actually already over time, so let me just try and summarize now some results and ideas.

pretty much every machine learning paper that gets accepted somewhere, they had to do experiments that show that their method works well in some cases. So they do various experiments on wasserstein distance and, looking at the wasserstein distance between samples generated by gans and like a real dataset and their method does really well. they're able to learn those non-linear functions like absolute value or w so they can learn zigzags and things like that with this. So it's pretty interesting. Going back to my original kind of curiosity about being able to solve discreet optimization problems. this is an interesting way that deep learning might actually be able to get a handhold, or a foothold on that because now you're sorting, you're, the non-linear activation is sorting. So it seems like you could probably sort a list using this technique. Like you said, Kevin, this relates to relu and K winners 'cause K winners just, you're sorting and then, changing the threshold so that exactly K units are on at any time. So the relationship of group sort to K winners is interesting.

throwing a lot of stuff out there, but, I had never heard of this dynamical isometry property. had you guys, no.

I think what dynamical isometry means to look more closely at the definition to be sure, but it's, something about having the weights of the network all norm preserved, norm preserving, and that means gradients are norm preserving. And that means as you take, optimization steps, the network continues to be norm preserving. So that's what I, think it means. And apparently one paper showed that if you initialize the network so that the weights are norm preserving, this can radically speed up convergence and training. And I had not heard that until I looked at the discussion in this paper. And so that seemed like a pretty cool, interesting thread to pull on. Like you said, heco. if the gradient norm is one, then you can stack as many layers as you like. So no vanishing gradients.

Finally, they also prove, something pretty interesting about being able to, approximate arbitrary one Lipschitzs functions using this method. So it, the math is a little involved. It gets into function spaces and like the wire strau stone theorem, which, I have never looked at until I saw this paper. But anyway, these are just the interesting kind of threads to pull on that this paper alerted me to. Interesting. Cool. did they show it on more like some complicated function, like non linear, your function or? they can learn it, pull up the paper and look through it.

no, but this is actually, this is just estimating the wire stress distance between, like the real data and GaN generated data. Yeah, did, okay. So they did mne, right? And then Yeah, you, no, you're right. They said they did M nest and ImageNet and cfar. I understand. Cfi. I thought so, but I think they buried it somewhere in the appendix.

It faces. Okay.

So I have a quick question. so we have this conundrum where it says the slope is one everywhere. if you're in two dimensions, that would be somewhat constraining. But what if you're, how do you define the slope when you're in multiple dimensions? are you, is it. you look at, I'm just wondering if what they're doing is by making, requiring things to be, ortho normal, whether they're sitting on some kind of hyper sphere where the local slope is one everywhere. But does it mean that in every dimension it projects to being one? I'm just wondering if that's the degree of freedom that they're allowing themselves.

Do you have a good intuition for that ko? No. Yeah, that, that's a good point, Karen.

I don't know at this point to think about it. if I run with that for a second, and if you basically look at, their con, their, condition for initializing the weight to be. Orthogonal, if you wish, you've collapsed the dimensionality down. So now you're sitting on a, a manifold that's ortho normal as opposed to all the various other combinations in. So maybe it has really good properties that you're starting out, making sure that each of the columns of your matrix are pointing in different directions and that they're actually capturing some independent aspect of, your input.

Yeah. For, so, the metrics is set it essentially if, let's say if it would be a square, metrics is, all the values are all one, right? And it's like identity metrics. If the are all one, then it's, nway is degenerate.

yeah. It's just a rotation. Yeah.

So that's clearly you've cut down the dimensionality if that's the case.

But it's, yeah. It's not gonna be just, a rotation if, the dimension, if the matrix is not square, 'cause then the singular val, it's only that case because singular values are the same as eigen values when it's a square matrix. Yeah. I think the singular values are different when it's not square. Yeah. yeah. The definition of singular in that case, I, I. You've gotta go to what, pseudo inverse or something like that in order to make sense of that. I think that's right. And so I think, yeah. Yeah. It seems right. Like then you get Yeah. Somebody to dump, project it and rotate. Yeah. Something like that.

But it's an interesting space to play in if in fact they found some more principled way of assigning the, weight.

and then you have the interesting notion of, okay, what if your weights are sparse? What's the implication then?

Okay. You're projecting a different subspace, maybe. I don't know. it's, the matrix itself is going to be singular, maybe depending, I don't know. No, that's, not true. I'll take that one back. But it's, it would be interesting to work through the thing of saying, okay, you, you create this dense matrix. It's got these properties that they claim is really good. Now how can you add zeros to that and still preserve these nice properties that they claim are, you wanna preserve? is there some kind of interesting. That you can say how we assign our zeros to continue to get these nice properties. Yeah. I think if you think about in terms of the square mattresses, so you then essentially have rotations and to get zeros, and once then you have like only 90 degree different rotations. So it would be a very limited set of rotations. it's al it's almost like you're projecting into subspaces then when you put the zeros there. Yeah. so your collapsing, can it also be permutation matrices? Those would have egen values of one and they can be square. Yeah.

Yeah.

okay. did you, understand at the end what the Wasser scene distance is about?

no. That's gonna be. But yeah, that's, yeah. Boin distance is optimal transport. So typically it's the boan stand distance between two prob probability distributions. And it measures instead of like you probably know, the colberg label divergence measures the distance between the distributions.

so for the wasserstein distance, it's like the, amount of transport, let's say these are the two distributions, the amount of transport it takes to make them overlap as much overlap as possible. the other name for it is the earth mover distance. So if you imagine a bulldozer kind of, pushing values from one bin to another bin, that's how much, energy does it take to, to make the one distribution match the other one? Yeah. And, the nice thing is, We worked with buses stand distance in the past. So the nice thing is for this, compared to Berg Lab blood diversions is you have a, very good distance measure, even if they're very far apart from each other, You have a nice gradient for the dis for the distance measure and otherwise ready for two go functions. They're far apart, the overlap is almost zero and you get very bad gradients. And so for something where you need the distance between distributions, vasel, stein distance is nice, nu for numerical reasons.

nice. Yeah. Ben, regarding the, results, what were the results on the cci, far on the faces? What did they report compared to other methods? Go back to it. I actually didn't look at it. table five at the very end of the appendix? Yes. On table five. Okay. So they have.

This is mde. Yeah. Okay.

The, which one is theirs?

It's gonna be C because, okay. So I didn't even talk about this technique in the paper, but, the technique they use to actually make the weights not the activations and norm preserving, is they use this thing called the Bjork something or other algorithm, which does like this iterative procedure to find the closest, orthogonal matrix to a given matrix. So they use this to try and find, weights so that then, so the last column would be theirs. Oh, everything with Min Max is theirs. That's the comparison, I think. Yeah. Both of these, both of these columns are theirs and they, this is the one where they used orthogonal weights as well as Max Min. Oh, okay. Okay.

And they get, and these are the test errors. Okay. So sort comparable with relo, slightly worse.

yeah, they compare against the resnet. So did they actually use it within the, like a resnet architecture?

I don't think so. I think they, they said that they just worried about, like multilayer perceptions in this paper. And they have a couple caveats that like you can extend it to more complicated. Table six is talking about wide resonance. Yeah. I'm looking at six I don't know. Maybe then what? Yeah. because what I was wondering, with the, avoiding the weight decay, do you need the, residual layers? Can you just do that standard, the standard linear, linear layers instead of the residual connections? Yeah.

I'm not sure. Let me jump back to the point where they said this.

Okay. So they, turn the convolution into the typical thing, turning into a matrix.

I, I honestly didn't read the results section of this paper that carefully. yeah, that's okay.

Yeah. honestly, it would probably take a few minutes to figure out what the results were and we're already 20 minutes over, Yeah. Okay. Cool. Thanks, Ben. thanks guys. I definitely learned a few things from your, comments today, so I appreciate the input. sure. All right. Yeah. Wish you both have a nice weekend. See you. Thank you. You too. Likewise. Thanks.