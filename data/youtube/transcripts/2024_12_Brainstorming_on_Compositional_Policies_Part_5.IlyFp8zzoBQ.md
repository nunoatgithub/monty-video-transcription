Okay. Um, so, yeah, really, basically I was, Over the Thanksgiving holidays, just doing nothing and then thinking about the stuff on the plane and had a couple of ideas, several things we talked about before, and they're not super new ideas, but I thought it was pretty relevant to what we talked about in the last meetings. Um, kind of talking a bit more about object behaviors. So I just kind of wanted to put it all in one place. And most slides are actually more about kind of framing the problem, uh, instead of the concrete solution. And then towards the end, I have a couple of more speculative ideas about how the brain might be implementing this. Um, and we can do more brainstorming or have like a shared whiteboard or something. Um, yeah, just, uh, just to remind you, I. I think I wrote about this in Thousand Brains, it's like, I've always found that if you completely describe the problem, then the answer is obvious. So to me, describing the problem accurately is the most important thing. And so, no, don't have to give excuses about that. Yeah, hopefully this, this, these few slides will help with that, kind of spark some interesting discussions and ideas in all of you. So yeah, why talk about object behaviors if the last weeks we talked more about like policies and decomposing policies and goal states and stuff like that. Um, but I think, uh, all these policies we talked about. They aim to change an object from one state into another, or aim to change the world from one state into another. And how an object changes from one state into another is modeled as an object behavior. So I think to understand policies, we first need to understand how object behaviors are modeled. Or at least if we understand that, it will be a lot easier to talk concretely about policies and what exactly learning modules might be communicating to achieve goals.

So I thought I'll start with just kind of defining what is an object behavior. Or how would we represent an object behavior? Uh, and this is just my stab at that. So, um, I would describe it as a sequence or space of states that can be navigated through actions or time. So, for example, the stapler can be in several different states. Okay, how did you make that animation? I, I downloaded them, credited them. I was gonna say, you got too much time in your hand if you made that one, that's pretty cute. No, I just, uh, got them. Um, but yeah, basically it can be open, it can be closed, it can be in several states in between, and we have kind of a model of how it can navigate. from between these states so it can't just randomly jump from open to close it has to go through several intermediate states and we also have a model of how actions can move the stapler through the state space and we have other ones where we're actually not changing the locations of the object but actually features so for instance turning the lamp on or off then we have states that have a bit more of a continuous space that you can navigate through like a joystick Where maybe you even need some path integration properties to kind of model how you can navigate through that state space. Um, and then we have, uh, behaviors that aren't necessarily affected by your own actions or that just kind of change over time.

Um, and then we have like very complex, uh, behaviors of compositional objects, like a human running around. Um, but I would say all of them can be described as like a set of states. And kind of rules of how you can navigate from one state to another state, whether it's just by the passage of time or actions that you exert on that object or actions someone else exerts on that object. Um, sorry, I'm just curious, what website did you use to get these? These are really nice. They're even like, Um, yeah, I tried to get like consistent colorings, but the free pick, they have like thousands of these little animated. Oh, okay. But so this is, this is through your selection process that they are, they match like, okay. Yeah, I couldn't get it for all of them. Like these two, I didn't find like a bluish version, but yeah, some of them worked out. I see. The little Newton's cradle looks like, uh, looks like it's almost like the old Memento logo, you know? Yeah, of course it's not operating properly. The middle two should be close enough.

Yeah. Yeah. Um, so yeah, a couple more requirements. So behaviors should be transferable to objects with similar morphology. So if I learned how one stapler works, how this one works, I should be able to apply that behavior model to other staplers that have slightly different shapes or color or even a bit of a different concept here. Or even like something I've never seen. I'll go even further to say like, you know, these behaviors are transferable to. But the object it's, it's transferred to may not have similar morphology, or at least much of it may not have similar morphology. Like, yeah, anything with a hinge, which might have a very, very different morphology, but it has some attribute of the same, there's some part of the object is similar. Yeah, behavior applies to that part. Yeah, so here's this kind of example of, like, the behavior of running, and you can apply it to a totally different object, uh, like a banana. Um, I mean, you kind of have to add some, um, which is what I mean with a similar morphology, because you have to have these parts that are actually moving that define that behavior, but um Yes, almost like a correspondence in morphology, like as long as there's some subset of the graphs for the morphology that kind of can map on to one another. Right, like a hinge could be, anything could have a hinge, but there could be multiple hinges or yeah, as long as the part that looks hinge like is there. Yeah, exactly. Um, so yeah, I didn't mean to say that you can only apply that behavior to other objects of that class of objects like staplers, but you could also apply it to like totally different objects. You could imagine a banana shape, a banana stapler, and you can imagine or recognize totally new combinations. Like you may have never seen a running banana, but you can Now you see it and you could easily identify that it's a running banana. Um, so, yeah, that's the one behavior should be transferable to objects. And then also one object can have multiple behaviors, like, uh, a human or a banana, can be dancing and walking and running. And, uh, it always stays that object, it's always a banana, but it has all these different behaviors.

Um, so, yeah. These two requirements kind of lead me to say that behavior models are separate from feature and morphology models, in the sense that similar to how we can have an object with this, like, objects with the same morphology but different features on them, like a coffee mug can have tons of different patterns on it or logos. Similarly, we could have behavior models that can be mixed and matched with different feature and morphology models. And that kind of, uh, implies that different morphological models need to be mappable to the same behavior space, or at least a range of them. So, for example, if we say an object has a unique, uh, location space, or the locations are unique to that object, the behavior model needs to somehow be able to map onto all of these different location spaces of, of different objects.

Um, and this one I'm just kind of throwing out here, this is a bit, just something I was thinking, I don't know if you would agree with that, but similar to how we kind of said that morphology is more important than features, um, and that I would maybe make the claim that behavior is even more important than morphology, like, Often we define objects by their behavior, like chairs don't necessarily share a lot of morphology, but the way you interact with them kind of, I think, if you, if you use a slightly different language might be better, the language I would say here is which, which of these might have precedence in sort of a processing learning model, right? So that may be one way to look at it, you know, what's more important, it's hard to say what's more important, but which has, you know, which, If you see something that has, you know, that has a, I'm trying to imagine in my head, but I, I, I, I might, I might agree if I thought about it more that behaviors, if I recognize there's a potential behavior in an object that that might have precedence over, over, uh, I mean, more processing precedence or, you know, attention precedence over something that says, Oh, it looks similar to something else. Um, yeah. One example. I was thinking there was like, you can animate, um, a totally in, usually inanimate object, like a lamp in the Pixar logo, and it suddenly becomes something totally different. Like you give it like a personality and you can kind of predict what it's going to do next and stuff like that. And I mean, the lamp in general is, I feel like a good example. Lamps can have absolutely different shapes, but what makes all of them lamps is that you can press a button and the light turns on. Um, So yeah, that's why I was kind of throwing this out there. I'm not sure if it makes, makes even a difference. It's just something I was thinking. I think it's, yeah, I think it's a nice way of, yeah, framing it just in terms of like, um, yeah, certainly in terms of what we care about as agents that interact in the world is generally the behavior of things more than the morphology, just because that is what impacts their affordances or how we can interact with them, how we can change the state of the world. Um, so it's like, yeah, in the same way that we're not just like a classification system or whatever. So, like, that's why morphology doesn't matter as much. Right, right. I agree. You know, um, a few weeks ago I was talking about these topics and I made a proposal which is just slightly variation from what you proposed here. I was suggesting that the way to think about this is you have, um, models or location and features. Right? And, uh, features at locations, in that both, um, you know, uh, that you can, you could describe all the things we talk about by saying, Any one of these things can change. The features at a location can change, or the location of a feature can change, and, and so the entire model can change, uh, but, uh, by just mixing those two, those two combinations, right? So, you know, the, the feature could be like a red light turns to a green light, or, um, or, or the stapler is one part of the stapler moves to a new location relative to the stapler. So in a little bit more general say, there's really only two things that that you basically have features and locations, and any combination of that can change, um, suddenly or smoothly, um, and that describes the entire space of, of objects, um, of a particular object. So I, I guess I'm saying that instead of saying, um, morphology is more, you know, is less important to behavior, I, I think models can, can, as long as we agree it's the same model, The same object, that anything can change in that basic simple idea that, you know, any of the combinations of the model can change. I can give examples of where features move, where features change, where orientation of features change. Um, and so I can get rid of sort of the morphology idea. It's just like locations and features and any combination of those can change. We can describe, um, all the different things we're talking about here. Um, that way. It's, it's a little bit, um, I think it's a little higher level and it's only really, it's a different, slightly different way of saying the same thing that you're saying, but it feels a little bit more general purpose.

Yeah, I think that's a, that's a good way of putting it, like, um, while, like, if you have one behavior of an object, All of the features and all of the locations could be changing by, while going through those sequences of states. Right. Or maybe it's not even a sequence, maybe it's just sudden, you know? Yeah. The object could be, you know, the light could be on or off, so it may not be a sequence in between, or, um, you know, it can suddenly pop into a different shape. Anyway, I think it's just, I've kind of moved, I'm trying to move away from this concept of morphology, um, As a separate thing. And it's really just like you've got locations and features and any com, any of those things can change in any way. And, and the features have poses and, you know, features opposing locations. So it would be simpler than what you proposed here. It'd be a, a super set.

Yeah. Yeah. And when I'm talking about morphology, I basically mean just the model of, uh, location and locations and orientations relative to each other. Right. I, I, I know that, but I'm saying you could. You could just replace the word morphology with great other features and just say we have models and, um, and by, um, and I just define what a model is and then, um, anything which changes that model, any of the attributes would be it. Um, could be considered, um, behavior, or it could be, um, you know, states of the object. It's all the same. Yeah, yeah, yeah, I would agree with that. Yeah. Yeah. I was always bothered by the, even though I was using it for quite some time, this idea of a morphology model just didn't seem right to me. So I think we can just generalize it to say, well, Mythology is a word we use sometimes, but it's, it's, it's, it's not necessary, actually.

Yeah, um, yeah, I'll keep using the word in these slides, but No, of course, of course. Yeah, uh, yeah, definitely understand the point. Yeah, um, okay, yeah, last kind of slide to recap the kind of outline and requirements. So Key ideas I'm kind of proposing or repeating here. I know we talked about this many times before. Um, so, that there are three types of models. Feature, morphology, and behavior. Uh, feature and morphology can be in different states. So, state conditions, Which features will be observed at what locations? So depending on which state the object is in, the, like you just said, you might see totally different features and they might be totally different locations than if the object were in a different state.

Um, and then the behavior model represents how to transition between different states of the feature and morphology models. Um, and I just put this here, I'm not sure about it yet, that it would be nice if the behavior model would have path integration properties as well. Yeah, I agree with, I agree with that. Um, in fact, remember when I was talking about how the behavioral model might be in upper layers? And, um, I felt the same way, because when you want to, when you want to, um, generate a behavior, you want to, you want to move some object from one state to another, um, it's just like moving your body from one location to another, right? You want to be able to do that on a path you've never actually traversed before, right? You want to be able to say, oh, how do I get from point A to point B, even if I've never done that before. And I felt that would be a similar property you'd want to have in behaviors, um, where you sit, right? Yeah, I think the reason I'm saying I'm not sure about this is because I was having a very hard time coming up with an example of like a behavior where you would have never observed a certain transition because it seems like there's only a limited number of behaviors and we usually have a lot of exposure to them. Like even the joystick example. You could theoretically, like if we can't find a mechanism that can do this with path integration properties, you could theoretically learn a detailed transition model of every possible transition of the joystick. Um, and then when you see an object that has similar properties like your arm, uh, limbs are similar to a joystick, you can just apply the same behavior model to it. Like, I couldn't think of one example where there's like a totally new behavior that I haven't seen on a different object. I couldn't just apply it to, yeah, I feel like kind of the, what I feel with path integration, it's more like, yeah, every edge needs to be observed, but a particular path does not need to have been observed. So that's the difference between just memorizing a sequence. So if you've observed every edge and then you could follow a novel path using those edges. But of course, if you've never observed an edge, then you don't know that it exists and you don't know you can use it. So the only reason, the only, you know, it's interesting, I see your point that, and I'll have to spend a little bit of time thinking, see if I can come up with a counterexample, but right, sitting here right now I can't. The reason I propose that path integration might work is more of an elegance argument, um, so that could be wrong. The elegance argument is that, oh wow, it looks like you've got these, motion sensitive cells in the upper layers and the motion sensitive cells in the lower layers. The ones in the upper layer seems to be object movement and the lower layer seems to be moving through the world, sensors to the world. And so if, if you, I just said, oh well, let's on the assumption assume that the same mechanisms are at play. And so if we have grid cell like mechanisms in the lower layers, we could have grid cell like mechanism in the upper layers. And then path integration might, might come for free, it might be part of what we know that grid cells do that. So that, my, my argument was more of an elegance and simplicity of design argument, but those are often wrong. So I think your point of that, you have trouble coming up with an example is pretty poignant. So maybe I'll try to think of, see if I can come up with some counter examples, but I'll accept your point for now. I feel like a general example might just be like, if you imagine. You have like two states, like an intermediary state and a final state. Um, let's say it's, yeah, I don't know, filling coffee in the machine and then like powering it on or something. I don't know. Then you might have learned multiple edges or experienced multiple edges into the first one and then multiple edges between that one and the next one. And so I feel like this is where like that sequence of two edges, you may have never traversed, but, uh, you'll have. Experienced every edge, and so you can combine them in novel ways, like, so the point is like, I guess it's different from just like, memorizing the whole sequence, because of course, as the sequence becomes longer, then like, there's this huge space you could explore, which, if you have to like, visit the whole sequence, Um, you'd be more constrained in your behavior, or like, if you have to have experience, the whole sequence. Yeah, I can think of examples of solving problems where we're doing it in a very novel way, uh, a way I've never done it before, perhaps, maybe you can argue otherwise. Imagine I just want to pour coffee into a coffee cup, and, um, For whatever reason, um, uh, there's a, something at the top of the pot, coffee pot is, is blocked or something. And then I say, oh, look, there's a little drain hole on the bottom of the pot that I can open up. And so I, I decided to fill up my coffee cup, I opened a little drain hole and then it come out the bottom of the pot. And, and, um, sort of, you know, it's, it's like, okay, that's, I've never done a coffee pot like that. I don't know if I've ever filled the coffee cup like that. Of course, I've seen other things of holes in the bottom of water comes out. Um, I'm just, I'm just, I'm not sure that's a counterexample or not, but it is, I'm thinking of examples like, well, I'm solving this problem in a, in a very novel way, uh, for coffee. Um, but maybe. Yeah, I think a coffee one's Uh, I was just gonna say that that example sounds a bit like applying a different behavior to a novel object, like applying the drain hole to the coffee pot. Um, but I had to, I had to come up, I had to somehow figure that out. Right? It's not like, um, yeah, I'm not sure. It's a counter example. Uh, I'm just thinking out loud. I mean, I like one other Go ahead. Just like one other example with the coffee one is like. Yeah, let's say for whatever reason, like, I need to put the fluid into one vessel before I put it into the second vessel, like, that's something where, okay, maybe the first time I experienced it, I, like, when I normally do it, I always kind of, uh, use, like, I don't know, the sink to, to fill it up or something like that, but then you're in your situation, Jeff, where there's, like, no water in the house or something, so you have some other source, like, You can, that's where you're kind of like following a different edge, like you've filled up vessels with other things before other than sinks. So although you've normally not followed the sequence, like normally your sequence of making coffee isn't like, it is to fill up with a sink and then pour it into the pot into the machine. But, um, because you understand this edge of like, oh, I can also fill up the pot first with like some other source of water. Then you can, like, follow a novel, a novel path, um. Let me, let me, yeah, I guess I just feel like, yeah. Let me give you, let me talk about what we think about path integration in terms of, like, grid cells and navigating an environment. So the animal is moving around and it needs to get home. And so it says, oh, I need to get home. I'm going to follow, uh, straight, go straight there. Even though I've never gone from A to B, I'm going to go from A to B straight. And that requires, you know, what we might call properties of path integration to know where you are after you've moved. And, um, in that case, the animal is still executing a behavior it's done before. It's moved in all directions. It's, you know, if you think about compass directions, it's moved in all directions. So it's just, it's taking a behavior You could say very simply it has a repertoire of moving in different directions and so it's applying one of those repertoires, one of those behaviors, uh, in a, in a new situation. But we still call that path integration. You know, you might say, well, it's traversed all those edges before, it's gone in all different compass directions, therefore, um, uh, you know, it's, it's, it's not path integration. But I'm not sure how But we do call that path integration or related to it, so I'm not sure the other examples where the same behavior has been applied elsewhere, why we not say, okay, it's not path integration anymore. Yeah, I mean, well, when I, when I wrote path integration, yeah, I was thinking of the example of where you move from A to B to C, but now you know how to move from C back to A, even though I moved from C to A, which was maybe slightly different. Different. But I, I definitely, I think that needs to be learned though. 'cause that is an edge. Like if it's an arbitrary space, you don't actually know if it's bi-directional. Like some, some systems you are, you can only move in one direction. Yeah. That's what I, what I would say too, that um, for an object behavior model, you, you need to have learned, um, you need to have emerge all of the edges basically. Like it is not a space like a three, like 2D or 3D space where you kind of. You know how that space works. It's a more abstract state space. Yeah, it seems like you have observed those edges. I'm not giving up yet on this because I think it's still a good idea. Um, what we, to me, what's, what's important is, I mean, sometimes path integration, oh yeah, I go from A to B to C, therefore, you know, I could have gone all the way to C. But I was thinking in terms of the behavior, you're trying to achieve a goal. Right, so that's what we're talking about here, ultimately, achieving a goal. And so the animal wants to get home or to a safe place or back to the food, and so it traverses a path through the environment that it's never traversed before. Um, it can calculate how to get from its current location to desired location. The equivalent in the object state space or the behavioral space is that the object is in some behavior and you want to get it to a different behavioral state and you've never transitioned from behavioral state A to behavioral state B. Maybe you've gone to behavioral state B from other directions in other ways, but you're now trying to say, oh, I'm in a new location here in behavioral state. Um, maybe I've, maybe the stapler is, You know, completely open, and I've never made a staple by when the staple is completely open is a bad example, but you know, so I first, what do I have to do to get the staple to come out? Or that's not a very good one, but the point I'm trying to say is that you could make the analogy between like, I'm trying to go from behavioral state A to behavioral state B. I've never made that specific transition before, um, I don't have to retrace my steps to get there. I should be able to go right from my behavioral state A to behavioral state B. Um, that would be the goal. That's what I'm, that's what I'm thinking about when I'm talking here. I'm not talking about the other path integration A to B to C. I'm thinking like, how can we, can we calculate How to get from one behavioral state to another directly, even if we've never directly gone there before. Um, I'm not saying, I'm just saying, I just want to clarify what I'm talking about, which is a little different what you were talking about. Yeah, yeah, that makes sense. I guess what I was, yeah, saying here, I'm not saying, Nick, we don't need path integration, I just, Yeah, I couldn't come up with a great example of where we need it concretely and, uh, it seems difficult to imagine, uh, any kind of path integration in, in behavioral, like in state space, um, but maybe it's possible and maybe we can use the same mechanism we use further. Right. I don't, I don't know. I think it deserves, it deserves a little bit more thinking. Yeah. Yeah. I mean, if, if we agree that path integration. It's kind of being able to follow, like, being able to make links that don't follow a single memorized path, then would you not agree we kind of need that? Like, we clearly can't just, like, learn a single sequence and then always use that. Yeah, no, I agree with that. We wouldn't want to have to memorize sequences. We would want to be able to move from edge to edge in novel combinations. I just mean Yeah, what I meant with path integration is I can't infer an edge that I've never seen before, because I don't know how these abstract states relate to each other. So you're right in the sense that, why do the balloons come up? What happened? It's some kind of gesture recognition. Oh my gosh. Um, so I think, I mean, it's really good observation you couldn't think of. I mean, that's an important observation. Um, but again, if I was thinking like, Uh, you know, I was starting with the idea that why would you have, you know, how does the brain process these, uh, motions of the object, and which are, which I've been arguing are the motion sensitive cells in the upper layers that have restricted, uh, receptive fields. Um, they would clearly sort of represent motions of the object. And so they look very similar that you have minicolumns of these motion sensitive cells that have the same directional, it just looks like the same stuff we see in the lower layers, but with smaller receptive fields, and it's the stuff that we know the grid cells require to build grid cell representations. So I've just, I just lumped, jumped to the conclusion like, okay, well there's probably doing grid cell like mechanisms up there, and if you have grid cell like mechanisms up there, then path integration comes for free. But that could be all wrong. Maybe there's no, maybe there's no grid cell like mechanisms up above, but then I'd have to understand, like, well, how does the upper layers process, um, process object motion? Um, how would, you know, which I still don't understand. Um, so I think we can just leave it as an open question that perhaps there is a grid cell like mechanism in the upper layer, in which case path integration would come for free, or maybe there isn't a grid cell like mechanism in the upper Um, and it's, it just looks like that might be, um, and so then we need another hypothesis about how those upper layer motion cells, uh, are being processed and interpreted, um, somehow. Yeah. I mean, just for what it's worth, I feel like you could argue even with grid cells, like, what that's kind of doing is, like, you've, you've visited all the edges in that you've often navigated over 2D space and realized that, like, okay. Um, these basically form edges with their neighbors, or whatever, um, and then when you're in like a novel space, you're almost like tiling that space with that behavioral model, um, but you still at one point had to learn locally that there's this like, these edges, um, um, And so, like, I feel like there's not necessarily any setting where you can kind of path integrate without having, like, learned the behavior of edges. Right, right, right. And that's true with grid cells. And so I'm saying, uh, you know, grid cells, you, you must have known you can go in different directions from where your current position is. And um, And so then, I don't throw away the idea of path integration. I, you know, just because I'm doing something I've done many times. It does feel like, at a very high level, it feels like I have a problem to solve. I look at my situation and say, well, I know how I want to get this situation resolved. I know what state I want to be in. But, uh, uh, I, you know, maybe, maybe I never started in the current state and so I have to figure out how to do it. You can even argue this in the place, the table setting thing, right? You know, that maybe the plates and the utensils are in a sort of wonky position you've never seen before and somehow you're able to, um, you know, rearrange things to get it right. I'm arguing, I'm not, I'm not 100 percent believing these points. I'm just arguing for sake of. Yeah, so I guess that would be an interesting one to, like, talk some more about sometime like, how would we use a grid cell mechanism to represent state space, which is a lot more kind of abstract with more abstract ways to move through that space, or more constrained ways to move through that space. Um, so, I mean, I think we talked about it sometime before, how grid cells might be very general and might be able to learn such, uh, Novel spaces, they're not 2D or 3D, but yeah, right, right, right, right, they don't really, they don't know what they're representing, right, they just, they just say, here's a, here's some movement, I'm going to interpret, I'm going to represent, take the movement, represent it, turn it into a space, um, and, and, uh, whatever it means, I have no idea. That's what it just starts with movement factors and says, okay, that's my space. Um, yeah, yeah, yeah, yeah, I guess that can bring me to the next part of like how this might be implemented in the cortical column. Um, and then we can see about how that matches. Um, so what, what it was, I took all these requirements and constraints and I have this big overview today. So this is something that I kind of work on over the past years, where I just draw all the anatomical connections that have been found. And if you like zoom in, there's like quotes from papers and figures from papers for each of these connections and references. And I just went through it and zoomed through it and figured out which of these could work out for behavior models. And also, some of our previous brainstorming notes where we talked about this a bit before, and try to kind of, you know, Map this, um, and I'm just going to show a very simplified version, but if we want to brainstorm more, I can also pull up the full view and then show the detailed connections. Um, but yeah, so just to recap, uh, we have the inputs here, the main feature input to layer 4, and then a movement or displacement. Uh, to layer somewhere between layer five and six and then an orientation, both of these relative to the object that the orientation goes like right at the border of layer three, four, uh, roughly. Uh, and then we have associations that we learn between features and locations down here. Um, and then that, that can be like after some successive movements, uh, the activations here can be pooled into an object ID. Um, which would then be the main output of the column out of layer 2. 3. Um, and then the second output is the motor output out of layer 5. A in, in primats, at least. Um, and then we have these long range connections that we talk about, um, as the voting connections. So those are in layer 2. 3 where we usually say that's voting on object ID. And then there are also long range connections found in layer 5. E. And what I'm basically suggesting, or I'm not, I don't think that's the first time we talked about this, would be that those might be voting on the behavior model, since layer 5 seems to be quite involved with motor and behavior and movement. Uh, it's layer 5b, not like layer 5a is the actual motor output to the subcortical regions. Layer 5b has these long range, um, connections to other models. And I have another slide on, um, another side note on, um, how it might be actually what's, what we refer to as affordances. Um, but then the other two, if we go with this three model idea, uh, we talked about this before that layer two is a bit more sparse than layer three and how, Maybe layer three is like the general model ID and layer two is the specific object instance. Um, so maybe we could, one could also frame it as like the feature model is layer two and the morphology model is layer three and then behavior model is down here. So the three models kind of add on to these three long range connections that we have. This is speculative, so. Right, right. Yeah. Well, another, another possibility, um, are you ready for another possibility or do you want to finish the slide? Yeah, go ahead. Um, a few weeks ago or months or so ago, I was talking about, you know, I made the argument that, you know, what is, what is an object? If all the features can change and the orientation of the features can change and everything can change, well, what's the commonality between them? And the commonality I was the location space that is. Um, an object, not in terms of Monty, but in terms of grid cells or in terms of cortex, you anchor all your, your grid cells in some sense, and that, and that defines a unique space which you can move around in, and that unique space defines anything within that space is the same object. So within that, therefore, everything can change, the location of features, the orientation of features, the actual, what the features are, and, you know, that would include changes in morphology, because when you move features, it's a new morphology. So in that case, one could argue that the layer 5b cells, are voting on the space. It's like saying, you know, in some ways you could say it's a class of objects. It's like, you know, this is the space that I'm working in, you should be working in this, we should all be agreeing that we're working on the same space, uh, defined, which defines a particular, you know, object. Yeah, that was just, that was an alternate behavior. I'm not saying it's right. I'm just saying that was another way of looking at 5B. Yeah, I guess the problem that I see with that is that we can apply the same behavior to all these different objects. So if the behavior models how locations can change, Then it would be difficult to have Right, right. Well, then I said, then I was saying that in the patient spaces. Well, well, uh, that's what a moment ago I was saying, well, maybe the behavioral model is in the upper layers. Um, because that's where we see those restricted receptive field movement cells, uh, in layers two and three. And so it's the behavioral models in the upper layer, it sort of exists independent of the specific space, right? It's just, okay, here's a behavioral model, I can apply it to cups, I can apply it to bicycles, I can apply it to coffee makers, whatever. Um, and so that would be like, you know, I, upper layers I have this behavioral model, uh, and then I project to the lower layers to say, you give me my current behavioral models, uh, how would it impact The current object, which is in the lower layers, you know, all these are like whole But couldn't we have the location space in layer six, and then layer 5b is the general behavior model that can be applied to any location space of any object and just tells it Well, right, but when we, remember objects have to vote, and um, learning modules have to vote, and we've always talked to now that the learning modules are voting in this layer two and layer three. Uh, this would be different. This would be saying they're voting on the object, meaning the, the, they're voting on what base object we're talking about. That would be layer five B. They're saying, okay, we're all agreeing, we're talking about coffee cups. It, we don't know what the state of the coffee cup is yet. And so maybe the upper layers are voting on the particular state of the object or, or the, you know, the, that would be an alternate alterna. So you'd, you'd still have all the same things you have here. Um, it's just relocating them. I'm not saying it's right, or even better than what you're proposing, I just wanted to point out that I was thinking about it slightly differently and, and I don't know what's right. Yeah, I think that's a good point. I think it's actually quite similar to what, maybe, if I'm understanding it right, what you're describing, what I was thinking was being voted on here, and one thing I actually am missing in this picture is where they would be voting on the state of the object. Right. So, yeah, I'll talk about that a bit more. Right, right, so it would be kind of like saying, yeah, we're all looking at a, we agree we're looking at, my hypothesis is I'm looking at a, one of my hypotheses is I'm looking at a stapler because of the local feature I'm seeing looks like a local feature in a stapler. But the different, um, uh, but the different learning models don't know as a whole yet. It's a stapler. What state the stapler is in? Is it open or closed? Each, each learning module says, oh, I'm looking at a part of a stapler, but what is its state? Um, so they could, they could vote on the 5b, could be saying, all right, we're all, I'm looking at a stapler like something, and then you could be voting on the state of the stapler up above. Um, it's all very speculative, so, um, you're right, it's just, it's, yeah, I think one other thing we've talked about that, I mean, yeah, I guess there's lots of different connections, but I think one thing we've talked about before as well that would fit nicely with kind of behavior being modeled in L5 is just the amount of reciprocal connections with L6. Because it feels like, you know, obviously it needs to be able to have a huge amount of influence on the location representations that are kind of emerging depending on the I can make the other argument is that, um, with grid cells, you have grid cells, which are sort of these cells that respond no matter what orientation you're in and how you're moving, they're just location specific in some sense. And then you have, you have to have these motion cells, which lead to grid cells. And they're highly interconnected, you know, so you have to have motion vector cells or velocity controlled oscillators. So there's this, so one could argue that, oh, um, I don't know, I'm coming back to counter arguments. I'm going to leave it as still, like, they're good arguments for both cases here. I'm, um, I'm just, I don't feel like, I don't think we can make it, I don't think I can make it, um, settle. Well, one other. One, I guess, maybe just is just like, um, yeah, and kind of, I think, like you guys were saying before, like, they don't necessarily need to be mutually incompatible. Like, it could be behavior is kind of more, and modeling behavior is lower down and then like state as like a, almost like a single thing is higher up, but that would, I guess, also just make it easier to send the state up the hierarchy. So, um, you know, if, if you want to influence compositional representations, basically. Then that's useful to have in the superficial layers. Right, right. The next thing you set up the hierarchy has to be the state of the object. It can't be just stapler. Yeah, I was, I think what I was thinking, so actually I drew two more connections on here that I have in my like scientific, uh, neuroscience evidence diagram. There were like tons of connections, so I just thought these might be relevant if this is the case. Is that there's also seems to be a reciprocal connection here, which. Could work, like, could work well if we say that the morphological model ID is actually the state of the object. So, like, what kind of shape does this object have right now? Um, and that, that has reciprocal connections to layer 5b. Um, and then the other thing is that there's also connections between layer 6b and, I didn't, like, the paper didn't talk about if it's, Five A or five p just said layer five, so I'm not sure. But which could have, like, you could learn kind of how, or you could learn how different states influence the locations that should be active in that objects, uh, reference frame. So yeah, I put this in gray. Maybe this is info about object state. Um, one, uh, one caution unfortunate is that when the people talk about these reciprocal connections. Sometimes, um, sometimes people report them, sometimes people don't report them, and one of the reasons that that happens is, depending on the method they're using, um, so a connection from one layer to another layer can be onto inhibitory cells. Um, or it could be onto excitatory cells, and, um, and so if someone's tracing, anatomical tracing of, you know, oh, where's this dendrite project I made a, you know, then they might, they'll say, oh, it ends up in layer five, but if someone's measuring the effect between activating a cell in layer six and seeing if the cell in layer five is active, they may not see it, and they say, oh, there's no connections there. Um, so just be careful about that. Yeah, the quotes I have here for that is that layer 6 cells appear to provide a significant excitatory input to layer 5 pyramidal cells. So that's one direction. Right, right. They say, but then you say they're bidirectional. Do we know that they do the other thing as well? Yeah. Um, dendrites of many layer six B neurons also reach through layer six into layer five. That's, yeah, that's, that's the anatomy. That's the anatomy one. So that could be onto inhibitory cells. You even know. Yeah. That doesn't throw out those results. It just. Just adds a little bit of, like, eh, more things to think about. Yeah, right. Um, yeah, and again, all those grey parts are like That was from layer 6 to layer 5, the first one you were saying, right? Uh, yeah, layer 6. Yeah, so in the Thompson paper, she writes, the majority of intracortical excitatory input to layer 6 comes from the deep layers of, like, layer five.

So that's the other direction, then. That's the other direction. So it is reciprocal, at least according to those two pieces.

Um, I guess just to throw out one more idea and then we can also just brainstorm more, but one side note on affordances that I thought was, um, maybe if we say there are behaviors folded on in these lateral connections, um, affordances could be exactly the same. Uh, these lateral voting connections. So basically, um, behaviors and visual models associated with motor behaviors is kind of what I think of what affordances are. So basically if a vision call has learned, I don't know, like the pinch movement to zoom out on your phone, and we have a visual model of that behavior, um, that would be voting with a behavior model in, uh, like the motor cortex of, um, your fingers pinching. And since, yeah, basically, if I'm seeing that behavior, the vote would kind of invoke it here, or if I'm invoking the behavior here, I would kind of expect to see this behavioral change on the phone. Um, so that's kind of what I was thinking, maybe that's how affordances are implemented. Let me try to absorb what you just said.

Would then, just to check, is this for the vision model to almost enact that behavior in the touch model? Because that feels more like the kind of goal state that you put in. Project with, like, L6. It wouldn't be the goal state. It would be more like, so the vision model wouldn't have learned, I mean, pinching maybe that's a bit of an abstract example on the phone, but the vision model can't know about, doesn't know about how the hand works and how the hand grabs a cup or something, but it knows Hojae's behavior looks like, so it knows how the hand looks like when it grabs and it can recognize that behavior. Let me give you an alternate thing which sort of counterfactualizes. Imagine the behavior is, um, a door, like a doorknob instead of a knob, it's a lever, you know, like a bar, and you rotate it down. So the affordances, oh, I see, right now I'm looking at the door in the conference room here, and it's got one of those, those levers on it. Um, I don't, I can do that with anything. I can do it with my knee, my shoulder, my elbow, um, there's lots of ways I can move that down, and so I can do it. If that would be an affordance, like it's not like you look at it and you just think of moving it with your knee. Well no, but I just, my point is I, I know that it has to move and it's independent of what I move it with. So to me it's not associated with a particular, it's, it's a, it's a movement of the handle itself as opposed to how I'm going to move it. So I'm doing it, it's like on the screen on display, you have to, you have to use your skin or that doesn't work. Um, so I picked a something where it doesn't, it's completely independent of what physical thing I use to move it. Yeah, so I think like I would think of it as the doorknob and its behavior of twisting, that would be modeled like in your vision column, you model how the doorknob can transition through the states of being twisted and turned and how that changes the state of the door even. But then naturally, when you see a doorknob, you kind of have this affordance to grab it and open it with your hand. So that would just be kind of through the voting bias towards that. But It doesn't mean that's the only way you can do it. You still have that model that the Dornhoff can twist and that's independent of how you actually twist it. It's just Right, well then I could probably come up with examples where there isn't a preferred way of doing something. And then you don't have an affordance. I think that's just a different, at least how I understand it. Oh, okay, so maybe I'm using the word affordance wrong. I thought affordance was a, um, to me the word affordance as I've used it in my past is something, something physical about an object. That suggests how it behaves, uh, and how it could be manipulated. It doesn't suggest how I might actually manipulate it, it just says, you know, it's like the stapler. The stapler, I see a pin and two bars, and I say, oh, that's a hinge, maybe that's a hinge, and that, that feels independent of any way I might actually move the stapler. Um, it just doesn't, it feels like I can, the affordance is just part of the property of the object itself as opposed to how I'm going to actually do it. So maybe that's different than the way you're using affordances. Yeah, I guess that would be just. Yeah, in that case, that would just be in the model of the behavior itself, that you're observing, like, how it can move based on some features that you detect in there, like, yeah. Well, that's, I guess, that's how I was defining the for instance, you're, you, you may be defining it differently, sounds like you're Yeah, I think I was thinking of it a bit differently, like, um Is there a technical definition of affordance that, you know, if I looked up in the dictionary, what would it say? It's like, um, you know, I don't want to use it in a way that other people are not using. Yeah, I'm just having a quick look. Affordance is what the environment offers the individual.

Well, that's okay. That's pretty generic. Yeah, but I think, yeah, I would say, yeah, I'm also kind of thinking of it like you're saying, Jeff, that it's like, it's kind of a property of the object that's fairly independent of the specifics of how you interact with it. Um, so like, an affordance is like being able to hold fluid or something like that. Yes. So in that case, it would be more like the features on that model kind of inform what behavior that model has. Like, which behavior you would detect here would be informed by. Right, right, right. So some, some learning module sees this feature of the hinge looking thing, the pin. And it's like, oh, this could be a moving hinge point. Um, yeah, I guess, yeah, this is more than an example of how, uh, sensorimotor, uh, um, how would, how do you call it, coordination, sensorimotor coordination work well. But I was reacting to your first line, which says a forwarding curve encoded as lateral connection. Yeah, so then I was using this term too narrowly, uh, yeah, if that's, Yeah, then the inferences can be just encoded within one column of features. It can be encoded within the states of an object, observed state changes of, you know, objects.

Yeah. You know, the nice thing about this is we're getting closer to having all the right terms and the right words and the right components of the problem. I don't think we're there yet, we're still disagreeing about some things. Um, or we still don't know about them yet. Um, but we're getting closer to it. Um, you know. Yeah, I think, yeah. That was another reason why I wanted to just put this all together because we had like a long slack thread about it and like lots of terms were coming up and I just thought like feature morphology, behavior, states, those are like key terms that kind of Um, yeah, it's just kind of good to go over again. I'm going to, I'm going to push back, push again to, to think about, um, the argument earlier, which is that, you know, objects are defined by their space, and then an object in theory could have any set of features, poses, um, within that space. Um, and, you know, and then, um, and then using that as like a generic idea of like, okay, these are all the different things that this object can be existent, and then how do we transition between them? Um, it's just a little bit more generic way of phrasing it. I'm trying to get rid of that morphology thing again. Yeah.

I thought that was a good, good progress, get rid of morphology, but. Yeah. Um. I just wrote down a few kind of open questions. So this one we already talked about, um, do behaviors need to be path integratable? Um, and then the second one I was briefly thinking through but I think not. So could rotation just be a specific type of state? Could we use the same mechanism? The rotation of the entire object? Yeah, like object rotation. I don't think so. Yeah, that's, yeah, what I, you know, what it could be, you know, if I have a compositional object, in some sense, all things are compositional objects, um, then the, the orientation of a feature, um, is part of the, the, the state of the parent object, um, but on, but the child object on its own, I think the model would be independent of its current presentation to you.

Yeah. I would think.

Yeah, I was just thinking through like, I guess this one is a bit more of like an engineering question as well. So like with rotations, we kind of test through different possible rotations and depending on which rotation hypothesis we have, we expect the features and locations to be in different locations relative to the body. Uh, so kind of a similar thing applies to state. If the object is in a different state, you expect features and location to be in different places, uh, to be different. But you kind of have to test different states of, like, state hypotheses, basically, of the object. So, yeah, that's, that goes a bit into, like, how would we implement recognizing the same object in different states? Well, that, that goes back to the point we were making earlier, like, when it seems like, Um, we have to be voting on that somehow. Right? It's in, it's insufficient to say, these are all staplers. We have to say, no, this is a stapler in this position. We rec in a flash inference vision, I would see the state of the object. Um, so somehow we have to reach that by voting. Well, I mean, even the single learning module can be rotation and variant. Uh, and, and the way we do that now is by testing multiple hypotheses, so Right, right, right, right. I think what you're saying, Vivian, is that yeah, we have to test multiple states. Does that mean there's some kind of, almost like, symmetry or whatever between rotation and state? But the way I think about this, remember, that we're relying on the thalamus to overcome this rotation problem. You know, it's like, okay, I don't want to have different models of the object in every orientation of this. Coffee cup to my body shouldn't be a different state of the object. I mean, that's just, I don't want to encode that. I just have this model and, um, and then it's a temporary problem of how do I, you know, convert my senses and my movement behaviors to the, to the current orientation. So to me, that was like a, um, a real time compensation being thalamus, whereas the state of the object is really a property of the object. Um, um. I would say it's more property of the behavior because like With the running banana, we don't have a model of the banana in every possible running position. We have a model of, like, how this behavior changes the locations on any object. Well, that was, be careful, when we talk about the running banana, now we're mixing a behavior learned in one place with an object learned in another, which is fine, which is great, I just, it's not an inherent property of bananas that they run. We'd have to see people running. We learn the model that way and then we try to, so I, it's a great example because it basically, it just reminds us that we have to be able to learn models of behaviors independent, that can be applied independently to an object. Um, yeah, like when I see the banana running, I don't think, oh, there's a person running. Oh no, it's a banana. It's like, oh no, it's, it's something's running and it's a banana. I can't even imagine a non describable object That's running, you know. Yeah. Um, you could just look at the, you know, if they, if you had a bunch of those, uh, I forget what they're called when they put the little white dots on here, the ping pong balls and they do the motion capture. You know, you can just see a bunch of white dots moving and say, oh, that thing is running. You know, it's like, there's no object there. It's just, it's just running. You might imagine an object there, but, well, yeah. Yeah, yeah, I guess. All right, so, I guess I, I think I'm agreeing with your, your, I think, maybe not, and I think, I don't think so. Well, I think the first one might be maybe yes, the second one I would agree with you, I don't think so. Yeah, yeah, and then yeah I guess that relates to the big question that I haven't really answered here is like what mechanism could the brain use to model general object behaviors that can be applied to many objects? Well, that was my point of Yeah, if you had a state, if you had a space of states, um, okay, I ordered months or so ago, then, then that could be applied to any object because it's independent of the object. It's just a, um, yeah. It's how various pieces of the object are moving relative to the other, and it's independent of the actual object. Yeah, I guess I'm still a bit unclear on if every object has its own location space, how do you have a model that tells you how locations of features change? And then apply it to different location spaces. Right. Well, it's, it's, uh, I don't know yet, but it seems like it's quite doable. Yeah, it must be. Yeah, it must be doable. So it's like, there was an answer to that question. Just because we don't know the answer doesn't mean we give up on it. It's like, okay, it seems to be happening somehow. I don't know. Yeah. Interesting. Yeah, that was everything I had. I don't know if you want to do more whiteboard brainstorming or if you want to just kind of sleep on it for a bit and, uh, yeah, go into the minicolumns idea. Um, it, one thing I just wanted to comment on with the presentation, I just thought it'd be interesting to touch on again the, the, uh, the L5, uh, voting thing you were talking about with the, uh, What initially you were calling affordances, but yeah, as you were saying, it's more about like, um, coordination, just because, yeah, I think it is kind of interesting that like, we might send goal states, but then also have some bias to a particular thing. Um, what do you call it? Way of solving it. And I think that's kind of what you're like showing there is that like, yeah, it would, it wouldn't enforce a way of doing something because it's a lateral connection. And so it's not going to, like, it's not going to on its own instantiate a particular representation, but it could be like, Oh yeah, generally when I'm doing this with my, or like when this behavior is happening with my eyes, this behavior is happening with my head, so yeah, that's interesting. So, you know, um, I'm talking about more complications here, and I might have alluded to this recently, I can't remember. When I think about what's actually going on in the brain, although it's possible for a single region to learn behaviors and learn objects, what typically seems to be happening when we learn a new object, we learn it at multiple levels, actually higher up in the hierarchy. All right. Like, I don't want to dedicate V1 or S1 to some new thing. Um, and so we build a model higher up in the hierarchy, maybe even in the hippocampal complex. And, um, you know, it's temporary, here's a new object, I'm looking at it. And now when I want to actually behave, interact with the object, I have to basically say, I have to move back down the hierarchy and pick a specific way of interacting. It's, and then, as we talked about with typing and playing the piano, things that you would do over and over and over again can be learned very low down in the hierarchy, so I don't have to go up and down this hierarchy. Uh, I can just type without even thinking about it. But in the beginning, I have to think about it. In the beginning, I have to choose what fingers to use, and so it involves, involves multiple hierarchical levels. I only mention this because this is another complication when we think about, oh, what, what. You know, what, how am I going to type, or how am I going to move the lever, um, you know, in the beginning, I may not have a preferred way, I, I just say this lever has to move, or this button has to be pressed, and, um, and then it goes down the hierarchy and says, well, it's easy for me to use my finger, or it's easier to use my hand, or it's, you know, whatever, as opposed to some sort of learned thing, um, uh, only with this really high practice do you actually assign a particular You know, I see, you know, I want to type a word and I go right to S1 and M1 or whatever and type it. Um, it's just, it's just more complications in this process. So, in some sense, separating out what I typically might do for how I manipulate a new object with its, and how I might manipulate a highly learned object, um, would involve different levels of hierarchical composition. Um, so it just makes things much more complicated to think about for me.

I guess another, um, reason why I was thinking about these, uh, lateral voting connections, um, like voting on behaviors, is because when we talk about the, the goal policies, It's always, it seems a bit difficult, at least when I'm thinking of it, to have the same model that recognizes the behavior, also output the actions to transition to a new state. And it just seemed like, uh, well, just because like the vision column that, uh, actually sees, um, I don't know, the key going down or the lights switching on, uh, doesn't project subcortically to whatever controls the finger. It moves the eye. So, uh, it would be nice to have kind of the spatial pooler. behavior model of the lamp switching on in the vision column to recognize what's happening, what behavior is happening, but then being it being associated with behavior model in more of the motor cortex. Well, again, this gets back to the point I was just making, that I think there's hierarchy involved in many of these behaviors, and thus they're practiced in Um, so, you know, S1 or V1 wouldn't be informing S1 directly on, even take a lamp, I walk into a room, you know, unless this is like a lamp on my bedside, I touch it every night, I know exactly how to do it, I don't have to think about it, you know, I see a lamp, I have to find a switch, I have to then reach for it, this involves a fair amount of the cortical hierarchy, I believe. Um, because it just has to, and it seems like it does. You have to think about it a little bit. It's not like it's, it's mindless. It's not mindless. You have to think about it and say, where is that? How am I going to reach it? Um, yeah. So, again, I think some of those thoughts you just said, Viviane, get complicated or get different answers when you're assuming or not assuming it's a highly repetitive learning procedure or if it's going to involve hierarchy.

Yeah. Yeah.

Anything that goes within a primary region and between primary regions, to me, would be super highly learned objects or behaviors.

Most examples like the lamp would fall into that category. Even the state bird probably wouldn't fall into that category. Different state birds, you pick them up different ways.

Yeah, I guess what I mean is, like, you can have a very learned model of the lamp turning on, or the stapler, um, just kind of how it looks, but then you can invoke, invoke different behavioral models of how you might apply pressure. So then I would, I would, I would argue then that that's going to always involve a hierarchy. It's not, it can't be done primary region to primary region, because I have to have some point where I say this thing has to move, or this has to press, and I have choices on how to do it. So if, if, if there's choices, it has to go down a hierarchy to go different routes. I can't have the vision V1 connecting to all different fingers that might push the button. It's more like, oh, a button has to be pushed. Um, I'm in a novel situation, so starch higher up says, okay, what do I have to push the button? Like, I can use any of these different physical parts of my body to do it, and just pick one and go down the hierarchy. Or pick another. Yeah, that might be more of a temporary environment model of like, where are your body parts? Right, right, right, right now. Right. That's why I mentioned something that's extremely highly learned, like maybe a lamp on your bed stand, nightstand, you might not, you could just have Reach and do it, not think about it at all, because you always drive it in the same place, you always push it with the same finger, and it always behaves the same way, and you've done this a thousand times, um, then you might be able to do it like that. Or just like, also, as I talked about, like reading, playing piano. You know, a pianist can recognize phrases of, or sequences of notes quite long, and recognize them as one thing, and their fingers just do it, and they don't have to think about it. We're an athlete, you know, same thing, but for most people we have to think about it and decide, oh, which finger to move and how we're going to do it.

One thing I was just thinking with the, uh, time we have left, I feel like Viviane and Jeff and I have had a lot of chances to talk, but yeah, hopefully we've not been sucking all the air out of the room, so yeah, I don't know if anyone else had any, uh, things they wanted to add.

Well, there's a lot to take in. It's a lot of, there's a lot of detail here on particulars of the model, so I'm still just kind of processing it, absorbing it. That made it hard. Yeah. Is there anything that's unclear or that would be useful to get some more details on? Well, I think the, the part that I'm like, I don't, I don't know that there's any more details, uh, at this point. I think when I'm, I've been thinking about behaviors as being pretty closely linked to the underlying object model and the idea of sort of. Abstracting away the behavior away from the morphology of the model to make it more general purpose. That's a fairly difficult kind of thing to picture exactly how that would happen.

Agreed. Yeah. It's hard to picture. Yeah. To think about.

Yeah, one thing. We have to think about it. I guess next Wednesday, maybe Hojae, you'll probably be talking a bit about NeurIPS and stuff. And then I think the two Wednesdays after that, uh, there's holidays. Although maybe the week of January 1st we could do a research meeting on another day. But um, but I was thinking maybe one thing could be useful. So like a while ago we talked in terms of like concrete ways of doing behaviors and stuff like that. We talked about sort of like essentially what's like message passing and graphs. Where like nodes are kind of sending messages to one another versus like learning sequences. Um, and I could try and kind of give a summary of some of that discussion that we had and stuff like that, and that might help us kind of just rethink about, like, think again about, uh, possible, like, concrete, uh, proposals. Yeah, I thought that was interesting, a part of that was interesting, because, um, I think we talked about learning like general purpose edges and nodes that can be reused in many different objects, like you might learn how a hinge works, and then you can apply that hinge behavior to edges in all kinds of graphs. Um, so, yeah, I think that was an interesting idea.

I, um, Go ahead. Oh, no, yeah, um, so, you know, it's funny because at one point, uh, Viviane had said, oh, when we get together in February, you know, I keep putting it again, what are we calling it? It's not by the, it's, um, it may be peer retreat. Retreat, yes, seems like an easy word to remember. Um, you know, maybe we'll, you know, we can make some progress on this whole object behavior stuff and object state. So in my mind, what we're doing right now is sort of prepping ourselves by thinking about these problems over and over and over again, which, Historically, this is what it takes. You have to just pound your head against the wall over and over and over again on the same topics and then after you do that enough time, then ideas, the answers pop out, so. I view that this is all preparatory to like, maybe sometime in February, figuring this all out. So that's my goal. That would be great. I know, but today, but to do that, we have to like, you know, sit here and get puzzled over and over and over again. Um, I've always found that somehow that works, you know, you just rethink the problem, rethink the problem, rethink your problem. Oh yeah, we forgot about that idea. Oh yeah, we forgot about that idea. There's too many things and then somehow at the end it settles down. So I think these meetings are very, very important.

We're making good progress, even though it may not seem like that, it's what we need to do to move towards a solution, so that's my goal. February, beginning of February. Maybe, maybe that would be a nice thing to do, figure this out by then. Yeah, I guess related to that, at some point, I mean, I keep saying I'm going to give an overview of the connectivity and cortical columns. I could also just share my big paper at some point because today when I was making these slides it kind of felt a bit like a puzzle where I would go into that sheet and look into all the evidence all the connections and try to kind of piece the pieces together and figure out which parts could be doing what what properties do the neurons have in the different layers and stuff so yeah I think just kind of having that context again and in one place too could be useful. Totally, I agree. On the other hand, I would argue that Looking at that graph will never tell you the answer. Um, it's like, like all projects, you have to have this sort of empirical evidence and theory, and they have to inform each other, because there's so many unknowns about the anatomy and physiology of the cells, and so many details that are relevant that we don't know which ones are relevant, that just, I have found, looking at it alone is not sufficient. But when we come back in, like we're doing today, with sort of theoretical constraints and say, well, it has to do this and it has to do that, then you can look at those diagrams and say, oh, yeah, I remember there was a connection for that. Well, yeah, that, that makes sense. And this explains that physiology. So, um. Yeah, it's funny, because in some ways it's like a highly unconstrained space, like so many things are connected to so many other things that you could come up with all kinds of crazy theories or whatever. Right, right. But it's nice how you kind of start with like, okay, feature inputs are coming here, and Like movement information is coming here, like that kind of grounds it, and then like, as you say, kind of like slowly building it up. Right, it's this iterative process between theory and empirical data, which is the hallmark of all scientific progress, um, and we have to do both. Um, we don't do the empirical research, but we have to read it, interpret it. And this is why, you know, over the years, many neuroscientists have said to me, oh, it's impossible to figure this out, you can't figure out how this works, it's just too complicated. Because they don't, most of them don't think about the theoretical constraints, at least a lot the way we do. Think about it very narrowly. Um, so that's our unique value add here. If we have a second or two, I'll just, if you don't mind, I'll just give you a teaser of what I wanted to talk about with the minicolumns. Can I make one comment about what I thought about the whole behavior model and then it's, yeah, um, so I guess what's been on my mind was, um, how we can generalize behaviors to different kinds of. Like, let's say, pouring behavior to all different kinds of cups, or sitting behavior to all different kinds of chairs or flat surfaces. Um, what I think babies might be doing, and babies try out a lot of different things, and like somehow implicitly they're sort of learning the physics. of how things would be right that that way they can predict what will happen even though something is new like for like AI system, maybe behavior is something that we can, I don't know if we can like encode behavior like or like what can be done or affordance directly, but we can keep like a observation action. So, like, it did some kind of action, um, here is the, like, resulting observation, um, and the difficulty is to be able to kind of generalize that, so if I was able to pour water into this cup, then we, we wanted to know that it can also pour water into this cup. I'm sorry. Do it to me. Water and coffee. Um, uh, so, but then I think it. That observation and action, like somehow, we're not just connecting that between this cup and like this particular cup and that particular action, but somehow from this cup, we're extracting, oh, like, this is a, within this cup, there's a concaveable feature or a wall feature that makes it pourable. So, like, we need to associate that observation and feature not to its specific ones, but to its kind of general form. feature level, and I don't know how to do that, but that's how I'm thinking of actions as like a sort of a It seems similar, I think that's right, and Scott mentioned something very similar, it was like how do you separate out behaviors from objects themselves, right? We want to have behaviors somehow exist independent of the object. Uh, of course, when you're thinking about pointed driven cups, I'm thinking like, oh, I could put fluid fluid into a shoe, which is much more unusual. Um, yeah, but both have a concaveable surface. That feature must be shared across cups and shoes. You're right. And then you have, then if thinking like a shoe, well, but if it was an open toe shoe, I know it wouldn't work. And yeah, exactly. It's almost like a mental simulation where you play out what effect would happen, at least if it was a novel object. And then, yeah, eventually you could learn, like, just by rote almost, like, okay, a couple, like, things you could pour into. But you kind of almost need to mentally simulate the fluid going into it and how it will disperse. Right, it's not, it's more than just like, oh, it's a container. It's like, oh, it's a container that's waterproof or has no holes. Or, you know, I want one that has, a rubber shoe would be better than one that's, you know, fine leather. It's like, all these things play out in your head as you're about to pour the water into the shoe. Or like a paper bag that's going to like collapse. Like you can kind of pour liquid into it. So I think, Hojae, that, I mean, this is the key of one of the things we're talking about here today, is just how do you separate out, uh, behaviors from the objects in which you observe them? And I think just stating that problem is a long way towards solving the problem, um, just knowing that you have to be able to do that, um, and it just gives you, you know, and assuming that that somehow happens on a column by column basis. It tells you a lot. It puts a bunch of constraints on the problem. One kind of maybe interesting experimental thing, um, that just came to mind was that, um, in terms of like, you know, we talked about, wow, okay, each learning module is learning physics kind of to some degree. Um, that seems like fairly complicated, but I just remember once doing, uh, rotation with, um, Uh, this guy called Kevin Smith, but he's in, he was in Joshua Tenenbaum's lab and Joshua Tenenbaum's lab do a lot of stuff about like intuitive physics in humans. Like how, how humans have this kind of notion. But what's interesting, I just remember asking him, like, is there like a region in the brain that like you lesion and that people don't have physics kind of? And I remember him saying that there wasn't a known one, like it, you know, a lot of times people talk about like a face area or whatever, like all these different things, but it really did seem like physics is, is a very distributed thing, which I think fits well with the idea that kind of any model that might be subject to physics is going to be kind of where it's learned. So, um, I thought that's just like, that's nice support for kind of the general approach we're taking where it's not like, Oh, we have like a totally separate part of like these learning models are learning physics. And these other learning models are learning objects. Right, right. Yeah. So physics, physics is an, it's sort of a, an emergent property for most people of observation. In fact, to really understand physics, you've got to go to, you know, take university classes for quite a while to really understand what the physics is. You know, you try to know like, oh, this, this, this works this way, you know, but then you say, well, you know, it takes a Newton to figure out why did it work that way. Yeah. It's, it's almost like language, like your native language, you speak, you use the grammar perfectly. But, you know, if someone's like, oh, this is past participle, whatever, like, what the hell does that mean? Exactly, right, right. The rules, the rules are not, we don't encode rules. We just go and code observation. It sounds like, you know, based on the hinge comment, like composition might be a good way to sort of get us. Out of this abstraction problem, like how, you know, a behavior could be applied to many different objects. Like you identify that there's a hinge and so through composition and identifying the, the objects that enable the behaviors, we can sort of abstract it to many other kinds of, uh, compound objects. It does seem, it does seem almost a requirement to, to see an affordance or seeing a possible behavior. There has to be some physical attribute about the object you're observing that suggest. That, um, behavior. You know, if I see a sphere, I'm not going to assume there's a hinge there and I can open it. Um, maybe if I saw a seam around the hint, the sphere, maybe a little thing that looked like a hinge on the side. So my point is that you, these, there are, there are associations between the behaviors and the physicalness of an object. But it can be done locally, as we said earlier, you know, local, you can observe a local feature that invokes a behavior, um, model, and, um, but there has to be this connection somehow, something about physicalness. You know, Hojae was talking about the shoe, you know, like, oh, there's vertical edges, or there's, you know, an opening or something, there has to be some physical thing that connects the two, even if it's only a subset of the object that suggests there's a behavioral aspect of it. Okay. Thank you.

I was just going to say, I think by what you're saying, Scott, it's true that it will simplify a lot, um, like we will need a lot less specific behavioral models that we learn if we can compose behaviors into each other, like, um, the running person is a bunch of limbs that move in certain ways, um, instead of having to model each part individually. And I guess that reminds me a bit of another point that like you said Jeff, like sometimes you talk about something and then you forget it again and then you remember it again. Um, so I, I just thought of it again, but way back, another by the bay when we were brainstorming, we talked about how we can't really recognize behaviors. In one column, um, at a time like that, that we actually need some flash inference to recognize the state of the person, because if we, like, if we have a running person and we only have one sensor, by the time we move the sensor, that part of the person is in a different place again already said, like, Okay. Especially for modeling states and behavior, especially for modeling behavior, states not necessarily, you need to do a lot of, actually, you need a lot of voting to kind of see the change in states. Yeah, I guess you're thinking about, if I'm looking through a straw, it might be hard for me to learn running, right? All I could see is one little piece at once. It might be hard to learn, but run it. Yeah, I'm glad you brought up that example again, Viviane. I also reckoned about that. That, uh, yeah, that we kind of agreed that I think it was the Elvis Presley dance. No, no, no. What was it? Yeah. We had like a John Travolta dance. Like, we had a few different examples, but we were like, yeah, you can't learn this really with the straw world. I mean, maybe very simple ones, like if you have a tiny button and if you press, it changes color. Maybe you can learn, you can learn that behavior, but more complex and compositional ones. like really repeated, like, I don't know, if a stapler is opening and closing and you just keep going over it with the straw, like you'll eventually kind of be able to kind of mentally map. Right, right. But it's a lot harder. It would take a lot of observations. Oh, yeah. Interesting. I forgot about that. I still forgot about it, but it sounds like something we would have talked about. Niels, you remember the specific dances. That's pretty good.

Oh, gosh. That's all right. I think Scott, you were going to say something too. Well, I was thinking about the composition of behaviors and all that and thinking that there might be multiple solutions for different kinds of problems. Like the hinge solution might not be the same as knowing that something holds water.

You know, those, or continuous deformable objects, like, it's less clear how to turn that into a composition or something like that. So, so I, I was making that argument about thinking very, very conceptually about behaviors. Is this any change in a model? As long as they're in the same, um, location space. And so, I'm trying to get around that issue, Scott, where you say, oh, it's hard to imagine this as being the same as that. You know, oh, you know, containing water seems very different than a hinge. And I think the answer has to be, you know, science, it doesn't have to be, but the answer is likely to be that the column doesn't make a distinction, the column really doesn't know these distinctions, it just observes a few basic things and, and says, well, all these things are part of the same object because I'm in the same space, I haven't moved on to a different space. And somehow we can learn any of these things, that would be the nicest solution, the nicest solution to say, yeah, we can come up with a hundred different things that might, variations of this theme. But they all fit into this idea that there's a space for the object, and then different states represent the different compositions of the object at that moment in time. And somehow that encompasses all the things we just talked about. Um, because we try to solve, you know, go ahead, sorry. Yeah, I was, yeah, I think in the pouring water example, it seems actually more like it's a behavioral model of the liquid than a behavioral of the cup itself. And then kind of like the point that Niels made that we're actually just simulating the behavior of the water and applying it to the cup. Instead of the cup having the model of being plural or something like that. I don't know, it doesn't seem like, you know, if I see a bunch of cups in a cabinet, I know they're all suitable for putting, you know, tea in them. I feel like you eventually learned that, but yeah. I think you can only learn it if, um, uh, if Um, if you've observed it in some object. So horrible water has to be observed in something before you can say it's that. You can't just simulate the water. It's like the interaction between the water and the, the physical object. All right, so initially you see the behavior attached to a particular object, that's how you learn it. Um, maybe then you see another object and somehow it invokes the same behavior, uh, even though it's a different object, and you say, oh, I can apply this to this object, and then eventually you can say anything, any object that has a set of properties, it might be pourable, um, into it. All right, we're repeating ourselves, we already lost Hojae. One quick thing I just wanted to say, Scott, related to what you were saying, so like Yeah, I'm not sure if, if this is right or whatever, but one thing I've been thinking kind of that I think relates to this is how like, you know, maybe some behaviors it's like behavior, it's almost a little like model free versus model based where you can imagine like some behaviors are learned kind of like as an arbitrary sequence. And that sequence can, can be anything, but you just kind of like learn it almost by road. And then some behaviors are more like a more kind of general computation where you're thinking about like. How it evolves or something like that. So yeah, I don't know, like in some ways it'd be nice if it's, if it's just a smooth continuum, but maybe it's also more like an actual separation in like, uh, if you're seeing like a totally new object, then like you just learned this kind of very idiosyncratic behavior, but then there's certain things that you're able to kind of disentangle and then generalize across different things. Um, I have a lot of ideas, but we're out of time here. Um, yeah, I didn't get to say my minicolumn idea.

Should I leave it for next time? It's up to you. I mean, I'm brief for a few minutes. I don't know if everyone else is. Yeah, I'd be curious. Yeah, I just, just to make a, it started with an interesting observation. When we move our limbs, um, different ways, we have to use different muscles, right? You know, to, to extend my forearm is a set of muscles and to retract my forearm is a completely different set of muscles. It's not like I can just send a command to the muscles, like move, move this direction, move that direction. I actually have to send. Muscles set A contract, or muscles set B contract, and then it occurred to me that minicolumns, one of the things that always puzzled me about minicolumns, well, Well, is that, you know, it seems like they represent movements in a single direction, um, and a different minicolumn moves movements in a different direction, um, but when we kind of come up with, like, grid cells, we want grid cells to be independent of the actual movement that led to them, so there's this tension, this dichotomy between we want representations that are independent of direction movement and ones that are sometimes not independent of direction movement. Well, so the thought that encouraged me, and this is very simply stated, if we think about a minicolumn and Someday I'll have to review exactly what the minicolumn is, but it's very, very skinny, right? This is 120 cells vertical. Um, what does layer 5 cell in a minicolumn represent? Well, it could represent, initially in evolution, it could represent a contraction of a specific muscle. It could have said, you know, to affect this movement, you have to contract this muscle, and another minicolumn says, if I want to move the other direction, I have to contract a different muscle. Now, that's not the way it works today, but that could have started that way in evolution, and that could be defining a language, to help define a language of layer 5 cells. Um, when they invoke movement vector, I used to think like, oh, they're, they encode some sort of movement, like move, you know, in some compass direction, but maybe they're not, maybe they're just individually saying like, more like, you know, contract the muscle and move in this direction, or if I want to move in another direction, I can invoke another minicolumn, and then another one. Um, that's the idea, it's very simple, but it was, uh, it was an interesting idea that, Evolutionary might start that way, and maybe when we think about the motor output of the cortex, we should be thinking about these encoding movement output in the cortex in this manner, sort of individual cells representing movements in individual directions, which makes a lot of sense. So this would be movement relative to the body? It just doesn't really matter at this point. It's just in a column, whatever the column is representing. That's not relative to the object reference frame. Well, it could be. Well, well, it would be if it was an object, if it was a learning module that's representing an object, it would be relative to the object reference frame. And then that gets converted to the body's reference frame.

But it would be initially generated. relative to the object. But, but, remember I've argued that layer 5 cells might be doing the same orientation transpose that the thalamus is doing, and so when it starts out in the cortex it says I need to move this relative to the object, and then it gets converted right before the layer 5 cells themselves convert it to like move it towards this thing relative to the sensor or to the thing I'm moving. Um, I think it's, it's, it's just a saying when I think about layers representing movement vectors being transferred someplace cortically, maybe the right way of thinking about individual cells represent movements in different diametrically, you know, posed directions. And so, um, imagine if I was moving my finger in a circle, it would clearly involve a whole series of minicolumns invoking in sequence. As is, each muscle has to be involved in different, different points in the circuit. It's a very simple idea. That's interesting. Do you know if there's, I guess with like the minicolumn, you know, if they stick a probe in perpendicular to the surface of the cortex, they can see like a very distinct response profile, uh, for the sensory ones. Do you know if there's anything like similar where they've tested? I don't know about it, no. Yeah, I'm just wondering, because, yeah, I can imagine if they, if there was present, that if you stimulated throughout the minicolumn of L5, you would basically get the exact same, but then as soon as you move over, you'd get a different contraction. So imagine now that if all the, if those movement cells are all representing, it's an observed movement of the sensor, right, so you're observing a movement, in some direction, then the layer 5 cells initially would fire the same way, basically saying, okay, I'm part of that, I'm going to fire when we're observing moving this direction, and then that layer 5 cell would then associatively link to the muscle that was actually making it move in that direction. And so it's, if layer 5 cells learns to, like, control a muscle, that actually led to the movement that was observed in one direction. And then, um, later we can, Without the input, you could say, oh, we want to move in that direction. I now know what muscle to contract. I don't know what cell fires. I'm not, I lost your, I forgot your, your questions. Oh, it was just about the experimental evidence. Like, oh, there's a lot of things that are hard to decode, but it seems like this might be actually tractable. Right, but I think what happens is when they measure these cells, almost all these experiments were done using like sinusoidal gratings. So the animal's not really thinking, um, the animal's not really observing a real object. It's just, you will see the layer five cells fired the same way as the layer three cells or the layer six cells. Yeah. They're all like tuned to the same thing. But I guess, cause if it's a motor output, it'd be interesting if they actually stimulated layer five, what happened, uh, if they stimulated a particular mini column and, and, and like whether you say you see that kind of same, almost like invariance. Within the minicolumn and then changing across the minicolumns. What would be really cool is also to say if the animal is awake and observing something it knows, you would see the layer 5 cell fire before the observed Um, change in the other cells. So the layer five would proceed, it would basically generate the movement that then gets observed. Uh, that would be the, that would be a great experiment too. I'm not aware of anyone doing any of these things, because they, you know, they don't really have theories about why you should do those experiments, so they generally wouldn't pick up on them, but maybe someone has. I'm not aware of it.

Anyway, that was, that was the idea. I'm working on minicolumns is trying to, um, really get to the core. It's another piece of the diagram that you didn't have there, Viviane, in your diagram. It's another piece of the puzzle to think about minicolumns as these independent units and how to interpret them. as part of a greater whole. And if you go read Maumcastle's work, he actually argued that the minicolumn was the unit of replication of the cortex. He didn't argue the cortical column was. He said minicolumns are the computational unit and you take a bunch of minicolumns and now you have a cortical column and it does something.

He had no speculation at all what minicolumns did. as far as I know, but he said they're observable, they're physical, they're there, um, they have these common properties, the cells in a minicolumn have certain common properties, therefore he said this is the unit of replication, um, and take a bunch of them together and you've got a cortical column and it does something bigger. I'm working on that right now. I think I have some interesting ideas about it, but so hopefully before February I'll get to present those ideas. Or maybe I'll just write them up. At some point.

Anyway, that was just, uh, that's it. It's just a, it's like, oh, what an interesting idea that these 35 cells could be, could have started by just actually activating individual muscles at some point in the past.