okay. So yeah, this presentation is about optic behaviors in Monty, how they would, could be implemented there and related open questions. And so I structured it. so this is the high level structure here.

the behavior capabilities that we would want to add to Monty would be, one, to learn models of object behavior. So to be able to model how objects change. Two, to use those models to recognize object behaviors, independent of the morphology that those behaviors are on. And then three, to learn associations between behavior and morphology models.

currently the idea is to use hierarchy for this, to speed up recognition and learn compositional models, so assigning behaviors to different locations on morphology objects. And then four, to compensate for object movement to make accurate predictions in the morphology model. So for example, using the behavior model to not update where we expect to be on the morphology model as it's moving or, and this is gray because it's partially one of the open questions they have, using model free signals for this. and so if I were to implement this in Monty, I would start with one and two as a first block, just learning and recognizing object behaviors, which is big enough already. But I feel like there we have the most concrete idea of how we would do this and it would be very powerful to add this capability. And then I'm gonna argue that three kind of will automatically fall out of that, once we have implemented the general capability for multi two model composition objects, because we can use composition, the compositional mechanism in exactly the same way to a assert. Like associate behaviors to morphology models. and then four is the one I'm least certain about right now on the solution we have for it. and, I think it's not necessarily required for, being able to do one and two. so I would probably, have that as the last step. and so this, the general structure of the presentation here will be that I go through each of these four and then concretely go into how I would implement that in Monty and the related open questions That make sense? Yeah. The only thought I had was that, I thought number three would occur naturally as you do number one. I mean we Yeah, exactly. That's what, why I put the dash box. 'cause I feel like if we implement one or two, three should naturally come out of it. Seems like when we learn about, behavior, we almost always learn it in the context of a particular morphology model or particular and therefore, and then we do three again later. Okay. Yeah. So you'll see you on the slide I have for three. The main thing to do there would be to have some test bed and evaluate that capability, but hopefully not needing to make any changes to Monty's internals in general.

So then, yeah, I'll jump into number one, learning models of object behaviors. So there, there are a couple of requirements that would need to be added to Monty. Number one, I think the biggest thing would be to add a new type of sensorimotor module that can detect changes instead of static features and send them to a learning module. So I marked here in yellow. what we currently have is this. We have a sensorimotor module. It extracts a pose and features at that location, and sensorimotor to learning module. And the features can be like color or curvature, and things like that. And then the learning module learns a feature morphology model of the, so learning features at locations. and then the addition would be that we have another type of sensorimotor module. And instead of detecting static features, it would detect movement or changes at locations. So it would still send a location in space, but then it would send like a movement direction or an orientation or a change in a feature, like a color change on a traffic light. and then the learning module would remain unto untouched, same mechanism, same internals. It would just get a different kind of input and learn a behavior model from that.

and so how we talked about before, the, I ideally the sensorimotor modules would be set up potentially, or a static one might need some updates too. So that basically only sense input to the learning module when the object is not moving, and then when the object is moving, only the sensorimotor module is sending input.

so before we, go on, just one little minor point. So this is great. this, again, we've made this analogy before, between envision for the cellular mag cellular pathways. one thing I just, I, don't know if it's important or not, is that we've discussed that in, in primates. It seems that the cortex gets, this, gets these, parvocellular mag cellular centers surround fields without, there's no edge detected and there's no movement extracted yet. and then that get extracted in the cortex when in non primates it seems that the information's passed up already in a form like you've shown it here. I think that matters why nature went to saying, oh, we're gonna send in some magnocellular cells, and the cortex itself will figure out, oh, this is a movement in this direction.

so I, just wanna point out, I don't know if it somehow nature went in that direction for humans and other primates. And I don't know if it's important or not, if that did, if that was, that understood what I meant.

You mean that an in primates like this would happen in V one and not right? the, if you follow the back, the cells coming out of the LGN to the th to the cortex, there were centers around fields. So not di they're chain sensitive, but they're not directly sensitive. and the par sale of the cells are not, oriented. They're just, bits surround bits. So for some reason, like if you look at a rat, it looked like we had just shown here, if you look at a human, that processing that extracts the edge and extracts the direction occurs in the cortex itself.

I, don't think we should make any changes right now. I, it just always struck me that, evolution went through the effort of changing that. Maybe it's a more general purpose, something going on and it's just worth mentioning. Yeah. Yeah. It's interesting. Definitely not a concrete idea, but I just wonder whether it maybe, it could be related to depth perception and if, yeah, maybe by trying to process, pre-process it, you limit what can ultimately done be done with depth or something like that. I always felt that somehow vision for a rat must be a little bit different than vision for a human because of this, humans have some ability to extract some other information that rats or have lost because it was pre-processed.

who knows? I don't think it's, I just wanted to mention it again and I'm not sure I would change anything, what you're doing. Yeah. I think that maybe relates a bit also to the proposal I made a few weeks ago about basically the extra layer, for Alpha. I think that directly feeds, forwards the input to MT and then behavior models being learned there. But that's a special kind of connection that doesn't go through pooling in layer three and then and input, Maybe just a bit of pre-processing and the path that goes through V two might be extracting more like three dimensional movement of objects actually, like the rotating cylinder or something. So it might need some extra pre-processing even, or a bit of object detection to know which thing to associate those flow patterns with. Yeah. another way we should get off this, but another way to think about it is maybe to following up what you just said is got these extra layer four, maybe we should not think of those as cortex and think of 'em as, sensorimotor module extras. Yeah. Something like that. They have to be stuck in the cortex. Yeah. All right. Let's, Steve, you should keep going. I'm sorry I debated whether to bring this up, but it's, I think we should keep in the back of our mind. Yeah.

about the point about what Jeff made, I think we'll have to probably understand the significance of why evolution went bother to change that mechanism from non primates to primates. partially because I think in Monty we can put in all the different kinds of sensors that we cannot even detect. like we might have a lighter sensorimotor or a, I don't know, can see in UV or infrared.

and there's no like biological equivalence for those. So if we understand like why, this change has been made, then I think we can look better considering whether we should make changes in multi, but yeah. I don't know how we can, I don't know how we can figure that out now, which I just, yeah, it's just an, it's an empirical observation and dogs and rats and cats see, don't see the same as we do, but they see, so I don't think, I don't think it's a sort of fundamental limitation. and especially if we take the view we just said where those lecture layer four cells, maybe they are just like more pre-processing, so they just throw them into the sensorimotor module.

and the second one, we don't need to go, I think this might be a little bit too much detail for now, but, We currently have a feature change sm that only gonna sends signals when we detect a significant change. do you think we will have something similar for behavior where it, like we're only sending if the changes are the changes or the changes our, yeah, so the difference between the two is, as far as I see is that the feature changes, looks at changes between movements of the sensorimotor. So I've moved, my sensorimotor has what I sense changed significantly versus here the sensorimotor is not moving, but something is moving through the receptive field of the sensorimotor. and so that's actually my next point that, that might be something a bit difficult to, or something we'll need to think about when writing this new sensorimotor module is that we wanna, we need to disentangle feature movement from sensorimotor movement. so it's easy in the case where the patch is not moving and it's just detecting that something is moving through the patch. but if the sensor's actually following the object, then the sensorimotor location relative to the body's changing, but there's actually no feature movement within the patch. But in reality, like what we wanna communicate to the learning module is that the feature's moving in the world. And not the sensorimotor, not just the sensors moving and the feature remains static. So we might need to, figure out some kind of mechanism to deal with, like calculating in how the sensors moving versus how the feature maybe this, may not be a problem. we're talking about like smooth pursuit here, right? Yeah. And I don't know enough about smooth pursuit. Maybe someone else does, but it's not clear that you would do smooth pursuit on something like this. I know you do smooth pursuit of a bird fault flows across in front of you. then your eyes will track that. But on an object like this where, it's not clear that smooth pursuit would work here, that see this, the right scenario, the number two may not really ever happen. Yeah. Okay. I guess maybe still there's a third case where, if we, do observe an object behavior, we would still be moving our sensorimotor sometimes. we wouldn't just keep the sensorimotor static. But we don't, generally don't, we don't process vision during those, during psychotic eye movements. so that's it's like a nothing, a null input at that time.

so you could just imagine. so I'm just saying I wouldn't worry too much about number two here, right? Okay. Yeah. that's a good, and we don't, and we don't wanna, we don't want to, if we are moving, we don't wanna sense anything while we're moving.

Yeah, I think it will become more relevant later on. Like for example, if we see a car driving by and we follow it with our eyes, we still wanna use that information to update our location in the reference frame on the car or, to keep it, it static in those. but in some sense then the car itself becomes, your, the car itself is not moving, how do I phrase this? the car itself is not moving, in, in terms of relative to the eye. So you can observe the car as if it was standing still or moving. So it's a separate thing, right? One noting the car moves is not the same as noticing things in the car moving relative to each other. Yeah. And I guess your point there, Viviane, is that we, even, the fact that we even know the car is moving and it feels like it ties into what you've often talked about, Jeff, this like local versus global flow that like the cars, the kind of local flow that's not moving very much because of smooth pursuit. Yeah. Like it's not very moving very much on the retina because of smooth pursuit, but then the global flow is changing because our eyes are moving. And so I, it'll be more complex I guess, taking a difference. The car moving, you could say, oh, that's just that's like a street scene, object that where a feature is moving. So it's an object behavior. Like the car is one of the features of the street and it's moving. But I don't know, that doesn't seem like that's how we perceive that more. We perceive it like separately we're saying, oh, now I'm looking at the car and I'm tracking the car.

and we, but yeah, basically still like in the model that models the car right now, if the sensor's tracking the car, that sensorimotor movement would update the location on the car that we expect to be, but we would have to compensate that with the actual movement of the car. So we stay on the same location on the car model. but if the, if s smooth pursuit works as I think it works, you don't have to do anything if this automatic Anyway, I think we should keep going. I'm just saying, I don't think we should focus on this number two very much. Okay. we can go a long way with before we get there. Yeah. And yeah, I have some slides later on that I think a lot of this can be solved with model free information, and doesn't require anything specific with behavior models. Smooth pursuit is model free.

so then the second big change that would need to be made to Monty is. To the learning module, which is to include, time or state in the models that, a learning module can learn. So basically a fourth dimension in the models. so in the behavior models, the fourth dimension could be time in, but, I would, propose to add this fourth dimension to all of the learning modules. Also, the ones that get static feature input, so that also, like morphology models can represent different states of the same object.

and yeah, we could model that quite explicitly right now as a fourth dimension, basically X, Y, ZT coordinates. but yeah, I'll go into that. you're saying time and state for the morphology models, it's not just different states. It's like this, the order in which the states occur. I'm not so sure about it. I'm currently, I'm tending towards just saying state and then it can be associated with different points in time in the behavior model.

yeah, in some ways, yeah. It's an interesting question. It feels like it could maybe be time because, 'cause I didn't, we've talked about how there's behaviors that are just kinda like instantaneous changes and it feels Almost the better way to represent that is with key frames in morphology models rather than, in a behavior model because there's no actual change to observe. It's just like a switch between states. So then, but unless it's, but there might be a time component to, Like a traffic light change in color definitely seems like a behavior model to me. it's behavior, but I guess I'm wondering whether it's more a morphology model with time that would learn it rather than a model that gets incoming changes. Because it's not like there's any smooth change that's stored in the model. And that's, and we've often talked about like you don't really store red to green. I think that's the problem. It's the red to green problem. Yeah. I think it's a good, it's more like you store, right? Yeah. Good observation. 'cause we did some, when we talked about the traffic light before, we did have some issues with it. It's it's, different, right? 'cause moving as you're saying, it's just changing. And so you're saying maybe those are actually represented, if I'm hearing what you're saying in the morphology model. and so I could have time in the morphology model saying yes, the red changes, the green, which, whatever. So in which case, all models are behavioral, but it's what kind of behavior they model is different. And of course in the biological theory we have about time, as far as I know, the time signal goes everywhere. So every learning module would have time available to it. If it's useful. Doesn't mean it has to take advantage of it, but the timing signal's up there and if something has time element to it, it would, it could learn it. So I like that idea. Anyway, again, this is another one I, we perhaps don't have to deal with right now, but I think the idea that time is available to both models is a good way to go forward. And, yeah, and then you could apply that because you had times slash date. I'm like, oh good, you can, but I wanna make sure I understood what you meant by that. but anyway, some, oh, still open questions, but I think it's a good idea to think that time could be available in both models as Niels Judge just suggested. Yeah, I'm currently thinking that, I'm not sure about the best name for this yet, because I feel like it can be several things, can be time, it can be a specific state, it can be something that's action condition. Like sometimes you only go from one state to another if you perform a certain action and it's not really time dependent. So it seems like there's just some kind of context signal that transitions you from slice to slice. I would suggest in terms of architecting Monty, that we think of time as its own thing, because I think that's how it is in the biology. And then you can add additional context from someplace else. If we wanna just take a broad swipe in the neuroscience, you could say, Hey, there's a whole bunch of different things to project a layer one, you can think of 'em all as context, everything that goes until this April, Denver, just context. So time is one of 'em, but you can throw other ones up there too. So my recommendation is consider time as a separate entity that every learning module potentially can use. And then you could throw in others, other ones as well. Other contexts. Yeah.

when we talk about the, traffic lights, changing colors, it feels like this is a different change, different kind of change than the morphology being changed, like, a stapler opening or closing. And I, think it's happening in a different layer. when the color changes, this is being represented in layer four, but when look, locations changing, that's represented in layer six because the features are the same. It's just the locations are being changed, but when the color changes, locations are the same, it's just the features, and therefore are being changed. So it is, it's a different behavior that's be just being applied to a different layer. Is that, the layer six would be movement on the object. Reference frame. Whereas for these kind of changes here that are illustrate as like changes allocations, I was thinking of those being inputs to layer four. Basically saying at this location, I'm detecting flow in this direction from the directionally sensitive cells, which are in layer four. Yeah, it would be layer four. It wouldn't be layer six because layer six is gonna represent the location of the sensorimotor. or the sense feature on the object.

it's not quite the same thing. yeah. But w would the change in color be represented in layer four? I don't know. that's too detailed the question for me right now. I, the color is processing is wonky to begin with, I think, you, I'm, you can see I'm anxious to get going on implementing this, and so I, I would then say, okay, let's take this color thing and say, yeah, okay, we have some questions about it. maybe it's the same as these other behaviors. Maybe it's different. let's not worry about that for now. Let's focus on, let you know, I think it'll come out. We had a lot of discussions about traffic lights already and I don't think it's gonna be a problem, the details, maybe we won't, get right now.

so I think we should focus on, Vivint's proposal and see what we can start building. Maybe that's not traffic. Yeah, maybe that's not traffic lights. Yeah. And we can start with just, like movements that we learn. we don't have to go like recognizing something, walking or running would be fantastic, Yeah. And yeah, so let me just get to that point. So the third, one, third requirement for learning use models is a way to keep track of time. and so everything is gray. Here are things I'll come back to in the, open questions or we can discuss now. But one question here would be, if time is an additional input to the learning module is from a global clock, or whether each learning module itself has some kind of internal clock that ticks and that it can use, to transition through the different states in the model. And yeah, I dunno, Jeff, if you have a strong opinion on that. I do. I think definitely it's the external, I've talked about it. if we, that it would be an external signal to apply.

the basic idea is if I have a complex behavior like a melody, I can up and speed up and slow down the time, the tempo for everybody, right? Yeah. it's not a column by column thing. And so everybody has to be, told, and same thing. I'm executing a complex. Behavior, like signing my name or I can speed up everything. I can slow down everything. it's, it takes a lot of conscious efforts to do it separately. and then the, whole projection from the th was suggested global. So my model has always been that this is an external, or, yeah. External input that's applied to all learning modules. It within a modality. Yeah. Within a modality.

Yeah. Now that you say that, I remember that, that makes a lot more sense with being able to speed it up and slow it down globally. And also being able to, if one learning module notices where does in a sequence or that the sequence just started, it can reset the clock for everyone, okay, great. Then this question is, already answered. and a way to keep track of time, should be the last of the three requirements to add for wanting to be able to learn models of object behaviors and then to be able to recognize them. requirement number one would be that also our hypothesis space extends into that fourth dimension. So I tried to illustrate that here, that we might have like evidence for different locations. in the 3D reference frame we have right now, but now that might extend into like the temporal dimension of like where we think we are in that sequence. and yeah, another question around that is I'll bring up later is can we interpolate the same way we interpolate in 3D sp space, also interpolate in temporal space.

and then second, the output that we get from each learning module. the kind of CMP signal will need an additional entry, which is the state or temp time point in the sequence. so it will be the location orientation, but then also state or point in the sequence and the ID like hinge behavior or whatever behavior Id we recognize. Are you saying this is the this is in addition to the object id? This is a behavior id, yes, the behavior id. Oh, oh, I see. So behavior, ID like the idea of the behavior model that was recognized. Yeah. if I think about, it seems like when I hear a melody, I recognize the melody. I do know where I am in the melody, but the recognition of the melody is, not particularly, it's not tied to that where I am. It's, like the name of the melody is, yeah, there are separate fields in here. Oh, I see. I'm sorry.

so yeah, they're in independent, but still, I think including this in the message it sends out will be helpful to make associations between like morphology, different morphologies at different points in the sequence and also, for voting, in, any case, the matter of what it is, putting this into the message that sent can't hurt. And Yeah. And also for our internal evaluations of whether it's correct and We were in the board meeting yesterday, were talking about, Tristan was talking about, our process for integrating code and research and so on and breaking systems. And so this is by putting more stuff in the message protocol right now, even if we don't eat it, that's a good way of some sort of insurance against breaking things in the future. Yeah. Yeah. And so this would also be a relatively minor change. we add another field, to the state class, which now the term state is really getting overloaded. Maybe this will be like point in sequence or something.

and then third, this is, a question that we may debate as well as updating our voting algorithm to take into account that state, We could just basically be voting on locations in four dimensional space or vote on it as a separate variable or like voting on Id, voting on state, voting on orientation separately. but yeah, I think especially with behaviors, it will, it'll be very useful to do fast enough inference on an object that's moving if we can also vote on where in the sequence we think we are. yeah, it feels, yeah. One, sorry. One, one other thing. One reason why we might not need it is if we have a global clock and instead of voting on state, a learning module's, very confident can just reset that clock for everyone. but remember, the global clock is not object specific, so at least that's not, I think it works in the brain. So it's, the clock doesn't tell you much, right? It just tells you how fast you should be transitioning. But it would still tell you, wouldn't it tell you where in, you would have to already know what behavior you're doing and then it would just tell you how to progress through that kind of, but it wouldn't the clock communicate. What I think the evidence, all the evidence we have is that the clock is just like a, a countdown between events. And so it, it doesn't, it's not a global time. It doesn't, there's no global time involved here. It doesn't represent where you are in the sequence. It just represents the time between elements in the sequence. So maybe if, I can use this metaphor, if you have a metronome, you can have a setting where it does a different sound of click at the first of four beats. So it would beep boop, beep boop, So would this global clock just be doing the normal sound the whole time? So it's basically just telling you what the intervals between each beat are. Are? Or does it also tell you, are you on the first of the forum? No, I think it's, I think it's the former I, okay. the evidence and not, this is not just neuroscience evidence. This is evidence from, again, I don't wanna rely too much of the guy who wrote the book, can't remember his name now. Your, this is your brain on music. anyway, after conversing with him and reading some stuff, it basically came to conclusion that, which could be wrong. Let's take this could be wrong, but this is how I think it is. it's just like when all sequences have to somehow be broken into sort of events. like beats on a melody, right? and or, not just beats on a melody, but like a note in a melody, an attack on a note, and then you, then the time basically says how long to the next attack on a note. and then the time, then sep then starts again, how long to the next, so you're basically just learning intervals, the duration between events. that's, at least that's how it seemed to work with music. so that's how I always thought of it. It's, and so it's just a, it only, it is only good for up about a second and then it doesn't work after that. okay. Yeah. Then in that case, I think it would be very useful to vote on the state. You'd have to vote on the Yeah, I think time, as I understand time in the brain, it's not gonna be helpful for voting. Yeah. And I was just gonna say the voting on state, the importance of that kind of feels analogous. Analogous to the voting, using locations for morphology. If we don't, if we don't vote on state, it's kind of bag of features in a sequence where it's two behaviors that are the same contain the same events, but in totally different orders, aren't gonna be distinguishable. I had the same thought. Something like that. Yeah, you expressed it nicely. Had the same thought there. There's our pattern recognition working like, Hey, this is like another problem.

Though, I think it's maybe slightly different because with morphology we can not move. like we have multiple sensors that are not moving and doing flash inference, but with time you have to move through time. You can't recognize a behavior without moving through time. But the analogy hold, it's not the same, but I think that the analogy that Neil said still holds. It's yeah, guess you can recognize some states, but yeah, you won't know for sure where you are. But, yeah, I don't know. Anyway, slash inference on a sequence, right? I suggest that we think of time for the moment. We should to think of time as, that is, it's a non-model specific signal and we should treat, see if we can fit it into this idea that seems to have a lot of support of, like a, an interval timer that everyone gets. And, and then it gets reset on sort of events. So it's really, obviously imagine what those events are. In music you have, all notes really have an attack. They have a beginning of the note, which it's really helpful, to hear that. And then you have a duration to the next note. and then it starts again. It's a little bit less clear when, if you're watching someone walking, for example, what would be the equivalent of the attack on the note? I don't know. I don't, I can't see that. so I don't know how time would play into that one. I don't know. So it's a bit of a mystery.

Yeah, but I still think it, doesn't feel, I, don't think it, I'm almost certain that you can't, the view of oh, if I know where the time I, my time is, I'll know where I am in the sequence. I don't think it's gonna work like that. Yeah. Okay. I have a question on state voting.

Yeah. So the reason we can vote between learning modules, because every learning module learns reference frames for an object independent of other reference frames. But there is a common body reference frame. So there is a way to translate from the other. Now, if we have learning modules, there's zero state, it'll be arbitrary. And if you have two of them that learn different arbitrary zero states, how, without there's, if there's just a unemphasized global talk, what zero state, how are they going to, what's a zero state? I'm sorry. Is that like the beginning of the sequence? like how are the two modules who learned? The states independently go, they don't have a common reference frame to go between them to translate the vote, is what I'm saying.

And in ref, when learning modules learn objects and different reference frames, there is a body reference frame through which they can skip and learn. and there's a translation that can happen. But what is the translation when they learn states. In different reference frames. How do I go from one learn translate from one learning module? Think it's actually easier than the location thing, isn't it? 'cause it's just an associative connection. It's just that they both have that state represented at the same time.

like the issue with voting on morphology is, yeah, there needs to be some common space that they're talking about. I don't think we need a common time. It's just you would associate the two states like snapshots with each other. Okay. So it works just like a sub id. It's just another id. Yeah, exactly. That's just changes faster than the big id. Yeah. Yeah. Okay. Got it. At least. so this state is not specific to, behavior, it's just, it is just like a global rotation of the reference frame? No, I think it would be, no, it would be specific. Yeah. So it would be a specific state in the hinge sequence. So it build associative connections between the states that are specific to behaviors. It would be a lot, it would require a lot of connections, huh? Because every behavior has its own set of states or, I, I'm confused here.

is this, I thought this was all, I mean I should just be quiet because I thought this all was obvious, but I'm confused by now. About the questions. I'm just, wondering if, so if the state is, is specific to a behavior, then voting would be a lot more complicated because now we need to associate, a lot more states together because it's right. It's, it, is be, I'm thinking in neuroscience here, so I'm thinking like, oh, this state is just like our sequence memory, the temple memory algorithm where, you've got, you, you're representing some particular pattern of, imagine you've got some feature movements that's the input to this learning module, right? There's features that are moving in different directions, and that's some, and then you're representing that, let's say in layer four, and then that's gonna be somehow represented in Minicolumns, and then individual cells will represent that particular, set of movement, things at some time. and these have to be high order sequences. I, if I'm dancing and if I'm running and if I'm doing something else, I'll have, I might have the same individual pattern. I have to represent different in different sequences. so I, what I'm guessing, I'm agreeing with that state seems like it's a SDR that's unique to the, that's unique to the, to the, the particular behavior. and those, would be difficult to, to vote on. So I don't know. But yeah, to your point, rammi, I, I feel like yeah, there it is a lot to learn. I think that kind of fits with both the fact that learning behaviors is harder for humans than learning morphologies. Like we, it does take us a lot more time studying it. And also we tend not to probably learn that many behaviors, like it seems like we learn compared to how many morphologies we learn and recognize. There, there are more, widely used kind of thing. And yeah, it seems not inconsistent with the fact that it's like it is a more effortful thing to learn. And also if you think of it being the same mechanism as we could use for different states in a morphology model, it's like both the open enc, closed staplers are a stapler. It's both the stapler id, but they're different states of the stapler. So it's like a sub id. It wouldn't make sense to have the sub Id shared between other objects because it's not something that, like those two looks of the stapler don't really generalize to other objects. Yeah. But what are we, I was thinking we could, what we're trying to vote on is where we are in a sequence, not what the actual state is in that. Like we're not trying to vote on a halfway open stapler. We're trying to vote on, we are halfway through this sequence or just halfway through a behavior. I, I'm just saying that if we could extract some meaningful information about just a location in a sequence rather than, the full state, isn't that almost too general? 'cause like basically every behavior will have a start of a sequence. And so if they're all saying, oh, I'm in the start, if that's like the thing they're voting on, I don't know. That feels yeah, confusing. That's fine because there's a, there's also a, we're also voting on the behavior id, so the information is there. it's similar to how, we had the global rotation of the, in layer six B, we had the global rotation of the object or the reference frame. And we're just voting on that global rotation without any, it's agnostic to the object itself. Here I'm just saying we can extract information that we can vote on, in, with, without any respect to the object itself or to the behavior itself.

that makes sense. May I suggest a way of going forward here perhaps? So last week, in the last two weeks, maybe, we spent a lot of time talking about how columns could vote, taking advantage of the relative positions. we don't have a problem for a single learning module. Single learning modules is just fine. And, and that's true here too. A single learning module here just is fine. we're now introducing complexity of voting between, learning modules. I don't think we resolved all the issues with voting between learning modules, for the mythology models. we, so I was, I've been working on that the last couple weeks, last week. 'cause it, I don't think we resolved that. I think we, I don't think we're gonna resolve it here today either on this one.

and the solution was probably gonna be the same or similar. So all I'm saying is this updating the voting algorithm to take account of state. Yes. That has to be done. we have to update the voting algorithm to take advantage of, I know we've done it in Monty, but I don't know if we've done it rightly, for morphology models. and anyway, I think there's some holes in it, so maybe we can just say, yeah, there's, there has to be done, but we don't really know how to do it yet. exactly. Yeah. Yeah. Okay. Just one last thing. If we're voting on the state that's specific to a behavior, do we actually need to vote on the behavior id? Isn't that redundant? it flows from it, right? It is redundant once you know that, if you know the unique location in a sequence, ANU cation object. it is of course tooled into the I object id. yeah, and I think it's kinda like a robustness thing. Like you can go from one to the other. You, it's a one, you can go from the unique one to the object id, Yes. That's temple pooling. but doesn't mean you still don't have to, okay. You still need to know where you are in the sequence. You still need to know where you are on the object. Yeah. And, actually that's what we do with when we vote on locations as well. We're actually not explicitly voting on object IDs. We are just voting on unique locations on each object. And since the locations are unique to each object, yeah. We don't need to, we don't do an additional vote on just the object id.

Yeah. that's part of the, what we discussed before about what are we actually voting on and, between locations. and I think it, it has to be, abstract of, the object itself. Even though the location, the layer six A is, is a specific reference frame for a specific object. But what we're voting on is not specific to the object, probably. I don't know. I don't know why you reached that conclusion. I thought we, I thought I reached the opposite conclusion. I always, I feel like all voting has to be specific to the object or the behavior Id. Yeah. Yeah. Maybe, we come back to this at the end. If, we have time to discuss the voting We're gonna, we're gonna fall into the following, but yeah, definitely write that thought down Ramy so that we, we remember to talk about it at some point. Okay. Sorry for us not to get back to voting, Yeah, that's right. We're trying not to do that this week, although I still stuck on it, yeah. Let me briefly, let me continue and then we can get back to voting. No, let's keep going as we all want to apparently.

so now I think the biggest thing that we need, these are still bigger changes, but another big thing is that we'll need a test bed. A place where, an environment where we can learn and recognize, object behaviors. And requirements for the environment, I was thinking of was, the objects move repeatedly in the same way. there can be different morphologies with the same behavior can be found in varying orientations on different morphologies. the same behavior can happen at different speeds and objects can stop moving at some points in the sequence.

and then potentially this might be like a next level thing. That behaviors also involve changes in features instead of movement.

and so some questions would be for first setup to test, this is one learning module. Enough. And my proposal would be, yes, we could start with one learning module, but probably only if we supervise the learning and tell it at each step. so how do we supervise? We would provide the object ID pose and state during learning. So we would tell it where it is in the sequence so that as the sequence repeats and it moves over the object, it knows like where to lay down the points in the sequence. and then how would we evaluate the same way we do right now, just including state as one of the outputs.

and one other thing I was thinking is we could start doing this, testing this in a 2D environment, since that would make it easier to visualize the third temporal dimension. But I'm not sure if you have some thoughts on why that would be a bad idea because we might miss some 3D issues. it's interesting when you were talking about this, I was thinking like, oh, think about walking, right? So imagine, and I can recognize someone walking into walking left and right. I can walk, recognize someone walking towards me, I can recognize someone walking away from me. And those are really different patterns. And so it's like behaviors can be three dimensional objects, three, the behavior can be three dimensional movement vectors. and I have to be able to somehow infer. Just like I can infer an object via rotation through plane, I have to be able to infer behaviors, in 3D through plane, a rotation through plane. So some, somehow I have to be able to associate someone walking away from me with someone walking left to right even though they're very different patterns.

I just throw that out 'cause oh shit, that's a complexity that's difficult, Yeah. I mean I think it's a similar complexity that we have, like you said, with recognizing objects in 3D space at different distances and orientations. Yeah. That basically it's the sensorimotor module that takes into account the depth information. It estimates depth, then takes into account depth information to get it all into a consistent 3D reference frame. Same here as how I'm thinking of it. It would be the burden of the sensorimotor module to take into account depth to calculate how the flow detects is actually movement in three dimensional space. All right. So if you feel comfortable that the same methods we use to do three dimensional rotations of morphology objects would work for three dimensional rotations of behavioral objects, then I think you could do with a 2D environment to start. It's this, that's just the test out the code. yeah. I think it's a good point though, 'cause I'm not com I don't know the literature or like methods out there for 3D flow estimation from like a 2D camera image. So I'm not sure how accurate those are. so yeah, it would be something good to at least, look into before we just, because it might be a bit more noisy with if you only have a small patch that only recognizes some flow pattern to then figure out how that corresponds to Yeah. Can we, can you detect flow in plain The flow in the Z direction? Yeah, exactly. yeah, I don't know. I think in the end, even if we're not good at it, it's just, it just consider it as noisy input and, the system should work really well with noisy input. So it, it, may not be, fatal flaw if it's not as good and Right. Yeah, I think it, yeah, I think it shouldn't be a fundamental problem. It might just be a bit more noisy. So it might be harder for me to detect someone, walking away from me, different from someone, running, running or maybe I'm trying to, I'm trying to think. There's certain behaviors which I have much more difficult time in inferring if I, if most of the movement is in plain, right. If it's like I can't see, I don't know. I can easily imagine it would be more difficult sometimes, to see what's actually going on. Alright, so I'm not, I'm not a decider here, I'm not gonna implement this, but I, it sounds to me that you could, if you wanted to do 2D it seems like it would be pretty safe to go that way. And also, Jad, if you remember Jeff, had implemented a dataset, that we could potentially use for this had a stapler kind of type object and stuff. no, it could be a I have, okay. I'm sorry. Yeah. But now that you mentioned sounds familiar. yeah, that's what I was thinking too. We could use that, environment he set up, in Blender. We could also animate any of the objects. Like we can take a YCP object. So just add some, we just gonna make it animate if we want to. We just changing the mesh.

Okay. Yeah. If that's not too complicated, yeah, then we would be back in 3D space. But, yeah, same. We can, we could do it in 3D if we wanted to, can create a well theory, we should be able to create a system that would work in 3D and then start by testing 2D. yeah. that would be, yeah. The general approach I would take, I would make it like it wouldn't change to send the learning module to be 2D space wouldn't change that, but just only show input with X and Y coordinates and Z is always the same. why do you, I didn't understand this question about supervision.

that seems like you have, you're providing an awful lot of detail here that wouldn't be available. Is that, yeah. yeah. I'm thinking of it as a first step. So I think if we would start with unsupervised learning, we would run into a lot of different problems, which we will have to solve eventually. But I think as a first step, how is it different than we did unsupervised learning for the sequence memory algorithm?

I, it is not obvious to me how this is different than that. And we solved most of those problems in this sequence memory algorithm.

are you resetting it when a sequence starts? does it detect when it starts and ends and when it repeats? Okay. That, that is, that, is a difficulty. but we never, provided state during learning, but it, because we were with a, HDM, you were getting like a unique feature each time so you could immediately know where you were in the state. no. Just the opposite. The whole point was you didn't want to have unique features. You wanted to have common features. Yeah. And rely on the high order sequence.

there, there were, the problem, if I recall, the problems we had were along the lines of, you're trying to infer something. And yet you might have to be learning something new. And maybe the thing you're trying to for isn't, doesn't exist. You're, hearing a new melody, but you don't know it initially, or it sounds like the beginning of the melody is similar to another melody, but now it deviates. so I think we had issues with knowing when to, start creating a new sequence, learning a new sequence, and when to keep trying harder to infer what an existing sequence that Yeah. That's similar to the issues we have with the morphology models and unsupervised learning. the one that caught me here is that you're providing state during learning and, yeah, what I, the main thing I would wanna provide is telling it when the sequence starts and when it starts repeating again. Yeah. Yeah. Because I think that would be very difficult to, for the learning to figure out if it does not have a model of the sequence at all yet. Yeah. One of the problems I think oh yeah, just going back to my earlier comment, I think what I was getting into, because yeah, I agree with your point that the whole point of HDM is, it can handle like a particular feature appearing multiple times, but it's just here, it's, we have this issue, we just have this tiny receptive field. And but we need to know what the state of the object is in order to work out where we are in it. and so the alternative to this is voting, but I think it's just a way to avoid having to sort out voting from the very start. Is to just provide that sur supervision. I don't think you need voting. I don't understand why that, I think it's more imagine we're just looking at something moving up and down like the stapler had. or maybe it has a little fancy pattern. It goes down and up and down. So it, there's, throughout it sequence, there's a couple points where the, input is the same.

I think it, it would automatically learn to represent those equivalent inputs uniquely just because they're in sequence. I think Viviane hit the nail on the head, which is the problem is when you know a sequence begins, how do you know you're starting over? and that's the problem. We never really, there was it never completely solved that. If you think something like a melody, we usually have, there's usually a break of some sort between the beginning of a melody. When you play it again, you don't, just continually go right from one melody to another, repeat it over and over again. But then there are sequences in the world like walking where there is no beginning and end. it is a repeating loop. there's no demarcation at all where you might say, this is the beginning of the walking sequence and this is the end of the walking sequence. We never resolve that. That doesn't exist in music too much. That would be like background trance or something like that. yeah, but yeah, we could use some signals like the stapler stopping move, stops moving when it's at top and stops moving at the bottom right marks. But I think, actually what Niels pointed out is another difficulty with that, which, that the sensorimotor might be at different locations on the objects. It's not like it will be sensing the same feature again 'cause we are moving the sensorimotor while the object is also moving. Seems like during inference is not a problem. But during learning it's a problem. Is that correct? Yeah, exactly. I'm saying yeah, only supervised during learning. And so now we're getting back to the idea that, oh, maybe the problem here is 'cause we don't have a good, voting for learning type of thing. Yeah. And I think actually when we learn behaviors, a lot of them are learned by interacting with the object and actively moving it through the sequence. So we open and close things many times and know, how like we are controlling how we move through the sequence. so maybe that makes it easier for us to learn them too. Maybe. Although we've decided there's certainly many cases where we don't, we're not doing that and so we, melodies are like that or the or someone walking is like that or metronome sweeping back and forth. So in those cases we have to be able to solve those too. so I'm happy to agree.

I'm happy to say, oh, go for the supervision as you suggested it. and, we're just gonna say the problem has to be solved as a voting problem during learning. it's while I'm learning, I need to have other information that's about the rest of the object type of thing. is that a fair statement? Can I say that? You'd be happy with that? yeah. Yeah's, because I agree. Yeah. I feel like there's two problems. there's this unsupervised learning bit, the start end of the sequence, and then there's learning when the behavior happens relatively fast, which is most behaviors that, so we kind somehow vote on state. We had this from the very beginning. we said, Hey, it's easy. It's a po You can imagine a learning module has a complete model of the behavior that is internally. It knows what movements should be occurring at every location at any point in time, but it can only observe one of those locations at a time. and, and yeah, learning through a straw would be impossible unless it's like moving so slow that, maybe I think the real problem is the learning problem. Alright. The inference problem. You could maybe, you can imagine my, my, yeah. No, I'm, talking about learning. yeah. So this is the thing I was focused on last week, which I'm not prepared to talk about today, but I, keep coming back to this issue that. We have a learning problem in, Monty, we have to have all these learning models learn the same model, yet they don't get exposed to the same things. And so how do they somehow end up with the same model when they don't all experience everything. So we can just leave that as a problem to be solved. And then I'm good with your proposal here.

Yeah. And yeah, this is just a proposal for a first environment, a first way to get money to learn and recognize object behaviors, and then there'll be further steps on top of that to, to Absolutely. I just wanna make sure that we're not going down a path that we're gonna regret. and I don't feel that way. I think your answer, and that's why I'm just exploring these corner cases because we have to solve this problem. This is unrealistic to provide the state during training. So we have to solve that somehow. But I think as long as we understand, oh, that's the problem we've already been working on, we're gonna find a solution to that, Okay. Good. Yeah. Yeah. So that's one of the open questions I would keep on the table to discuss in all the next research meetings, so we can Yeah. Figure it out. Yeah. And we would do that.

oh yeah. Any other thoughts on the kind of environment, the first test environment?

Okay. So then, yeah, just a couple of open questions regarding one and two. besides the environment, so one I mentioned earlier is, can we, I interpolate in the temporal dimension. So for example, in Monty, that would mean basically just performing K three, like nearest neighbor search in 40 space where all of the points in the graph are X, Y, ZT coordinates and we just look for nearest neighbor in the temporal dimension as well, which I feel like is actually a a nice property if we're able to interpolate in time as well.

but yeah, I don't know if you have any thoughts on that.

Are you saying basically we can't, store just like we can't store every point on an object, we can't store every point in time. Is that basically what you're saying? Yeah. Yeah. And then I guess the biggest implication from that is that we would represent state in the sequence as like kind of part of the location in the four dimensional model, which I think is really easy and elegant to do in Monty, but I just have a hard time imagining it how that would work in the brain if we have locations represented in, layer six A and then time and layer one, how that temporal dimension would become. I don't think this is the way I think about this, the way we've been talking about time, the way I've been talking about time. It's not a regular dimension.

It's, it's time is a timer between events. Just let's to say we assume that for the moment, the, sequence, the, you have this, series of SDRs, which represent, points in the sequence, and they flow, they a associated with each other. It's in the temple memory algorithm, right? You go from state to state. So the sequence of the flow, the order in which events occurs is dictated by the, not by time, but just by the association between the different SDRs and layer four, for example. So time, the only thing time does is it adds, how fast did you transition between these states? Again, if time doesn't represent a point in the sequence, it's just a duration between points in the sequence, then there is no, there is the time doesn't really represent a fourth dimension, in the, sense that you can't, there's no representation of where you are in that time sequence. Yeah, I guess what I'm getting at is let's say you have four slices of time, four beats that you, perceived and you store different changes at different locations for each of these beats. But let's say it like. The second beat. You haven't observed a change at this location. Could you still retrieve the changes at the, slice before or afterwards, and if they're sufficiently similar, that kind of tells you what you should be expecting at this time? Step.

I think I, I'll flip around and say, I don't think you can go, oh, say what will be my state be six times steps from now. The only way you can do that is to walk through it that actually flow through the patterns and, to get there, you can't. So, that's, a different question you asked, but I'm saying I don't think it's possible to say I'm a time T one. And what would be the state of time T 10? You can't do that. You could say, what's time T two? What is time T three? From T three, you go to three and three to four. it's possible that you might be able to predict, one or two steps ahead and one or two steps backwards. that the system might actually represent those in the same way we've talked about phase possession and grid cells that, the activ actual activation of the cells in the cortex, again, not Monty, but in the cortex might go through, it's like a sequence where you'd, the same cells would fire in, in at different phases and could represent, a little time ahead and a little time behind. There's no way to jump ahead 10 steps, Yeah. I'm not thinking 10 steps. I'm thinking like the step before and the step after to be able to interpolate when you're missing points. I, yeah. I think that's a reasonable assumption. if we take the example of phase possession and grid cells as a, an example, that's what they do. If remember, okay, as you're moving to space to grid cells, they represent where you're going to be. Then they represent where you are, and then they represent where you were. And they do this all rapidly within one cycle, one, cycle of a background, frequency. Yeah. Yeah. Yeah. And then maybe to paraphrase, kinda what you were just saying, Jeff, like when we talk about these kind of slices, it seems like yeah, referring to them as kind of state or like state in a behavior is better than, because then as you say, time is like on top of that. That's like time has to do with when, how long between these, but that, that can vary and all this stuff. These are independent of time. They're just, or not totally independent, but like they're just slices in. Yeah. In behavior. it's just like in a melody. You cannot, you can imagine the next note, then the next note, then the next note. it's very difficult to imagine the previous note. You can't really do that. You can't work a melody backwards. but that's the way the temple memory algorithm works too, right? It just goes, okay, from this date, what's the next one? What's the next one? The next one. so it's a sequence. the neurons themselves represent a sequence without time, and then the timing signal is just this transition between individual elements. Is that right?

I don't know what to say beyond that. So yeah, I think that's a really nice way of putting it. If we're gonna infer through time, we can only do a little bit like you suggested, you can only go like maybe a little bit forward type of thing.

Yeah. Okay. cool. but then, yeah, that sounds like generally, and Monty, we could do a lookup in like the state space, to the slice before and the slice after, to do a bit of interpolation.

and then this one, I, we already talked about the time signal gets provided externally to all of them.

how does the learning module know when the sequence repeats? I guess we also talked about that. We will talk about this later.

but this is, yeah, still an open question to think about.

how would the learning module infer and adjust the speed of a sequence?

so I think there, it's, oh yeah, go ahead. No, you go. So yeah, the pro, I think we already have a fairly concrete proposal about that. I just included it here because I didn't include it in the changes to Monty. But what I was thinking is that basically whatever temporal offset the learning module notices between one state and the next state will be the speed it will communicate back to the, global clock.

Yeah, exactly. Yeah. That's what it feels like that, oh, Jeff, you're muted. yeah. it would be like, it would, it can only say, Hey, my, my event occurred sooner or, later than it expected, Is that what you're saying? Yeah, it's the actual time. Yeah. That's a nice way of signaling that. The actual, oh, to say half a second occurred doesn't tell me anything. It's just oh, I was expecting it to be four tenths of a second 10, and now it's half a second, so things are going slower. Gotta speed it up. We slow it down. Yeah. Yeah. Things are slow. Yeah. Things are going slower. I remember that we talked about the, this tempo thing, as being similar to scale in, the morphology, like in the object and Right. I also remember that scale from the morphology columns, It used to be a signal that's stored in inside the column, and it was just basically sending back information to the, to, modulate the inputs.

I was wondering how, 'cause now we're talking about the tempo as being dictated to the, to the column from external sources. it's rec a reciprocal thing. It's like neither the global clock nor the learning module are sufficient. It's like the global one tries to define a tempo, and then the learning module is making the behavioral specific predictions. And depending on whether those predictions are coming too early or too late, it feeds back to the global one to say you need to speed up or slow down. So it could infer, if it's getting a, also like multiple inputs, it could infer that tempo and it could send back, to a thousand. it doesn't infer the tempo. It just says things are happening quicker or slower than I've learned it at, than the model that I have. The model I have, the amount I'm thinking about it. It's a very different idea of time than most people think about it. Just this is not what most people think about time. and it's an interesting question in Monty, do we wanna do time the way the world thinks about time? oh, it's this dimension and you are some along this dimension. Or do we wanna think about the way the brain thinks about time? Which I don't think is that way at all. I think the brain thinks about time, at least in certain situations, like melodies as a sort of an event timer between events. and, and so this is one of those cases where I'd say, we probably think we should probably go down the way the brain does it until we know why we wouldn't wanna do it that way. it's just, it's weird, but, there might be good reasons. The brain does it the way it does it. I don't know. I'm just rambling about this a bit, but, so I don't, it's a kind of unique thing the way time represents. it's similar to little bit how scale is represented, but not quite the same, right? And it feels like all these things like rotation and scale and stuff, we could probably have like model free signals that also give some oh, everything's happening really fast right now. start your kind of hypothesis of what the tempo is quite quickly or something. and then, And the same thing. Yeah. Just to make it a scale, right? Scale, model. Free model scales. Model free, as you just said, right? It's yeah, this is too big or is too small. Yeah. Just, yeah.

okay. Yeah. So I think we have a fairly concrete idea of how we would do this in Montana. Can I just throw one crazy eye on you? Sorry. Sure. we were talking about voting. Last week voting, using relative positions. that also relates to scale, right? Because, if I have two learning modules that are detecting features and the further apart or closer than we expected, then it tells me something about scale. So maybe the scale of an object is gonna be solved also with, voting based on relative positions. Maybe they're tied together the end of advertisement.

Yeah.

okay. I'll go on with the questions, before we drift off to voting again.

another question is, can we have other state change signals besides time? so for example, actions, like other things that get us from one slice to the next slice.

and don't, I don't think we necessarily need to answer this for the very, for the first implementation or first prototype, but I think it's still an interesting question to think about for a general design of it. I think you're right. the answer is we have to have this right.

the light only turns on when I put, when I hit the switch. or the, as the staple only opens when I, grab it, right? we know the answer to this question, right? The answer is yes. It seems right? And then yeah, the, then the question is, oh, do we have, yeah. How can we go forward without it for now? I think we should. Otherwise we sit here and research hell forever.

so is it, I would turn this around saying we know this has to occur. can we, can we go forward without it right now? Is that's how, yeah, that's how I think the sentence should read. Is that right? Yeah. Yeah. I agree. I, that's how I was thinking of it. I, for the first prototype, I, wouldn't include actions as a way to move through this, but I think, yeah, I agree. We should design, think of it as this will necessarily need to be included at some point. And this is actually like one of the one or two big remaining questions after this presentation. Like, how do we get actions into this picture? so yeah, I think that'll be a good one to just talk more about in, in following research meetings. But yeah, we don't have to answer it before we could implement something that works for just temporal behaviors.

Yeah. Then maybe related, but does the fourth, we also briefly touched on it, does the fourth dimension in the morphology models correspond to time or just discrete states that are associated with times in behavior models? So yeah, we touched on it when you asked about this label here. Basically, do we get temporal inputs here that transitions between those? Or is it just like discreet slices and then there's associations between behaviors that get time as input and, yeah, I The answer, yeah. Yeah. It sounded like at least one possibility is like these labels should all almost be state and then all, of 'em can get time. but whether they use time, I guess depends on the behavior. 'cause some, might be driven by time, some might be driven, as you say, by actions. You Yeah. It, strikes me is that if I have a morphology model and, and it transitions between states, it moves, if the neural circuitry would say, Hey, if those movements are predicted, that is a, they're in a sequence. In some sense. It would learn it, it would just say, oh, this state, follows this state.

it gets to get back to the traffic light example before, right? There was no movement there, really, there's a change. So that's maybe, but, there are states and they follow one another, and so I'm gonna learn those transitions. I guess in a very high level point of view, if we assume the same neural mechanisms are occurring in the learning models, that the learning behaviors and the learning models of the learning morphology, that if the morphology goes through a sequence of predictable, transitions, those transitions would be learned. I, it just seems like they would be, so yeah. I don't know what to make of that.

Yeah. Maybe also what Neil's mentioned earlier, that sometimes there are just morphology changes that happen, without a detectable flow or anything. Like when my laptop is in standby and the screen is black and then I touch the keypad and the login screen pops up, it's like a predictable sequence. But I wouldn't necessarily say there's any particular change stored here, but more like one morphological state going into another one. And they would, and those associations would be learned as part of that model. So, now it seems a little, maybe a little clearer. I, that we have these two learning modules, one's focusing on changes. And the other is focusing on static features. they both can learn sequences.

if, but the sequences in the morphology model are not flow. They're not, it's not movement. It's if things move, the morphology model doesn't know about it. It's when they stop moving. Then the anthropologist is, oh, here's a new, here's what it looks like now. then they learn this, those are your key frames, something like that. And so it could learn the transition between key frames, without learning the flow, the motion. So if I was walking in my arms and continual motion, I wouldn't learn the, necessarily learn that in the morphology model. but if I stop and freeze and I could learn what that pose is, I, yeah. Yeah. It's interesting. It would be nice if the learning modules all work the same. They could all learn transitions. I was just gonna say, yeah, it feels like it's movement in that direction, right? Yeah. If it fits really nicely with using the exact same learning module for all of it, there's no difference of whether it gets morphology or change input. Both can learn temporal sequences or nont temporal sequences. and yeah, there's just, and they can both take advantage of time. the traffic light could be timed, Yeah, exactly.

okay. Nice. Yeah. Then another question maybe related to the environment and setup is like, what would be the learning modules policy? And as we touched briefly on that, we don't wanna do a smooth pursuit, of the behavior, but like, when should the sensorimotor be moving to learn that behavior? Or should it remain static while the, it goes through a sequence once and then move to the next location, then it goes through a sequence again. like how so we, so, this is a real critical issue here. I think, and, we never, it's seems impossible. Like how can a learning module observe all these things, right? And jumping around and you've already proposed Hey, for now maybe we do a shortcut and we just train it. We just give it everything it needs to know, cheat, supervise it. I think the answer to this question will come back to us again, having to do with this voting 'cause I think columns partic the idea I'm working on right now, how do column, how could columns train each other, at any point in time. that is one guy's getting in one column's, getting input and is learning something, but it's gonna transfer that knowledge to a whole bunch of other learning modules. and The answer may not be, how does the single learning module do this? It may be impossible for a single learning module to learn behaviors. yeah. Where it wasn't impossible to learn morphology. 'cause we could take our time moving around, but it might be impossible. And you can imagine that you're just looking at the world through a straw and you're looking at something moving. Is it ever possible you could learn what that behavior is? it seems doubtful. we moving all over the place. what the hell is going on here? This is so confusing. so the answer might be a form of supervision. yeah. That, that it's being supervised by the columns that are observing. Yeah. So I wouldn't put too much time on this yet. I would say supervise and then we'll figure out, it may not be a single columns learning, learning po policy. It would be the group's learning policy. Yeah. Yeah. That, makes sense. And then for inference, I think our current policies should still work.

yeah, if the this, yeah. We also kinda touched on if the learning module follows the object doing smooth pursuit, how do the sensorimotor movements get processed? Like feature movements? but I guess for now, the conclusion is we're not gonna do smooth pursuit.

Okay. That's, those are all the open questions to one and two. oh. Yeah, I know that there's like more section on like how number three learning association comes naturally from one. but it's also close to nine 30. And I know also NS had something that he wanted to present, and we also wanna get back to voting. So should we do a part two? I don't, we, I don't, I didn't think we're gonna get back to voting today. Were we? I'm not prepared to do that. Oh, you're not? Oh, no, I said I working on it, but I said I'm not prepared anything. yeah, I think it's fair to wait with the voting, but Yeah. But appreciate you bringing it up, Hojae. Okay. I appreciate it too. I think this is good progress. I think we should, I don't know what Neil has to Niels you up to speak it. It's something super important, but, so yeah. Again, thank you Hojae, but yeah, I'm, happy to, if, you wanna continue, and then, if yeah, we can see if there's time at the end. yeah, I mean if you like, I can only do 0.3 'cause I think it naturally falls out of what we already have. Can you remind me what 0.3 and 0.4 are? I don't remember what they are. Okay. three was learning associations between behavior and morphology models. Okay. And then four was compensating for the object movement to make accurate predictions in the, okay. Yeah. So three feels like quite a simple one. Four is like a can of worms. Yeah, exactly. So I don't have to open up today. Maybe just doing three. yeah. Okay, cool. Yeah, we'll wrap up. So when you said can of worms, I just imagine there's this can all these worms in it wiggling around. I'm trying to learn the behavior of the can of worms. oh that's a, it's like the, that's like the t-shirt equivalent of behaviors.

We'll have to have Monty learned the can of worms behavior at so point. You can look at it, you say that's a can of worms, but how the hell, you know what it is if you can't make any prediction. But it's like a t-shirts all crumpled up. It's I won't open that can of worms today. And I'll hand over to Niels after the third section, which I hope is quite short. so yeah, learning associations between behavioral and morphology models using hierarchy requirements. I wrote down nothing in Monty after compositional modeling milestone is complete. because yeah, as we said, we basically wanna use the exact same learning module structure, the exact same communication between them. Only difference is the kind of input they receive. So we can just reuse all of the composition modeling machinery that's already there. And so the idea, how I would think of it is that if we wanna assign a behavior to a location and orientation on an object, we would just take a. The behavior ID and it would become input to layer four of the morphology model. And that would assign the behavior on a location by location basis to the morphology. And I didn't draw this here, but it would also get the orientation, of the behavior relative to the parent object. and then on the feedback connections, when we just see the morphology that can bias which behavior we expect at that location as well. this is really elegant. the only tricky part I'm thinking about is, for this to work, like when we talked about V two and V one, the column on the right had to be observing the same location or basing input from the same part of the retina as the column on the left.

that had to be co-located in the sensory array, right? So now if we've separated out, the behavior learning modules, there's someplace else like MT or something like that, we still have to have that relationship that is, there still has to be a one-to-one relationship between a column on the right and a column on the left. So representing the same location it's in, in retinal space. That may be fine. It would be easy to do. I don't know. it's something easy to do with Monty. Not a problem at all. Yeah, it should be easy to do in Monty. I'm not sure how. The evidence is in per, in the MT mapping. So it's it. So this goes back to your proposal of separating these two out, which I still think is the right thing to do. I just wanna point out that it makes me nervous a little bit, because now we have to say, oh, there's gotta be this to this retinal topic mapping between MT and V two, for example. maybe there is, I don't know. Maybe it's not a problem.

so I just throw that out. Something to think about, but Monty doesn't care. Yeah. You can do this in Monty. As long as we agree that's the requirement, those have to be co-located in some sense. Yeah. And yeah, I can look back into the literature, I think, yeah, MT is still very, has still this kind of structured organization that I'm not sure what the relationship is between columns that have feed forward and feedback connections with each other. They are co-located, but I'm sure there's some, research out on that. Yeah. don't worry about it too much. And, it's, we good to worry about it, but we got a lot of other stuff to do too. So True. I think we've, I think we've committed to this direction, so Yeah. Go way.

yeah. Then one other point I added here on the side is that whether I'm not so sure about is whether we need to associate date in the sequence. I'm not sure if, it might be sufficient to just associate that the behavior exists at this location, but then if we are in a specific morphological state, that kind of tells us also something about where in the behavior sequence we should be. So yeah, I this is not right after what we discussed now that this kind of global temporal sig signal wouldn't be able to tell us this, but, maybe there, there needs to be some additional mechanism to communicate state in the sequence as well and associate that.

And yeah, in Monty, again, this would be relatively easy 'cause the status is, part of the CMP message that gets sent to the parent learning module. So just, laying that down is another feature at that location and slice in that model, shouldn't be too difficult to do, but, in the brain, I'm not sure how it would be done. No, it's not clear to me this is, bidirectional. That is, you've shown here in some sense, the behavioral models are a child of the morphology models.

and it wasn't clear to me that's the right order.

could it be that the morphology models are a child of the behavioral models and, I'm, it's a little confusing to think about, but I can say, oh, if I see the stapler. I I would predict the movement. I would, but if I see a movement, yeah, that would be the feedback connection here. But if I see a movement, yeah, I guess that's the way you've drawn it. If I see a movement, would I necessarily say it's a stapler? I guess if I saw the, if I saw most, if you see the movement, it's just the feature. So any kind of morphology model that has a hinge behavior would get that feature as like evidence for that model. So we just bias this towards any object that has a hinge behavior. Oh, maybe I was jumping ahead trying to predict what the morphology would look like based on the behavior and we didn't solve that problem.

yeah, that's 0.4. That's the kinda, Okay. This is good. This is good.

but yeah, I think the, at least in my mind, if the morphology model would be the child and the behavior model would be the parent, the behavior model would only get input if the, something about the child object is changing, if the child object is rotating or moving in space, then that would become an input of the behavior model.

lastly, we potentially want to add associative voting on Id. This is again, nothing we need to talk about more since we already talked about voting on, not ID on stage. is that right? that's, that would saying I see, I'm looking at a stapler.

that wouldn't tell me that, I guess you're saying is in general the stapler has this behavior. I'm not saying where it is, I'm just saying Yeah, it would just bias and it's just saying bias. Oh, staples. and that would work well with this issue of, maybe receptive fields, aren't always co-located and things like that. But it is just I don't know, you see a dog and maybe the barking behavior is it's a, it's a bias, but it's not like you start hallucinating, barking or something. You don't hallucinate it. You don't predict it. You don't even predict any details about it necessarily. Maybe some dogs you could, but in general, but, if a sound occurs, you'll be more likely to interpret it as a bark than as than something else.

Okay. That's good.

so then, yeah, I think the main thing for 0.3 would just be to have some way to test this capability. So we'd want to be able to assign different behaviors to locations on one object. So basically have objects that have different behaviors at different locations and orientations. And then we'd wanna test seeing a static object and inferring the behavior. seeing a behavior and biasing which morphology object to recognize and the state in the behavioral sequence can activate a state in the morphology model. And vice versa. if we're at the end of the hinge behavior, we would activate the open stapler morphology representation. I guess on B point B, we were just saying that you would say seeing object, beha object. Static object, and biasing behavior. Yeah. So there I was thinking of the feedback connections. We, we, could solve it just with the feed forward and feedback connections. But yeah, if we don't always, then would be, that would be seeing a static object at some location with it for behavior or something like that. Yeah, exactly. That would be, I think that would be the ideal solution. But like Niels mentioned, if we don't always have co-located receptive fields, then maybe we need, we would need to add some associative voting in addition. Okay. so if I take these words exactly, literally, then they need some more elaboration. 'cause we can, different ways you can interpret this. But anyway, we could just say generally we need to test out these feed forward and feedback and actions to see if they work. Yeah. Just a question here. we can associate multiple behaviors to a single location with this mechanism. Could you, it, it seems reasonable to think that maybe, some morphology can have multiple different behaviors. I guess so, so like a body could, a leg could be jumping or flexing or Right. Stretching its toes, something like that. Yeah. So yeah, we should be able to do that. Okay. So it would be biasing because, inferring implies like one-to-one. I think this is just basically what Jeff is saying also, or Right. it, generally case it biases for one B one B. Is it, you're saying it's, yeah, I can just change that to biasing. Yeah. If there are, can be multiple, behaviors at one location, then it's, yeah, still biasing. But if there's just one behavior stored, then it should be predicting that one, one behavior. But, the example of the leg is a good one, right? You say you have a bunch of motion dots on different points of the leg, and, then you could say, oh, there's lots of behaviors that those, all those dots would participate in different behavior. Yeah.

okay, so last slide.

yeah. I think this relates to what you mentioned, Jeff. If the morphology model is the child and the behavior model is the parent, what would the input represent?

and so we wanted to represent changes in location and orientation of the object recognized in the child column. So if the staple, if the child is the stapler column, it says, alright, right at the stapler top The stapler top is changing its location, orientation, and that change, relative to the behavior model is sent as input to that behavior model. I'm saying what this, does this feed forward? Make sense? you're saying it does.

what's the, giving an example of what this, what, when this, what this represents?

maybe just said it. I missed it. I'm sorry. So the child column represents a morphology model of the stapler top. Oh yeah. Okay. They recognize the stapler top and it recognize Oh, stapler top not the whole stapler. Yeah. and the stapler top is changing its location orientation, like it's rotating as the stapler is opening. So really what we should just say that the child object is representing a child of the parent object. It the state, remember the staple top and staple bot and the whole staple? That's a weird one because they have to figure that out. But we could just say there's a child object on the left and we recognize the child object. is that okay to say that? yeah, I think that's fine. Okay. So now we recognize a child object. Let's keep going forward. Let's keep going. So I see, and we would wanna send that kind of rotation change that is being recognized of that stapler top to the behavior model that learns the behavior of the stapler.

and so for the orientation, that's relatively simple with the existing mechanism, we calculate the relative orientation change to the parent's columns, reference frame, and give that as input to layer four.

but then for location changes, I don't know how that would work because, locations are represented as locations on the stapler model and not locations of the object and the environment. So those locations change as the sensorimotor moves over the object. and we would wanna communicate to the parent column would be location changes of the object in the environment. a bit of what we talked about, last week about having, but yeah, that this would require like a cent, Of the object. And then I'm really confused by this. I don't know if anyone else is confused. It, seems to me if I just see a child object and then the child object changes, like it changes its orientation. Yeah. It seems like the, parent object would detect that. The parent behavioral object would detect that. I, it just detects it as motion. Yeah. So you mean this behavior model would just rely on rough sensorimotor input to detect that movement, maybe.

It, yeah, that's the other potential answer that it just doesn't use that information from it just seems like if I see a, if I see a child, so listen, I looking at the logo on the coffee cup and the logo has, it, it rotates or it has some big change. It does.

the left column doesn't really note that change. It just notes, oh, there's a logo at disorientation. Oh, now there's a logo at disorientation. 'cause it doesn't, it's not looking at behaviors. It can't, the left object. It can't know. it can see that the, it can infer the logo at different orientations, but it doesn't see the motion at all. So it can't tell the upper the other guy, it's moving. You can just say, Hey, by the way, there's a logo at 30 degrees. Oh, now there's a logo at 60 degrees. whatever. There's no, so the actual detection of the motion seems to have to be done on the right side there, for more sort of raw sensory data.

anyway, this is a, yeah, and go ahead. maybe the smile could at least gate the attention of the sensorimotor to, to restrict it to that area. That movement of that area. But, if the column on the right is looking, getting any kind of sensory input, it would detect a change all on its own. It would just say, Hey, I, I just, I'm detecting changes. Yeah. Remember that's what that guy's doing. It's not really yeah. Yeah. I think it, it is true. This one should, can do this already without this one. I thought if we could use info from here that could make it more robust and more based on the actual model and shape of the object. but it's, Yeah, yeah. In some sense, we at least want some of that, or it feels intuitive that some of that change is communicated up and would be useful. Both, like you say, gives more information here, but also if we want movement to work in abstract spaces that cannot come from the sensory, direct sensory input. maybe presumably that is gonna come from like a representation in a low level column changing in some way. But that could be from another behavioral model.

I, my recommendation is, I think the goal today is to, get started implementing this, I think, right? Which is really, we need to do that. So I would put this into the category of, oh, what the hell? We don't really understand this too much. All let's get going. And, if we're lucky, it'll become obvious how this works or it doesn't work, or we don't need to worry about it or take care of itself or something like that along the way.

I can see this, particular issue. We could spend probably, a couple weeks of research meetings talking about this issue. Yeah. Seems so complicated. yeah, I agree. We don't need it for a first implementation. but it's still like an kind of some, an open question in my head if we want to allow for communicating object location at some point. 'cause it seems useful in several applications. but yeah, just something to keep in back of our minds when we talk again, maybe when we talk about voting again.

but yeah. I, it's like when we did, when we did the compositional object research and, we had a set of very specific problems. A lot of 'em were laid on coffee cups and mugs. mugs and coffee mugs and logos.

and that probably didn't capture all the world of compositional structure, but it captured a good portion of it. And so we said, let's go for it. we're gonna do that and we're gonna implement that. And it's, we're probably gonna find someplace where it doesn't work, in the future. I think the same thing's gonna happen here. We're gonna have to pick a set of problems we wanna do, for behavior. And we're gonna say, this is a pretty good set and we can write a paper on this. It's gonna work and it's gonna really, and we'll, and we just decided to get going forward on it. So I'm just arguing that again, saying that, we, this particular problem is good to think about, but not spend too much time group thinking about it. Yeah. Yeah. I think after this meeting, I'm, just gonna go through the questions we discussed today again, and, sort out the list of things that we move to discussing later. So we have a record of them. But for now, I have a pretty concrete idea of any open questions I had that would be necessary to answer before starting to implement it. So I think, yeah, to me the, alright, so you're saying some questions we have to answer now, some we can answer later. Yeah, exactly. And I think all of the ones we need to answer now, I at least would have a initial idea of how I would implement it, and then we might run into issues and Right. Talk about them more. But yeah, it's exciting. It's exciting starting to build this, I'm so excited I wanna do this because I'm, yeah. And the reason I'm structured, structured, overview. Yeah. Thanks. Yeah. Yeah. The reason I'm pushing that is because I feel like once we start implementing it, we realize a lot of things we're not thinking about when we are just conceptualizing it. So I feel like it's good to do like just some early prototypes of it to Think about these issues. Whereas composition objects is already quite the mature idea. We have still, we're noticing issues, but, I'm not saying the whole team is gonna start building object behaviors. Right now we're still having a lot of work to do for composition objects, but I would at least to start exploring some initial prototype of object behaviors to see. So I see an analogy to what, Tristan's been doing with discussing, our platform and breaking it. so here we are. We're gonna, we have the same issue here. Like we, we want to incorporate all these changes, do behavioral learning and inference. and we want to reduce the chance. We're gonna break it later when we have to introduce some new features. but there's no clear answer to how to do that. So I just, it's fun to see analogies.

should I give a short summary now before we switch to Niels or, yeah, that'll be. Okay. Okay. I hope I'm centered and not Yeah, done. Move your head. Perfect. Yeah. Okay. All right. So we talked a lot about how we could concretely implement, the current proposal for modeling object behaviors in Monty. And specifically we want went through four capabilities that we want to add to Monty. It's learning object behaviors, recognizing object behaviors, independent of morphology in different locations, rotations and states learning associations between behavior and morphology models, and compensating for object movement to make predictions in the morphology model. And, talked, we didn't really, that was number four, right? Number four. We didn't go into detail, so we wasn't number we went into Yeah, I thought you said that was number four, right? Yeah. So we, I thought you said we weren't gonna do that now. No, we're just gonna, you're not allowed to interrupt though, Jeff. I'm sorry. I thought you said this is the summary of what we're gonna do, and I thought we weren't gonna do number four now. No, This, was supposed to be the one minute summary, but, it's, oh, I'm, oh, I see. I'm not supposed to interrupt. Oh, I'm so sorry. Oh, you're good. You're good. So number four. Yeah. The proposal is that we start with number one and number two for an initial implementation. And we went through the concrete steps of what would need to be done to implement learning and recognition of object behaviors and any open questions around that. And then we talked about how capability three of associating them should naturally, be available to Monty once we have, modeling compositional objects implemented. number four is another big topic, but we didn't go into that this week and we're not planning to implement that in the first prototype, of object behaviors. and yeah, we went through a lot of open questions. some of them we decided that we will figure them out later on because they're not relevant for our first prototype. but all the relevant ones we have some concrete answers to now. And so I think, we have all the information to start on a first prototype of object behaviors in Monty.

Cool. Nice. I will, I screwed up. It's alright. I think we'll edit. Edit. I think there was a good cutting point. Yeah. yeah. And then I feel like conceptually one thing that kind of came outta the discussion, that maybe is useful to paraphrase again, is that, and also seems obvious in hindsight, but just this, that. morphology models and behavior models. Again, everything's doing the same thing, so behavior models are just getting incoming changes. Morphology, morphology models are getting incoming static features, but they can both have concepts of state, and modeling transitions between states. And then the time kind of sits on top of that and can help, inform as like a global signal. But, yeah, which I think we had said before, but it was just slightly clearer I felt today in terms of, yeah, that again, all the columns are kinda doing the same thing. Behavior isn't really unique. Behavioral columns, we almost need to come up with a different name for them.

dynamic input. I think something, maybe this we just let's call learning modules and they're just the two types of input you put into them and maybe there'll be third type, Yeah. Who knows. the beauty of the whole column architecture and so on is that in theory is one learning module that seems to do everything.

and it's always been like they, you give 'em different inputs, they learn different things. So we've always thought about different modalities, but here's a different mo there's another type of modality, which is movement. So really it's not a change at all. that's Vivian's argument for this. Yeah, and that simplifies the implementation as well, because we barely have to make any changes to the learning module. We don't have to have specialized learning modules for behaviors. and yeah, one quote I wrote down, I think from was, the neurons don't represent, they represent sequences without time. and then they can move through those sequences at different speeds. that's what Neil said, that we just have a sequence of states and we can move through them at different speeds or move through them by applying actions. and yeah, that it's really just these different, states of objects.

Did everyone else take those things away or did you have any other takeaways, or were you unclear on those?

Okay. Yeah. Sorry, Neil. This, took a while. No, that's okay. I'm almost thinking I could also just, record myself saying what I was gonna say because it's not a big thing. It was just, there's been a little, there was like a little thing that bothered me about the behavioral connectivity and it's just an observation about a potentially interesting anatomy thing. we've agreed that our meetings can go longer than two hours. Is there some reason we don't wanna just do it now? Yeah. Cool. We can take, if anyone need to take a small break for any reason, I'm, happy. Yeah. And I'll, I think we can do it relatively quick 'cause as long as we constrain the discussion and, yeah. And if anyone has to head out there, so we still the recording too, right? I'm, gonna listen while I'm filling my coffee cup, but you get started. Don't wait for me. I'm just, I'm listening. Okay. Nice. yeah. Do you guys see my slides? Yeah. Yeah. Cool. yeah, so what this is about is what felt like an asymmetry. And by that I mean a, not all columns are doing the same thing, that potentially arose when we were talking about behavior and in particular kind of how behavior, how you can apply, a beha emotion from a learned behavior to compensate in a model.

so this is from that nice idea that Viviane, you and Jeff had come up with that, and I don't want us to get into detail about this today 'cause as we said, this is this is the can of worms thing basically, or part of the can of worms, which is, how do you, correctly predict what you're gonna see in this example on the left with the, logos moving up and down. But then you've learned this behavior of motion up and down. And so there was this nice idea that we could apply the motion. To move us through this reference frame, as a sort of kind of compensatory movement. And that would enable us to predict what low level feature to predict. I still think that's a really nice idea, and it was just this thought that, this connection, in terms of sending movement to the lower level column in this hierarchical correlation, I'm not seeing your cursor. which, which commu which connection? So the per the purple line. The purple connection. Yeah. Okay. specifically I think it's the purple dash line lot. There's a lot of, I think it, it's the purple dash line that's, there's so many things moving on this screen. I can't find your cursor. Do you see the, like the flicking arrow here in the middle? Yes. Okay. So that one.

but it wasn't so much the fact that we didn't have an anatomical connection for it. That bothered me. 'cause like we often say there's lots of connections and you can always find something. It was more this feeling that, that seems specific to movement. It wasn't clear like what a morphology mo if, the higher level column was a morph, a morphology model, what would it do with that connection? And then it's okay, now suddenly behavioral columns and morphology columns are sending different types of connections. And then that kind of potentially starts moving away from this idea of all columns are the same, mountain castle, et cetera. So anyways, just a simple observation is, Yeah, sorry, this is just summarizing what I was just saying now.

and this is just showing our usual connectivity. So we have a behavioral column, morphology column, and then, of course we have this, the feedback has these or these kinda local synapses and L six, and that's the predicting, the specific location. and so this simple observation is just, which I'll show anatomy evidence for in a moment, is just that often these synapses also actually go into L five, which often we've, I think you've mentioned Jeff, maybe we have cells in L five that are the equivalent of the double bouquet cells, but for movement vectors.

and so if, if they were to sign UPS on those, then it feels like, that could be a way for a kind of hierarchical connection to provide essentially movement through the space of this reference frame.

and then importantly that this is the same projection, at least I think that's what the evidence is in the sense that, this isn't a totally new connection. Basically these, this projection going up through the column. Could choose to form synapses in on these double K cells or in the reference frame, depending on what the parent column is trying to predict in the low level one. But that would basically mean that all columns, when they send hierarchical feedback connections go through the column and then, up to L one. The difference is just depending on the kind of things that are being learned, they would pri primarily form synapses in L five to predict particular, movements. Or they would form synapses in L six to predict particular locations. but we don't, but yeah, but they all have this same core hardware or connectivity. And this is just showing some diagrams from Rocklands, papers where a lot of them actually show these, local, connectivity. So this, sorry, I should give more context. This long connection here. This is the feedback axon projecting up through a column. And so you see at the top this widespread L one connectivity and you basically see the same thing here, but then lower down in L five, you see these more local, projections and and indeed, yeah, when he writes about it, or, yeah, I think it's a man I don't actually know. Then, they, yeah, they comment on the fact that, yeah, it's layers one, but layer five and six, so not just six.

and so this is just summarizing that so that they can form an L five or six as necessary and, maybe it would fit with the double K cells. But, we can think of probably half a dozen other things that this L five, connection could be doing. It could be involved in goal states, it could be involved in, action policy, all kinds of stuff. So I'm not saying, oh, this is clearly what it's doing, but, at least that this was something that had been bothering me about this, how this would work, in anatomy. And I felt like this maybe helps a little with that, that, that's it. So are you saying if, the higher level column is a behavior model, it'll synapse or, the ideas that it would synapse in layer five, but if it's a morphology model and a hierarchy, it would synapse in, layer six. Yeah. and that, that would somehow be driven in a learning dependent way. So it's not predefined, it's more just that if, I dunno. yeah, if there's a lot of movement going on and this one's picking up movement, then somehow it can be more predictive by forming that synapse in L five.

something like that. Yeah.

Yeah, I have a couple thoughts on that. it'd be, thought, do you want me to slides again? yeah. Let's leave that one up there.

this one? Yeah. So I was lost, right? So I, I remember we talked about this stuff, but I got locked, but you don't need to explain it now. All right. Because it's oh yeah, this is so complicated. I can't get my head around it. Definitely think moving around oh God. I think this is a, like a complicated picture of the idea that you also proposed about having the behavior model send a movement down to the mythology model. But I can't remember Niels, you're talking about what you didn't like about it. And, but I have a general answer to the question, just a biological answer.

my interpretation of biology as follows, it doesn't mean it's correct, but that's how I've always thought about it. The genes, your genetic specify a lot about the overall structure of the brain. They will specify what cells exist in different layers and what, and they'll specify where their destinations are to some extent, but they don't specify any particular synapsis. That's, it's almost never the, in the cortex, it's in other animals. Like you take a sea snail, yes, they specify everything, every synapse is the same, but in a cortex, the i this, first of all, there's not enough genetic material. There's no evidence that's happening. So all synapses are essentially associatively learned. You can almost say that all synapses are heavy learned, heavy association. So, your proposal that is this axon rises up through all these layers. My interpretation was that axon will form synapses with anything it feels, it can, that it can associate with. And, it doesn't know, it doesn't know where it is. And as it's rising up there, it's just a little thing and it's, and if it see other axons or or dendrites nearby that can be learned to form, a pairing of, synaptic connections, they'll do it. this is for excitatory cells. Some inhibitory cells are a little bit different. so your proposal that, oh, it could form of layer five or layer six, depending, it seems totally reasonable to me. it's whatever. And no, the reason it doesn't form synapsis in layer four is because whatever's going on in layer four doesn't associate very well with what's going on in that Excel. So otherwise it would, Yeah, yeah, It's not like it's predetermined, yeah, So the just very opportunistic. Nobody's telling it where to go, and it is told where to go. It has genetics. Say, yes, go up here and spread out. But, but even how much it spreads is determined on associate connections, they're shown over and over again that axons and dendrites will continually grow in different directions. And if they form synapses, great, then they continue growing. But if they don't, they re tracked and go in another direction, Yeah. They're just looking for things to connect to that are associated with paired. So I think if this not following the exact problem you were dealing with in the first diagram, if this solution solves it to your satisfaction, I'm totally good. Whatever the, the, solution seems like a reasonable, answer. Cool. I don't what that, yeah. And to maybe paraphrase the problem again, it was just in terms of understanding this biologically, we hadn't discussed exactly what connection is. Is, is providing this movement information from the higher level column, and it had been thrown out. Maybe it's a totally other, it's a totally different connection. Like it's one we, I don't know, L five sends a totally different kind of top-down connection that, we haven't ever discussed before. and the reason that made me uncomfortable wasn't because we were adding a connection just in general because Yeah, there's lots of connections in the brain that we don't explicitly talk about. It was more the feeling that it was asymmetric got it. In the sense that it was specific to the relationship between a behavioral and a morphology column. So that was kinda breaking. Yeah. then it's okay, morphology columns don't send that to others. Why not? And, yeah. So that's an interesting question for Monty. what does Monty think about this? what do we do in Monty? We, do we handle it somehow similar to biology and say this is a signal and we figure out which one it connects to? Or do we, I don't know. I don't have idea what you do. Yeah, no, it's an interesting point. Yeah. I, think Abhi Go ahead. No, you do it. You gotta, you f you guys have to figure this out.

yeah. in terms of Marty, I think, if we say the brain can connect to the different layers depending on where it's most useful and predictive, then I think it's fair for Monty to just interpret them also how it's most useful to interpret. I guess the main. We don we don't do synaptic, we don't do synaptic learning in Monty.

what's the Yeah, we would, but yeah, we would have to explicitly say okay, these things can form associations and these things, whereas like right now we only do. But would you, how would you know whether to learn it or not? You might, Monty might. Yeah. Yeah. We would also need like heavy and, I don't think you have the heavy and equivalent right now. We say these things should be paired not right now. Yeah. Yeah. So I guess what I was gonna say on the general proposal, like the reason why I never mapped it onto the feedback connections is because I was thinking of those as general movement factors that are not specific to any object. And so it wouldn't really make sense for them to be the same as the context signal that goes to layer one. And it wouldn't even make sense for it to be anything object specific. And also they would need to be, they would need to have reference frame transformations likely, applied to them. So yeah, the reference frame might be an issue. But those double bouquet cells, I thought the idea was those are also non-specific to a particular object. Yeah. So I get the general movement vector. Yeah. So I get synapsing to those, but then the, it's the same axon, it also goes up to layer one, which is supposed to carry information about the object id.

yeah, I just don't think that layer one would benefit from it from information about a general movement vector.

I guess 'cause that's a biasing, that's a biasing signal. So that's okay. It's the stapler behavior. So it's like you need to do this movement to compensate for, that, what you'll see. And you're also potentially gonna be seeing a stapler, but you're not gonna hallucinate a stapler. But I'm gonna bias that. But it's not saying anything about staplers. They're just saying, apply movement in this direction. It's not, it doesn't have anything. I'm saying the L one connectivity could be biasing the ID of the stapler, but it wouldn't be like, it's just one movement vector stored in the stapler model. It's not the whole hinge behavior that we're sending down there. It's just one movement vector, which is just movement in one direction, which can change if the orientation of the stapler changes. And there are only so many directions and all of the directions can be on all of the objects if the objects are in different orientations. So I, I don't see how it would be, how we could use, learn any useful associations from that. Alright. Isn't this all under, category four from your earlier presentation? Is that right? Yes. And we said we're not gonna do that. We don't have to proceed further. Yeah, I, actually have a, an alternative, or an additional slash alternative proposal to this mechanism, but, that's definitely too much for today. But yeah. Alright. So anyway, I Good point you bring up Niels and maybe Yeah, definitely agree with your points, Viviane that Yeah. Especially the orientation thing because Yeah. Yeah. I like the reminder though that they also go to layer five and that they can just form wherever it's most useful. So it might still be something useful to keep in mind, as you said, for other things like goals and stuff like that.

Cool. Yeah, that's me done. All right. All right. So more we can slide into weeks of talking about number four if we want to, and voting, and don't forget that. Yeah. so are we about done here? Yeah, I think so. Mi can I just throw out the idea that I'm working on and, but I've done, I don't have any images or anything like that. Yeah, that'd be great to also prepare for next week's meeting. There's, there seems to be, there's a problem of learning that, as I said, we, we require all these learning models to learn the same models, but they don't really all experience everything. And then that was true in the morphology models and it's doubly true now in the behavioral models.

So the, I, and there's only a couple ways this could get solved. Like how do these different learning modules all learn the same thing, even though they're not exposed to it. And one way is, to use hierarchy. That is all the learning modules really don't learn all things, but once you get higher up in the hierarchy, there's the modality independent and you can learn up there and then you feed back down. And the other, possibility is that there is some sort of local sharing between learning modules that are learning. All right? So anyway, the basic idea I'm working on right now, and I have no idea if this is gonna pan out, is the idea that all learning modules or some large set of them, even if they're not receiving anything. So I'm a learning module. I don't, I'm not getting input from anything that I might be getting. I might be fed the movement vectors of nearby learning modules that are actually sensing something. So I could, move through a reference frame space. I'm a, learning model that doesn't get any input. I'm just sitting there dumb, nothing's happening to me, but I'm now I'm being fed movement vectors. And I said, oh, yeah, okay, I'll just as if I'm modeling something, I'll move through some space that, and I'll be essentially modeling through an equivalent space to the learning module that is learning something. and then I would somehow have to be told also, what is that other learning module? Learning? I wanna learn the same thing. So I'm trying to figure out mechanisms by which. Two or many learning models are near each other. One that's getting input and the other ones aren't getting input. How could they share, and train them nearby guys? And that may be, and that may be sufficient, but it certainly would help a lot. and it may be required that we have this hierarchical method as well. But anyway, I'm working on the idea. Is it possible that on input null learning modules are actually learning all the time? they're just getting input from someone else and see if Kai can make that work. Are you thinking of this as mostly for touch or or are you thinking with vision that like columns that receive non foveal input might get, higher information? I guess remember in vision most columns, if columns, all the inputs are centered, surround, right?

if I am a column that's not getting sent, if I'm looking at a blank space, it could be blue or green, doesn't really matter. I'm not gonna get any input. the center, there's no center around activity. So I could be looking at the object, but I wouldn't be tracking my location on the object. I would just be saying, eh, I don't see any input because there's no center surround activity. Good point. Yeah, I forgot about that. I was thinking, oh, the rationale. All of them get input, but yeah, they're not, they're getting, they're receiving accents, but most of those accents aren't active.

Anyway, so that's the idea I'm working on. Yeah. Sounds interesting. Yeah. okay. That's it.