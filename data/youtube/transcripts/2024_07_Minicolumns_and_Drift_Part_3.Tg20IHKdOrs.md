I was just exploring the idea that you have these two cell pop the same, you have a, here you have, you have these two ways of doing this. One is you have the same set of cells have two phases, or you have the same set of minicolumns where you then in one level, one layer, it stands and the next layer is sparse. Those are the, are we on some level, assuming that the cylinder, so I, see that the cylinder has, overlap with can and have overlap with, it's no overlap. This is, the same, but these two are different. Yeah. But in terms of the actual populations, does the cylinder has overlap with can, in both these examples. We wanna drop, in, in this, example where you're doing it on different phases, these are the exact same cells.

In this example, based on the tank paper, they're the exact same minicolumns or grid cells. Imagine grid cell has both. Imagine you have the classic grid cell and below there's a bunch of other cells, whether you call 'em mini or not. they all, the path integration works on the set of 'em.

but they're actually physically different cells.

yeah, they're physically different cells.

Does that make sense?

in, in one case, you're gonna look at it in one part of the phase, and, the other part of the phase, you'll have a completely different representation. Here you look at one set of cells. Now you look at the other cell cells underneath it. It's a different representation. Although there are commonalities between 'em, but you're not gonna see, I guess there, in some sense, they're overlapping populations, right? The sparse one is a subset of the, in this case, the sparse pattern is a subset of the non spar pattern. In this case, the sparse pattern is not a, it's not a non, it's not, it's a different set of cells. Exactly. Completely. Yeah. Because if we're assuming that we're dropping, cells from representation to be more specific, then they have to be subset. no. It's more it's more like you have a dent measurement. we're looking down on that, on the tank paper. You're looking down at these cells and they're, acting like grid cells, and then the question is under below them in the column, there's another set of cells. That are co-align with these, but they're, sparser. So like when you do path integration, what you need to have happen here is that path integration has to work on the set of these, like a little mini column. So the mini column is path integrated. but then we pick a subset of the, that's totally possible. By the way, the minicolumns could be path. So I don't see a problem with that. so you path erase the whole set, but then this is a difference. This is not the same cell as this one. But when this one's act, when this one is active, this one's always gonna be active. when this one's active, only some of these one or fewer of these are gonna be active. Okay.

something like that.

So these aren't exactly the same, these two scenarios. Because this is literally the same set of cells at different phases. This is, it is literally a different set of cells. But if you look, if you think about their co vertical co alignment, they're overlapping on that. Yeah. I'm just trying to figure what would the implication of these two ideas be? How, what would allow us to do classification? Would it allow us to do, I suppose if I, a lot of ideas run into my head.

What, are the problems? We wanna let's say we wanna be able to, we don't wanna have to learn relearn points on common objects doesn't solve that problem. So let's say I start by learning, I'm not like, why would I start in new object? What, why would I pick a cylinder versus something other shape? clearly here, if I do the prediction based on these guys, it's got any prediction based on the cylinder would be common between the can and the coffee cup, right? It is that, but, sometimes I'll wanna make a prediction about this, which is, unique, which is a handle. How would I know, how would I only know to do that then? like I, I'm moved my finger around. I have a unique, I have a unique representation down here and I have a less unique representation up here. Why wouldn't I learn every feature on the cylinder at this using this representation? Why would, I say, oh, no, only on the smooth parts of common parts if I gotta use this one and, on, and how do I know when to make a prediction based on this one?

what I'm saying about that. yeah, here's an idea. What if, what if we assumed it was this one, this method? We, you, go through, phases. So you're gonna have, at one moment in time, you're gonna have the cylinder, and then the next moment in time you have can, then you have the cylinder, and then you have the can. Every phase, if this prediction is correct, correct prediction, then, I just ignore this. I don't wanna learn anything here. It's like there's nothing, don't learn a unique thing for the can because it's correct for the cylinder. but if this is not correct, then you don't wanna, then you wanna learn. This one. You say, Hey, that's, not an Lego cylinder, that's something unique. I'll pull under representation. I'll use the representation for the, I guess it was coffee cup. I guess it should have been. You're right. I'm gonna use the representative for the cup. I, I found that there's a, feature there that says, oh, it has to be the cup.

As I go through learning, I go, okay, similar. I've learned a cylinder already. I don't know how that happened, but I learned a cylinder already. I'm going around and I say, okay, here's what the cylinder feels like, and now I'm gonna present a new object. And I come along that, and it's incorrect prediction, but I already felt I was on the cylinder, it must be a variation of the cylinder. so I'm gonna learn that on the cylinder. There's a unique representation for this. I've had that unique representation. Oh, here it's, I've had that unique representation all along. Yeah, cup in cup, but I just ignore it here and I pay attention. I ignore that here. But I paid attention to here. So now I'm going on predicting cylinder, but I do, but I'm going cylinder cup, making my predictions on the cylinder. But then I get to the cup, I say, yeah, in this case I have a prediction unique for the cup. Therefore, this was a cylinder with a handle on it. so we always learn on the generic one until the generic one does not become plausible, right? The general idea, I'm just, beginning is yeah, I'll stick with them. The denser one, that's, at least I'm trying to solve this problem. I don't wanna relearn all the common points on objects, right? So as soon as I know I'm on a cup versus a, this is a cylinder, but I could be a, if I knew I was on a can, I would make a different prediction. so my, point is, as soon as I lack onto something like, oh, it's a can versus a cup versus a, football, whatever. I have this alternate representation is unique.

that's only speaking, learning a morphological model or because the cylinder can be like different colors as well, or like, even if you're on the main part of the cup. You would still make different predictions, like depending which cup it is for color that's being sensed there, for example. you're, getting ahead of where I'm, just thinking this mechanism because I have a mechanism that represents a base object. Yeah. And, non base object. I'm not thinking even anything beyond that. Yeah. At this point in time. Yeah. Would it not work? Are you thinking of examples that wouldn't work? Or are you No, I'm just thinking it, it will be a bit more involved once we take into account features like color. I don't know. Basically I'm alternating between a generic SDR and a specific SDRA generic SDR and a specific SDR. And I could, with a specific SDRI can associate with anything color. it's just a, it's a, I can just associate whatever the input is at that time. with this specific SDR, So I don't think why it would be different with color or anything else. It's just which, kind of location am I representing it as? Am I using the generic one or the specific one?

Yeah. I just mean, then we would basically never use the generic one. Why we use it anytime. We don't have a specific prediction.

yeah.

to predict the morphology.

I'm not even stuck on morphology. I'm just saying I'm altering representation here.

All I'm saying is I have a generic something representation and, if. I've learned that and now I have something which is incorrect prediction, then I will switch to the specific version of it to make that prediction. For the specific version?

Yeah. The only, like, when I think of it through it practically, I'm just thinking like the prediction will almost always be off in some features for the cylinder. if we're on a can We will feel like the, we will sense like the redness of the can that might not be in the generic model and we will sense like this riffle structure of the can that you usually have the little wavy structure. So what's wrong? What's wrong with learning that? nothing's wrong with learning that. I'm just saying it will be rare that it actually matches the generic model. Exactly.

Does it have to, you say generic model might be purely morphology, it might just be very rough morphology. Okay. I like that actually. Because otherwise it wouldn't be very generic. Would Yeah, no.

it would be like, I really like this idea. And I think, it's nice because everything can be decomposed into these basic object shapes. if you go into drawing books and stuff, they compose the scene into Yeah. Cylinders and circles and stuff like that. I, yeah. But it, it just seems if you're oscillating like this, and once we take all of the different features into account, it feels like we almost never will make a. Accurate prediction with the generic model for what we're actually sensing. What if we said, what if this, what if we say this one is only morphology. Yeah. This is where you're going. Yeah. It's only morphology. So somehow we, are able to learn this without paying attention to features. Yeah, That's right. And then, that means they're not connected the same. That would mean Yeah. It's really more I was thinking, are you thinking of this being happening with like with the grid cells or in layer two, three right now? I don't know. I'm really confused at this point because Yeah, when we were talking about it being, in layer, like with the idea that, that made a lot of sense to me because that way we only have morphology in that generic model, and it just tells us like, how are we pathic integrate along that three dimensional shape. so let me, let's, review, let's take a look at layer four here. Therefore it is composed of minicolumns. And those minicolumns have the same, they represent the same orientation of a feature, right? That's it's all they represent. And those minicolumns are, I believe, are defined by these, bipolar cells, which are separate set of cells that, that these guys then connect vertically across all these. So what the spatial pooling occurs on these guys, it doesn't occur on these cells. It occurs here. This is like this, is a set of cells. Each one represents a mini column. These cells exist just like this. They have literally no tail of. And so this is where the spatial pooling occurs from. And these cells are, these are picking around with lots of synapses. And, they can learn too. So if I want to just purely learn morphology, I would be trying to learn with these cells. These are like, this is an orientation of this location. That's all it represents. it's like an edge at this location space. So these cells themselves know nothing about color or texture or any other detail. It's just this is like pure orientation. And then these are the, the details of what is at that orientation, at that location. And the, so, these cells would represent orientation plus feature, plus location. 'cause they're getting location from down here.

so if I wanted, if I, let's say I had, if I had. If I had, I'm trying to, where was my generic in my specific SDR here? I was trying to imagine, I'm not sure what, let's say it was in the le let's say it's the six A, and then let's say there's a layer of cells here, which is the, classic grid cells, and then there's a layer of cells underneath it, which is more sparse. So now, now we're going back to this idea here. There's two things on top of each other, not in time.

these grid cells could connect to these guys, where these guys would connect to these guys. Would that work? my grid cells are pretty, they don't, they're not very super unique.

and, but I could learn to associate, That is the way grid cells work and they can re-anchor. There's not that many combinations, 'cause they're not sparse enough. But there's a fair number of combinations, but not, there's not like a million or a billion or a trillion or, whatever. So you could say there's some number of ways that grid cells can re-anchor. It's a limited set. they may correspond to the number of ways I can, represent morphology. So as I go through these, locations here, I can say, oh, there's, an object, there's an edge or something at some orientation at that location. That's all I know. And so they would be, path integration with basic morphology, featureless shapes, something like that. Yeah. What and then these cells here are these hypothetical sparse cells that I made up.

they also, they path integrate in the sense that.

the same alignment would be, would actually put it, but there's farer and so this should be unique. This should be unique with representation, of location and that would be associated with these.

yeah, I like that idea. Would this also, I don't know, it might be a slightly tangential question, but I remember before we talked about how we can path integrate on three dimensional objects. Like how do I know if I go once around the cup and back at the original location? Yeah. would that kind of explain it as well? Like we would have, grid cells in layer six A that learn specifically path integration properties or like how you move on a cylindrical object and then use that to communicate to upper layer four. Orientation should be sensed next. I don't know. That's what it's, that sounds complicated. I'm, lost on now. I, partially don't understand the question. Oh.

but basically like grid cells are usually explained in two dimensional space, so as a navigation task, but the question was basically how do, how would they work on a three dimensional object, like where we can move once around the object and end up in the same location? Yeah. and if they're, I think in the past we talked about how we might be learning grid cells for different spaces, and if we are saying now that we might be encoding like general morphology or what do we call it, the abstract object, then that might be you. How? So as a general question. How do we represent three dimensional space versus two dimensional space? I think we've talked about this before. I'm very confused by this. It, there's a lot of evidence to suggest that reach those don't do three dimensional space very well. And remember I gave the example of, the cube with the faces on the six faces of the cube. You remember each face, but you don't remember where the faces are relative to each other. Yeah. Yeah. That's the only, I remember faces like this, but not, I can't really recreate a face like this. It's really hard to do. Yeah. That's what I'm also referring to, that, that we might not be representing the whole three dimensional space and then the object within that 3D space, but instead like an unrolled surface of the object. Like the, yeah. like when you have a cube and you unroll it, it's But, it's funny because it's, when the cube example, you don't really unroll it in the sense like, oh, I learned this and I learned this and I learned this and I learned this. It's like.

I can't, I like, like often if I showed you a D, like a, if I showed you a cube of two pictures on the sides here, I can't, I generally, I can't remember which picture is relative to which picture. You know what I'm saying? I can remember this picture, I can remember this picture, but I can't say unless there was like a story that's being untold. You, you know what I'm saying? I might see the cube like this, and I see there's a, there's a pattern here and there's a pattern. There may say there was different, not A and B, but there some picture A and picture BI don't actually remember this. View it, I don't, never can remember this view. I can't recreate this view. I can only create the space on Zoom, the, like that I, you can't draw this. I can't remember. I typically can't remember what's on the space versus this space.

So I always wondered is are you actually just remembering a series of two dimensional patches? They somehow stitched together. Yeah. But then you do know that if the dice gets turned around four times, you're back on the same. You know that.

But I, don't remember the sequence of what I see as I do it. Yeah. That's pretty rare.

OO obviously if it was just a series of four images and No. And ing on the other side, you might be able to do it. can we put that aside for a moment? Let's, yeah, sure. Let me just, let's, can we just try to finish this one for a moment? Maybe, you're onto something. I don't know. Yeah, I was just thinking this might be a solution that, it might be, but maybe I, you'll, have to help me understand your solution because confused by it.

where are they? where were they on this one? so you got these, are these bipolar cells, and I always forget which ones they are. They're not the chand of their cells, they're the, I forget.

and then theories that they drive these guys. So at this level, these cells are representing just orientation with, no, I'm just repeating what I said earlier. Earlier. This is just orientation with no features and this would be a dense representation. And, so I could learn, could re-anchor these based on the basic orientation of the room or the, or orientation of the, morphology of the object, excuse me. And then, we could always form a unique representation here.

And that would, you would associate that with these guys, which is also a unique representation here. I know how I, how do I pick and see unique rep? How do I pick one here? I guess pick a random set here and a random set here.

imagine, imagine, I'm gonna draw this, a little differently. Just make it look more parallel. You have this, idea is you have a set of grid cells. These are classic grid cells, and then you have a set of ones underneath it.

And maybe tell me a few, it doesn't have to be a lot. this is a unique, location because these sparser, whatever, because they're picking one of, one of many in each, under each grid zone.

I want path integration occur. path integration could occur for these. I don't think path integration occurs for these so easily.

if I move from location to location, I have a different set of sparse patterns here. I don't know how I would predict the correct sparse pattern given a given a lo one location where I have a sparse pattern here and then I move to the next location so I know which, where to move up here.

how do I know which sparse pattern down here to activate? I don't think I would know. I would just know it's, only if I learned these connections between them, could I do that? But I can't really do that.

so I, the thing I'm trying to do is I'm on an object and I move to a new location. Wanna be able to make a specific prediction on that new location, that means I have to path integrated and select the D cells correctly.

and could I have done that? Could that be done with the layer four connection? How if we if lay four projected back down here? Yeah, it could. Yeah, I suppose it could. So what would that tell me? I would say, it would give you information on which object we think we're on. Then I'd have to learn, it wouldn't be path integration, what it would be like saying, oh, if I know what the feature is, then I know where my location is. I wanna, be in a situation where I've got, I'm at some point in an object I move. I wanna be able to predict the next feature. So layer four is not active yet. so I can't rely on what's here. Like I, this will only represent the previous location, so I can't, this can't tell me what my new location is yet. It's like I wanna path integrate to the new location.

I thought that comes from the grid cells, the new location. I'm just thinking how do I path integrate the, the, sparse version here?

Are you thinking of this as like minicolumns underneath? I don't grid cells. I'm not certain I, it could be like minicolumns because then we could have like underneath the grid cells are doing the path integration and then underneath each grid cell would be like a mini column and then the inhibitory connections between other four. But how do I know when I go, when my mini column changes? Sparse activation, minicom changes. How do I know which sparse, which cell in the next minicolumns gonna become active? I can predict what next column to go to, but I can't, I don't know which cells be selected in that column I should go to.

could this be selected by the, inhibitory connects between L four and L six? could be. I, actually think I have an idea how to do this. Oh, wait, hang on a second. I didn't hear what you said. I'm sorry, but I think I have an idea to do this and Matt, forget that these are minicolumns.

remember that the tank paper, we had, basically six grid cell modules and there were like, there was a, little cluster of activity in each one of them.

And, so the idea here is that, these move around as a group. You re anchor, you pick a different set to move around as a group.

that's what it looked like. Then I imagined there was another cell underneath each of these was another, an active cell below it and only, and those that was more sparse.

imagine the whole, path integration. Okay, here, okay, here's, let me rephrase it. The way I've always thought about minicolumns in the past is you have a random cell in each one that's active, and the next moment you have a different random cell in each one that's active and there's no correlation to them. But what if in the case is this, I had a cell here and under this cell and I had another one, here under this cell and another one here under this cell. Path integration. But these were separate little layers. And so path integration means that this, just like these cells all move in path integration, the next layer down would all move in path integration. The next layer down would all move in path integration.

that, the active cell moves in the same layer. It doesn't, it can't change to a different one here. It's not like a random choice. It's like you have a, so I, have, let's say I have six active cells here, and underneath it, there's actually be more than six. There's a bunch around here. Let's say there's, let's just say six for a moment. let see if it works. And then underneath it, let say I have two in the next layer down and underneath that I have another two. And the next layer down, that's the one. So there's imagine the layers of cells are actually really thin little layers. And that, I'm making this up totally making this up. And so the two cells here move around as if just like these do, and the two cells below that move around just like these do. So I'm not picking from the whole set, I'm just picking these two, move around, these two, move around, these two move around. So I have a, then I can do path integration because each cell is path integrating correctly. It's not it's not randomly jumping up and down. I'm not picking a mini column and picking an active cell in it. It's like saying, it's that they have to stay. that there's a, path integration within its own little thin layer of cells. and not, I'm not just picking some random SDR out of a set of minicolumns, which is what we've been doing in the past. this is interesting. I dunno if you're following this, yeah, I'm not sure if I followed all stuff. Okay. Okay. Our usual way of doing it is say, here's a bunch of minicolumns. Pick one in each column randomly. That gives you a random SDR gives you a maximum random SDR. But there's, but when it comes to, then if I learn a sequence in the, temporal memory, I can learn a sequence between this random SDR, the next SDR that doesn't work for path integration. because I need to have a sparse representation that when I do path integration, I always get the correct next sparse. There's a, determined next sparse representation. I can't just pick one randomly. I has to be determined. I have to be able to say, given, a sparse reputation on the cup, and I move, I know exactly which sparse reputation I'm gonna get next. I, it has to be determined in the mechanism. It can't be just learned because I need to be able to move through the object space in the sparse representation of the object, without. I, have to predict what the, based on this, the motor input, I have to predict the next SER location. And I can't just pick random cells to do that. I have to have a, mechanism for picking the next bar representation. So that's the problem I'm trying to solve. And I'm saying like, even if I just had, even imagine that each little layer, I don't know if this is even possible, but each layer of cells, the, an individual cell that's in this more, in one of these little sparse layers will be path integrated. It'll move to the next, to one of these next columns.

it'll move to one of these next columns based on path integration. And it won't, I have to be able to predict where it's gonna be. So I need to go from a sparse pattern to a sparse pattern to a sparse pattern deterministically based on motor input. And so if I just imagine these are sliced little slices that are sparse, instead of six, just say there's two active in needs, a little layer. Then each layer path integrates each cell moves to the next column, just like these up here do. And and then it's deterministic.

I don't know if the numbers work out, but it would be deterministic. Let's say I had, let's say I had 15 layers of cells here. It's not atypical. 15 layers of cells. I have two active say. 'cause again, I have six bumps here. Let's say I, I say I have two active in each one.

and, I'll say two to, so that would be two to the 15th. space. It's not huge. that's not really big.

oh, that might be nice. It might be fine. So any point in time, I, I have to think through this. I might have to do this.

How does it help us to have these layers? we need to get to a, this the thing we saw on the tank paper is not sparse. Yeah. Yeah. It's, there's only so many combinations. what we wanna do is have it simultaneously to this representation. We wanna have a very sparse representation so that we can make specific predictions to therefore, and we say, given this location on this particular object, I need to do that. I need a very sparse representation. Yeah. That make sense? Yeah. The challenge is how do you have a sparse representation where the sparse representation path integrates that if I told you what the sparse representation is right now, and I told you, move this direction, it deterministically tells you what the next sparse representation is, and the grid cells would determine the next grid cells. These though aren't sparse. I can't, I Yeah, but they would create the sparse, whatever the path integration method would be. Okay. Whatever the path integration methods is, okay, we're supposed to move in this direction, right? Yeah. that would apply to each layer here, whether it's done through a minicom or if it's done some other way. I haven't testified yet. I'm just saying, how would I represent this? What would be a possible cellular representation that would work? So imagine now as these, as under each, of these, in the first layer cells down there is, only two of these guys are active. These two of the equivalent ones. like under this one. That's not a very good representation, right? I got two extra cells active, and, but let's say those two cells, all the cells in that layer try to path integrate, but only these two are active. I know this is, crazy. I'm just trying to work it through, see if it works at all. but if I had a whole bunch of layer of cells, let's say I had 15 cells deep, which is not that far, Then in each, layer is only two cells active. And each, and in each layer the same path integration occurs. Then I would solve my problem. I would have, I never, I dunno if I solve my problem. So no. How do you pick the two cells that are active in that Could be random to start with. And if it's random, how is it path integrating once I've, locked in onto those cells?

so let's say you picked two random and now you're moving in a certain, so that, now that means is that those two cells, the active cells, these would have to move. Only these cells move, right? There's, there are other cells here which aren't active underneath these guys, right? These guys aren't active. But when I do path integration, the active cells. S they move to, the, next cell over. It becomes active, the next cell over it becomes active. It's not the cell moves, it's the, within this little thing here, the active cell changes have removed. So it's selected by being co-located with the, I don't know how it, I don't know how it would work yet. I'm just trying to see even if the representation makes sense. Okay. We, why we have this problem from little bit started. Lemme just rephrase the problem. Every, when you have a, sparse SDR, you need to have a deterministic path integration mechanism that give given any sparse that it has to do the right thing every time, even if you've never been there before. That's what path integration requires. Our original paper, we assumed that there were lots and lots of, grid cell modules.

The, and each grid cell module anchored independently. The others and therefore vi two and anchored independently of the others. Then in each one did path integration. Then you'd always have the correct thing. Once you anchored, once you figure out which is the active cell in each of these, then the whole thing would move.

Make sense? Yeah. Okay. We don't see this, we don't see a lot. What we do see, we have to, we assume there's 20 of these things, maybe 20 grid cell modules. Each one has, what we do see is we see six grid cell modules. We don't see 20, we see six, which is not very much. And so I, if I were to say, oh, I, there, there's maybe, let's say there are 20 different locations in this bid cell module. 20 different cells that rep tile the space or something like that. Then you'd have, there'd be 20 to the six different ways of representing, location.

but we, it, doesn't look like that.

I'm sorry. I'm not able to communicate about this.

yeah, no, I, understand that part. it's even worse than that because in these, in the tank paper, you wouldn't find too active. There's six grids of the modules. The assumption that there's a sparse, there's a, that these, are not just randomly chosen in here. in fact, it is not an assumption. You showed it that you, can't randomly choose a location here, randomly choose a location. You randomly choose the location here. Some, these are all linked together. so if this cell is in the upper lefting corner, this active cell will be upper, then this cell would be an upper lefting corner. So you don't even get, you don't, All you really get is one. You have a very limited number of locations. You could represent this way. Yeah. 20, 30, whatever many cells there are in this, in each of these things here. or maybe it's more, I don't know, but whatever it is these are redundant. they're not building a, a, unique representation. You're just, they're just copies of the same thing. We're all moving together. So you really can't represent much uniqueness here.

so the question is how do you get to unique SDR, where path integration always provides you the next unique SDR. This method here, which we didn't invent, someone else came up with this idea that you have a lot of grid cell modules, then it would be in each one, anchored independently. Then you'd have a very high representational space and half integration would work. That given if you have a unique SDR all, you would get the correct unique SDR, the motor. Doesn't seem to exist. So I'm trying to come up with something else that might work in the same way.

yeah, now I'm thinking about it. Really what we're saying is I could in theory say, here's, this is like one, this is like one, grids. This, it is like one grid cell module that's redundant, right? Because they're not really representing different things. So it's just like six for redundancy. You could under argue like underneath it, there's another one and underneath it, there's another one, and underneath it, there's another one. So I if I had, 15 or 20 of these, it would be back to the thing we had in the original paper be like, and if you assume that this one anchored independently of this and this one anchored independent of that, that's what we have in original paper. But that's not what I was hearing from Tank and so on.

I don't know. It wasn't clear. I, have the impression maybe I got down the track since we saw some missing ones. I started imagining that this was sparser and I started mention this representation of sparse that the cells underneath here were sparser than these cells. But this is, close to I all, it requires that I'd have to, the cells here that this module, and this module ought to be anchored independently. Then it would be what we had in our original paper. but there's, not a lot of evidence for this.

yeah. And then the, layer idea that you just had up there, how would those get the extra information that's not in the six grid cell modules right now?

up here, I. like the, 15 layers underneath to have the unique SDR for each location. All. if the grid Sam module doesn't have enough, representational capacity, would, those 15 layers get the movement input additionally, or would they only base there two active neurons on the grid cell? Okay. I might have, I, I might have made a fundamental error here. So let me just back up. Do you see the equivalent of what we had in our first paper Yeah. And what we have here? Yeah. If we did, what if implements what we had in our first paper that these were independently, independently, anchored. Do you see why we'd have a sufficient representation? Yeah. Because we might have a choice of, let's say we have a choice of 20, 20 different locations here, and if we had, Say 15 of these things, you'd have 20 to the 15th different unique representations depending how he's anchor. Yeah. So that's a big enough number.

so that's the first question. Does that make sense? Yep. Okay. I got down a crazy tangent, which may or may not make sense. I got down this tangent thinking like, oh, these guys aren't just like this.

there was no evidence of that. There's a whole nother good cell module. Ethe, another good cell module under ethe, and another good cell module ethe.

that would be anchored differently because if, they, if this is right below this, and this is only we're talking really small, like maybe, one cell with 10 mils. Between layers, like one cell body then some of its imaging would show multiple act, you'd cast some cells are just below the, border and some cells just above the border, and you'd see two cells next to each other that looked like they were active at the same time, or like two cells in the same module. Like you'd see one active from this, guy may be active down here, this guy's active up here then, but you might capture and see both of it at the same time. He didn't see that, right? it wasn't like that. He never saw two in the same module. Active. Yeah. so then, but then we did see this idea that if you look down below this, at some point some of these cells disappeared, which might have suggested oh, he, was, actually going down into this layer here and and and he just under the border and he caught one of these cells that was, off. And he said, oh, he was measuring part from here and part from here. these things aren't, perfect layers or like the wiggly stuff, right? So he was, capturing this part here and then, and maybe these parts up here. So this one like sparse. So then I'm thinking like, oh, maybe this is just a sparse version of, this here. And I asked him about that and he said it might be, but he didn't know evidence happened. That's, I started going down that path thinking oh, it, it doesn't look like, it's like what we hoped in the beginning.

so maybe something different going.

then I was trying to make a, see, do the numbers work out. If I, I'm, this is all very speculative. would the nu would the numbers work out if, I assume that there was a dense, regular good solar array up here? Then, underneath it there were multiple cells that were, multiple cells only have only one of those was selected or there's few. And so these would all move as a group. It wouldn't be independently anchored, but they would be, sparse anyway.

so I'm assuming that the, grid cell mechanism, the, path integration will move all of 'em in the same direction, as opposed to be independent and separately anchored.

clearly the mathematically, the simplest one is assuming these all independent grid cell modules and they all anchored differently.

there wasn't much happening.

Do you know if any more paper pull up papers came out from that lab since then? they probably have been, when I talked to Tiger about this is very typical. I think he was, like curious why I'm interested in this stuff because it wasn't interesting to him.

That's my recollection.

not very strong, but it's like what, and you try to explain why this might be useful and, it, usually falls in deaf ears.

they're not, thinking about sort of unique SDRs for location representation. They're trying to show, to explain basic itself.

so he said, yeah, it could have been, there could be cells underneath there that, I'm only, he said, maybe it's in the paper. He says, I'm look only looking at the right thin layer. And and I said, how do you know which, where you are? He said, it's difficult to say. we just took a layer where it looks like good cells and then we map 'em out. And then I think I might have asked him like, what if he went further down? He says, I don't know. I'd probably see the same thing. I'm not thing like, okay. And then I said, why is this, why is that one not showing? He said, I don't know. And I said, could it be that you were on the label below and you, I don't know, could be, but could he have further papers? Yeah, he might, I don't know. You could look, maybe look to the, for his lab tank lab.

I'm, driving this right now. Purely from a, map representation point of view. What would be required for our models to work. Yeah.

Just lemme do the numbers here really quickly.

let's say, lemme second pick two. How many ways you pick two out of six? Let's say, 15. You have 15. 15 times 15.

That was like.

15, 15, 6, 25. Those 15 or 15, right?

15.

What is 15 or 15 pounds?

that would be approximately 4.3 7 8, 9 times 10 to the 17th power. That's what I would've said if I assumed, I, if I assumed there were 15 cells here spaced out.

like roughly about 10 millimeter, 10 microns apart. so you have a total layer thick of 150 microns, which is reasonable. That's about a sixth of the.

and then you assume there were two out of each of these, so there was sparsity maintained throughout the columns. Here. You might have two to the 17th representations you could form, something like that.

all this is going back to the, idea that, we have two representations, a very sparse one and a not sparse one.

let's leave it at that. Is it possible? Yeah, it's positive. The number numbers could work out if that was true. And then you'd associate this, the less sparse one with the orientation.

Unless, yeah, doing something that's okay. Oh, sorry, I'm, it's looking for more papers from the, okay. I'm just, here's an idea. What we've just proposed here is you have a layer of grid cells that look like real grid cells, and underneath them you have sparse versions of 'em, where you only have the two out of each module active or something like that.

Then we were talking about, it's parallel a little bit that we were talking up here that layers three was gonna be more dense and the layer two, which was gonna be less dense. So it seemed like there's a little bit of parallelism there, perhaps. I don't know, you are looking for general mechanisms that might apply elsewhere.

I saw that's the only point of that is, so this, represents the, generic shape, something like that. And then this is the unique object locations.

This next one. And then this would be, well, and this one, goes up to the, what do we have here? What did we say? We went to, it was like, it went to those bipolar cells.

This is the lower layer three. Remember there's a, separate this, let's just take it for, just imagine this for a moment. This would be specifying, the minicolumns and layer four.

so this would be like you'd, between these two cells, you'd be, somehow learning and inferring the basic shape of the object. And then between these set of cells and this set of cells, you would be determining, you would be learning the, this, bike holy connection is like this, connection here is basically learning morphology and this set of connections for you Here are specific, objects and features.

Yeah, I really like this idea. I'm glad you like it. 'cause it seems to feel like not good to me.

someone trying to contact me seems important.

That's a very nice solution to not relearning similar morphologies for similar, like not relearning cylinders for every cylindrical object essentially. Yeah. You just learn the specific features on that shape. So there's a limited, there's a limited memory here. You can only learn so many shapes. Yeah. But there are only so many shapes, basic shapes maybe, I don't know. But there's a limited number. It's gonna be pretty limited.

it might be, pretty, I'd be interested to walk through the numbers and see what would do, would it handle shapes that are, would it say all these with similar in, in. How much generalization would exhibit its on its own because it blurry bits, you know what I'm saying? Okay. you don't get a lot of, you don't get a lot of specificity here, so it'd be really blobby shapes, seems like it would be blobby shapes. I'd have to think about what the representation is. but yes, under, for each one though, we have a very unique pattern. So does this, how many, what, does it solve for us here?

I guess in some sense you don't wanna learn all punch on an object. So that's the idea that if I'm going around the coffee cup here, I'm doing this, I'm not really learning features. I might be trying to learn features, but basically I'll be learning just also this morphology. And so the morphology would, there's really not much uniqueness here until I hit something that's different. so in all cases, I could learn, I. the general morphology wants, and I would make predictions on the general morphology, any object that fits in that sort of, somehow stuff comes on. So you don't have to learn, you don't have to, you don't have to be on points on common objects. Yeah.

and you remember how we talked about like having a separate feature and morphology model? Yeah. And mix and matching them. Do we have an idea putting different features on the same morphology? Do we have any idea how to do that? I don't think so, but I think this could be an idea. Yeah. I guess that is that same thing, isn't it? Yeah. I forgot about that. So I'm too old. that's what this is a morphology model. Yeah, exactly.

it's a rough morphology model. We might still want to learn right. Detailed ones. But, there's all kinds of variations that could occur here, There could be, instead of all these cells protecting all these cells, it could be laminar projections, which case you might have a gradient of between generic and specific. I don't know. I'm trying to imagine stuff. But anyway, let's just say, but let's say there's a morphology model, which is basically based on grid cells and, featured columns, column orientation. So you basically, essentially the, the edge is at this normal, at this angle, at this location, another location. Is that another location? Is that, yeah, I'm have to, I have to tweak this to get that to work well. but then there's a sparse pattern here and a sparse pattern here.

this par sparse pattern has to do path integration. that, that is like I move, I have to move to the correct new sparse pattern. so that's a feat, that's a requirement of, the model. But then I'm always able to appear, unique location with any kind of unique feature.

when you say the sparse pattern needs to do path integration, do you mean it just needs to be consistent? Like, when I move from A to B once and then I move from A to B, again, it will be the same to SDR R. it also means if I've moved from A to B and then from B to A, I have to go back to the same one I started. Yeah. But, or if I move from A to B2C to D to A, I have to come back to A Yeah. But it, we could, like it doesn't have to be some predictable change in the SDR. Like it, we could just associate ran three random SDRs with those three locations and then I don't, How would it know then, it wouldn't be able to do that. It wouldn't be able to do path integration. Because when I, go to A to B2C and then I go back to a how would it know? It has to know to go back to a, I can't learn that. I have to just know it. I have to know that I'm back in the same location. Yeah, but couldn't the TZA modules do that part. but they, but I'm just saying that the sparse pattern has to exhibit the same property. Yeah. But could we just associate random SDRs, with the representation? Because, if I associated a random one with every location, then when I go back to a, I can't predict the random one. It can't be random. It has to be a, again, if the activation activity in the grid cell module, it's the same, wouldn't it then reactivate that originally this grid cell module up here is not sparse. Yeah. It's not very sparse. So this one, th those, cells, in, in the, go back to the tank paper. There were six. These guys, these are all work, go in a predictable form. like you go A to B, you always come back to here, right? Yeah. so what I'm saying is what if we just associate a random, SDR with each of these possible states of the grid cell module and whenever the grids are modules in the same state, again, to make, you can't, I guess you could do that. You could do that, but it can't be learned. It has to be, I, I don't, at some point it has to very quickly say, I know how to do this, even though I've never, one of the beauties of grid cells is that even if you've never been to a location, it forms the right representation. I, what's the right representation? maybe it would work the way you said, but you have to, that random, association. It would have to, it might work. I don't know. Maybe it would work. Oh, it perhaps, We can all agree though that it has to come back to the same unique pattern. Yeah. I think the main issue with it, if I understand it right, would be that I think that's one of the requirements that is maybe not written yet, is that we don't want to have unions of possible locations. But, so basically we wanna have locations unique to the object. So we would have different SDRs associated with the same state of the grid module, depending on which object we're on, if that makes sense. Say it again.

the grid module only has a certain amount of states that can be in, yeah. Yeah. But this should be unique to each object, right? So we might have the exact same state of the grid side module, but we want a different STR down here because we're on a different object.

yes. Oh. You're right. I forgot about that. How do we do a unique per object? Okay. Touching, you just went a big hole in the whole theory. Oh, I'm sorry. No, it's good. no, no. Some sense, each one of these layers of cells has to be anchor.

which two cells are active in those layers? Okay. I'm keep going. I'm getting confused, but keep going. Start again.

you were talking about, yeah, I wasn't even talking about the different layers right now. Yeah. What I was just proposing was what if we just associate the random SDR down here with each of the possible states that the grid zone module can be in?

and then in the beginning when we don't know which object we're on, we would have multiple random SDRs, active here For each of the possible objects. So that would be a union of possible locations. Dunno if we want that or not, but that would be the idea I was suggesting.

and then the path integration will be done by the good side module. So if we go from A to B2C, back to a, this would be back in the original state as the association to the specific SDR would reactivate that same SDR that we had active when we were at a, I'm just getting lost a little bit here.

We can, we, have to do two out of the six modules, two, two cells out of each six modules. We randomly choose them, which two cells are active. Yeah, I, wasn't, even talking about the two out of each layer, I was just saying disregard the layer. And you just picking a high dimensional sparse SDR down here. Okay. Okay. But, alright. But I'm not sure how it works if you just pick a high dimensional sparse SDR down there. He was trying to tell me how it would work. Yeah. Maybe I'm missing some constraint. I, was like, if I have a unique pattern here, I have to come back to that unique pattern after I've moved around and come back in space. Yeah. How would I do that if I just pick the random set here? So the first time it will be a random set, but it will be associated with the state that the return modules in. So then whenever the grid cell modules in that same stage, again, it reactivates the same SDR, but sometimes I want to activate a different SDR. If you're on a different object. So that's why where we're saying then. If we don't know yet which object we're on, we would activate a union of SDRs down here. But when would I, if I knew what object I'm on? Yeah. Okay. That's how do I know which one to pick here? That's a good question. Yeah. But, we have to solve that problem anyways. It earlier there's a suggest that layers there. Three projects down to six. So this is the object. Yeah. either that or the layer four connection.

Okay. I'm running outta steam here. it's a good ger of an idea, right?

I guess I'm, my brain's not working well. Yeah, we can take a break and, yeah. yeah, we should probably also get to coding a little bit. what time is it? It's already 4, 4 15. So I'm not gonna stay late.

I think this might be the end of the day for me.