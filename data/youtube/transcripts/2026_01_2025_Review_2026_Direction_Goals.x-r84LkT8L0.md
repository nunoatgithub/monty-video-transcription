Viviane Clay: Okay, 2025. So, yeah, I unfortunately didn't get as much time as I wanted to for this review, so I just threw together a couple of slides of the kind of high-level things that got done in 2025, because I think it's still... It's worth just kind of briefly recapping. There were a lot of big things that happened in the past year, and I feel like you often lose track and forget about them quickly. So, this presentation definitely doesn't cover everything we did in the last year. But... Hopefully, it's kind of a fun reminder of all the things we did. And... How much, can happen in one year. So, if you all remember, except, I guess, for Rami and Jeremy, who joined this year, or last year.

Jeff: On January 1st last year.

Viviane Clay: We started operating as a separate non-profit entity, and so a lot of organizational work went into that in the beginning of the year. Including setting up all the new accounts and subscriptions, paperwork, setting up our... Repositories, board of directors, press release, announcements, and everything around that.

We also received our 501c status throughout the year, so... had submitted the application, which was some paperwork, and then, yeah, heard back from this later in the year, I think October or November. So now, donations to the project are tax deductible, and we can get some discounts on our subscriptions.

We have a new website now. Since we split from Nuenta, we'll set up this nice website for us, and it's been gradually evolving over the past year. There were loads of communications with various people in various industries and universities. It's just a... collection slide, so some university.

collaborations, professors that we talked to or interacted with, students that we have projects with, like Loughborough University and Penn State. Places where we gave talks, like the Chen Institute, here's another student. visited conferences, like Alberta Reinforcement Land Conference and Coxai. Talked with people in the ultrasound space, like Butterfly, Sonastar, Bloom Standard, GH Labs, IPRD Solutions. For various aspects, like hackathon preparation, or... building some proof of concept with Monty on ultrasound. Priya Panda, you'll see, through the Caddy collaboration, And, yeah, basically turn... So many, kind of, interactions and closer collaborations that we now have a spreadsheet to track them all.

And then also presented the project at various venues, like podcasts, talks in... at university groups, the RL debate series. Tristan presented at several places, like Austin Robotics and AI Meetup, visited a conference, And, in another podcast that Jeff went on, Scott presented at the Chen Institute, And... And so, yeah, hoping to continue more of that in the next year. We released 162 videos on YouTube, so yeah, we'll put a lot of work into processing all of these videos and publishing them. We have now 1,600 subscribers, and... like, I guess 6,000 hours of watch time, and we got 56,000 views in 2025. And yeah, it looks like it's been gradually increasing, especially after the preprints were released in July, and after we started posting these YouTube shorts.

Yeah, here's an example of the YouTube Shorts. And then also the website views increased and became more global. All over the world, people are looking at our website, and you can see a bit of a kind of spike after... Either the paper release or starting to post towards, or both.

And similarly, on this course, we've been seeing increased Users, especially anonymous ones and a bit of crawlers as well. And then, yeah, all of you have been active on the Discourse forum, so yeah, thanks for all of you for posting these replies to people, and having all these interactions. And all this takes a good chunk of time as well. And then we've been starting to get more substantial external contributions. This is, just a random selection of things that I... quickly found on slides. Will gave a nice overview of it during the meetup on the map. basically contributions on all aspects of the project now, people writing blog posts, proposing features to Monty, testing improvements, like, speed-up possibilities. the video about 1,000 brains theory from Artem Kisarnoff. There are so many blogs now that we started a section in our documentation, collecting them all, meetings and community working group meetings that we're publishing on YouTube, people contributing to Future work items here on our roadmap. Several people who are doing really larger chunks of work. Helping us with... our goals. And then also, people's, starring our project and forking it, 270 people forked Monty by now, and we have 21 contributors to the code. And, yeah, just some people sharing on the Discourse forum, we're not sure if there are other people who are using it or not sharing, but... yeah, applying it to MNIST, Robot, Audiobot on the phone, or at least thinking about it, and some other projects.

Will added this nice interactive future work widget to our documentation, where people can now go and filter those items on our roadmap and see about contributing and read more about those items. And, yeah, added more data and text to those.

Everyone has been continually helping on improving our documentation. Here's just a random peek at how it looks now. Yeah, images, videos embedded, there are a bunch of tutorials. That are, yeah, kept... kept up to date.

Yeah, added some... some documentation around reference frame transformations with animations.

It's... Yeah, all of these pages have basically been updated in the past year, I would say, or almost all of them. Or at least restructured and cross-linked. Tristan and Jeremy added some nice style guides around code style and typing. To, yeah, kind of help. contribute code in the way that we like it. Nils added some language around large language model use. I have some more Items in our project showcase now.

the intactive. Future Work Roadmap. More descriptions on the future work items. And, yeah, a new page around theory for object behaviors with various figures.

So yeah, lots of documentation updates. Published, the heterarchy paper preprint and submitted it to a journal, and obviously a lot of work went into getting the paper ready to be published. Writing it all, getting all these figures made, and... yeah, reviewing, adding references, all that, and then also the DMC paper, which we've had separate presentations about all the work that went into that. I know one slide doesn't do it justice, but... yeah, published this as a preprint, and got accepted into a journal, so that's been a big... A huge achievement this year. And then also publish resources around the preprints. Like, explainer videos, blog posts, shorts, and so on. We had two retreats, one in California and one in Brighton. But, yeah, we had a lot of fun, and did a lot of stuff. We had two brainstorming weeks. That were virtual. With a lot of whiteboarding. And the Robot Hackathon in two locations. Where we built a lot of... cool Monty demos, and I've made this a separate achievement, because I thought it was really exciting. First time Monty moved its sensors in the real world, and actually learned and recognized By moving real motors and sensors. So... Yeah, that was... Really cool.

And then I explored Monty for ultrasound further, made several write-ups, had a lot of meetings with various people about this, and made this new dataset.

And published some tutorials on using Monty in custom applications, as well as documented the Monty for robotics projects as examples for people of how Monty could be used. In real-world applications, with some nice animations and figures and inclinations. On how to reproduce the projects we build.

lots of conceptual progress, literature reviews, and brainstorming. I mean, so many, it's hard to pick pictures for one slide, but there were just tons of write-ups, literature reviews, people making cool presentations. Us just thinking on Zoom calls and having intense discussions. And... yeah, I think we made some really significant progress on... how to model object behaviors. We disclosed the initial idea earlier this year, and some videos of how we got there, and then developed the idea further. concrete enough to have an idea of how we would want to implement this in Monty, and so we have a concrete outline of What to do to add this to Monty, this capability to Monty, to model object behaviors.

Rami added the ability for Monty to resample hypotheses quickly, so now Monty can quickly switch its hypothesis when we move from one object to another object. Before, Marty just couldn't do it, and afterwards, it could do it really well. So that was a success. And then Rami added some more sophisticated ways of adding and deleting hypotheses, so the hypothesis space isn't just grow a lot, but it's actually quite small for most of the time, when Monty is certain of where it is, and only samples once there's uncertainty introduced again, like when it moves onto a new object. And this is the exciting stuff that... Rami and Jeremy are integrating into Monty now.

Then made the compositional object dataset and metrics, so we have a way to evaluate Monty's ability to model compositional objects now. Prototyped saliency-based decade policy, so Scott worked a lot on this, detecting saliency, moving to salient points, and having some inhibition of return, so we don't just keep switching between the two most salient points. And did a lot of testing there, and also with Jose during the hackathon afterwards. And then Scott and Tristan worked on integrating a large part of this into Monty, so that was a huge integration project. Where, sensor modules can now emit gold states, and there's a gold state selector, and then those goal states are sent to the motor system to move to.

How do I prototyped the extracting of 2D features, so we can now detect, like, texture features instead of Lettering on a cup, just being, like. Right now, we just detect the shape of the curb, like the curvature and surface normal, but now it can also... we have a sensor module that can detect these kind of printed things on there.

We worked our way up to Monty version 0.17. Tristan worked a lot on disentangling the motor system and allowing the swapping of policies. Here are just a small collection of PRs that went into that. There are more. And Jeremy worked a lot on, the Mujoku integration, Python update, and habitat dependency removal, which are all intertwined issues that are quite complex and entangled. And so here's, again, just a small collection of PRs that went into that. And then, Tristan and Jeremy during the Focus Week in November. switched our configuration management over to Hydra, and then also in the weeks afterwards, finalized this process, and now we're using Hydra to manage our configurations, which was a big change. And many, many other improvements to Monty and co. So, 425 merged PRs, that's more than one a day, and 677 opened PRs, almost 2 a day, and that's just to tbp.monty. We also have 29 repositories overall, and a lot of those had a lot of activity over the past year, like the feature branches developing some of the new research ideas, our internal infrastructure, the plotting tool, video processing tooling, and so on. Just showing a few examples here.

Tpp Plot, was developed by Rami, so now we have a really nice library of visualization tools.

Tristan did a lot of platform planning, so yeah, you've all seen those nice breakdowns of all the items that need to go into building the platform. And we started a new prototyping process of where we build prototypes on a fork, and then integrate them into Monty once they are proven. Organized the community event. And yeah, this was the slide I... Made, I think, February last year? of, kind of... The direction we were looking into, where we were looking into figuring out object behaviors, we didn't know any... how to model those yet, we didn't have any ideas yet, or not the one we have now, at least. We were looking into removing the one episode per object assumption. So better unsupervised learning, which was a prerequisite for compositional modeling. So adding a testbed for that, and adding policies, and LM updates for this. And then on the... Implementation site here is various code refactors, like, data loader, data set refactor, and making the policy... motor policies more modular, as well as documentation and fostering the community. And... Yeah, looking at it now, we did a lot, so... We added a lot of documentation and published videos, fostered the community, refactored data loader and dataset. Other refactors, the policy refactors are ongoing. Rami's changes remove this one episode per object assumption. And it works really well. We have a compositional modeling testbed, and are working on all of these items here to do better on that testbed. We have a theory for object behaviors, and have various ideas around all these other topics, or at least spend Time talk. about them. So, light green is still in progress. But, yeah, I think then... Made some good progress on all those, as well as, all these other things I mentioned in the past slides. So, yeah, great job, everyone. It's been an exciting year.

Thanks for all the work you've been putting into this. Any... Any questions before I move on to 2026?

Jeff: Well, I think it's... it's pretty interesting just to see it again, you know, at least for my old brand, I figured out all this stuff. I was like, wow, we did all these things. And I've really adjusted our first year. So, it was pretty good.

Viviane Clay: Yeah, I agree. Always feels more than you remember, like, especially when I was looking at the numbers today, like, how many pull requests we made, just to Monty, and, like, all the posts on Discord, and all the interactions, and all the... research prototypes and ideas we tested and features we added to Monty, yeah.

Niels Leadholm: Yeah, and I feel like a lot of the things are kind of building momentum in that, like, from a technical perspective, a lot of it is about, you know, being familiar with the code, being familiar with how Monty works and stuff, and then all the organizational stuff you were talking about, Vivian, like, I guess a lot of that is kind of a one-off. Things, so, yeah, it's cool that... We've kind of made all that progress, and then... It feels like we could do even more, then, in the next year.

Jeff: really, this was... you know, I know Niels and Vivian worked on this beforehand, but... But... we did, but but really starting a year ago, you can think of it that way, starting a year ago. with a running start, but still there's a huge amount of stuff that was accomplished, and I guess, you know, starting this a year ago, you don't even know how it's going to turn out. You don't... any of the issues, you don't know the organization issues, the funding issues, or... anything, and there's so much to do, and so much uncertainty, And it all turned out pretty good so far. I'd say this is an amazing accomplishment for the first year of an organization like this. Pretty impressive. Now we can worry about next year.

Viviane Clay: Definitely makes me excited for the next year.

Jeff: I'm always, I'm always worried about the future. I'm always like, okay, yeah, all right, we did pretty well last year, but next year, you know?

Oh my gosh. I'm a worrier.

Viviane Clay: Yeah, I think I'm pretty helpful with, like, what Niels just said, it feels like... a lot of things we've already put a lot of work into and are gonna kind of pick the fruits this year, basically, so...

Jeff: I think that's right, right.

Viviane Clay: Pretty hopeful for the next year.

Jeff: I'm always worried about, oh, one of the big things we missed? What have we not thought about, you know? I think it's just my normal, Normal way of operations to always be, like. Always worried about the future, and that's a good way to go, because, you know, that keeps you on your toes, you never get complacent, you never sit there and go, oh yeah, we got this all figured out. It's like, oh no, what are we missing, you know? But so far, it looks great.

It looks great. We had no real setbacks this year. We could have, but we didn't.

Viviane Clay: Yeah, I mean, there are always a lot of unexpected things, a lot of issues you encountered that we didn't plan, especially with research. We don't know if things will pan out or not, but... or especially also with developing the theory, but it seemed... it felt like we... We made some great progress with object behaviors, for example, and some... much more clarity on some of these other topics. Right, right.

Jeff: Alright, the further you get along, the more of the pieces you fill in the place, the less likely you're gonna find some big hole. That you're like, oh, we never thought about X, you know, but, it seems less and less likely. And so it does feel like, oh, we're filling in the remaining pieces.

You know, I've asked... I expressed my optimism before. I said, oh yeah, we're gonna... we're gonna wrap up the big theory things this year. If not, then the next year, but I'm hoping to do it this year.

so, it feels like... it feels doable. But you're gonna tell us what we're gonna do next year.

Viviane Clay: Yeah, we gotta do just that. Wrap everything up

Jeff: Well, on the theory side, I think the other stuff's gonna take a long, long time.

Tristan Slominski: I just have one question. Jeff, you said no major setbacks. What would you have considered a setback?

Jeff: Well, you know, you don't know... it's just one of these things you worry... That, you know, maybe, like, some theoretical... deep theoretical mistake we made. You know, like, oh, it really doesn't work with reference frames, it works with something else, or... or, oh, we... I don't... I don't know, it's like... it's like, you don't know until it happens, but... but there's... we have a whole bunch of fundamental assumptions we've made about Monty and the Thousand Brains Theory. Ben... There are... there are... there are proposed theoretical ideas, and And, you know, in science, nothing's ever proven, and you can always, you know, discover later, like, oh, we made a fundamental error someplace. You know, whether you think about, like, you know, Newton didn't get gravity right, but, you know, Einstein got it better, but maybe Einstein's wrong, you know, that, you know, there's always things you could get wrong. And you don't know what they are at the time, so... I just always have to be on... you always have to be thinking about that. You don't want to be caught... like, getting so far down the path, and you realize you made a big mistake. So, you know, it's like Andy Grove used to say, it's the CEO of Intel, he used to say, you know, only the paranoid survive. And I... and I interpret that to mean, like, you've always got to be worried about, what am I making... what am I getting wrong? What are we... what are we doing wrong? And I think about that all the time, and yet I don't see anything, so... but I don't stop thinking about it. You just gotta... that's the way to be successful, is you're always thinking, like, okay, don't get complacent. What, you know, is there something I'm missing here? You know, what's the big idea? I don't know. I mean, I watched... I watched the, you know, all the stuff of deep learning, try to figure out, like, am I misunderstanding? Is it more than I think it is? Is there something more fundamental going on there that I don't understand? I haven't seen that yet, but I worry about that. You know, I still feel very confident that our approach is really the way to go for AI and machine intelligence, and But, you know, you worry about it.

Niels Leadholm: I feel like that's actually, yeah, another interesting one, where over the last year, I think there's been greater acceptance in the broad community that, you know. deep learning, at least in its current form, isn't going to kind of deliver the sudden kind of fast takeoff to AGI or whatever that... that I think before then, you know. There has been a lot of hype around that. So, in some ways, I guess that's another encouraging sign, is that, kind of, at the same time, as we've seen a lot of promise in what we're doing. More people have looked at, kind of, these... and also predictive coding from a neuroscience perspective. there's been more and more skepticism, so it's kind of like one of the leading alternative theories in neuroscience and one of the leading, kind of, or the leading alternative approach in AI have both had kind of a growing... chorus of, kind of, skeptics, which doesn't... definitely doesn't prove that what we're doing is correct, but it is, I guess, an indirect signal Suggesting that it's, that we could be on the right track.

Jeff: I, you know, I feel like... I don't... I accept... I think our work is... successful, not because, you know, as you point out, Neil, it's true that, you know, it's good that, you know, if we're right, these other approaches we'll see holds on, but mostly I feel like, oh, this has got to be right. This is the way it has to work, you know, you put all these pieces together, so many pieces fit together like this so nicely. that there really can't be any, you know, that's hard to imagine them being wrong. And, But you just never know 100. It's just... as much as you're almost certain of it, there's always skeptics. And I think one of the strengths of... and we talked about this recently, about, you know, entrepreneurs and stuff, one of the strengths is really having the conviction of your future vision. And being certain of it, not just wishful about it, but being certain about it. And you have to plow through, despite the fact that maybe 99 of the world doesn't believe it, and believes in something else. But it's, you know, it doesn't matter. If you're right, you're right. And so, I always say, you know, time is on our side. That's the way I look at it. Time is on our side. If we're right, time is on our side. We're just... the world will come around our view is thinking. So, then I'm always asking, well, are we right? You know, is anything we're getting wrong here? What's the big wrong thing we're getting? I can't see it yet, so... maybe it doesn't exist.

Viviane Clay: Yeah, yeah, like, the only real judge of things is time.

Jeff: We're right, because, you know, if we're right, the world will come around to our thinking, you know, you just hope it's not 50 years from now, you hope it's, like, one year from now.

So we have to promote our ideas, and we also have to let these other approaches, whether it's predictive coding or deep learning techniques, they run their course until people kind of... You know, there's a lot of people in the world who don't really have much of a clear vision about the future, and they just say, oh, this is the hot thing, and this is gonna have to work out, you know. But over and over again in the history of science and technology, you see, that isn't the right way of going about it. Things that people thought were hot just fall apart after a time. And you gotta... you gotta stick with your... your convictions. And it's not just wishful thinking, you know, we're not just, like, hoping this is right. I mean, it seems... everything seems like it has to be this way. Anyway, so I'm very, very confident. At the same time, you can be pess... not pessimistic, you can be, paranoid, or, like, asking, so I don't know what could go wrong? And as, you know, you just ask, I don't know yet, we haven't seen anything wrong. It doesn't seem like anything's wrong. Constantly worried about it.

Tristan Slominski: What makes me paranoid is that what you said about the current things have to run their course, because there's been an ungodly amount of treasure poured into the current things, which might really extend the time it takes for those things to run their course.

Jeff: Maybe, I think...

Tristan Slominski: paranoid about.

Jeff: Maybe, or I think my observation is a different sort of course would run out. What might happen is everybody sours on all this. And it's not that deep learning's gonna go away, but people might think, like, oh, you know, this is overhyped, and it's not, you know, it's gonna be... it could be a year from now that people are looking back at deep learning and thinking, like, oh, this is ridiculous, you know, they thought it was going to do all these things, it only does a few things. And then that could impact us, because then nobody's interested in machine intelligence at all. And they go, yeah, that happened to me with mobile computing, right? You know, there were some early failures, like the Apple Newton, and then everyone said, oh, this is a stupid idea. No one's going to want a handheld computer, that's stupid. And no one put money into it. Zero. It all dried up overnight. just because of some notable failures. So we could... that would be something I worry more about, like, there's no, you know, people just give up on this altogether, as opposed to it takes forever for... The nice thing about deep learning right now is just putting so much money into it, there's almost certainly going to have some big fallout of it, so I don't think it's going to.

Niels Leadholm: Right, yeah, it kind of feels accelerated, actually, because it's like.

Jeff: Gift of this.

Niels Leadholm: without all that money in it, they can just keep saying, oh, scaling is all you need. But it's like, if you actually draw the... the... like, finish the plot, almost, and like, kind of say, okay, let's put all human resources into this, and it's like, oh, the payout's still not that great, then it's like, okay.

Jeff: Yeah. Right, it's... yeah.

Niels Leadholm: Need to revisit.

Jeff: I...

Niels Leadholm: Is that a thumbnail.

Jeff: I remember the earlier wave of AI in the 80s and the early 90s, when they were doing the, What was it called? What's the technique they called then? The expert systems. And at the time, a lot of money was put into it, and people were reporting all the successes, you know, in the press, there's all these, oh, they're using this expression to do this and do that, and it's all wonderful, and it's really working. You could be... easily believe that it was going to be huge, but it all fell apart. Even though they said it was working, I mean, you know, and the current wave is a little bit like that. There's a lot of projects going on with deep learning that aren't really panning out. You know, everyone's excited about it, but there's a lot that aren't panning out, and some are, but a lot aren't. So who knows? We'll have to see. But I think we can be confident that our approach is right. There's a lot of things we can do that no one else can do. It's gonna be ultimately successful, we just have to keep going, and improve our stuff, and we have to let the other guys run their course and see what happens.

That's life.

But it's great. I feel... I feel like we're on the right... we're on the right team. So it's like, yeah, we're gonna win.

Viviane Clay: Yeah, it definitely feels pretty exciting.

Jeff: Yeah.

Viviane Clay: Yeah, with that, let's talk about next year.

So... This is a bit borrowed from... or a lot borrowed from the diagrams that Tristan made that I showed earlier, so don't want to take all the credit for this, but I thought it was a really nice way to visualize things. It's a bit simplified. And feel free to interrupt if you disagree with something or have a question.

But so our basic process right now that we've developed over the past year or years is we develop some theory. Then, the ideas that are developed in the theory turn into prototypes, so those are research projects that the researchers are developing on separate forks of Monty. Once a prototype is proven to work, so we actually show it does what we want it to do, turns into an integration project, and a researcher pairs with an engineer, and it is integrated as a Monty feature into the main TBP Monte repository.

the more features Monty has, the better demos people can build with Monty, so if Monty has New features, new kind of demos can be built with Monty. And after building demos, those could be turned into actual applications, actual use cases, people... deploying it in real-world applications. We're not there yet. There isn't any kind of... real-world deployment of Monty at the moment. We dabbled a bit in demos during the hackathon, for example. And then there's some... some other errors in between there, like, if there are more Monty features, you can build new prototypes. Prototypes kind of build on top of each other.

And then also, as we prototype ideas, that can also feed back into the theory and reveal some issues we didn't think about.

Jeff: or some...

Viviane Clay: Things that actually don't work how we thought they would work.

And then we also have the platform quality. which... helps on a bunch of things, if the platform is... and with that, I refer to tbb.monti. If the quality is better, then it's easier and faster to build prototype, it's easier and faster to add features to Monty, to do those integration projects, it's easier to build demos, and it's easier to build applications with Monty.

That makes sense so far?

Okay. Yeah. So, our researchers are working on this part of the picture, developing theory. building prototypes, and then integrating the prototypes together with the engineers. And we've been spending a small amount of time on building demos, like, during the hackathon. And some of the wrap-up work afterwards. And then the engineering team is not working on theory or prototypes, but working a lot on platform quality improvements, as well as the integration projects, and also part of building demos.

Generally, this is how we want it to be. We don't want to spend our internal resources on building applications, or even spending a lot of time building demos ourselves.

Just simplified the picture a bit, removed some of the back arrows so it doesn't get too complex. There is another part of this equation, which is the community. So, we have the community members, which can contribute to essentially anything. It's a bit hard for a community to contribute to theory, or at least it hasn't happened yet. It would require someone to do really deep dives and watch all our research meetings. Maybe people could test the theory and experiments, like experimentalists or something, but for now, I didn't include that in the community box. But they can build prototypes, add multi-features, build demos, build applications, improve platform quality.

And also promote the project, and... Help us grow the community. and... What really makes the community grow is seeing demos and applications. I mean, some people are intrigued by the theory and ideas, but what's really going to make Monty take off is once people start using it and seeing its benefits. So... Right now, we don't have any applications and few demos, so most people in the community think this is a cool idea, cool project, but... I think once people actually start using Monty in demos and seeing its benefits, then that'll... that'll actually, Kick off this community growth a lot more.

just like with LLMs, they were kind of an interesting thing in the research community, but then once people saw ChatGPT and could just play around with it and saw what it did, it really took... started taking off. It was, like, a simple, easy demo that led to a lot of growth. And once the community grows, that means that this whole green box has more and more resources, and can thereby contribute to a lot of these other things more and more, and it's kind of this positive cycle. So what we really want to do is kind of kick off the cycle of the community growing, and community contributing back to a lot of this project, because if they're actually using Monty, they want to improve it.

And so... How could this tie into our goals for the next year? So some things that we want to do to encourage this is, when making platform quality improvements, focus those improvements on improving the ease of building demos and applications. So, I mean, initially Monty was developed as a research codebase. So it was initially built for building prototypes and Monty features, which used to be kind of the same thing, because we didn't have this prototype feature workflow. So it was kind of built as a research code base, and now we really want to turn it into a platform That you can use to easily build Demos applications. I mean, we don't want prototyping to get worse or harder in the process, but we want to focus the improvements on ease of use. Ease of building demos.

And then, similarly, with prototypes and multi-features, really think about okay, is this a feature that will unlock new capabilities that's useful for building demos with Monty? So, kind of, focus on developing features that promise unlocking new applications. Those are kind of the big things that, once we can model compositional objects, a compositional world, that unlocks a lot of new a lot of new application areas. Once we can model object behaviors and things that move, that unlocks a lot of new application areas. Once we can actually use Monty's models to manipulate the world and interact with the world, that unlocks a huge range of new applications. So, just kind of... as we prototype and think about these things, think about, okay, what can we actually do with this? What does this unlock in the real world? And then... Lastly, this is a cool idea that Tristan had.

Which we termed Solutions Incubator now. It's the idea to encourage community members to build solutions with Monty, and I can talk a bit more about that. Right now, I guess. So, the very basic premise, and we have to figure out all the logistics around it, would be to offer, grants or stipends to people who come to us with an idea of, something that they want to build with Monty. So basically, it's not like interns, where they come and say, I want to do an internship in the summer, and then we have to come up with what they should do. But instead, they come to us with an idea. They pitch us the idea. And if we think this is a realistic thing that you could build with Monty, we provide them, with a grant, some money, as well as some guidance, like, Meetings at a certain frequency, and they spend a couple of months, actually building that demo and, and, yeah, presenting it to us, writing it up somewhere. Publicly. and... And that way, we have... Idea generation from the community, people coming up with ideas that we would never think of ourselves.

and kind of start to try to kickstart the cycle of having some actual demos built, people actually spending time on building useful things with Monty, and that hopefully leading to community growth. So that... so that's kind of the idea, that having more demos, more use cases of Monty leads to community growth, which leads to more community members, which has a positive effect on everything.

Does that make sense? I said a lot of words, but yeah, kind of the basic premise of trying to focus our both platform quality improvements, as well as prototypes and multi-features on Increasing the amount of demos that exist with Monty. And the community growth.

Ramy Mounir: Is this only going to be for demos, or is it also gonna be for prototypes? Like, are they building research prototypes as well, or...

Viviane Clay: I mean, community members can be building research prototypes, but that wouldn't be the solutions incubator's focus. So it would be more about actually building something where Monty is used in an application.

Any question... other questions?

Jeff: I have a... just an observation.

Viviane Clay: Definitely don't mind.

Jeff: I mean, the right hand of the diagram here is a complete unknown to us. We don't know what the first applications are gonna be. And that's typical. That's the way it goes with new technologies. So I think having, you know, and usually the... typically, it comes from someone outside, you know, would be from our community. I think what happened with ChatGPT is, as you pointed out, there was a thing that anyone could use, anybody. You know, you didn't have any technical expertise to play around with it. And I don't know if that's gonna happen for us, right? You know, I think one of our first board meetings, I had colleagues and says, well, what's your chat GPT moment? like, you know, what's going to be the similar thing for us? I'm not sure we'll have one. It's not... it's pretty unusual, because, you know, our technology requires interfacing with the world of sensors and stuff, and it's not like you can just sit down in a browser and type something in. But maybe there is. So, I think there's a big question mark on the right-hand side here. What are they going to be the big applications? What will be the kind of applications? What will be the kind of sensors? And we haven't been focusing on that, and so I think the idea of an incubator is a... it's not something you would need in the long term, but it's something you could really use in the short term. Just to get ideas and have people explore, because that's... that's where the... You know, we need that. You know, we can build the greatest platform in the world, but someone has to have the insight to say, like, oh, I see how to apply this to something. It's something that's exciting. And, that takes domain expertise that we may not have. So, in general, I like the idea, We need to, sort of. Gently seed things on the right-hand side here to see what catches. And maybe a lot of those incubators won't achieve anything, but it's still a good idea.

Because eventually something will catch. And we can all think about it too, right? We can all think about, like, what would be a really cool way of getting people excited about... and people using it in some fruitful way. but that's not our main goal. Our main goal is to build this platform first.

Viviane Clay: Yeah, yeah, and the nice thing about the Solutions Incubator is that it should require a minimal effort from our side to actually build the demo. I mean, it would require some amount of meetings and communications, but it would be someone external, basically, building this, and as you said, likely someone who has more, kind of, domain knowledge of the specific application, and so, yeah, that's a nice thing.

Jeff: Right, right. And, you know, we made two 20s, and one or two successful. I mean, that would be fine. who knows? I'm just saying it's not like... It's really kind of... it's really hard to predict what's going to happen over there on the right-hand side.

Viviane Clay: Yeah.

Okay, so what does this mean for the next year? So... What do we really want? We really want Monty to be used in sensory motor applications to solve real-world problems. I mean, there are so many challenges in the world, and unless we innovate, we're not going to solve those challenges. And I don't think we're going to solve them with deep learning. I mean, maybe some of them can be solved by just throwing a lot of data and compute at them. But many can't, and even the ones that can be, it's usually not the most optimal solution. If you... if it requires to burn a bunch of energy and throw a lot of money at it. And so, Monty's kind of our bet at the elegant solution to all of these problems. And I think it has immense promise and potential for making a positive impact on all of these applications and more.

But we likely won't get there this year, I don't think it's realistic to expect much to be used in a real-world application that requires, actually. Interacting with the world in this year. But what we can aim for this year is to pave the path for building sensory motor applications with Monty. And so what we can do concretely for that is figure out the open theoretical questions, especially around using models to manipulate the world. So that's the big, kind of, open thing of, like, how do we actually interact with the world now, instead of just moving the sensors to recognize stuff. So hopefully we might make a lot... make some progress on that this year. Maybe we figure it all out this year. We'll see. And add the capabilities... add capabilities to Monty that unlock more interesting applications. That's kind of the focus of the prototypes and integration projects, and the kind of focus of, like, picking which prototypes to do and which integration projects to do, always keeping in mind of, like, what applications will this unlock? How much more useful will it make, Monty? And then, making it easier to build applications using Monty, that's kind of the platform side, platform quality side. What can we do with platform quality to make it easier for people to build applications? And then lastly. actually get people interested in building applications with Monty. One part of that is the Solutions Incubator actually offering some amount of money to build something, but then also promoting the project, presenting at various menus, publishing about the project, interacting with people on discourse, and so on. So the kind of community-building aspect of it all. figuring out how do we get people interested in this approach in the first place, before we have a lot of cool demos, before we have our chat GPT moment. Or... or... different moment.

And so, going back to this diagram, kind of concrete goals for the next year that... I think... are doable, or... yeah, would hope to see by the end of next year when we do year reviews. Figure out theory around causal interaction with the world. So, learning causality, learning how to produce actions that cause effects, And manipulate the world? Then, in terms of prototypes, prototyping our theory that we developed last year around object behaviors, and testing if that all pans out. And in terms of Monty features. Being able to model compositional models, and we have several prototypes in the pipeline there to integrate and develop further, and then... several, more, kind of, theory ideas that we need to prototype and integrate. And one of the, kind of, sub-points here that we're... Planning as some kind of publication around modeling compositional models. in Monty, and the benefits of that. We're gonna scope that in the next month or two, and see if we want to do a paper, or just a simpler form of... Publication, but some way to kind of demonstrate the benefits of this. And then, in terms of platform quality, there's a lot to unpack there, and Tristan has some really nice detailed diagrams of all that, but I, very crudely summarized it now as, separate platform, experiment, and environment. basically the idea that if you think of Monty's class structure. The platform turns out to only be the Monty class, and the classes within it, so learning modules, sensor modules, motor system. And then separate from the platform is the environment, which could be a physical robot, or it could be a simulator like Habitat or Mojoko, but it's separate from the platform itself. And then the experiment, which kind of wraps around both and controls, for example, the simulator or the setup of the task, and how we measure performance and so on. But it's also not necessarily the platform itself, because you might be using Monty in a robot in a factory, and you don't need to run experiments with episodes and epochs and so on. So basically separating these three components out, so it's easier to have Monty as this platform that you can then plug into any kind of environment, any kind of experimental or application setup. And that involves a lot of sub-bullets, and a lot of... Complicated subtasks. And, yeah, if you're curious, there's Excalibra boards from Tristan, as well as from me, some more simplified versions of Of all these items, I can link later on the Slack channel if you want to take a closer look.

And then on the community growth side, we have continued documentation of the project. So when people come through the project, they find documentation on how to build stuff with Monty. Publications, interactions with the community, and collaborations. Damn. Like we did in the past year. And then, on demos, the solutions incubator, which, again, the large chunk of time will be on the external person who receives the grant, but will still have to vet people and start this whole program and have interactions with them throughout. And yeah, that's the kind of high-level view. Top-level goals for the next year, if that might... Makes sense. If some questions. That sounds... sounds reasonable to everyone.

Jeff: think it's a nice summary. It's a nice way of looking at all of it.

Viviane Clay: Cool. And yeah, thanks to Tristan for, coming up with a lot of this framing and also the solutions incubator idea.

Jeff: Yeah.

Tristan Slominski: I agree, Tristan. We could never come up with the diagram, so... Thank you for simplifying all of that.

Jeff: I'm gonna say.

Tristan Slominski: It's more insane.

Jeff: Yeah, we need our own process with Tristan moving from his ideas and theory into practicality. How do we make this easier for everyone to see? You know, just... this thinking about... watch... looking about this, and we're going back to the other question about You know, what will be the... what will bring our technology into the world. Sometimes you have to wait for new things to occur outside of our world, like new sensors or new robotic things. And, you know, one of the things that occurred to me was, like, drones, right? Drones, I don't know, they weren't a big thing, I don't know, 10 years ago? I don't know. When did they start really becoming a big thing? I don't know. But, but that's a whole new sensory motor platform that lots and lots of people have. And all of a sudden, you know, the appearance of something like that is something that, you know, maybe these killer applications for our technology require a new thing like that, that hasn't happened yet. And so, so we may not be able... again. we can't necessarily predict that now. That's... this goes back to the incubator idea. There are probably new technologies and things like drones, maybe drones themselves, that are coming about that where our technology really will take off. And so we have to be... somehow have ears to the ground, or let other people think of those things. Because that may be what's required to have our big moment, you know, something that, oh, millions of people have this, and they all of a sudden could do it, and it's a new thing. And we can make it better.

Viviane Clay: Yeah.

Jeff: Anyway, it's a great summary.

Viviane Clay: Cool. And then, yeah, there's also...