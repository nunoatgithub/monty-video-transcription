so yeah, today I guess won't be a typical research meeting because the plan is to talk about the research roadmap and what's up ahead in the next months instead of An actual research topic, and Niels and I have been, talking through this for the past weeks and also met with Jeff, in December and revised the roadmap a bit after that meeting. And now we thought it's a good time to share with everyone and kind of get everyone's feedback and then we can finalize it. and I just have a general first slide, which I hope will put everything a bit into context. And then Niels will go over the research roadmap in more detail. And I usually don't ask this. But maybe you can hold your questions until after Niels goes through his part, just because I don't want everything to stop on the first slide and then we try to discuss everything that Niels is going to talk about in the next slides. That question addressed to me.

I'm not going to name any names. Okay. I already broke it.

obviously if you won't be able to follow without asking a question, feel free to ask a question, but I'm going to pull up the overview slide again after Niels part, and then we can talk about everything in the big picture. just want to make sure like we got the whole overview out first and then we can talk about all the details.

hang on, are you seeing my screen already? Yes. Yep. Okay. Okay.

yeah, basically I thought about what's our high level goal and this is a rough one I came up with. We can refine it, together more, but basically we want to get our approach widely adopted and I divided it into three pillars. One is.

to achieve that, we need to make our implementation easy to use and to extend, to reach a wide, range of people who want to use this. We also want to, actually have our implementation do useful things, so people actually want to use it, and for this we need to implement, everything that we figured out, over the past years, all the research progress, and then there are also several larger open theoretical questions, and we need to make progress on those fundamental research questions as well, like object behaviors and manipulating the world are two big ones. And, I guess they build on each other. So first we try to make fundamental research progress, look at the neuroscience, try to figure out how the brain does things, and then go into the implementation stage, actually implemented in Monty, test these ideas, see how well they work. If we forgot about anything and then, make it easier to use and, try to get the community involved, try to get people to build applications based on that. and this is the very new aspect that we just now started and we'll figure out a lot about, in the next month, I'm sure. so just, to go into a bit more detail of what these three entail. So making it easy to use and extend and getting it adopted and reaching a lot of people. one part of that is to document and educate people about the approach. another one is to foster a community around it, find researchers who actually want to use this, who maybe want to extend it, who want to work with this, find people with applications, encourage people to test our approach. and then another part is, To actually make the code easier to use and extend, and that involves some code refactors, and two bigger things on the roadmap there. and I'm not going to, in this presentation, we're not going to go a lot into this part of it because this is more focused on the research roadmap, but I have one slide that goes into more detail of the yellow part. At the very end, if we have time, but just on a high level note, yeah, I won't go into too much detail on this part. so then on implementing research progress, Niels and I basically put down two, Bigger objectives, one is better unsupervised learning, which is required for the next one, which is actually testing, our heterarchy implementation and seeing how well we can model compositional objects.

first we need to be able to remove this assumption that we show one object per episode and then system gets told when the next object is being shown when the next episode starts and that we give it labels. And there are several things that we have under this approach. And then once we can do this, once we can have compositional objects and move from one part of the object to another. And the system can deal with this and we can actually test how well we can model compositional objects and how we can utilize the hierarchy that we implemented. and then on the research side, one of the big topics. it's going to be object behaviors, and we've been talking about that a lot in the past research meetings, since this, seems to be an important prerequisite for implementing policies to manipulate the world, which is ultimately what we want the system to be able to do, like being able to manipulate the world, it's like what the system will be used for in the future.

and then something I will also probably play into this is, thinking more about compositional policies and how we can hierarchically decompose policies.

and then again, once we make enough progress on these on the theoretical side, they will end up on the third column and actually be implemented and tested.

there are several other theoretical questions that might come up in the next months, but that are not Main topics, one might be scale, object deformations, or dealing with abstract space, but, just putting this on there since they might relate to some of these.

and then in these, research milestones, Niels and I were talking through this again and divided both of them into two subparts, policies and learning module computations. and Niels will go into more detail of these will entail. And then on the code refactor, we have this plan to refactor a data load and dataset. Tristan already got started on some first refactors for this and has, planned out the follow up refactors. And after this is done, can make the motor policies more modular, basically having motor modules, and this will be a prerequisite for a part of the code. This research milestone, which is why we have this focus here so that we can make these code refactors that then support the research, and implement it, implementing the research ideas.

and then I just marked the ones that are next up or in progress in green. So here we have the refactors, we have unsupervised learning as the big next topic and then, object behaviors as a big research. topic, and I hope, I'm not sure if this view is useful, but I just overlaid who will roughly work on which part, but, obviously there will be overlaps, and so this is the next three months roughly, looking at unsupervised learning, thinking about object behaviors a lot, from the research side, and then in the next six months, hopefully we can transition towards, Compositional objects, hierarchy, and starting to think more about policies as well.

compositional policies and actually manipulating the world, right? Then, Oh, and the last thing is, one thing we did to encourage people, especially the research team, not only to work on implementing stuff, but also to make progress on the fundamental research questions is this new, what I learned this week, Slack channel, and just wanted to highlight here again, that. It's, this is a big part of the work as well, so really encouraging people to also keep reading, keep thinking about these fundamental research questions and that we want to spend time on as well. All right, let me hand over to Niels.

Nice. yeah, so I'm just, carrying on, where we've been left off, but I guess, Yeah, as she said, I'm going to go into more detail about these things, but were there any kind of burning high level questions just about, this overview? And, yeah, you're welcome to ask me questions when I'm talking. It was just this first part that we thought it would be helpful to just go through it in one go.

Okay, cool.

this is just a, slide from, this last summer, June 2024, when we set out what we felt were like the two main themes, we wanted to focus on for the research. And those were hierarchy, and actions. And I would say broadly, this hasn't changed. that's still like the main, things that we're Introducing and wanting to be working on, but what we realized, when we were putting together these, more kind of short term goals was that, actions was always going to follow kind of hierarchy, to, to a large degree. But before we could ever, we could even really get to kind of hierarchy in an interesting way. we needed to, improve unsupervised learning, which is why we think this would be the thing to focus on next, and then that naturally leads into compositional objects, where then the hierarchy kind of starts, manifesting.

and this is, a lot of text. I'm not just going to read through this, but this just gives an overview of some of the kind of concrete things that we're planning on, looking at in order to address unsupervised learning and compositional objects. Thanks. But I think the kind of one thing to highlight is we already have a lot of things about Monty that make it really well suited for unsupervised learning. And we've already implemented some of the groundwork for compositional objects. So when I'm talking through these kind of concrete ideas, It might seem like a relatively random arrangement of, tasks, but it's because most of the problems are, I would argue, filled in terms of what we need in order to do things like unsupervised learning. There's just a few sections of the sort of puzzle where we just need to fill in and improve performance, or make sure we have certain properties. And then hopefully a lot of this should start to fall into place.

and for both unsupervised learning and compositional objects, you can broadly break them down, the kind of different projects into the kind of overarching one, which is developing kind of a data set and evaluations that we're going to look at for that particular task. And then, so that's essential. And then within that. In order to essentially solve that task, be able to do interesting things, the two main places that we want to make changes are either going to be at the kind of level of the sort of learning modules, representations, and computations, or at the level of kind of policies, although some of those will be model based policies, and that's also the case with the compositional objects.

but yeah, so then, unless there's any questions on that, my plan was just to go through each one of these and Not in super level detail, but just give a flavor of what the kind of task is and I guess why it's relevant to these problems. and yeah, I guess one thing to start thinking about, Scott and Hojae, I know you've both kind of mentioned projects that, that have come up that, that you thought sounded interesting and stuff like that. but yeah, it's worth just thinking about what you find particularly, interesting burning questions, under the supervised learning in particular, in terms of what next projects we can work on. later this month.

so in terms of kind of a data set and evaluations for the unsupervised learning setting, the kind of good news is we're not planning on, going back to the drawing board or anything. We are going to just use YCB as we have done. But as Viviane was saying, we really want to remove the assumptions we have right now about, kind of supervisory signals, and those are Implicit and explicit, so we have things where, we show, for one episode, an object, and then we reset the network, we reset the evidence values, so that's more of an implicit signal that like, okay, you've just learned one object, now there's a new object. but we also have a more explicit, and then we have more explicit information on often where we're, learning where we pass the label, to the system, and, tell it whether certain episodes are associated with the same object. And we also have things like we give these 14 very idealistic views of every object. which, really helps us get good coverage of the object and make sure we've learned all parts of it and things like that. so it's, a really good kind of, initial setup to have to check the performance of the system. But, if we want to move towards more complex, more, realistic, kind of situations, including, the kind of compositional models. Cause as soon as we introduced compositional models, there's very few data sets where the. kind of parent and child objects are actually explicit and labeled or anything like that. we can, potentially construct a dataset like that, but, generally speaking, as soon as you have multiple levels of representation, some of those are going to be implicit and, any kind of, direct supervision like we're doing right now, wouldn't be possible.

so that's the, kind of dataset side. And then in terms of kind of computations, in the learning module side, so we discussed a few possibilities about how we are basically to get learning modules to be able to deal with the fact that they are moving on to new objects and they're moving off the object they were on back onto it, that sort of thing, basically not just having this eternal memory of going throughout all time. yeah. And we think that the simplest solution that should work well is basically to have the evidence values that we currently have that accumulate just decay more rapidly. So what this plot is showing is the highest evidence score associated with a given object as a function of the episode. And this is an episode where there's multiple objects, so depending on where the sensory patch is, this, these colors at the bottom show what object is actually being sent, visualized by the system. so this is ground truth information. the, at the start of the episode, the agent is on the potted meat can, this like blue line. And it gets lots of evidence for that. And then around step maybe 32, 33, it generally moves over to the bowl. And then it stays on the bowl for a while. And as we would like, the evidence values drift down because once it's on the bowl, it's no longer getting evidence consistent with that. But even after it's been on the bowl for something like, 15, 20 steps, it still has a really high, evidence value for the potted meat can because it had accumulated so much. And this, accumulation is essentially unbounded. and, has very little, if any, decay at the moment. it's hyperparameter tunable, but we don't do, generally any decay right now. But what we think is if we just make this memory much shorter, then in general the system should handle the, kind of fact that it can move onto a new object. and then it's just going to forget about whatever object it was on before and start building new evidence. But in order for this to work well we generally want it to move fairly efficiently when it is on an object, because it doesn't have that much time to accumulate a lot of evidence, and so what we want is, as the system is starting to do now with the kind of, hypothesis testing policy, quickly move to interesting features and things like that, but a big theme of the other work, under this kind of unsupervised learning is adding things that will, basically get it to explore, These kinds of interesting features more and, and more quickly converge.

and so one of those is bringing voting over, into the kind of unsupervised setting. as well as just in the single episode setting, we have supervisory signals. We also have those in when we do multi LM experiments. Because at the moment, the LMs, the learning modules basically know, that they're all seeing the same object. And they know to establish a connection based on that.

And, but if, as soon as we're throwing away supervisory signals and, we don't know if there's multiple objects or a larger object and a sub object, visible at a given time, we need to actually naturally learn, these associated connections. So I think this is a nice problem to work on because it's a kind of deeper problem that we had ideas for, how to solve and need for the, long term capabilities of Monty. But it should also help nicely in this kind of shorter term focus on unsupervised learning. because you can imagine in this case, if you have a collection of five learning modules, each with their own sensory patch, and they all move on to an object, you're quickly going to get evidence for or against different objects. And you won't need to do this kind of piecemeal, memorization of everything you've seen before. of course. we're still keeping in mind the idea that a single learning module can do essentially everything that we need to do, or most things. so it's not that the voting is here to enable something that a single learning module can't do, but it's just going to, help it do that more, robustly.

and then, so those are the kind of learning module computation, changes we were thinking about. And then on the policy side, I said we want to move around more efficiently, and so for the distant agent in particular, the one that kind of saccades across objects, it's not doing things like this right now. It's essentially just moving these tiny little increments like a random walk. If it does any larger movements, it's based on this hypothesis testing policy, but that tends to move it in space rather than it actually looking at a different point. So we have some ideas for both kind of model free and model based signals that will really help us Move to, the kind of most relevant, parts of the object much more quickly. and again, this is something that will be useful for unsupervised learning, but it's just like a really useful thing to have more generally if people are going to actually use, Monty in the long term. I think this will make a big difference.

And, a subtle, subtly different one is a specific focus on an exploration policy. at the moment, I'm The exploration policies are designed to try and be fairly exhaustive, but they're still dumb in that they just, follow some pattern of moving, for example, with the distant agent, it does this kind of spiral scan to try and cover as much as possible. That works really well in the kind of supervised setting we have right now, where we make sure we show each object 14 different angles, but assume we just give the object, and then in that one episode Or maybe two or three episodes, but each time, the, agent just gets it from a totally different angle. In those episodes, it needs to learn, all about the object and really explore it. and in order to do that in an efficient, principled way, we think model based exploration policies would be really useful. And, we heuristics might be, but, just a simple thing like this, where you've seen kind of part of this bowl, there's likely a variety of ways that we can we have this heuristic that, all these kind of gaps or these kind of edges of surfaces and things like that are going to be things, or, potentially points of symmetry where you would expect to see an object, but, currently there is none. These might all be points that kind of the, agent tends to think, oh, that might be an interesting area to move to, and to move around on.

and then lastly. Again, it's a subtle distinction, but, there's this kind of important ability to be able to switch between a learning and an inference focused policy. So at the moment, we do this, switching, explicitly when we transition from the learning phase to, or the training phase to the, inference phase, and again, we're able to do this explicitly because we know when that transition happens, because it's a supervised experiment, but when we move to true unsupervised learning, we, The system can, detect when that transition happens, but, we also then need to actually, define this, switch to happen naturally. This is actually more of kind of an infrastructure change, so probably fits more into what Viviane was showing on like the left hand side with the, the kind of yellow boxes, on the infrastructure stuff. the hope is that kind of once some of those things are changed, it should be a relatively simple change. It's not implementing a whole new policy. That's essentially what the top two points are, but, but it will still be like an important point to getting unsupervised learning working well.

that's everything on unsupervised learning.

Any questions?

I'm sorry, I didn't want to stop everyone from asking questions. It feels very wrong now that I said that. It's such a different atmosphere. I have a question. On the previous slide with the accumulation of the points that a hypothesis is getting.

this one? Yeah, that one. Yeah.

so as you saccade around and you move off the object, and you sense something potentially new, like it goes from a mug to a banana, and now you're sensing banana. rather than start decreasing the score for that object, shouldn't you Another possibility might be just create a whole new thing that you start scoring for and then if you go back to the mug, then you can start adding points to that again. so in the learning module you'll have multiple hypotheses that you're keeping score for or do I, have I misunderstood everything? No, I think, no, I think it's a fair point and I guess there's two, kind of things about that. So one is, Yeah, we've also discussed that we might just have a system so that the learning module thinks okay, I've moved off of this object. I was on it, but I've now moved off. So I'm going to basically just reset my evidence values and then start accumulating for whatever I'm on. we wanted to try first to see how it works with this kind of evidence decay. but, that's another possibility. The, other thing is that, yes, definitely, that is also important and that's where the hierarchy will come in. We don't think it would be the lower level learning module that's keeping track of both objects. the lower level learning module would still forget about the potted meat can after it moves off of it, and it's on the banana. But, assuming the potted meat can and the banana both exist in the world, and those are both possible locations for those objects, as it moves off the potted meat can onto the banana, a higher level learning module, which is modeling something more like a scene, would then, understand that, oh, okay, now I've moved on to this banana that it's receiving from the lower level learning module. And then when you move back to the potted meat can, that high level learning module can then reinstate the potted meat can representation in a low level one.

and so yeah, so once we're not doing multi object, like we're not doing like scene like data sets now. That was one thing we discussed with Jeff that we want to wait with doing. But with the compositional objects, which are multi objects to a certain degree, that, that will definitely be, yeah, important. Out.

I guess the way that I was thinking, to Will's question was that, we don't, I guess one thing is that we can learn that we have, moved off from what we can and went to Banana, but then in an unsupervised setting where we don't have the labels, we don't know maybe that we have moved off, is it a It's possible that we're actually seeing a potted meat can fused with a banana and it's actually one object, for whatever reason. I don't think there's a straightforward way to say, okay, I'm in a different Potential object that I can start accumulating for. Maybe if we see this trend, where it's okay, I'm continuously losing plot in Mikan, and after some episodes, after we lost like 10 points, let's say, then we can say oh, have some kind of rule to say okay, let's just accumulate for a new one. But, yeah, but we have to assume in this case that we don't know if we're actually going to a new object. Is that how it works? Yeah. Yeah, no, definitely. And I think one thing that comes into this as well, which We don't have a specific point on here, but I think we'll be important. It's just in terms of with this kind of unsupervised learning, how you forget things over time. that you might initially have built, let's say the first time you encountered a compositional object where there was multiple things together. You learn that as one object because they were together, as you say, you don't know when you're moving off of one object onto another, but over time, you might see, okay, the logo was in, I saw that, in one instance, at one time, and then another associated with a different object at another time, so you'll start forgetting the kind The, all the information that was fused to the logo and developing that as its own kind of singular representation.

and, yeah, so I think, that'll be really important. it's basically an unsupervised learning thing that we need to do for compositional objects. We don't necessarily need to do it for this most, basic unsupervised learning, which is where we remove one episode per object.

So I guess, yeah, we might need to assert, in terms of the episode transitions. But, but yeah, I don't have any questions about what you're all presenting. It's really great. I have a lot of ideas on how to implement this, but I don't think that's what we want to do right now. Is that correct?

I don't mind writing those up and posting them after this meeting. but yeah, sounds good. Yeah. And then maybe. yeah, I just, I guess there's a chance we get time to discuss that at the end, and then otherwise could do, discuss those like next week or something? I'd want to write them, I'm taking notes, I'd want to write them up just so that I don't forget them. Okay, nice. yeah. But I, I assume that's, Viviane's comment early on was like, questions if you don't understand, but maybe not try to solve all these problems right now, yeah. Yeah, I think we should definitely talk about any additional ideas. I just wanted to make sure we get like the big picture and the roadmap into everyone's head first, and then we can talk about the details afterwards. But yeah, I definitely didn't want to say this is not up for discussion or something. I've got five specific ideas already on how this How we might go about this unsupervised learning. It's on top of what you already presented. but I don't, they could take an hour. So I think I'll just leave them for now and I'll write them up after we're done here because I probably have some other ideas too. but it's all good. I have no question. I like the way you organized it. Nice. Yeah, no, that sounds great.

any other questions on the unsupervised learning stuff?

it seems like some of this stuff especially this bowl picture, ties into something that's been on the research roadmap for a while, which is using negative information. there's a difference between a point that hasn't been explored and a point that has explored and you know that there's nothing there kind of thing.

yeah. Yeah. We talked a bit about, yeah, whether to include that here. I guess the first point is. Yeah, there's, the negative information that you might get where you have a model of an object, you move off, you don't see anything, and I think we felt that's most important for, scene like arrangements where you're moving through empty space a lot, because we're going to be trying to move on, start with, simpler compositional objects where it's like everything's in one place, It's, not as necessary, and so, we might just wait with that, just, for the sake of prioritization regarding oh yeah, I was just going to quickly say, and then, as a, maybe a separate point is just, and I'm not saying this was what you were suggesting, but, one thing we don't want to do is, store negative information, we don't want to, actually have to explicitly say, oh, there's nothing here, just because then it becomes, an unbounded, like costs associated with that, but, but yeah, anyways. Yeah. I think another big thing was that the idea we originally had was related to the fact that we can detect when we're not on the object, like what, since we have an object floating in empty void, but once we move to more realistic scenarios, there will never be an empty void. So whenever you move off the object, you'll still sense something.

We still want to be able to use this information eventually, like if I predicted to see a certain feature, but then I saw something different, we still want to be able to use that. And we are using it to a certain degree, but I think we'll need to think a bit about, a bit more about it, how we would use it if it's not this explicit off object signal.

Yeah, that makes sense.

Nice. Okay. And then, the other kind of main theme then that will follow up on the unsupervised objects, unsupervised learning then is the, compositional objects. and again, this kind of starts with like about making sure we have a data set and evaluation pipeline that we can actually explore this in. and, This will probably be something we revisit, and isn't finalized, what the compositional dataset would actually look like, but I guess the most recent thinking is that it would be something like this, where we might have a series of four objects, say, and four different logos, and those logos can be associated with each one of those objects, as well as you can have the objects and the logos in isolation, and basically then we learn, in an unsupervised way, on that, on that data, and this kind of avoids some of the kind of awkwardness we had with the dinner table scene, which, had some advantages in terms of, policy and stuff like that, that we don't need to get into, but this, this basically is just A simpler in general compositional object, but also enables us to explore some interesting problems, which, yes, are on the next slide, which is, I think this is a really nice example of a research idea that we, have a good idea of how to implement it and what we expect it to do, but we just haven't implemented it yet. which is having these location specific compositional representations where if you have something like the, Numenta logo, which is bent at this angle, that you'd maybe use, two kind of learned associations to represent that, but that as the kind of deformation between the logo and the, object become, and the mug becomes more extreme, you need more, neural hardware, or in this case, more associated connections to, map that relation and understand it.

and yeah, I think it would be really interesting to finally test this idea, because this is something we've discussed for a while, and, and it's very much implementable and, it's something we could explore with this kind of data set.

I just want to, I just want to make the argument, here that this location specific, representations, I don't think it's just for sometimes, like the bent logo, I think it's all times. everything is representation. Just want to make that clear. Yeah. No, that's a good point. Yeah. Yeah, I guess it'll become most, like it'll make itself most apparent in this situation, but it'll make itself more apparent. That's correct. And because otherwise, like myself and Subutai and others, we've spent years assuming somehow you could represent the logo as a unit, on the cup and then to decide to abandon that. So that's going to be true for all compositional objects. And then I think the harder part here is just. how do we not have to learn every point, but how do we, which is a general problem of the system, we need to extrapolate. so you really need to focus on, oh, at this point there's a change, and then after that you don't have to remember everything. Right.

and so once we then have a dataset, then they on the learning module computation side, the kind of main thing we need to add is the top down connections, which again, I think we have a pretty clear idea of how these should work. As Jeff was saying, these are going to be location specific and in all instances. And, but it's a case of, obviously implementing the code and testing it and things like that.

And then another thing that we've discussed, that would be relevant here is this possibility of re anchoring hypotheses. So what that kind of means is, if, if you're familiar with SLAM or even if not, if you think about path integration, as you move, you are integrating movement and you're getting sensory information that's, reinforcing where you think you are. but at the moment we rely. Very heavily on that path integration. We assume it's essentially noiseless and close to perfect. but what, real robots have to do in the grid cells and entorhinal cortex and, probably us in the situation is at certain times, if you get strong sensory information that will, kind of recalibrate where you think you are in that map. For example, your doorway might be a very prominent landmark in your, in your living room. If you touch your doorway, you close your eyes, and you walk in a circle, you'll maybe have a sense of where you are back now. But if you touch your doorway, with your eyes closed, then you are very certain about where you are again.

but anyways, this could operate both at the level of single learning modules and also in terms of hierarchy, in terms of what the high level learning module is telling us where we should be and re anchoring location. and, yeah, so that's just something that, will be interesting to explore, but likely this will help just with noise, but also distortions as we get into things like this bent logo or a, a logo kind of distorted by the side of a mug.

and then lastly then are the kind of policies that then, again these are quite general, but we think will be particularly helpful in the case of the compositional objects. And there's kind of two themes. One is policies to recognize an object before moving on to a new one. And then the other is that once you recognize an object, move quickly to a new object. So that you basically, when you look at the world, you get a sense of what you're seeing. And then you move to the next interesting thing, rather than just randomly looking around, constantly being confused about what you are looking at. and so yeah, this, again, just brings in things like saccades where, you can imagine, looking at this, or, and then doing two or three movements on it, confirming what you think it is, and then, okay, what's the next interesting thing, as you move on.

and all of this, as Viviane's earlier slideshow was eventually leading into things like object behavior. And, I thought it was just worth revisiting this was, the example that Scott, suggested of kind of a lamp with a switch. And, this is a very simple compositional object, not dissimilar to a mug with a logo on it. But you can already imagine very complex, behaviors and motor policies that we might want to try and learn, how can you maybe generalize from the understanding that, okay, this is a switch, and this is a lamp, and then this is a lamp with a totally different morphology, but maybe you still understand that you're meant to look for the switch. in order to turn it on and at least the kind of where'd you find that picture? That's an amazing complimentary picture. That's impressive. He got the switch and everything. Wow. Yeah. Just Google, Wow.

But, yeah, so I guess it's just to say that hopefully the simplicity of the, of Compositional data sets we're proposing doesn't, give you the sense that, we're just going into a narrow tunnel and this isn't going to be relevant for later things like this. I think as Jeff often remarks, like a very simple task is often a useful way of kind of understanding a much more complex problem.

yeah, Viviane, yeah, you can Or do you want me to just keep sharing? screen up and we can just have that overview and talk about any comments or questions or suggestions. Yeah. Are you guys seeing the screen again? I have two monitors, I just want to make sure. Okay.

I was just thinking, I don't know who would be able to do this because I'm not friends with people in machine, computer vision labs, but it might be a possibility to put out there on a forum or something to say, hey, we are looking to work with new datasets. If you are a lab that, can scan these objects and do some labeling or something like that, we would love to have, the ability to augment our datasets with some, with compositional objects.

Yeah, that's a good point, especially with something like object behaviors and stuff, because, yeah, I feel like maybe like these kinds of things we could implement in like a 3D renderer fairly easily, but, but something where it's like objects that, when we eventually want like objects that move and things like that, or have interactive parts. That could be quite complex to implement and yeah, that's a good point. That'd be cool to see if anyone in the community wants to help. You could start with even simpler compositional objects and I think I mentioned this. Once, a couple times, it was so confusing how compositional objects work, so at one point I spent a lot of time thinking about two rectangles, and one rectangle represents a room and the other rectangle represents a rug in the room or a table in the room, and just trying to understand how those, how you can learn a model of where the table is and what angle it is in the room, so it's a two dimensional problem, We have just two rectangles. it was really hard. I didn't have a solution. I couldn't figure out a solution, but my point is we could start testing the code on some really simple things just to, it's a common technique, right? You just start with something really, simple just to make sure the code is working and that, you don't have any bugs in it and then work up to some simpler objects that we might be able to do. And then ultimately. I wouldn't want to dive headlong into, okay, let's start understanding a bicycle and its compositional, it's okay, that's one step at a time here. so my point is, I think there's a lot of work to be done just, Building the code and testing out the mechanisms before you even get to something as complex as a mug.

I'm not saying we don't want to get more complex objects, we do. Yeah. It's really simple. Yeah, it also occurred to me that we are talking about moving simulators at some point. And we'll do a new simulation environment. We can hand design some, I don't know how this would work with the particular environment that Tristan was reporting on. I looked at it briefly, but presumably you can design your own objects to put in there and, yeah, it wouldn't be real. Yeah, and yeah, so that was something that, for our summer hackathon, just, yeah, that for our summer, hackathon, we, in particular, Rami had implemented this, dataset of 3d objects. this, cause this was when we were talking about doing this, dinner set thing. And, we might still return to this example, or this dataset later, but yeah, I guess it was just a nice example where you're talking about Scott that with this 3d software, we have some flexibility. One interesting thing, we did have a lot of problems importing this into Habitat that turned out to be arguably much more difficult than developing the actual 3D model data set. and maybe other simulators would be easier. I don't know.

but yeah, it's worth thinking about the timing around the change in simulators and, the stuff. I think that's it. Yeah. Another advantage of this first work with the supervised learning, we can just continue using YCB, given that we have a fair amount of infrastructure stuff we also have to figure out.

it's in line with the simulator thing, but, and essentially game engines are simulators. But, some of the game worlds are quite extensive and have essentially infinite. I'm thinking of Minecraft, like it's simple, but it's an infinite world, so it could be an interesting data set already there in a sense.

how soon could we actually jump to actual real world? you guys did that for the hackathon thing, last year. the real world is easy, right? We have lots of data. We don't need a simulator. I'm just wondering, at least from a vision point of view, Yeah, I think the reason why we stayed in simulation mostly so far is that it's much easier to evaluate. we know the ground truth of what is currently being sensed. versus in the real world, it's very hard to objectively evaluate and run many experiments. but, Ken, I wasn't thinking like, I wasn't thinking sitting out on the street corner and having a look at things go by. I was thinking more take real objects, you can have them on a white background or on a white table or something like that.

you could have controlled environments where you say, oh yes, there's three objects here and this one's compositional like this and that.

I'm not saying you should do this, it just seems like we spent so much time on simulators. And, I'm just wondering, could we somehow shortcut that?

Yeah, that's a, that's an interesting idea for sure. one idea we had was that during the next, the TVP retreat, we do a two day hackathon where each of us gets a little robot kit and we do mini projects with that just to keep working with real world data. And also then someone, everyone can take it home and we can, everyone can do real world experiments wherever they live. But, Yeah, it still requires more, some more infrastructure to be set up.

It's harder to do repeated experiments that are comparable between everyone, wherever they are. The real world also runs in real time, which is quite slow. it doesn't have to. if you're talking about static objects, you set up, I have a white table with a white background, so all I see is the objects that are there, and, you start. By the way, the reason I was thinking about this, because I was thinking, we're into the discussion section here, right? Yeah. Okay. When you're talking about this, when you're talking about unsupervised learning, I was thinking, oh, I think the way we do that from vision and touch are quite different. they have different mechanisms involved, and I was thinking in vision we have all these clues of what's going on, we have a parallax, and that works with one eye, we're constantly moving just slightly, and every time you move you can tell what object's in front of what other object, what object's behind what other object, and that they're not the same object, so just the slightest amount of movement of your head not the eye, but the head. Gives you this huge amount of data, oh, these are separate objects. and, also we can take advantage of convergence of the eyes. That's not as important as parallax, but it's also important. and then there's also issues of like policies about, for explanation, we know that brain uses silency detection, so it jumps to places that are interesting, that these are not model based, these are just like, hey, there's more stuff going on over here. so these are things that, especially the parallax one, which would jump down and be like, hey, that's something that would work in real life and vision would be very important. but, and you can't really get that from an image, right? You gotta have, you gotta have a, something that actually models depth and you can take advantage of parallax.

yeah, so last time we did the real world was we just used like the iPad camera, which sends rays out and then gets an actual depth image from the right. All right. So that may work too, but that isn't as powerful. I think as Parallax tells you right away, the two objects are not the same if they have a slightly different in depth. Whereas I might be looking at one object that has, if I take a, dumbbell and I put it in an angle to me. the depth difference, one end to the other end is not really important. What's important is that, what I'm saying, I could say, oh, that's further away, this is closer, this is separate, but no. From a parallax point of view, they're not going to be separate. and I just thought that there are some things, and I was thinking about touch too and I'm thinking, there's some things there too that, it's really hard to build a touch sensor, but, but, there too, I think having real objects in a real world suggests solutions that are, that are, that make this problem much easier, really much easier. Yeah, I just want to think, so with the, when we did the Monty Meets World, we, Did create kind of the data set where we took like those depth images of the different objects and that included some like different kind of adversarial conditions where there was like bright light, or there was multiple objects, or there was like a hand kind of coming in and stuff like that. But but one thing with that is it did limit the degree of sensory motor interaction, because then we have these like depth images that are taken from one angle. And so the, Monty can saccade over that depth image, but it cannot actively explore the object. And so in fact, learning still took place with a 3D scanned version of the object.

and it's just a fairly complicated pipeline because then we need to 3D scan the object, which often is imperfect. And then I said, yeah, at inference time, we have this one image. So it feels like we almost. I think we'd almost need to take the next step, which is to have a robotic arm that can actually move, before we could, we could do it, at least to the same level that we do it in simulation now. And if we have a 3D camera, I would think we'd want to think through it maybe some more about how we could, that's a different sensor than humans have, obviously.

I don't know, I'm just pointing out that, if we're going to build systems that interact with the real world, that, it might be helpful to be dealing with real world objects from the get go, maybe not exclusively, and then we could look at some of the systems that, that biology uses, like parallax, which seems very important for, vision.

and it really, it has a huge step up on, to help with uncivilized learning.

yeah, instead of, us having to build or someone having to build like a complex object behavior data set in simulation, not in the next six months, but maybe once we actually look into object behaviors and motor policies that we actually just go into the real world and, then we will probably notice a lot of other things that we didn't think about when we were just doing simulations. I don't think it's going to take us very long to figure out how this action policies and model behaviors work. I think we're going to get to solve that pretty quickly.

Sounds good. Yeah. Remember, that's my goal for that, for our off site, our retreat, whatever we're calling it. We'll see.

All right, that's some thoughts I had there.

at touch, discontinuity of locations is really important. Like with vision, you can just move your eye a little bit, and you slide from one object to the other object, and you don't really know that in a 2D image. Even depth information doesn't always tell you that. But with touch, It's quite different. It's, much rarer that objects are, that are physically on each other, touching each other, that are separate objects. It can be, sure, but it's much less likely. And when you're running around, if you finger around an object, it's much less likely you're going to just, accidentally slide into another object. There's always going to be some point of discontinuity, some, even if the two objects are touching, they will be, there'll be an edge of some sort, or an angle or something, and those are clues, right?

those are definitely clues. And then when we touch things in the real world, often we move them slightly, and if we move them slightly and we're looking at them at the same time, then we get a, there's your parallax again, and then you know they're two separate objects. there's a lot going on in biology and real world stuff that, that we, don't want to kill ourselves trying to come up with alternate solutions if some, if we have to use some other biological solution.

Just stuff to think about. Once you, just to come back to the time thing, once you put a robot in it and you're moving, now you can only run in real time. But why can't the robot go slow? no, it can go slow, but, from an experimental point of view, real time experiments are going to get pretty painful pretty fast.

I guess I may say, what, Oh, I see. Now you're going to need to put a new thing in your robot arm, wait for 15 seconds to go by. Like that process becomes pretty painful quickly where if you were in a. In a simulation environment with physics engine, then you can pick up your objects and knock them over and yeah maybe that maybe the compromise is that you keep in mind constantly how you're going to do this in real world And don't you know and think about the things that are going to help in the real world like the parallax it's something we haven't really introduced at all You think about that in the real world and then you do your simulated world But you make sure that simulated world is going to work in the real world as best as you possibly can. and then you can always, drop back out and test a couple of real world things. my fear in the simulated world is we end up solving simulated problems. And then when you go to the real world, things are quite different. and so There's a lot of research in that direction at the moment because of robotics and autonomous cars. Yeah, there's sim2real and real2sim people who are working on exactly bridging this gap between simulated world and real world, and I think when that time, I think we should always keep in mind of the real world, and then when the time comes where we can, or hopefully where we understand kinematics. I'm not going to go further than that. keep in mind is not the same as, we might think about doing it so it does work in the real world and test it in the real world, but then don't run all your tests in it. As Michael was saying, that can be very slow. But I think the real danger is saying, oh, yeah, we'll get to the real world eventually. Let's stick to the simulation world.

Yeah, I think a fun way to encourage this could be to have every six months we do a real world hackathon where we implement, like, where we actually work with real world data and try to solve different problems, using our current algorithm. But then on a day to day basis when we evaluate our, our experiments, we, still run them in simulation, run our benchmarks in simulation. there could be a hybrid of that was like you, you run them in simulation, but then you always have at least one. One sort of test, like you did in the hackathon, Monty Meets World, we do one real world test to, just to make sure you're not losing sense of reality, what I'm hearing that ought to be possible, is we currently have unit tests, integration tests, end to end tests. This would be like, Real world test, right? So it would be probably the slowest one that runs the least often, but at least we would be beginning of the feedback loop that's regular, right? You think of the real world test as like a unit test for the real world. How about that? Because we don't have to do anything super sophisticated. We just have to make sure. It works. Somebody runs it once a month or once a week or something, but it's it's the start of that feedback loop and then we tighten it, it, doesn't go on.

They automated the real world part of it as well. So like in part of that test suite, HP also tested like building one print cartridge or something in an automated function.

Maybe next year I can, once a day run the real world test of washing my dishes with Monty.

I, wanted to ask, I wanted to ask that because Jeff keeps looking at parallax and, Like that's, I just want to do a check of my understanding, right? But parallax is something we can test with existing things by adding an agent that moves the distance sensor's location stochastically. And as long as we integrate that information to the agent's location, that will generate parallax, right? It doesn't control the head movement, but just imagine it's an eye that's stuck on the head and the learning module. It's a model free, it's a model free, yeah, you could just move your head a little bit left and right.

So it doesn't have to be stochastic, it could just be continuous. Okay. Yeah. So that seems like that's a actually very nearby experiment that we could add as we're figuring out. But would you do that on real objects or you do that in the simulator? No, it's in a simulator. it's an agent sitting in a, it's just moving the agent location versus turning the distant agent, right? Yeah. Yeah. It's the first person shooter moving. Okay.

that would be good. So that's, exactly what I think, you think about parallax, because that's something we know biology does, and we, and it's really powerful. but I just, but we, so it may be something we want to include. Even if we're doing it on simulated objects in this, I think, Tristan, that was exactly what I was arguing for, somehow, somehow keeping the real world in our thinking here.

yeah, and it definitely appeals the idea that we don't, we're not too reliant on specifically depth based cameras, or like cameras with depth, because that is quite a Let's try it because I'm sure that it doesn't, it's not as, the depth isn't as, isn't as, an important indicator than as parallax.

of course we're not really aware that the parallax is happening. We just, sense that objects are different or different depths and, we just know it, how does we know it? And it works with one eye. So it's it's okay. We're not relying on two eyes. It's parallax. That's the answer. there was a small amount of, lens. when you focus at different depths, the length changes a little bit to get it in focus, but that's not, I don't think that's nearly as important, signal. Yeah. sometimes there's some of these things that, like in general, it feels like a lot of the subcortical stuff is where if we are going to ever use deep learning in Monty, like that's where it could be useful. And for example, deep learning is quite good at doing just basic figure ground segmentation, where it's just gives a core sense of oh. What are the objects near me? what is their approximate depth, whatever, and what are the ones, is it doing that on, stat, on images, or is it doing that on a real world? there's lots of different systems, but some of them are specifically video based. but I, guess I'm saying is yeah, even video images. the thing I think, we're gonna create systems that work in the real world. I think it seems like crazy just to make a system that work with two, two dimensional representations of the world. because we're going to be moving through the world, our agents will be moving, they'll be moving not just their eyes a bit, they'll be moving their bodies and they'll be rotating things and I always felt that the reliance on Yeah, as in, so your point is, yeah, you'd want to, it would still be a video wouldn't it, but as in, you're pointing to, you'd want to make sure that the agent or the system, whatever, is moving, and it's that kind of video that it's a video, yeah, the problem with video, we think of, that's what I see on my screen, I see a video, right? Yeah. I always, I've made this comment multiple times in, in talks I've given in the past, where computer vision isn't, isn't vision. it's it's not vision at all. Vision is an agent moving through the world, moving its eyes, moving objects, and that's the problem we have to solve. So we have to be really careful not to try to solve the, the image on the screen. I don't even know it's possible to learn the world through a 2D presentation like that. It's something we're able to infer after we've learned the world, right? but I think it's, likely that you really can't, we couldn't learn the world by looking at 2D representations of static images or moving images.

we can probably, we can do a pretty good job of inferring it later, but I don't think you can learn that way. Certainly it may be much harder.

But I think, I liked, I forget who said that, it was Tristan's idea, just, or Michael's, whoever was saying, oh, you just add a little bit of movement to the eyes in our stimulators. That's pretty cool. I like that idea. yeah, that was Tristan's idea, and yeah, it sounds like a pretty, quick thing to add and test.

could be interesting.

yeah, if we have a couple of Go ahead. Yeah, go ahead. no. I was just going to say, if we have a couple more minutes, I could go over the last slide, which covers a bit more of the rightmost column of, the community aspect and make it easy to use. But I just want to make sure we first discuss everything related to the research roadmap. So if you have more comments on that.

I have an idea that I'm writing up for the, for object behaviors, but I wouldn't have to do that now. I can just write that up and post it and we can talk about it another time.