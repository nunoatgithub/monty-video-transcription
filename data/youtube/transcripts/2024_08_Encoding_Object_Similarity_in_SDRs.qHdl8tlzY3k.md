Okay, yeah, so this is, encoding object similarities in SDRs, and that's the work I did for my internship. So a quick outline, I'll be going through, some motivations, goals, and limitations, in the beginning, and then we'll go through some toy examples for how I compute similarity from evidences, and how I encode those similarities into SDRs. and then, towards the end, we'll have schema. We'll go through some synthetic and Monty results.

so, in Monty, we have sensor modules, and these modules, they go, they take, they make some observations of, let's say, a coffee cup, and, regardless of whether these are distance or surface, sensors. They still get some features, and these features are, later, converted into some, in observation space, these features would be some patches or some RGB, images or whatever. And then we, convert those into, features like hue or point normals or curvatures, and that's the job of the sensor module. And, it does that so that the learning module that comes on top of it can take those features and can apply some similarity metrics to them. This is very important for generalization.

without it, the recognition will be very brittle, and, we will not be able to, this is how we apply some tolerances on the features. And, that's very important because if we want to, create this modular structure of L LMs stacked on top of each other, then the higher level LM needs to take those whatever it is being sent from the lower level of them and also apply some similarity measured on it so that they could do it can be generalizing generalized to different object variations or feature variations. So this allows us to do something like this, and then, Monty can then generalize to those features and figure out. Okay, so we're looking at the cup, regardless of whether we discover this, there could be different lighting variations or there could be different, features different morphology or point normals. We can apply some tolerances and thresholds, and then we can still recognize that this is a cup. Now the problem is, in compositionality, is when we add another layer, stacked on top of it. What do we want to send to this higher level? this higher level is only, looking at objects in a scene, right? So it knows, or maybe yeah, so at an intercept like this, we have some, plates and, knives and utensils and stuff like that. And it knows the relations between them. And it knows that, okay, I'm looking at an object, but what is an object in a higher level scene? what does it expect to see?

there, we have different, options here of what we can send up. We could send up the full graph, and then, the higher level is going to have to figure out how to apply some similarity metrics between the graph that it has learned and the graph that we're sending up so that we can apply those tolerances. Otherwise it wouldn't be able to generalize to different outfits. We usually send it to full graph. You mean all the location feature pairs? Yeah. No, no one, there's never been a thought or consideration that we'd actually do that. yeah. That's, gets rid of the whole point of having these learning modules, Right. Okay, We're not doing that. Okay, so that's why she mentioned it. Nobody's saying what are our options, so what option could be that's not. I don't think that's an option. Yeah, that's not. That would be a, that would be a naive approach. That's not. That's not. That's not. But yeah, to your point, it would enable some degree of generalization because you could have, you could compare graphs. It just, yeah, it wouldn't be a very, it'd be terribly inefficient and weird way to do it. The whole point here is you need to form representations of, that are constant over these variable inputs. And that wouldn't be doing that. That would be passing all possible things. Okay. But if somehow at the higher level is able to compare these graphs, then it functionally would still work. but it's not really summarizing any of those objects into a single, representation. what we really need to send up. And of course, we can't send up just the object I. D. Because that assumes that all of these objects are equally similar or equally distant from each other. So we can't really apply any similarity metrics on those object IDs. If we send 1, 2, 3, we don't really know how 1 and 2 are related together out of respect.

I'm gonna push back on that a little bit.

you're showing a whole bunch of things here in ways I don't think about this. you say you can't pass an object ID. It's not clear why you can't pass an object ID. mean you said, because I lost similarity. I think the whole point of here isn't this whole exercise to ask if object IDs can be represented in a way that there is similarity? Isn't that the point of this exercise? Yeah, I think he's saying when he says object ID is like an integer, oh, it's object 23 versus 42. I think that's what you meant in our theory. Object ID are always SDRs. It's never like that. So I, refer to the object ID as an SDR, so I never refer it as an integer so if you're. You're going to slip into putting integers in. I know in some versions of Monty it was integers. maybe. I guess I'm thinking about it. That is how it currently is. Okay. Alright. I'm thinking about brain theory. yeah, so that's the baseline that we're trying to improve on. it would be explicit. oh, if we use another PID that was an integer, then it wouldn't be helpful at all. I couldn't determine similarity. Yes. yeah, for object IDs, there is no similarity embedded in those IDs. As integers, because we're just being able to separate them, but we don't really know if object ID 1 is closer to 2, and so on.

what we really want to do is send up some representation that encodes similarities, such that these two cups are closer together than they are to the serial box.

if we, so for example, if we replace those cups together, if we switch them, and it senses cup two versus cup one, the scene, has some encoded, tolerances so that it really doesn't matter. it's close enough to what it has learned. so it maybe wouldn't change the scene that does things to this. but if we, for example, change it to a cereal box or something like that, then, it would need to say this. I don't know if I should just let you do your presentation or I should keep reading. No, you can keep asking. you made a statement, we can't, we have to, these things have to be co similar. It's not clear to me that's the case. for example, we've talked about various ways of doing this. a model can consist of, sort of morphology of an object, and that's independent of what the specific features of that morphology are. I was talking about the orientation, which is the morphology. Just the orientation of features is enough to recognize and see similarity between objects. So instead of, you might not think this is a monolithic thing you're passing up, but a model, what you're recognizing could be, it could be both morphology and specific. there could be two different things you're passing up. It's not that there has to be a representation that encodes both of those things. Yes. So that's an assumption that I'm not sure I would sign up to be. Maybe what you did during your internship. But I'm just like, why not? That's not something I would sign up to. Yeah. when we get to the part where the limitations are, I'm going to explicitly say that this inherits the limitations of evidence LM. And when we do evidence matching, whatever we accumulate. as extra evidence, that's going to go into the similarity and that being some time. when Vivian did this, she added, she said, okay, so we have features we can add, we can use those features to add evidence, or we could just only do for morphology or whatever is being used to add evidence. This is what we really use to create the similarity scores. So it really inherits the strengths and weaknesses of these, of these evidences. we are only here, we're only sending one representation, one SDR representation, but, and that includes whatever is being encoded in the evidences, if that makes sense. I think the other thing is, even if, Jeff, we had something to merge, these two representations into, a generic mug morphology, which we probably do want to add as, as well. I think it's still useful to capture that mug is more similar to, coffee cup, like, a traditional coffee cup or, a soup can or whatever, then it is morphologically similar to something totally different. So I feel like pooling together the, kind of morphology models into a single one doesn't, solve the entire problem. But wouldn't the morphology model capture the similarity between those different mugs? The graph does. I'm not sure, maybe, I understand what you mean by morphology model in this case, but that's essentially what we're trying to encode here. Yeah, just, the morphology is basically just the information that sends up that the object exists at this location in this orientation. And we also talked about, that usually the sensor module would send a skip connection also to the higher level, module, so we would get some point normal information there, which, would be similar between the two cups, but not to the, not to the I guess I would think about this differently than you guys would think about it. so maybe I'll let you go through this exercise here in the quiet room. Yeah, there has to be multiple types of information sent up. it seems to me that's the case. And, we don't understand all that, but it seems like we've, in my mind, we've framed some of the constraints on this problem. and I think so far the presentation doesn't adhere to those constraints, but it's okay. I'll let you keep going and I'll interrupting you.

yeah, yeah, okay. So some clarifications, We do not assume. So when we encode similarity in the, in the SDR, so now we switch the SDR to certain we are going to, yeah. Okay. So what we, are sending up is going to be an SDR that combines, morphology and depending on how we accumulate evidence can also combine features as well. You gonna send up one SDR and you're gonna talk about how to encode things in that one, right? So if we change the way evidences are set up, could we change? Would that automatically Yeah. So fit in yourself if we did what Jeff's saying it had. different types of evidence or information. Could you apply this technique to? Yeah. Yeah.

there is a version. I think I don't know if there's a version of that, but I think in evidence L. M. Sometimes we can ignore features or we can say that we don't want to use the features like colors or anything like that to add evidence. And if we don't do that, then, the, similarities that we encode are not going to consider features, it's only going to be a morphology, And it would be pretty easy to, yeah, accumulate those separately and have two different, you could, and then you could easily have, yeah, okay, this is the morphology evidence. This is how confident I am on this object given morphology only. This is how confident I am given things like color and lighting only. Yeah, so this is the point to whatever we accumulate as evidence. This is what gets translated into similarity in the included SDRs. Yeah, I guess for what it's worth, or like a bit of context, I guess we didn't really maybe worry about changing that in the current setup because Monty does tend to predominantly rely on morphology already. if we just look at how it weighs different objects, it already does assign a lot of importance to the morphology. So we just figured, okay, let's take those evidence kind of similarity scores and see how we can turn it into an SDR. Yeah, and I think this is by design. I think that, EvidenceLM only uses, adds feature, adds evidence when there's, good features, but it does not subtract evidence, as well. Yeah. Yeah, morphology is treated differently than features, and it can recognize an object just based on morphology without the features matching.

in an SDR that we send up, we are, we do not assume that there's a specific bit that has a meaning, so we don't say that the, bit five does. Represent some cylinder like feature in the object, but we still are saying that similarity is represented as a distributed code of multiple bits.

the similarity itself is distributed on the feature on the bits, but there's not a specific feature for every bit. And, hopefully that's somewhat addresses the capacity issues. including similarity.

and also, in what follows, there might, it might seem like I'm doing some sort of deep learning, but I'm only using some optimization, algorithms like gradient descent. I'm not really doing any, deep learning, to encode those into SDRs, to encode the similarities into SDRs. It's a, I'm not even using a single perceptron in this model. So there's really no learning and, it's very quick because there's no weights to be learned.

the goals of this is to extract, similarities scores from evidence. So we want to look at the evidences as we are looking at a cup. We're doing some sensory motor experiment and we want to extract the similarity scores from there. And then we want to precisely define evidence scores. Yeah, so that's coming in the next slides where you're using already. So I don't, I'm not sure I know what it means. so we, when we are looking at an object and we have some other graphs in memory, when we're looking at that object, we can see, we can accumulate evidence. on, the objects that we have in memory and this object as we are going through it. So it's just basically your, confidence of what objects, a set of objects that you might be looking at in the conference, but yes, that's right. Yes. okay. So evidence scores are scores for the different objects that might be, yeah, scores for the hypothesis.

and then we were, we're getting SDR representations from, those scores that we accumulated from that similarity that we found. So these are two parts and I'll go through them in details. Continuously adapt the SDRs to the new similarity targets. So in Monty, when we're learning from scratch, we're adding, we keep adding more and more objects. To the graphs. So we want those SDRs to continuously adapt to that similarity matrix can grow larger and larger. So we want to, the, object SDRs should continuously adapt.

And, we also want to maintain some sort of stability between the old objects that we have saved in, the old SDRs that we have saved in, memory and the new objects that we, received in. So that the higher level, it's expecting some object representations, and these representations shouldn't change too much so that it can still represent the hierarchy.

and then maintain some useful SDR properties. we did some testing on that, but, we still want to do some more. Limitations as I've talked before, I mentioned those, that if evidence LM is, for example, sensitive to some scale variations, then, for example, two objects, one is exactly the same morphology with one small cup and one large cup. these are going to and we don't accumulate evidence, on the large cup because the small cup is a different scale and that's going to carry over to the, to the SDRs. So the similarity between them is going to be small as well in the SDRs.

And, yeah, and also, as I mentioned this before, morphological features, if they're not, if only morphological features are considered, then the color will not be represented in the SDRs. or non morphological features will not be represented in the SDRs. one thing that also I need to mention here that we still need to store these graphs in memory. we will still this is not a way to, get rid of storing graphs in memory. We will still have those, stored in memory What we're changing is that, what are we passing on to the next level? That seems like an obvious thing, so I'm surprised you didn't mention it. how could we not store the graphs in memory? That would be not knowing anything. No, so, We're not using SDRs to store the graphs, I guess you're saying. You're just using, you're keeping whatever was there before. yeah, so after, after we convert those, after we create object SDRs for the graphs, we have to still keep the graphs, and we're only using those to send them to the higher level. But isn't the graph the memory, not the object? Yeah. We're not replacing the graph with this. I can't imagine how you could ever even consider replacing the graph. No, the graph is the definition of the object, so how could you not store it? yeah, I think you would always need to store it, but you're not storing the graphs as SDRs like in the brain the graphs would be stored as SDRs, graphs stored as a pairings of multiple layers of cells associated with each other over time. and there will be SDRs, right? So But I think what I was, what I meant to say here is that, one way of going towards neural representations is that if we can figure out how these graphs, can be represented as SDRs is that later on we can delete those. I, think that's, absolutely impossible. the graph is not a singular thing. A graph is a set of Observations at locations. You can't get rid of that. That is, I can't imagine it possibly getting rid of that. That's it. That's the whole. Yeah, because I guess to clarify like, yeah, because even if we went to grid cells, like even though at any given time, a grid cell activity could be viewed as an SDR. We would still need like multiple sets of active grid cells bound to multiple sets of active feature cells.

there's no way to summarize an entire graph with Yeah. I think it's more accurate to say here that we're not changing the representation of graph there. Yeah. Just, yeah.. And by the way, I dunno if it's clear, at least to me, and I think to Jeff also, like the synaptic. the, dendritic synapses that are stored on active dendrites and things, those are also SDRs. just to be, so the permanent storage is also SDRs, it's not always just dynamics. if, Subutai suggesting, hey, we don't have to change the nature of our graphs, you're not doing that right now, you're just talking about creating I'm not suggesting, I do think we need to I'm just saying that, but this statement on its own makes absolutely no sense to me. Yeah, I agree. It's like saying, we don't need to memorize anything. Of course we need to memorize stuff. it's it just struck me that oddly. maybe say explicit. No, not even explicit. It's it's a non normal reference. Maybe just say, yeah, this is not affecting how we store graphs in memory. Okay. You're only changing what we're transmitting. From learning module. This is changing essentially the common part of the common communication protocol in particular in the case of hierarchy. I'm not trying to change how we represent graphs is what you're Maybe that's exactly that. That's how I, that's not what it says, but that's what meant.

yeah. I'm still not quite sure I understand evidence LM, but maybe we can, I know. Yeah.

Oh, evidence lm is not the part that I worked on, but. no, I understand, but, yeah. OK, so how do we compute similarities from, different objects? Let's say we have these three objects, and, we're, using evidence lm to do the matching.

we do this matching in a sensory motor experiment. So we're looking so in one of the every episode is going to look at a different object. And let's say that we have this sensor module looking at the fork right now. And before we start the episode, we have accumulated zero evidence on all of them. This is, by the way, a toy example. This is not a real experiment.

So after moving through the, after getting a sequence of observations with the action policy and all of that, we basically accumulate evidence on all of these objects. And we're just accumulating evidence on the. the similar objects, they will have a higher evidence, score than the dissimilar objects. So we basically accumulated more evidence on the knife here than we would have accumulated on the cup. Because there is not, a sequence of observations that matches, what we have seen on the fork, for the cup. Now you're going to do this, in theory you're going to do this for all the objects that this thing knows? All of the objects in it, yeah, all of the objects from all the hypotheses on all of the objects. which is biologically impossible. Just so we all understand that this is the engineering way of doing it. Yep. So is it a single number per object? This is the maximum evidence. so every hypothesis is going to You know, accumulate some evidence for every pose and every, for every pose and yeah, and every location on the cup and so these are all of the, possible poses and locations. and basically, this is what I'm showing here is the maximum. This is the best hypothesis, from every object. Okay. So at this point, it's a single number per object. Yeah. It's 0 to 10 hours. It can keep going up. It can, so if we have, so we're going to normalize this, but it can keep going up. Okay. Not normalizing it yet. Not yet. Okay.

okay. we're going to calculate the relative evidence with respect to the object that we're looking at. And just. relative to that object. And then we're going to linearly, map it, to the range of overlap bits that we want. in this example, we're using SDRs of size 2048. So 2 sparsity of that will be 41. And, so we basically linearly map it from the range negative eight to zero. We linearly map it to, zero to four. So these are the number of bits in the SDR. That are active out of 2048. This is the target. This is the overlap. This is what we're going to aim for, yeah.

Wait, not the number of, total number of bits active. It's the overlap. The overlap between, yes. So this is what we're going to. But isn't the maximum number 41? Yeah. So between itself. So this is basically between the fork and itself. So this will make it clear. If we put those in a heat map kind of thing. this is, this would be the row that we populate. Between the fork and itself, this is just going to be 41. And we don't really have to aim for that because the representation that we create is going to have to set. So this is the desired overlap score? the target. But yeah, like the knife would still have 41 bits in its SDR, but it's expected to have 36 overlap with the fork.

and that 36 comes from, how did you get the 36? just from the evidence score. so it's not a target, it's what it is.

Because when we encoded, we still haven't created SDRs yet. So that's going to be the target. Okay, so if I have infinite knowledge, I would know that the fork is supposed to have 36 bits overlapping. And then we're going to encode those. So you're processing determined that or something. Yeah. Okay. So that's what we're getting from the evidence, scores.

And then we can do the same thing for episode two for a different object. And then. You get something, like that, which is filled in the second row. So could I think of this as, if you're just comparing those numbers, could we convert each number using our scalar encoder and then just do overlaps between them? Or?

Convert which numbers? The max evidence score. So what we want to create is object representation, not So these are only the similarities between them. The scalar encoder is just it's going to take a number and it's going to give you a similarity score. But we want to scale encoder would give you an SDR, right? Given a number, but it wouldn't be like the there might be another object that would also have 10 evidence, but it has nothing to do with forks. Okay. like the, scaler value itself doesn't really tell us what we want. It's the relative. Yeah. Okay.

Okay. Yeah. we don't wanna encode the similarities. We want to increase the object. The object information. Yeah. And, the similarity is going to be calculated by the higher level lm.

And same thing here for the last object.

okay.

and I noticed that these are not symmetric, and they're supposed to be symmetric, but because of the nature of how we're doing this, we just have to average those. yeah, so basically we're creating symmetric matrices. so I'm just gonna average these numbers. why is it supposed to be symmetric? That's not obvious to me. It's not supposed, so in a sensory motor experiment, it doesn't have to be symmetric because we can take different paths. it just doesn't, just inherently, I don't see why it should be symmetric at all. It shouldn't be here, but we, when we encode the SDRs, they, it's a distance, metric, and that basically a distance from the So if you're detecting, you're exploring, they need to reach the metric. Is that what you're saying? when I'm exploring, they are asymmetric, and that makes sense. No, you're putting it in a symmetric matrix, symmetric representation, is that what you need to make this technique work? Yeah. Okay. Because the similarity between A and B should be the same as B and from B to A. Yeah, why do you think they should not be symmetric?

Or like the lack of symmetry is just noise. Like we're essentially averaging over noise. if you visit the exact same points. and that's all points on the object that yeah, you would assume that they're day to day. You wouldn't observe the same points. In fact, almost never would observe the same points. So just like at a logical level, you would think similarity should be symmetric, but not practically. But isn't that what you want to convey roughly? I don't know, but it just wouldn't. I'm just saying, where does that come from? It doesn't come from observations because observations wouldn't result in that. Yeah, which is what we have, and I guess this is the ideal. Oh, maybe it's the ideal, but if you're going to A real system wouldn't know the ideal. It just wouldn't know it. Yeah.

what neurons are going to look at this and say, Oh, yeah, these should be symmetric. It's no, I don't know. Okay, fine. At a certain level, I feel like neurons could average over experience to get something similar to just basically to, to factor out the fact that there's no importance in terms of the directionality of it. if you observe the fork many times, and you observe the knife many times, some cases you'll think of one more than the other, and yeah, you'll have this noise on both sides. But there shouldn't be any, directional bias. Yeah, I don't think noise is the right word for it. It's just what you, sense so far, if you only sense the handle of the fork and you happen to think it's could also be a knife, that's not noise. it's just what you, yeah, you're right. I guess I just feel like any sense of a one directional bias is noise, but I don't know. I, I, I think in real world, this wouldn't be true, but I'm like. Let you go forward, in the real world, you would see many forks and many knives, and you could add up these scores over many times that you see these objects. So over time, it will become more and more symmetric. Without having to do this explicit effort. I think what actually happens is that you learn, behavioral patterns that are, seek out the answer. And you don't come up with a sort of these, general overlap scores like this. I wouldn't like, if I feel something like maybe the base of a Fork, I would immediately jump to the top and, or if I, I, it just wouldn't be, it's not like we get all these points. We're gonna see them all over time. It's just not like that. The brain figures out action plans that are not symmetric, but I don't like it. So I keep going. I just, so I, for what it's worth, we are using that policy when we're getting these observations.

but yeah, I also want to say that this, we don't really have to do this in a sensory motor experiment. We can actually do it on the graphs that are saved in memory if we want to. during sleep time or whatever for the Monty system. And, we can parallelize it as well, if you just use these similarity, scores, you can end up doing some sort of hierarchical clustering. And, I don't know if this is Yeah. Okay. So you can see that, cups are clustered together and, Lego blocks and all that, I think Niels has shown something like this before, yeah, I think it was Vivian who first made this up.

yeah, And this is based purely on morphology? Depends on, what you accumulate and the evidence. yeah, I think it's, it has more weight on the morphology than it has on features because we don't subtract. I'm guessing this is going to, this is really going to represent the similarity between objects. Often, small difference in two objects makes me perceive them as completely different. I know they're completely different objects because of that one small difference. And that's the problem I always have with this approach. It's yeah, I got this one bit difference here, this two bits difference, but it's That's the difference that matters, and I don't gloop everything together, I sometimes completely separate them out, oh yeah, this is the I feel like that's an orthogonal problem that we could easily solve by basically outputting in the CCP two SDRs. You have the general SDR, which is the one, this one, which is for generalization, and then you have the more kind of pattern separation SDR, which is going to be like completely random for every object. And, so that would, yeah, that would be like the Numenta logo or, Numenta cup. Okay, fine, I just, that wasn't stated up front. Okay. that was a possibility. But I, yeah, I guess the problem that Ramy opened with the data sets, that's where we're really hoping that this will help. for example, we learn a dinner table set up with. normal color and now, okay, now it's like medieval stuff. How do we generalize to that? Okay, but yeah, to your point, we would also have an SDR for like medieval cup. All right. And then that way we know we're on that scene. It just, from a presentation point of view, it would be helpful to state that problem up front like that. No, like this is what we're assuming these things, otherwise I don't know that. And, and then I look and say it's not going to work. I think it's also, I think, hard, just to be, for Ramy to come in and, have 10 years worth of, I understand that, I understand, but it's also, I don't want to sit here and just, be silent and I don't understand what you're talking about, that wouldn't be useful for anybody. I think the nature of his thing is to focus on a very, one very specific thing. I do understand that, we talk about passing up one thing from one region to another, that's not what we're talking about. Because that's not going to work. You're saying, okay, this is a, an overlap similarity SDR, but it's not going to be sufficient. We'll have to have something else in addition. That helps me a great deal. Yeah. I also have a slightly different view on the, problem of the, the, there might be a, one feature that gives you a completely entirely different similarity. And that is if you are, moving, so I, think it depends. It still depends on how you accumulate evidence. And that also depends on what motions that you took on the, objects.

so for example, if you look, if you're looking at an object and you know that it has a handle and then you, your action policy basically tells you, oh, I need to go check the handle, then you're not going to accumulate any evidence on the other objects. And that is going to be reflected in the similarity scores, right? So basically, the action policy, knowing that this graph is different is not going to, it's going to, during exploration, it's going to go, to the features that it knows, is, is different or discriminative about this object. And if you're doing that in a sensory motor experiment, that you're not going to, accumulate any evidence on the other objects. for example, here. Are you trying to say that the objects will be more separated than Yeah, so basically you're going to go to the discriminative features because the action policy is telling you to go there because this is, for example, here we're looking at a fork. then it's The action policy should be smart enough to say, Okay, so I think maybe that I'm on a fork. Let me go to the tip of the fork and test what's there. And if we do that, we're not going to accumulate evidence on the knife, because we have, the difference in evidence is going to be large. when we encode those into similarities, then, that's, it's going to inherit that they are different. Yeah, or at least there's not, as much evidence. but I think it's, if we want it to be robust, that we say, this object is very different from this object, I think it is still useful to have that as a separate representation. that's just the way SDR bits Like SDR representations work that it would be useful to have one that's okay, you can corrupt a lot of these bits, but I know that this is a unique object. Whereas like these two are going to have some overlap. Yeah, I think you two are talking about two different things. I think if I like a version of Jeffs would be like you have a fork where the right tip. Of the four little prongs, the right one is slightly dented. And that happens to be my personal favorite. That I grew up with as a baby, so I really like it. And no amount of this is gonna, The example I have, That's what he's talking about. In my kitchen cabinet, I have a set of mugs that I got for my wife's birthday. And they're clever mugs. They all look identical. They're like camp mugs. I like them. But one of them has a little chip in the corner. And it's just coloration. They all have the same mind, they all work the same, I use the same action policies on it, but I do know that one is unique. It's how I picked that one today. And I know that exists, I can look for it, but it's also identical in all other features. So that's what I was thinking, it's just like what Subutai said. Yeah, I think it's relatively more identical, but you can still figure out that it's different. Yeah, but it has this, like in our case, we have this metal. thing that fell off during the 89 earthquake and dented itself. So we remember it. It's like a memory of, our memory of the earthquake, My mug example, they're identical in every way except for a slight discolor, a slight little chip discoloration in one part of the lip. anyway, but I do, it's like a separate object, but it's the same as all the others.

yeah, so we can have these hierarchical clustering figures. so now that we have this, symmetric, target overlap, now we can encode it into an, into SDRs. So the target here would be, the goal here would be to generate three SDRs with these, pairwise similar overlap bits in between them.

So we start with, the target overlaps, and I'm going to assume here that we have, these target overlaps they have. we're going to create toy SDRs. Basically, SDRs with size of 10 bits and the overlaps between them. The number of active bits is 3. And this is just a toy example.

So, along the diagonal here we have 3 and 0, 1, 2. Just a toy example.

what we're going to start with is a dense representation between Of three objects, and we're going to optimize those, these dense representations into giving us the SDRs that we want. So these dense representations are basically just, float values. And the size of the each dense representation is the size of the SDR that we want. Then what we do, and they're randomly initialized? Yeah, randomly initialized. We're going to use gradient descent to optimize them and, yeah. so we, take, we do the top k of them so that we can binarize into SDRs. And, then we calculate the current overlap between those, SDRs. This is the pairwise overlap. So then we can calculate what the error is between the current overlap that we have from these random initialized gen representations. And the target overlap. So it gives us an error. we cannot, calculate the gradients, we cannot use this error to modify the SDR representations or the dense representations. because this is, it's not a non differentiable function, the top k. so what we do is that, again, as I mentioned to you the other day, we have done that. Yeah, we do have a way of doing that. Yeah, this is not what I, yeah, what I ended up doing here.

what I ended up doing is I basically just moved the dense representations further away or closer, depending on the error that I have here. basically that error Controls the pairwise distance. This distance is in Euclidean space. And basically what this is telling us, is that if I have, if I want the overlap to be higher, I'm just going to minimize the distance between the dense representations and so on. The gradient goes through this, through this, way here and it does not go through the top k. Is this clear? Is the loss just one number or is it a loss for each of the cells and other overlaps? So this is, yeah, this is what it looks like. so the dense representations copy the pairwise, and then the loss is going to be the, so for example, here, basically I, need to make 'em closer, minimize the distance because I want, the overlap. Error is basically the target minus the current. I multiply these values by the pairwise distance and then I mean over them and then backpropagate. So it is, yeah, you have a matrix over all of the pairwise and then you just mean because this is how we backpropagate.

Yeah, so it becomes a summary value at the end, but it's based on element wise, like in order to reduce the overall loss, we're going to improve each element wise value. Yeah, and these are, basically proportional to how much error that we have, if you're older or not. Oh, okay. Okay. yeah, some results. we did some results on Synthetic and some results in Monty. the Synthetic are, nice because they can show, we can do a lot of, things that we can't do in Monty. Testing, the limits of the system.

this is the baseline, we have 10 synthetic objects, and, these are the target overlaps that we want, and, this is a predicted, and we start from here and then basically we just, we were able to match the target overlap that we want by just minimizing this loss. And, it's very nice that we're able to minimize advanced representations, and that gives us, the top, the SDRs that we want with the features.

We did some relations on the SDR size and the sparsity and the number of objects that we can encode it using this. And these are the results.

if I fix the sparsity at 2, so basically the number of active bits in these representations are going to be different, and I fix the sparsity.

and I change the SDR size, we can have, this is the overlap error, the average overlap error for all of these, so basically this lower triangle here. What is the average overlap error between this lower triangle and this one? I'm just trying to say what, error means in this case. error in matching the, target overlap. error in the pairwise. Okay, so error is not error in data recognition, it's just an error in the target overlap. Yeah, so in this part of the presentation it's basically just encoding that target overlap into the Yeah. Could you go back to the previous slide? What's the input to the system? No, the, when you have the setup. What is the input to the system? That, the target overlap. This is what we get from the the, similarities, of course, basically, from here. That's after it's been normalized, to the, to be the SDR, bits that we want. So in the toy example, it's three, but here it's forty one out of two thousand forty eight.

And yeah, that's the only input. It's just trying to find a representation which has these, but does it have to know what object? It is, I'm a little, No, yeah, This is a very under constrained optimization problem, right? We're telling it find me three representations in this space that have these similarities between them. So it doesn't really have to know anything about the objects, we're basically just We just want to encode the similarity in those SDRs. That's it.

it really doesn't really look at the actual objects. we only look at the objects when we're calculating how much similarity should be between them. That the representation is, right?

Okay.

And then when you're done, you will map the object ID to each of those representations? Yeah, so basically we're saving, every graph now has an SDR representation, and we're just saving it, and we're always refining it with more evidence. So that'll be an integer associated with each SDR? a graph, yeah. A unique ID will be associated with each SDR? oh, okay. Or just the graph even, you don't even need to store the integer. It's just whatever the object, it's an object associated with it. Yeah, it says object 23 will have this SDR, object 55 will have this SDR. And at the end of the day, you're going to have these relative overlap scores. Yes, and then we're just going to send them up. You don't even need to say object 23 anymore. You just say, okay, this is the graph. When I recognize this, I will pass up this SDR. is basically the name of this object. Yeah, but what I meant is there's a discrete number of them. Yeah, of SDRs. It doesn't know anything about the properties of the graph. It's not like two similar graphs will have similar representations. It's just if they have, happen to have similar overlaps, target overlaps, then they will have similar representations. Yes, but it happens to be that the similar evidence scores correlates very well with the similar graphs. It's not actually looking at the graph, it's looking at the evidence.

Differences, Yeah, so I guess some totally different way of doing this might be to like, try and put the graphs on top of each other. And, like techniques Ben was exploring back at the start of, Monty and stuff like that. But but, this is really nice because like when we're recognizing objects. The learning module is naturally rotating objects in its mind based on the most likely hypothesis and stuff like that. And so that's when, why we end up with these really nice alignments between like objects that are not just morphologically similar, but morphologically similar when they have like appropriate rotations to align with each other. But if you're learning a completely new object, which has its own graph, you have to do the comparisons with everything else. Yeah. So this is an interesting, thing, like a follow up. Yeah, sorry, maybe Ramy, if you want to talk about it. Yeah, okay, so you're just, it's just finding ten SDRs that have these target overlaps at the end of the day. Okay, that's what we're encoding in the SDRs. And you can't do that with some form of scaler? I guess not. It's, there's a quadratic number of comparisons.

Yeah. Okay. And in my mind, what evidence LM is doing is that it's basically aligning them and then it's moving and testing all of these hypotheses because every, the hypothesis space basically includes all of the rotations and everything. So in my mind this is really what it's doing. It's already been done for evidence matching, so why not just use those scores instead of having to do it again. So we talked a lot about the zeros last week. I don't know how that's handled here. I have a few, a couple of active bits. We're going to talk about the receiving end of this, these SDRs and how is that processed?

I have a, yeah, not in this presentation, but I'm working on it. That, that to me has always been the problem with overlapping SDRs like this. It's not that you can't come up with some way of encoding it, but the way neurons work in recognizing patterns. it's problematic, right? And so you have to think about, in real neurons, okay, now I have these overlapping scores. we're not going to try to recognize these things uniquely because it doesn't work for that, right? what does it work for?

it's you can't have a cake and eat it too. You can't say, oh, these are all similar, but I'm still able to tell the difference because if there's only, if there's 80 bits or 100 bits on, two bits are different. The receiving end really can't tease that apart. It's as much as you can vary the threshold that you match that. And then, but then again, if you have a separate SDR, I think for generalization, like you would have some threshold that you could vary. I don't really care what kind of dinner table I'm eating at. But the generalization part means I don't really want to tease these apart. I want to say these are all the same. It's the same solution that we use for the features that come in from the sensor module. So basically, the receiving end does exactly the same mechanism that we're doing already for features like color or curvature, where we say, okay, these two shades of reds are very similar, so we're not gonna encode them uniquely. We just look at the similarity of these two shades of reds, and if they are similar enough, then both of them add evidence for that model. And we would do the same thing with the object SDRs, or we could basically just calculate the union of those two SDRs, and that gives us a distance between the two, and if the distance is close enough, then we add evidence. If it's too far apart, we don't add evidence that would be the straightforward approach.

In the higher level, I don't think that we really need to distinguish these objects from each other, but that's only in the higher level. If we want to attend to that, this cup is different from this cup. we need to go to the lower level, but the higher level should only be, dealing with recognizing scenes. But I guess, in the back of my mind, I'm thinking about how the neuroscience could possibly do these things and in my model, which could be wrong, it's always been that there's sort of two representations that coming in. You have this mini column representation, which is not very sparse. And, and it's like you're, it's not very large, it's not very sparse. And you're essentially, and that's what we call the, the spatial pooler. It takes a whole bunch of patterns and maps them into a much smaller set of patterns. And therefore, you've got the similarities there. you don't really tease things apart. But then, you have to run it through the sparsification process, which just says, okay, under these different contexts, I really want something unique. So you have to have a unique and a non unique representation.

here you're trying to do the, this is the non unique representation, but you're trying to do it with more objects, is that correct?

So I'm trying to understand that, but it's done in a way we're trying to be able to do it. Oh, it's the morphological, the graph similarities. As dictated by the evidence. So I'm now trying to imagine, okay, I've got region 1 and region 2, or column and region 1 and column and region 2, and I'm passing this information up. we've already said I have to have a unique, version. Yeah, which this is not. And then a non unique version. So I'm just trying to make it all work. I'm trying to make. So the problem with the spatial pooler representations includes just the feature similarities. It doesn't have the graph similarities. So this will have graph similarities. as computed by the evidence algorithm. So it's got whatever the limitations of the evidence algorithm, which does have graph similarities, this will have, be able to encode that. So this is, in that sense, better than the spatial pooler of representation. Yeah, maybe we can have a reading sometime where we talk in detail about where we want to encode similarities and where we want to encode things uniquely and also how we best do that with SDRs and that Ramy present, his slides. Cause I feel like he came up with a really nice algorithm. Okay. A bit of a unrelated, like next. I'm trying to make it work. I'm trying to make it work in my head. Like how to get the whole system to work and this is a piece of it. But I can just let it go and just say, okay, I don't know how the whole system is going to work yet. Yeah, I think that the core of this, if I understand it, once you have a set of pairwise similarities that you want to, how do you create SDRs on them? Which is not an easy problem. So that's what he solved. And then we can use that where it's a technique we can use wherever we want. But I think as a technique, I have to be convinced it's usable. And, I have to understand how neurons will take advantage of it, whether it's really practical or, I don't want to say just because you did it, it's, We're done. That may not be right. it's a good, interesting result. I'm not taking that away from you, but it's okay. It's an, yeah, it's another way of creating similar spatial pooler is one way of creating it with purely based on static features. And here it's based on. A graph of some sort. And I'm just trying to lay it on the neuroscience thing. Where do these things exist and how do you pass both these representations off and how are the neurons going to tease them apart and all that kind of stuff. So what he's, it's not clear from the slide, but the algorithm he implemented is extremely neurologically plausible. I'm not objecting to that. Yeah, and I think we'll keep talking more about this, but I feel like that's a whole that's maybe not so related to Ramy's project. Okay, I'm trying to understand the whole thing.

yeah, so these are just different ablations. I just wanted to make sure that they, it works for different ranges of SDRs, at different sparsities. So at 2 percent here, I'll be at 21 bits, and so on. that's the overlap error in matching the target overlaps. So this is, if we create these representations and then we, try to compute their pairwise overlaps, this is the kind of error that we're looking at. Maybe one average, on average one bit if we're looking at this number of, this size of SDR. Now, of course, because, I, try to normalize this, overlap error by the number of active bits. because I feel it's not fair. it needs to be normalized by, let's say, this representation of 1496. It has 81, active bits.

one I overlap, error is, very little compared to, so when you normalize it, it is what you get. So 1024 is a bit of a high error, high normalized error, and as you go up it seems to be the same.

no matter what, these are tiny numbers. Yeah, this is half a bit in error. 41 to 81. It's pretty small. Yeah, even at the max. It doesn't matter. I was just hoping for this, like basically those, two here. Yeah, but at this scale I'm not sure it matters. Yeah. Yeah. Just, after one bit. and then I changed, I fixed this, the SDRs at 2048 and I changed the sparsity. And this is what it looks like with changing spar. Number of, yeah, number of bits, number. Yeah. that number could be five, it would be, or 10. It would be still fine, right? Varying number of objects. this is the trend that, we're seeing with more objects. So 10, 50, a hundred. Also, again, this overlap error on, the side here. Yeah, just on your point Subutai, if we make it too sparse, if it's five, then we have less chance of encoding a lot of different objects. no, not at all. Perhaps, you were saying we could have five bits of error and it wouldn't matter, right? Not that you were saying we would encode it. Oh, you mean the bits of error? He's talking about overlap error here, right? And so the error of a few bits is nothing. In, out of 41 bits on, or 81 bits on. Yeah. That's what I'm saying. Okay, yeah.

this is what it looks like in Monty. this is the, it's different from this. Because, this here we, have, for synthetic we only, we have the full, target overlap. In Monty we have, basically filling those rows one by one. These are the kind of results that hang out. Yeah, I'm not sure, I think you'll have to explain what's going on here. Yeah, so basically it's the same, it's a target overlap, scores for 77 episodes. Basically we're looking at these objects in the YCB dataset. And we're going through them one by one, and every time we finish an object, we basically add that row to the target overlap. The challenge here is that, it needs to adapt to adding more and more objects every time we finish an episode, right? Because we don't really, we don't see all of them at the same time. I still don't know what's going on here, but maybe it's obvious to everyone else. I think he's learning, you're learning objects in an incremental fashion here. Yes. One at a time, so that if you've only learned three objects, you can only have a three by three matrix of similarities. The rest of them are random. What is prediction and target though? Predictions is the wrong word, but this is the current overlap., this is the current overlap, so this is similar to that. Basically just, go back to the, just, use words predicted, overlaps. I'm sorry.

So the one on the left is, basically showing the SDRs that the algorithm has made and how good they are at clustering. The one on the right is what we want them to look like. And so the good thing about this graph is they look very similar. Okay, you can only do it up to this. Yeah, Yeah. Yeah. There's only a square in here. I don't know if you can see my mouse, but yeah, there's only a square in here. That matters. The stuff to the right and the bottom don't matter. Is that Right? As we see more, like I showed in the previous evidence.

Yeah. Yeah. I don't know if it's the correct word for the left prediction. It's. Actual? yeah, current overlaps, maybe. Current, and this is the target, or, Calculated? Calculated overlaps, maybe, or trained overlaps. Or, object SDR overlaps. algorithm result. Okay. So why'd you call it prediction? yeah, deep learning. Terminology is really Oh, is that right? Yeah, whatever the network outputs, it's usually prediction.

Yeah, but I think in our context prediction is something completely different predictors are.

Yeah, Okay, I got it. No problem. We keep adding to it because every episode yeah, I got it. So when it goes from 22 to 23, it's gone through a bunch of iterations, and settled, and now you're showing the actual This is the error after the bunch of iterations. So basically, it's supposed to stabilize like this after every episode. This is the error what's an episode basically looking. So after we add every each row, that's an episode. So look, doing a sensory motor experiment on one object. On a new object. Every episode is a new object. Yeah. Every episode is a new object. So this is the error after training the encoding, algorithm. To see that, to encode this. Error after each new object. Yeah. Call it that. But this, okay. No, I don't, that's not right. Because the scales don't match between the left and the right. Because the scale is now at 260, 270 objects, and then Oh no, what is the number on the right there? The 409, 425, 435? This? Yeah. this is just a, so I have 77 objects, but I'm showing it, 10, Oh, okay, so divide by 10 is the number of episodes. I'm showing 10, 10 variations per, okay. Yeah.

I have more detailed slides when I presented them to, to the, team. I just copied those visualizations.

This is what it looks like on a t SNE. basically I took the trained object SDRs and then I plotted the t SNE as basically a dimensionality reduction algorithm. It tries to map the representations into a lower dimension while maintaining What does t SNE stand for? How am I going to set it? Is that a standard term? it's a, standard. It's just doing similarities. Trying to project high dimensional similarities to two dimensional. While maintaining the, similarities, yes. It's pretty good stochastic neighbor embedding. Yeah. The T, I think, is for the T schedule. No, It's basically dimensionality reduction in a way that you want to keep similarity. Yeah. So it's not linear, as opposed to PCA.

Yeah, so basically what it's doing is that we're seeing that similar objects are clustered together.

And to your earlier point, things like scale and the use of color and the evidence accumulation and stuff like that could explain why the clustering isn't perfect. And then I guess, yeah, also, trying to cluster quite, Abstract space into just two dimensions is also challenging. So overall, this looks pretty positive. I see a lot of cylinder things that are not all in the upper right there. So I'm just wondering. Yeah, like on the left there's a chef can and on the right there's a tomato soup can, they're basically cans. If you would add a third or fourth dimension, it might look totally different again and they might be similar on one dimension but then different on another dimension like color. so this is done just for visualization for us. Yeah. Okay. So this, is not an actual result, this is just visualizing something that may be perfectly correct. But the visualization still struggles. Yeah, because of 2D. Got it. Okay, so you can't collapse. Yeah, it's like a nice sanity check that it's working well. If this was, if it looked random, if it looked random, then, but yeah, I think ultimately, as Ramy was pointing out, we need to, it doesn't seem like there's the limitations with the algorithm. It's more about, just getting Monty to represent things that are similar, better, being able to address scale. When you do the projection, does it, is it, is there some kind of significance that we try to orthogonalize it or try to find the most significant dimensions when you're doing it, or do you just map it to, the random lowest, lower three dimensions or whatever? No, I just map it to a 2D dimension, and I define the distance function as the overlaps. But am I correct in saying that this is irrelevant? This is just it's nice to see that it looks maybe like it's doing the right job, but the actual overlap is what matters. yeah. But, I don't know if we're expecting. I can take this. I could just take the chef can on the left there and I could actually look at the bits in that and it might, overlap with those other ones up there. Just. In some sense, right? Yeah, it should. Yeah. and this is also an iterative, procedure it like if I sometimes I've seen these, sometimes, some clusters basically just move, snap together. for example, this part over here. with these balls, tennis balls and all that. Sometimes basically it will just, snap to the lemon and all that. And sometimes it separates. It struggles, it uses gradient descent and sometimes it struggles to also map them together. But you're not changing the SDRs underlying when you're doing this, right? No. It's just it's just a visualization technique that's flawed. It's just visualizing it. the ultimate measure of the SDR encoding is the loss between those two matrices that Ramy showed, but that's a bit hard to see. And this is just nice to look at. Okay. All right. All right. All right. actually, this distracted me because the other stuff is a real employee. This is just, yeah, so we don't really know if We extracted the right evidences. We can look at a heat map and we can see, okay, so this object is similar to this object based on the heat map, but we don't really know if we, so this will tell us that the encoding algorithm works and also that the evidences that we extracted work because it means that they will be closer together than random representations. similarities extracted. And I think the first time you made this, actually, it didn't look like this and it helped.

Yeah, I should have. And was that because it wasn't encoding it correctly or because you weren't using the TSNR thing correctly? Yeah, I, yeah, I had a bug in the code and that showed basically how if it's random, it looks completely different. Everything is over the place. What I was wondering about these outliers is that, some of these outliers look like they're mistakes, but they may not be mistakes, right? One thing we could do to check is to actually look at the tennis ball object SDR and the lemon, for example. That's what I just said. I said that you look at the SDR for the chef can and the other cylinders, and you could just literally look at them and see if they're overlapping properly. Yeah, another thing that might be helpful is give a reference frame. So generate a completely random one and show it to us. And be like, this is what the random would look like. Here's what it looks like. So we can at least have a reference frame to something because I have nothing to compare this to. Yeah, I have a figure later on Slack. Yeah, that's it. you could show this before the learning happened before and after.

but there are some obvious examples, for example, the Phillips, screwdriver and the obvious examples of what, of objects that, look similar, or are clustered together. Like for example, these two objects, the knife and the, these utensils, they look similar. toy airplanes, I don't know. These are not toy airplanes., what? They're, parts of a toy airplane. I guess someone in the YCB world would build. But here you have a medium clamp and a large clamp. They're not near each other. I think we can spend too much time looking at this. I don't think it's that useful. It's just like a quick visualization. Yeah. But also, to Neil's point, it doesn't really capture the scale. So this is a medium clamp and a large clamp. And the scale is not captured because of the evidence wave for what we're doing. They were the same size and they would probably be closer together. I don't know how much time, how much more time do we have?

I guess 15 minutes. Yeah. I think it would be good to have a five minute break in between. and what's next? We have our models training me. Oh, I see. I did some experiments on streaming objects. Just basically, Streaming also is not very, yeah, it's just basically adding more objects. So I start with 10 objects and I keep adding 10 and 10 and so on. And then I look at what happens to the error as I train more. This is an experiment to test for, the SDRs are adapting to more and more objects. And this is like what the error looks like. Hoping to see this as we add more, objects to the, target similarity. It goes up and then the optimization brings it back down. So yeah, as expected, Is this with having the first 10 SDRs then frozen or do they keep changing as well? No, in this example they keep changing. These are the results I showed before. I haven't done, I should do these experiments with also stability to see how the error would change.

Did you see how much these original SDRs changed after you added more objects?

no, not really. I know that they change a lot, but the stability PR should help fix that. but the thing is, so I know that the stability PR fixes that, but the thing is, would be to look at the error and see if the error is still manageable because Some of these changes, they need to happen. Some of these SDRs, they need to move because we're adding evidence to either bring them together or, so there's some distribution shift there.

yeah, some extra experiments. I don't know if we should go through these, but basically not giving them enough time to, to train. And this is what it looks like over time. What if, you, you picked like a, was it 2 percent sparsity here? Yeah. What if you picked a much higher Number 10.

What would it do? For the streaming? This whole thing. Take the entire argument you just did here, and you didn't make it that small. You made it less partial. I used 5 percent here. What about 10?

I'm jumping ahead to the problem of forming unique representations. The reason I always thought that these overlaps would not work is because they do not allow you to determine unique representations. The neurons don't, they won't be able to tease apart these different objects. Group them all together and you can't get around that. And you need to be able to form a unique representation. So if I understood from the beginning of the exercise here was to form two types of representations, one that's unique and one that's not unique. And then I would have been much more amenable to this whole approach. I feel bad because I was, I've been arguing with Niels about this isn't going to work. And the reason it's not going to work is you can't tease apart these overlap representations. But now we're saying, no, we're not going to try to tease them apart. We're going to have a separate representation that's very unique. So one way to get between the overlapping SDRs and unique ones is to go sparser. You can start with these, and that's what I'm asking. What if I had a 10 10 percent sparsity for these overlapping SDRs you've just been describing here. But I want to form unique ones by self sampling from those, randomly.

then Yeah, so you can do that to a degree, and, but I, still think You would want to do that alongside having completely random unique SDRs. Cause that won't be as robust. So yeah, I think the way it was in my head is if you had a higher level learning module, it would basically have two matching filters. For each point, like you move to a point, what does it learn there? It's learned a unique SDR from the lower layer. And that's what the very specific object was. That's only for that thing like Numenta mug. And then it's learned like the more generic SDR. Let's say that's from a different layer. I don't know if that's layer two versus three or whatever. That is receiving the input from and then, but that one, I think it's important that you can vary that threshold. depending on the context, so if you, say, oh, I don't really care what dinner table I'm sat at. I just want to be set at a dinner table. You're going to lower that threshold. And as long as you see, okay, this is a cup. the reason I didn't want to go that way is because I don't really have any evidence that there are two sets of neurons projecting up the hierarchy, and you really only have, evidence of one, and, at least from layer three, for example, the paper I just reviewed the other day, they said layer two didn't project much at all. I don't believe them. but so one possibility is you could have, the same population representing both the sparse, the highly sparse, and the less sparse one. and that's easier to do, that's easier to do in a sort of time sequence type of thing, maybe, like in one part of a phase you get the denser one, next part of the phase you get the less dense one. But I think your point was, Niels, as you said, if you did it that way, You just, you're subsampling the denser representations, you'd have more errors is what you're saying, I think. Yeah, it's a trade off. imagine I had 4, 000 neurons and I had 10 percent active, then I have 400 cells that are active. at one point in time, and then I go to a moment later, I have 2 percent because I have 80 cells active. There's a lot of ways you could pick that 80 out of 400 and still uniquely capture it. I think there would be. I'd have to look, work on the numbers. So it's not clear to me it wouldn't work. But, that's, we have that sort of dichotomy. That basic decision is like, is there one set of cells that are doing all this, or are there two populations of cells? And at the moment, I don't have much evidence for two populations. I'm trying to think of some way to make it work with one population. But, maybe a layer 2 does project up, and people just didn't see that. I don't know.

That's why I was asking, would you get the same results, similar results, if you had a less sparse? Yeah, it's not clear how sparse this is. I don't know if you did the active How often each bit is active. Yeah, maybe we should try to wrap up. But in general, I think the higher dimensional you make it, and it's still sparse, it's just going to get easier. I don't think that's going to create a challenge for this approach, is my intuition at least. I think that's consistent with your results with the overlap error and stuff like that. I'm not sure yet. I had a question when I talked to him last time. How many bits are actually on during the entire cycle, what is the duty cycle of each? Yeah, I think that's a good point, but we could add some regularization to get that to be better. Yeah, so we have to do that with a spatial pooler, by the way, we have to add a regularization factor to make sure all the bits are used. Yeah. Yeah. So because we are forcing, some, like specific overlaps, these overlaps, right? Then there is going to be a large populations of dead neurons. And exactly. Yeah. Yeah. And that's just a matter of being, this is by design. you don't want that. Yeah. You don't want that. And it's not a useful, that it's not really an SDR. spatial pooler had the same problem. We had dead bits and we didn't like it. So we put in a, a factor that basically boosts the activity of dead bits, and they don't like to be this is just a function of how many objects you have. If you only have 10 objects, but you have 2, 000 bits, and you want the 10 objects to have some overlaps, you need to have If you have a lot of objects, then you have better utilization, right?

I think that's part of it. But also the other part is do we want to do unions or not? And if we want to do unions, then, you need, I don't think you'd want to. I don't think you could do unions on this type of thing.

I wouldn't assume you wouldn't, I don't know. I think you could, because you could have yeah. It could be maybe a little bit. you can't do unions when you get less dense, right? less sparse. The denser you make it, the unions don't work. Exactly, yeah. so my guess, I'm sitting here going, Hey, could I do this in a single population of cells? These representations would be less dense. You wouldn't, therefore you wouldn't form unions. But the sparser ones, in some sense, if you're sub sampling from this dense, this overlapping population, in some sense, the overlapping population is a union of all the unique odds, right? That's the way to look at it. It's a unique, it's a union of all the unique objects, all the unique cups. I take them and I form a union of them, I get this representation. Unique is, is, the opposite of encoding, or overlaps. This is the problem. You need to both, how do you do both in one population, or do you have two populations? Yeah, for now it's one population, but I still think you could do unions in the sense that, let's say you're just very early in inference, and you don't know if it's a cup, or if it's like, I don't know, something very different, like a ball. there's nothing stopping this from representing the union of those two SDRs in one SDR and passing that up. But of course, the more similarity, the more similar, the representations, you're basically going to converge to the sparse union, what is just, the most likely hypothesis. yeah. And this goes, this has to be counteracted with what Vivian's saying, is if you have objects with similarities between all of them, then that's forcing you into a denser Representation.

One of the ways that this, I don't want to get too far, but one of the ways this probably works in the brain is that you have this very fast inhibition, which is enforcing sparsity. But that is fast, but it still follows on the initial activation. So you could activate a whole bunch of stuff really quickly, and you can have this very dense representation, but very quickly it's going to get sparse. It's a time based thing, right? So you can say, oh yeah, a union real quick, but then quickly go to the, that's how we did the temporal memory work like that, and that's what would happen in neurons, right? they would, they could become active, they all came active at the same time, but then the inhibition would quickly shut them down, most of them.

okay. Okay, anything else? Just, a comparison between synthetic and Monty. So this was a, synthetic, and I had a hunch that in Monty it would be slightly better. Less, dead neurons. What is, I'm not sure I understand these two charts. Yeah. In the left, the previous one, what is the x axis? Sorry, I forgot. This is number of objects, basically. Ten objects. And before, it's before your learning algorithm? Yes. And after your learning algorithm. the blue one is random. And this is basically after learning. here is the same thing, but these are synthetic experiments, basically, synthetic, some random, target similarity. And this is the actual target similarity in Monty, because when we discussed this, we showed that there's a lot of blue, there's a lot of low, target overlap, and then there's just only a few bits of, high overlap, target overlaps. in, in Monty, it will do this, thing where, there's a, there's only a few ones that are very highly similar. Highly similar objects, and there's a lot of low similar objects. So that And this has to be with zeros in the matrix. It's not zeros, but it's low overlap. So low overlap will contribute to less dead neurons, basically, because it's not forcing that high overlap between too many objects. But if every object has to have at least some overlap with every, any other object Yeah, I think it's easy to address this, It can't be more than 80 bits apart, right?

But yeah, if, I think you mentioned maybe to Ramy, Yeah, we could have a threshold where it's like, Okay, if there's a, if they're below a certain, amount of overlap, Then we just enforce that they have no overlap. That's what I'm talking about, the zeros. That would, significantly reduce this, But it's just something we haven't done yet. Yeah, but I'm still confused about this, Even here, if every object, has to have at least one bit of overlap with any other object, then the representations cannot be more than 80 bits. Why, would they have to have overlap? that's what he said. He said there's no zeros in the matrix. It doesn't have to be. There's, what I'm saying is that if there's high overlap, that makes more than neurons, and there's, low overlap, that makes, because they now have to go and take other neurons that were there before. But the furthest two objects can still have to have some overlap.

So you cannot use more than 80 of the bits. But why are there no zeros in the matrix? you can have negative evidence for a lot of the objects. I'm just reacting to it. He said there's no zeros. There could be zeros, but there's The experiment doesn't have zeros. So if in So I don't understand how you could have so many Active neurons.

You understand what I'm saying? It's just, if there are no zeros, and every representation has to have at least one bit of overlap with any other representation, the entire space of neurons that are used cannot be more than about 80.

Am I wrong in my logic? Because, you're using 41 at a time, and so the furthest something could be is another 40. Yeah, there's a hundred objects. Yeah, a hundred object, but every object has to have at least one bit of overlap with every other object.

Okay, maybe my logic may be wrong. Yeah, my logic may be wrong there. I need to think about that. But Ramy, why are there no zeros in the matrix? Wouldn't we have zeros? Especially after Yeah, there could be zeros, but the point that I was trying to say is that Because we have a lot of low, values in the overlaps, then this will spread to, occupy more of the dead neurons, this is what I'm trying to say. but there, there could be zeros when we normalize, there will be at least one zero in a row.

Yeah, we can wrap up here, but, Yeah, go ahead, it's good.