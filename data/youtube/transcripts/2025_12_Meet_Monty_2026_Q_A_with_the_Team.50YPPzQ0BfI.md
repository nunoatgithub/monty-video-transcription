Thousand Brains Project: first question, do you expect that this sensory motor approach can also be useful for cognitive functions? Who would like to.

Jeff Hawkins: Any one of... any one of us can answer that one.

Okay. I'll start, sure. We know that, studying the brain, that the neocortex does all these different things. It does and all the cognitive functions we think about. And yet it has the exact same architecture or very similar everywhere. And that's the starting assumption, the evidence is strongly suggestive of that. that all cognitive functions are built on the same principles. And only very recently did we start really getting to deeply understand that. We haven't discussed it publicly anywhere yet. We knew that if we just started focusing on how learning modules work in a generic sense, like, how can I sense through vision, how can I sense through touch, how can I sense through hearing, and understand the world, that the principles we learned there would apply to high-level cognitive thoughts, and we're now beginning to understand that. So the answer is yes, but we haven't really explained it yet completely. If someone else wants to jump in, they can do that, too.

Thousand Brains Project: Thank you, Jeff. Okay, next question. Is Monty capable of moving through time in time series data?

Viviane Clay: Yeah, I'm just raising my hand now. I guess we can all just raise hands if we... I feel like we have an answer. fundamentally, Monty is always moving through time in the sense that it senses different locations in space over time, but you're probably referring to basic time series data where the progression in time is not anything under Monty's control. And so that's gonna be one of the things that we're gonna work on a lot next year which is what Jeff alluded to, modeling object behaviors, so how things change over time and learning models of how things change over time. So at the moment, Monty's models don't have a representation of time but we're planning to prototype and integrate that in the next year.

Thousand Brains Project: Great, thank you. Okay, next question.

Can inference and training run at the same time as I suspect our brain does? If so, is there a moment where it decides whether it needs to create a new model for a new object?

Viviane Clay: Alright, I'll answer that, because I worked a lot on that part. Yeah, inherently, there isn't really, like you said, in the brain, there isn't really a distinction between learning and inference. We are always learning new things and we're always doing inference all the time. We actually need to do inference to be able to recognize that there's something new that we need to be learning. And Monty can work the same way. We currently, in our configs, you'll see setups for training and evaluation. But the only difference between the two is whether the model, the things that Monty observes during its exploration, will get stored permanently in memory or not. Everything else is exactly identical. And if you do learning Monty will first start doing inference and will start to recognize whatever it is sensing. And if what it is sensing is not matching any of its models, it will create a new model and store all the points it just sensed in that new model. If it recognizes a model, it will just integrate the past observations into that model and fill in the gaps that exist in that model. Make it a bit more filled in, add more details to it. If you run Monty in kind of training mode it will do learning and inference.

Niels Leadholm: Maybe just to add to that, Vivian mentioned that we've had some more ideas on attention recently, and most of these haven't been implemented yet. But, in general, our view is going towards that attention is essential for a lot of things including learning. In general, you don't, certainly at a cortical level, learn much about the world unless you're attending to something. But I just thought I'd mention this because I think in your question you asked about teachers and the importance of that. So although clearly you can learn without a teacher just by exploring the world, in humans this kind of shared attention between either parents and children, or teachers and children, where you both look at something, and then the teacher describes what's being seen, that's really essential to how learning takes place in humans, and you can imagine something similar working in Monty. But that would be bootstrapping and just making the process go quicker. Like Vivian says, you can totally move flexibly back and forth between inference and learning.

Thousand Brains Project: Alright, great. Thank you, Niels. Okay, Agent Rev asks, so far your focus has been mostly centered around vision, along with a lot of discussions about Touch 2. In the documentation in some of your videos, you talked about eventually dealing with abstract concepts, language, world models, logical reasoning, and more. I suspect this might require more types of sensor and learning modules for the other brain regions, e.g. motion, audio, scene mapping, attention, reasoning, prediction, etc. Along with a distributed associative database of sensory SDRs and live global workspace to solve the binding problem. All of it optimized to run a standard workspace. I'm curious to find out how far the theory of columns can be pushed.

Alright, Jeff has raised his hand.

Jeff Hawkins: It's a tough question.

It just relates early about the question about can the column theory apply to cognitive things? And we've just recently, in the last few months, had some further insights in this which I alluded to. So I don't want to get into too much detail about it because it's very speculative. But we accept the fact that all columns are reference frame based, and all columns have a reference frame. The question is what does a reference frame refer to? And the way we currently understand it, they're very much tied to physical things in the world. You even when you learn mathematics, you do it by looking at images of equations and things. When you learn about history you look at pictures of images of the world, or so on.

But an answer here that might go beyond what most people would be able to capture right now, is that as you go up the cortex, and you go higher or higher, which you end up with the representations of objects that are very compositional. They're very objects composed of objects and the power of abstract thinking comes from that, not from the reference frame itself. And so you might even use very simple reference frames and movements to learn mathematics and language. But the features that are being fed into those reference frames are themselves much more abstract. So the abstraction comes less from the reference frame but from the accumulated compositional objects that are being represented in reference frames. I hope that was maybe slightly helpful. I don't know if it was.

Viviane Clay: Yeah, I think maybe just emphasizing that we don't think we need to build custom learning modules for reasoning, or attention, or language or something like that. They will all get fundamental sensory input at the lowest level, and then if you go to higher up in the hierarchy, they get the more compositional features, like the outputs of other learning modules that have learned entire models. And some of the things you mentioned in the question was reasoning and prediction so those things would be things that are going on inside each of the learning modules themselves, and not specialized to one specific region. All of the learning modules are doing prediction, basically.

Thousand Brains Project: what's the largest scale Monty has been trained on in terms of data and compute? Do we have confidence it will scale well in performance, and if so, why?

Niels Leadholm: Yeah, so I'm happy to take this one. So yeah, this is a great question and someone else asked something similar. So in terms of the experiments, for example, that we do in the paper, and just generally, 77 objects at 32 rotations is about the most that we've looked at. Not because we couldn't look at more, but really, as you've probably guessed from the results we were showing, we're really focused on showing what Monty can do, and exploring what Monty can do, given very small amounts of data like what humans can learn from. But in terms of scaling, there's more details in the paper if you're interested, but at learning, If you remember those charts, we could learn on 10,000 objects, and it would still use, because it, sorry, it scales linearly. you could learn Monty on 10,000 objects and it would still use fewer flops than that very short deep learning run. The one that had the same amount of training data and it's just learning 77 objects. So the difference there is huge and we're not concerned. At inference, it's a bit more subtle, because again in the current version of Monty, the amount of flops scales linearly. But really the current implementation of Monty is very naive. We've not, as I mentioned earlier, we've not designed it to be computationally efficient as an aim or a goal, aside from some very basic optimizations. And so there's a huge amount that could be done to make that nonlinear. Everything from sparsity to hierarchy to reuse of models. All these things that we believe the brain's doing and that we eventually want to implement in Monty. Both that learning and inference I think the amount of flops is definitely not our kind of current concern. It's something that should just come naturally from the design of the system.

Jeff Hawkins: I'd like to ask a question, which I'm not sure if the person who proposed the question was, this, but that's how we've talked about scaling to the task, but another way of scaling is how many learning modules have we used at once and sensorimotor integration. Because, in the brain we have 150,000 of these and yet we can do a hell of a lot with one learning modules. And so we spend a lot of time just exploring the capabilities. But another question is, what's our confidence level that we can scale up the number of learning modules to. And I don't know the answer to that question so someone else can answer.

Viviane Clay: On a concrete level we haven't really scaled it beyond 16 learning modules. But it's mostly because we haven't done a lot of hierarchy yet and at some point you can't just add more sensors to a system and still get meaningful input on different parts of the object. But so once we add hierarchy we're gonna run more experiments with more learning modules and since Monty is an inherently very parallelizable system, one thing you can do is you can just use a bunch of CPUs and parallelize all the learning modules across them or other custom hardware that Xavier, for example, one of the university collaborations, is exploring on other types of hardware to scale it to actual thousands of learning modules.

Thousand Brains Project: Great, thank you. Okay, next question. Ray asks do you use spiking neural networks in Monty? If not, do you think that the spiking behavior of biological neurons is relevant to an artificial intelligence in any way?

Niels Leadholm: Yeah, I'm happy to at least start on this one. Yeah, this is another interesting question. For anyone who's been following work associated with Jeff for the last couple decades, first Numenta and then now TBP, you'll know that there's been kind of various levels of biological realism and in the kind of systems that are being built. And this is a struggle we're constantly thinking about and trying to figure out: what is the right level of abstraction? And that includes whether you need to model things like spiking neurons. In general, our approach is to try and find the most abstract level possible that still captures the kind of key computation we want. For example, with HTM and the neurons that were used in HTM, they found this kind of interesting balance between the old kind of point neurons and a kind of overly complex multi-compartmental neuron, if that means anything to you. Trying to get the key role of dendritic branches and what those were doing of putting a neuron into a predictive state. And that's actually something that's still made its way into Monty today, even though we don't have neurons in the same way, we're still capturing this notion of prediction following movement and things like that. So then the question is, what might be the kind of key things that spiking is delivering? And every now and then in our research meetings we talk about things like temporal codes, and how maybe spikes at certain times relative to a background phase might be important, and things like that. But a lot of that is more about how the brain is doing it and doesn't necessarily need to constrain how we implemented Monty. So I guess the answer is that, at the moment, we don't have anything where we think we definitely need it, and given the complexity of training spiking neural networks, it's something we want to avoid if we can. But it's also not something that we've ruled out that there might be elements, at least, of what spiking neurons are doing. Things like STDP, causal learning, some of these kind of key elements that we might take out and integrate into Monty.

Jeff Hawkins: That was a nice explanation, Niels. I think something like SCDP, which is a spike timing-dependent plasticity is maybe critical but you can achieve that without spikes. It's just really a temporal causality that you're capturing. And so we've only put into the theory things we absolutely think have to be there to support all the principles. But Vivian often points out that you can build learning modules any way you want as long as you adhere to the cortical messaging protocol. So if someone wants to come along and say, I'm going to build a learning module built on spiking neurons you can do that. And maybe you'll do something amazing but I think it would be more amazing in terms of performance, power reduction, something like that. As opposed to some new great new capabilities. I don't think that's gonna happen. But it might be a great way of building stuff if you have the right semiconductor substrate and you know how to build that and you might get tremendous efficiencies or something like that. So it's not like we rule it out but I don't think from a principle's point of view, it's essential.

Thousand Brains Project: Alright, great. Thank you so much. Avinash asks, how soon is Monty expected to be capable of language learning?

Viviane Clay: Yeah, I think we have several posts around that on this discourse where we go into more depth of what we think of how language could be modeled in Monty. I think maybe one important point to make is that we don't think language is where a system should start. It's not you start out learning language. A baby comes into the world and it starts out interacting with the world and learning a bunch of different things but language is one of the last things that come on top of everything else. Of learning how to move and walk and interact with toys and all that. And once language comes into Monty, following this basic principle that also columns that learn models of words, for instance, have the same basic structure, the idea is that it will fall out naturally from the other things. It's just based on what kind of environment you place a Monty system. If you place Monty in an environment where it hears language or it reads words, it will learn models of these letters and words and sentences, and it will be able to associate meaning with words, and ground those words in physical reality. But since we haven't really scaled Monty and made it hierarchically deep in the way that would be necessary to get multimodal integration and language associations with meaning we haven't really had a way to test this.

Jeff Hawkins: Just adding on to that a little bit. If you think about language, what it mostly is, is a way of transferring a model I have in my head into your head.

And, so often we express language in terms of the same sort of communication protocols we have in Monty. Oh imagine this, you're at this intersection and when you know there's that store over there, inside of there you'll find the such and such and yesterday we did this and you're basically recreating models in one person's head into another person's head. Once you realize that, it becomes not very mystical at all. But it does require you have these existing models, as Vivian just said, you don't start with language, you start with models of the world. And then once you have models of the world, we can talk about how to communicate them to someone else and I actually think the process is going to turn out to be fairly simple.

Thousand Brains Project: All right, very next question. We have time for a couple more, and then we'll call it, but, Falco asks. Is the hypothesis generating and pruning based on neuroscience? that one? Ramy Mounir: I think I'd try and take a stab at this one. We've already worked out a lot of details about how a single hypothesis in a column, like a hypothesis about orientation, can actually transform the movement that is being sent from the thalamus. We think there are hypotheses about these orientations in the column. But the question here is how many can exist simultaneously in a column? That's a bit of a more complicated question. We've discussed this a lot in our research meetings about how maybe we can have different populations of these representations exist. And we can oscillate between them based on phase, or oscillations, or something like that. And we could also imagine maybe like a fixed-point attractor network where, you know, these can compete together. But it's something that, again, goes back to the abstraction of how biologically plausible should this be and what do we need in our system? We think there are multiple hypotheses. Whether they can all fire together at the same time, or we oscillate between them, that's a kind of a different question.

Jeff Hawkins: I agree with everything Rami just said. I'll just add in the neuroscience, we have a pretty strong hypothesis about one form of keeping these hypotheses going. Which is a union of sparse representations. So you can literally activate multiple hypotheses in the same set of cells at the same time and nobody gets confused. We've shown that, how that works mathematically and there's a lot of biological evidence for it. We don't do it that way in Monty. We could. We don't have to model it that way. We can do it other ways and work just as well. The answer is yeah, it's going on in the neuroscience. It's going on in the biology. We think we know some of the ways it's happening in the biology but we're not actually emulating those. We're just achieving the same result in Monty.

Niels Leadholm: And yeah, maybe just to add as well this point about generating. So Rami was showing some results earlier from some nice work he's done recently, where, in the latest version, not necessarily the version if you just use Monty in most of the configs, but the latest version will expand its hypothesis space dynamically. And that has a lot of similarity to how we think sparsity is playing out in the brain. Where basically when there's a prediction error, when something is unexpected, suddenly lots of neurons are active. And that's something that Monty does in this version that Rami's been working on. Where, you know, if the system's failing to understand what it's seeing, suddenly it bursts this hypothesis space and expands it. So it's just another example of something that has a flavor of the neuroscience without necessarily needing to be implemented to the letter in the same way that the brain would.

Thousand Brains Project: Alright, great, thank you. Okay, two more questions, So Robin asks, how is the sensor module able to infer curvature? Is it analyzing the 2D pixel data, or does it get depth input from the sensor?

Viviane Clay: Yeah, I can do that, it's pretty quick. It does get depth input.

Yeah, we use depth input to both determine where the sensor is in space. Because, for example if you have a camera, the patch itself on the object is not where the camera is, so we need to know the distance to the object. And then plus, from the patch and the depth values within that patch we can calculate the curvature and the surface normal.

Niels Leadholm: Maybe it's worth just adding though, that right now, we're limited if we want to apply Monty in the real world to having some sort of depth camera, like time of flight and things like that. But one of the items on that future work section that everyone was alluding to is to use things like parallax, both motion and binocular, to extract depth, because this is more similar to how the brain does it. If that's something you have familiarity with and would be interested in working on that would be really cool.

Viviane Clay: It also depends on what modality you get. For example, for the ultrasound project we did, it wasn't a depth camera itself, it was the ultrasound image. Which includes information about how far the probe penetrates into whatever it's scanning. Basically, you can write custom sensor modules however you like and connect them to whatever particular sensor you have. And the sensor modules can extract whatever features you want them to extract. And they can extract them however you want them to be extracted. The important thing is that sensor modules need to be able to extract where in space they are, and what orientation they're sensing there. And that's part of the cortical messaging protocol so that the learning module can then infer how it has moved from the previous step.

Thousand Brains Project: Great, thank you. Okay, last question, it's a good one, it's a big one. Do you believe that Monty would be able to achieve knowledge generalization and creativity without modeling hippocampus and neuromodulators as reward signals?

Jeff Hawkins: Okay, so there's a bunch of things in there. First of all, we believe we have to model the hippocampus. And one of the ways you have to, not just for this creativity issue, but we have to model the concept of attention and fast learning that occurs in the hippocampus. So we've been talking a lot about that lately. The good news is that a lot of people believe that the neocortex is a derived structure from the hippocampus, so there's analogous processes to cortical columns in the hippocampus. So we don't have to introduce new fundamental features, we just have to introduce new concepts of fast learning and slow learning, and temporary learning, which it all fits within the current framework. The neuromodulator one is interesting. There has to be something that tells the system to learn or not learn, all right, and it could be as simple as a switch that says "we want to learn, learn everything." Or you might say, "oh, we want to have value-based learning, learn the only things that have certain types of values." I think when we think about neuromodulators that's really what they're about. They're mostly about deciding when to learn. They get involved when we're trying to do, understanding causal relationships in the world. Oh, these things led up to this desirable event, like reinforcement learning. So we're gonna model some of that. But I think the idea that you can generalize knowledge and they don't really require that. We're going to have those in the system. But the system basically learns models. They can generate some knowledge from those models. But we will have to have at least some sort of equivalent to neuromodulators decide on an application-by-application basis, what do we want this system to care about? How much do we want it to learn? Do we want it to be fixed? Do we want it to learn all the time? Should it learn everything, or just certain things? Those are kind of application-specific problems. They're less about theory, overall theory. A little bit of a complicated answer to your question but it's a great question and we think about these things a lot.