Okay, so yeah, thanks a lot Scott and Hojay for volunteering to, to present today on such short notice.

so yeah, the two of them have been going into the literature for the past days, looking at what are other approaches people have been taking for this kind of object and pose recognition task that Monty is currently doing. So that we have some kind of baselines that we compare it to when we write up a paper about Monty's capabilities.

so we can talk about that now. And then if, that doesn't take up the whole time, there are also a bunch of questions people have been sending in that we can talk about more, or we can do that next week.

Yeah, then I'll give it over to you, too. Sounds good. Okay, so we'll put together some slides for, that will primarily go into some state of the art methods for pose estimation, and I'll get to why we're focusing on this first.

so About a week ago, Scott, me, Vivian and Nils had a research kind of brainstorming meeting on how to demonstrate capabilities for a paper that we can write up by the end of the year. So some of the starting point was, these kind of broad objectives like, demonstrate rapid learning for computer vision, whether that be from number of images, or, total data that's being collected. Being used by Monty rapid inference with mobile sensors, improved slash robust recognition and multimodal transfer. We came up with additional things that we may be able to show, but I don't think these are current or not fully implemented in Monty yet. But, some other ideas that were suggested were generative or predictive reconstruction of input. So if we're seeing part of an object, if Monty can reconstruct the rest, and continual learning. Can I just make a comment on that? I think, predicting the rest is the right thing to do. Reconstructing doesn't seem right to me. there's no place in the brain where images are reconstructed.

we're not good at that, but we are good at predicting what will be someplace and seeing that. So as long as it's, it's I don't think our system is going to be good at generative models. Humans aren't good at that. so I think the, predictive, ability of money should be really good. Yeah. Yeah. That's what I said too. And I think, like you said, it should be really good at predicting using its models to predict, but if it's supposed to be like photo realistic. generation, for example, we would need to train some kind of decoder on top of that, which might actually also be pretty good because it gets better information to use for this kind of photorealistic reconstruction. So it wouldn't be too difficult, I think, to add as a, like another readout layer, but it's definitely not what Monty is. That's a good, that's a good way to put it. so we don't have to give up on the idea completely. Yeah. it could be a flashy demo or something because people are really into generative AI right now. But yeah, it's not really like the, I think what we have to be careful of is, not falling to the trap of trying to do what deep learning models do. Yeah, in trying to match them because probably we won't match them. we're not but I like the idea that could be an add on piece, but I would make it clear that brain, brains and money is not a generative system per se. And, certainly for images. And, humans aren't good at this. We can't draw things. We can't, we actually can't picture things with detail in our heads. but as an add on piece, oh, there could be a feed into a, feeding into another system that does generative modeling. That should be good. Yeah. Yeah. I don't think we'll include this part in the paper that we're planning on. And then, yeah, on the second part, the continual learning, I would rename this to unsupervised learning because we're already doing continual learning all the time, like for number four. Point A, for example, this is all continual learning. Like you see the object from one angle and you build a model and you see it from another angle and you add points to the model and you see a new object and you learn about that object without forgetting about that other object. But, what we call learning from scratch and the benchmarks, the big thing there compared to the Current default setup is that we don't provide any labels to the system. More balloons. And we don't make assumptions about the order of the data being presented either. Yeah, Yeah. Yeah. That's, something that's different to deep learning. Like we can present the. Objects in any order. We can learn one object from all angles first and then the next one and then or you can move your finger over an object one direction and learn a model and going over another direction, learn a model is it's really a motor based system. Okay.

And is there a reason that A and C are bold? Yeah, because, from these four milestones, the way that we went in deep into post estimation was, we wanted to at least, find on a common basis something, compare Monty against, so that, one is order, we wanted to have, a task that people in the deep learning world does understand that Monty can also do, and also, I realized that task is, highly. related to robust recognition.

that's why it's bolded.

so I tried to put this in the deep learning, lingo of current moth capabilities. so I think what with current currently implemented, we can do option recognition and something called six degrees of freedom post estimation and some other future mark capabilities that is that may be able to do in the CV. Lingo is 3D reconstruction. Again, just put the word prediction instead of reconstruction. Yeah. Yeah. And, and some other, possibly like robotic grasping and manipulation, semantic and instant segmentation. But these are not yet implemented, but possibly maybe compare these tasks may be compared against in the future.

Because the post estimation subsumes object recognition or often involves object recognition, we, so a lot of the models, if it's doing post estimation, can already do recognition or sometimes segmentation, depending on how it's implemented, which is why I focus on, okay, let's dig into existing models for, six degrees of freedom post estimation that's how we went from the milestones into this very specific task because otherwise it just looks like we're doing some random literature beyond post estimation.

I think, again, we need to be doing is we need to lay out the basic capabilities of a sensor motor system and then start showing that we're tackling each component of those, and as opposed to being best of class or anything like that. I'm a little bit concerned about the robotic grasping and manipulation. That is, a very robotics point of view. it seems to me we need, what we really need is a, essentially, a motor output theory for Monty. A more higher level theory about how columns and learning modules, can move their sensors. in a logical way to achieve, achieve goals and that may or may not be a robotic graspment. It could be a lot of things. So I just we don't want to get down the I think that would come after we understand a broader theory of motor generation.

and we shouldn't focus on that particular type of thing right now. Yeah. So this is in the future capability. So that those three, I don't think they will be, I'm very sure that they won't be included in this. Yeah, I got it. But I'm just saying, even in, I'm trying to correct potential miss thinking about these things right now. but then also motor behavior will be like one of the prime applications of Monty eventually, I think. So we need to generate a general theory of motor behavior in a sensory motor learning system. Yeah. And then we can implement that theory in different ways. One might be robotic grasping manipulation. But, we don't want to go there first. We need this general theory. Yeah. Great.

but for now, precision is very highly detailed, but post estimation kind of one deep, pretty much all, like all the model talk we're actually going to do is going to be about object recognition and post estimation. So I think we're just outlining that. Yeah. so just, to give a quick. more context, I think, so this is not to, say, Oh, these papers do exactly what we're doing. As far as I know, I don't think you found anything that does what we're doing. It's more like we're going to write a paper. We want to show results from Monty. And it's always good to have something to compare to, to show how this is really good, because I assume a lot of these systems will need a lot more data or. have other drawbacks that Monty does better. So we just want to know what's out there and what can we compare to at least on some of the dimensions, but it's not like there will be any like perfect comparison here. Yeah. I don't think there'll be any like perfect comparison at all. And, I will go into how post estimation is, there's different ways to tackle it and how for each of them we can. Try to relate it back to Monty and how we can compare that to Monty in some similar dimension that Vivian mentioned. the six DOF or degrees of freedom. Are we talking about the pose of the object to the sensor? Is that what we're doing? Yes. Okay. Yeah. Or just as well as the pose of the sensor to the object. Those are flip sides of the same thing. Yes. Yeah.

yeah. So when you just say pose estimation, there's actually two types. Like one is I think the more common one, honestly, is human post estimation because that's very important for, cars not to hit people and whatnot. but, another kind of category of post estimation is object post recognition, post estimation, which, Is generally about estimating these six variables, translations along X, Y, Z and rotations along X, Y, Z with respect to the camera or monty agent or sensor, whatever you have to call it. So we first, didn't, at least I first dug into the data sets and after talking to them on Monday, I think there's some. categories of datasets that we have preferences for. So this is the preference from top to bottom. So we want datasets in order to train Monty or deep learning models, whatever, something that contains 3D meshes. So that, it, so that it's something that Monty can interact with. That sounds reasonable. the next preferred data set is, Maybe has point clouds, but not the mesh. In that case, from the point clouds, we can we may be able to get the mesh, but it's not. It's an additional step. And then the vast majority of data sets out there are actually RGB or RGB images, which is not really compatible with Monty. Exactly. It's it will probably have to do much more processing. Pre processing like extracting the point cloud from the RGBD and then from the point cloud, get the, do a surface reconstruction to get the mesh and whatnot. in general, Monty could learn from RGBD images, but the thing is you would need to know the movement between two images. so if we would have the exact pose of the camera when it sees the image and then another pose of the camera when it sees. the object, then, we could use that. We wouldn't have active movement that Monty determines, but we could somehow learn from it. And then just to clarify on the reference for 3D meshes, it's not like we're giving the meshes themselves to Monty. But we're using them to render them in an environment which is then sensed by the sensors. for example, we could also just learn in the real world. So we only need meshes when we learn in simulation because we need something to render it. But we could also just let Monty have an actual sensor and sense actual objects in the world.

the good thing is, I think there's still plenty of, 3D mesh datasets already that exist. The three most common being LineMod, YCB Video, which is similar, it's not the same as the YCB that we have in Habitat, but a video version, and something called TLS, which is, Sans for textualist objects, something like that. And for whatever reason, YCB video is in a lot of these models. Yeah. As opposed to regular YCB. I don't know if that's because they're emphasizing pose tracking as well on a lot of these models because they're trying to maintain a stable. Estimation throughout time, but yeah, it's a, for a lot of the papers, we didn't actually see benchmark against YCB, but YCB video other common things I've seen was line mod. So these three, I think, are definitely, I think line mod is the OG kind of the longest existing benchmark data set and text TLS is often used when there isn't like significant features. Features. and so this is a kind of particular use case where, so texture list would be something like a baking sheet where it's just flat, all gray, so when you move around, there isn't a particular edge that you can extract out from, and those are often for deep learning models harder to do post estimation for. Why wouldn't there be an edge on a baking sheet? I'm missing that. there would be an like, for the majority of the, let's say image of a baking sheet, but it's a large pan inside the baking sheet is just gray flats. And it's not really changing. But if if I'm thinking of my fingers running across the baking sheet, when I get to the edge, I feel an edge, right? that would be that's captured in the 3D mesh. Yeah. Yeah. Yeah.

it's just, this is more maybe this is closer to our idea of a morphology class of objects, right? Where you're, not paying attention to other features other than just this surface morphology. Is that would be correct? to Monty, Monty will be, I think would do fine on TLS. It's just deep learning models will not do well, does not typically do well in textureless objects. That's all. Okay. Could be interesting to test on that and compare. Yeah, in some sense, it's getting at the, one of the core capabilities we think brains are doing. We see, in some modalities, you, maybe all of them, you're able to detect morphology of objects and features disappear.

it's just right. So it just seems like it would be testing that exactly.

It's like vision without color and, touch without texture or heat or, things like that.

Sure. Probably I will, I'm not going to go into all of that, but there's a lot more than these with, some of them I heard are already implemented, like shape net and model net. But from what I see, they don't have, Let's see, not this one, but, they don't have, pose annotations. So I'm not sure how, it was used in multi before, but, they don't need pose annotations. We just initialize them in an environment, like in a 3d rendering engine, and then we can just ourselves. We can just set how we want to initialize them in this environment.

so that kind of people kind of summarizing the different, data sets to exist. We need to dig into them. So how, I'll dig into how post estimation is done. So the good news, or bad news, if you want to compare exactly, but I think it's good news. So there's no existing model that I could find or Scott could find that does post estimation like Monty, which is I'm thinking about it as like a, model or agent that can explore and, and touch or sense something and then try to estimate the pose of it. So even I did find some reinforcement learning based models, but these are still not. it, just to be clear, in the brain and in Monty. You, don't really do pose estimation independent of object recognition, right? Those things all happen at the same time. Three things, as you explore, there are three separate things that are resolved. One is what is the object you're on, where you are on the object, and the pose of the sensor to the object. I just want to be clear. You can't really separate those three out. So when we talk about pose estimation, we have to also in a money setting. We have to also be talking about, inferring the object underlying it. You can't do a pose unless you know what the object is and where you're on the object. Does that make sense? Yeah. Okay.

So there's several ways, that, I think also the existing models also does object recognition. So yeah, these, I think these problems are.

They have to be, in some sense, there's no way to do pose estimation, unless you know what fingers you're looking at, you're sensing. So the reinforcement learning methods that you found. How are they different? Like, why, do they not learn through moving along the object? so they, at least, so I found, I'll get to that when I talk about iterative refinement, but they require, They don't use reinforcement learning for the sensing part, they use it for the inference. Okay, I see.

yeah, I'll, get to that point when I get to, iterative refinement. So there's four different methods, four different ways that post estimation can be done. Regression, keypoint detection, template matching, and iterative refinement. So direct regression is your brute force, you feed in your RGB or RGBD data, and you just try to regress, the, six, parameters that define the pose.

this probably the, this is like the most deep learning brute force y way of doing it. There's, we just let the black box learn, just to, function from the RGBD to, the six parameters. how they might be compared to Monty, that, this is just like a, Thought, but, maybe we could possibly like. So when we're exploring with Monty, we sense color, in terms of HSV. and we may be able to extract that and feed that into the deep learning models to train. I'm not sure if deep learning models can even learn, rotation and pose at this, with this kind of small number of patch data at TCs, but, it's just a way that I thought, that could be comparable, trying to put them into the same dimension between Monty and these kinds of models.

The key point method is where the deep learning. So instead of directly predicting the pose, it tries to extract interesting points like edges or, or places where it changes features. and then. It tries to match those key points to an existing 3D model, key points in the 3D model. So this one requires a, 3D CAD model. and then if you have the 2D, key points and the 3D key points matched, there are some algorithms, one called Perspective Endpoint that you can solve for the pose. So This seems closer to what we're doing with Monty, is it not? in some sense you can imagine, in Monty, you don't sense the whole ducky at the same time. Yeah. You would, be moving. You'd have to start with one learning module that would move over and detect different points on the object. And through that sequence of points, it would infer the object and pose. So that seems like something that they're doing here. They're creating a series of. And then I don't know how they do it, but they got a series of points. And then there's, that together helped, them define the pose and the object itself. Is that, correct? Yeah. So the, features, yeah, I think the difference is that this system gets the entire image at once and then uses the whole image to detect those key points. Whereas we're getting a small patch that. Just moves along the object and uses whatever points it sees there. But yeah, it's similar in the sense that we also have kind of 3d models in the background that we match these points that we sense too. We have a 3d model and we're taking a series of points until we get enough points to infer the object, pose and ID and where we are in the object. Yeah. This is also making me think a lot of SLAM. Which is slightly different scenario where you're trying to locate yourself, not an object in the world, but they use key points a lot in that. Yeah, but you can use SLAM to map an object as well in the environment, and you can compare that to Monty directly, I think. SLAM is actually in concept a very biologically, it's something brains do. the details of how they do it might be different, but that's the closest existing, research area out there that's close to what I think Monty's doing.

How about training those models? do they, just get point clouds or meshes directly? And then, or can you actually get them to learn from observations? Yeah, I think they use a sensor, like a LIDAR sensor. Yeah. for the learning procedure? Yeah, but anyway, so for the key point detections, these work pretty well, but, they don't, I think this is probably the most common way that I've seen, but they don't do well in occlusions. Again, if you have occlusions in the image, it can't, extract at the key point where the object is hidden, textualist objects again by textualist. Okay. it doesn't have interesting features, so we won't have enough key points to match between 2D and 3D, and symmetric objects, so for this, it, we may be able to, I guess one way is that we can maybe just use this as is, but in settings where, we have these occlusions or textless objects or whatever, And, kind of show robust recognition compared to existing, key point detection methods.

In this case, there's, they process the entire image at once. So how does it do segmentation?

segmentation.

Yeah. Does it first assign these key points to the different objects on the table?

I think it depends on the, so if it's a, so I think there's some literature that. Assume a world uses only single object on scenes, which is like a simple in that case, I will do segmentation. I think the others try to do some insects imitation first. This is a general overview, but I think many models do segmentation as part of this key point detection.

it's a mystery, you go from this third image to the fourth image, okay, how did that happen?

so another third method is template matching, which I thought was similar to Monty's graph matching. This one is where You have some. We definitely need 3D CAD models, but from the model, we tried to basically take 2D projections at various angles and try to match the input image to, the 2D projections of the 3D. So if I had like a object, then I can take a bunch of pictures, at different angles or different poses or different projections or whatever you want to call it.

and then try to, compare two images. So once there's like a good match, then, you would know from which image that it matched, like what the pose was of that object. So I thought this was felt like graph matching.

But in the sense that we show the object in a couple of different poses and then we recognize it in any pose or I think in the sense that when we see like a single patch and we later try to construct the object, we need to match the pose. of the object in that in the memory. the object in memory doesn't really have a pose. We, just have a hypothesis of how the object we're sensing is rotated. And we use that to rotate our input and then match it to the 3D model that we have in memory. Yeah. And so in this case, it's not that our template is, We have a bunch of instances of template, but whether we're moving the object inside our head or input, for me, I think they're the same things. which one are you rotating? you don't yeah, it's not really multiple templates. It's just one three dimensional model that we have.

so in that sense we have to avoid thinking along those lines.

But so is this method able to interpolate? So if the object is in a new pose now that is not in the templates, can it recognize that pose?

Or will it just match to the nearest template?

Oh, sure. Yeah, that's a good question.

Yeah.

This seems, less similar to Monty than the previous, key feature method.

Yeah.

Yeah. do you know how it's trained?

It's trained on the CAD model plus a bunch of views, right? Yeah. So the one nice thing about it, is that if we wanted to train one of these models, we have, We can generate all those observations from different angles in the habitat and we have the models. So in this method, so there is actually a longer training procedure involved. It's not just like you have these 2D views of the object and you're doing like a kernel operation over the image and see if it matches anywhere.

do they actually train on like thousands of images? before this works, or can this kind of work out of the box with just a few images? I definitely need to be trying to put in my speaking, because, so again, this one would definitely involves segmentation because the models would exist without any background or seen. so I think, one of the key aspects. of these models is to do segmentation, remove the background and try to learn to minimize whatever kind of difference with you, difference function that you said. So for example, the models might not have like the colors on them. So how basically like it will still need to learn to minimize a loss function, whether it's a structured loss or whatever.

To, yeah, to say, oh, this 2D image matches, this point of view of a 3D model.

Is this still, a state of the art model? I think this one, in general, because there can be so much, it takes a long time. Because there can be so many poses that the object can be in.

I don't think the sodas that you found are template matching based, right? Maybe this multi view convolutional neural network. Okay, but, okay. That's also an older one. It's almost 10 years old now. So I don't know if that would be considered state of the art or not. Probably not.

And then, the final way to do pose estimation is, iterative refinement. So this one, starts with the initial hypothesis of where the pose, what the pose could be. And then, so this one requires an initial pose, hypothesis. So that can come from direct regression or any other previous methods, and then try to rotate the target or the object until they match. Yeah, this is, these are all parallel vision methods here, right? they're not really sensory motor type of systems. Yeah, It's not money like. Yeah, there is, I don't, think there is a sensory model like that. Yeah, I know, that's our strength, actually.

we're, yeah, okay, let's leave it at that. I'm just trying to remember, are we, looking at these different methods, proposed estimation? as ways we might implement it in Monty, or are we using it ways for, testing Monty, or are we using it ways for comparing Monty to other systems? It's just for the last one. So we're not thinking of using this method, any of these methods, to implement in Monty. It's more, we want to show the capabilities of Monty, and to do that, we should compare it to the current state of the art in the field. Yeah, I think Monty is already doing better, so we don't need these methods. but we need to find some common ground to compare them, I think. So that's why I was looking into the kind of different methods and what are the common grounds that we can say, Yeah, anyway, so the reinforcement learning methods are under these ones. this one kind of, they pose a problem as, navigating through like a hypothesis space. So instead of trying out a bunch of different rotations and trying to randomly match it, the RL method just learns a better action policy to rotate or like move the object in a, in a more sensible, or in a more computationally efficient manner.

but they're not actually, navigating or sensing, by themselves. It's just a method to speed this up. Because this is usually the thing, after template matching, this is the slowest one. And often, post estimation, there's a huge goal to make it fast, aka real time for, again, for autonomous cars. for those applications, iterative refinement, Isn't used because it just takes it improves accuracy, but at a huge computational cost. So that's why some of these like oral base or other methods exist to, make this shorter because we want to have both, real time tracking and high accuracy. So wherever this, method here, is there a model of the object?

it's, not clear. Is it, just, I think they just give the raw ground truth cat models and then put that pose into the renderer. Yeah, So you would have some initial pose for your object, right? So think of that as like a point cloud or mesh, that would, let's say one is flat and, there's an object that's flat on the table. and then, we have an object in the cat model that's upright. Then the object on the table, it will try to move around until it matches the kind of orientation. So the CAD model has a sort of preferred pose, and then it's, it tries. Actually, yeah, I think they actually mostly try to find that the relative pose between the CAD model. And your, existing. Yeah, I think I get the, what do you mean with your question, Jeff? Cause like, where's the image going in here? Like the image where we're trying to detect the pose. Where does that come from? so in this case, the, usually the input is like a point cloud. and that point cloud is coming from posts that was estimated from images. Oh, so there's another preprocessing step. Yeah, because, we need to have some kind of initial guess. So that, that often involves, direct regression. So it often comes from an image. We have a, some kind of post estimate, initial post estimate, and then we can use that post estimate. To, convert the 2D to the 3D, and then from there, we try to rotate the 3D object to match the CAD model.

this one's a little confusing to me because it just seems like it's, unless it's doing all those CAD models in parallel, it does them all one at a time.

from the initial, I guess there will be an object that was identified, Again, so we already know what the object is. Yeah, and now, okay, it just I'm looking at the little flow diagram there and there's nothing in the flow diagram that explains what the model of the object is. It's just this, initial pose, change the pose, initial. It's pose of what, I don't need to get into the details unless it's something we're going to really rely on. Yeah, but I think this one probably isn't. Yeah.

So, I have a question. I'm sorry. I was, my zoom was cutting in and out, so I missed part of the discussion. But, did you cover, ransack methods for pose estimation? Yes, I didn't say it here, but, when we do, when, during perspective and points, it's ransack to not to generate possible pose hypotheses. Okay. Okay. Yeah, that I hadn't seen that one particular image where they were going from features to pose. Got it. Yeah, sorry. I guess I copied these diagrams from a review paper.

Okay, thank you. Yeah, but yeah. which of these methods would you say is the best state of the art at the moment? For me, it's keypoint. I think that's Okay. Yeah. So you said there was nothing out there that uses the same kind of inference regime where you have a sensor that moves over the object, right? Yes. Yeah. Okay. Let me ask a sort of a higher level question. So in Monty in the brain, we have learning modules, each learning module of course can do this independently of all the other learning modules, but of course they also vote on this as well. So that's why we were able to do flash inference.

Are we going to try to demonstrate both of those initially, or are we going to just try to show how a single learning module does this? Both. So the voting is, I think it was. 0. 2 of the capabilities that we want to demonstrate. yeah. If I remember. Yeah. Rapid inference with multiple sensors. Okay. So that multiple sensors includes multiple vision patches or multiple sensors. Yeah. Yeah. that's even I got fooled by that because. Maybe if it said multiple sensor patches, that would be clearer. I took that to mean, and I think most people take that to mean oh, vision and touch or vision and hearing or something. Oh, that would be B, a multimodal transfer, Okay, maybe we could just change B to say multiple sensor patches. Yeah, I think I also confused that, Jeff, when I was first going through it. it is a general rule here.

Monty works very differently than almost every other kind of learning system out there, and we need to be demonstrating that we're making progress on those components of how Monty works. And that's more important than showing that Monty is better at other systems. that's a losing game initially, we, we, don't want to go there. We really want to say, hey, it's some unique capabilities. We're starting to show how they work. it's almost like illustrating and teaching people how these systems, how Monty and the brain actually do these things, which is more important than actually saying, oh, we'd be, we're better at this benchmark than these people are. So I just want to make clear that. To me, it'd be more important to say, oh, let's talk about how a single learning module does this. Let's talk about how they vote to reach a consensus more quickly, and demonstrate that we can do that. I'm not against comparing it to other systems, but we have to be really careful. If you compare to other systems, you're not better than the machine learning world will just ignore you. They don't really care how you do it. They'll just say, on that benchmark, you're not as good or you're about par, so who cares type of thing, you know what I'm saying? We don't want to, we won't, we don't want to give people that view. We want them to say, hey, this thing works differently. And look how cool it is that it works. That kind of thing. Yeah. but one, I think we, we are, we would beat all of these. approaches if we, make it with the same amount of training data, like they get as much training data as Monty gets, they would probably all fail completely. But now seeing that you didn't find anything that even works closely to how Monty does inference. I wonder if it would make more sense for us to just put out new benchmarks. We could ourselves try to train a reinforcement learning agent on it, or something that we think would be best to transform a model or something, but then just put it out there, say humans can do this. Our system is very good at it. Try to solve it in a better way. Kind of thing. Yeah.

you could do that in the form of a demo.

Yeah.

because, that, then that gets people intrigued with, how are you able to accomplish it? Rather than trying to go against you Oh, here's a thousand object library and, here's how we stack up against something else. But if you can, if the demo illustrates unique features of Monty, that might serve as a teaser for people to get more interested.

Yeah. Yeah. And the demo could be something like a benchmark as well. Not in the sense as it could be still be something cool, but in the sense of try to solve this task in a better way than we do. Yeah. I wouldn't want to make that challenge. I think what we would want to do is we want to show how the system works. First, that is the most important thing. How does this system work? Single learning module can do it through inference over time movement. Multiple learning modules can vote to do this more rapidly and then characterize that. maybe we do some sort of capacity testing or, performance testing on just for ourselves for starters, And then after you've done all that, you could then compare it to other systems.

but really the goal here is not to get people to say, Oh, this is better than system X. The goal here is to educate people that there's a whole different way of doing these things. And, I think in some sense, we're going to have fans who just have bought into the whole thousand brains theory. And those are our, audience here, the audience of people who, basically think like deep learning can do everything. we don't, our goal isn't to try to convince them, at this point. we have to stay away from that, I think. So as long as we focus on, hey, this is how Monty does it. These are the capabilities of it. This, it's, it's, limits and potential. Oh, by the way, if we, and if we can say, it's better than other systems, but that can't be the main focus, because then someone will come along and try to do something different with another system. I don't know. we don't want to get stuck in this benchmark hell.

We also need some screen sharing session to creatively show Monty, or maybe not even creatively, but show Monty capabilities and, not just about how not just. Focus it on how it performs, but how it's a different paradigm for doing it. But so yeah, that, that is, it'll be a while before Monty is beating other systems at various benchmarks. maybe I'm being too pessimistic on that, but. I view that as not our goal. Our goal is to really just continually talk about this is the way the brain does it. This is a different way of doing it. This is one that's built on movement and, movement and sensation, and that's unique. And here are the components and we're making good progress on them. And if we can then say, Oh, by the way, it's, it's performing state of the art. That's okay. But we don't want that to be the main goal. Yeah, so I would characterize that as focusing on qualitative differences rather than quantitative differences. Yeah. here's how I'm thinking of it. So we already have a lot of written up on how the system works, what the principles are, how, we do these things. And I was planning to turn all of that into a white paper that we are already going to put on archive pretty soon. So like once the project is open source, people can really read about how does it work. But people will ask. What can it actually do in what cases would I want to use the system? What are the capabilities? And there we want to show okay, it can learn from very little data. So if you have an application where you don't have internet scale data sets, you might want to consider this. All right. I agree with that. That's the right approach. It's more like the unique capabilities and where might that be useful as opposed to comparing to other systems that, some sort of performance benchmark, you know what I'm saying? it's got to be careful about that. So yeah, and yeah, we don't need to compare to a bunch of other approaches. This is just an initial literature review to see what is out there. Are there similar approaches that we should look at? it doesn't look like there is an approach that uses the same kind of data to infer from, so it might be difficult to even do a fair comparison. But Yeah. It would be nice to at least have a show where we take the exact same data that we give to Monty and then we give it to a transformer or reinforcement learning agent. And it just fails because it's not enough data, for example. That would be nice. It'd be nice to do that. but I don't think it's an absolute requirement. So if we can do it, that'd be great.

Sounds good.

Yeah, I think it's gonna be hard to find a 1 to 1 comparison. let's say we even trained a transformer with the data we collected from Habitat or something or 77 objects, we'd still need to feed that transformer a whole image, like what a viewfinder gets unless we want, because there's no, architectures that we know of that are going to take it. Image patches and try to detect an object, from image patches. So we could give it a series of image patches, plus the movement or the change in location and orientation over time. With transformers, I was thinking like patch views, like the small patches that it takes for Monty to get to recognize. So it takes a thousand steps and it'll be a thousand patches. And the positional embedding can be a difference in location, but it is that we're designing our own transformer network to do this. Like we're, yeah, those aren't preexisting models. if I understand it, why? Yeah. I'm not saying let's not do it or whatever, but. to my knowledge, there aren't any pre built systems out there that are going to work in these patches. Yeah, there's no, yeah. If you look at vision transformers, they actually break the image up into patches, just as part of the training with encoding of relative positions. it's not totally divorced from what you're talking about. Yeah. Yeah. Okay.

we just need to figure out how to get our patches. They just realize an image, right? They actually, they actually break it on up into, each patch is an input that is encoded. It's in a way you could think of serialized, but they try to keep some of the two dimensional aspect of it. Yeah, so the transformer doesn't have to learn the whole serial to 2D transformation.

it's, using the word patches, but it's, somewhat different. it's, a subdivided image, so it's, not, they're not free floating patches, at least the way it's conventionally trained, doesn't mean to say that you couldn't try to do something like that. So I'm just trying to sit. Some space in between we're looking at totally transformers and we're trying to look at Monte, vision transformers have an intermediate aspect to it, which is not totally divorced from breaking the image up into patches.

Yeah. Could you, in theory, then just let's say you want to just. Simulate having looked at three image patches. Can you just mask out? Yeah, I think basically they tend to subdivide the image, but I don't think there's any reason as long as you have some encoding of the roles of relationships between them of having patches that overlap. Or in some other aspect, so you could gradually move from a fully, rectilinear, Cartesian orientation for the patches, maybe the things that were starting to get looser. But, at this point, I'm speculating because I can't point to a paper where they actually did that. I'm just saying, there's the, thing about, vision systems, whether it was, convolutional networks or vision transformers. Was that there was a huge amount of redundancy because there is this 2D, things move slowly across the image, type of thing, as opposed to linguistically, where things are not nearly coherent, and within the space of that, if those things work, you can start. kicking the system and relaxing some of the hard constraints and see how well it does. But, like I, said, I'm, just saying there is a framework there if you want, if for whatever reasons you want to draw an analogy or, comparisons, there's a starting point. Okay. It's good to know worth looking into a bit, but more, I feel like that's almost the best way for us to go to keep the exact same training regime we have for Monty, and then try our very best to train a transformer model or a deep reinforcement learning agent on that same data. I wouldn't want to spend a lot of time on that. That could take forever, maybe. I don't think it would. Okay. Yeah. Can I ask a slightly different question? In mining today, do we have, do we, I should know this but I don't, do we vote on POS already across multiple learning modules?

not, really. So we take the relative pose of the two sensors that are voting into account, but they are not communicating their hypotheses of the object pose to each other. Is that something we should be working on? Yeah. It's on the to do list. It's a very long to do list.

One of the things that occurred to me is, we want to, highlight capabilities that are unique to the system, regardless of how they compare to other systems. And, we think about everyone, almost everyone thinks about vision as this image and then there's these patches on the retina next to each other, but we're not limited to that at all. I've said this in the past, you could have an image system where you've got three cameras in different corners of the room, and each one is attending to some patch in its visual space. And as long as they're all looking at the same object from different directions. It all worked just fine. it's it, people just don't think along those lines. And so one of the things, it'd be nice to demonstrate what, having two fingers touching the object in different locations.

it'd be nice to somehow, if there were, illustrate the benefits of that. it's a system that can handle these sensors that are in different locations and unite them in a single, very rapid way.

Yeah, that reminds me. Are there any approaches that do object post recognition from touch sensors? I didn't see anything like that. so I can go back and refine the search with that particular modality in mind. Because I'm not surprised that it wouldn't have come up, given the way I was searching, if that does exist.

In the, in the, neuroscience world, they almost completely ignore somatosensory sensation because they have no idea how to handle it. the, the classic, a hierarchy of feature detectors model doesn't make any sense with fingers and skin.

just, people have avoided it. They're like, on the neuroscience side, I'm not aware of any theories that explain how we recognize things in touch. just point that out so I wouldn't be surprised if there were fewer in the machine learning world as well because people, it just doesn't, people can always say, oh, it's an image or these patches. I, I'm looking at the object, when you have fingers touching an object at different places and locations, what the hell is it? What's going on? they don't know what to make. They have no idea what to make of that. Yeah, they're, yeah, I'm sure there's a lot less, if any, machine learning research on that. So like recognition from touch, but I wonder if it, might actually be good for us to phrase it as like recognizing the object through touch. Because exactly like you say, people are, if you think about vision, they're used to thinking about the entire image and for them to understand that those are a bunch of small sensors moving across the image, it's going to be hard to. Like a lot of people, we might lose a lot of people there, whereas when we say, okay, we have three fingers that are moving on the object, that might be easier to understand. Why do I keep getting balloons? It's weird because, this has really messed up all kinds of AI and machine learning and neuroscientists all these years. it's, the real breakthrough is understanding, patches of the retina are really no different than patches of your skin. And, it's, it is, it's, something that most people just don't understand.

our job is to educate them.

For touch data, what the closest thing, what would a touch data, I guess one question, what would a touch data look like? And maybe the closest thing I can think of is depth because you can think of depth as how much I need to move my finger to get to that object. and if I phrase the question as, is there any models that does post estimation from depth? not just from depth, it, usually involves RSV as well. So that's sadly that I don't think I've seen model that just. On depth, but, those might exist, Yeah, but then they again, they get the entire image probably are not just that moves over the object. yes. I don't know if there's research, but in, in like autonomous driving space, a lot of the new LIDAR sensors. Are not scanning. They're just fixed orientation. So they're looking out from the car with a specific focus and are going to move with the car. So maybe there's something there. Is that right? Yeah. The, Definitely like the Google cars are still using fully scanning LIDARs, but it's a much more expensive device. And so there's a whole slew of companies producing essentially LIDAR cameras, if you will. Okay. Interesting. They still scan the laser within a defined window? They still scan, but they don't rotate. Exactly. that's just a, it's a matter of degree, right? I'm just thinking of a place where someone's You know, getting a small patch of LIDAR information. Yeah. They probably build up a three dimensional image from that, and then they process that. Yeah. Yeah. Yeah. I don't know how they're doing it.

Did you have any other approaches to talk through? those are the four major categories of how post estimation is done. I think the rest of the slides are sort of models.

yeah. Metrics of how post estimation is, how their performance is measured. Because, if we were to go this route where we're comparing to Deploying models and we probably want to compare against, use the same metric to compare. but it's not, I guess it's not the highest priority now, but yeah.

the rest is like just a collection of what we assembled is like most promising. If we want to go down this route of creating a model on Monty type data for comparison purposes, these are some promising options, for object recognition and, pose estimation.

Work in slightly different ways and in, in order to know like how much of a haul it would be to get Monte data into a format that's, that is compatible, we'll still take a little bit of work. I, need to download some training data and see what format it's in.

some are pretty interesting, foundation Pose is probably, it's, up there as state of the art, that just came out this year.

it's got two sort of modes of training you can give with those CAD images. Or CAD files, or you can give it, images from different angles, and that's its model for your training mode. And from that, it works on it develops category ideas. So it can encounter novel objects and estimate their pose by associating with some kind of similar object or category of object.

it takes a ton of training.

is that for the model free or also for the model based version? Okay. The model free version to my understanding, it's a preprocessing stuff. It's going to get you to something like what a cap file will get you by giving, it'll basically reconstruct this mesh.

so it's more of a preprocessing step, but yeah, it seems almost like it would be fair to compare it to the model based approach. saying the CAD models are a bit like our 3d graph models. we disregard that the training is different and that they just get them. And then, I don't know, I'd be curious about how they then, what kind of data do they then use for inference? And how did I process it and recognize the pose? This single, I think it takes single RGBD images for inference. It's pretty fast. You can do it in real time. Like you can click on these links and get the demonstrations or whatever.

I wouldn't call it continual learning. It's not going to keep adding to its, database of objects, but it does at least have. the ability to Estimate pose of novel objects just by associating with the category. So it's like a little less brittle than something that's It's not continual learning, but at least it doesn't freak out, want to see something brand new. Can you open up the paper and just show the figures in there?

They're so funny. Yeah. Oh, they use language too. Okay. Yeah.

The objawars. Is that a universe of objects?

Good question.

Okay. This looks a bit too complex to understand. with a quick look, but yeah, I think we're like this kind of, comparison seems like where we're going is, if we want to, again, low priority, we, I agree that we can use transformer to feed in patches, but it will be patches at Monte C's that we feed in. I don't think we can train these models with 1 million images, nor I don't even want to, or I don't want to personally, I'm not trying to make a recreate a soda, deep learning, like a thing, but, It's and so I think what we can do is we can make start from like a basic vision transformer. We can possibly add some walls of bells and whistles from, so that some of the techniques, but ultimately will probably be training our own kind of custom transformer. Let's see. It's the same things as Monty. Let's say for patches. and Okay. That made that would be a comparison to Monty. It strikes me a little odd that we would take Monty's training data, which is a series of patches and movement information, and then try to train a transformer model on that.

it's I'm trying to put my finger on what's odd about it.

yeah, it's, I'm just going to repeat what I said earlier. Our goal isn't to say, oh, our algorithm at doing this is better than, it's more like our algorithm works differently. These systems don't work this way. And, because we train a transformer on a series of image patches that Monique gets. It'd be hard to argue that's a fair comparison, for the benefits of doing it Monty's way.

people might dismiss it as oh, you trained this thing, it's weird data, why would you do that? I don't trust that you trained it properly, that kind of thing. And there's that kind of disconnect.

yeah, for all these things, just because of the way the money collects observations and does inference, like the actual training process, the inference process, what the information is for these different types of models versus what information money sees, it just makes it very difficult to, try to do a one to one comparison, I think, with any of these other models. I think possibly what could be getting similar is using insert transformer possibly RNN. So for every step patch that they process it might have a pose and then the more patches it sees it can try to refine that pose. Maybe that's closer. but again, I think that one at least takes into account of kind of sequential kind of processing perhaps, but yeah, but otherwise it's just, yeah. It's probably nonsensical to download these foundation models with the 1 million or images and try to train that and compare that to Monty. Like it, there's no point in doing that. Agreed. Yeah.

it looked like they tested on YCB. Can you go show what the results were there? YCB video, I believe. Yeah. Oh, okay. But it does have a potted meat can. Some of the objects in YCB, I think 21 of the objects from YCB were made into YCB video, I think. Oh, true. Yeah. These all sound familiar, the MasterChef can. Seen that one many times. Mug and bowl.

Okay, and then those numbers are accuracy on pose detection? This stands for average distance. So after getting the pose, they make it into 3D, and so they calculate, and then after the 3D model, they compare that, so compare that with a ground truth pose rendered 3D, and then calculate the Euclidean distance between every point. and then average that. So bigger is better?

no, smaller. I would have thought with distance that smaller would be better, but here they've got the largest values. I guess it's 96 percent correct or something like that it means, maybe? possibly. I know, that's confusing. So I did Okay, it's not that important, but Sorry, I did write like a metrics section, but that doesn't, help.

okay, nevermind. By the way, if you are interested in any of these papers, there is a folder where we've dumped a bunch of these papers into. On the Google drive.

Yeah. So I wonder if it makes more sense right now, we just focus on creating plots that show Monty's capability. So how we are good at all those things, learning from little data and so on. And then we mentioned all of these approaches say, okay, this transfer based method uses a million of images, instead of actually trying to retrain it on the exact Monty data and, trying to compare it.

I'm not in the machine learning world enough to know whether or not this would be fair or not, but to actually just side by side show how much actual information is being provided the system before it comes up with a positive result, like something like that, comparing that with other models. I'm step beyond just the training data, because if we are just using patches. that could ultimately be a lot more parsimonious of a, or, we can show efficiency basically, on inference if we're talking about sensor patches versus whole images. Yeah. Possibly.

Yeah. We could have one of those log scale plots with number of images seen, and then, it is at the bottom and transformers are all the way up there, but I, yeah, I guess the problem is that the performance measure is also different and we get. We have patches and movement data and they get full images. How do you compare those numbers to each other?

if they're, I think if the reader is like a kind of actually done, if they're if they're truly concerned about Monty performance and they can like just read these papers and see like how. people are doing it, these YCB video datasets, because the way that we're doing is so different that it's basically incomparable, for the most part. Yeah, you have the input data side, but you also have, the intrinsic number of parameters that takes Monte to once it's trained to represent things compared to. some morally equivalent, transformer out there. I'm sure you'll win on the parameter side of things. True. Yeah. That's another comparison we can make.

But I think I agree with, what you said earlier, Vivian, about, focusing on visualization and plotting to show multi capabilities.

Yeah, and then we can just do more of a literature review of the other approaches and highlight how they are so very different from what we are doing and then maybe have like plot or table comparing number of examples seen and number of parameters internally. Yeah, I guess it could go into some for writing a full paper, like a background and kind of What's been done previously and that would be much easier to pull together on a table of number of images and how it's doing than trying to test a bunch of these models for sure, right? I agree with that. We just don't want to do that. Cause yeah, we already know. And I think every machine learning reader would know that if we would train a transformer on the amount of data Monty's trained on it, it would just, fail.

Like we don't need to train a model that would fail, right? Yeah. If you need someone to train a model to fail, I'm your man. Okay.

yeah, I guess that's about it on our side. I think that was helpful. Useful just to see what the state of the art is out there.

and it was also good to have this discussion about what we're trying to do, what we need to do. Yeah. Thanks for putting together those slides. Yeah. It's very interesting.

And now you know more about object pose estimation, probably more than you care. instead of a, I wasn't even aware any of the state of the art out there. So to me, it's a good introduction. I dropped the salient image of vision transformers into the chat. It just came from the Wikimedia. Wikipedia, section on just you just input vision transforms, just to show that parsing that goes on.

Sorry, I can't leave the chat to the screen. Sorry. So you'll just have to open the chat yourself and see, but. Okay. Okay. yeah, I guess we took the whole meeting. but I think this was a good discussion. And then. Yeah. for next week we can go over all the other smaller topics and I'll have a bit more time to prepare answers. And I think Jeff, you offered also to give some answers to some of the questions. Yeah. Remind me again, like on Monday or something. All right. We'll do it. And yeah, if anyone else wants to send in some more questions and topics you'd like to talk about, just post it in the research channel.