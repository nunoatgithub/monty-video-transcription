I think the first topic is Niels is gonna be talking about the, positionality and more specifically how to evaluate or, how to, because in Monte we have those stacked lms and we want them to, just basically look at objects and be able to evaluate the positionality and figure out what that object is. so how do we evaluate that? it's not very straightforward. Sonia is gonna be talking about the challenges. and then I think if we have time, at the end, I doubt it, but if we do, we'll probably talk more about the behaviors. And I think Viviane has notes on, the can worms, which is basically how to compensate for, objects, object motion in the morphology model so that we can predict morphology features, accurately.

I think this is the topic, but also we may still talk about time if we have time. yeah.

go ahead, Niels. I. Cool. Thank you. Yeah, so I'll just get started. So I don't have slides, but what I've been doing is putting together the document I was going to share and it's, it's not as polished as I was hoping. It's already 15 pages long and continuing to grow. 'cause there's just like a lot of elements and stuff. So I probably won't go through it in a ton of detail, but I thought it would at least be worth going through at a high level and just talking about some of the things it discusses. Maybe discuss it a bit now and then if nothing else, when I do share it, it'll be a bit clearer.

why we're even talking about this stuff. And what are some of the things to, for you guys to already start thinking about because it might help us come up with some solutions. 'cause yeah, a lot of it has is open, questions more than and answers.

yeah, so as Rahmi was saying, it's all about how we evaluate, learning and inferring compositional representations. So like how do we actually benchmark that? How do we convince ourselves Monty can do that? but also how do we mediate that? So what do we need to do to enable Monty to, learn and recognize compositional, objects And that includes Things like supervision, oh, we're gonna tell them, tell Monty this is a child object with this label, or whatever. But it can also be things that would be out of scope of this in terms of implementation, but maybe more important than we realize, like things like attentional windowing and stuff like that.

and, yeah, so there's, a mixture of just like notes as well to myself in here right now. Let's all focus on kind of some of the figures. I think this is familiar to everyone. This is what we mean about compositional objects and modeling them. So you'd have like a logo, on a mug, but that logo could take different, forms. and we can very quickly associate these together. But in the long term we might even have the concept of a handle as being like a child object and things like that. But for the moment we just want to get a basic, relatively basic version of compositional, representations, emerging and being able to evaluate that. And for kind of a bit of context, right now when we have the single object scenario, we show an object, we put it in the, habitat simulator, we know what object we've put in the habitat simulator. And so the evaluation part is very straightforward. We're basically, and we also know what pose it's at. We're just, I'll stop sharing for a second 'cause I'm not pointing at anything on here. In that setting, that's very straightforward because we know exactly what the object is and what its poses, and we can compare Monty's output representation to that and see how well it's doing. but as soon as we start adding multiple objects and creating our own data sets, a lot of, so there's two ways we can know what the ground truth is and therefore evaluate. The simplest one, which we've relied on in the past is you just have one object that you're evaluating with, and so you know all its properties. As soon as you start adding multiple objects, we need to know what is the sensorimotor module actually on, and therefore what should we expect the learning module to be observing and, what would be the correct inference at that time given what the, learning module's actually seen. And we can do that to a certain degree through what's known as the semantic sensorimotor in habitat. and I believe something similar might exist in mu yoku where we can basically, read out, oh, okay, this is the object that the, agent is currently, or, kinda the sensorimotor module is currently sensing.

so part of what I've been trying to think through is how can we still benefit from the semantic sensorimotor, or how can we avoid needing the semantic sensorimotor, or even if we have the semantic sensorimotor, what are still some of the open questions about how we. Evaluate Monte.

so going back to this document, and actually maybe just to kinda show, oh, sorry. Yeah, go ahead. so when you say evaluate, now that we have two lms, yeah, we are going to evaluate at both lms or is it just a higher level one that we want to? Great question. So that's part of the kind of stuff to think about. I think that one, I feel pretty confident saying we want to evaluate both lms, okay. Like higher level and lower level.

it's generally more straightforward if we just focus on the higher level one. And so that, that's kind of part of what we can do, at least to begin with, is, is is be more concerned with that. But, but yeah, it's, I think those are the kind of things we just wanna decide. okay. But yeah, just to show and yeah, please definitely stop me. Any similar questions, comments? yeah, just in case you haven't seen the semantic sensorimotor before, this is a very rapid fire demonstration of that. So what you're seeing is the sensorimotor and the, kind of agent moving around in this multi object environment. Sometimes the sensorimotor module is pointing at the, golf ball, sometimes at the mustard bottle. And up here, you have this kind of label. That, that is something we can get from the, simulator when we have multiple objects like this in the environment, we can actually beat that out. which makes, yeah, evaluating a bit more straightforward.

but to start with, I think one interesting thing is that even if we have access to that information, so let's say, okay, we have a lower level column or, lower level learning module, and higher level one, they're both observing the, mug with the, logo. And let's say for now they're looking somewhere here and in this first kind of setup, it's okay, we want the lower level learning module to represent the, logo and the higher level one to represent mug. But it feels like an maybe equally valid, ask of the system is that the higher level one should be representing, mug with logo.

and this is, and but if, and this one maybe feels intuitively more correct, but then this raises interesting questions about, okay, how do we learn mug with logo without throwing away or restarting, given that we already have a model of mug, how do we kind of branch that off?

the mug is in the lower level. Is it in the higher level also? It's in the, yeah, in here. I'm assuming it's in the higher level, but in general then this is another issue in general, we would or not necessarily issue, but the challenge in general, we would want models to be in both, levels, because you could have a mug that's a child of a logo.

so in the very beginning, like the first time we see the mug with the logo, we would definitely just have the mug representation in the higher level column. But then I guess over time you would learn that compositional object and learn the association of where the logo should exist on the mug. And then I guess the question is, would that be an update to the existing mug model, or would that be like a new model that starts learning mug with logo?

Yeah, exactly. And yeah, it feels like maybe a branching of some kind would be a nice, like almost a fork or Yeah, a branch, if it was a repository where it's okay, I'm not gonna start over from scratch, but this is now a new object with this kind of prior information. But it's weird to think about from a biological perspective. Yeah, it feels like that's just possible if we represented as an object with different states, let's say the mark can be in different states and depending on what the L one input or representation is, it expects certain features or doesn't expect them. And yeah, those could be the logo and no logo. Yeah. But it seems hard to just make a fork of an object, like a copy of the object and then learn extra things on it.

But maybe it's something that both happens at the same time. Maybe the first time you see the logo on the mug, you're like, oh, this is the mug, but it's in a different state now. It has some extra features here. And then, but then as you see it more and more often or at in parallel, you're learning a new model of mugs. Logo. Yeah.

Yeah. As in, how would it be? Sorry, in parallel.

oh yeah. one thing I just thought of is, I guess if you learn it at first as a different state of that existing object, then no, maybe not, but maybe. If you then encounter them independently, if you can just assign a new ID to that state, a new object ID to that state, but then like they would still be sharing a reference frame, so I'm not sure how much that would make sense. Okay. Yeah. Yeah. As in, sorry, mug and mug with logo, or Yeah, instead of being instead, oh, this is mug and state one and mug in state two. Yeah. Being like, alright, mug in state two gets its own object id now, that does output from layer two three because I've seen it. I guess if, nothing else, the intersection of the state and the reference frame ID would give you a unique id.

yeah, so like this is standard mug is also its own ID because it's state zero or whatever. And then, but yeah, in general I like the suggestion of using states to help with this. And then, yeah, in terms of kind of concretely what we do next, I don't think we, we have to solve this problem to begin with. I think we could already look at interesting, many interesting problems and things about representing positionality and having that emerge just by considering this situation where. We do learning of the ob, the low level objects in isolation. And we just see whether this happens because regardless we need to do additional learning before we get mugged with logo. And typically we just do. But if we don't have, we just have the first scenario, we wouldn't have any composition object. We would just be recognizing two different objects. Yeah, it would be, but it would already be hard or interesting I guess to, to see that, that, you would have different learning modules, recognizing different objects when they're looking at the same part of the world.

yeah. And then, I guess the assumption is that if you then do further learning, you would get mug with logo.

I, I would've. I'm not, I have to think about it some more intuitively. I would've said start with the second one. Be like, we never learned that just mug model. We just learned the logo and then we see the mug with the LO logo on it. And in addition to storing the raw color values, we store the logo ID on there as well and learn a compositional model. and that doesn't really, then we don't run into all these new issues of how do the two pay attention to different things in different areas and space. But we just look at modeling a compositional object versus the first one is not modeling a compositional object. It's really more focused about these two learning modules. Pay attention to different parts of the world. Yeah. Yeah. It's interesting. I guess the only thing is with this, one, if we learn mug with logo without ever having learned mug, I'm just trying to think about what is the actual benefit of the logo. are we really learning a compositional object or are we just, I, know we'll be passing the idea in, but are we actually using that in a meaningful way or we just essentially learning an object of the, whole thing and then there just happens to be this being associated each step, but this could just be random information we're basically passing in. No, because when we recognize mug with logo, we can automatically make very accurate predictions about the logo itself with the feedback connections and those are all the benefits of modeling composition objects. the logo could exist at different, on different objects as well. If you see the logo that kind of informs additionally which type of object the higher level column is seeing, it's not just colors. It's not just light blue is on a bunch of objects. It's no, the logo is only on these objects, on those locations. So I feel like storing that object idea of the lower column does help a lot with inference. And then it also helps a lot with predicting in the lower column about like how the logo, what logo features to sense where.

isn't that, I feel like the second scenario is the whole modeling composition objects, idea. Sure. I'm not saying we wanna stop here. Definitely not. I guess I was just wondering whether this is a more constraint thing to start with, but, but I agree. we ultimately want, yeah. The kind of confide tissue. Yeah. yeah. The question is, do we need to do the first one to get to the second one? do we need to figure out how to pay attention to different parts first before we can model compositional objects? I feel like off the top of my head right now, I need to think through it more. I feel like we don't necessarily need to. And if we only do the first one, we are not modeling composition objects yet. We're really looking more into attention.

but I'm not sure, maybe there are some arguments why we can't do number two without doing one first. Yeah. So I guess generally when I've thought of composition objects, it's yeah, again, it's you would have mug to begin with and then you would learn mug with logo because you could have different logos. And so if we jumped straight into this, we'd have like mug with TVP logo and that would be like a totally different object from mug with mento logo. That's, but it kind, again, it might, be a useful stepping stone, but it feels different from, I think it's a different object because in your cupboard you have a cup with Menta logo and cup with TVP logo and there are separate objects. It's not like that cup ever transforms into the other cup. in my mind, I have both cups in my kitchen and in my mind there are two separate objects. Yeah, they share some morphological similarities and stuff, but I'm not sure if they need to be learned, if that needs to be represented in the model itself, or if we could just be like, there's like a general morphology model of that big coffee mug type.

Like a different, a different learning module that just knows generic mugs. Yeah. So for scenario two, are you also learning the mug without the logo? in the lower level, I would argue we don't need to, at least not to, investigate modeling composition objects. But then what does the sensorimotor, output, what object ID do we store? when the sensorimotor is not on the logo?

I guess it's a, yeah, it could be mug. 'cause then we could have mug with lo could be associated with points on the logo. Oh, you mean the lower level column? If that one loads the mug? Yeah. The, lower level. when it's not on the logo, it should output just a cup, like a mug. Oh, okay. Yeah. Yeah. that one could have a model of, a, mug too. Yeah.

I guess how would it not, if you're learning this TPP mug, the lower level is gonna see the whole object, During learning? it could be that it's constrained and the size of objects it can represent, like the lower level in the hierarchy might only be able to represent objects up to a certain size and then. It would maybe never learn a large object like the cup, but only small parts on it, like the logo. But yeah, I don't see an issue with it being able to learn the mug either.

Yeah. In some ways that would be better for evaluating this because then, we could, basically see that we're binding a different object to different locations. I think the question is how does the brain do that? Because it's only seen mugs with logos. How does it extract features that are only about the mug without any logos, so that it could just be in the lower level and use it with other, logos? I would, we can set it to start with, I would set it up that the lower level column first just launched the logo in isolation. Like the first time you saw the Menta logo was probably not on a mark. It was probably on the website or something, in a pretty isolated way. So you could argue that you can learn it in isolation first, and then you can recognize it projected onto different types of objects.

it's rare to see it in isolation. I don't know, maybe, but especially, but the, like on the other hand, with the cup, with the mug, you only see it with logos or.

is that true in the real world? normal bugs all the time. if you go to our thousand Brains website, it's it's pretty isolated up here. You can, yeah, just like there's a lot of white space around it. You clearly see that something else starts down here. yeah, I agree. We, would probably need some mechanisms to do this kind of model free segmentation and try and stay in that area while learning it. But yeah, I think to me, too hard. I guess the second approach would be this kind of statistical approach of, I'm seeing this consistent pattern, but on a bunch of different backgrounds. So the backgrounds get averaged out over time, whereas the consistent part of it remains in the model. I think that's why we, and like I think it would be a mixture of, probably a mixture of those. And I think that's something we can definitely weigh with. 'cause that's, a more general kind of unsupervised learning thing, which yeah, we definitely want to address. But, I think just to the kind of aims of this is really to see whether things like a location by location binding in the hierarchy, that's a proposal. Like is that giving us what we believe it would in terms of, Being flexible about these compositional relations and predicting the appropriate thing in the low level. and that it's fine if they learned it in a kind of semi-supervised way where we showed, okay, this is one object. And yeah, I guess to your point, Viviane, then we can already evaluate that kind of stuff with, yeah, with the, just having a unique ID at the top for even TVP mug with bent logo is a totally different id. Yeah. I would argue that's reasonable to say those are different objects. Yeah. At least as a first, unless you see it bend, I would say we classify those as different objects. if you're not, if you see the logo rotate on the cup Yeah. And those are different states of the same object, but if you just see two cups with a bent logo and non bend, I feel like we, we would say those are two different cups.

I, don't know if this is helpful. It's a, or it's a tangent. If it's a tangent, just feel free to ignore it. but I, I think, just hearing the supervised and I think it, it might be a source of confusion trying to figure. What an object is. And I think it might be more a approachable to kind of what Niels I think Niels is saying is like supervised saying, this is an object. You, this is a not, this is an object. We are just test testing compositional here. Yeah. In a supervi and just kinda supervise manner because to Ram's point, like whether we're learning C cups with logo on it or not, like I think that's all very fuzzy and I don't think we know how we learn what an object is. I was thinking about through this conversation, we don't actually have a definition of what an object is. And so we can, but we can still experiment with compositionally by saying this is an object. This is an object. How does the comity work? Without getting caught in the weeds of what is an object. Because immediate example comes in, I know what a tree is, but every tree is different, right? So it's what is an object there, right? So, we don't need to get into that, to progress it. And I would just say, let's not get into that because we will confuse ourselves farther. That's probably a different problem to solve of what is an object. it is a different problem. Yeah. I just wanna make a point of the point I was trying to make is if we give Monty just a mug with one logo and we just tell it to learn it, it'll probably just learn this as one object. It's not going to need a compositional until you see that same mug with a different logo. That's when it decides, okay, no. Now I need a more compact representation. I need to extract the cup from the mug, from the logo and then so that I could use them on different things.

and same for the logo. If you just, if it just sees the logo in that same cup, it's the same, it's the same object. but once you start to see the logo in different places, then you start to say, okay, now I need to factorized that so they could make a more efficient representation of the logo that can be used elsewhere. and I just wonder if, but it might start, I think it might start the other way around. You might first learn a bunch of different objects. Like you, you learn about letters in, in preschool or school. Yeah. And you learn those in very isolated way, ways on paper and everything. And then you recognize those letters in a bunch of different places. You recognize the letters on the cup and stuff like that. And it could be very similar with other things. You learn generic models of cups and generic models of curves and letters and stuff, and then you learn the composition models of them. I think it's very difficult to, actually factor out totally new features that you've never seen in isolation before. I, would argue that, it's. This is like a, special supervised case where you, like you, you have a curriculum learning approach to it where you just focus on the little things and then you start building. like I think that both of them are, are approaches to learning objects. but we also can't, we can't ignore the, learning where we, just see the object and then later on decide. No, it's breakable into different parts. Just like when we see the STA stapler at first and then we say, oh no, it's two different parts. Not just because of motion, but maybe because we saw that later on in life. We saw that part of the stapler on something else. Or we saw that handle on something else. That's, yeah. Yeah. I agree. we still need to solve the problem. but I guess ma mostly to what, Tristan just said, I think we don't need to solve all the problems at once. So we could focus on just how do we represent compositional objects and how do we infer them based on those composition models. give ourselves a few crutches around learning, like supervising a lot and showing things in isolation. And then from there we can move on to like, how do we do this with less supervision? How do we remove some of those assumptions during learning? Yeah. I totally agree. Yes. No, we, I, totally agree that just to validate the, problem of positionality, we need to tell both learning models need to give a supervision of what is, an object at which level, and just start to make those binding associations, yeah, No, it's definitely a important problem to, get back to at some point.

so yeah, so I think that was already a useful discussion so that, yeah, right now we can think more about it, but as you're suggesting Viviane, we start by, yeah, we would still learn the, logo and the mug in isolation.

but then we would, learn TVP mug, for example, as a new object and we can, again, provide a supervised label, if needs to be at least begin with. and then, yeah, and then we can look at things like, yeah, where we are associating locations on the logo with the location on the, TPP mug and things like that.

yeah, which, lemme just write this down before I forget.

okay. And then, but yeah, so that was, the opening figure because the, question was basically, okay, assume we have access to the semantic sensorimotor information. What would we actually, what would we actually be looking for at each level? And so for the high level, it doesn't really matter too much because no matter what data set we're using and stuff, if we're presenting the mug with logo, like the sensorimotor modules in general are only, they, they're never pointing into the void. They move back onto the object when they're, when they go off into nowhere. So we're always going to be looking at the mug with logo. So as long as we get that representation emerge here, like mug with TPP logo and not bowl with mento logo or whatever, then this one's correct.

yeah, go ahead. Yeah. I'm just thinking about this mug with logo thing instead of mug and I know this is an edge case, but there are stickers and magnets. you can put a logo on something and then peel it off and then put a different logo. And I don't wanna overcomplicate things in the beginning, but just like in the general case, like I think about a dinner table, like talk about the dining set. If I take a fork off of the table, it's not an entirely new setting of, the table and the arrangement of the table. it seems like, mug with logo might be like overly constraining in terms of it being like too specific for the exact setting that, that we're trying to compose. like it almost feels like it could just be too specific. Yes. my TBP mug is different from my Numenta mug. They're distinct items.

but I think if you're freely combining objects with each other, ad hoc, feel like you wouldn't learn both the forward and backward connections. if the fork can be anywhere on the table and still the dinner table, there's no way for you to learn, right? When, if I'm on this location on the table, I expect to be at this location on the fork. So you certainly couldn't learn the backward connection if there's, the constancy there and I guess with the forward connection it could, yeah, there might be a bit of tolerance to like features being at slightly different locations and stuff, but it seems Yeah, you can see them at different locations, but maybe at that point you're really just recognizing the table and forks as individual items and not really a specific set dinner table model that you can use to make predictions about where things are on the table.

There's a, so does that mean that each dinner table would be its own or, if I take the fork off or I move a fork around now we have an entirely d new object. Now it's, yeah, I guess it depends on how much you've, how much you've attended to the fork and how important that is. it would likely be like a quickly learned model in the hippocampus or something That you can use to make short-term predictions. Like I just saw the fork over there. but it, and that I would say would have its own kind of id, it's not like you would say it's a different table now that's still like the kind of stable table model you learn stays the same. But for the kind of scene representation you built up just now, I would argue it's not some generic dinner table id. It's this is my dinner table right now. Id, yeah. So one way to describe it is that like in region two, you would have many columns that are just representing mug. Looking at this, and also some that are learning this specific instance of a mug. And and both of those can, help you inform how to act in the world or whatever. If someone asks you to pass the TBP mug, you know which one to pass. If someone asks you, Hey, do you have something I can pour coffee into? Then any mug you can see right now is valid. and I guess one important thing to keep in mind is that there isn't necessarily like one output. It's like all of the learning modules can be, outputting different things. I might be recognizing generic mugs, I might be modeling the logos, I might be modeling the mug with the logo, and those would all be correct and valid interpretations of reality that can all be active at the same time. So it's not like there needs to be just one overarching classification.

so I guess it seems like we don't need composition, we don't need hierarchy to do mug with logo. We can just do mug with logo with a single lm. but that's what we have now. the so why, yeah. The point of learning a composition model is that. This now gives you a lot of predictive power and helps with inference in the future. if there is a constant relationship with the logo on a specific mug, like I have the menta mug here and I know that the Menta mug has the logo on the front and on the back. So even if I'm just seeing the front, I can predict very well what I will see if I turn the mug around because I've learned that model of this specific cup. if I see this for the first time, I would still recognize the Menta logo. I would still recognize that this is a coffee cup, but I wouldn't be able to predict what's on the other side of it. Does that make sense? but I think Scott's point is that yeah, if we learn the entire object with the high level one that gives us, I, I think unless we constrain the high level one in some way, like it gets coarser sensory input or it can't, there, there does, yeah. I feel need to be something that's distinguishing the logo model in the low level column from the sort of logo submodel in the MO mug with logo model.

if those are equally detailed, then. It's hard to, yeah, they wouldn't be equally detailed. The higher level model would get like larger, lower resolution, receptive field. yeah, but that's like an important thing that we need to ensure is there otherwise, there won't really be a clear difference. So if they, but if we do constrain them in terms of size or resolution or whatever, what happens when we want to zoom in on a logo and just look at sub components of logo like letters? Do we need to bring the whole thing much closer to our face so that the top level one, has, good enough view of the logo itself?

we have to deal with nesting beyond two levels eventually. And I'm concerned that by, just to repeat what I just said by constraining each by a certain size or resolution that we will lose the ability to, to nest, further, down to the letter or even subletter level composition.

I may be missing something, but when, if you need to get details of the lower level, like lower level details, you would just use the, model that you stored in the lower level lm, right? Because that's the, detailed model that when you store the object ID in the higher level column. You could use that to basically figure out what the morphology model and the lower level lm, what if the logo itself is compositional? So it's not just a single thing because a logo is compositional, it's composed of a bunch of letters. Yeah. So I guess the, idea there is that you would shift what R two and R one is representing, and this is why it's important that both learn multiple objects in that at that moment when you're focusing on the logo, R two would actually represent TVP logo and R one would represent like letters or the TVP neuron thing. And yeah, I guess the way the kind of attention shift is something we haven't really fully figured out. At least I don't know how we Implement that, but yeah, that's a good, so if we're looking at the logo in isolation, for example, and we're just then in that case, in order to enable that be behavior, R two would be learning the entire logo while RR one would be learning the letters, right? Yeah. Yeah. But then somehow R one is also going to represent the entire logo at some point when R two is looking at mug with logo. Yeah. And so during learning, they would potentially all be learning, if you're learning the logo. R one and R two are learning the logo, but each with their own kind of flavor of model.

if that's I guess the level you're attending to, but then at imprints that's where we're saying like you we need a way to like shift the attention, that seems like you only at the low level get the, the, child representation. But when we shift the attention by that, do you mean the entire learned model is actually moving down the hierarchy? Because at some point No, 'cause it would be at both levels. It's just, it's constraining. I see. So I think the example Jeff gave was like the mustard bottle with the French logo or whatever. That maybe initially when you look at the mustard bottle, both R one and R two are saying, oh, it's mustard bottle. And then when you narrow in and look at the logo, R one's input is being constrained. Maybe in, in a way that it's kinda oh, okay. Yeah, there's something interesting there. And so it's getting a more narrow constraint input and it's represents logo. Then R two is still getting a broad input and just keeps saying mug, mustard bottle.

okay. Does that mean that theoretically during learning R one would learn. TP logo and mug with logo because it seems Yeah. That's what that would mean. And it would mean that every lower level thing is learning sort of multiple levels of composition, version of some object while the higher ones, if they're constrained to size or resolution or whatever, learn fewer. Yeah. Or it would at least, it would imply maybe that we also need some amount of attention at learning. Again, this is for the unsupervised learning to not necessarily the basic supervised setting because, yeah, if we have attention during learning, then we can also say, okay, I'm learning the logo, you're learning the letters, or I'm learning the mug with logo, you're learning the logo.

or not even learning, just representing, yeah. It's almost as if learning maybe through attention only happens at one level, like building new models and it can rely on inference happening in other learning modules. So if you're learning mug with logo at a particular level, if you already know the logo, the previous level isn't learning anything, it's just representing stuff that it already knows, like logo and mug.

At least it feels like that might help with some of that.

Yeah, the logo, if the lower model already knows the logo, it would just recognize the logo and not start learning logo with mug and feed that as input to the higher level model.

Yeah.

But yeah, maybe just another interesting example to think about, and this is actually a data set we could potentially work with, is the Omni Cloud dataset, which, those of you don't know, is a, dataset of alphabetical characters from a bunch of languages around the world, including made up languages like the alien Alphabet and Futurama.

and each letter has been drawn by, I think it's 20 different people.

and so the idea is that, yeah, you can test, generalization, but it's, it's also a challenging data set because it's very small. there's al there's very little training data and so it'd be a nice. It'd be an interesting one to look at because, Viviane had previously set up a data loader so we can actually evaluate with it. And it is, one that traditional kind of deep learning methods struggle with because of the amount of data.

it's, also, I think the Amazon Turks who were writing the letters must have been like writing with a mouse pad or something. Because if you just look at the Latin characters, this is a G and a K, I think even as native, I dunno, Latin alphabet users, those are funky. for this dataset, we would, if we wanna supply, if we wanna do it in a supervised way, we would want to, do we have the strokes? Do we have the low level strokes? So that Exactly. So yeah, so the dataset, includes the individual strokes, as well as actually timing information about them. So you could also imagine, in the, if we do work with a, and it works well, this could also come into like behaviors and motor output, and we could even get Monty to maybe learn to draw these.

but but yeah, but you, so you do have that breakdown. Yeah, we actually evaluated Monty on it before. and basically the current statuses that Monty can learn and recognize the same version of each character quite well. But then as soon as you show a new version of the character. It doesn't do well because they're like, the global shape is similar, but the local shape is very different. So it's like Exactly. Showed the need for hierarchy where you represent strokes individually, and then the arrangement of strokes instead of the arrangement of all the pixels. Yeah. The lower level stroke could be a little rotated or, but then I hope I don't know, if we're gonna be able to do different scales with it, like what if the lower level object is at a smaller scale or larger scale than what we're used to or what the higher level model has stored? You mean if we're, deliberately changing the scale or just like natural variation? Just natural variation. Yeah. Like we see here. Yeah. yeah. And also are these strokes low level enough?

may not be, like, I could see there could be a circle and then a, little thing. I don't know if they're like the, most primitive, low level ones that we could use.

but I guess that's something to find out apparently. Yeah. I think we'd, I out, I think the scale, I don't know, it feels to me like the distortion just from the way people have written it, is, as much. a challenge is the variation in scale or it's a similar kind of Yeah. Magnitude. but yeah, there, it's not perfect. 'cause I think, again, 'cause it's not native, I, think they basically showed these, the like ground truth characters to people and then ask them regardless of where they were from, to draw them.

so not everyone uses the same stroke patterns, and so all of these won't be recorded in the same way.

the order, like matter for auntie, the order in which they were drawn, but there's no, but as in if a variation within each stroke. Yeah. So but like for example, here, someone has used three different strokes for this thing.

and this comes back to like supervision and stuff, but this would probably be different object, like low level object IDs from This stroke. Yeah. and so that, that could lead to some issues, but, but yeah, but I'm sorry, go ahead. We might wanna find the most canonical version of each letter and then try on that, Start with Yeah, yeah. Or draw it ourselves.

but yeah, I think at a conceptual level. I think when you think about this dataset, it fits with the mug with logo view because for a particular letter, you don't have that letter independent of, the, it's, clearly a compositional object, but it doesn't exist independently of, the sort of binding of the child characters, if that makes sense.

it's not like we have, I don't know, generic Korean letter and then it comes along, it says, oh, okay, this is generic Korean letter with, five, these five different strokes. Now it's Korean letter two. It's it was always learned from the start, this way. Maybe to your point, Viviane, maybe later, like I feel like even though I can't read Hong I can recognize Korean letters very quickly versus Mandarin or, other kind of alphabets that are based around characters. So maybe you do develop a certain degree of a generic representation, eventually, but, yeah. But anyways, I just thought this was an interesting example both in terms of this discussion, but also, It could actually be a data set, that we consider focusing on first.

yeah, I think it's a nice data set because it's very simple. It's two dimensional, it has already a composition defined within the dataset. and it's a relatively well known benchmark that as far as I know, hasn't really been cracked yet. so yeah, I don't, it would be also a cool thing if we can show some results on it. Yeah. Yeah. And I was thinking in terms of the semantic sensorimotor. So generally I think if you add an object, like if you add something object by object to the environment, then you can assign unique, semantic sensorimotor IDs to it and it'll track that. We, don't know, we don't need that for gl, because we are not putting them into habitat. It's like a separate data loader. And the data loader knows when it is on which stroke. Okay. So we I guess I was just thinking could we read out what stroke, if, we wanted to pass labels for the different strokes? 'cause I think that could be interesting to see what stroke, the low level learning module is seen. Could we read out what the ground truth stroke is that the Yeah. Is currently being sensed? Yeah. Okay. Nice. Yeah, so if we then it's not co it's not consistent between versions of letters. if they drew it in a different order, they will have different Yeah. Then it's gonna be different. Yeah. Like basically you get the color of it.

Okay.

that's, yeah, that's fine. It's, not a make or break, it's just, it's at least not an advantage for Omni Cloud then that.

yeah. first, so with the kind of logos on mugs scenario, we don't have a lot of variation of, lo mugs with slightly different logo placements and stuff on them. Yeah. This is a bit of a different task, I think. if we would start, we could start with just the first character of each, the first version of each character and Right. In which case we would and we would have, or, yeah. Or we could hand pick ones where people did follow the same order. Yeah. there's quite a few don't, yeah. We don't have to start with the generalization task. We can start with the inference on the one that we learned on. or one or two other variations.

The mug with the logo. we can control the scale of the low level object so that the lower level LM always knows how to, recognize that object at the same scale. Here, the exam, even the examples that Niels were show was showing we have, different scales of the lower level objects, like the lower of, strokes. They, some of them are very small and some of them are large. I don't know if the lower level LM is going to be able to recognize it at different scales just because Monte doesn't really handle scale that well at, at the moment. Or are we just going to handpick ones with similar scales?

if you show the examples that you were showing again, Niels second can Sure.

yeah, I mean I think as a first example, kinda like Viviane was saying, like I would be tempted to, let's say, start with these three something or 1, 2, 3, those three, which I'll have yeah, we can even change the order if we need to, we can hand label and say, oh, this is the upright stroke as a very first example. I would start with just one and doing learning and inference on the same one and just looking how the composition object is being learned and represented.

okay. Are we talking about learning strokes in isolation then, and then learning letters? Yeah. Yeah. That would be our direct analogy to the mug example. Yeah. So, we need to be able to display the strokes individually as well.

Yeah. Yeah. It seems like this is also a really small data set and there's nothing preventing us from making our own, if we wanted to make Yeah, we can make something similar ourselves, it's more if we wanna eventually about this, people know about the Omni Cloud data set, And it already exists and we already integrated it into Monty, so it would be a bit simpler, but, for sure. Yeah. I was just thinking in the early prototyping, if we just wanted to have go super simple, make a few strokes and make a few letters with, because there's not that many strokes in English either. Like it's pretty, if I think about the letter eight or something, it's got two small circles. Those are strokes. Like the letter seven's got like a, a vertical on the whatever. So whether it's GL or not, it seems like we could have a lot of control over how exactly we wanna. Yeah, that's actually, interesting. I think. Why do we need to do handwritten recognition? Why can't we just do the compositional on actual, typed languages? like just letters from the English alphabet? It still requires, 'cause we wanna learn the strokes, in isolation first. We know we, we can figure out all the strokes in the English alphabet, like all the vertical lines and circles and all of that.

I'm just worried because if you look at, these examples that you're showing like the, bottom right and the top left, they have just different scales at the lower level strokes and yeah, I, think, yeah, I agree that if the, if scale's gonna be an issue, then, easier if you have type letters, how would you get variation into them?

Why we were trying to learn the compositionally, why do we need variation? Yeah. That's why I'm saying let's just take the first character of each glt charact, like the first version of each omni character. It's the same as just stick the type letter. Yeah. No. Oh, I, yeah, I guess just forget about all the versions. Take one version, learn on that version, do inference on that version. It's the same as taking a typed character.

I, guess I was thinking because strokes can be directly reused in like the typed versions, so if we wanted to say, let's say like a little circle or something like that, and that's a stroke, if we were using like a typed version that, that, circle could be exactly reused at the same scale in different letters or different digits or whatever. but I guess if it doesn't matter that strokes get reused. And that was just like, I'm realizing, it's an assumption that I didn't voice, but, if we wanted strokes to not be completely, diagnostic of the Yeah. The high level object. Yeah. Because that seems to be the, part of the compositional is that okay, we can borrow all these common things, and then not, no one piece of it is necessarily a hundred percent identifying of the whole, yeah. Yeah. That's the nice thing about the cups on Logos example, where we have the same logo on different objects. Yeah. yeah, I think we can do a lot of different things. The thing that's appealing to me about the Omni Guard data set is it's very simple. We already have it integrated. it has all the factors. We, we can just look at the first character, forget about generalization, And just te use it to test kind the plumbing of learning composition objects. and then from there we can move on to more complex things like, logos on cups or type characters with, straight strokes reused in different objects or different versions of characters in the oligo dataset.

yeah, there, once we can do composition objects, we can really test the hell out of it and do all these different, combinations and test all these different capabilities. But for now, at least, what I'm thinking here is let's try to find the minimal viable test bed to just see what does Monte learn as a compositional object. Yeah. Yeah. And I think especially I think we're all itching to get a compositional evaluation working during the hackathon. And so maybe one of the main other contenders is the dataset that, that you developed, Scott, which is obviously great for being much closer to what we often describe. And therefore I think being a better test bed for things like, okay, now we have a logo with the bend in it and now the logo's rotated, all this kind of stuff. But it does present some challenges in terms of evaluations and plumbing and things like that. In terms of, yeah, we definitely don't have access to the semantic sensorimotor for the, like the logo. Like we don't know which logo is there. I don't think that's like a killer issue because we could, let's say we're presenting this one as opposed to Menta, yeah. Mug with Menta logo, we obviously know what the high level object is that we're expecting and we know the Thousand Brains project logo is somewhere in the, in the visualization. So we can at least just look at, okay. Does the learning module, lower level learning module at some points represent TVP logo, whether or not it's always exactly matching when it falls on it. And then there's some kind of unsupervised metrics which are discussed later in the document that we could supplement that with to just generally get a flavor of how well it's performing. So yeah, so to, summarize that, I think from a kind of supervision and semantic sensorimotor frame of mind, I'm not that concerned about this one. Maybe my biggest concern is actually more about the, 2D nature of the logo and the challenge challenges that represents because, if we learn this logo in 2D, like we don't really have a good way of representing 2D objects right now. We don't extract edges and things like that. And so what we would essentially have is like a kind of a bunch of colors at locations in a 2D plane. That would be the model. And then when we are inferring and it's wrapped around something we would, I guess it would be able to predict colors again at particular locations following movement, but it wouldn't really be using morphological features of any kind. That's what the 2D sensorimotor module would be for. Yeah. So we would need that basically.

and so in terms of kind of an action plan for the hackathon, I think that puts me in favor of Omni GLT of the two, but obviously this is something we still wanna return to at some point. Yeah, it could be a way to work up to it. So we could test the basic composition of modeling plumbing in Monty on the first version of each character in Omni Glot, and at the same time implement the 2D Sensorimotor module and potentially, and think about the tension mechanisms and stuff like that, and then test on this data set, afterwards. Yeah. Yeah. If, we make different arrangements of the 3D objects in a scene, doesn't that solve the problem of because we have semantic sensorimotor and we, this just, we already have models of the objects, so that's fine. Is that what you have? Okay. Sorry. Yeah, no, it's a great question. So yeah, so a potential data set, which I've provisionally called Inception YCB, is that we embed.

certain YCB objects and other YCB objects so that they're partially visible, which yeah, kinda like you're suggesting Rami, so that this was meant to be a dice. I dunno what happened, why that got deleted anyways.

but, so wait, a dice just turned into a strawberry?

I had, I had two, I had the dice, then I had the strawberry, but then I wasn't happy with the rotation of the strawberry, so I thought I deleted this and put in this new one. But clearly I deleted, I deleted the other one.

anyways, the, but yeah, exactly like you say, Rami. 'cause then if we construct them in habitat, so we wouldn't construct them in some, blender or whatever. We would literally construct them in habitat by adding the objects and series. Then we would have the semantic sensorimotor information. We would also have, morphological features because everything is still 3D. Yeah. and then, yeah, so this would be the, another option, or I think the main, you can also, another option is the, data set that Ramy put together and that we set up at last year's hackathon of the dinner set. I think the main, issue with that one is. I think we, would want, Scott's, CIC policy to be working well because those kind of objects are separated in space and so to stick co onto them, wouldn't find it would still, why would the, you find that, make a difference between that? Like once, if we're on the bowl, how do we get onto the mug? Whereas here, we would smoothly move over both objects?

You mean before we have the Cade policy? Yeah, before we have the Cade policy. Oh, okay. I thought you meant the Cade policy wouldn't work with it. Oh, no, sorry. I, as in we would want the Cade policy in order for it to work. Oh, okay. I understand now. Okay. I guess in theory you could crawl over the table, but I just feel like it's gonna go everywhere. yeah. I guess just to, just to note that if we're talking about hackathons pos policy during hackathon, I think inhibition of return right now is just going to more likely zip between objects, like alternate observations on objects than it is, stay on one for a while and then move to the other. so I'm not sure that's gonna be different than, five time. Yeah, like in the short term. Yeah. we could also supervise the policy so that, it does a few steps on one object and then just moves on to the other object. Yeah. Since we're doing supervision anyway, for, the, for what is lower level and what is higher level.

But I wouldn't do the, dinner set because we wouldn't have the semantic sensorimotor. but if we have yeah, habitat, or we just have three objects like in a triangle or like some, and then we just change them, we would have different arrangements. We could control the scale and we could just move between the objects and build the compositional, object.

Yeah, I think the main thing is it would just be nice to have a clear concept of kind of child and parent, but so that, yeah, it's not just two objects next to each other, but it's but yeah, it could be a big triangle in a small circle or sphere or whatever.

but yeah, so I, I think the main challenge with this one is it would just take a bit of tinkering to figure out the positions and stuff that we like, the locations that we need to initialize the objects in order to, have them look like this. But maybe it's the kind of thing where we could work out. We can import the models into something like Blender based on their dimensions and stuff, work out what the relative assets between them needs to be. And then one shot it in, in our configs.

I think this one was my least favorite, but, if we do go with it, can we just use the multi object setup we already have and then say the composition object is how the multiple objects are arranged in the scene and the child objects are the individual YCB objects.

Yeah, I mean we, yeah, we can, Yeah.

delicious.

There we go. I see.

yeah, and the, yeah, I guess if we use the distant agent, it probably could between them.

'cause the surface agent's kind of stuck on an object once it's on it like a continuous surface. But yeah, I guess the distant, which one do we currently use on that? The, I think it is the distant agent. Yeah. And it, do you remember if it moved between objects? Yeah. 'cause we were trying to at one point implement a policy that prevented it from doing that. Okay. but yeah, that's a fair point that, Yeah, we would just have to adjust it slightly. 'cause I think right now it adds objects randomly, so we would just need to, that would be a pretty simple change, but we would just say kinda okay, this set is computational object A, this set is computational object B.

and in our definition of the object, there's nothing saying that void cannot be like that. It has to be continuous in the void. It can just be, they can be spread out. No, not in the definition of the object. It's just practically in terms of the policy that right now, every time it goes into the void, it's oh, and then it goes back. But, so that means that if the objects aren't continuous either through like occlusion in visual space or their surface is touching, if it's the surface agent, the policy won't go from one object to another.

is there a problem with supervising the policy? Like I mentioned, like after, after terminal condition on one object. So just manually move the agent onto another object. I guess it's just, I feel like split myself with policy is always surprisingly difficult to implement. It doesn't seem to me like something that would be easy to add, but I could be wrong. Okay.

But yeah, I don't think it's like a issue by principle or whatever. 'cause obviously yeah, we're introducing a lot of supervision and stuff here deliberately.

okay. Yeah, I think this is the main thing to talk about and for us to just all think about a bit and, then that can be maybe something that we decide over the next couple days or on Monday is, is which data set to, to, to work with. And but yeah, I think my preference right now would probably be Omni clot.

and then did you have some more on the supervision and evaluation part? I guess we already talked about most of it, but Yeah. So on the evaluation, nothing kind of crazy, but just, yeah, just to show more concretely in case it was an obvious, like in terms of why it would be useful to have the, semantic, sensorimotor information is just that we can have plots like this then. For the, for both the low level and the high level learning module. and I guess for the high level learning module, actually it'll just be during a given episode, it'll be the same object composition object being shown. But it would be interesting to be able to see, okay, the low level learning module, oh now it recognizes its stroke so and it's oh, interesting and thinks it's stroke and it's oh, that makes sense if it rotates that stroke. Just stuff like that.

is why it would be helpful to have that, that information.

a few ways we can get around that is, so one is just, like the thing that Hojae added, we can scrub through. and I think this is probably good enough for, the hackathon, 'cause it's gonna be only a few episodes anyways that we're dealing with. We can scrub through and see what the sensorimotor modules are actually seen and just as a human kind of manually say, oh, okay, yeah, it thinks it's on stroke. And that's clearly what's being observed by the sensorimotor module.

if that, if that makes sense. And then I just had some notes on in the longer term I think this will become more important, but in terms of other metrics for performance. Looking at kind of prediction error is generally I think just something that will be useful, particularly as we get towards eventually we want it to discover composition without us telling it what that is. So it, the semantic sensorimotor isn't just a case of like practicality eventually. It's something we don't wanna rely on anyways. But, but you can imagine if the representations are good, then obviously the prediction of what, the learning module is gonna see and what it actually senses is going to, those are gonna match well. And so that prediction error on each step is gonna be low. and so this is just showing an example where kind of at the start, maybe the most likely hypothesis is wrong. the kind of prediction error is high, but eventually the most likely hypothesis, that kind of emerges is quite good one. And so we get very low prediction error and then we move on to a logo. And in this case, because we don't have a compositional representation, suddenly this spikes. But if we know that we, it's the mug with logo, then we would expect this to be lower. And this is where we could also look at. Okay, like to your point earlier, Scott, if we just have a high level learning module that learns mug with logo as like a giant object, but it has a course level representation, it might do an okay job. But it would still be up here. Whereas if we have a true compositional representation and the, high level learning module can work with the low level learning module, and then the low level learning module, if we look at its prediction error, it might be much lower. and it could actually, the high level learning module could tell the low level one we're moving onto the logo, get ready to make predictions at your level of resolution. Yeah, I think that's a great idea. I think we should try and add prediction error as one of the measures. especially it will be generally a nice measure also for symmetry and things like that to know what's the prediction error of the most likely hypothesis at any step. 'cause like even if the poses off something, it, if it's the metric, it will have a low prediction error. And that's ultimately what matters in any application, whether you can predict what you're gonna be sensing. yeah. And if that's low, it doesn't even really matter anymore. what's the semantic sensorimotor saying or how did we Yeah. The objects and stuff like that. So yeah, I think, I don't, I wouldn't say this is like a long term thing. I think this would be a great thing to add during the hackathon. Okay. Yeah. and yeah, I think generally, yeah, we will. Want to manually inspect what's going on and all that. But it would be nice if, at the end of the week if, that is a project we pick, if then we have a benchmark thing with measures that we can rerun and have as a test bed to see how, those measures are affected by any, changes we make in the future. Yeah, and I think that's what's nice about it is it's like this would be useful information when you look at it like this kind of broken down across the episode, but the mean prediction error would also be an interpretable kind of summary statistic, which as you say, that's something that we could have in a table and we see how that changes as we introduce new architectures and all that kind of stuff. yeah, this is just for kind of clarity, just showing an example where predictions are particularly bad.

but yeah, so that was most of that. Oh yeah. And the last kind of figure I had, was just on this kind of attention type thing where, oh, sorry. if we can talk briefly a bit more about the evaluation measures. Yeah. I think one difficulty, if we want to evaluate the representations of the lower level learning module. Is that it will change throughout the experiment. And it's not like with high level one where it reaches the terminal condition and then it'll be like, oh, did it classified correctly or not? But we need to think about like, when do we measure it? How much time do we give it after moving onto a different subpart, how confident does it need to be? Like I think that will be one of the major challenges with modeling, composition models. Right now we have the same kind of confidence threshold for the lower level model to send something to the higher level model as we do for the higher, for any model to converge and terminate the episode. So one, we'll have to think about the terminal condition and how to adjust that. 'cause right now if we have a hierarchical experiment, we basically, the first time we're sending an observation is like when the episode, when the experiment ends.

We have to rethink that and then also rethink like at what points do we check the performance of the lower level learning module and how would do we make sure that the high level model gets enough input?

given that it takes some time for the child object to be recognized after it's recognized, you still have to keep moving on it or integrate that past information into the model, So yeah, I think in my mind those are some big questions that, that would be good to figure out. Yeah. Yeah, especially I think the first and the last one, I guess the second one I feel to a degree, like for example with the prediction error, like if we just measure it across the episode, it should always get lower. If, it's doing a better job of e even if it's gonna spike for a bit when we go into a new object. if, the com like if the hierarchy is working right and the high level one's telling the below level one, oh, I think you should be on this object, ramp up your evidence for it, then we should see that kind of metric come down. And then of course we can examine it as well and, see that okay, yeah, it's the number of steps before it, it drops. So you mean for the lower level, like basically for composition objects, we won't be looking at ground truth anymore, but just prediction error at least in that. Yeah, if we're using the prediction error. and then if we do have access to ground truth, then we could look at like what proportion of steps does the, label in the low level one ma, like the MLH and the low level match, what's actually being observed. And so 75 would probably be pretty good, whereas like 10 would be pretty bad. Like it's taking forever to, and those are measuring the same thing, but one's a supervised and one's an unsupervised measure. Although that metric can be hacked by adjusting the policy to just stay on that child object after it's been recognized. yeah. That's true. maybe, and dictionary can definitely be like, I think that's something that people, whenever Carl Friston explains, free energy principle, whatever, people are like, oh, but then wouldn't you just go into a dark room and close your eyes because then you can like perfectly predict everything. and then he is yeah, that's what people tend to do for eight hours a day, but before they start predicting hungry. I don't know. Anyways, it's a tangent.

What you were gonna say something wrong? Yeah, just a quick question. why do we need to evaluate the low level, elements? If, we're just getting the high level correct? 'cause I think we want to see, we just wanna understand how the system's working and that it's working like we think it is. I think we only look at the high level that we convince ourselves, oh yeah, everything's working. And then it turns out it's doing something really strange. I think it might, it's a fair question. We might not need to look at the ground truth comparison to low level and we probably, it probably won't be realistic in more complex. Scenarios very soon anymore. Like it might just be that we use prediction error at every level and then only classification error at the highest level. But or, in the very long run, just look at how well the system is interacting with the world, like the motor outputs essentially. but yeah, I think, like Neil said, as a first thing, it would be good to just for us to see what's going on.

yeah, I think this would be nice to be able to inspect and figure out that it's doing the right thing. But I'm talking about more of a, like a benchmark, is, so the, does, the performance at the lower level LMS go into the benchmark? The total number of accuracy that we need to report for the compositional, because I could think of example, ion error would 'cause Yeah. To the 'cause that's where it's if we wanna know what we're gonna be seeing when we go on the logo, we can't rely on the high level one. Otherwise, again, we're just trying to learn like a giant supermodel of everything in at one level. So I think if we want to see compositionally doing something useful and interesting, then I, it doesn't necessarily need to be in a supervised way, but I think we do wanna measure and report how well the low level learning module is doing.

yeah. Another measure I would add, like what is the prediction? Yeah. Another thing I would measure in the lower level model is. How many of the steps actually sends something to the higher level model? So basically trying to get it to be confident, most of the, confident enough, most of the time to send information up.

'cause maybe it's not something to be concerned about, but that's my main concern at the moment. that it will just take too long before it gets confident enough to send something up the hierarchy.

and that's something we might wanna try and improve in the next months. Yeah.

Yeah. The pandemic, sorry, go ahead. Oh, I was just thinking, yeah, I mean we can see how the hackathon goes, but if we focus on adding the prediction error metric, we potentially try and do both Omni Glot and Scott, your logo with, object dataset because, yeah, I guess, yeah, there's still the 2D stuff. we could have two teams. One team cannot, one pitch could also be to implement the 2D Sensorimotor module, and then at the end of the week it's evaluated on the logos, on marks. Yeah.

I don't think this is a topic that like a huge team can work on. 'cause it's hard to break down.

yeah. Sorry, Rammi, were you saying something? No, no, it's not a point. We can also break into teams and do the same challenge of positionality and then, oh, just see who solves the brain first. Basically. Who solves the Yeah. Adversarial.

since there are, I don't know, trophies at the end anyway, so Who, gets the lowest prediction error and someone will put their Monty in a dark room and win.

sorry. yeah, just the, last thing I'd drawn a bit on was the, this kind of attention type question stuff where, you know, from the sounds of it, I think we agree that yeah, basically any learning module in the hierarchy, at least to a degree, can in theory have a model for a particular object. as in we'd have TVP logo here. We'd have mug here. We definitely have mug with logo here. We'd potentially have mug with logo here as well.

and then it raises this. The questions about like we were talking about at the start, how we learn that and also how those, we get different representations given that at any given point in time the kind of sensory input is potentially con consistent with multiple ones of those. if you're looking at the, kind of surface normals and curvature and stuff like that, and then maybe you'd recognize, oh yeah, this is part of a mug. If you're looking at the edges and things like that, then maybe you'd recognize it's a logo.

I think, personally, this isn't something we need to solve now, but it's probably a kind of a multifaceted thing based on things we've discussed in the past where, different sensorimotor modules would focus on detailed versus course stuff, and that would be reflected in what the learning modules learn. We've also talked about how some learning modules might get more input, more consistent with 2D representations. So both the features, but also the movement information coming in. And then Jeff talked about some stuff around how we could window attention, which also ties into kind of policies that, that would affect Okay. This, learning module is mostly getting a narrow input and so it focuses on the child object and things like that.

but yeah, if nothing else, I just thought it was worth. I was being aware that to a degree this is still like a, an open question and something for us to think about.

and yeah, that's basically, everything I had for today. Yeah, even, not even just that, it's consistent with both, but also if, the lower level model strength to recognize the logo and it moves onto the cup, there'll be something that's inconsistent with the logo model and we're not really getting that input right now. So like we're saying, oh, we're not on the logo anymore and it's not like we're now in the void like we currently are with the YCB objects. If we move off them, we just don't send it to that learning module anymore. So I think even just inference on the child object disregarding there might be consistent with two objects is, difficult 'cause it will get all the surrounding inputs as well that are not on the object. Yeah. As in you're saying if we move on to the, off the logo and onto the mug body, but we are narrowing the attentional window.

if we don't have an attentional window mechanism. Yeah. 'cause it, I mean 'cause then wouldn't it just then be okay, now I'm on the mug?

I guess so especially if we, if it knew the mug and the hierarchical, the composition object had learned mug at locations, then it could also help, get it in. Yeah, you're right. It shouldn't even keep thinking that there is a logo. It should just be like, no, I'm not. Yeah, it's that out of reference frame movement thing. Yeah, you're right. Yeah. Which is why, yeah, it might be still something to return to at some point, but, cool. But, yeah, it feels like we've at least got enough of an idea to start on something next week. I'll still try and clean this up a bit, so that I can share it, but, but I don't think there's any secret knowledge in here that's, either something that you guys haven't already thought about or we will work through next week.

I may have missed what are we doing next week for we're all each going to present an idea or, is it like an actual presentation or just, just look through the spreadsheet and go through the ideas that are there?

yeah, so the idea is that everyone. A project idea from that spreadsheet. ideally not more than one per person, but if you really have two great ideas, I guess that's, okay too. but try to figure out which one, could be the best to pitch. And then you just do a, as short as possible pitch of that idea, like two to three minutes. You don't have to prepare slides or anything focusing on what would it be about, why is it important to do that? what would be the impact of it and the kind of positive outcomes.

and yeah, after all the pitches, we'll see who might be interested in which project and try and form groups around them. Some of these projects are very involved that maybe, if I work on the stacked one, maybe, it requires two people to work on this full-time or three people to work. Like some of these, projects may not, I don't know if I wanna give my full time to, this, the positionality one, I might not have time to pursue another one. so is this, are we, is this a rule that everyone should have? One project, one, Ideated. Yeah. you can be, you should have one main project. You can help out on another project if, the, if people wanna consult you for your opinion, but you shouldn't be like, doing major work on multiple projects.

Okay.

Cool. That's, and yeah, I guess with the, pitched ideas, kinda trying to think about whether this is a good project to do with a two to three person team Yeah. To make the most of the teamwork nature.

yeah, that's everything for