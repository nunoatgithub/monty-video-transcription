Hi Colin. Welcome. So we're here to talk about the hypothesis updated prototype.

Discuss further. Yeah. So what should be the next steps? like reading your summary? you mentioned that, like phase one, two makes sense. Phase three, like what are your questions? We only had one or two exchanges about it. Anything else, we should synchronize on or, I mean in terms of Yeah, like the overall workflow plan. I think that makes total sense. start on a different branch of fork. Start trying to. Develop a GP background and CPU background, get it all integrated.

yeah, so, what, I'm thinking, is specifically like on your TBP monthly fork, if you create a branch, and then, the other thing is if you, create a draft pull request with that branch Then me as a maintainer, that allows me, pushes to them. Okay. And so that actually can facilitate like me adding something or so like we're actually working together versus just me trying to work through you. Yeah. Understand. Actually say Hey, how about this commit? How about this commit? so that might be helpful. but I think do then my fork create a new branch and go ahead and start a pull request and we'll just kinda work through that pull request flow. Yeah. As a draft. because that would allow.

so that would allow, I believe if we look at a pull request that we have, what's an external pull requests one of them.

Okay, here's an external request. So over here there is a, their maintainers are allowed to edit this full request. And so I think it gives me branch permissions to the branch. hey, can we try that? Actually, let's just try it, see if it works. You could, yeah, just make a branch, make a, pull request. See if I can see if I can push to it. That'd be a nice experiment. Yeah. Yeah.

Stop presenting.

Do you want me to do that now or follow up after? I was gonna say, do you think it's a good use of time? Let's, I, have some, things I wanna talk about. Okay. Let's talk about that. Maybe we should prioritize. Let's do that. Cool. Yeah. yeah. Let me share my page.

Yeah. So I just started looking. Making it a little document to talk.

Okay. Getting used to the Google, Google workflow.

I can see your id.

So anyway, the key thing I wanted to talk about, 'cause basically, when I first proposed this. I was looking at trying to batch operations across lms and then that kind of initial post in the comments with Viviane, she was suggesting that keeping the kind of LM more separate is more conceptually better. And because that runtime is actually not easy to predict exactly like which LMS will get what inputs at different times. So it might have some more complicated structure to try to batch across lms. So that's what led me to think about doing this different cuda streaming approach.

and so I started trying to outline that also in the, in the comments on the separate, posts that you created. but I think that's where I would just maybe start to change a little bit of the, the interface between the Monte experiment and thes to the hypothesis. Okay.

Key thing here is, watch. Okay. Do you feel like it'd be helpful to, for me to explain what a Cuda stream is or do you get that from the post I made? I think, I I, get it. I think I would get it out of from context. yeah. If you want to, you're welcome to. No, that just, I think I'll. Let know, feel free to ask questions if I, it's like a coulda threat that runs in parallel, but basically you can put kernel operations per stream. Yeah. Yeah. And so it would benefit because we can. The code kinda keep the LMS more separate. They could launch the same kernels, which is with different inputs or slightly different kernels. they can all do that fine on their own stream. But the GPU itself can actually, schedule work, from all of the different kernels across all streams that has available. And so we can do, GPUs are designed to have really complex scheduling to maximize throughput. if one, Warp in one kernel, starts a memory access that's gonna take hundreds of cycles. It'll just, switch that out for a, a different warp that has compute work that needs to be done. it just cheap user design to just fly through all of the work that they have stacked up. And so we would still benefit from parallelism across the lms, but it would keep the, keep them defined separately and allow for a little bit more variation in behavior. I know you were saying in response that, based off of the data flow diagram that you outlined, we want LMS to all be doing the same operations at the same time. We don't really want different flows for the different lms. I maybe misspoke about the same operations because. That presumes that all LMS are the same implementation. But they could be different implementations. The internals of an LM could be different, right? Like one of them could be the, current one feature graph lm, but another one could be LM using like grid cells and whatever. And we're trying to compare them, right? Or we have a mix of 'em. So, even so, I miss, I, I over constrained and misspoke. It's not that the same operations but that are all going through the same phase. But each one could have different operations. Yeah, that makes sense. And by phase, page, they get inputs and they need to produce outputs before moving on to the next phase. That's what, yeah, no, that makes total sense. I think even in my original kind ideation of the Stacks implementation, we would group LMS based on their type and or input I, I didn't really figure out the details about whether they all need to have the same input for the kernels, but, That would also be a little bit more complex to add that overhead of figuring out how to group them into the proper buckets, right? That could be stacked. And so kuda streams just avoids that, complexity. So yeah, we can have different types of LMS processing different inputs. or like you say, like some could be doing no ops or, just like other kernel operations, and we could still benefit from some paralyzation. It'll be less than if we stacked it. that's the most efficient way to do it. But I think the flexibility that comes with it, I do think that's a more. I think it's a better approach for Monty, like I think that maps to the existing architecture better.

but what comes with that is basically we need to switch from the sequential, blocking loop that we have right now for how we process hypothesis updates to a non-blocking ball. So right now, the Monty experiment well loops through each LM and call the updates, wait for that to finish, and then go to the next lm, right? We wouldn't benefit from any parallelism, even if we use kuda streams, if we did that because, one stream finishes before the next one begins. So basically we would just need to switch to something that's, basically got dispatch phase, right? So we can obviously work out the details more. This is just kind a quick pass and basic structure, but, we just wanted to launch the kernel update or, like the launch, the pipeline of, kernels. For each LM and then return immediately, right? So it actually goes through all of the LMS and dispatches the kernels, right? And then some sort of just kind of synchronization waiting phase while that work completes. And the Monty Experiment knows not to do anything until that's done. But once it gets that update that it's done, it can then, read the results, move on to the other steps in the experiment, right? Yeah. It's a, pretty common Pattern. pretty straightforward, but it will just require a little bit more refactoring of the code. Like I was starting to take a look at, the different calls through here. like step learning modules. Yeah. I. Right now we just, call this LM step method. Instead here we'd need to do like a dispatch step and then maybe rework some of the logging and such outta this. And then, do, wait, so one thing immediately pops up here is that dispatch step, aren't we getting Yeah. 'cause we're getting, we're still getting sensory inputs for every LM right line 81 sensory inputs. Collect. Yeah. But that's still done in series, collect those inputs.

yeah, so I think it, yeah, we'd have to figure out exactly what operations are worth parallelizing. yeah, I didn't really look into sort of the details of some of these other pre-processing steps. If they're pretty quick, it might not be worth the time. But we could also look into whether we can paralyze more of the pre-processing. Can I do the full pipeline? yeah, I guess I'm just wondering if in the, parallels switch should happen at the step learning modules level, like it's an alternate step, learning modules implementation as opposed to reusing it and, swapping out calls and parallel sort of thing. That's, the question that I have. Yeah. So instead of having those. Learning module in step learning modules, like we do this kind of a level up from this function? No, What I mean is, okay, I see. I see what you're saying. This is why you're talking about non-blocking because the dispatch step will add an unblocking call. And so if we have a CPU backend, you can still do the same thing. You just essentially collecting it later. Where does this, where do you collect the, where's the gathering set? Yeah, so this in a super, high level that I was doing right before this. Yeah. I kept the same code just to kinda see where it would insert. But basically after that, you'd want these calls to all return. So if the loop finishes before any of this work is actually done, all that happened as a dispatch, then you probably just have a blocking call after that. So once all of the LMS have dispatched. We just wait until they're all done. There's more complex ways we could do it too, right? Like as soon as one LM finishes you go ahead and start with some of the work. But I think that could come later. I think it's much more simple to just wait for the backend to say that it's done processing all the work. And then you can go ahead with the rest of the code as normal. You would just need to add some call to, to read the results back from the backend. 'cause that would also probably, that will need to read the. Whatever results you want, right? whatever hypothesis, values or everything, you have to actually read that from GPU memory back to CPU. So you want some sort of explicit read call, and then you can do the rest of the code. Okay. Yeah. So scatter block, scatter, gather, and then block and then gather. Okay. Yeah, that makes sense.

I'm, just, And this is step learning module. Okay, got it. Because that allows us to, okay. Yeah. Yeah. And this, yeah, that's helpful. certainly, you know the Monte code better than me, right? Like I, I was just going through and following the functions, and at least for the experiments I was running, to get a sense of where everything is. It seems like this is, because this is where the looping over the learning modules happens, Where we would start with adjusting how we dispatch the work. so would it be helpful to you? so I'm just thinking about, more generic processing, like when, if in case you did wanna stack things together. Because with this approach, dispatch step is pretty much, if I'm understanding correctly, a dispatch step. We'll, create a GPU. Stream, was it? Yeah. The stream. So, dispatch step as a stream could, does, is that what the output of the outcome of a dispatch step is? I actually think, because, correct me if I'm wrong here, but my understanding is it's a pretty set pipeline of operations for the hypothesis update steps. I, I, guess what I'm asking is, you said that stacking things can be more efficient than adding streams. If we wanna stack things, then we would probably want to scroll up if we wanted to gather all the sensory inputs all at once and have a set of sensory inputs and LMS and then stack them together and then dispatch that stream. That would be, yeah. So yeah, what I'm saying, stack them like that's in like my original kind of proof concept code where we're actually doing one kernel dispatch for. Each step in the pipeline for all of the data across all lms. So in that case, we wouldn't even need multiple streams. Like it would just be one cuda stream, just, one Yeah. sequence of kernels. And just one kernel dispatch for the entire operation. Yeah. and that would be more complex because Yeah, we would have to accumulate all of the data. Into kinda one continuous buffer to pass into the kernel and so there's some overhead, right? Whereas with this Cuda Stream approach, because each stream is handled separately, has different kernel dispatches, you can have different references to the memory objects and everything. It's much simpler to implement, but it might be less efficient because you're dispatching multiple kernels, right? Which might have their work paralyzed by the GPU scheduler. But you're not actually defining, like thread level parallelism by doing one giant kernel dispatch that can just do it all in one go.

does that make sense with the difference? Yeah. that makes sense. and the reason I'm poking at this is because if we're gonna change the API, I'd rather change it once than change it now and then be like, okay, and we're gonna add stacking and we have to change the API again. Sure. Yeah. Yeah. And so that's what I'm saying is even if in this initial step we're not doing any stacking. I want the API. Like the, I want the swappable backend component to the API for that to support stacking if we want to add it later. Yeah, no, that makes total sense. So then, so that's why I'm looking at okay, so in this, as you wrote it currently, this is super helpful 'cause it like allows me to co completely understand what you're talking about. So in, in this implementation. We wouldn't be able to do stacking because we're dispatching a could a stream correct my vocabulary, please. dispatching a different, like a kernel on a different stream. Okay. So we're dispatching a kernel on a, new stream with every dispatch step, right? And sensorimotor inputs are just the inputs for that lm. So we're essentially dispatching a kernel stream, a new kernel stream per each LM with just its inputs. And it is my understanding that there may be a feature optimization that might, be possible where if we were to stack, like learning modules that are doing the same thing, do they require for stacking? For stacking, I'm assuming we would, that just the kernel needs to be the same. The inputs would just also get stacked. Is that true? Yeah. So it would be, yeah, like a step where we, Put all of the inputs, stack them into one. Yeah. Input buffer. and then one kernel. Yeah, so that, API would require multiple sensory, like an, like a vector of sensory inputs and not Just sensory inputs for the cell. So we would have a vector of sensory inputs and a vector of a lambs. And then in the current implementation, it could always be just a one like vector of we can always, it's the batching rule. We can always have a vector of one, but it's hard to introduce it. List late after the fact. But that's what I'm thinking about this API where it becomes where to make the breaking point. That's good to consider that. also, if we're actually stacking all of the operations across all lms, we also wouldn't have to do this sort of three phase approach where you do dispatch, blocking weight and then read, because we just have the one. Pipeline, right? So you just dispatch a kernel and then once it's done, read the results. But kinda like we were saying, we might have to do different subgroups, right? Like we might batch LMS of a certain type together, or ones processing different inputs, in which case maybe we would still need a, some level of a sort of dispatch step, right? Or maybe each group of LMS that are processing together run on one stream. So yeah. I don't know, it would introduce this level where we separate LMS into different groups and then you follow the same, this kinda same dispatch API. And, it could be as simple as, and this is why I was thinking and to screw up a bit, why this, backend might be involved with, like, where do we make the interface at? Because it could just be like, here's all the learning modules, here's all the sensorimotor inputs. Then the initial, even the GPU initial implementation is just a for loop of single dispatchers. But it opens up that, that, grouping then would happen in the future would have to happen inside, inside that, that API not before it. so, yeah. Do you think the grouping should happen before. Do you have enough information to do meaningful grouping before handing it to the backend? Or does the backend, should the backend do the grouping? I guess that is the question that I'm trying to pose now. Yeah. I don't know, because I think basically we could group alones like as long as they're doing the same operations. And we could figure out how to do it with, different inputs, right? Like even if they're processing different sensorimotor patches, as long as it's the same type of LM doing the same sequence of kernels, right? We could stack, them, but like in Vivian's comments she was saying, it's hard at runtime to know which kernels are gonna be processing which inputs, like some might, not be processing. The same input, in which case you might not wanna include in the back. So it does seem like almost per step, the grouping could change. I think that's what we would need to figure out. 'cause it'd probably be more efficient if it's more like experiment wide. We can do the grouping at the beginning and then have that be consistent throughout all the steps. Versus if at every step we're like doing a different grouping of, lms, I think that would start to get complex.

and then. this is a, naive question, but if an LM doesn't have inputs for a nu up, would you just zero in, zero out the sensory inputs and just, or, would just zero out the sensory? No, because we don't Would you just, would we just ignore the outputs? I think, would that be more efficient than trying to regroup things? Yeah, I think zero out. Then have a, you could have a simple case in the kernel itself for it, like a flag or, if it just sees, it's like the input is just the, it's just zeros. It, skips doing the work. That would probably be better than doing a whole new grouping if it was just to skip like a noop for one lm.

yeah, we don't ask that because that sort of just handles the The not everything will be processing. It's okay, then yes. So, that will handle that case. So the only other case would be, the other thing is so, lms, I'm just trying to, this is a sensory. Yeah. These, this is sensory inputs, but they could get NLM in the future could get.

So as, I'm building the CPV one, right? so what can happen is a learning module, learning modules, inputs, let me pull this up so I'm not trying to do this one memory. Do you wanna share? Yeah, I'm gonna share the screen. what am I looking for? Is the, I'm trying to say.

Okay.

So, this thing, okay, so learning modules received, okay, so the things that are getting delivered, okay, so this is the interesting, this is what I was gonna highlight. when the inputs into learning modules. Will be like the outputs from the sensorimotor modules. These are, gonna be stable. So like once your sensors are hooked up to your learning modules, it's always the same. Same. The difference is the inputs from the previous steps. Learning modules. what this encodes is hierarchy of learning modules, so, when learning mo, so learning module outputs. It can output, zero, one or two cortical messages, and so it can output no messages.

it can output a, goal or it can output a, what do we call now? Preset percept, like a perception. And so a goal would go to a lower level learning module, and a percept would go to a higher level learning module essentially. It's like a top down goal driven stuff, and then there's bottom up, like assembling a hierarchy of inputs. Does that make sense? Yeah. so the reason I wanna highlight that is the inputs, this will be constant, but the inputs to the learning module actually might be different width if it gets, because it's gonna get a sensory precept and then it might get a goal from a higher level learning module. And so so sometimes it might get no sensation, zero inputs. It might get percept from a sensorimotor module that's stable, but it also might get additional.

There's some bounded count of how many messages. It depends on connectivity. But, so the inputs will be variable with no input one or more To some finite amount. But that, but, that any step, that width can change. How does, so I just wanna highlight that. Yeah. I think at a high level that should be fine because we're already envisioning a, input processing phase, right? Where we stack the appropriate inputs and kinda assuming these would be, variable sizes. Yep. in prac, like when you get into some of the lower level details, there definitely could be some. Overhead by having particularly if you're stacking kinda recreating a new buffer and transferring that every time. Whereas if you had fixed size inputs, you could more easily pin the memory between the CPU and GPU and just push to the right place. But it seems like we could even get around that by just over allocating. So if it's a certain max. Input size and it's not an unreasonable amount of memory or something, we would know, give that for each input. We would know that amount because that is a configured connectivity. Yeah. So, that is defined at the start of the program and it's like you can only, get inputs from the things you connected to. So we will know that. Okay, cool. So yeah, that's probably pretty straightforward to get around, I think. Yeah. It's really just more, Figuring out the LM grouping and having that consistent, if the individual input data is, changing step to step. I don't think that's a big hurdle.

and right now just I'm thinking, LM grouping is by type, right? Like it's, if it's the same class as gonna have the same implementations, gonna do the same computations, it's It's, it should be. M before the backend. It should be trivial to group these.

but something backend wouldn't have to worry about. So backend, so the API could just get the grouping and I guess, in the simpler cases I was looking at or just, yeah. One level of hierarchy of lms. But to your point, you guys are envisioning a more complex graph of lms. We probably have to do the grouping. Based off of, like when the LM would be processing data, so like the first level would get grouped together and then if those are passing to a second level of lms, that's not how it's gonna work. So any hierarchy messages are across steps. Okay. That's kinda like the head archy kind of thing. So it's like they're all one level and getting it from. Previous steps? No, There's still like a set up. What, I'm, there is a, let me show a graphic. but it's super helpful for this, oh, let me just trying to find it.

How was it? It was, I think thread. I'll, download it and share it real quick.

it's, it was a give we go.

Take a screenshot.

Put this in here. Okay, cool. it works like this, so when it's getting passed between the levels, it only ever travels It's at, time, at frame one. That's one step. And even if it goes up the hierarchy. There's travel time, so it's step two that one LM will receive the previous steps from lower level and then same thing from top down. So that level L will send down to LL minus one, but in the next time. So what you see every step, but, what I'm saying is every step, all alarms, compute. There's not like a, there's not a hierarchy breakdown. Inside a step. So it's just so every step, every LM gets inputs, m puts outputs, and then go onto to the next step. Going to the next step. There's not like a, in a step we're gonna do level one and then level two and then level three and pass messages between that. We don't do that in a step. it's, gotcha. It's not how it's set. Does that make sense? Yeah, that makes total sense. So yeah, that would be even easier for the grouping. 'cause you can just do Yeah. LMS all in one group regardless of the level. Exactly, that. That's why I was thinking that. So that's why the, framework can just give you the groupings of here are the same things. It'll compute the same. Okay. Yeah. yeah. So it seems like stacking could be feasible with the system. Like it wouldn't deviate too much from the principles and require too many changes.

I do think it'd be more complex. I think the Coda streaming approach is simpler and I just don't know what the benefits would be. comparing them. Without profiling data. Comparing what's the alternative could of streaming. Oh, like doing like the coda streaming approach, versus trying to stack and do single kernel dispatch for all. Oh, I, okay. I would, so, I am not advocating that we try stacking now. Okay. You just wanna make sure we can support that. I just wanna make sure that the API, that we don't have to change the API twice. Yeah. Yeah. And so, what, like when you get a group, just I guess that's, a consequence of that, right? Because that means inside that call if you get, even if you get groups, do you then now dispatch block and get, scatter block and gather in that call? Or do you. Across the group calls. I see, the, I feel like could, we try to have the API support this idea of an LM group and for the initial implementation, it's just every LM is in its own group. We're not trying to, if we don't wanna support any stacking, but maybe that API would be consistent in the future. If we start to group lms, then they just become, that group is now a part of that dis watch dispatch block the results flow. Oh, okay. Yeah, I see what you're saying. my question is, do you, would you want to, so so if we, do the grouping thing, I know we're talking about something we're not gonna implement, so I'm sorry. Yeah.

if we do the grouping thing. Would the approach be then you'd wait for that group compete to, to, finish before starting an next group or, you're, because it's, you're essentially for that group, the, implementation would be for that group stack. One CPU, sorry, one GPU Stream.

dispatch one GP Stream and then wait for that to finish or for that group stack. Dispatch that one stream, then grab another group. Stack dispatch that one stream, and then wait for all streams to finish. And then the second approach, okay, so basically keep the same idea we're doing now for per lm, and then do that same dispatching scheme, multiple streams, but just have each stream be a group. Got it. Got it. Okay. Yeah. okay. I, I think I'm now caught up with what you were saying. The API is a list of groups. And initially it's, I can give you either the list of one group or I can give you a list of one LM groups. Yeah. But it, doesn't matter. Okay. Got it. But the API, you get a list of groups. Okay. Yeah. yeah. At least, yeah, the experiment level. We can design the flow around that. So kind a consistent interface, so then the backend, we can start to change depending on whether we do stacking groups or not. And this allows the backend to use the group if it wants. Basically list of lists. It uses the groups if it wants, or it flattens it if it does not know how to. Do any, except for the CPU version, we would just flatten it. There's no distinction between this group or that group. It would just become a for loop of what it currently is. we might wanna do some multithreading at that level for the cpu. Okay. There you go. Yeah, Some ization we could do. Yeah. But the naive, like current approach of, we just flatten into a for loop and that would be the equivalent of, I'm just trying to think of, if you recall, I was saying, sharing screen, this bit. Where I was like the, Q and CPU backend, like when, whenever refactor it that want the default implementation be the Q implementation. And so the default implementation of the SAP, I will just flatten that list of list into Forough and be exact the same code, basically. Yeah. Yeah. Start list. Yeah. Okay. It sounds plausible. I'm sure we missed something that will become, I'm sure, apparent as soon as start writing code and it's like this will never work. It's somewhere only a half hour in. It's a pretty reasonable somewhere.

okay. What else would we like talk about? Because I feel that's, not impossible and it seems like we can figure out a way.

the rest of it, I think is starting to just get more into the, into the weeds about what all we need to change. Like I was starting to look at the, the stack of functions before you actually reach the hypothesis updater, right? Like step learning modules. The matching step. Okay. Compute possible matches, update. we'd have to figure out how to do all of that in like a non-blocking dispatch way. That, that's what I wanted to ask.

which Kers work? What is the minimum set of, operations? Because for the prototype, yeah, we should just use the minimum set just to make sure that it Like end to end can run, without necessarily trying to make all the kernels work, make the machinery work. So it's just so just pick the kernels that are functioning and, yeah. So of the ones that I. Shared in that post. Everything is working except for, KNN with the caveat of a couple are like not quite as, consistent. So like the custom distance is Accurate to like a tolerance of e negative two, I think. Whereas the others are all to e negative five and I'm not really sure why custom distance isn't as consistent. Okay. There was a bug with the pose, evidence kernel, and I think for that, but it's also now it just has a slightly lower tolerance for it to pass.

so it's, Gotcha. But the kernels seem to be functional and I give pretty much the right answer. Okay. Everything except, K on. Okay, okay. So that, gives us, so that gives the target to what, the GPU hypothesis up there can swap, can say that again. That gives a target of what the initial implementation of the.

GPU hypothesis update. Yeah. I think the, thing that's missing and in my understanding of yeah, how well we could drop and replace. like this proof of concept that kind of took the core computation, made sure made that work, made it work for the trace data that I was saving through like the profile hypothesis update. But, I feel like there's still gotta be edge cases and like control flow linking these computations. Okay. I haven't worked through like how that would all map to A GPO implementation. Like I once I got to the level of breaking into these different operators, I lived at that level one and implemented the kernels. Yeah. But, some more work would need to go through to actually think about, okay, if we're actually doing the full end to end, you give a input, sensorimotor, and then sensorimotor reading and then, final hypothesis, update outputs. probably need do some work to chain those together. Is that something you wanna do? Not, or is that something you wanna do offline? Oh, we can start to go through it and talk about it. is there anything else you'd rather talk about?

because Yeah, I agree. Like it sounds like we have a idea for the high level API. And that's still not the thing that you need. Yeah. 'cause so, the thing that you need is actually. The, nitty gritty detail of swapping out, like what object can we swap out or Right. What we have to change to make that possible.

UMIs.

This is just the multi code base. okay.

So yeah, I don't have a specific question. I just need to spend some more time going through this and, thinking about how everything chains together.

but yeah, we can do that right here because the other thing I'm wondering like. We have the hypothesis update. are all the operations there? There's, I remember there's, and then the, is there like one, and then this, what's, in the, there's some things are in the displacer and some things are in the update, right?

but what we're talking about even is we're not even talking about, it sounds to me like we're not even talking about an alternate hypothesis update. We're talking about.

Actually inside the hypothesis update, there is a different implementation. so, let's say, yeah, I mean I think this could just be a different hypothesis update. I think. I think it's up to us figure out what, what makes the more sense. the reason I'm, the reason, I'm thinking is because if you look at, resampling hypothesis update, that's already an alternate. Implementation. And so the, question then is you probably don't want to re-implement a GP or re hypothesis of data and A GPU default one. And so there is, There is some operate. Did you actually have a chance to look at this and see if it has other, yeah. Okay. Gotcha. So that's what I kinda highlight. Yeah. I saw that it was updated, but I haven't really looked through it. So this does different things. Yeah. Yeah. Okay. So yeah, I mean it would probably be, make sense then to do, like just a different backend for the hypothesis updater so we can The current level API. And then it's really, it's when it calls into update hypotheses is where we start having the backend take over and actually do the consultation. Yeah. And I can, ask the team. 'cause I don't know off the top of my head whether Resampling one is the feature and just don't even bother with the default because it's gonna go away or something. I, can double check. Let me make a note to myself. I haven't just, haven't really gone through. It's what, at a high level, what's different about the resampling? I don't remember off the top of my head. Yeah, that's, Ramy built that one. I, I, I refactor a lot of the code. But it was like, I was very much from a mechanical software design perspective, not looking in the guts of it. so I have to ask, I could, hold on. Let me just write down, check if resampling, it might come back to me when I look at it. The resampling hypothesis update is the feature.

Let me not bother with GPU for default hypothesis. Okay. Let me, lemme I'm gonna look on my screen here real quick to see what the difference was. actually I'm gonna look at the pull request that introduced this 'cause because probably has a good high level description of what the change was. I feel like there was a, was there an RFC for third sampling? Oh yeah, probably a good call. Let's see that. I feel like I remember seeing that, but I didn't go through this. yeah. Resampling Dynamic Adjustments. RFC 13. Okay.

Oh, Okay. Yeah, so the Resampling thing is.

Yeah. So right now, so the default hypothesis update you like at the beginning of the program you set ev, this is how much sampling you have to do for every iteration. It is basically fixed. And what we want to do is with the resampling thing is, and when we're talking about the resampling is, how many hypothesis are we testing? So resampling means do we add the new hypothesis to this hypothesis pool, or do we remove some hypothesis from the hypothesis pool? And right now that, prior to this, there was a fixed, like ratio of old hypothesis and new hypothesis, and it was just always constant. But the thing is, as you're, but this means hypothesis come from like unlikely things, but they'll still get computed and tested, et cetera, because you have a fixed hypothesis pool. The resampling means we don't, we wanna stop computing hypothesis that are very unlikely. And we want to sample more hypothesis from the object that we think we are actually on. So for example, if I'm looking at a cup or a potato, and if I'm trying to decide behind, behind a cup and a potato, I'm gonna have like maybe 50 50 split of, cup points and potato points that I'm sampling. And, but as I'm doing on the cup and the evidence for the cup grows, I want to get a more fine tuned sampling of the, different cup rotations and other things. 'cause rotation is a hypothesis as well. So I wanna sample more from the hypothesis about cups and sample, less about hypothesis around potatoes. That's where the resembling vocabulary comes from. My understanding is that. You do different hypothesis update operations for each object graph in the lm. So does that mean for that, if you're doing this resampled hypothesis, updater, like whichever is object, you wanna do more, you just set that one to use more hypothesis and its update step. And then in the separate potato step you just reduce the number of hypotheses that you're looking at. But they're still like separate. Calls to like the hypothesis updater, right?

I think so, but I, don't know the answer to your, yeah, to that. I don't know enough. No. It was just my understanding, from the, older version of the code that I was implementing this and it, it seemed all, every object graph is processed separately. Like independently. Gotcha.

Even in this case, like when I was shifting away from doing all LMS at once in one kernel dispatch, I was still gonna benefit, try to have every, all objects for one LM run in one stacked kernel.

I think nothing you said so far makes me think that wouldn't work for the resampling one.

just conceptually, I wanna make sure I understand whether okay. Object graphs or process separately in the existing code.

I don't know if, I don't know if this answers it. This is what I'm looking at. so one thing that's new in the resampling hypothesis update is inside the update hypothesis. This is big enough. Yeah.

so accepting hypo update hypothesis, These are the relevant bits, right? Existing hypothesis, informed hypothesis. So, these will choose hypothesis based on what's going on. versus, this does not occur in the default one as far as I recall.

Okay, and this is still happening per object graph. that's what the graph ID is there. Sample informed, that is an object graph. Yeah. Yeah. Is called separately on each object graph? Yes. Oh, okay. this is the answer to your question. Yeah, I think So so that's the, yeah. 'cause the API here is, yeah. So update hypothesis is called her object route. Just at the API level, if that, answers, so this just adds some. Overhead at the beginning to do some thoughtful adjustment about which hypotheses from that object graph we're actually gonna consider and do updates for. Yeah, Return hypothesis updates. Yeah. No, I'd have to look into like how the sampling operations work to see how easy that'd be to, do that on GPU as well, because we wouldn't want that. To go sequentially either, right? We'd wanna be able to do that in a batch. Let's see. Yeah, so this is the sample informed and sample existing.

sample specimen. I think this is just like a array operation. I don't think there's much happening here. Okay. Calculate, keep and remove IDs.

Yeah. like calculate slopes. it's, this is a, looks like a, an umpire Matrix thing. Yeah. The big thing for identifying Yeah, like what would, what easy versus a pain for the GPU is how much, does one value depending on all the other values. So if we can, if we have a big list, but everything. each index is being processed completely separately. It's like phenomenal for GPUs, whereas like KN is annoying for the GPU because it actually matters what the other ones are and you have to synchronize between them and all that. Gotcha.

I don't know, seems like this might be pretty straightforward, but so yeah, so, that, that might help. And, then it's I don't know if, yeah. If, that default hypothesis updated or uses that. So tracker dot, yeah. It doesn't even use a tracker. It's not a thing.

and this is what I'm talking about is I think this is the feature. And so I think if you're gonna bother with the GPU, I think it should be the resampling hypothesis. Updater. Okay. Be because this is too rigid. Like we, we definitely don't want, yeah, I would just say right now, do this unless like I come with counter evidence and I'll confirm it for you, but I think this is the one to care about. Yeah, I'll take a look at it and see, how much that would change things.

yeah, that okay. Yeah. Like in terms of next steps, it seems like we still probably have some more digging and discussions to figure out the, API and the way that we wanna integrate these backends. we're doing these graphs Yeah. Groups and things like that.

what's most helpful for me to start working on for the backend. So I wanna look into seeing what it take to make kernels for the re-sample type of update or operations. So I would think do that and I'll try to change the API on the monthly side of things With a grouping thing. See what the grouping API would look like. Because I don't need GP expertise to do that. Versus like, you you, could do that, but it's that's, not helpful. So let's say I, Jeremy, or I can look at, look at that. Okay. and then we'll, so I'm trying to think. So we actually have, from this thing, As we talked about the scope, so it's, this part, this and this. so actually this we're talking about, this is resampling. Actually talking about resampling hypothesis update.

and then this will be resampling. But then the other part is, so there's another thing that we're missing, a thing here. So this is, a, refactor, for parallel is and, refactor their into a grouping call. that makes sense to me right now. I don't know if I read it tomorrow. If I know what that means. Yeah, But, what am I trying to say? Can you help me?

You know what this is. Wait, what are you trying to say? this, so the hypothesis, the, so I'm thinking this is the stuff we talked about where it's like. It's the grouping call at the high level. That was not helpful at all. Helpful to actually the GPU conversation. This refactored the hypothesis updated to support moveable backends. This is actually the helpful part. Resampling hypothesis updated to support moveable backends. This was like, this is the thing that I said I can do because I don't need GP expertise to do the grouping. But, yeah, this is refactor step. Learning modules to use. Grouping? Yeah, like grouping of lms. Oh, lms. Whether that's just one LM per group or Yeah, larger groups. that's the, from the experience side, that's what it wants to see. It's just groups of LMS that it, and then we feel good about this, at least for now, the kind of simple, just dispatch. Oh yeah. Absolutely.

That should work fine for the groups. And then the backend can handle whether it's a, group that gets stacked into one kernel dispatch or just an L one. Yeah. Where we'll stack across object graphs. So yeah, we already have to figure out some of the stacking and so anyway, it's just whether we go across our loans or not. Then, yeah. And there's not a clear path between this and getting to the resampling hypothesis, so, this is agnostic of the backend basically, is what I'm trying to say. And I'm, this is like an independent thing. Yeah. Like I'm trying, I'm like, what depends on this? nothing depends on this. That's great. technically.

because if you do the Resampling hypothesis updater, then it's just cool. We can still run it in a for loop and it's gonna be terrible, but it still will work. Like it will still run on the GPU, but in order to help with that, this step, that's the question. So the, so this is Breakening. That's okay. So now, I have a cold question that I know the answer to is.

but we are out of time. My, the, question I have is hypothesis update called per lm? is there a clear linkage between the groups and hypothesis updaters, or is there like another loop inside an LM hypothesis update? Yeah, so every, right now, every LM has its own hypothesis updater. Yep. But it calls it separately on every object graph in the lm. Thank you for that. Yeah. And already we wanna shift it for definitely put GV back on where it's doing all object graphs at once within an LM and then stacking the group. Is that would be, do all object graphs across all LS as one chord dispatch. So that's, but that's still to be defined then that part of the API. Is that true or do you have a clear idea of how you wanna do it? Yeah, I think that.

Yeah, I think there's still some details. it's internal to the LM fine, because I think on the side, like it's just whether it's calling a dispatch on a group of LMS or on the LM itself, but then inside that, inside the LM, how it calls into the backend. And how we do the stacking, whether it's across LMS or not, I think there's more to figure out about that is, am I saying the right thing? Define how LM to object to hypothesis. object graph.

LM to object graph. Hypothesis update call should work, right? this is hypothesis update. You're confident we got this, we kind of confidence we got this. But then we are still missing the lm the path through the object graph. How the object graphs are getting Yeah, we're missing the ap, the, we're missing the definition, the clear definition of A API where stuff gets paralyzed across object graphs. This paralyzes across lms, this paralyzed across hypotheses, and we wanna paralyze it across object graphs, and we wanna paralyze across. Hypothesis update call, which is the same thing.

Script. Okay. Important to, say that in order for this Cuda streaming approach to work, I think how it works is we'd have one backend object that's shared between LMS because it needs to be.

It's like one backend that's handling the streams and running them across lms and then being able to report back when all streams finish. Like you just want one call to one backend and not have to go ask every LM whether it's done. And so also then across groups, is that one backend across? Yeah. 'cause the groups also call into the dispatch the same flow. So yeah, it's still one backend.

And then every group or slash every l lab references that same backend object. Does that make any sense?

it, it probably would, but I'm also feeling like time pressure that should be done. And so, when do you wanna meet next? when it's good, I'll just send you a schedule and then see what we can do if you're willing to meet next. In the meantime, while we've figured this out, I think I can at least attempt some progress on the refactoring bit. And then this is still a big unknown, and then I, yeah, I, think we then, so next Legion that will be trying to figure this out. Does that sound reasonable? Yeah, yeah. And we can, we can talk over the discourse and Perfect. Sometime early next week or something like that. Be, it'd be fine. Awesome. thanks. This was, yeah, no, it's really fun to actually finally get to talk about it, and spend a lot of me just like reading documentation and I think this is right, but, I'm not really sure. Sounds good. I'll, also see if if I can drag Viviane into this too, because that will, she is the expert on, that stuff, so that will, that should help tremendously.