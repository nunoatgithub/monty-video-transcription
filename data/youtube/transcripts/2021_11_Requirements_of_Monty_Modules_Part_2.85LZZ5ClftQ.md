I just there's a lot of questions to come out of this for me. From my perspective, I want to understand there's multiple ways I could go. I could say, Hey, number one is, understand the hierarchy better. That would be one thing I could work on. the other thing we could work on is, understanding motor pattern.

back to Viviane's question, it's how do I move? How do I decide what to move?

and for what purposes? And things like that, because these modules can move their sensors. They have the ability to control some movement. And, and why do they do that? How, who directs that? And what kind of information would go back and forth to do that? And how would that motor information go from here to here? So that's another question, thing I could work on, to think about that. and then, Marcus and I were talking about, yesterday, I'll just throw it up here for grins, We were talking about how it is sometimes you can recognize a very specific set of features, and other times you're very, you can recognize something in a very sort of fuzzy set of features, The fuzzy set of features would be all coffee cups, but a specific coffee cup has a specific shape and specific size. Or a face, like a particular face, like Lucas's, I recognize Lucas's face, it's a particular face, but then I recognize faces in general. And so we came up with this idea called the stretchy graph, the stretchy graph, which I could say basically, what could you, how would you, recognize things when you allow these edges to move? it's yeah, the same features, but they're stretched out a bit. But you have to handle cases where you don't always have the same features. But Lucas has a beard and a mustache. Yeah, I have to handle that too, So it's not just stretching. Yeah, I know, yeah. the other thing related to this is, is, states, logics and behaviors.

I would define a state of something is just the current position of all its features. Yeah. and if you want to think about stapler, the current position of all its features. we, the current poses relative to each other, even something as messy as a t shirt, you can say, I know the t shirts have collars and shoulders and arms and certain features relative to each other, but that t shirt could be on the table. It can be folded up. It's still a t shirt. I still have a concept though. It has those features, but I don't necessarily know where they all are relative to each other. And so when I pick up a t shirt out of my drawer in the middle of the night, I will pick it up, I will find the collar based on its, based on the texture, and then I will run my finger along that, and then I know, and then I can orient the collar relative to my body, and then I hang it up, and I know the rest of the things come down, so it's like, all those features are there, but it's a lot of uncertainty as to what are the poses at any point in time, so the knowledge of the links is really confusing at this point in time, but the local links should always be preserved. I know that this part of the collar is going to be related to this part of the collar, and I can get there by moving along this way, so it's like, objects have these local features that are near each other are more likely to have links, and you're stretching those links, but you're the remote features can be really off in the distance. So it's like if I, if I think our object is not like all the connections, between all of the poses and all the features but it's supposed to be local features, then you can stretch this graph and maintain the topology of an object, even though it's in a very deformed state. We have to be able to do that. and then sometimes those deformations have patterns just like the staple opening and closing So that would be like a behavior even if I look at your body You're a set of features that you're like a t shirt You're a set of features that are in some position relative to each other But the state of your body depends on whether you're doing this or sitting down it's like I still recognize you. So again, it's this idea that the graph You Has these components are all there your body parts are all there, but they're in different. They're stretching out different ways at any point in time and somehow the model is able to deal with that. So that's another thing to look at states. So objects have states your body has states. which are the, so all these graphs, another thing is, I just mentioned here, just remembered it. Another question is, models in the where columns.

that goes back to this point here, I couldn't develop, a graph in the where column. This would be the relative poses of, your, features relative, the, the question is there, the question, is there an equivalent model here and what would it be useful for? if we think all cortical columns are the same what columns and where columns, then you might have a model here and you say, how would I use that? one example is like, when I drive my car, I get in my car. I don't have to figure out where the steering wheel is because when I'm in the car, the steering wheel is always in the same position to my body. And so I don't have to go out and say, infer the wheel, where is it? It's no, I have this model. When I'm in the car, it's there and I can reach it without even looking at it. And it doesn't really move very much from that position. So that would argue that I have some sort of model of the features in the car. And when I'm sitting, I've learned that these are the features relative to that position, and I don't have to infer them. I don't have to rely on the, I don't have to say, oh, cars are composed of steering wheels and cabs. Where am I relative to the cab of the car? I just know, I know I am. And I can, I might want to model that. So a lot of our behaviors might be built in this kind of model here, where you're saying, yeah, once I know, once I orient myself, I know I've got So anyway, these are some topics there. and then finally, the uber question is, when we understand this, even if we can build it, Or even if we could build a more complex system, even if we could build a sub complex system, when there's less than this, maybe we just build the first one I talked about, and then we do just these models and this learning system. What would we do with it? That's a separate question. What would we want to demonstrate? What would we want to, what could we do with it that's useful? I didn't address that at all. I didn't, I wasn't even thinking about it. I was just thinking about, hey, what are the mechanisms? How do we talk about this? and, so I feel this is, to me, personally, this is real progress. I, never had this clear conception of all these things before. this clear. It's much clearer to me. What's going on. It's a much clearer description. Is it for you too? Yeah. Oh, good. one issue you don't have out there, I think is going to be important for any real implementation is this issue of what's a feature and how do you locate it. I think that's like the number one thing that tripped up standard computer vision. A lot of this stuff is, I could describe it in terms of kind of old style computer vision that they all ran and you can get, make, decent demos and stuff with it, but to really get stuff working well, you have to deal with these fuzzy things of what's a feature where, where's how to deal with ambiguity and then it's not clear yet how that fits in. That's a, that's a great question. I think we have enough information to start honing in on it once in a long time.

I'm gonna, one, it has to do with this fuzziness of representational space, in some sense. When I detect a feature, remember that the sensor says I'm trying to give a unique location that feature, even though there might be some structure in that space, there might be some, I, if it was like an edge, I could say, Oh yeah, I know the orientation of that edge, that's pretty easy. but even in that case, imagine you have, imagine you've got some sensor. And, and you're, and it's detect, it's basically looking at some volume of space here, and it's just as I'm getting some input and I'm just trying to characterize it. I'm going to stick it through a spatial core. I don't really try to understand what it is. I just, I'm just getting some inputs, some bits, and I'm trying to classify it.

what if there was an edge, but at one point the edge is here, and the next point the edge is here, and the next point the edge is here.

what am I reporting back? I think this is maybe a question you're getting at here. It's is this an edge? Or is this an edge? Or, this one's out of the view. It shouldn't be. This one's more in the center. This one's barely touching it. what do I report back from this? One way you could, you can answer this is, in a sort of a binary way. You start off by saying, okay, is there something, anything in that space? Anything physical that is in that space? this would be just saying there's a border, there's some edge of this thing that's there, and I don't really care, and I'm not going to make any further distinction about it, and you could start with that premise and start working away and say, how would this work under that condition? That would just allow you to define the morphology, essentially the boundaries of an object. it wouldn't provide any additional information, it would just say morphology. I'm thinking out loud. I'm just trying to think of how would you go about solving that problem, thinking about it. I agree it's a big problem. I'll just put up here, I agree it's a big problem.

feature, details. feature, I'll do my, what would be a good word to represent this problem. So that's it.

It's, it's almost like the granularity of a feature, a discrete feature, yeah. it's funny, once you get to this upper level module, it's a hell of a lot easier, because these guys are just all going to result on some answer. It's sometimes it's easier once you get this guy because these are so these are very, there's no fuzziness here. They're going to vote and pick something, it's either the face or the box, it's going to be this or that. I may be wrong, but I'm going to tell you it's one thing. And so these will be very discreet.

properties, where down here is where you start getting into the fuzziness. It's like saying, I'm some, I got some edge of an object. I'm trying to have some feature. Where exactly is it? And is that the same feature? I think it gets weird down here. It's I think it's pretty clear up here. because I've already bucketized it. I've already put it into, into one of N buckets, here, I guess what we want to do is we want the sensor to be, to do the same thing. It would want to, it would want to discretize, discretize the feature, in a way that doesn't cause a problem.

So from that point of view, we'd say, okay, it wants to pick one thing or the other, and not let you have subtlety turn up. Anyway, it's a good question. Yeah, if I would throw out a wild guess, I would probably say that in the beginning, features are just the raw sensory input. And then over time, you learn to discretize it to, group the sensory input into useful chunks of information, at birth, children can distinguish all different sounds from all languages. But over the first months of development, they lose this ability and they can only distinguish sounds from their own language. but they can do this much more accurately. That's a good point. let me just rephrase that in a different way, Viviane. I'm going to rephrase it in my language here. If a feature is just some, it's some quality at some pose. It's something that has a pose. and now I have a sensor that's trying, I should have drawn that in green to be consistent.

let's see, green is a feature, and black is a sensor.

and now I have a module that's processing that. The first thing I need to know is the pose of the feature. Generally the location is easiest to determine in some sense. it's, we can debate it, but in some sense it's easier. It's okay, where is that in the world? It's out here. I can, it's at the tip of my finger. It's where my finger is, or my eye. There's one column in my, my, my retina. There's this cone in my common patch of skin or my retina that's looking in this direction and I detect something At some distance, I might have some ambiguity about the distance. It's pretty much, that's not like really weird. The weird part might be figuring out the orientation of the feature, and I've, argued there are features which do not have orientation. So this is some sense of a color on top of that. You can say, okay, the most important thing is like, Something, and it may or may not have a pose. Some features actually don't have a pose, but it just do not have, you can't distinguish, I'm hearing a sound over there and the sound of a certain frequency. I can't necessarily say the orientation of the thing that was making that sound. Maybe I can, but generally I can't. so that's, the system doesn't work with ambiguity on the orientation. And, and then there's this additional thing, these additional qualities, that are, sensory specific, things that only occur in, in vision or hearing or touch or something like that. and, so I guess what I was, what I'm pointing out here is that, The system can start working with just location, and then we can start adding pose and these additional qualities.

if they're understandable if I've learned them or if I've, your example, the language, Viviane, it's like, maybe, maybe I've, settled, I've been exposed these enough that I've settled down on a few of these that I can say, okay, those are the ones I care about. But imagine I'm taking these bits and I'm running through a spatial pooler and the spatial pooler is just trying to bucketize these things and, and saying, okay, that's it. That's what I'm, that's what I'm going to know. and then I have to figure out the orientation, which is sometimes possible sometimes not.

it's a great general problem, but I think it's solvable. I don't see there's a, I think the way Viviane phrased it seems like you could go. Figure out some sort of a solution to a learning based solution, which she said was doing basically a clustering, yeah, or pooling or clustering or something. The thing that really gives me hope is that I don't actually need to know other thing other than location to get started.

The other things would be great if I could figure them out, but they're not absolutely essential. So there's a mix of orientation knowledge, there's a mix of, quality knowledge, that seem to be additional help. Those, in, in, the graph terminology, those, like those additional qualities become some attributes of each node that can help, but are not really representing the, the graph in its edges. So a color is not part of the graph in the sense it's a part of a node in the graph. But a letter is. What's that? Yeah, like a letter D. Followed by letter O, followed by letter G, you have dog. If you mix them together, if you don't know if it's letter D, O, or G. Yeah, but those are in the graph. Those are just three letters. Those are in the graph. Those are like, the D is in positions, the O is in positions, and the G, right? But if you don't know what's the feature in there, you just have, See, once I get to D, it's easy. Once I've already detected a D, I'm already past the first level in the hierarchy here, and I've detected a D, it's easy. It's like D's are D's. There's no, there's no question about it, and I know where it is, and we can go with them. I think the problem we're dealing with is the first level of sensation. Where you have a sensor reaching out to the world and it's detecting some, weird property that you're getting back, which might be in shadow or shade or this or that, There seems to be a whole bunch of, extra things that the brain or the body does to try to help you figure out features, like the, how the retina compensates for changes in light and how you, how you, you're able to do divergence and, and whitening type of things. You know what I'm saying? There's a whole bunch of additional stuff that helps you figure out the quality of the feature, but the distance of the feature is the most important thing, where it's positioned relative to the body is the most important thing. so again, when I get to the D, it's not a problem. It's the real problem is, I'm trying to recognize a D, and I'm doing it from just little patches that look for little edges or lines. And that D could be black and white, it could be white and black, it could be written that can be written all these different ways, right? And, it can have noise, and so it can be 3D with a shadow effect. I think the general problem with sensing a feature, if you're looking at this thing and you're saying, if I'm looking at an edge of a, what is it? If I'm looking When you actually just look at the wall that's coming in, it can be difficult to determine what's on that feature. I think the location can be known, but deciding what the actual, orientation of the feature is and its, and its quality can be challenging. So I don't think the dog is a good example. The dog is yeah, I know what's a D and O and a G. It's it's more like, how do I determine the D based on sensing little parts of the here's another way of phrasing it. Go ahead. You got a D, and now I have a sensory patch that's looking at like this, or like this.

it's I can, there's almost a continuum of things here, right? That's the problem.

I think, again, I get a lot of knowing that the ultimate goal is going to be trying to binarize this or quantize this. And, and, and so that's, what we have to do. We have to somehow. I, don't know the answer to it, but that's, the problem. That's the difficult problem. Once you know it's a D, it's easy. So I already classified, all these little weird things are D's, and that's it.

This gets back to the stretchy graph I did too, because these kind of different shapes is, somehow some relative set of features that are fuzzily, that are stretching morphology too.

In terms of kind of demos and what we build, I think the obvious one to me is you learned an object in one pose relative to the body. You go to a different location and you can recognize that same object again. That's a hard thing for, deep learning systems to do. Is, so that would involve just one model, is that right? That just could be one model, or do you want? if it's one model, it's, almost, you can just always output that category. You have to have at least a few categories. Oh, no, when I say one model, meaning a one modular system, that, that thing on the right, Represents many different models. It's, Did you say one module or one model? I should have been more clear. One module. Module. Yeah, it could be one module. they could do like one module, or you could do all the multiple modules down here. I think you'd start with the one module up there, which is It's like our columns paper you do one thing, and then you put many of them and it's faster. Yeah. so you could start with, you could start, I don't know what target, you could start with one sensor and one module and that sensor goes around and build a model. Then you could extend it to multiple sensors in one module, but then that means you can only use one of these at a time, right? Because that's what's required to learn, that's what's required to infer, but you could still do that. Then you could go to, then you could add just then you could add the multiple modules here, and now you can do like the grasping with a finger, By the way, I thought of an interesting way. for a demo, I think the multiple modules would be best because you just, I'm just thinking vision again. You have an object, you learn very quickly what it is, you move to another location, and you'd see. Yeah, I can do that. I can do that with one module here and one module there because this module can move and I can write that, but then. Yeah, it's just slower. It's slower, but it's gonna be I mean you could start there because it's slower But don't forget in a computer we can do this really fast I don't have to actually move, I don't have, if I have a camera, I don't have to move the camera. I can just pretend I can just say there's this feature at this point over here, attend to this point, attend to this point, attend to this point, attend to this point, so this could be done with a single sensor that you can at least, as long as you can, I can do it with a single camera if you will, as long as the camera. I can sub sample parts of the image of the camera. Then it'd be like, oh, there you have the camera. But now I don't have to move the camera, I could, do it have a different pose? remember, imagine I have a camera, here's my camera, and it's looking at an image out here that's like this, right? imagine, so this is like a, angle orientations.

I could, detect a feature here, and I could detect a feature here, and I can detect a feature, here, by not moving the camera, but by taking this image and processing in a way that I focus on this. It's like a covert attention. yeah, I'm focused on that. it's interesting, if I had a camera doing this, and I could do this really quickly, it would work. It also would work. Even with a single camera, it would work if I moved it. And it should work even as I move it. That's what I'm thinking. After you move it, it should still work. Yeah. That would be impressive. Or, once you get it after you move it, you should be able to get it to work as you move it. You should be able to take a camera and say, move it around this object. That's easier. That's easier? Okay. Then it's just tracking. I'm just thinking from an outside perspective. Okay, I don't know that. To me, that's more impressive, but if it's easier, then fine. Yeah, because that's a solved problem. okay. Once you lock in on something and you move around it and you keep saying, oh, it's still a dog, from the external point of view, that's easy. Okay. That's not what I'm mentioning. It's more like. I don't know what you're saying, but it's, because that's pretty damn hard because you have to be updating all of these locations as you're going. But I see your point, so let's put it aside. my point is the simplest system you can start with would be to have this plus this and one of these, if I could move that. Second would be this plus this, plus multiple of these. Then I could say, I can look at this from these different positions all the time, different sensors are helping out here.

that, for example, imagine you had cameras, but this camera was obscured, but this one wasn't, then you could use input from this one. Are you thinking each module is a separate camera or a separate image patch? in that case, it would be a separate camera. Separate and movable, a movable sensor. So that gives you the, I don't know again if this is impressive or not, but you're trying to recognize some object and you say, Oh, I recognize it. I learned it and I recognize it. And now you said it's impressive. I look, I see it from a different position. I recognize it. Yeah. Okay. My point is, what if I learned an object. And now, this guy was obscured, and this guy was obscured, but these two cameras here at different positions are not obscured, so I can, even though I've never even, I could, I can, basically I can say it doesn't matter, as long as I have some subset of these are able to look at this thing, I can figure out the object. It's the same thing, yeah.

I'm not talking about obscuring, like, I've now lost the camera. What might be better is like no camera on its own has enough information to recognize it. Okay. Together, all four of them. I don't know what's impressive. Here's the thing, it's interesting to me, it's You're trying to recognize this thing and various sensors are able or not able to participate at any point in time. As long as you have a sufficient subset of them that are working, even if they didn't, even if they weren't even used to train the object, they should work. If that's not impressive, fine. I don't care. I'm not trying to decide. Yeah, because you can just imagine each camera has its own deep learning network and they get the exact same input. you can decide. I don't know. they don't have the exact same input. exact, they're trained on the, on images. So if you can recognize a shifted object, you could apply that same system from a different angle. It's the same thing. Okay. to me, it's more interesting and impressive from a biological Because you're, because you know the internal. Yeah. Okay. All right. Good enough. anyway. But the version of that, would be interesting is if no camera on its own Can sufficiently disambiguate the object, but then multiple ones can now recognize these things and all the poses. Okay. I think that would be pretty good. Okay. Oh, I'm thinking vision, but we do Really interesting about this. It could be anything. It could be. You should be able to do this ultrasound sensors. Anything that can detect some feature at some location, especially, I imagine the robotic hand, our hands can't really detect anything until they touch it, right? So I've always had this uncertainty about reaching into darkness because you don't want to hurt yourself. And if you have vision, oh, I see where I am. I see there's nothing in the way. But imagine if your fingers on your robotic hand are little, in addition to the touch sensors, had little ultrasound sensors so they could detect how far away things are from each part of your skin. This system would automatically be able to say, I know what I'm about to touch. before I touch it. I'd even know the orientation of it. So if I was going to reach down and grab something, it would be like I'm looking at it, but I'm doing it with sensors with my fingers. And they tell you, I know this thing. It is the orientation. I can now know how to grab it. So there's a lot of, we don't have to restrict ourselves to vision. you can build a, you can build an object recognition system that works with, other types of sensors that like ultrasound or radar or, even sound, the system should work, in all situations, and it can be better in some than others, but, so anyway, my point is, we can think about vision, but don't want to be restrictive of thinking about just vision. And maybe that's what people will respect more, but to me it's cool that you can do with any kind of sensor. It should work. okay, building this plus this, plus one of these as a starter, then multiple of these, And then adding these components would be a, additional steps on top of it. Are you thinking there's a one to one correspondence between what and where modules? that's an interesting question. I would ask myself that. the way I've drawn it is that way. I don't know, it's interesting how the what columns and the where columns are separated in the cortex. They are not next to each other like I've shown here. And, one obvious potential reason why that's the case is that the what columns have to communicate with each other locally, to communicate that way. And that information is directly shareable, where the information that goes between a what column and a where column has to go through a pose transformation, right? So that may be centralized. That could be another part of the brain saying, I'm going to here and then up to here. So this step here suggests. That, that it might be more fortuitous for the useful for the brain to put these guys all together. And then these guys all together, especially if there was similar voting going on in the where columns. So now you have a set of what columns instead of where columns, you can think of it this way. Okay, so here's my where columns.

This is where, so what columns, this is how it looks in the brain, and these guys are voting. So there's a voting going on here. Maybe these guys are voting, we don't know yet. I'm guessing you would if I don't know why yet. and then, but to go between these things, I might go down to this, to some point, it goes up to here. This is the kind of stuff that Marcus, speculated, maybe it's a striatum, maybe it's a claustrum, something like that, where you're just, there's these parts of the basal ganglia that look just they look just like this. They look like people speculated they're doing reference frame transforms. And now the question becomes, is there a one to one correspondence here? my first guess would be yes. But I'm, I don't know, it's certainly simple to think about as a one to one correspondence. I can figure out what would work, I think, as a one to one correspondence, so maybe that's what we should do. but anyway, this could explain why, this could explain why the where columns and what columns are grouped together. including in the hierarchy, which we haven't talked about in the where column space. so anyway, the short answer to your question is we don't know. The long answer is I would. I would assume there is a one to one correspondence until we know the reason why there isn't. why would we better not have one?

Is there any in vision pose relative to the body is tricky to figure out? The pose of the feature? Yeah, in 3D. Yeah, I, again, I think we should take this very abstract view of it and say, first of all, determine the location, then take these bits, run them through some sort of algorithm that does classification of some sort. like a spatial pooler or something else, to quantize them, the input, and then perhaps, perhaps even at the first level here, the orientation is unknown. What if I just said the orient, it's, I don't think that's true. Sometimes, we do know these orientation columns, but what if at the first level in the hierarchy, I just couldn't tell the orientation, all you do is say, I'm going to quantize these inputs in different categories. I think the system would still work. Once I get up to here, the orientation is clearly known, but not, maybe not down here. I don't know. I guess I don't want that to be a stumbling block. We can start with just I'm taking this sensory bits, I got some location, and, I'm going to, I can try to quantify or classify the input into discrete buckets and orientations, and we can leave that as a separate problem, like how do we do that? But I think the whole system should work, even if I can't solve that right now. I'm not sure about that, but I think so. Seems like this, black line formerly known as the bus, is, I'm trying to understand. It's not a bus. okay. Yeah, I'm trying to understand its role. we talked about this in the, meeting earlier this afternoon.

There is no bus. Bad words. These lines just show who is sending information to who under different conditions. And it does not imply mechanisms. It doesn't imply exactly how it's sending information. I'm just, I'm routing information about poses. and about objects and and no more detail is specified here about how a node would go and how it actually sends a load. Is there a common communication structure or is there a messaging system or anything like that? It's just, it's more of a block diagram and there is no bus. So that's how we get rid of the bus.

Yeah, the reason I added the word bus is I have this idea that you can take a module like this. And you just stick it in the system, and as long as it knows how to communicate with everybody else, it should work. In fact, yesterday, Marcus has brought up an interesting idea. He was wondering if you could have pre learned modules up here, like face recognition modules. Modules that are, not pre learned, modules that are, They come with some graphs already built in. Yeah, they come with some graphs already built in, or a bias to certain types of graphs. maybe evolution said, hey, for these face recognition areas, we're going to just structure the graph already for you. And we're going to assume that there's, that it knows what faces look like generally, so not only are you doing a stretch on the graph, but you're pretty much going to assume everything's a face when I look at it.

Which is an interesting idea, It's yeah, It's an interesting idea. could you, so something, know, Marcus is thinking oh, you could build a face recognition system that has this built in genetic bias to recognize faces, because the graph is so pre structured, and and plug that in. and it'll, and they don't, have all the same data everyone else is looking at, but it says I'm just going to try to think everything's a face, distinguish between faces. And it seemed like we have some innate ability to recognize faces and distinguish and predict. We can run somewhere that was a Halle Berry neuron what's that? It's a Halle Berry neuron, yeah, I don't think that's built in. So many different versions of that I've heard over the years, it's funny. That's right, it's in Bill Clinton. Yeah. Yeah, I think they, so that's another interesting idea is like you could work on building, once you've got this infrastructure working, you could build modules up here that are more specific to different types of problems. so maybe, cortical columns are not identical, maybe some come with some pre, genetically, predisposed, graph structure. And the other thing that this would bring is if you have this prior that faces exist in the world, that in turn helps train your V1, expecting to sometimes see a face, expecting to sometimes see these two things called eyes and a nose and a mouth in a certain orientation allows you to now. Train your V1. Your V1 has something to latch on to now. Yeah, I would, put that in this, understanding hierarchy, because I didn't talk at all about, I said, oh, it is interesting, now these, this stuff goes in this direction, but of course we know that these things send things back down to layer one in this direction. I was assuming that would just all be, you could treat them all as voting connections or. I don't know if they're voting connections. It's, yeah, I'm sorry, the output of The models above also have green outputs. Yeah, going up or going down? Either. they're different, I think. The output of this guy, this green output, could be fed to here. It's like the expectations from above. Yes, it could. That's, if, that would only make sense if these models and these models both had, these modules and these modules both had a model of the same thing. So that was the example of a large A and a small. Yeah, or more generally it could be a many to one or one to many, just like between any two modules. There's a, lot of variations on the theme here. I don't think I understand here, I was thinking specifically there was these, layer six connections that go back to layer one, primarily in the lower groups. These are massive connections, and so they're, these guys are telling this thing something. I don't know what that is yet. but there's also skip connections. Yeah, so imagine, so that's interesting, What Luiz is pointing out is that This module here can get information from here. That's, a direct like layer. That'd be like a layer three to a layer four here. This becomes the input, the layer four input here. The output of this might be layer three. That's a classic connection that, Felleman and Van Essen would talk about. But there's also, if I have a sensor, down here, that this guy seems also to get input. from a set of sensors here. So a broader set. So it's basically getting sensory input from multiple, a bigger sensory patch. And so it could say, oh, I'm trying to detect some bucketized feature over this large area, or I can get this pre processed green one. And one thing I thought was like, if I'm, if there is a, a real consensus here, like there is an answer in green, like we know this is the letter B. And it may not, why would I even want to look at the sensory input?

there might be a precedent here, because these both go to, these both go to the same layer. These, both go to the input layer in, in here. They both go there. they have different places they terminate, but they're both going there. it's okay, they're both inputs. one possibility is that, if I, have a green, message that is, yes, we know this is the letter B, then I don't want to look at the low level sensory details, I could, but maybe I would just take precedence of that. But if these guys don't know what it is, maybe it's like a big B. I wrote a really big B, and the low level, V1 column only looks at little small pieces like this, and they can't, it's too much to build up a model. But this guy is looking at like big pieces like this. And he says, Oh, I don't know what a B looks like in a big picture. So this guy can say, if these guys don't know anything, if this is a question mark, then I'll look at this. But, if these guys do know something, then I'll just ignore that. it could be like that. That's just one speculative idea.

it seems reasonable to me because I can see how large things that are much bigger than the receptive volume of this feature detector or sensor would be very difficult to model down here. It's just, too big a space to occupy. We also have a gating mechanism between those parts here? between this and this? No, between the lower layer and the upper layer, like the thalamus. Ah, thalamus. Oh, God. Okay. I was hoping to avoid the thalamus. Okay. sorry. Sorry.

We can avoid it, or I can speculate, which you probably want. We should avoid it. Avoid it. I think it's fine. another question. We've been talking a lot about object detection and optimization and not about predictions. Is that going to come in later at some point? you have a model, right? And with a model, you can make predictions. So we have the data to make as many predictions as we want. I guess the question is, under what conditions should we be making predictions? locally here, let's see, if I know what model, what object I'm viewing, so let's say my green is determined, and we say, oh, we're all looking at the letter B, And then, I somehow know, I know I can take that, I know the location of my sensor relative to the, B, I should be able to run this model backwards and predict what my sensor should see. fuzzy, the information is there locally, as my sensor moves, I should be able to predict what my feature I'm going to see. So it'd be a local thing here. Clearly, if you violate your prediction, that's useful for, attentional mechanisms, which we didn't talk about at all. but, there is the infrastructure here for making predictions. So we need to find out why we want to make predictions. And one example of this, if you look at this situation, the table is covering the chair, so I can't see everything in the chair, but because I know I have a model of the chair, I know. I can predict the way the chair is if I know I left my cell phone on there. I know how to reach in there and grab it without ever having even imagine the chair I can visualize it looks like it's like I could picture it. So being able to understand the structure of things you cannot directly sense is pretty powerful. That's a type of prediction. Yeah. So I think what I, what might happen here is we start building some subset of these things, and that we can, test and build, and I'll let, It's a joint exercise to figure what that is. I won't say I know what that is. and, as we do that, we can think, we'll think of ways of doing clever testing on it. We'll think of problems that we haven't solved that we want to solve. It will start being a scaffolding in which we can build more and more detail on.

and that's what was exciting to me about this whole idea. I see this as a scaffolding in which you can start building the whole damn thing piece by piece. figuring it out as you go, where before we did, I had no idea.

Does the output of a where module is the pose of the feature relative to the body? no, that, that's one, yes, that's the feature relative to the body. That's it. It, that's just, yeah, you're right. It only does that, by the way, if, the green answer isn't known. for example, if I see there's a letter B, I want to send down to this module down here. Oh, I'm seeing an edge at this location.

It's it's the same sort of priority. If I, know there's a B, I want to send the B down to here, because this becomes, like the input, just this became the input to that module. It's oh, then I want to build a model of the B's relative to each other, relative to A and C, but if I can't recognize it's a B, then I'll send down the low level features and I'll learn what a B is. so you don't always this guy locally has to determine the feature relative to the body. That's, it has to do that for any of this to work. It only sends it down here. If for some reason I need to attend to that feature. either because I just decided I want to attend to it or because I don't recognize the object yet. I can't recognize the object yet. I just have to look at features. Why can't I be always sending it and that guy decides what to listen to? because sometimes it has to look, it could. Okay, it depends on how you want to consider the messaging here. So the same question we have here, the same question we have right here, right? You can say, I can give you the details or I can give you the big picture. If I know it's a B, I don't want to look at the details. Why would I want to remember that? now I'm trying to build a model to let it work. So, yeah, you could send it here. I'm not, I'm just, yeah, you could. Again, don't think of these It's just sort of information being available. But yeah, but I wouldn't want to always want it. If I'm, my goal here is, I want to recognize the highest level object I can. I want to build the biggest, highest level graph that I can. And when I look around and build a graph of this table, okay, Marcus today is over there, Luiz is over here, and today I have my coffee cup, and I brought my own pens, and there's my laptop. Okay, I got all that to my head. I just built this model really quickly here. I don't really care, what was the exact orientation of the pen, and what was the, or what was the, the edge of the pen relative to the cap of the pen? I don't care. These guys are looking at that, but this guy, I don't even need to know that. If I completely didn't understand this scene at all, I have no idea what it was, I would just be looking at little features and what's going on. So I guess, generally, you want to prioritize for, higher level green objects features. This is just a normal type of feature. You want to prioritize that down here, just because that assumes I've already learned the components down here. So as soon as I learned the components down here, there's no point re learning over here. But I could if I wanted to attend to it, right? If I said, today I really care about, you know exactly where that pen is. when we look at it carefully, okay, the caps this way, not that way. But wouldn't that if we only have one module, let's say this thing and this, we don't need this, right? No, we don't. We just have this. Yeah, that's it. Yeah. And you could start with that and that, and you could add this and that. But then you need those little ones. No, you don't need little ones because this allows you to, build your model with different senses. I'm not sure if you think that's interesting. That's like saying, oh, I can learn this object with my finger or another finger or if my eyes, but. But it's still one module. It's still one module you're learning, but I, to me it was interesting the fact that I could touch the pen with any part of my sensory skin. And it would, work, it just doesn't matter. I thought that was interesting from a machine learning point of view, maybe it's not interesting at all, I don't know.

Sorta impressive from a brain point of view, it's pretty impressive. You've got, tens of thousands of these things and any one will work. it's oh, that's pretty cool. but maybe, but you don't need to do that. You can just pick one and go to this module, model, this module, have a module with one sensor. then, the other beauty about it is whatever you use to design this, Combined module here, pretty much work down here too, it's pretty much the same thing. That's what I'm thinking, yeah. there's no, advantage of having this here until I have multiple of these.

So maybe that's the way to think about it. You've got a simple, you argue this is really too simple. it is simple. A sensor, maybe the part of sensing a feature is not so simple, but if I have a feature of orientation translated into body location, easy. Then I have to deal, build these two things. And I argue, we know enough to get started on this. I'm looking at Marcus, I think so. Do you think so?

I'm still, I don't know. I'm still figuring out what's going on. Okay, I'm sorry. All right. This, we've talked about how to build these graphs. This is like what we did in the columns papers. Sure, yeah. We can build, we can make a module that does this, we can make a module that does this here. We can easily do the reference frame transformation between them. This module models will not have everything we want. Maybe it doesn't have stretchy graphs. Maybe it doesn't do this and doesn't do that, but it can work. And, we can build that and rebuild that. Over time, we can improve this, but we could, whatever, you could almost take this, like this code base that you did to do this, you plug in right down here with a different learning rate. And then you could go bang. Or it could be fast learning down here. It just means that, you can make these interesting variations. In the brain, it needs to be very slow learning. Very slow learning because these guys are going to rely on the fact that these That's the advantage. Even with one module, that's the advantage of that guy. Because that would be slow. And the one on top is for really fast one shot learning. yeah. But I could make these tasks too, as long as they didn't have a hierarchy. then I would be able to How to stabilize on the green. It doesn't matter because I'm not using it. It all needs to be for voting.

anyway, these are variations on a theme. I don't think it really matters.

here's, I do the one shot learning. I go to look at the table, I love the example of the food on the table, and you're looking at a little bit of potatoes, there's the green beans, isn't it? You build this, you're building this one shot, module here. At that point, If I wanted to recognize it again, I'd have to go, I walk in the room from a different direction, I'd still have to look, oh, there's the green beans, there's the potatoes, from a different orientation, and I wouldn't be able to do this one shot recognition of everything. But I could if I did, if I held these guys on really rapidly. It's a, it's an obscure point. There's various dials you can turn here and make these things into different things. but defining this as one module, we've identified two big problems. One is, how do you discretize features? That's one problem, the big problem. The reference frame calculations are easy. the other problem is, let's be precise about how these graphs work and how these things work. I think we know enough to do that. I feel I know enough We'll find out, but that would be another big project. Okay, we're gonna, given a feature at some location in space, converted into a spatial location of the object, there has to be, there has to be a quantity, which is the pose of the body of the object. And, And now we build this graph with the idea that we're going to be able to infer which of these graphs, which object we're viewing, based on a series of observations. I think we can do that. I think I could, I could identify the steps that have to occur. I don't know how to code it. I don't know how to do it, but I can identify the steps. I would just start with something like a columns floating, columns paper. Yeah, that would be like, that would be like, columns paper is like this part here, With temporal pooling, so you have a stable representation of it. So these guys are temporal pooling here. But where these are not just features, they're poses of features.

that's the difference. In the Columns paper, we just said there's a feature at some location, which is like no pose. It would work and No, no reference. no. Yeah. I would almost do their edges of the graph. Yeah, the edges.

they're relative positions of two features well, in the columns paper. But we didn't have that. no. I know, I think, that's what an object. Yeah, but I think the object, I think the object might, this model in here, a model in here might actually have both. I'm, not sure yet. It might have both. You might have what we did in the columns paper. Paper, which is a. Remembering, features at pose, feature at pose, feature at pose. you can also, in addition to that, you can build the graph. And, I think there's some advantages to doing both. Yeah, because if you have a very unique feature, you don't need the graph. And also What's the difference then, from the Columns paper then? same thing, in the Columns paper we had that too, we had a very unique feature you could recognize right away.

I think the difference is that, is it a reference? Yeah, but the algorithm could be the same, it's just what is, what are you pulling? By the way, if I was, imagine, this is the way we did like layer four, remember layer four was something like, It's like your feature at some location. let's say it was this feature at some pose was relative to the object, so we just extend it a little bit. You could dynamically build up this graph as needed with this data. I didn't have, I don't, I wouldn't have to store this up front. I could, But I could also, dynamically, here's an instance, Which is advantageous. Yeah. Here's, in some sense, This is an efficient storage mechanism. This is somewhat inefficient. This is an n squared type problem. It wouldn't be, but potentially n squared problem. This is just a linear list of features at poses. I think you could calculate any, given any two of these things, you could calculate an edge of it. And therefore, all the knowledge you need is contained here. And, so it brings up the idea that you might I don't know, I'm thinking, but you might be able to dynamically do the equivalent of the graph with this information. I'll have to think about that. Oh, you can. You can. coding can. Yeah. As a neural system. Yeah. Also, remember, we can run things much faster than neurons again. it might take, hundreds of milliseconds or tens of milliseconds in the brain, but the brain doesn't want to do that. we might be able to do it super fast and just run through all these combinations really fast and just, bingo, I don't know, it's an interesting question.

here's just a, just imagine just diving into details. What am I getting here? I'm getting, I'm, I don't want to start just getting out of it. Yeah, and I just have to leave something for my talk slash doctors. Okay, so that's it for today. Hey. I think as an outlook for a cool demo on a bit larger scale, I would say that any behavioral task would be easier to solve with this as a backbone to understand and conceptualize the world as opposed to using high dimensional sensory input. So it would be cool on a next future step to test this on like a behavioral task or something. When you say a behavioral task, what do you mean? Anything you need to do in the world, like Subutai had the example of picking up the phone from the chair, would be so much easier if you have all these models to understand the world as opposed to having the raw sensory input from your retina, basically. Yeah, I agree. So here's my problem Viviane that's my number two up there, right? Number two is motor, right? Work on the motor stuff. My problem is, you seem like you're right. All the information we needed to solve really interesting motor problems is sort of here in the system. But I don't know enough about what people have done in this space. I don't even know enough about what they've done in the neuroscience literature. I don't know anything about what they've done in the robotics world. So to me, that's like a big open, oh crap, there's a lot of stuff to learn there. To me personally, maybe you, could figure out, I don't know. So to me, yeah, I want to work on that.

but it's a bigger, it seems to me like a somewhat daunting problem, because I don't even understand how to phrase problems. I don't even know what a very specific task or problem that I'd want to solve would be. if we could articulate some very precisely that things we know the brain could do that people don't know how it does it, then, that would help me at least think about it. maybe other people too. so I agree with you, it just, it's just oh, that's an unknown space for me. It's Motor commands you mean in general? What's that? Just how, even just asking what is, a, difficult motor task for, a robot hand to do? I don't even know that. Yeah. Yeah. And what it's like, sometimes I'll say something and Subutai will go, oh, that's easy. And everyone goes, I don't know, okay. And other times they're like, this is easy. And you say, oh, that's really important. I don't know, what's, what are the equivalent, touch points in robotics? Yeah, I would definitely say there's a lot of room for improvement in the field. And I think that having such a model in the background would make it crucially more efficient because it adds so many inductive biases that are specifically for the kind of 3D physical world that we live in, that it would make it a lot more efficient to learn and understand. But I need some specific Problems that people say this is hard to solve and they need to be, anyway, that's what I need.

I don't know what those are. to be honest, I just don't know what they are.

Christian.

Christian.

Sorry. Maybe, I'm sorry.

I just didn't make the connection between Chris and Christian. Can we at least let's start talking about that and I'll propose again, maybe you know better than anyone here, what are like the difficult challenges, in the field. Maybe you can propose something, give a short presentation and just so we can get the ball rolling on that as well. Yeah, sure. I can make a short presentation on some of the challenges in the field currently. Yeah, that'd be helpful. Another thing that scares me a bit about it is anytime people start dealing with your. physical movement and then you either, either you have to have a physical robot or you have a simulation and people always say that simulations really don't capture the problem. So in, I don't really want to delve into bio building physical robots here. So there's that aspect too. It's like, how do you, what, was, what makes a sufficient demonstration that you can do something? Is the simulation great enough or not? And it's a lot of work to do simulations, so just know, just think about that a bit. We don't want to start a project that takes us a year just to get the physical, the mechanical work done. But luckily, they say that simulations are a lot easier than, but in fact, we have a lot of tools at our disposal. Physical robots are still very hard, I don't think we have to deal with that, but simulations are quite hard. How real is this kind of thing? I've heard in sort of general experience that simulations are discounted because they don't really deal with that. They don't really deal with real problems. so I don't, think that's true. It used to be more true in the past, I think. But the recent, ah, shit, sorry. the recent environments, sorry I cut you there. I'm sorry. All right.

Reason inside virtual reality might be an area to build. Oh boy. there are, if you don't want to build it in real. There are simulation environments that are trying to be realistic for testing robots, but I don't think that's what we want to build, because those are slow. I don't know. No, they're not slow, that's my point. Yeah. The ones we're discussing, I've got the very realistic, things, and they're fast. Yeah. Yeah. So maybe five years ago there were slow. I was looking for a 2021 paper, don't compare me, I'm there about four times real time, and I don't know. What's slow doing? High time is like super slow. What does it mean slow? Doing what is slow? we have to run multiple experiments, right?

Maybe. That's the question. if you're just trying to simulate the real time, yes, but real time is too slow. And if you're trying to do machine learning, that's my point, you need a million samples, but we shouldn't, the whole point of this, shouldn't be that. That was my thinking, we're not doing this long. This is, this should be fast. This should be super fast. Yeah, exactly. That should be the advantage of having this model that we can dramatically increase the sample efficiency.

this really should do one shot learning in the sense that one shot meaning sensing one feature at a time is sufficient. like I can't learn a new composition or new object in one presentation, but I can do it by doing a series of presentations, one at each location, and then I can build a model.

so it's close to one shot, but you shouldn't have to do many presentations, unless you're trying to learn one of these lower ones down here. Those which are learning statistical regularities in the world, you'd have to do multiple, training sessions. But this, I'll clear this module pretty quick.

I don't know. Those are interesting implementation details. Viviane, it would be great if you could present, as Lucas suggested, some sort of these are some of the key problems that people would really think was amazing if you could solve.

Yeah, sure, I can do that.

Just to chime in here, I think we would want something that's a lot faster than real time to train these models.

Yeah, but like this, for example, the habitat we're discussing, it's running at a thousand frames, per second, something like that. That's crazy fast. yeah, exactly. That's what I'm saying. It can't just be, it has to be much, much faster than real time. But machine learning is still super hard on those, as long as we are not doing machine learning. So if I were to prioritize these, I would say, look, we can build a system like we talked earlier about that recognizes something from different positions. That's an impressive thing. We should do that. The motor part, behavioral part, is going to be harder. And, it's going to take a lot more discovery, and we don't know how to do that yet. it may become obvious shortly. That's my hope, because now I have a framework to think about it. but it is an unknown. And there's a lot of unknown things. I wouldn't, take that as the only one to do. I think we can start working on it, but I wouldn't wait until we figure that out. I don't know how long it would take to figure out those components. Yeah, I wouldn't say start on this tomorrow, but I think this is where we could show the most impressive results in the long run, because in the end, our brain, that's what it's trying to solve, trying to behave. I agree. In this world. Yeah, I agree. in a sense, AI and robotics are going to merge into one in some sense. It's all the same mechanism. we should be able to solve really interesting robotic problems.

All right, you can make a presentation on what would be an interesting problem.