So it should say sub goals, right? Yeah. And, I, I say sub goals because we're trying to understand how the, brain not only understands behaviors of objects, but affects behaviors and makes changes in the world and solves goals. And so really what we're talking about is a subset of that, which is, how do we learn models of behaviors? We make an assumption that we do, we learn behaviors. And Viviane gave out some nice pictures of that last week. and, then we wanna say, how do we predict? One of the things we've been focusing on is how do we predict a changing morphology of objects based on a behavioral model? Like how could we predict what the staple will look like when it's opening or a novel object? And so we, wanna apply behaviors to, not to the, just to objects that was learned, but to novel objects as well. So this is the only thing I'm talking about today. This is a subset of a lot of stuff we have to do. and we can think about it like, and I'll give, I'm give you, I'm gonna just make some statements, make some arguments, and then give you a bunch of examples that I walked through to help me think about this problem. so lemme just start with where we ended up. lemme start with a definition. a definition, of a behavior is when a child object changes relative to a parent object. We're gonna, again, we talked about this before, we're gonna abandon the idea that this can be done in a single column. It requires two columns. It requires a parent and child relationship, much like we have with composition objects. therefore behaviors require a parent object and a child object. Okay? So two and two learning modules. and then I'm gonna just state and, we've stated this last week as well, that behavior models exist in the parent object and not the child. So if I have two learning modules and there's a parent child, child, a parent object, a child object behavior model exists in the parent object, we can't do this in a single, a single learning module. so that's, yes. Just to clarify, at the lowest level, like the first definition, a child object change relative to parent object, can this child object be like the, feature that comes from the sensorimotor directly that changes like color changes? that's a good question.

and I've avoided that for now. One of the things I've learned about thinking about hierarchy is that if you focus on the primary sensory regions, it gets, it really messes up your brain a bit. And, then typically when you figure out the hierarchical region between two learning modules or two columns. Then you could back off and say, oh, So what's the acceptance exceptance that's happening in the, primary one? I didn't do that in this case. That's a good question. I, didn't focus on that. yeah, I think the probably will be, one thing, you, might be able to predict changes of orientation, but you won't be able to predict necessarily. you certainly wouldn't be able to predict an entire object, child object changing. So my guess was it would apply, but with some kind. But yeah, it's just, just simpler behaviors that can be learned. yeah. It or trialed objects. Are you willing to accept that Viviane or you wanna make some more points? Yeah, no, that, that makes sense. So again, this is a sub that, so this is a subset of the subset. I didn't deal with that particular point right now.

and then this is the main idea that. I came up with, and Viviane came up with, I think this is what, when I came up with, I think it's half of what she came up with, that was, felt like the big insight. And the big, and I'll explain this in multiple ways, that what the behavior model, how it communicates with the child. So you have the learning module on the, I'll, show images of this in a second. You have the learning module, that has the parent object and the behavioral model, what it sends to the child object is different than what we've talked about in the past. In this case, it's sending movement information to the child object. In some sense. it's not. And I'll explain what I mean by that. So here's a, here's two pictures. on the left is our, standard compositional model. And I didn't show all the details here. I'm just saying, okay, there's this 2, 2, 2 objects. A parent object and a child object. And they have their, the child object is, part of the parent object? It's, in the, it's a subset of it in some sense. we have two morphology model models. One for the child and one for the parent. Going back to the coffee cup. That would be, look at logo on the left and the cup on the right and the way, and they both are updated. They, have, they both have a, point where the sensorimotor is focused at that moment in time. so there's some location on the parent object and an equivalent location on the child object. That location gets updated by the sensorimotor movement. So Sensorimotor movement comes in and through path integration, we can move the sensorimotor around on both of those objects and the objects are tied together by the Blue Arrow. They're tied together in a way that the location and the ID on the parent object at that location is associated with the location ID on the child object at that location. So we can say, oh, at this point on the coffee cup, we're at this point on the logo. Of course there's orientation and scale, things like that, but the basic idea we're pairing on a one-to-one basis, the child object and the parent object. These are tied together through this Blue Arrow connections. And we, and that relationship changes as we move around in space. at a different point on the parent, I have a different child object, and so on. So that's our old compositional model. On the right, I show the same model, but now we've added a behavioral model, the behavioral model shown in green on the parent and, Vivin, we've talked about what that looks like. I'm not gonna show what it looks like here, just say it's a model of behavior. and in this case, the behavior model also sends a movement command to the child. it, does. We can't tie a. You'll see why in a moment you can't tie the behavioral model directly to a child object. 'cause we wanna be able, the behavioral model to apply to different children, different environments. So we can't tie the behavioral model and the parent to the morphology model and the child, because the behavioral model's gonna apply to different children. So the only way you can do this is by sending a behavior, a movement command to the child. That movement command does not cause the child object to move. It just changes the location of the, expected location on the sensorimotor of the sensorimotor to the child object. It's doing the equivalent of the brown arrow In some sense. It's, telling the child object. You know what? It's as if the, it's as if the sensorimotor is moving, but it's actually not moving. We know that the actual, the child is moving, this whole child object is moving, but the relationship between the sensorimotor and the child will change. And, therefore, if we wanna make a prediction. The behavioral model says, I know that the child should be moving right now, and if the child's moving, the location of the sensorimotor on the child will be changing. So I'm just gonna tell the child object that the location of the sensorimotor is changing and it can make the correct prediction.

It's, this is all these, what we might call allocentric models. They don't really know where they are in the world. The left learning model doesn't really know where the child object is. It's just saying, I have a relationship between the sensorimotor and the object, and I'm gonna change that position. So this is the idea about the basic idea about how a behavioral model could lead to predictions in, a child as a child object is moving. it says, oh, I, know how the stapler top is moving. Therefore, I can tell that the, we'll get into the staple of top in a moment, but I can tell the model that's modeling the staple of top that, it's moving through the world. But, basically what's gonna happen is where the sensorimotor is, gonna change. So these two, there's these two sources of movement information. One is the actual sensorimotor is moving relative to the parent. and one is the child is moving relative to the parent, which will make it look like the sensorimotor is moving relative to the child. and that's the gist of the solutions that, that, Viviane and I came up with. and before I go, I'm gonna now give you the next slide will be a whole bunch of examples to walk through this way to think about this problem. But before I do that, and that'll make this diagram more obvious. But before I do that, does anyone want to ask questions about this?

Did anyone not understand it? Raise your hand. Don't be embarrassed. I didn't know that. I'm a little unclear, but I'm just gonna screenshot this slide and then, refer to it while you're going through samples. it might help me, Scott, tell me what part that's unclear. Everything part of it. like I'm looking at here, it says, let's see. Does not cause child object to move. It changes the expected location of the sensorimotor to the child object.

so I thought that it was the actual child object itself that was moving relative to it is object. It is it, the child object is moving uhhuh. But, at the moment, the diagram I've shown is only trying to predict the result of that movement. It's not making the movement happen. It basically moves you through the reference frame of the child object. So like the reference frame, the more we are in, the bottom, the left column, the child column, we have a hypothesized location of where the sensorimotor is in that child object's reference frame. And then that, movement command from the behavior model moves that hypothesis location in the reference frame of the child object. Okay. Isn't, I think about it. Got I, understand that's a, it's a confusing concept, so I, totally get what the confusion is. Imagine you'll see in a second. Imagine I have an object. It just has its own behavior and it's an inherent behavior. No one causes it to do anything. It just does it on its own. I'm not making it happen. It just does it, it like, it's, I'll give you some examples in the next slide, but, the hand on the clock just moves. Okay, just moves. I don't have to do it, it's just gonna do it on its own. so I don't, the cortex or the learning module isn't making that child object move. It's moving, but we want the, but we have a model of it, and we want to predict what it's gonna look like after it's moved or while it's moving. So the goal of this green arrow is not to make the thing move, it's just saying, given I have a model of how it moves, what should it look like after a move? Remember? Viviane O'Neill had a couple images where we talked about a motion through an obs, an obscured area. I think, I know Viviane had this, and it's like we could predict, if the stapler opening and it's moving through an obscured part of the scene, we still predict it's gonna come out the other side at the right point in time. So we're just trying to make that prediction. We're not trying to make the object move, we're just trying to predict, oh, I know what's going on. I recognize this behavior. Or someone's swinging a bat at a ball. I expect the ball to move. I'm not making that happen, but I can predict what's going to happen given a behavioral model. and so that's all this is doing. I, and, reality changing the location of the sensorimotor to the object is what would happen if the object moved. It's also what would happen if the sensorimotor moved. So you can move one or the other. I could say, oh, the I my eyes fix it at some point and now I move the object. if I don't move my fixation point, the, location on the, what I see is gonna change based on how the object move. and that's all the Green Arrow is doing. It's saying the object is moving. what should I predict? I have to know how the object is, changing, how the child object is moving. it's direction, it's speed, it's change in orientation. All these things, that the green arrow represents. I just have to be able to say, okay, the child object is moving, therefore the relationship between the sensorimotor and the child object will change. I wanna make a prediction. I have to predict where that new location of the sensorimotor will be given the object is moving. I dunno if that helped or not. It's, hopefully Yeah, that makes sense. It was just like the coupling between what the tiled object is and the sensorimotor resting on it. It was like A little bit unintuitive at first, but it makes sense now The only thing you have to about an object is what your sensorimotor is telling you about it, so it's Right. So Matt, if you can see my hand here, I don't know if you can see my hand in my image. my finger is I'm touching my hand with my fingertips. So this is a sensorimotor and this is the object. our comp, our normal OB models, we, move the sensorimotor over the object. We make predictions now. We're just, now we're basically saying because of behavior, the object is moving.

And, it, I, it's the same thing from the models, from the model of this object. It doesn't know it. This child model doesn't know if it's moving or if the sensor's moving. It really doesn't know. It just says, okay, there's a sensorimotor at some location. It should predict this. Oh, now it's in a new location. did it move this, did this move or did this move? It doesn't matter. It can't tell the difference. and in the case where the, sensors following the object, the kind of smooth pursuit, case where we have the sensors moving and the object is moving. it would basically be that the sensorimotor movement and the movement from the behavior model cancel each other out. So we keep predicting the same location on the child object, which is exactly what we want. If we are following the child. We keep being on the same location on the child object. So the green arrow and the orange arrow will cancel each other. Yeah, exactly. Yeah. I'm not proposing any mechanisms here how to do this. I'm not showing this in layers of cells. I just felt we had to take a step back and try to get the basic idea down.

does anyone else want ask a question before I go on to the next slide? yeah. I have a quick question. So this, makes sense to me. and we're talking about movement right now. Are you gonna cover cases where like the color is changing? 'cause that's confusing for me right now. Yeah, I will, but not as much. I think that's the easier case.

you know what? I did think about it, but I didn't put it in my next slide. I have one.

I didn't really, I didn't really prepare to talk about that today. This is like the, like the traffic light, it's changing from red, the green. Exactly. Something like that. I think that's the easier problem actually.

and and I didn't develop it here. Today's presentation, all I developed was prediction for movement. so let's come back to that. Can we come back to at the end? That'd be a good idea. Yeah, absolutely. Yeah. Again, yeah, just for completeness, so relative changes in orientation, like if the child rotates relative to the parent that like, just to confirm in your proposal, like that wouldn't move you through the location space, but instead it would, adjust the rotation hypothesis. I didn't put that in this diagram. like I said, that the, I said the location ID is shared between the parent and the child. of course, it's, we're actually sharing orientation as well and scale, and I didn't put those in there. So this is a real sort of conceptual thing. It's not going to all the things that have to happen. I'll show some of those in the next slide. yeah, I think it, it was a good idea to simplify it because, yeah. Yeah. it, already, it felt promising and right when you were presenting it, last week, Viviane. But, yeah, I think this, it does help to show the, kind of core thing, and it, definitely seems Yeah, like this could be Right, it's really just this really basic idea that. We have two ways for a parent object to talk to a child object. One is through this mechanism we've identified before, which a associates the exact parent with the exact child at in very in precisely. And then we have another way which says, oh, take the behavioral model. And and we can't associate that with an exact child. and we can't know everything about the locations and so on. So we have to, go it indirectly.

so right that, and, that's it, everyone walks out of today. If that's all you remember, that would be a successful, I think. let me show you some pictures here. I apologize. I went back to using the coffee cup and the mento logo 'cause it was just convenient. I didn't have a thousand brain project logo with me at the moment. I was doing this. so apologize for that. there's is four little examples here and what I'm showing imagine. And so this is a magic cup. Don't ask how this happens, but you can imagine it would happen. So this is what I use in my mental imagery and so I, that's what I made pictures of. It's not maybe it's not the best example, but it worked for me. So on the left, you can see what I'm pos proposing here is imagine a behavior with a logo on the cup, changed position, just it's every, and I think I was thinking like, oh, every three seconds it would move from a lower position to an upper position and then three seconds later it would move back down to the lower position. I'm not showing that movement here. I'm just showing the two endpoints of the movement. so you can imagine this logo going up and down. See I have a thousand branch project here mug going. I got a special thousand branch project mug. It shows the bent logo.

it goes up and down and up and down, and that's the behavior. we can obviously learn that behavior, so we have to be able to make predictions about where the logo will be at every three seconds as it moves and the velocity and so on. We, the second one is we can just change the orientation of the logo. So the logo just rotates, again, same idea. It's changing down and then it rotates up and then it rotates down and rotates up magically. We can also change the scale of the logo where, it goes from a small logo to a large logo to a small logo, and then finally you could alternate between different object ideas. So we went from the, mental logo to a British flag. and then you go back to the mental, so magically it just changes blank, back and forth. It gets more complicated. You really just can't change the orientation. It's, I imagine that when I change the, second image here of the, below, the first one, the mento logo is changing its orientation, but it's rotating on a different point. In this case, it's rotating on the corner of the triangle. In the upper one, it was rotating on the center of the word mento, or like around the m. So you can't just say, oh, I'm rotating the logo. That's insufficient. This, gets back to the issue of, and we don't wanna specify a rotation point, right? So this, which is the tempting thing to do. but we know that's not works. And so the way, the answer is, we have to learn this on a location, by basis. So this in some sense, proof that. and the next one where the logo bends, this case you imagine the logo is straight and then four seconds later, three seconds later bends, and then three seconds later un bends. You can just animated logos here.

these just illustrate again, this has to be done on a, location by location basis. You can't do this. Like the entire child object is rotating or the entire child object is moving. we made that mistake with, composition objects, we then figured out how to do it. The same thing happens here. It has to be done on a location, basis, and the system can inter interpolate between points so we don't have to learn every single point.

so these same, these are all very similar to what we did seen before. I thought of another example, which is, this is a weird one, and it fooled me for a bit. Imagine I had a magic logo and any place that logic logo appeared, it would move. So I would place the logo on my, the front of my refrigerator and it would go up and down, and if I placed it on my laptop, it would go up and down. And I said to myself, that would imply that the model of the logo. The model of the behavior, excuse me, is not on the parent object. It seems like it would then be part of the, logo itself. It's the logo that moves up and down and no matter where I put it, it would go up and down. So I said, that seems odd. That's a counter example. And I concluded in the end that it's not really any different. it's, it, just expanded my idea of what a object model is, a morphology model. And so I've shown a little dotted line here, and I, think the way the brain would handle this is that it has a reference frame for the logo and it has, a bunch of features in that reference frame. And, or you could argue it's one feature if it's the whole logo, I don't know, but it doesn't matter. and all the features are moving the, behavior of that larger reference frame, the dotted line. Essentially that all the features in that reference chamber are changing. In essence, there is still an apparent object. It just, the parent object has No, it is just a co a concept. It's like there's a reference frame in which the entire, all the elements in that object move up and down. They have two positions they can exist in. and I know that sounds a little, I mean by think like a cheap, but I don't think it is. I think mentally that's how you end up visualizing it. oh, but couldn't you, that could definitely be happening in the, lower level, but couldn't you just have a third level in the hierarchy and basically the mento logo as a child on this little dotted square, which is maybe it's like a magnet or something, that you can put on different places. So basically that's what I do. That's what I do, that's what I'm suggesting. I'm suggesting there is. Yeah, you wouldn't have to move the individual features in the logo. You've already learned the logo. You can just move, I guess, what I'm saying is, there's an object.

moving features is what behaviors are, right? It's just moving reference frames. parent object. Maybe you're more saying, Jeff, that it's just an invisible, parent objects an it's just like an anchor point. It's an extra object that, that has no, has that, that it's like Viviane says it's three levels. You have the, yeah, if I'd have the cup. Then there's this object, which represented by a child object, which is represented by the dash lines, and that child object has behaviors, which moves the features of the mental logo up and down, Okay. Yeah. So maybe I didn't phrase it but the, point of this is I, at first I puzzled about, I really spent a lot of time trying to figure can I prove that? Can I be absolutely certain that the behaviors always exist in a child parent relationship That is, I couldn't, I wouldn't, I wouldn't have an object where the behavior applied to itself. and, I wanted to always say, oh, there's always a parent child relationship. This seems at first, like a counter example. but by, I, I think mentally what's going on is what we just said is that you've created a new object. It just basically has no other instantiation other than a behavior where the child components go up and down. One way to think about this, imagine that Rectangle wa was actually visible. Maybe it was like a sticker. And this was like a, little badge measurement was like a magnet. imagine it was like a magnet that goes on your refrigerator. And this magnet had a little unique icon on it. So I knew that this is, this, special magnet. Then anything I put in on that magnet would move up and down. So it would like, oh, so if I see that, if I saw that little magnet and then I stuck on Memento logo on it, I would expect it to go up and down. It'd be like, it was, it's the, it's behavior of the magnet, not of the logo. yeah, I feel like this is a really interesting example 'cause it's, it gets at like object physics, which is like something we've often puzzled about, but it's you can imagine gravity is a behavior like this where it's you just like a put, you just put an object in that behavior basically, and it starts behaving, according to gravity. That's interesting, sort of modeling physics as a, a behavioral, essentially a model of. It's like the refrigerator magnet.

and it fits because like we can imagine it's the world. We can imagine an object that isn't subject to physics or we can, we can play a video game where there's unusual physics or something like that. And so it's nice that we can apply it arbitrarily, it's a good point. Yeah. That's interesting. It's a good point. Niels, I, just wanna remind what I said in the beginning. What I'm presenting is a sub, a set of subset of the ultimate problems we have to have. And, I'm just trying to argue that this subset is valid and so then we would build upon it. and so that is an example of building upon it, thinking about physics, and how they would apply. Yeah, I think that would be a nice way to solve physics too. 'cause I think that's something you also shared, Neil said in the brain, there isn't like a physics engine area or something, a specific area where physics are represented. It's more like everywhere. So that would fit with. Kind of behavior models of like how objects fall and stuff like that would just, and they could fold, they could fall different in different environments, right? yeah. And the weight of the object, changes the, the Kitty Science Museum, they have a, big air tube with this air going up in it. So you can put things in it and they can go up and down. physics doesn't apply, but, once you know that air tube, you don't expect physics to apply. It's like you, very quickly, the things can go up and down. They don't just go down. yeah. Okay. That's interesting. I don't have a picture of this, but I'll talk about it. I also, thought about things like objects moving through space, like a bird flying, or a for American baseball, like a ball that's hit in a ballpark and it, leaves the batter box and it goes into an arc. And, I just, I'll just share some of the thoughts I had about that. I said, If a ball moves in an arc like that to the ballpark, that's not a behavior of the ball. It's actually a behavior of the ballpark. It's that in ballparks, I expect balls to start at one point and move through the air and trajectories. and I, have a sense of will they end up, and I've seen lots of examples of this, and so they might bounce on the ground or they, various things, but there's behaviors that a ball could exist in the ballpark. But the, it's not a, it's not a behavior of the ball itself. It's a behavior of the environment or the ballpark or something like that. and, obviously the, I could substitute a different item that would perform the same. basically physics moving through space, and then I thought about a bird and okay, a bird can move across your scene. and it's flopping its wings. clearly the bird flopping its wings is a model of the bird. And I don't have to see the bird moving to see that. I could, have a, video of a bird. I'm just seeing it flopping its wings. And, and I can recognize that behavior, like the banana walking, even though the banana's not moving. but I can also imagine, I said, what would, about, what would it be if the, if, the bird just flies across my scene? I'm getting a little bit off topic here, so I hope you don't mind. and I said, if there was no background information, it'd be very difficult to say much about the bird. but if there was, like, if the bird was in my living room, flying around the living room, then I could learn the behavior of the bird flying around the ribbon. Like the bird would go to this corner, then that corner, this corner, then that corner, bang against the window, whatever. So again, I just wanted to, I, was thinking about examples where, like a ball or the bird, the ball does not have any behaviors on its own that I can think of, but it is part, it can move as part of a behavior of the ballpark, in the physics associated with that. And the bird has, does have behaviors, but its movement through space is not part of its behavior. That's part of the spaces of behavior, anyway, that's a little aside. All that was consistent. Now we, unfortunately, we started, or I don't know, it's unfortunate, but we started with this example of the stapler and, and that really is confusing for quite a bit. so we've already talked about the solution. I just wanted to bring it up again. In this case, the child and the parent object are the actual same objects. So the two learning modules would be observing the same object and when it starts to move, if we're gonna make predictions about it, then we have to maintain the model in the trial object. We can't, we, and we, basically decided that the way this is gonna happen is that somehow the child object will be, will be somehow masked. So it's really only attending to part of the stapler, the part that's moving. and then once you've done that, the, the, everything else works.

And then over time, that could be used to segment the stapler into, top and bottom as like separate, right? It could be doing that.

I, Viv and I talked about related to that, Viv and I talked about this issue related to key frames and, and so do we remember all the positions of the stapler? And here's how I ended up on this. and that would be true of all these other examples, right? Do we remember all the in between positions of the logo changing and, so on?

I just imagine the following, a learning module says, okay, I'm always trying to learn the morphology of an object. So I'm always, attending to things and I'm trying to make connections between the features and the locations on the object. if that, if those features are changing or moving, I really don't have time to learn the morphology. It just goes by. I don't really have time to do it. But if the thing stops. then I can learn the morphology. I can continue on. So look at the example in the upper left hand corner, the mental lo in the two positions on the cup, there's no preferred position. And I drew this diagram specifically to make it look like there's no preferred position. there isn't oh, the organ is supposed to be in the center and then it moves up and down. No. and where, what I think the learning module would do, the, parent learning module would do, it, would learn the behavior of the logo of an item moving up and down this case, the logo. But if it stopped for three seconds at the top and stopped for three seconds at the bottom, it would learn to different, morphology models of that object there. There're there, it's, not like there's a preference. It's not oh, the, logo is always on the bottom and then it moves up and down.

and and if it stopped in the between, I would probably learn that one too. So it's fluid, it's the way I'm proposing, and this is true for all these things. If something is moving, you don't, really have a chance to learn a morphology object. But if it stops, then you can start to learn a morphology object as a, and you might call it as a, key frame, but it's not, acting as a key frame.

and, remember in my, I believe that this is very efficient in the brain to learn these in between positions. so it's not like a waste of huge amount of resources. it's very quick, but it doesn't have to be learned. That is, I can still predict where the logo will be in the middle if it's stopped. but, I don't have to learn it. And so it'll always be trying to learn the different morphologies of the behaviors. If, it has a chance, if things stop for a while, it'll say, okay, I'll try to learn this. So in all these examples I gave, I imagine them stopping every three seconds, and therefore I would learn all these as valid morphologies of the cup. and yeah, and that would true of the stapler as well. Yeah, I think that's a, I hadn't thought about it that way un until you mentioned it in the slack, thread. And I think that's really nice way of thinking of it because it's basically just taking our initial premise and ruthlessly enforcing it. Like basically whenever something is changing, we put it into the behavior model. Whenever some static features are observed, we learn it in the morphology model. So if the logo stops somewhere in the behavioral sequence, it will go into the morphology model again. And I guess the nice, maybe the, purpose of learning these different, states of the stapler would then also be to efficiently recognize the stapler, Right. Once it's open, if you just see the stapler in its open position on the desk. Yeah, it would be much easier to recognize if you have actually stored that in the morphology model. 'cause otherwise you would have to like mentally simulate that behavioral sequence and thi and see if that matches with your stapler model. Also, if you take the, if you take the two, like the two upper and lower the mental logos, if I would just say, oh, imagine that, and you had that cup and then the logo went up and down every three seconds and it went up three seconds and down after three seconds. If I just said, imagine that cup you, you might alt, you would imagine one of those two and maybe alternate between 'em, but you wouldn't imagine it in the middle. It's 'cause that's not a point where it stops. Just you, if I just said I'll take the cup outta the counter, either it could be moving or it could be at the top or it could be bottom. I wouldn't expect that. See in the middle, right? Because it never stops in the middle. So it it, helps you infer, but it also, I just, that's just an observation that you wouldn't know which one to predict. I then I asked myself, what if the logo went up and immediately came back down? It never stopped at the top, right? So it just went up and down and so then there would be like, okay, the logo is, supposed to be at the bottom and the behavior moves it around, but it comes back to where it's supposed to be. so I just went through all these mental things to make sure it would work, and I think they all do. I think I have one more thing on this page right here. Oops. Was that it? No, I think that's it. so that's all I was gonna present today.

And if Viviane wants to take a stab at explaining what it is I got wrong that the parts you, she was trying to explain. no, that's yeah. Basically exactly what I was thinking too, so that's perfect. but you, did say there's the problem of transferring location. I still don't understand what that problem is. Oh, the sec, the thing that got me thinking about it. Initially was, the feedforward version of how the behavior model is learned in the parent object in the first place. So how do we learn the changes in location? And the answer I came up with, and I think that you agreed with maybe, is that basically it also just gets the sensory input and, like it's, it, basically just learns location changes from the sensory input it gets. I, can I counter that? Because I wouldn't use those words if I were to guess what's going on. I would say we never learn loca, the behavior model doesn't learn anything about locate it. It's basically learning movements. It's saying, it's basically learning movements occur here. Yeah. At this time. So it's an observation. Yeah. That's what meant it's an observation in the parent. It doesn't come from the child. Yeah, maybe Viviane if you have a diagram from one of the ones you showed last time that could help.

yeah, let me see if I can pull one up real quick.

let's see.

I'm not sure if I have the, perfect one right now, but, So basically this is where started off with the one that what you just, walked through, but in a much more complex diagram, getting a lot more, stuff going on here. I tried to build it from everything we had before, but yeah, it was more, a lot more confusing. But basically this red arrow is that it's the arrow that you had shown in, green now, and that like similar to how the sensorimotor movement can move you through the reference frame. This, stored movement in the objects behavior can move you through the reference frame here. so that all matches, I think. And then the second part I was thinking through was how does this kind of local movement that we're sending down here get learned in the first place? And how does it arrive here in this column? So there are two options. One is. we de detect that local movement in the sensorimotor patch and then storage there. So these kind of arrows that are moving here. The second option that we could discuss maybe is that, the lower region could output the location of the child object relative to the parent, and then if the, no, it outputs the location of the child object relative to the body. And if the location of the child object relative to the body changes, that change is encoded in here. And so why if the former solution solves the problem, it's much simpler. yeah, it's much simpler, but it relies on direct sensory input to that column. If we would also output the location of the child object relative to the body, so in a common reference frame, independent of the child object. Then also higher order columns that don't get direct sensory input can learn that.

And I guess, don't we need the ladder for some types of behavior, like the ID change as well and things like that? it seems like maybe the more general thing is that we detect a change in the output of the previous column, like the union Jack going back and forth to the logo.

the funny thing about the union Jack is it's not clear that it's a behavior that can be generalized.

maybe, Remember I went from the logo to the flag. Maybe if I put a, a Greek flag there, it would say, oh, I expect the Greek flag to change back and forth to the local. But, in general, it's not like the other ones. The other ones, I could. Essentially take a stapler that doesn't look like the staple I learned on it is very, different. If I applied a behavior to it, it'll predict the, position. It'll, correctly whatever happens to the, as it moves. I don't know how that would work on the ID change. yeah. So for, ID change and orientation change, I basically assumed we, we get that information from the lower level column. So we have the, the detected ID from down here. That goes well. But can I, stop for a second? Why does the orientation change have to come from the child column? Can't that just be, imagine as I got it from the sensorimotor, isn't it sufficient to get it from the sensorimotor?

I guess 'cause I thought it's the relative orientation of the child to the parent. And not just some, low level sensory orientation so that we can Yeah. Make a direct prediction about the relative orientation of the child. same as we sent the re So basically I was thinking we are already sending the relative orientation of the child to the parent up here. So that already exists. And then we are just basically saying is it static or is it changing? And if it's static, it goes into the parent's morphology model. If it's changing, it goes into the parent's behavior model. Yeah. The way I thought about it is, just take vision and the retina provides, going back to the example I've used of someone playing a video game and you're watching 'em, the retina provides all the information you need to know about how the player is moving and that encourage or changes of orientation, forward back or left or right, up and down, but also changes of orientation. And so that information is available. It must be coming from the retina. And so I just said, great. All that information can be, picked up from the sensorimotor. I don't need to rely on any other connections between the parent and, the child object. and so that's simple. so I'm not sure that's right. maybe they're some things that you, that can't be done that way. So the two are the two, the counter arguments are, oh, what about something which I'm not getting direct sensory input from? That's one counter argument. And the other counter argument might be something related to changing IDs, which I don't understand yet. changing IDs, I agree with you that this is not so generalizable. yeah, I don't think we can just generalize which Id changes to what, so it's like a fixed change that we'd have to store. But I guess I was. For changing IDs, I was just, again, doing the same mechanism that we already have. We're sending the ID up to column two and learning that on a location by location basis. And if the ID stays the same, it gets stored in the morphology model. If it changes, it gets stored in the behavior model. Alright. I'm beginning to understand what you're saying there. in fact, what, I would argue is that it's always being stored in the morphology model. even if it's the, even if it's not changing. So if the stapler, if the logo's not changing or the staples not tops not changing, there's no harm. It's still saying, I'm looking at the logo. it's, the connections are there, you could learn them. But if, the ID changes, then it would say, oh, now I have a new ID at this location.

so I'm agreeing with you. I think the, change of ID would involve the layer three to layer four connection. 'cause it's changed. Yeah, but it's not a special case. It says that the lay layer four connections are always trying to learn, connections and, if it changes, we'll have to learn some new ones. but if it's not changing, there's nothing, to learn. Yeah. And it's like you said, most of the time it would be laid down in the morphology model, like one change from one flag to the other. It will, most of the time there's no change happening. So we're actually learning the flag or learning the idea of the flag on the, cup. But then when it changes that basically like that way we can know like the timing of when in a temporal sequence does the, child ID change. Yeah. It's a little bit like, imagine again, the logo that's moving up and down on the cup. What if it didn't move up and down? What if it just appeared at the top and then appeared at the bottom? There was no movement in between.

But we do know if the logos are close enough together, you'll perceive movement. But if you separate 'em apart enough, you won't perceive that it's moving. You'll just perceive that it disappeared one place and appeared someplace else. That in some sense, very much a changing, ID like the flag. It's oh, I had a child object here and now it's gone. oh, by the way, there was no child object up here and now it's here. and I put, the child object was the mug and it's changed too. say, oh, that confused me. The mug. I thought I was thinking the child objects the logo. No, But as in when it's just a, mug, both the child and the parent are the mug, so there's still a change id. It is change. Exactly. That's stuff ahead of me there. Yeah, that's an interesting point because maybe that's why we like, with a lot of UIs and stuff, you usually simulate some movement because maybe it's easier to do this kind of, movement change, in, instead of actually having to lay down an, like actually having to learn a change in object ID at a location. The brain wants to detect movement. That's why when you show two objects and they, and then you turn off one and you turn on the other, the, you often will perceive it as moving. You'll say, yes, it moved, I saw it moving and it didn't move. Your brain wants to believe it's moving.

but if you separate 'em enough, it says, no, didn't move. This would reappeared. You could also mess it up by saying, let's say, the, bottom logo turned off and then half a second later, the upper logo turned on. or maybe they were overlapping the bottom logos on and for half a second, the upper logos there and then the bottom, so you can mess up the whole concept of movement. It just, it, I guess it's really that, changing. it's really oh yeah, there's, it's, you have a model and at any point in time a new, child object can appear and another one can disappear. but they're not moving physically. Yeah. It could be a, maybe a weak argument for communicating the child optic location relative to the body, because that way you don't actually have to detect optic flow in the sensorimotor, but you would just detect a change in the location relative to the body. which doesn't have to be a continuous change. It would just be a change. Yeah. Yeah. It feels like it's not, it doesn't need to be either or like that they would both be useful. maybe. I, went down a real rabbit hole, Alice in Wonderland type thing. When I started thinking about, oh, okay, do we have to incorporate both egocentric reference frames and allocentric reference frames? the what in the where pathways in the brain, because when it actually comes to manipulating things, you need to know where they are. And, so I started trying to do, got so overwhelmed with that and then I came back and I said, I can, under, the problem I'm trying to solve is predicting what it's an object's gonna look like after it's at, if it's executed some behavior. And that does not require knowing where it is in the world. It's just, it's all allocentric. It's all just this object. I don't care where it is. It doesn't matter. It's just I'm attending to it. And, so I, think when you start introducing things like body centric, it scares me. but I think it's actually, or maybe shared, coordinate system is maybe the better term, right? Because at any given point or any given part of the cortex, it might be generally using allocentric or it might generally using body centric, but it's, it doesn't really matter. I guess the way I would fit with this, I'm happy that I could maybe solve the particular sub goals I set out, without using the, the, other reference frame, the, common reference frame. And then I would like, I would prefer then to have a, problem, a task that absolutely requires a use of, a, third party reference framework. yeah, and then, using that to, to, because it's gonna be a difficult thing to think about. so I guess two points. one, it's very straightforward information that would be available in every cortical column because. We get the sensory input, we can get, I guess if it depends. So let's say it this way in Monty and how the cortical messaging protocol is defined. we communicate in a shared co coordinate system. so if a sensorimotor, like whatever sensorimotor modules are sensing, they sense it relative to a common reference frame, like relative to the body. so every learning module has that information of where the sensorimotor module is relative to the body. So if it would want to output where the object is, it's straightforward, just take the location of the sensorimotor and add where you think you are on the object. and I'll put that, and then that would be a, perfect way to stay within the cortical messaging protocol to have that object location in the body centric reference frame. the, I guess the issue with the brain is if we are only communicating movement and not locations relative to the body, then like the movement information doesn't contain, this. but still the brain to interact with the world would need to have a representation of relative to the body. So maybe that's just in the wear pathway that's happening, but I'm, just saying if we assume that we get that information as input to the column, it's easy to output it. It, again, that's a really, interesting point that you already have it there in the, the learning modules. I, made a conscious choice this time not to show the layers of the cortex and the connections. Picking up on, I think you know, what some of you have been doing. because I think it was really the concepts that were important and and this could be a, really great example where the concepts and implementing are, it doesn't matter, we do 'em differently in the brain. maybe the brain has what and where pathways, maybe we don't need that here. maybe you've already taken care of it, taken care of it by having learning modules, have a shared reference frame. and that may be the more efficient way of doing it. so I guess that would make my life a lot happier if I'd never had to think about where Pathways in the brain. We just, thought about the problems we're trying to solve. We come up with the right solution for them in the, framework, and then you implement 'em the most efficient way.

that would be great.

So maybe that's the approach I'll take from now on. don't think about where pathways.

Yeah. I think just for clarity, it's always helpful to come back to the idea of just a shared coordinate system, and then it doesn't matter too much whether it's body centric or Or like egocentric or centric. I don't know what's going on in the brain, whether that's, that's happening or not, I don't know. But, anyway, but yeah, I think there's reasonable evidence that both those kind of exist in the brain, but Yeah. But to your point, like how important is it that both of those things exist? Is it a quirk of evolution or, yeah. Again, just like we don't, Monty today doesn't really do all this exactly how brains do it. we don't, there's a lot of things we do differently and, and that's fine, I think, but the principles really matter. and and we've laid down really good principles so far, and so we can, As long as we stick within those main principles of what's going on, the actual implementation I think won't matter as much.

and yes, so I guess maybe to summarize the last few minutes, it sounds like we agree that it's reasonable maybe that location information is passed up and if we find that it's helpful or necessary that, that, that could inform that kind of change in location could inform like the behavior models and you wouldn't just have to rely on direct sensory inputs. Although I do think, sometime I just, the thing the brains do, I think our robots are gonna have to do too, which is, detecting, very quickly detecting changes in the, sensory periphery and attending to them right away. what's going on over there, something's happening there, unanticipated and So I don't think we can ever get rid of that. So I'm not sure how that relates to this question. I'm not sure if, sending information location information to the child, the child has to know it's moving. How does the child know it's moving in relative to the world, But somehow there has to be very quick a way of saying there's a change in the sensory environment, that is not anticipated or, and therefore we need to attend to very quickly. 'cause it could be dangerous, it could be falling. Yeah, it could be hitting me. Yeah. I think in a realistic lower sensory region scenario, the higher column would, will be much, have a much easier time to, to detect movement of the child object from the sensory input. Then the child column actually detecting the movement of the child object itself. I guess I'm thinking a bit more like that. This will become more useful if we go higher up in the regions where we don't have the sensory input and we have like abstract space. I'm agreeing with that. I said that's great. I just don't think you can get rid of the sensory thing. Yeah. ultimately we'll have to have that. it's gonna be working in very fast moving environments and yeah, just like humans, you just, if you get slower then you, your danger problems happen. You fall, things hit, you have accents in your car, whatever.

Nice. Yeah. This is exciting though, I think. Yeah, this definitely feels like real progress. I was wondering if it would be useful to talk through, The kind of example you gave, we can either use the stapler or the logo that kind of rotated on that, on its kind of end point.

just 'cause Yeah, I'm still struggling to visualize the inverse movement or whatever. It's, clear in my head when something just moves up and down or, whatever. But if it's rotating and moving at the same time, just visualizing, it's that how, yeah. I don't think it's just those two examples. I think what I didn't talk about at all is how you go from, first of all, how do you learn a behavior model? 'cause we know there's lots of problems with that. even though we have a concept what a behavioral model looks like, we don't really have a practical way of learning it quickly. And then second, How do you, how does it actually apply to do this? it was easy to say. The movement command moves the sensorimotor. That works great. so I, and I'm trying today's argument, I was like, trying to say, it must work that way. That must, be what happened. But how do you go from a behavioral model to generating the correct movement command? at moment, any moment in time is not something I talked about. And it may not be hard, but I didn't wanna think about it. It was just one more complication. It was like, oh crap, I, where do I begin on that problem? so I think that's what you're asking about, Neil. There's there's a lot of pieces here. we didn't really walk through the details of. that being one of 'em. How does the movement, how does the behavior model generate the correct movements at any point in time for any module?

Vivian's gonna have the answer.

I guess how I was thinking of it was that. We have these local movements stored in the behavior model, which are these kind of black, small errors, which is yeah. Local movements of the child object or, let's be careful. That is, that's the behavior model specifies at some point in time on some location on the parent. Yeah. At that the child is moving in a certain direction. Velocity. Yeah. So we're at that location only. Yeah. So we are in some, at some location in that behavior reference frame and at that location we have stored a particular, movement. Like a local movement. And so it does, wouldn't have to be that all these errors move in the same direction necessarily. In fact. they are not moving like the same way. They are rotating and moving a bit. but basically we had one location in here and there we have a local movement storage, and we would take that local movement, which is the expected OB object movement. That's what we expect to sense as an input here, or what we have sensed when we learned it. But then we have to invert that movement. So take the opposite of it and apply it to that reference frame here to compensate for that movement in the child objects reference frame and Right. But it'll get more complicated if there's scale, then we also have to com compensate for scale and, but let's, let me, so that's correct, but let me, lemme tell you where I was getting tripped up and maybe there's an easy answer to this that makes sense, but I'm looking at one location. I say, okay, the child objects should be moving, but the child objects, all the locations are moving. So a lot, maybe the whole, the whole logo is moving or the whole staple top is, rotating. I'm only sending the, the movement command for the one point I'm looking at right now. And so that only tells the, learning module in the child object. Oh, you are expecting, this is where your sensorimotor should be. But there's, but now, I move to a different point on the object, a different point on the child object and the parent object and it hadn't gotten those movement commands or, like how does it know where it's supposed to be?

it's saying, imagine I'm looking at the stapler and as it's moving slowly upward, I then look at the bottom of the stapler. So I'm not really looking at the top anymore. And then I go back up to look at the top of the stapler or, I expect it to have continued its movement, but I wasn't sending any, my learning module wasn't sending any movement commands to it during the time I was looking at the bottom of the stapler. Yeah. it, feels like it gets at, we talked about a bit on the retreat, Do we like path integrate through this behavioral space? do we take all the sum of all the steps and, is that displacement applied in the top down connection? Maybe because at a single point in time and single point, there's another solution's not really sufficient. What, if you, just assumed you had a whole bunch of learning modules and they covered the entire parent object and child objects and all of them are running simultaneously. then all of those would be sending the correct movement commands to the, at any point in time.

the, problem I think really comes about, because I was thinking of a single column or single learning, yeah, I mean I was thinking of it working a similar way as, the occlusion. working. So basically, even though we have part occlusion of the optic, moving, the behavior model still keeps stepping through its sequence. So we'll keep sending these movement commands in you and we're still moving in you, even if we're not getting the feature inputs right now. But it will still keep moving you through that space. but if this is a single learning module, it's only sending the movement commands for one spot on the child. the problem is all parts of the child are moving. So if we could, but they're moving together, there's no concept of togetherness here. that would get back to oh, we know that there, there's somehow that this is the fallacy to say, oh, this is a, an object that has, it's a bar, or it's a line, or something like that. It's, almost like you have to do it on a point by point basis. Unless, I guess we argue that. If there's multiple parts moving independently, those are gonna be broken up into multiple Yeah. Child objects. I think that would be like the temporary attention mask, in the beginning and then later on, like Neil says, if they're moving independently, they'll be represented else as independent children. And if, if you sensorimotor moves between different objects, maybe the individual columns don't immediately switch their representation to the different objects, but like the column that was representing the stapler top keeps moving through the stapler top space and where it would be if we move back there or something. I'm confused. Imagine I have a single column and it's, looking at a point on the staple top. if I focus on that point, on the staple top, the, behavioral model will tell me, oh, you what you're, as a, as the staple top moves through my fixation point, the, staple top should be, you should be moving relative to the staple top. But now I move, I now attend, I move my sensorimotor to the, other end of the staple top. At that point, the other end of the staple top has not been doing path integration because it hasn't been getting any movement commands because there's nobody looking at it. And I think, that kind of mixes up the rotation and the location. So when, we, where do we have the opening stapler? It, I don't think it has anything to do with whether it's location, rotation, pick whatever you want. It's the same problem. basically, this model here makes sure that as we are at the top of the stapler and the stapler opening, we keep predicting the features that are here at the top of the stapler. But only if, the column is looking at that. If a column, nobody's looking at it, then nothing. if nobody, if there's no learning module attending to that point, then nothing's happening in that learning. that the only thing that the learning module knows is where it's looking at. Again, if every, yeah, I guess maybe one, I dunno if this is helpful, but yeah. Maybe one way to phrase this is okay, imagine you just come across a stapler and it's already halfway through opening. I guess maybe the first thing you would do is because of the movement that's been observed, you would infer that you are at a point in the behavioral space, like you're, basically at one of these kinds of points where the dash lines, actually no. I'd be careful. I, if, it was moving, I would infer that I'm in the, that's what I mean. Yeah. I If it's moving, yeah. If it was just stopped there, I'd say, oh, it's stopped. Better learn this morphology. Yeah. but then I feel like you'd start learning that morphology. Yeah. Yeah. I guess how would you'd probably be able to say something about, oh, this was halfway through the behavior. How do you do.

That, might be some sort of path integration or something. But isn't there a time signal is the global signal that's telling you this? So even though you're like, if you know that the behavior started, the time signal is proce proceeding. So if you never attempt to the top of the stapler while it moved away, if you staring at the bottom when you go to the top of the stapler, now the state of the parent column already changed because it's getting a time signal at the end of the sequence, which will trigger it to know where it should be in the sequence, which would tell the child object, you should now Look way farther to see the top of the stapler. But again, I, that's part of the solution, but I, don't think it's the whole solution because again, if, if there's no column looking at that point, that's moving, the, I just, I, just, the whole system, I mean lies, it lies on path integration. These movement commands have to be path integrated. And so if I'm not sending the movement command to some point portion of the moving object, it can't path integrate. Yeah. I think that we, don't necessarily, we are necessarily very good at that either. if you're not looking at something and you have a behavior happening in your, like somewhere you're not looking, you have to like actively attend and mentally simulate that behavior to be able to predict where it will be when you look back at it. I don't think so. Again, I think this would all work if I said I have a whole bunch of learning modules and I'm gonna do, I'm going to attend to all the ones they're, and, at any point in time they're, oh, someone's covering every part of this object. So I'm gonna, I'm gonna activate the learning modules that are currently over the stapler top. Then the system would work. But if you only had one learning module and you can only look at one part of the stapler top, I don't see how the other part, how when you move to the other parts, you would know where to be.

Yeah, it's definitely, I, feel like it almost like I almost need to see it simulated or something to convince myself that it works.

but it, yeah, but like you say, Viviane, like worst case maybe it's just yeah, we can only do it if at least in a single or few LM situation if we're mentally really thinking about it and stepping through the behavior.

yeah, I feel like maybe we need. A better term than half when we talk about this, like this is a bit of, yeah, if, you're listening to a song and it suddenly stops for five seconds and then you want to predict what the next note should be, if you hadn't turned off the volume, it's really difficult. You have to, in your head, keep counting or keep singing the song in your head or something to be able to say what it will be when you unmute it again.

Again, I keep coming back to the idea, can you do this with a single column or do you need to have many columns or sampling a whole object at once?

what, or hear me out if you're following this, here's an example. Imagine I have a single column and I've moved it over the object and I've inferred it to stapler. Now I'm looking at a point on the top of the stapler and it starts to move. That might be sufficient all on its own to invoke the stapler top movement behavioral model. I can now infer the behavioral model because that's the only thing that, that fits a movement at that location, in that direction.

and therefore it must be the stapler is opening. So I've inferred the behavioral model.

and now what if that's, that, that column just stays on that part of the top of the stapler, it just follows along. or if it doesn't move it, the point is it doesn't look at different parts of the top. if it followed along it, it would say, okay, now I'm, I, made this correct prediction. I now the staple top is open. My point is my location is, sensing the part of the staple top? Could I then infer the entire object from that? From that one observation of being like, I'm on point X on the stapler top at this angle, therefore I can infer the, rest of the object. it could be voting, it could be just like, what do you mean, sorry, the last bit with infer, because the opening kind of description of this was we had inferred the object. Do you mean we inferred, did you mean predict, where the features are? Or Right now we wanna predict now the object is moving and, we wanna predict what it's gonna look, what I'm gonna see, after it's, moved now. Yeah. I think that's make, things a little bit more complicated. Perhaps. I'm looking at a, novel stapler, and and it, may not even be the same size or the same shape exactly as the original stapler, but I recognize the behavior and I say, okay, yes. So now I'm trying to make predictions about this novel top. As it moves into new positions, and I've never seen this novel top in these new positions. I've seen that. The state, yeah, I think, I'm sorry, go ahead.

so at least Todd, I was thinking of it that would be solved by the mechanism we have because as we, if I understand your scenario we are still following the stapler, like the stapler top. So that means we're integrating the movement from the behavior that's stored at that location. Only one point, remember Only one point. One point, but it means there's, nobody else integrating. It's just that one point. I know. I know. But it means we are still in the correct location in the Child Objects reference frame. We're still on the correct location there and we're also updating the orientation of it. So we are in the correct orientation, We're rotating the incoming movement correctly. So because we are still making correct, we are still in the correct location of Child Object, we're still have the correct orientation of the TAD object. That should then allow us to make predictions about everything else again. That's what I'm saying. That would be like, like almost yeah, then I would basically path integrate. If I moved the Sensorimotor at that point in time, I would path integrate over the stapler top in its new position in orientation. Yeah. You would use the sensorimotor movement again to move through the reference frame because you were in the correct location to start with. That should work again, especially if the orientation. Yeah, as long as I know where I was initially.

yeah, And the incoming, I think you're right, movements would transform by the new orientation. I think you're right. I might give up on this. This concern. I don't, think you have enough information to know whether the stapler is opening or whether the stapler is translating. If you're looking at a single spot and you observe a change. The stapler could just be flying off into space. You don't know if it's opening or whether somebody's pushing the entire stapler together. I thought we assumed, we, recognized the, I thought we assumed we recognized the hinge behavior. If we're not recognizing the behavior, then yeah, we, the constraint Jeff said as you staring at one spot and it starts to move. Yeah. Okay. That's an interesting question, Tristan. I guess so. You're right. If I'm just looking at a point on the stapler and that point starts moving, I can't know yet whether it's the stapler is opening or whole stapler is moving. That's your argument. But yeah, I think in that case, like I think that's one of the main assumptions we have to make predictions. We need to recognize the behavior. In order to use the behavior model to make predictions about morphology, I don't think that it would work if we don't recognize the behavior. I don't think I would be able to make any predictions if I was just looking through the straw and I don't, know what behavior I'm observing. I think it's, again, this might be an example where you actually might need to have multiple columns.

yeah. But you would need them to recognize the behavior, not, necessarily to, what I, that might be one where, again, it helps if you're passing the location or the change of location of the child object, because that's going to be consistent across the learning modules, whereas, or more consistent, whereas each parent, lm, if it's just looking at the sensory patch, it's seen, it's getting very different. Kind of movement information coming in.

There's almost an assumption I didn't follow that nails, I'm sorry. okay, that's fine. I dunno if you want to make it again, but, you go ahead. I can mention it. there's this idea that, the retina can detect motion, right? And if you detect motion everywhere, consistent, then it says, oh, the eye's moving. The world is not spinning, the eye's moving. And, but if it detects motion locally, then you assume that the object's moving relative to the world.

and we've and, that, and, the, child object moving relative to the parent object is just a, a special case of that, right? It's so we've had sort of an assumption in the background all along that we could tell the difference between an object moving versus the world moving, or the object moving versus the. Eye moving and, now we have to extend that same idea to a, parent object. And so in some sense, there's no way in the world you can determine if the whole staple is moving, or the top is moving if you're only looking at 1.0 the stapler top. It's this fact as you point out. but we, might be able to say that we have additional information, that tells us whether the whole thing is moving or just part of it's moving. So I don't know the answer to that question yet, but I, it there must be solution to it. It, doesn't bother me too much know. Yeah. And I guess what I was just trying to say was like going back to the kinda shared representation of the location of the object, that's also just maybe useful to vote on in terms of both recognizing behaviors, but also segmenting the object like. Maybe something about that all these LMS are saying, yes, the object is at this location, or yes, this object is moving, in a particular direction. Can, that's both much more efficient if you're yeah, trying to recognize how that's moving and then communicate that up to the next level. But yeah, maybe it's also helpful for the segmentation thing. 'cause it's like, how does one LM know how to segment, the, object, like the top of the stapler, without some, sort of information about what the other LMS are seeing.

can I, try to summarize something? it's, it helps me to do this for me. I was worried about the fact that. We wouldn't be, we would have to have multiple sensorimotor modules, multiple running modules observing all parts of the moving child to, so they all get upgraded, updated by the, behavioral model. And I think I've been convinced that's not necessary. All we have to do is we can track one location using the behavior model and, but, the child object is known and now we, know where we are on the child, we know where the orientation on the child object. So if I move, I use the standard path integration to make predictions because I, have a model of the child object. It doesn't work if you don't have a model of the child object, but if you do, then just standard path integration will take you to the right place. Yeah. So it's a combination of as things are moving wherever I am, I have to path, integrate using the movement from the behavior model. But if I move across the, child object, I have to path integrate using the. The movement of the sensorimotor. And so you have to beat both these things at the same time. And as long as you do that, when, the, musical tear stops, meaning the child's office stops moving, you'll be able to path integrate the normal way and, know what everything looks like. So that problem is now, I think in my mind, sufficiently solved. I didn't think about that answer. So now that's a good, and the second thing is, oh, sorry, is, I'm still struggling with how we learn the behavioral model, with the single, without having lots of columns and, I haven't thought about that recently. So that's, I still, I, still think we have a, we have to address the issue of how do we learn models efficiently, using multiple columns. maybe someone should come up with a battery of behaviors. Then everyone has to try and learn them through looking through a straw, and then we can see if anyone actually can learn them or if it's impossible. I also think it's almost certain that even if we're just learning morphology models, we don't sense every location. It's, it just feels like there's some sort of sharing that's going on. You already know that if you use the straw, it takes a hell of a lot longer. So learning seems to be doing some kind of shared learning, just like we do shared, we do voting for inference. Somehow we have to be doing shared learning. So they, it just feels like that. So then that's, then a variation to that is the, the, in the inference and learning of behaviors. So even learning morphology models seems to have to require shared learning. And maybe that we have to have shared learning from behavioral models too. I'm, just gonna, it just feels like that. So maybe I'm wrong, but that would could be something, with kind of. Hierarchy could come into place too. It doesn't necessarily have to be shared laterally. It could be top down sharing of general model. Every time I start thinking about that, my head's spin and I get it just goes crazy. Yeah. I made this point before. It seems like when we learn a model initially, we don't learn in the lower regions, we learn in the higher regions. We learn it. We don't learn a vision model. We learn a model that can apply to vision, hearing, vision and touch. And it's, and it probably uses the hippocampus or upper regions of the cortex. And so somehow this idea that you learn the upper level model then quickly and then you push it down, I, it's like really confusing.

yeah. It won't be confusing in the end. It'll be very simple as we're coming towards the end of the meeting, Yeah. I just to share, I think we're making some good progress on the problems, especially now. So the last time I showed the slide, I had these great check marks here. Because I didn't have any idea how location changes would be incorporated into making predictions and applying behaviors to different objects. And now I feel like we have a pretty good idea about it. So I would probably still leave those gray 'cause we still want to play through the mechanism a bit more in our head to make sure it actually works in all scenarios, but definitely a lot closer. I have one other new open question that I didn't bring up today yet related to the key frames, but maybe something for another topic and then also the sharing information and voting mechanism. I guess there's another bigger one to talk through more. Yeah. But I feel like we're making good progress. At some point we have to even brought up broader, go back to the idea of, interacting with objects to create a desired. States in the world. yeah, I think I, proposed the solution to that a couple of weeks ago that, actions of objects are basically just object behaviors applied to scenes. So basically the two objects are multiple objects that are interacting are just two features on a parent object and those moving. so it's a behavior of how to child, how do we make those things happen? How do we, how do I flip? You mean how actions come into this or learning causal relations, I guess like that, yeah, it's the fact, yeah. I guess pressed a switch that turned the light on. It wasn't just today discussion was, behaviors that had nothing to do with my interaction with them. They just did. yeah, actions, I'm taking out of this entire topic right now because I feel like I. oh, I was just saying we have to do it some, I say we have to do it sometime. definitely. if, we already solve all of this in the next two weeks, or three weeks, then we'll, have a new topic for the brainstorming week.

okay, so we have the, so let me, you were saying, we had the big question about how we do learning, shared learning. is it hierarchical multicom voting, things like that. What was the other topic you said? I didn't, ask the question, but it's, basically with key frames. So if we lay down, different points in the morphology model for the different states of the object, we need some way to condition the morphology model, what to predict at what point in time. 'cause otherwise, like we would make predictions about the open stapler and the closed stapler at the same time. we need some way to say, now I wanna predict the points. I think what you need to do is.

Like I give the example of the logo, the logos, the top and logo is the bottom and there's no preferred position for it. So somehow you have to have a morphology model that has multiple possible learned states and yeah, you wouldn't, unless one was happened 99 of the time and the other one didn't happen. If they happen equally, you, somehow have to be able to infer any one of those states. And yeah, basically a way to tell it when to predict which state. if I've seen the stapler open now I wanna, there we saw the behavior, maybe it's just in inference, like same as we infer rotation, we would have to try different states, but, we, at the moment, we don't, our morphology model doesn't, have the concept of different morphologies at different points in time. Yeah. So now we would have, so we've yeah, we talked about having different states. Yeah. We would have to add a state dimension to our morphology models. Where we try to infer, which, here's a possible, I know you wanna end, but we do have a behavioral model, and the mythology would be associated with one point in the behavior model and a different mythology would be associated a different point in the behavioral model. And so this goes back to the very, very beginning, which I talked about this, I talked about states of objects. The states are, would in some sense be the point in time in the morphology model. And so you would be inferring not just the morphology, but also the state of the, point in the, behavior model. Yeah. So we can, we, can learn like an associative connection between points in the sequence and the behavior model to states in the morphology model. Yeah, as long as we don't try and learn tons of key frames, it, I think it's an issue and and the key frame issue is, again, I think it's less of a problem in brains because the way they learn, but, it's not required and it wouldn't be a lot of association. It's like the state is like a global variable. So it would just be like point in the sequence to state and morphology model. It, would be like a one-to-one connection. It wouldn't be like every feature on the model would need to be associated. So it's, it would, it's basically when you're inferring an object, you're inferring, you're looking at features, their orientation, their IDs, whatever. And you also have to, in just like you're inferring the orientation, inferring the location, you have to infer the point in the behavioral state.

and it's easy to say, but, if you could do that, then it would work. Okay, so we can check off that question too, then. how are you gonna do that?

It's the same way we're doing rotation and I actually don't deeply understand the way you're doing it in Monty today, but I'm happy for it, Great. Add it to your list instead. Nice favor. Okay. maybe I'll think, personally, maybe I'll think about the, shared learning, problem.