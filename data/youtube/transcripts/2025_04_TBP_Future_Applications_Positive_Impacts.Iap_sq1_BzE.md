And I hope my voice will, stay with you through today's meeting. this is a bit of a different one, than the usual monthly review, but I, originally just made an overview for myself. Then I felt very inspired afterwards and I thought it would be cool to share with all of you. and it's basically an overview of how the Thousand Brains Project can have a positive impact on the world in the near, medium and long term. And what kind of applications we can already think of. we always say we'll probably have tons of applications that we can't even imagine right now, but there are already so many things that we can imagine. Are not that far off, and just showing how our, the things on our research roadmap play into actual impact in the real world.

So how I kinda structured this is that we have a couple of key capabilities on the research roadmap. You can probably think of, modeling, compositional objects, object behaviors, being able to manipulate the world. and each of these key capabilities unlocks a whole new range of applications. And so I tried to structure it. by that. So currently we can do object and post recognition. And in terms of applications, we could do this within existing robotics pipelines such as manufacturing where, there are tons of papers out there that try to solve this just object and post detection. And, there are lots of industrial applications where you need to detect parts and the pose of them, and then you have a gripper that then can pick them up, or you detect the pose off boxes and then you have something that can pick them up and move them around in a warehouse, stuff like that. so that's, Current solutions. there are some current solutions, but it's definitely not a solved problem. And they particularly struggle with like changes in lighting, objects occluding each other, changes from the training distribution, like a different texture on the box, if Amazon suddenly prints, differently on its packages. And then you need to rearrange the, such pipelines Monty could already be plugged into and we can already work close that we would be able to evaluate on some of these. six degree of freedom objected and post detection benchmarks.

and like what we show on in the DMC paper, the benefits of our approach are the reduced cost and training and data collection, being robust, being able to do continual learning and quickly adapt to new conditions, and generalize to objects with similar shapes. But then, the next thing on our research roadmap, something we plan on being able to do, in the summer is to model compositional objects. So actually stacking learning modules on top of each other and using hierarchy to model compositional objects. And that then unlocks, much more complex object recognition tasks. So one application we, talked a lot about lately is, ultrasound, And why is this kind of relevant? So a big thing is that in a lot of remote areas, there aren't trained physicians to actually administer an ultrasound. And ultrasound is a really crucial, meta technique for lots of different diseases, but also particularly maternal health. checking on that, the baby's healthy age of the baby, and it can have a lot of implications if you're able to do those scans in remote areas and. If you can use these mobile devices with, in combination with ai, they can become usable by untrained physicians. and there are a couple of existing AI solutions, like blind sweeps where you basically scan the belly of the mother in pre-described movements and then use ai. to determine fetal age, for example, which worked quite well, but they require collecting large data sets. Niels and, I just talked to someone yesterday, who mentioned 10,000 subjects, to try their, Model on. Whereas Monty won't require collecting large data sets, which means it can also be used for more rare conditions. and it's generally just much cheaper to, train it for other things. Besides, fetal age, it can also learn continually and adapt in the application to the specific population where it's being used. It can suggest where to move the probe to get necessary information instead of having these pre-prescribed movements. it can detect abnormalities without training on examples of them. That's a big one. Because Monty, you could train it on only healthy fetuses. And Monty makes prediction when it sees a new fetus. And if those predictions are wrong, it has a high prediction error. It can recognize whether they're is abnormality, like something is misshaped, and then refer that person to a, to a clinic to get further checks, without ever needing to train on, examples of these problems. And then it's also much more interpretable since we have these structured models, which, is, good in medical applications. and yeah, just some examples, yeah, in maternal health would be like determining fetal age, the age of the baby. Then also things like breach babies, are in the, wrong position. Because Monty can actually detect the pose off the object as well. It can also detect relative locations of objects. So placenta privia, where the placenta is in a wrong position relative to the baby or malformation.

so lots of opportunities that could be explored there that we're actually currently looking into more. I'll have more on it in the monthly review slides.

then there are a bunch of other, remote sensing applications, particularly in healthcare that we recently talked a bit more about because that's the focus of the Gates Foundation as well. For example, detecting anemia in pregnant women, to know whether to give IV fluids without having to do a blood draw. they take videos of the eye and the tongue, and then. You can determine from that. You could predict whether they have anemia, but since they're already taking videos, that's something that Monty could potentially, improve on. Other things would be, like the drones for larval source management of malaria mosquitoes, or drones that serve health and water access in remote communities or deliver medicine there.

This is a bit more of an abstract one, but that we've tried Monty on before, and realized it will need compositional objects.

It's just one of these, benchmark data sets in the AI community where there's very little data available. Just a few examples of handwritten digits, and in different alphabets, and then it has to learn from them. And it's kind of sensorimotor in the sense that we follow the strokes, or these digits, in the one of these data sets that show how AI can't learn from those small, data sets.

another one just generally mapping disaster zones, with drones or other kind of robots that go in there and build a structured model of what's going on.

and then also seen understanding in just existing pipelines, like maybe again, warehouses, stacking boxes on top of each other, understanding of to grab the bottom box in a, stack of boxes. But the top one.

and then this one was something I also didn't think about before, but apparently like waste management is a big deal that's not really solved yet. there are like some automated systems that can reach accuracy of like 85 to 95 to identify some types of recyclables, but it's definitely not graded. A lot of times still use humans just to sort waste. and it particularly has problems with defamations of objects like. Crushed coke cans and stuff like that. And also like the variety of textures, like you could imagine a can, can have so many different labels on it that it's just hard to collect a large enough data set to recognize all of them. Whereas Monty could just recognize the general shape of a can, independent of the label. and I put this kind of in blue plus deformation here because just adding composition s in this case wouldn't be enough because currently we. We don't, we can't deal with object deformation either.

and then, yeah, generally structured knowledge representations to have better search, making new connections and, teaching and learning. And I asked ChatGPT to generate an image for, which is this, what it came up with. And then I asked it to make it more concrete and put an example on it. Which I just included because I thought it was such a great example about how current AI is not good at this. Because it wrote like the apple has flavor, red and apple is a concept and the plant is made of apple. so I, think, if Monty can represent more abstract, knowledge and have more of associative connections, we could do much better than this. I.

yeah. Then the next big kind of watershed capability is, modeling object behaviors and the dynamic world, like a world where objects are actually moving and not just recognizing static objects and object behaviors, which is what we're currently brainstorming a lot around.

and that would again, help with ultrasound, where we actually have moving components like a beating heart or lungs or, even a fetus moving around. and where we need to either recognize the behavior itself or at least be able to compensate for the movement of the objects.

Then, yeah, anything in the kind of self-driving cars or transportation area where we have lots of moving objects that we need to recognize and anticipate and know their pose.

a grounded language understanding, which would, Could be used to combat misinformation, which might be a prevalent topic, lately. But, yeah, also generally just having a grounded language understanding, with less hallucinations or no hallucinations, large language model models have, would be, really beneficial. And then, yeah, big sectors, generally agriculture at this point. So like we detection, disease detection, pest prevention and detection. there are tons of agricultural robots and lots of people try to apply AI to it and a lot of them are sensorimotor systems. so I think. Monty could also have a, big impact there. And agriculture is so important for the world to supply food. that, yeah, could be a very positive impact.

on a more abstract level cybersecurity, Protein dynamics is an interesting one. I don't have a lot of knowledge about this. I supervise like one thesis around it. But basically we have big static data sets of proteins, but not of their dynamic states because they're really hard to record and we can do physics simulations, but they're very expensive and they only work on small time scales. and there are some machine learning methods that can do approximations, like graph neural networks and variational autoencoders, but they kinda lose their physical link very quickly and start to hallucinate and require large data sets or overfit.

but being able to model the dynamic states of proteins is super important for all kinds of medical and pharmaceutical applications. yeah, I talked, I, this is literally a quote. it would be a trillion dollar industry solved and change the face of medicine. So this is a huge impact. I'm not sure if Marty could actually do this because it, it seems like a, very complicated area and I, don't know enough about it to be able to say how we would solve it, but, it's an interesting one to think about if we could model object behaviors, whether we could have an impact there. And just generally understanding how, these wiggly proteins move and interact, is useful to understand misfolding like in Alzheimer's, mutation effects and signaling, but also like how pharmaceuticals would actually have an effect on the body.. Not as real world, but I think Tristan, you shared the second version of it yesterday. the ARC challenge, which is like this benchmark for AGI, which, yeah, I think Monty could do really well on once we add this capabilities. since it's like, very little data to learn from a lot of reasoning and structured, knowledge that needs to be extrapolated, which, Monty is really well made for.

and then yeah, the next big watershed capability manipulating the world. And that gets us into all these new exciting, things like anything in robotics, so like caretaking, where. We currently have less and less humans who, wanna do these things and more and more people who need care taking. that would definitely be something of use. home helpers, there's this quote where it's I want AI to do my laundry and dishes so that I can do art and writing. I. Not for AI to do my art and writing so that I can do my laundry and dishes. so hopefully Monty can be that kind of ai, so we can do the things that we actually enjoy doing.

and then, yeah, just, dangerous jobs, mining, construction, firefighters, police, where. Some people are dabbling into robotics, but definitely not as general solutions as, would be useful.

and then, yeah, space exploration again, a lot of spaces very hostile towards humans, we use a lot of robots for this, especially on Mars. so having more intelligent robots would be very useful for that. And then again, agriculture, if we can actually manipulate the world instead of just detecting stuff, it opens up all these new things like automated picking and sorting and spraying and pruning and all this kind of stuff. And then this one was pretty exciting too, we talked about before, AI scientists that actually design and run experiments and the sensorimotor system is basically coming up with a hypothesis and designing an experiment and then running that experiment and looking at the results and coming up with another experiment, which could have all kinds of implications. curing diseases, solutions to climate change, drought resistant crops, renewable energies, developing vaccines quickly and so on.

And then, yeah, generally tying into that also logical reasoning. Doesn't need to necessarily be running experiments, but it could be logical reasoning for other things. Sorry, can I interrupt? Can you expand a little bit just on the, how manipulating the world leads into logical reasoning? Yeah, sure. so yeah, I put it because, it, felt like manipulate, like knowing how to move. Through the world would be similar to moving through conceptual space internally. So basically manipulating concepts in your head and imagining different combinations of them would be similar to actually manipulating objects in the world and putting them into different configurations, if that makes sense. Perfect. Yeah. Thanks.

yeah, feel free to interrupt.

almost done too. yeah, last one. I just put plus, for all the other capabilities and I already peppered in a couple of, other things in here with in blue. but yeah, basically having multi-agent systems, which, might be pretty natural generalization since we already have these general learning modules that can talk to each other. So it, it's probably, it can probably generalize easily to them being separate agents and still talking to each other. but then, yeah, generally, representing abstract concepts, which we're also hoping will generalize out of our algorithm without us having to add a specific mechanism for it. But we'll have to see a lot that, scale and deformations and, theory of mind, which again plays a bit into the multi-agent thing.

and that's really now this, robots on Mars example where Mars is really not a place for humans to go and start building, stuff there, but more a place where robots would need to go first and build a colony. And I. It's such a different environment that it's hard to imagine how they would be pre-trained on large amounts of data, on Earth, and then go there and be able to do the stuff. and it would really need more of an intelligence like ours, that can generalize and extrapolate to new environments, to, actually be efficient here, not, break down when they encounter an unknown problem.

Education. so one, yeah, potentially actual AI teaching, children or, adults. But then also, understanding intelligence in humans, which will be one thing that I'm hoping will come out of this whole project as well, like a full understanding of how our brain works, which then. should help treat mental disorders and also for us to know the best way to actually teach children, and learn about the world.

yeah, the, big, theme is, the hope is that this will make the world a safer and healthier place, raise the standard of living, solve current problems and existential risks, and also applications we can't even conceive of today.