So yeah, so the kind of... there's some new ones here, which we'll get into, but one way you can think about it, this problem coming up is the What is happening, at the level of column, in terms of modeling and perceptual input, and what, so forth, when we perceive nothing? And maybe to be a bit more concrete, maybe not. Okay, let's be more careful. It's not perceived, but we're actually not sensing anything, right?

Yeah. We might be receiving something, but not sensing something, like we talked about the other thing. we're talking about no physical sense of. Sure. Yeah. Okay. Yeah.

And it's more common with touch, because, basically anytime it's not in contact with the surface of something, arguably, it's not sensing, any... anything like a, edge or anything like that, a kind of morphological feature or a surface. But even with vision, you can think of examples like, looking into, the sky, some other void, where there's no kind of clear surface that we're perceiving.

And, Monty also looks a lot into the void, She looks into a void all the time. hot. Yeah. Yeah. I don't get that joke. What? Yeah, not so much anymore.

My mother. I think we can just be really specific. When a sensor patch is not receiving any sensory input. that's... That's what I get. Good afternoon. I guess it might be receiving, color, for example. That's where... where I feel like maybe it's... it's not sensing, a surface or... or a morphological. let's just go for that, because my understanding is that every angulin cell leaving the retina is a sentence around ganglion cell. if there's uniform color on a patch of retina, my understanding is that you don't get any output. You only get an output if there's an edge of color. an edge. So I was trying to, yeah, read about that. I I didn't go too far with it, but it seemed because of opponency and stuff, you would still... Get some baseline activity that would... Because otherwise, if you're... if you're looking at this, let's say you're looking at a full wall of color. You perceive that as a color, you don't perceive that. that's... that's the mystery. that's the mystery, but the mystery is... this... this. But I... A lot of this was written a long time ago, so I haven't read anything recently. But in the old days, people would write, hey, there's no input, yet you perceive something. How is that? Yeah. I will Maybe even if we... even if we have some... Representation of that, constant color. The biggest thing is that there's no location or orientation being detected, so that would mean, No mini... if we say orientation is represented in the mini columns, that would mean there's no activation there, I think... I think this is actually a really important point. I know it sounds weird to say, oh, you're looking at a blue wall, and all those columns are not getting input, that's what people said, but if we really... the idea of the center-surround receptive field I've come to really love it. It's a... it basically a sentence around receptive field, says, I'm not going to report anything unless there's some change. It can be a spatial change, like a papagranus cell, or it could be a dynamic change. But there's got to be some change, so that's, so that's... that's all that I care about in the world. I love that idea, and so that's general... that's general neuroscience dogma. And I think we should ask ourselves, do we want to pursue the idea that you might get something like pure color or not? I'd prefer not to, honestly. I would just say, how do we explain the perception of color? Yeah, without... I guess towards the end, I'll have some slides that kind of relate to that.

yeah.

that will... that will be revisited.

But yeah, so I guess for now, let's assume you also don't get color. You... you perceive nothing. Sense nothing. I guess it's really a question of, If it's uniform, there's no output.

that means you could be touching a surface. I don't know, but, but it's like your finger has to be moving, or something like that. Yeah. Yeah, I don't know. Alright, let's keep going. Also, if there's no, centers around output, it... we can still... perceive it as, okay, whatever I'm looking at is uniform, or doesn't have any edge, one of the questions here is that, even though we're not sensing anything, by... centers around That still that may tell us the information that, okay, whatever it is, it's no edge, or it's a continuous. I think Jeff's point is that, yeah, there would be no activity, but then the question is, how do you know what color it is? there would be no spiking, basically.

it has to come from the edges of the color. That's... But maybe there are... yeah, I feel like we Stimulate where there are no edges. That was... that was the example I gave with the checkerboard, remember? with a checkerboard illusion I wrote about yesterday, so I don'. Oh, yeah, I don't have it in front of me. But the reason that illusion works is because you see the edges of the squares. But you don't see the gradual change occurring within the square. It's gradual enough that it doesn't excite any of those ganglion cells. And the brain... however it does it, it assumes that it's continuous across that. You don't perceive that it's actually changing from black to white. And there's a lot of examples like this in the arts and visual arts and so on, where, where the tricks of colors and so on. it is mysterious that you perceive that you see, it looks like you see there's red across the entire surface of the fire truck, but... But it seems from the retina, that's not true. The teams from the retina, there's edge... red edges at certain places. And the rest of it is just perceived. It's, crazy, but that seems to be pretty... I think that's well-established science, but I don't know. It may have changed. Oh, sorry, one more thing also, I know Tristan posted a question, is that, I think sometimes we, model sensors at, really at the single-cell level, original, centers around, or, but then pers... we had a brief discussion about this, like, when we were doing edge detection, is that, instead of directly, implementing Gabor filters for edge section is that maybe this perception happens because it's retinal ganglion cells plus some other color opponency cells, so it's okay, no input, no output from the retinal ganglion, no output from the center surround, but some baseline in this RG, let's say... But baseline coming from the retina? From color opponency. In the retina. I'm just saying, okay, we got axons coming out of the back of the retina. That's what we're talking about here. By the way, I think, they could be... some cells have a constant activity, right? they increase or decrease. when we say no activity, we're saying there's no change in activity.

and, when you say color opponency, is that another signal coming out of the back of the retina that's not centered around? Yes, but I'm not sure if it... And that might exist, I just don't know about it. Yeah.

Maybe it's it. But... is that, maybe there's a combination of cells that give rise to, the perception, even if that particular one... But why wouldn't... I don't know. I'm gonna get to Tristan's question in a second, but it feels to me like... We should at least have... the working assumption in ophthalmology and neuroscientists for many, years was that That all the in-between stuff is perceived, but not sensed. Now, that could be wrong. I don't know what the current literature states about this. Maybe they found some new cells that are just red cells, or something like that, but... I think we should at least work with that assumption as far as we can before we abandon it, because that was pretty well established a long time ago. Yeah.

And Trista, I don't think blinking... it's a good question whether blinking would activate things like that, but I don't think so. I think... when we blink. the... did someone else remember this? It's like the brain shuts off everything. Yeah, I think it gates the input. but I don't think blinking... the actual turning on and off of the... the light to the retina is, the general idea is... I've never heard anyone say it's suggested that's somehow useful, other than... keeping your eyes wet. Fatial gradient and not a temporal gradient. It's just also, it's just it's like there's mechanisms, as Vivian said, that just basically say, ignore this. Don't, pay any attention to this, it didn't happen. Sexotic suppression, or something like that? Yeah, but this is, specifically for blinking, I think, right? Because otherwise, it'd be like the world would be blinking on and off all the time, so it somehow gets around that. I think, look, all these are possibilities, but I don't think we should... we should happily go beyond what the physiology has suggested in the past, unless we have new evidence. I would... throughout this whole discussion, I'm going to ask myself, how is it that we perceive the color when then we actually don't get an input in the middle? But we do get it at the edges, so that's some... that's a really interesting question. it's okay, there's some... all kinds of weird questions come out of that, or ideas come out of that. But... Yeah. Anyway. No, I think... yeah, interesting question about the... Blinking. I just thought this might be a helpful example as well. this is an illusion where It's definitely... you're perceiving what is not sensed. Because we perceive a kind of blue circle. But in reality, this is pure white.

So it's just because of these, edges, that you fill it in. You can almost... you can almost see the... you really perceive the blue edge between the two circles, between the circles. Yeah, Yeah, it feels like there's an edge there. It feels You, yeah, it's maddening, but then if you cover up the screen, you can convince yourself that, Yeah, it's all in the. I think that's a really strong indication that clearly whatever cells are reading that area are not getting any blue. And, yet you're perceiving it. But I guess what I would argue, there, the perception is subtle. it's there, but it's, Do you know what I mean? It's not as vivid as when you actually see the red of a fire truck, or whatever. It, feels different somehow.

Because you actually... because you actually don't have those Blue cell... the blue edge defective cells there, right?

But I guess they're active on the edge. I don't know, yeah. alright, I think it's an interesting artifact, it's what you're saying is, imagine I see a red truck, and then I say, if the center was not red. I would notice it. Yeah, but then I'd have an edge where it goes from red to white, or something like that, Yeah, so it's impossible to set up Almost the control experiment. That's. Although, if you do look at a pure... if you can really isolate yourself and look at a pure color, like, all you can see. Over time, that color disappears. You don't... you're not even sure what color you're looking at anymore.

some of you... I forget the context of this experiment, but you can do this where all of a sudden, an environment... your entire environment turns red, you perceive red. What changed, that was important, it changed from non-red to red. But then, over time, it just... you forget, you don't really know what color it is anymore. it happens, I think, just when you close your eyes. If you're not sleeping, and you focus on your visual field when you close your eyes, you start seeing colored patches that kind of move and oscillate. that's a different problem, but you're right. Anyway, so it's not clear... I think that example, that little illusion you showed, was a very clear example. Also, there's a thing called the one where they show with the little Pac-Man with the triangle. Yeah, the, Casino Triangle, I think it's called. Yeah. He's a triangle. And there, too, you perceive the edge. It looks like you're seeing an edge there. But you're not. It's... it's incredible. It's not... it's yeah, you're perceive... So here we have an example where you're perceiving it as one, there wasn't one. And then with the checkerboard, we have examples where there is a transition from black to white, and you don't even perceive it.

Just for anyone who hasn't seen it before. It's this illusory... Triangle that appears to be floating.

That's the illusion.

And you Even though there are no triangles in the picture. That's true. But you don't... you don't perceive the one in the background. You don't... you don't perceive the edge. you assume it's occluded. And, but you actually can... you actually almost, you're sensing the edge of the white triangle, which you... which it doesn't exist. You feel... it's the same as the previous illustration. Yeah, this is this example. Where... Yeah, so he had the illusion... again, most people probably know it, but in case you don't, the... the kind of counterintuitive, truth is that A and B are the same color. the same, grayscale value. But of course, B looks like it's white.

But I guess this one, it's not just the, I don't think it's just the, gradual change. It is also playing on, higher level. No, you know what? Perception with the shadow and stuff. But I think either in my book or somewhere, showed that's not true.

Oh, really, if you cover it up. You say, this is not the exact image I started with, which was not a green cup, it was a cylinder. But you can remove that. And you still... I can maybe try to find the image I have. You can remove that. Interesting. And so you don't know there's a shadow. Yeah. Are you still... you're still. You still think they're the same? Oh, sorry, I still think they're different, rather. And, you think they're different. It's so... I've... I've narrowed it... I even took... I took these images, and I, I edited them so there's just little, skinny rectangles that go across the A and the B. And I really tried to figure out, what the hell's going on here? How does this work, right? So you can remove all this extraneous stuff, you can get rid of the fact that it's a checkerboard.

And I concluded, I could be wrong, but I've concluded in the end that I got it down to this example, where it's basically a gradient between edges, and you can't... you just don't perceive the gradient. Now, this has got a lot going on in this picture, so what I did, and this has also got a 3D perspective to it, all this stuff. So I isolated it down to a very simple version of this, and it was still very strong. And I think the only explanation there was, that, in a sense, that shadow is almost a... it makes you feel like there's a higher order effect going on here, but it's not really true. I got rid of it. And it still works. Okay, yeah. Yeah, anyway... So if, Cool, what? If they don't have, We can only detect the changes in colors. How's... how are gradient, slow-changing gradients? Detected and modeled, What would happen if the fire truck would have a gradient towards the middle that slowly changed from red to white, but there's no clear edge? Yeah.

Is that just The receptive field size, some receptive fields are large with low resolution, and for them it is a gradient. And it's an interesting question, it's like going... it's very similar to the little checkerboard we just showed there. I'm wondering if... I got rid of the idea that there was a cylinder and a shadow, but I didn't get rid of the idea that there were... very clear edges. In a regular pattern going across here, like a checkerboard. Whereas, if you don't have that, if you just have, a fire truck.

But yeah, basically you're saying, Vivian, like, how do we perceive this, or like, how is this modeled? if this... Essentially, Kind of drive activity. here, this sounds really crazy. But... one possibility is, and I'm not... it's hard to believe this, but this is what I suggest. Imagine you really only are detecting the edges, right? And those edges are just. and this is all in your mind? Yeah, yeah, obviously, if the gradient's strong enough, it would trigger a, a center surround cell, but if it's very weak. But what about, like, when there are no edges, like the sunset, for example? that's my point, when you get rid of the edges. then... then these... that's like being in a total green environment. All of a sudden, you start losing your color abilities. It just doesn't... It doesn't even... you're not even sure what color it is anymore, if everything is the same color.

It's just like you lose the ability to tell if it's brighter or not. It's just... These are all relative The basic theory, which is hard to believe, but it seems to be a lot of evidence, is that everything is relative. if you don't have a relative change anywhere, you don't perceive it.

obviously, in those pictures there, we had a gradient. If the gradient is sharp enough, then you might perceive it, but if the gradient is gradual enough, you wouldn't. So a sunset... there's always an edge, right? Someplace. You got clouds, you got the horizon, I don't know, you got the building, I don't know what it is, you got edges someplace. It's hard to see a sunset without any features.

I bet you people have studied this. Somebody has studied it. Fine. Something on it. But yeah, that was a good, I guess a good primer on illusions and, perception of color. do. Yeah, I think it'll help with the discussion later. As I said, I've got some stuff on This kind of filling in effect. If we did want to explore it in Monty.

Do you see my slides? Yeah. Okay. Yeah, so the... there's a couple of situations that can arise, but you can imagine We have a mug, in Habitat, that Monty's perceiving. It has a bunch of hypotheses about where it might be.

This is where the sensor actually is, and this is where the sensor's gonna move. And this happens to be one of the hypotheses. I'm saying 1 and 2 are where it actually is? We wanted to go in red. Green... green is the... oh, no, red is... I see, red is... Yeah, red is actual, and green is... a hypothesized location. Okay. moves from 1 to 2. So in this example, this hypothesis thinks it stayed within the mug. But it actually moved off and is sensing nothing. Either it's looking into the far distance, or it's the finger, and it's not sensing anything. And this was the first situation that sort of got us thinking about this problem, because right now what happens in Monty is we Monty, for lack of a better word, kind of panics, and then goes back on the object. And forgets about the fact that it saw nothing, that it stared into the void. Actually, it doesn't send that to the learning module. No, that's what I was gonna say. Yeah, so it moves back onto the object, and it... that information is not sent to, yeah, the learning module.

does it update its hypothes... does it update its hypothes. Nope. Oh, that would be a mistake. Yeah. It seems like something ought to happen. Yeah, so this hypothesis seems clearly wrong, and so it makes sense to process this as a prediction error. you perceive nothing, but you expected something, and to have negative evidence. And so this seems like a straightforward change, and so that's why we planned on doing it. But then, yeah, there's just some... Kind of edge cases or situations that arise that make it a little more complicated than that.

One of those is we can also have, in this context. What you would... what we call an... Can I stop you for a second? When we say we have a hypothesized location.

That's a spec... a location on a specific object, right? Yeah, so this is within the reference frame of the mug. this is weird. Lately, we've been talking about the idea that attention has to do with some volume in space. Yeah. And, in some way, when I see that sensor movement from 1 to 2, It... Does it think it's on the right side of the mug, or is it... that hypothesis... oh, I'm sorry, the hypothesized location, it thinks it's on the left side of the mug, so it thinks it's still sticking within the reference... it says, I'm still. Exactly. So it's going to predict... it's going to predict, oh, you're gonna sense surface, you're gonna send smooth ceramic, whatever, but actually, it gets nothing. Okay. that would... okay, alright, sorry. So that's where we're saying, okay, that seems pretty clear, okay, it's good. Yeah, I'm sorry. No, that's okay. It should be a prediction error. This hypothesis isn't a good one, because it's not. we should get rid of it.

But then... In this context, as well as other contexts, you could get what we've started calling an out-of-reference frame movement.

imagine the hypothesis thinks it's here. And whatever movement takes place, you can imagine it goes off the object, Ends up moving it out here. And in this case, it's actually moved beyond the learned reference frame. Imagine the learned reference frame is a point cloud corresponding to the muck. So it's beyond anything it's learned before. Or knows about the object.

How's this different than the previous case?

In some sense, if it senses nothing. then this hypothesis is actually correct. It could be correct. It could be that the sensor actually was here. And it moved off the object. So why would it go outside of the volume of the object? what if the system as a whole knew where the object was? Then it would know in advance that this would be moving off the object. What if the system as a whole. if I could look down and say, okay, I know where I am with this mug, I know where the volume... I... this particular... If I could... if I... it could know in advance that it's moving outside of the bounds of the mud. What if it just knew in advance it's... it's... Yes. I think that's the hypotheses, the hypoth... what Monty thinks is actually correct, so it's correctly predicting that it should be off the object now. Yeah, so what's... so what's... so what is the problem there? At that point, this column shouldn't be participating in... in voting. Oh, yeah, or in kind of perception, exactly. if this hypothesis is correct, yeah, exactly that. It shouldn't maintain, or certainly it shouldn't pass up the hierarchy, mug at XYZ, because there is no mug at XYZ. and that... Yeah, I'm way behind. So this hypothesis was correct, but it's no longer relevant after this movement. I was... remember, we've been moving towards this idea that there's, a tension in a body, a body-centric space. And all the columns know whether they're attending within that space or not. Yeah. Just assumed that they knew this. and only the ones in that space get to vote. So if you're outside of the space, you don't get to vote. If you're inside of the space, you get the vote. And... and that's the gating factor. But so this column is attending to that space, it's just it's not sensing anything there. But it's outside of the bottom of the object. If it knew... Yeah, this one, yeah. so at this point, it'd be like, imagine I'm moving my retina around, and I'm looking at different parts of this cum. there are many columns that are not on the cup, right? And columns that are on the cup, when I'm looking on the left side of the cup. They will be off the cup, I'm looking at the right side, and vice versa, right? There's tons of columns that are going in and out of the actual object all the time. And, the... the way I've been thinking about this recently is that there's a sort of a... a global attentional volume of some sense. And every... every... every column, it's either in that volume or outside of that volume. If it's in that volume, it's... it's part of the... it's part of the inference process. If it's outside of the volume, it shouldn't. And the way you, might... you might enforce that is this... by some sort of voting, you don't get to vote if you're outside of the column. You can get an input, I don't care, but you don't get the vote. Your input doesn't make any difference to me. We're only attending to this spot, and this area. But that spot is in body-centric coordinates. I agree that we should restrict voting based on that, but to me, that feels separate from this particular hypothesis being inside or outside of the reference frame. but you said yourself, does it pass up the hierarchy? No, it doesn't. If it doesn't vote, if that column says, I've gone... I'm out of bounds. I don't get the, my vote doesn't count. then it doesn't go anywhere. It doesn't... it doesn't affect anybody. Nobody. You could just scream in the wind all you want, doesn't matter, you're not part of this team right now, you're outside of the boundary.

So I just think, like, when we think about what happens in this case, I haven't let you get to that point yet, but when you... as soon as you said the word, oh, does it go up the hierarchy or not, I'm thinking, no, it shouldn't... This is all recent thinking, But what do you say? Yeah. Would you say it's still... it doesn't output anything to up the hierarchy or for voting, but would you say it still gets sensory input and still maintains the mark hypothesis, so that. I don't know, I don't know. We don't know. Since... since I... we don't... we haven't proposed yet a mechanism for this sort of global attention space, right? Does it gate inputs coming from the thalamus and says, you don't get to pass your input through? I don't know. Is it... Yeah, because in this case, isn't this... let's say the fovea is here. And then now the fovea is here.

I don't... in this case, I don't see why you would gate before the column has even received anything. It feels like it's more the column's decision, based on its hypotheses, to say, okay, I'm not sharing this, because I'm out of any relevant reference range. But why would it be that I can't? Somebody... the idea of a global attention space. Yeah. It's not just global, it can't be at the local to the column. A column wouldn't know that. A column knows where it is. A column has a point in that space, but some... and we haven't proposed a mechanism for this, but... but it seems to me it has to be a somewhat extra column outside of the column to decide Yeah, basically I'm saying that there can be multiple kinds of gating or deciding whether to pass information, but... but if it's... if it's the global attention, that's basically saying, okay, all these columns, you get to perceive stuff. And, after the, Nexicod, all the columns here, I get to perceive stuff. No, no, they wouldn't, because they wouldn't... they'd be outside of the attended space. You don't... Why would... why if we've... if we've saccaded here, why would we be attending here still? Attending is... not where... we could do, what you call it?

Yeah, what's that term? For when you attend... Separate to where you're fixating, but... it's. That's obviously unusual, yeah. it's doable, but it's not really that unusual. It actually happens all the time. Could it just for the... sake of argument, if we just say that we're attending to the same location that we're fixating... No, I can... It just feels like a separate issue from this out-of-reference frame movement. I don't see it is.

Attention is separate from Fixating is just... it's just a term for where the center of the retina is pointing. That's a fixation point, right? But the columns are covering the entire visual space. Different resolutions. And I think fixation often involves, we often fixate within the attended region, but not always, but it's generally a good idea, because you have higher resolution for doing that. But I think, the example I've given a lot is, if something unexpected happens off to your side or behind you, you'll attend to that spice before you can even move there. Before you can move to your, or you'll try to... it'll be like covert attention. You'll, something flashes over there. Yeah, technically, that's the word.

I couldn't remember it either. The covert attention, I think the basic idea is that you immediately start attending, it's a very fast reaction. And then it takes a bit of time for the eyes and the head to move and all that kind of stuff, so... you'll start attending that space, trying to perceive what it is before you actually fixate on it. If it's possible. But anyway, I have this vision, you got this point in some location in this global body-centric space, and at any point in time, all the different columns, sensory columns, are either in that space or not in that space. They're attending there or not attending there. And whoever's attending there gets to participate, and it could be low-resolution stuff initially, but then you'll try to move the high-resolution stuff into that space. So in this case, if I moved my fovea from the 1 to the 2 location, I'm not sure why I did that. Normally, that would be... the reason you would do that is because you wanted to attend to something besides the bug. That's why you would do it. Yeah, maybe it'll be clear with another example I have in a minute, a bit more, yeah, maybe I'll just continue, but... because I think... yeah, anyways.

I think both these mechanisms are relevant. I guess I'm just trying to say, I don't think it's the same thing. What both mechanisms, what two mechanisms? A learning module keeping track of a hypothesis that moves out of reference frame. And a global, attentional window that gates what columns are receiving information.

those would both... take part in determining how information flows when this movement takes place, but I feel like they're different things. maybe we shouldn't spend more time on it if you don't it, but... it does seem like they're different things, but I do feel like... It's for example. I agree, yeah, I agree, like, when you... when you move your eye, that global attention is definitely going to change, and... and that is definitely going to affect which columns, are getting information. No, I think the global defense... attention's independent of which... where the comms are pointing. There's a... there's an attended volume of space. Sure. And a column is either in it or not in it. If a column moves out of it, it's possible. It's possible that the column moves out of it. It's still in the reference frame of the object, but it just doesn't get the vote. Yeah.

if I haven't re-anchored my reference frame, then I'm still... I'm still thinking about that object, I'm just not on it, right? but my reference frame is still anchored. And so my point is outside... in that reference aim, it's outside of the egocentric reference aims, therefore I don't get to vote, and I'm not... and my input is ignored. Whether it's... whether we shut it off or not, I don't know, but it... my... I'm ignored. And... but I could still be modeling the object. I'm just outside of... I'm in the reference frame of the object, but I'm not on the object. I think that's all consistent. And then when you attend to a new location in space. that's not the same as moving your eyes there, but this is a big change in my thinking here. Attending to it's not moving your eyes there. Attention is, I've now decided as a brain, we want to look at something else in the volume of the world, and now I would assume that I'm looking at a different object. if the eye moves to 2, and I don't change my... if I don't change my attention, just my fixation point, then everybody's still thinking about the cup. But if I move to 2 because there's another object there, and I now have a new volume I want to attend to, then the cup is forgotten, and everyone's looking at the new thing. I know this is hard to think about, but... Maybe we should just. Yeah. Yeah, maybe the next examples that Niels might bring up might have a bit, because... I think maybe the point that might come up next is, sometimes it gives you information about what you're sensing. Yeah, it has practical implications. Okay, let's go, for it then.

But yeah, but I think generally we agree on this, that, yeah, even if this hypothesis was right, it's not sensing the object anymore, so it shouldn't Pass this... it shouldn't continue passing up in some way, mug at location 2.

And... And really, we one thing we realized is we don't need to wait for an off object. It may be that there's another object here that we sense, or maybe that we're now off-object, we're pointing at the sky, or whatever, but we don't need to wait for that. Because as soon as we moved out of the reference frame. We know we're not on this object anymore. Just one last thing. Do we still increment the evidence of a hypothesis that correctly predicts that's in the void now? Because it might come back onto the object later, and... Yeah, so this is the question of how to best handle this, which I'll get to. At the moment. it's the same as before. If it went off into the void, it would just move back onto the object, and nothing would happen to the evidence value. But this is... this is where we did some experiments where we thought we could prune hypotheses quickly by, if we have this out-of-reference frame movement, we get rid of that hypothesis. I now think that's maybe a mistake. anyways, we found, at least with a small, narrow... a small spatial filter, that kind of made us susceptible to noise, because Basically, if we were slightly off of the reference frame. We would discard hypotheses and say, oh. This isn't on the object. it seems like we shouldn't be doing anything because we didn't make a prediction. if we're moving out of the reference frame, the model can't predict what's going to be there. So there's no prediction. Anything could be there. There could be any other object, there could be nothing, there could be... The same mug again... the same mug type again behind it. if there's no prediction, it seems We shouldn't be able to increment or decrement. Exactly. evidence. I just want to say this again, I think we're going to have to be more precise in our use of language, and so this is new, it's not like I've been talking about this before, but when we say out of reference frame, that's a very confusing statement. There's a reference frame? You're right, that's true. Maybe out of model. We moved to a location in the model that doesn't have a feature associated with it. but... That's even more confusing. I feel like, isn't it, if we say the reference frame is the space. And the model is the learned points in the space. I, was... Dude, I... I've always used the word reference frame to refer to the anchoring of grid cells. to me, a reference... the anchoring grid cells, that reference frame covers all of the universe. It's an infinite space. Once I've anchored the grid cells, I can represent points everywhere. It's independent of the area occupied by the model. And so that is, the attended area, or the... the attended area is also the models area.

I wouldn't say... to me, is when you... if I say out of reference frame. It doesn't make any sense. It's either... either I change my reference frame to something else, I have a new hypothesis of what the object is, or I'm in the reference frame, I'm just out of the expected attended area of the object. I'm out of the area that's... generally occupied by the object, which is the attended area, in this case, if I know the object. We haven't talked about how to do any of that stuff, but, Yeah, out of model movement, it's still in the reference frame of the object. It's just out of the... models. or it's features, where the features are known to exist. Yeah. Which is also represented globally, somehow, Yeah, no, that's a... that's a... Clear, what you call it? Yeah, we're gonna have to practice this, because we've been using the one sloppily in the past. Not poorly, but now we understand it has to be emphasized. Okay, a practical. Yeah, and so there's a practical problem. Vivian first came across this when someone was working on MNIST, And trying to get that working, and basically, there's this issue of distinguishing a 1 versus a 7. But I'm gonna use what I think is an even kind of clearer example of An eye versus a vertical bar. And you're trying to distinguish these. assuming you've learned models for both, you have an eye model, and a vertical bar model.

And now you have this visual field where you're going to see a small part of the world at any given time. You're gonna sacrad around. Okay, and now... You need to predict what you're going to see up here.

We don't know yet, right? You don't know yet? You have two hypotheses at this point. Yeah, you have two hypotheses at this point. You have equal evidence for both of them, they're both equally valid. And you move to that location, now you see... you see there was nothing. So then the question is, okay, how should we handle this, internally?

Because the way Monty would currently work is it would... it would basically move back onto The vertical bar, because it was a vertical bar. And it wouldn't update its evidence, and it wouldn't, be able to deal with this. If we, so when... a natural thing to do is... We don't get input, we don't send the observation to the learning module if there was no feature in there, if there was nothing in that field. So yeah, a natural thing to do is... Past this, off-object observation. That basically... so this is the model that moved here, and it predicted something because it has these points in the model that's not consistent with kind of a null or absent morphological feature. And so this hypothesis should receive strong negative evidence. That was the kind of simple example we had before. there. It's not there. If the dots on Yeah, The vertical bar is a bit trickier, because, yeah, we initially thought, okay, this is an out-of-reference frame movement. So in the past, we've discussed, actually reducing evidence for that object, because we're moving off of it, But... You could al- the... polar. Oh, look... As you could say, is this actually evidence for the vertical bar, given it correctly predicted and there was nothing there? No. I would think not. and that... I would agree, which is good.

what I think we've converged on, it sounds at least, is that If the hypothesis goes quiet for some time, but does not receive negative evidence, Then, because the issue is, if we move... we need some way to actually build confidence that this is... this is the bar, the vertical bar. But if we at least... if that goes quiet, until we move back onto the object. And since the other hypotheses that predicted something would get negative evidence. Then that should be enough to basically separate these out. I'm confused about the mechanism here. It seems like you have two hypotheses. Those two hypotheses would have two reference ranks. That is, they would have two anchorings of reference ranks.

whether they're simultaneously held or oscillating between them, I don't know, but there's two different reference frames. And in one case, there's a prediction that, that there should be a dot. In the other case, there's no prediction it should be a dot. in some sense, I don't know how you do this, but as soon as you test, you could... put the mechanisms aside, you could differentiate between the two hypotheses by, in the ref... in the object model for the dot... for the dotted bar, then you go there and it's not there, that just eliminates it right away, right? it just says, I can't be right.

I don't understand how that's consistent with, What was the term you used a moment ago? On your record. Okay, that's what I was... sane. Maybe the terminology's unclear, but yeah, the out-of-movement... out of model movement. We basically want to maintain We're not going to add evidence to that hypothesis, because it's not predicted anything, and we're also... we're outside of that, model anyways, so it doesn't make sense to accumulate evidence for it. But... I guess you wouldn't move... you wouldn't move to that location. if... imagine I can maintain... I can maintain... let's say I can maintain one hypothesis of the other. Just imagine, oh, is it A or is it B? Is it A or is it B? under A, I would say I should move there and see if there's something there. Under B, I wouldn't move there. There's... I wouldn't... I'm, Wouldn't you? Because you're trying to tell, if I had given you this, and you were trying to distinguish which of those ones, that's the natural place to go. but I have... imagine I have to go between these two, right? if I'm considering hypothesis A, I would go... go there just to... because I expect it to be there. But if I'm... if I'm considering hypothesis B, I don't need. You're considering both. likely at. Good point. But you're acting on one, I think that's what Jeff is trying to say, you're... you went to a hypothesis, but, there's, one, that's... let's say it's, leading... Sure, okay, I agree it's hypothesis A that's telling you the reason to go there. There's nothing in B that makes that point... there's nothing in this model that makes that location interesting. I agree with that. which is A? A's got the dot, or B's got the dot? I guess we're saying A has the dot. We can just say aye. I have forgotten. Critical Bar does not. But the weird thing is, as soon as you go there and you see nothing, even though you're not on the... bar anymore. You instantly know that you were on the bar. Because, it's not the eye.

I think, okay, this depends on what kind of models that you have, in... Monty's brain, if it's just eye and vertical bar, then I think we know that's the vertical bar. But then we need to consider, every other model that exists in Monty, and we might need to give, positive evidence for all those models, we're basically... if we give evidence, positive evidence, then we're... increase in a discriminatory space, right? the difference between. Or you mean rather negative evidence? In this case, we're only giving negative evidence. Yeah, so I... yeah, negative evidence with the eye, and then positive evidence with a vertical bar. Do we want to give positive evidence for the vertical bar? My statement is no, my position is that we shouldn't, even though I know that, if we don't see anything, it is immediately vertical bar, but that's only the case for when you have eye or vertical bar only. in general, I don't think we should give that positive evidence to the root bar. Yeah, I think we agree on that. I guess the question is just, I think we agree that... we want to give negative evidence for the eye model, because it predicted that there should be a bar, that there should be a dot, but there wasn't. And then, we don't want to give positive evidence here, because we're not actually on the bar anymore. But it also seems like if we move back again, we know it's the bar, and not the I anymore. I guess I'm staring. Maybe that's... When you're moving up and... when you're moving between these Imagine these two hypotheses have to have two different grid cell anchorings. Yeah. They're not the same. There isn't one reference frame. you can either be in hypothesis A or Hypothesis B, and each have their own reference frame. if I'm just looking at the bar, there's no reason The bar model's happy. It's a happy camper, and it doesn't... but only when I get to the eye model, then... then, then we have to switch to a different hypothesis. I guess I'm trying. Yeah, it's It's not like we're. Yeah. reference frame, and you're moving in and out of the reference frame, it's like there's different reference frames, and there's different hypotheses somehow. Yeah, I think that's how we're thinking about it, but I guess, maybe to give a bit more context, one thing we... Jose tried a couple months ago, and we were thinking about is, if we move off the object, and the model predicts that the object isn't there anymore, like in the case of the bar, we should just delete that hypothesis, because we're not on the bar anymore. could you say the sentence again? I got lost. So we're on the bar, and we move up, we... our model knows the bar doesn't exist there anymore. But why would I move there? The bar model wouldn't go there. The bar model. It goes there because it, thinks, it could be the eye. Oh, but then it's above... The way to distinguish them. At that point, it's... the dot. No, at that point, it switched to the iModel. It's no longer in the bar model, it's the I model. Do a new model, new reference frame. And... haven't we discussed how... yeah, haven't we discussed how, columns could, within, a phase or something, they could have multiple hypotheses? I'm just pointing out that the moment you move up there, the hypothesis you're testing is the I model, not the bar model. So I wouldn't move up there on the bar model. Yeah, but we still have the bar model active. It was still a. And it's still gonna get the movement that integrates to move it up.

Yeah, but at that moment, the bar model is not active. It's the eye model that's active. aren't they... I agree, the bar model doesn't give that hypothesized location to move to, but they're both going to be integrating movement to update where you are. but they're not both in... they can't, We don't know the mechanism. It could be that they're doing this union thing, we don't know, but the point is, let's think of it at the moment, it's you're alternating back and forth between these things. At the moment, you're saying it's an I, or your moment, you're saying it's a bar. And it could be very rapid, it may or may not be conscious of this, whatever, but in the case that you're only going to move up there. If you're on the bar model, there's no reason to move up there. Yeah, but if we moved... if we were on the eye model, and we said, let's go up there and see if the eye is there. We still have the hypothesis that we alternate for the bar, and that one... that space would also need to integrate that movement we just took. But only if I... it would... yes, but we don't know the mechanism, but... but fine, but I'm not on the bar model at that point in time, Exactly, iModel comes along and says, this is me, and... And I'm... I'm good.

You're saying highway limit. Actually, it's actually perceiving the vertical bar. So the eye model's gonna say, oh, I'm bowing out, this is not me. this is the case where it's a null, up there. Yeah. Which is the problem that we're considering. Is the part of the... It's straightforward when it's not the null, and we're still on the object. That way, that's a case we already handle. Oh, I would think the harder part is you see the dot, and how do you tell it to get rid of the bar? No, that's straightforward. That's pretty straightforward, because... doesn't exist. The bar model doesn't store features there, but we get features as input, so that gets negative evidence. But it could be the bar, and then there's some noise in the background.

Anyway, alright, I... Yeah, it's... I thought that's a harder one. Why do you think it... you think the hard one is... is what? Let's walk through exactly. There's a... there's a dot there. There's no doubt there. Oh, okay. The hard one, where the dot is not there. Okay. And how do you... Two reasons. One is we don't send that observation at all at the moment, because there's no observation there.

I guess no sensors around cells being triggered. The second problem is, even if we were sending that observation to the learning module. the eye model would not have a prediction of what should be there, because, like you said, anything could be there. It could be a textured background, there could be whatever, so it wouldn't make a prediction, so we wouldn't... It wouldn't think that the eye is there anymore. Because that's not where its model exists. I still think it's... I'm still thinking it's easier the other way around than what you're saying. I think. In our code, what's missing from our discussion is that our movements doesn't have, a doesn't tell us the reason, right? we will move up to the dot part because we think that we're acting on hypothesis I, but in our... in code, we don't know why we're moving, we just moved there, and we update all the hypotheses for all objects, I, L, A, whatever. But it sounds like what we might need to do is that, okay, I moved because I thought I was an I, so when I do that, I'm going to update only the evidence for I. Maybe that's a term. we do have that information, the learning module knows what's its most likely hypothesis, and that's what it tests with the movement. but I'm not sure if it would help to only update the most likely hypothesis at every step. Did I.

What? Nope. Okay, I have a different approach. I have a different approach to think about this, but finish your thought with Vivian. I think... we will be updating all the hypotheses associated with that object, not just the MLH, not just one hypothesis. But all the eyes... So it will be... if there's 100 hypotheses for every object, then we'll be updating all 100 of them. Which would include the MLH.

Yeah, but what would be the benefit of not updating the other model's hypotheses? if we only ever update the evidence for the most likely object... The other ones are not taking any advantage of the sensory input that we're getting. Even, Jeff, even in the 2019 paper, in the model, multiple hypotheses were updated in parallel, multiple reference frames. And yeah, I feel like we've talked. But that was. Ways that even without a union, it could be done. maybe, but. Bam. I have a. I'm not saying it's a good idea, but, it sounded like this was, like, okay. this is one way so that we don't have to give positive or negative evidence to the vertical bar. Anyway, that finishes my thought to Jeff. I just want to... I'm a... a different way of thinking about the whole problem, okay? Just... so just bop up a level here. So imagine, we've learned, these two models. And, but now, normally, we would infer using multiple models at a time, like voting, right? imagine I have a whole bunch of columns, and they're voting, and each column knows both of these models. Because all the components would be observed at once, there'd be more evidence for the I than the bar, and you would quickly... you would immediately... you wouldn't have to move. The boating, you would decide right away, oh, this is an I, or this is the bar. no movements required in inference here, you've got lots of columns going at once.

Assuming I have these two models in the... in all these columns. Now we're introducing sort of an artificial system where we're looking through the straw. And we're moving around, trying to observe the world. Of course, we know that this is actually very hard in reality to observe everything through the, through the straw. But imagine I was doing that. and I... I move around, and I see my... see this eye. At that point, I don't think... I'll question, would I say, oh, this... I see the... excuse me, I see the bar. I'm seeing the bar. In the bar, and then you see nothing. no, I haven't... no, I'm looking through a straw, so far, I'm on the bar, I'm moving around the bar, and I go... Don't know... Now, do I sit there and go, hey, maybe there's another hypothesis that I haven't observed, is more to observe, I'm gonna go look for that. I'm gonna question that. I'm gonna say maybe not. there's... I can come up with lots of situations where I give you some data, and you think you understand it, and you have... you say, yeah, I got it, I know what that is, and then I say, oh, but there's another piece over here I didn't show you, and then you, oh, that changes everything.

Now I have a different perception of what it is. I'm just... let me just walk through this thought experiment. it may not hold water, but let's walk through it. Somehow, I've got these two models learned, I come out with my straw, I'm moving around, and I see the bar, and I go, good, damn, I'm done, I got a bar. I have no reason to look elsewhere. Now, I might be done, and I might miss the fact that it could be an eye. But in reality, in a real brain, a couple things would be going on. One is, of course, I would have somebody observing the dot. above it, and that person would know about the models, and that would make a difference. I would know it's an I. If I didn't have that person observing it, it's I can't tell. I just... I'll have to assume, if some part of the world is occluded, and what I see is consistent with what I know, then I'm gonna go with a hypothesis. I think that we might do that. Also, if the dot appeared, all of a sudden, I would say, oh, there's something up there, I have to move up there and attend to that location to see what it is, or something like that. But I'm wondering, is if this is the case where looking through the straw is not a good example? Because it seems like voting... It felt like when Neil was showing the example, it was pretty intuitive that we're looking through a straw, and we are able to recognize the bar, and if we have the task set up that it could be an eye or a bar, we would move to where the dot could be. I don't know if I would, I'm questioning that. If I have no other context... If someone told you that's what you had to know, that's what you're trying to figure out. but let's say, okay, I didn't know that. I'm just saying, I'm a column. And. If someone says, you should be looking for an eye, then I would be invoking the eye model, and I know to look up there. But if I'm just asked to sell, I'm looking at the store and saying, what is out here? What am I observing? Here's a bunch of features, they're all consistent with the model I have. And my straw hasn't gone anywhere else. Would I feel like I need to go anywhere else? Would I feel... Assistant with two models you have. Yeah, but I might just make... I might make the wrong assumption. I might just say, okay, it's a bar. Good, I'm done. that bar could be part of a picture of a banana. It could be part of a tank. I don't know, it could be anything, but at the moment, I've just said, what am I observing? I found an object, here it is. I'm good. I have no idea at this point, is there anything else? I wouldn't even know... to look elsewhere, unless I had a different hypothesis. And the question is, why do I have a different hypothesis? I just... bear with me on this, I think there's some validity to this. I don't know, doesn't it... it feels With... with voting. The classic example of, a mug with a handle or without a handle, the whole idea is we'd move to where we think the handle is. the handle is just like the dot on the eye. We're trying to... we're trying to realize, what mug we're holding. in this case, imagine if I'm moving my straw around the edge of the... of the mug, I would see an area where there is a point that doesn't... where the handle comes in, and it's not... it's clear that this... that this morphology is different. In this case with the eye, it's off... it's often space. It's not... Deliberate, because, yeah, Tristan, you asked, like, why aren't we doing the 1 versus 7? So that was deliberate, because... In that case, there's this alternative signal for distinguishing them. Is it an alternative signal? It's just that I get to the point where the morphology of one object doesn't fit anymore.

imagine I'm looking at the 1 and the 7, right? and I see... I'm inferring, and I go, hey, this... I think this is a 1, if I could observe the whole thing, and it was consistent with 1, I'd be done. But if there's some other dot off to the side that makes it into something more than one. I'm not sure I'd go there. again, voting would work. Remember, voting would work. If you had multiple columns, it would reach the right conclusion. It's just, if I'm providing impoverished input, where I'm only allowed to look through a straw, I'm questioning the fact that I would leave the ob... I would leave an object that I recognize. To test a different hypothesis, unless there was some reason to do unless someone told me I had to do it, unless the instruction was, you are where you should be seeing a 7, or you should... no, excuse me, you should be seeing an I.

That kind of thing. Yep. This one did, yeah. This could be an artifact of two things. One is we got a single column, like a straw, looking through a straw, and B, we have discontinuous objects. And in that case, I'm arguing that the system will fail.

But it just feels like a human can do it. A human does it because we vote. No, but with a straw. If I say, tell me whether you are seeing an eye or... that's the difference. A vertical bar, feel free to move as you need. no, the human example would be, look through the straw and tell me what can it be both? Why... why can't. Why, what you... ask a human to do either. You would... you would set a long hypothesis, oh, I got it, I see what it is. Imagine it's the two theater faces, in the theater, there's like this, I don't know, there's a smiley face. frowning face? You know what I'm talking about? Oh, dude, trust, yes. Yeah, Now, imagine they were just, two faces, but I didn't... I'm looking through a straw, and I see one face, and I go, oh, I got a face I'm done There's no reason to go anywhere else. I'm done, I got a face, it's all consistent, there it is. I'm... I recognize this object. I wouldn't go, hey, what other things might be out there? I wouldn't have thought of that. it just wouldn't occur to me, to that... that there's an alternate hypothesis that's different. It's no, I got an object, it's consistent, everything's done, there's nothing wrong with this, it's a face, I got it.

even if we vote, doesn't the problem still exist? that if we're on the eye... er, on... if we're on the... Bar, without the dot. the... It wouldn't be voting... the columns that are where the dots should be wouldn't be participating in voting. all the columns that are voting have equal evidence for the bar and the eye, so they wouldn't be able to resolve it. but... but there is a... wait, if... but if... if... I don't get it, I miss... maybe I'm missing it. If... if... if the dot is there. There's additional... The daughter's not there. That's not there. It's the vertical bars. so what's the problem with the dots not there? So if the data's not there, then... The columns that are observing the empty space where the dot would be. Wouldn't be participating in voting, so they wouldn't contribute any information about the absence of the dot.

but that is consistent with being a bar. So then, the columns that are modeling the bar would need to learn about. the absence of votes? no, because the columns are... everyone has models of both. And... and the evidence, all the evidence sums up this thing at the bar. There's... there's no other evidence to support. Same amount of evidence for the eye. No, it's not, because. The same number of columns say. It's an I, is the number of columns that say it's a vertical bar. But I... they wouldn't, because I have this additional information on it, right? I don't get it. But, yeah, and so they're all silent, because there's nothing there. If there's nothing there, it's a bar. I'm missing this. Maybe I'm... I'm sorry I'm being dense about this. But the columns don't know it's a bar, they have equal evidence for the bar and the eye.

I see.

It's basically one model is a subset of the other. And the missing parts from the subset, that information is not communicated anywhere, because we don't communicate about things not being somewhere.

I think it's still gonna work, hang on.

I don't see it. For this is maybe to modify voting somehow. It sounds like such a simple problem. I know. we come back to it, this was one of... this was, I think, the first item I put on the Future Work Roadmap, 3 or 4 years ago, because it seemed so simple, yeah, let's send off object observations and... Use them. Every time we come to it, there are some tricky things. I'm still... I'm still not convinced there's a problem. Okay, can I... Can I maybe just go through a few more slides for... Sure. what I wanted to show, and then at least then people can think about that. But, I definitely take on board your point, Jeff, Yeah. How much... do we consider the infinite space? I like this example, I think this was from Chris Summerfield, but, and this was a critique of Bayesian models, but it was like, if your doorbell rings, you don't think, oh, is that the Prime Minister of Finland with, a low probability? you don't cover the, like... infinite hypothesis space that is... is possible. And so I take your point. lacking further reason, would you really consider, other hypotheses? But I still feel like there are conditions where you can you can say to a human, this is the task, and a human can solve it, but anyways, maybe I'll just. Again, the human being... Separate a human looking through a straw versus a human looking at the whole retina. Yes, here's a... yeah, okay, go on.

just Misha, I just thought I'd answer your question briefly. yeah, I'm not sure if you were here at the start. we are planning on, passing a no-feature observation, but one of the tricky things is we don't want to store that in the model. Because we don't want to store no feature, at least, yeah, and that's another thing I'll talk about later, is, maybe how could we get a hybrid? But at least we don't want to store no feature throughout all of space. And so, that's part of the... the trickiness of this, but. Yeah, makes sense. Cool so then... I'm not sure it means the process and no feature, then. If you don't store it, what's the point of it? What is it? Yeah. Okay, anyways, was, like, the very first slide that Nils showed, where we all agreed that If the hypothesis was that we should be sensing the mug, but we're not because we actually moved off of it, then that hypothesis was wrong and we should delete it. that... That currently doesn't happen in Montana. What was... We've left them up. Sorry again, if I move off the mug, why would I delete the hypothesis of the mug? Can you go to that slide again, Niels, maybe? Real quick?

Yeah. The hypothesis on the... was that we should be on the mark after the movement, but actually, we were off the mark. Oh, But this is the straightforward condition. That's the straightforward condition why we thought, oh, yeah, we should add off-object observation, it's just a technicality that we haven't been using them at first. First time I wrote this down as a to-do, it was like, oh, this will be easy. Let's just process it and say, oh, this... That was actually... hypothesis was wrong. But then, if we do actually sense that we were now off the mug, or empty space, then we don't want that, we don't want to be storing that in the model of the mug, and yeah, someone. But the point, as soon as you move to 2, the pink 2, the red 2, You, eliminate the current hypothesis, right? Yes. Yeah, but right now in Monty, we don't, because we don't send that observation to Monty... to the learning module.

Yes. Why do you have to send an observation? Isn't just the fact you didn't get one sufficient?

can't a car... We also don't send the movement.

Alright, now you confuse me. I'm a learning module. I'm in a simulation, and I don't get... I don't get any input, and I'm predicting one. That's local knowledge. I don't need to send in a null hypothesis, right? It's just... the fact that I didn't get one is a null. Yeah, Yeah, It's the same thing. Okay. Basically, we need to step the learning module. It's just a... Okay, you have moved, you should have perceived something, but... Didn't that... And you predict something, but guess what? We're not giving you anything, because nothing... nothing was perceived. but it's a local thing, it's really local. I just... I didn't get an input. I'm a bunch of neurons. I didn't get an input, therefore, it's not like the retina had to pass me a null hypothesis. No, Okay. Alright. Yeah, in Monty, we need a data structure that at least tells the system this isn't. 5. But yeah, I agree, in neurons, we wouldn't... There wouldn't be anything peasant.

can I ask. Yeah, please. Why is it that we don't send anything to the learning module in that case?

Yeah, because... In that case, we would want to, because it seems straightforward, but it just... it's all the complications it introduces, that's basically the issue. you're right, in that case, we should say, you didn't observe anything, and it'll eliminate that hypothesis, and it'll be great.

In general, I recall, the sensor module processes the observation, tried to extract features. I guess there aren't features to extract, because we're looking at the void. Decides not to send anything out, right?

Yeah, the useState variable, this is very code-like, I think the useState variable is... if the on object is false, then I think the useState will be false even if we extract. other things.

Yeah, there are several reasons why we don't send observations to Monty. For example, also, if the features haven't changed significantly, we wouldn't, send it to a learning module. But I think, yeah, this is the kind of, clearer part of what we definitely want to keep on the roadmap, that. Yeah. We want to send that to... we want to at least send a movement to the learning module, and even if we don't have any features, since that location, not even an orientation, not even a morphological feature. We still want to... Send the location chain, and then... Yeah. use that.

I agree. Yeah, that could be a way to tell, if there's no feature sent, just a location, don't incorporate that in your model.

Yeah, exactly. But yeah, and I guess at a practical level, Scott, I guess part of the issue is it just will change a lot of things. But you're right, it is a change we should make, that one at least. But, yeah, we want to make sure it doesn't... we don't store these during learning, yeah, but there's ways we can handle that, but also logging tends to make... or, the buffer tends to make use of off-object observations to, filter and stuff, It's the kind of thing that, unfortunately, once we change. There'll be some, knock-on effects, but yeah, maybe I'll quickly just. I'm not following... I didn't follow any of that, I couldn't have told... Yeah, and some of. Yeah, it's a cool thing. Yeah. It's not really... for a suggested approach, to rephrase what we want what I'm suggesting we want the kind of learning module to be doing is, so you've moved up here to this location, you're no longer looking at the vertical bar. We're just gonna have that representation go quiet. And I'll suggest what that means at a neurological... a neurobiology level in a minute. But then the representation for an eye is actually going to get negative evidence. And I think what's nice about this is, as well as potentially helping with some of this, it has a natural kind of biological analog. So for the representation that goes quiet, this is the out-of-model movement. Dendritic spikes, which are generally associated with predictions, are known to have these much longer, time constants that they maintain the membrane potential at, versus spikes at the soma and the axon.

And what this could practically mean in Monty is basically the evidence level stays the same, or is, slowly decaying when we move out of the model.

And what that means is that if we move back onto the object. Shortly after, let's say we go up, oh, there's no dot, we move back down. The evidence will continue to grow and maintain its level. But if you remember the I, it's going to get negative evidence, because there was an actual prediction error. And so that would separate the hypotheses and actually enable us to say, oh, it's a vertical bar.

And, yeah, and it's also worth pointing out that from a biological perspective, like a dendritic spike, it can maintain the membrane potential at this elevated, Kind of level, without necessarily initiating a spike. Which means... which that satisfies this kind of... Property of going quiet, in that when you move out of the model, we don't want those cells to be telling... that column to be telling other columns, oh, this object is still here. So it would be primed to still expect that object, but it wouldn't be telling other columns, yeah, this object is here.

I think that... that sounds right. Yeah. Good observation. I'm having trouble relating that to the overall big problem, but I think this is a mechanism by which you could maintain a hypothesis that's not actively doing anything. I think so. The cells will be primed by the previous hypothesis. And so when you go back, it's not, completely forgotten. exactly. And, we've talked about how part of that will be from top-down feedback. certainly over longer time scales, if I'm walking around my room, and I turn around, it's gonna be top-down feedback that is telling me to, expect the table, or whatever. But this is more at a... I guess it could be at, a cellular level over very short time scales, as your eyes are just darting around. I'm not sure I agree with the second thing you just said, top-down feedback. Again, we have to... we're trying to differentiate between, I'm trying to infer an object, and I'm not sure what it is, or I have a model, and I want to make predictions for the model. when I come back to the room, I'm... then me, that's oh, I have a model, I'm just trying to remember where things were, and therefore, I'm making predictions, but it's not like I'm... this is... we're trying to understand how we infer the dot from the I, or the. Yeah, I'm just talking generally. Anyway, maybe that's a distraction. I think that's a point we agree on, so maybe it was just the way I worded it. I think it's just because we considered that as a possible alternative to the going silent, that instead, the hippocampus could be building a very quick temporal map that here used to be the vertical bar, now I moved off, now I moved back on, so re-invoke the vertical bar representation. Yeah, maybe. Yeah, but it, yeah, at least when we thought about it, it didn't feel like a good solution, Yeah, especially because we didn't know yet that it was the vertical bar, we thought it could be the eye. I wanna go... I wanna go back... Sorry, I have to repeat these things over and over the head in my head sometimes. I'm going to go back to the... what I thought was a critical observation. Or at least I'm arguing it's a critical observation. that if I'm thinking about a single column, Inferring an object. I'm gonna be following the edges of the object, that's what I do. There's nothing... that's all the information. There's no information in the middle of the object. there's no information from the outside of the object, I'm following the edges of the object, or the features of the object, which are discontinuaries of some sort. And I'm going to move continuously, say, around this object, and, because I... I have no other reason to move off the edges. I'm just doing a model-free movement around this thing. and if I come up with a hypothesis that's consistent. with all the data, and I can move here, and then I can use hypothesis-driven predictions. I'm not going to search for another model. I'm not going to search for stuff. I find the bar, I'm not going to search for the eye. I'm done.

That bar could be part of lots of things, but I'm done. I've got something that's consistent with my data. maybe that dot's off somewhere in space someplace, I don't know. why do I know to look for it? And I have an answer. It's not like my data is inconsistent because it's... There could be other answers. I think that the answer here is that it's going to settle on the bar. That's it. And obviously, if the dot is close enough to the bar so that as I'm going around the edges of it, I detect a dot, sure, then I have some more data, but if the dot's further enough from removed. Then, what if... If you're looking at, a line drawing of an airplane or something. Or, a smiley face, and... You're not gonna explore the internals of that at all, or... again, we're talking about the straw example. Yeah. I think in the straw examples, I... I... I would... if I explored the internal space, I might say, oh, it's a circle. I found it, I'm following its head, it's a circle, I'm good. And maybe the nose and the mouth and the eyes are small features in the middle of this big circle. I wouldn't know to look there. I wouldn't say, oh, it could... Can you explore the rest of them? you might, but... I feel like if you had the hypotheses... But it's like you said with the Prime Minister at the door, right? I have a solution. I haven't occurred chrome across any evidence against a solution. All the evidence I have is consistent with my solution. I'm done Okay, I think the example, or what we're considering is essentially, it's almost like you have the goal state, which is distingu- determine whether the.

Zach? It's a different text. That's a different time. Yeah, fine. Fine. let's... You still need a way to solve it. Okay, alright, let's just go there. Now, before I go to that, let's go to a separate test. Vivian brought up the issue, I said, hey, if we have voting among many columns, we would immediately determine the IR bar, and I assume that's the case, but maybe that's a problem, and she's... I think, Vivian, you said you don't think that's going to work. So I don't know. But I'm assuming that's going to work. So now... so... if I have lots of voting, columns voting, I'll get my right answer. If I don't have lots of columns voting, and I only have a... I only have a little straw, I can... I will... I will settle on the first hypothesis that is consistent completely. Now, there's a separate question, is I'm told to look for the eye. Or is there an I here? And then, at that point, I would invoke the I model. I would say, oh, this is consistent with the I model, this is consistent with the I model, and I would suspect that appears consistent with the I model. But I wouldn't just stumble across it, I'd have to. But it's... remember, in this case, it's a vertical bar. Okay, Would you have some... representation of which model is the simplest explanation? would they have associated complexity with them, and you go with the simplest one, or would you. I would... I would say. about when you come... We're talking about the... we're talking about the straw. And if... if I... if I did not... was not given any other, priors. and just says, what are you observing out here? I would... I would, in some sense, the simple... I would... I would stop the moment that all my observations are consistent with a single object, and I haven't discovered anything else by tracing the lines of this object, tracing the features of this object. But they're also all consistent with the eye. die. But again, with the straw example, I can't know that. Maybe... it's I think, actually... I'm gonna go with the simplest one, right? The one that solves the problem. I'm gonna reach my conclusion, maybe there's additional evidence in the world that would change my conclusion, this is a general truth of life, but if all the evidence I have right now is consistent with the conclusion, and not... it's not partial, it's all there, it's consistent, I'm gonna go with that hypothesis. You have a representation of how simple the explanation is, so you go with the simplest explanation. No, I go with the first thing that's completely consistent with the data I have. And they didn't... Are you tracking when you have... how much of the model you have covered? I... when the bar example, I have... I've observed the entire bar, there's no pieces missing, there's not... it's all there, I'm done. It's not oh, I have a partial bar, I need to keep looking. we said, oh, I could spoil the whole thing, it's there, I got a bar, there's no question, there's a bar. It is a bar. So you can track off if you have covered the entire object. I don't know if I have to keep track of it, it's just at some point, you have enough data to say, this is consistent, everything I've done here, I can make some predictions, I don't know. when do you give... when do you... when do you stop looking? I don't know the answer to that question, but... I'm just asking so many follow-up questions, because if we do go with that premise, we have to redesign all of our experiments going forward, because we can't do straw world experiments anymore. We can't do experiments with one learning module anymore. once we have a more complex environment... No, you can.

I'm sorry, go ahead. You wanna say something, RJ? And, I think... so in Monty, we are naturally driven by one task, which is recognition.

we... so when we, when the task is just describe what I think bar is right, and if it... if the task is, what could this be, then bar... it will say bar or I, because it knows both bar and eye model. And then... and then if the task is, what is this, then at least Monty will... then... then it will... then it will... it has motivation to explore, because it wants to narrow down to one hypothesis only. even if it... explores and sees only the bar, we force Monty to explore for a minimum number of steps, it's more, it's more if you take the Yale dataset. And you're saying, okay, these are known hypotheses, we need to determine between these. That's additional information that you don't always get in the world, right? I think maybe it would be helpful to clarify, I feel like what you're discussing is more the policy. But the point... that I was originally trying to raise is that even with the policy that moves you to where the dot is going to be. Monty currently cannot distinguish the hypotheses, that's the issue.

I... Maintaining... maintaining this hypothesis, but negating... maintaining the one, even when we move out of the model, and negating the one that Fails to, that incorrectly predicts that it should perceive something. That should help with that situation. But. Now we... I think now we're getting down to some very important, implementation details, which... Or maybe not as conceptual as they sound. it seems like all the information is there, You. Yeah, we still need to update so that we do the first case that you showed, like, where, we need to eliminate the hypothesis if we're off the object, but the hypothesis thinks that we're still there. maybe, think of it this way. Let's say a learning module will behave the way I just described. It'll say, I'm gonna reach the first, I'm gonna reach a conclusion. When I have enough data that is completely consistent with one of my models, and there's no contrary evidence, I'm... I'm done. But... and so it wouldn't recognize... if the dot's too far away, and it didn't see the dot, then it wouldn't think about the eye. However, if we want to, impose some other things, this is what you should be looking for, or these are one of the 70 things you should be looking for, or, let's go through these in order, is it this, Then you have all this extra information, which every time you do that, the column will invoke a different model and say, okay, alright, I have a new model, I can make predictions based on that new model, and is the data consistent with it, and do I make further predictions? oh, it could be an I, let me see if it's an I. I didn't consider that because I didn't have any evidence for it, but you want me to look for it. Now I'm invoking the I model, I will go look for the dot. And so the learning module could itself be pretty dumb. It doesn't have to search through all the hypothesis spaces. It's just gonna search until it finds one that works, and then says, I'm finished. And maybe that is what happens, but then I agree with Vivian that then we almost need some sense of, model simplicity. Because what is special about the vertical bar? Why is that privileged, in some sense, over the eye? Because... Again, an existing Monty, they would both have equal evidence at that point.

Yeah, James had completely consistent with the model, which I think, to me, it means that basically how much we've covered of the model. we'll be adding evidence proportional to how far we are in exploring that model we have saved. So it's if we're at the end, and we know, we've covered 90 of the bar. then we'd be adding more evidence than if we've covered 70 or 80 of the eye, because there's a dot. Yeah, I don't think it... I don't like this idea. I don't like the idea that we have some sort of tally. It's more like... It doesn't even consider the eye once it's settled on the... on the bar. Pick one randomly. Do we have to be deterministic about it?

I could.

it could, if it... that's an interesting point. If it just said, okay, I'm gonna pick a hypothesis, and, And if it works, good, I'm done. I'm not gonna look any further.

so if I randomly pick the eye, then I look for the dot, and if it's there, I'm done. If it's not there, I eliminate that. And if I pick the bar, and I see all the bar information, I say, I'm done. I don't look for the eye, because I didn't, it could have been a random choice. To begin with. Yeah, could have been.

Again, just imagine you're looking through the straw, and when you're looking through the straw, you have no idea about anything else in the world other than what's in your little pew.

it doesn't look... you're just following this line around until you find out, until you recognize it, and, And there's a ton of other stuff that could be going on in the world, but you just can't... you can't deal with all of it, right? Just deal with what you got. Again, this is a very strained example, because we're doing the straw example. In reality, it wouldn't happen that way. In reality, it'd be voting, and I think voting would solve all these problems. But in the... in the... in the straw case, I... I think that's what it would do. You could... you could... you could pick a random hypothesis, a random... one and follow to its completion, a random model, and follow to its completion, and when it's... if it's all consistent and there's no alternate evidence, you're done. And so I wouldn't... I wouldn't think to look for the done. Yeah, I agree, you wouldn't, like... in reality, you don't spend, forever on every object trying to cover all of it until you move on to the next thing you want to do. You get a reasonable amount of evidence, and you move on, you have other things to do, kind of thing. But, Yeah, I feel like, besides how the policy would be and how efficient it would be and all that, I think the general issue still exists, and the general thing is still that we want to process the observation. if we... if we expected it to be an I, like Tristan wrote, if it was in a word, and we expected an I to be there, and we moved up, and the dot wasn't there. We still want to be able to deal with that case, and why wouldn't we be able to deal with that case? We would... why wouldn't we notice that the dot's not there, and then... then we... and now we have a problem? What... what... what... why wouldn't... what's the problem now? We don't see an expected feature. we would have to make the changes, that we discussed today, but maybe there's no controversy about that, and... Yeah, I think the. And I think we've settled on maybe a reasonable approach. I guess there are some edge cases, quite literally. We don't necessarily need to go... Through, threw them together, but maybe there's time. Yeah, so I guess just to recap, the necessary changes are in Monty to, first of all, send the observation to the learning module, which we currently don't do, and second, in the learning module. to process that observation in a way that if the model predicted to see something there, but it didn't, it gets negative evidence. If it didn't predict to see something there, because it's an out-of-model movement. To silence that hypothesis. And keep it for a short amount of time, so you can get back to it when you move. Back, the next step. Yeah. Does everyone agree with that? I just... I just watched you at the critical moment there. My phone decides just to switch my audio to something else.

I'm sorry. Can we back up a little bit? Oh, I think we all just agreed, so that's... Okay. Yeah, I was just kidding Good idea. Let's keep going. I have a question. This is your... Where did you lose it? About, I don't know, 10 seconds ago. This is your way of silencing me. You have abundance. This silence just sounds. Sorry. Who... was that Kristen who just spoke? I forget, I didn't hear. Bobby. Will. Oh, Robbie, Rami, I'm sorry, Robbie.

I forgot, I'll ask. But yeah. Do you want me to repeat something, Jeff? I think there's no harm in saying it again. Yeah, it's. You said it enough. Okay, so I'll just start at the beginning. So the two changes we would make to Monty is, one, to send the off object of the... so the... No feature observation to the learning module when the data's not there. And two, to process that observation, and the way we would process it is, if the model predicted to sense something there, like the model of the eye predicted there should be a dot. It doesn't get something, it gets negative evidence. If the model didn't predict anything, because it was an out-of-model movement, the model doesn't store any feature at that location. Then we silence that hypothesis and keep it for a short amount of time so we can get back to it if we move back onto the bar in the next step.

Yeah, it's saying... what if I had an answer to the problem, I've inferred something. And I just decided to go someplace else.

And test another hypothesis. I don't want to forget my first hypothesis, which is correct. It's oh, it looks like a bar, but you want me to see if it's an I. Let me go see if it's an eye. No, it's not an I, but don't forget, I was still. Yeah. bar. Yeah, I didn't get any evidence against it. a hysteresis sort of effect, right? Yeah, yeah, that seems... you'd have to do that. It's like you don't want to forget everything just because you... Once you get older, it feels like you do this sometimes, you're in the middle of something, and then, the door opens, you look over there, and you come back, and you go, oh, what were we talking about? I can't remember what we were talking about. Or Dory the Fish in Finding Nemo, right?

That's what she does. She gets distracted and has no idea what she was just thinking about. We don't want that to happen.

So yeah, so I think, actually, Vivian, I won't go through the, the literal edge case. I call it the literal edge case, because it's a scenario where if you're near the edge of an object, weird stuff happens. But basically, I think as long as we're doing this clamping of the, outer model movements, it's not an issue. I guess the only thing is, in our... Standard Monty. One of the main ways we eliminate hypotheses isn't... we don't call them out of model, but just when we have a location hypothesis that's far from The existing ones, it gets negative evidence.

So I guess... Say that again? But those are when they predict to send something, Is it only then? Yeah. Okay. in those cases, they would predict to sense nothing, because they're far away from the model, but you do get an input. And so there's... this is a prediction error.

there's... you have to be careful, too, because if you go beyond your current hypothesis and into some other space, just randomly, you might find something there. That's what I mean, yeah, because in... often there will be something there. I know up until now, we've been talking about empty space, but... but actually. In a lot of environments, there's just something there. I think the key things here is, under hypothesis A, if, if a column is observing outside of Hypothesis A, whether it moved there or it's just a different part of the retina. we should ignore its input. Just ignore it, even if it's some input. It does... if I'm outside of hypothesis A, it's irrelevant over there. It could be another object, it could be something else in the world. If, I have a... if my object... if object A makes a prediction about something at some place. and I go there, it should be exactly the right prediction. It should be that feature that should be there that I'm observing, and that would... that would be consistent. But if it was a different feature. Then there would be negative evidence. Yeah.

Then it's a more radical thing, where we basically say any kind of out-of-model movement, any movement beyond... nowhere near store locations, we just I think clamp is maybe the wrong word for the evidence. I think what we want is it to slowly decay as long as we're out of The, model. like in the EMA neurons that, Rami was working on. I'm still... I'm still thinking about, when a bunch of columns are voting. a lot of them are going to be off the model at any point in time. But if they're voting, they all have the same hypothesis maintained in Layer 1. They, they're all... the voting neurons are still voting, just because I'm... it's it's it's like I'm a column, I know we're all looking for... we think it's object A, because it's coming in on my Layer 1 neuron, so that's... that's the voting hypothesis, but I'm out of object, I'm, I'm off someplace else, so my input doesn't really matter at this point in time. But I still know what everyone thinks it is. And if I become within object, if for some reason I move and now I'm within object, now I can make a prediction.

Remember, most... in a real scenario, most of the columns will be moving around together, and most of them will be off the object. At any point in time. And, the hypothesis, I think, would be maintained by the voting neurons.

Yeah, so that could help stabilize it. I think that would stabilize it. That would be... that is persistent. Yeah, maybe with the last... the final time that's remaining, I'll just talk a bit more about this... this kind of color filling in thing, and... I'll just... so I think, yeah, I think we have an action plan, I think we have something we can implement. we don't need to change the policies, agree, I think we actually agree on that, but I think we also agree we need to make some changes, some minimal changes, to... For Monty to work a bit better. But the question is, can we do more?

good out-of-model, movements, which, provide positive evidence. does this mean storing some concept of kind of empty space? but what if you move on to another object? Of course, can't then predict something in that case. And just to yeah. Try and motivate this. imagine how you represent an empty room. that's very different from an unexplored room. You really do seem to have some concept of this, emptiness when it comes to, motor planning and things like that. But why is it... why is it... isn't empty just, at this point in the reference frame, there's no... no object expected?

But in the current implementation... Yeah, but... The current thinking is equivalent to, you have never looked at that location.

Fine. But maybe I don't know the difference.

in this case, you have looked there, and you know nothing is there, versus you don't know what's there, because you're not storing anything at that location. Or maybe another example is, the concept that an object is solid, that this space, beyond the object is empty, but if I was trying to predict what I would feel inside the object, I know there's some solidity to it. Huh. I don't see this as a very... I don't look at... I'm just not jiving with this particular problem here. Maybe. Maybe another example is just, what we... what we predict when we're looking at 2D objects, Okay, can I just go back to the room, for example? Yeah. the room is not a cube. In the room is a set of walls, and you've explored those walls, and there's space outside the walls, and there's space inside the walls, and I'm not sure there's much of a difference between those two, It's... it's just a set of... it's not like it's a cube, and there's the inside, and the... it's just there's these edges and these walls, and they go around, and And so I'm not sure the inside treated any differently than the outside. Even if I haven't explored all the reasons around the outside, it's not important. I haven't explored all the places on the inside, it's not that important. I've just, as long as I've explored enough of the walls. Now, obviously, in a situation like this, we look at the entire room, and our retina can quickly scan over all parts of it. And we... and we don't see anything that's inconsistent with a wall, and a ceiling, Nothing is blocking those things. essentially, the objects in the room is... actually, maybe what we just discussed solves this, because objects in the room are like the dot on the eye. So in this case, there are no objects in the room. the hypothesis for crowded... crowded room is... is going to receive negative evidence. The hypothesis for empty room is going to be stable and basically wait just through... It gets complicated in a situation like this with vision looking at the whole room, because in some sense, the dot never occludes the bar. But a chair in the room might occlude the window. And, so there... it's hard to come up with something in the room that doesn't interact with any other part... that doesn't occlude any other part of the room, right? So it would be difficult as you... and we're looking at a distance, we're not feeling our finger along the way. If I was a blind person, I was in the room, I was walking along feeling the finger, and I went all the way around it, yeah, I would have no idea there's a chair in the middle of the room. But, from vision point of view, it's very hard to miss it, because it would occlude something. I can't... I can't observe the wall without seeing there's a chair edge. So I'd have to deal with it. But if I'm just touching it, then the inside and the outside of the room are all the same. Yeah. I don't need to... I guess I'm saying there's no reason to encode there's nothing there in the room, just like you don't have to encode all the things that are not outside the room. I... The room is the wall. If I'm gonna... I just want to skip ahead in the last... Bye. By Vance, just to... Because it ties into what we started with, which was, this kind of filling in, because... And I think we've talked about this at some point before.

We didn't solve the filling-in problem at all, right? We didn't really. But whether, we... if morphological features have a concept of a sort of directional prediction. Yes.

The prediction or a directional... an orientation? an orientation, but that's also providing a prediction about what you would sense. Oh, I would... no, the orientation says I would... it's part of the... it's part of the feature, so if I go to there, I expect that... maybe... maybe it'll be clear in the next slide, but just imagine that... we want something that, basically, a representation that'll help us predict, okay, this is empty space, or... or in the other direction, oh, I'm expecting white, and I'm expecting white anywhere. I don't want to store white at all these points. So... it's going to be a particular color, a particular texture, empty space, solid space.

And it feels with a fairly simple and biologically plausible kind of spreading function. When you sense a point, this could spread until you've I'll show it an image, it's clear. okay. You... you're sensing here, you want to know whether that's empty space or solid space. and basically, this kind of ring goes out, and then the first thing it touches is this point. And if each point has oh, in this direction, I've actually learned, I've stored. when I was sensing it, there was empty space beyond it, and I'm presumably solid below. So it's actually stored. So this location, I'm going to assume, is empty. And this kind of... hopefully it's clear how this kind of relates to the color filling in thing, that, if this was a red edge. red on this side and blue on this side, and I'm sensing here, basically, I spread out, and I see, oh, okay, in my stored model, the nearest thing is, this edge that's red on this side, so I'm gonna assume I'm seeing red.

Can I just put back a little bit on this? I think the general ideas seem pretty good, but... but... again, I don't think we should be storing anything off the object and saying there's nothing there. That's not part of that. anything off the object, it's stored at... at the Along the boundaries. I guess I would... all I would say is, It seems more like the following, because obviously the golf ball is the object, and the stuff in the background could be trees, and dogs, mugs. I guess in this case, I'm assuming a finger is moving here, so it's hovering above... Okay, let me... hold on, I don't even know what to make of that. Can we stick with it? We got a picture here, can I think about vision for the moment? Sure, okay, if we're saying it's color, if we're saying color, and then... I've learned that golf balls, yeah, okay. imagine you just have a circle. it's not a line, it's a solid circle, right? we could do a line circle, that would just be a bunch of features at locations around the... and now you get... we're done. Now we're saying, oh, there's a... there's a... there's a... the edge of the line is not really a line, it's a color boundary. First question I ask is, how do we know that the white is... is inside the color boundary? why is the white part of the object? How do we know that... where the object is and where the world is? I don't know how to answer that question, but let's assume you have some idea of closure here. you've got... so how do you... I think what's right is to say, at the boundary. You have this, transition where you have a color boundary, and the idea that you just assume... you just assume that all things in the other direction are continuous at that same color. Exactly. But I don'. And you haven't come across any other learn points. Obviously, if you had another learn point here, then you would rely on that one to tell you what to expect. But I don't... in this case, I don't think I'm going to predict anything about the golf ball, because I don't know what... on the outside of the golf ball, because I don't know what's there. it could be grass, it could be, the carpet, who knows? That's... yeah. Yeah, I think the... Just the proposal was maybe this would help with the empty thing, but maybe it's more appropriate for just doing, color and texture. it's an interesting question. How does it know that, at this point on the object, that In one direction is the world, and the other direction is the golf ball.

Yeah, you could do something similar for... I guess one thing... one thing is to say, you have this... imagine your edge... your edge is now a color edge, right? White on one side, and something else on the other side, and everywhere on the object, you have the same white transition, and there's... imagine there's no other... any features inside. In this case, you see the dimples. And you see a bunch of stuff, but imagine it was just a pure white disc. Yep. then essentially the idea is that you just keep going until you get to the next edge, and you assume it's all the same. You just... you just keep going across.

What I'm struggling with This is almost, a slightly metaphysical question. Is there a neural representation for the white on the middle of the golf ball? Is there... are there actual neurons that are firing, representing that white in the middle of the golf ball? it's a perception that it's there, but there's a lot of things we perceive that are not really correlated directly to neural firing. It's an axon could put a spike and you feel it's pain, another axon is a spike and it's a color transition. yeah, it feels like the location representation that's active would be here, this location in the reference frame. But the feature prediction that's active It's deferring to... And so, in that sense, kind of white neurons are active. I don't... but are they? I don't know, because is it... is there a feature there?

we have these color blobs in the cortex, and, maybe color in these sort of things are really some special mechanism for doing this.

You know what I'm saying?

I think the general idea, as you just said, is probably right. You've got... you have to absorb the feature from the nearest boundary, or from the nearest point where you have a... the nearest feature, from some other near feature. It's Because there's no feature here But why is, how is it we perceive that? I have no idea. I don't know what the hell that means. But yeah, what's nice about this is it'd be... it'd be relatively straightforward to implement in Monty. I think even, Vivian, at a retreat a while ago, we discussed an idea around... similar to this with, Yeah, as a way to basically have sparser models. What are we doing? For surfaces, we could, just store a few nodes, and then you rely for all features, I'm just your, whatever your... Yeah.

This is something I've been thinking about. Oh, sorry, just quickly, so we have a bit of that already. One is in our distance measure, we look further for neighbors along the surface, like in direction of the... curvatures, then we look away from the surface, so in the direction of the point normal. We don't look that far in that direction when we look at nearest neighbors. And the other one is the direction in which the surface normal points tells you in which direction the space should be, and in which direction the object should be. we're just assuming that... by basing it on the curvature, the... Yeah, surface. The surface normal. Alright. Surf thermal always points out of the surface. And one of the issues I tried to deal with pretty early on in the project is thin objects. I guess if, at the most extreme, think about a sheet of paper, and one side of it is red, and the other side of it is blue, and if you're sensing a blue point. you know immediately which side of the paper you're on, but in Monty, if you're just looking at what's the nearest point in the model, the red points are very close to the blue points. And what we do in Monty is, I think at least we still do this, is... To look at the surface normal, and only look at the points that have surface normals that are in the same direction as the one we're currently sensing, so we would just not consider the red ones, because their surface normals point opposite of what we're sensing.

Yeah.

So that's about as much of concept of surface as Monty currently has. Yeah, it's interesting, it's I've asked myself something, a question like this. Do you perceive the color of the surface of an object as its neural representations, or does... do you have to go and ask? It's I could ask the system. I could say, what is the color at this point? And then say, okay, let me calculate it. I calculate by looking out and finding the transition points, and then extrapolating from there. That's a different... that's a different system than one that is representing red everywhere. it'd be more like, okay, what is... I can ask the question, what is the color here? And then it... and then you can answer it. I'm looking around the room here, and it's not clear which of those is the right one. It's it's hard to diff... it's hard to separate the fact that I'm trying to ask what is the color from what do I perceive if I didn't ask what the color is. Yeah, because as soon as you attend to something, it's like you're asking. It's what do you perceive without attention? And you can't even do it by trying to use memory, because it's okay, now you're attending to a memory. It reminds me of, I grew up in the age of black and white TV. And, when I was a kid, we had a black-only television. And I was... and then we got a color television, which we had both. We had a color television and black one television. I'm always struck by the fact that when you watch the black and white television, you have absolutely no idea that anything's different. Once you're in it and you're watching it, you don't... you never ask yourself, where's the color? What color was that? It just seems totally normal. There's absolutely nothing you didn't understand. It's all completely normal. Of course, we have to do this when we see things in the dark. We lose all of our color sensations. if it's dim... dim light, you lose color sensation. We're not even aware of it. We just don't know that we... color's no longer there.

So it makes me wonder, oh, is color something I can ask about and then give you the answer to, as opposed to something I perceive? I think it's definitely perceived, because we have blind spots. We're constantly missing color at some spatial thing, but... But my point is, what I'm saying, PoJ, is that there's maybe no neural representation at all. In the middle of a red object, until you ask the question, what is the cloud in the middle? Then you can calculate it, then you can go, oh, okay, what's the nearest thing, and bring it over, as opposed to it's a neural representation there all the time. Yep. I feel like the vision in dark is a good example, if I'm looking for a t-shirt in the dark or whatever, initially, I'm just like, yeah, my vision's normal, or whatever. And then, then it's hang on, these all... I can't actually pick out the green shirt. It's impossible. You're right, in dim light, you lose... Or, I think I pick up the green shirt. it's consistent with my expectation. But then I realized, actually, I can't tell. So those examples indicate... would suggest, they don't prove it, but they suggest that Color is just this extra piece that you just don't need in color transitions we get, right? Those are... those are represented on the edges of objects. Those are there. But... but, Surface color, or large areas of monotone illumination. may not be represented, and they... they're just, you can ask them, say, oh yeah, they're... they're... I can tell you what it is, I can ask the question, get the answer. I don't know. What would happen if you would close your eyes, and then you put on, glasses that are only one color, there's some kind of light source under your eyes, and then you open your eyes. There are no edges, you just see one color. Would you know what color it is?

Probably... maybe related to Tristan's blinking thing, maybe from the transition from no light. when I put on, red-tinted glasses, everything looks red, and the only way that I could infer that it was a different color was because the color of the object is interacting with the redness, so I know that it's, a darker red, and, therefore... I do know that if you were... if you... let's say, if you could only see the world in one color, let's say you're watching a black and white TV, but it was blue and... it was blue and white, right? even then, no, you have to get... I do know, as I said earlier, if you're just in a saturated color, all you can see is a saturated color, your sense of color goes away. you lose... you just don't know... But, there's, lithium lights, I think, or some sort of thing like that, where it's pure yellow, and it's, it's a single frequency as well. Yeah. And, you can see it perfectly fine, but basically the only color is yellow. And you can... I've spent a long time in a room with that, and... Are you in the... your perception of yellow goes away. But you have a set... you know it's yellow because you can still look at something else that's not lit by that light. like a pistol.

things are either, I guess a sh... shade of gray, or whatever, or yellow. All these... all the light is that... that sodium. Yes. Interesting. Do you... do you... do you... I wonder, do you lose the perception of yellow at some point?

because it's this Icelandic artist, he's famous for doing it, but I think they also used it in, before green screens, they could do some clever, filtering of... No, I'm talking about, I'm talking about, you put me in a room. Where there's no other light except for sodium light. That's the only thing that generates light. Exactly, but that's exactly what this Icelandic guy does. He fills rooms at. So do I... but once I... once... and I can't see anything else. would I always recommend that for people, right? people, and so let me... maybe... clear if I send a screenshot of what it looks. They're lit by... they're lit by the lithium light. Oh, but the lithium light could change color when it reflects off of something. Yeah, It, it essentially looks like this. So you see, shades of yellow that border on black, I guess I was wondering, if I spend enough time in this room, would I... would the light... the yellowness disappear? Would I... would I no longer know. I at least didn't experience that, and... Yeah. I stayed in for a while. Yeah, This one was really cool. I'm sad I didn't go to this one, but... What was it like when you left? When you left the room? I don't... honestly, I don't remember anything unusual. say, for example, when you're running on a treadmill and you step off. And you have that sense of, moving forward, or you step onto an escalator that's broken, and you have that, jarring... I don't remember anything, similar in terms of color perception. Tristan. But maybe I wasn't paying enough attention. So I have to read... Tristan, you're always chatting as opposed to speaking, but go ahead. He said, do we have detectors or cones? You're right, we do have... the cones do detect specific colors. Or... I guess what happens in the dim light is the cones don't work at all. That's the problem. That's why you lose color. When the dim light, the cones, they just don't work. And I thought the reason you lose... The color perception, if you see only one color a lot, is that the cones fatigue, so that's why you see the after images, after you look at, a very bright red, you look at a white wall. yeah. Alright, how about... maybe it's a little bit off-topic, but... but, I think we could... we have a hypothesis, perhaps, that... at least I've thrown out a hypothesis that the color across the uniform surface of an object is not actually represented, at the locations, but if you go to the locations and say, what is the color here, the system is a way of extrapolating at that moment from From boundaries.

Something like that. But yeah, I think, yeah. there would be complications, because... because then it's okay, any point in a model is now considered making... to be making a prediction. no, I'm saying it's not. It's not part of the model. My point is that the interior of a... there's no representation of the inside of a blue circle. It's just, if you go there, you can... I guess maybe what I mean is, Yeah, maybe with color it's fine, actually. It's more about the off-surface thing. It brings in that whole off-object thing again. Okay, maybe... I was just trying to... I thought you brought back color, and I thought we could just Yeah, okay, yeah. Color is definitely simpler, I don't know. did... Yeah, should I do a summary? Do we feel like we've reached any kind of... I think so. I think we have an action plan. Yeah, we have action plan for, off-object, observations. I think, you've, yeah, made some good points about, policies and how we think about that, in terms of... Exploring hypothesis spaces. And then, yeah, we talked briefly about this kind of color filling in, and then also at the start. I just think that you had a specific problem you were trying to address, and the question is, do you feel like we've made enough progress. Yeah, I think... and I think when Vivian and I were talking about it yesterday, we already thought that, okay, maybe we now understand it. I think the thing is, we've revisited so many times that it felt like. until it was discussed more broadly, and also with you, I didn't feel confident saying, okay, we have... but now I do feel confident that we can update this task. Okay. And so that the person in the community who's interested in working on it, Carter, he can hopefully get. Started. Great, I'm worried now that problem you brought up towards the end is a problem, where now if... how do we deal with if we... if the hypothesis is too far out of the reference frame that... Where we currently just delete it, but... Yeah. Probably. Talk about that one offline. I think... I think it's solvable, because it'll just mean... I think it'll just mean... anyways. Yeah, worst case, we have the fallback of just saying. a human won't go there, and so Monty doesn't have to go there.