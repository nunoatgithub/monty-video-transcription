Okay, so yeah, everything I'll present today is what I've been working on for the past two weeks. So it's still pretty fresh and there are not like complete final evaluation runs or long evaluation runs done yet. It's more like conceptually what's new, what's happening in the code and a few preliminary first results, but it's still a lot of content. yeah, a lot. Of new things have been added. So I hope it's not going to be too overwhelming. I'm, I tried to make some visualizations and stuff to explain everything. But yeah, let me know if you have any questions. They're two topics that I have today. One is evidence based learning module. that are implemented. It's basically, it's similar to the previous learning module we've been using, only now, only this one doesn't have a list of possible objects and poses that gets narrowed down over time. Instead, it has a list of hypotheses, so objects and poses, and each of the hypotheses gets its evidence updated at every step. So you can have more or less for a hypothesis instead of just eliminating it completely. And then the second topic for today is, voting, for which I already shared the report from Weights and Biases, but I made a few more. visualizations to try and explain it a little bit better and just, yeah, if anyone has some more questions there and, okay. Just a quick note on the weights and biases thing. I think there's a way to make the report publicly available or, have a public URL because some of us don't have actual accounts. Yeah, I wasn't sure how to do that. I recorded it as PDF and then sent in Slack, but Yeah, the PDF version is not as good, unfortunately. Yeah, it inserted some blank pages and stuff. Yeah, but I think Louis can tell you there's a particular way you can get a URL that's only, you can only send to specific people, but it'll be, you don't need. Okay, I don't know. Yeah, ask him about that. Yeah, do that next time.

Okay, yeah, then I'll get started on the. on the evidence based learning module. so first a bit, generally, on how I define an object at the moment and how it is represented in the learning module. That kind of applies to also the other learning modules, that are based on graphs. So basically, each point in the graph is defined by a 3D location and three pose vectors. and the three pose vectors define the rotation at that point. And those three pose vectors are three orthonormal vectors, so they're all orthogonal to each other and they have unit length. The first one is the point normal, which is orthogonal to the tangential plane of where the sensor is, and then the other two are the two curved principal curvature directions, which is the maximum and minimum curvature. at that point, and yeah, this, those, three vectors define the pose of that point or that feature. just a few more things. principal curvature one. Can I ask a question about that? I'm confused already. Sorry. Yeah. Let's go back to the previous slide.

The normal vector is always normal, right? So how does it add anything? it's always going to be like, I'm confused. Is that, relative to the plane? Or is that relative to a global coordinate system?

It's within the object models reference frame. And it tells you the orient, like the orientation of that point. Here is an, as an example, if you look at, at all the blue, vectors here, those are the point normals, and they always point away from the surface. Got it, but then to represent that, it's not just a scale of a value then, right? Yeah, no, the, length of it is one, so it's basically a location on a unit circle, and it can, the location of it is saying which direction does the point normal point?

It says two numbers. Yeah. I'm just confused. it's three, it's three numbers because it's in 3D. It's x, y, z on a unit circle. So it's always, maybe the question is which reference frame and it's, it can be something to attach to the object. I'm just missing a very basic idea here, which is. I understand the concept pretty clearly, but it seems if I have this, if this point normal is always tangential to the plane, I could represent the point normal, in, in three dimensions. I could be pointing in any direction, right? Yeah. then, but then the other two, you say three pose vectors, the other two vectors also representing in three dimensions. are they relative to the point normal? Yeah, they are orthogonal to the point normal. okay, so we have a unit sphere. All the points on the sphere are distance one from the center and it's a three dimensional sphere like this. Yeah, and we have the point normal in there, it can find any, it can find anywhere on the circle. So it can be an XYZ location here or here, that's just the direction of it. And then that's the point normal. But if we only have that one, it only specified, we can only say, I understand, what you're trying to represent. I, just trying to say what is actual the representation your storing. It seems like the point normal requires what two dimensions.

and then the other two are relative to the point normal. Is that right? Yeah. Or they, should mention, how many numbers you're storing here for this pose vector? So each pose vector is three numbers x, y, z, and we have three pose vectors, so it's overall nine, nine numbers. It's not like the directions just constrain the orientation in the tangential plane, because you could like, if you have only the point normal, it can still be like rotated around itself in any direction. With the curvature directions, we constrain that. Yeah, there's three independent numbers.

And they're all relative to the object's orientation pose. Is that right? I'm confused now. Yeah, they are all on a, unit circle, and then they are associated with one location on the circle. What was confusing me is I thought, oh, you were saying three one dimensional numbers, but no, it's not. It's nine numbers. Yeah, no, it's just the length of them is one, but they are in three on a three dimensional unit circle. So they have XYZ coordinates. All right. But if it's on the unit circle, in principle, you only need two numbers, right? You don't need to have three numbers. Yes. The third one can be derived from the first two just by making it orthonormal. Yeah. I guess that was the part that was confusing me too. Can we think of this unit circle as being centered, fixed on the object somewhere? so it's in the object's reference frame, and so this point is, each point on that, on the mug there, will have some different, point on this unit circle. Yeah, so this one would be associated with one point on this model, basically. I think I got it here. I had this, I think about 3D location, I think of three numbers. but I think, then you say, and then three, the 3D location, three pose vectors, I was thinking, oh, those are unit numbers, six, three numbers too, but it's nine numbers. all right, I got it. Yeah. And, you need different numbers because you could have different, flatnesses of the curve. I got it. I got it. I got it. Sorry, just in terms of the reference frame this is in, I appreciate, in the model, it's going to represent, represent it in the object's reference frame, but the input from the sensory module, presumably that's in the environment's, global reference frame, or?

Sensor sensors from the sensor. It's like the, it, would be the coverture direction point. Normal relative to the sensors. Relative to the sensor. Okay. So we'll have to convert that. Yeah. Okay. So I, for the distraction, I just didn't know before we got too far, I wanted to know exactly what we're storing. Yeah, it's important.

Okay. yeah, so Principal Coture one is always larger than principal Coture two. and they are orthonormal to each other, so all orthogonal to each other, so they span up this reference frame. And here I just visualized this on a mug, so the color of the, it's a bit difficult to see everything, but the color of the dots shows the principal curvature magnitude, and then the, those vectors going out kind of show the direction that the curvature is pointing, and this is all, three together, it looks a bit like a mess, but the blue vectors are the point normals, they always point away from the surface, and then the red and the yellow ones span up the, the curvature directions, orthogonal to that. and we basically store these three vectors for every point on this model, and we store also the location of all the points.

and then one more detail is that. If principal curvature one and two have about approximately the same magnitude so for example if we are on a flat surface or on a round ball, then the curvature directions are meaningless. so they could be pointing in any direction. So in these cases, we disregard them and only use the point normal.

But their values are important, even if their directions are meaningless. yeah, the magnitude is used, but the directions are not used to specify the pose of these features. Yeah.

Okay, so now, I'm getting to the evidence based model and how it works. are there any more questions to how the morphology is defined?

Okay, so we have the first step and then we have all the following steps. The first step is a bit unique in the sense that it's being used to initialize the hypothesis space. So at the first step we sense a point in space, And then for every point that we have stored in the model, we determine what would be possible locations if we would be at this location. for example, if I sense a point normal, in space, then I can say, okay, if I would be at the top of this can, then the can would be at rotation zero. But if I would be on the side of the can, it would be rotated by 90 degrees. or by negative 90 degrees. If I would be on the bottom of the can, it would be rotated by 100 degrees. and we do that for, all of the objects that are possible. That's a large, space then. Yeah. but it's the, smaller space. then if we would test all possible rotations for all, the points, all the locations in the model, it's still like an informed hypothesis space. And when you say possible rotations, is there some fixed number of rotations that you're testing? yeah, if I have, the point normal and the curvature directions, then that's basically two, two rotations per location, it's. zero and 180 degree flip because the curvature directions could point either way. if I don't, if I can't use the curvature direction because I'm, for example, on a flat surface and I can only use the point normal, then I have to sample kind of some possible poses along of, along the remaining axis. and that's like a hyperparameter where we can say how detailed we want to test there. Okay, so there's some, if some object was, rotated at, 12. 3 degrees or something in between like that, it would snap to the nearest one that we're checking.

Yeah, in this case, if, we do have the curvature direction, then we have, we can initialize it very precisely. Oh, okay.

Is there, is there any sort of like simple way to specify a policy at the moment where you, ignore kind of input features that don't have a full kind of 3D defined pose and just continue until you yeah. Okay, yeah. Yeah, that's a to do that I have in the code. we'll have to deal with the case where an object only has that, we just have a ball, then we cannot write directions anywhere, but that's a special case of the special case, But yeah, all other objects, it would probably make sense to first move to a place where we have curvature directions and then start initializing your hypothesis space from there. And even though it might take more steps, the kind of wall clock time might be much quicker because you're focusing just on like informative. Yeah. Yeah. And that's definitely something you see right now. If we start on a flat surface, like on the top of the can, all the following steps will take a lot longer because our hypothesis space is a lot larger. So we couldn't constrain it with the curvature direction. Thanks.

Any, other questions so far?

so now next step, all following steps will be testing hypotheses. So at the first step we initialize possible locations and rotations. Now we use the incoming sense features and displacements to test these hypotheses. Okay, so we loop over all possible hypotheses. first we calculate a test location by taking the location of the hypothesis. the rotation of the hypothesis, applied to the displacement that we just sensed. So we sense the displacement, we rotate it by how we think the object might be rotated, and add that to the location we think we were at before the displacement. So that's the kind of a search location. And then we also apply our hypothesized rotation to the sense pose features, so to the point normal and curvature directions. Okay, so this is our query, and then we go to our model, and we find the nearest point stored in the model to the query location in a given radius. So for instance, this radius 0. 1, 0. 01, which I am using in most experiments at the moment. So I have a query location, and then I look within this radius, which points do I have stored in the model. And then for these nearest points, I calculate The error and from that an evidence for the hypothesis. So if I have any points in this radius, it can happen that I don't have any points, if the hypothesis is just absolutely off.

I calculate the angle between the pose features stored at these points and the sensed pose features. So what's the angle between the point normals that I sensed and point normals that are stored here in my model? That's the pose error. and I add this to my hypothesis evidence. and then that's the evidence for the object morphology. That error is, not just the point normal vector, it's all three vectors, right? Yeah, it's all three vectors except if the curvature directions are not specified. Yeah, okay. And then optionally, this is the case now where features and morphology are separated a bit more, we can add additional evidence for features that match. So I can also look at all the points in this radius. and check if they match with the features that I'm sensing, like color, for example, or the magnitude of the curvature. And both of these things are weighted by distance from the query location. So if it, if a point further away matches, it's, it weights less. I'm sorry, you just said that. You said the curvature. I thought the curvature was part of the, a part of the point. Normal three vectors there. curvature direction, but curvature magnitude is just the feature itself. Oh, power curved. So those three vectors earlier didn't include the curvature magnitude. It was just the I see, I didn't understand that. Yeah, just the direction. Just the direction. So any, if I had a saddle. It would, all saddles would be equivalent. It doesn't matter how severe they are. Yeah. And you're calling that, you're calling that severity of the saddle more of a feature. Yeah, because it's independent of the rotation Yeah. When you, can you, on the first part of your hypothesis line there, the way you were describing it, made it sound like you're rotating the object as opposed to moving the sensor. Is that? I'm rotating the displacement that is being sensed. So it's, it could be the finger moving or it could be the object moving, which is a finger. You don't really, it's not the same thing either way.

yes. What are you thinking about? At the moment, it's the, sensor moving. Okay, all So the sensor moved and that displacement is what I'm rotating. Okay. It's just the language you said earlier confused me a little bit. Okay, got it. And what is. The evidence, is that a scalar value? Yeah, it's a float, so usually it's, so basically the pose error can be, up to pi, in degrees, pi halves, and then yeah, in radians, and then we can, we have a tolerance value which is, the naming convention is because I'm keeping all the parameters that I can from the previous learning module, but this kind of tells us when the evidence is positive or negative. for instance, we can say if the pose error is below 30 degrees, we add evidence. Otherwise, we subtract evidence because the pose error is just too much of the evidence that is being added here is negative. So I guess there's a whole bunch of hyperparameters in how you accumulate this evidence. Would a better term be bias instead of tolerance, since you're biasing the evidence a little bit? yeah. yeah, actually, let me get to the next slide. So here's everything that goes into the evidence updates. Sorry, a quick question about the previous slide. I request you to. Yeah, I'm sorry to hold you up. Just could you explain just one more time would get featured match on the very last line, the optional. And what's that doing? Yeah, so this one. Okay. How I'm imagining it is it will calculate like a distance weighted error between the sense features. and the features stored at those points in the radius. At the moment it's just a binary thing, so it checks for every point in the radius, do the features match or not, using the same tolerance values as we were using with the previous learning module, and then weighing it by the distance. So the interpretation, check me on this, is that, if there are a lot of points in this radius that have similar features to what we're sensing, then we think it's more likely that it's this object. Yeah, so at the moment this is, if all the points in this radius have matching features, so all of these points have the color that I'm sensing right now, or all the points have the curvature that I'm sensing right now, then this will be one. If some of the points on the outskirts of the radius don't match, it will subtract a little bit from, one. If the points at the center of the radius don't match, it will be a pretty low number. Gotcha. Yeah, so it's pretty interesting because there might also be just as a side note, we don't have to get into it, but like different objects are basically going to have different, speeds with which they change. So like some, you could imagine adversarial objects that have like different colors at every single point nearby or like they're extremely wiggly. So like this, kind of evidence update here might be object dependent, but that's a nuance that I'm sure we can worry about later. Yeah, that's yeah, definitely a complication we should keep in mind.

Regarding the pose and the feature match, how do you, since there are different scales, how do you balance the two? Yeah, at the moment that's also a hyperparameter and I'm not sure yet what the best values are. So there is a weight parameter for the feature, so how strongly do you want to weight features into the evidence? and at the moment it's. set at 0. 5 by default, so this value will be between 0 and 1, and then it will be multiplied by 0. 5, and then the pose error can be between, I don't want to say anything wrong, I think plus pi halves and minus pi halves. But, I've been playing around with a lot of different parameters there so I'll go back but yeah, those are like all things I'm still testing and playing around with. Okay, yeah, Jeff has a question. Yeah, I just want to make sure. The one time we were talking about explicitly storing displacements. and I'm it sounds like in this model right now, you're not doing that. You're just calculating on the fly. Is that correct? That's correct. yeah. Okay. And then I'm a little surprised. I don't quite understand why the curvature is considered a feature. It's it seems to me what we're trying to separate out the models. Two things. It's morphology and the. And the features that might be associated like a specific sensual modality feature like color. And, it seems to me like the curvature is part of the morphology.

Am I wrong about that? Yeah, that's true. So that's a case where the categorization is looking at two different factors. So how we've been talking about it is Morphology is like everything, yeah, related to morphology, which would include the magnitude of the curvature. Here I was defining, like the, I actually call it the pose features, as features that depend on the orientation of the object. So if the object rotates, these features will be different at the same location. Okay, is that an important distinction for you, or is that just something you did right now? it seems, ultimately, it doesn't seem quite right to me. Yeah, in the code sense, it's important because I have to apply the rotation from the hypothesis to these features. So I have to rotate the point normal and curvature directions. which I don't have to do with the curvature magnitude or color or something. But it, yeah, I definitely agree we should still make a distinction between the curvature magnitude and things like color or texture. So if you have, enough just point normals, just the one vector, that defines a morphology too, right? If you have enough of them. I don't need the curvature component. Is that right?

Yeah, so this works completely without the last line. It still recommends objects. Okay, I get it. Is it fair to just have kind of structural features and non structural features, and so it's still features at points in space, but, Yeah, it's still. Yeah, I don't know. That's at least how I've been thinking. Yeah. one way I've been thinking about this is that across modalities, like between touch and vision or so on. And even within vision, there are different sort of some modules get color. Some don't. The morphology component is common.

I, can learn the morphology of it to touch and recognize the vision and vice versa. But the actual sense features may not be able to do that. I can't feel the color or I can't see the texture of the temperature. so the morphology is this thing that we can vote on across modalities. And so I'm trying to, in my mind, I'm trying to make a clean separation between things that define morphology and things that are on top of it, additional components. And, so I think I understand here, you could recognize the object just using the point normals. You've decided to put the curvature as a feature. It's not even necessary, perhaps, to do that, right? you said you could recognize the object without it, right?

so maybe, in my mind, I might want to come back in the future and rethink this through a little bit, but I think I got my questions answered. Yeah, so basically the reason I made the separation there is because everything I have in the features here are things that I do not have to rotate depending on what I think the object's rotation is. and I agree it's not really very nuanced yet and, yeah, it would be nice to add some more options where we can weigh different features more or less so we can say we are, we mostly want to look at the curvature and not really at the color or not at all. There's an option to not look at color at all for example one learning module may know about color. Another may only know about curvature. And they can still vote and communicate. Yeah, okay, I'm good. That, works, but. Yeah, I'm good, And what are the set of hypotheses in the beginning?

It's locations and, poses at that location, the rotations at those locations. Okay, so in the beginning it's every possible hypothesis. Yeah, it's basically all the locations stored in the model, and then for each location, a set of possible rotations. Okay, so that's a pretty large outer loop in the beginning then? Yeah. Yeah, okay. And there's even an extra loop over possible objects too, is that right? yeah. Yeah, exactly. And then, yeah, I'll get into the more details in a bit, but. Since we have an evidence based model here, we don't need to test every hypothesis at every step. There's a hyperparameter to say, okay, I only want to test the most likely hypotheses or only hypotheses above, that have evidence above a certain threshold.

It's yeah, it seems like the evidence should narrow down the hypotheses, even after the first touch, right? Yeah, so but as opposed to the old learning module, you can still come back to a hypothesis that you previously had. Even though I may have like a bunch of very likely hypotheses, if I then get a lot of negative evidence for these, I can still come back to a less likely hypothesis that I previously had. once, yeah, if that makes sense. Would it help to put like, have the different features have different types of features with different weights. Like you said, like the ones where the curvature is the same in both directions, that would necessarily be a very low weight. But if you had a very sharp edge where you have a very high curvature in one direction and low in the other direction, that would actually be more of a unique feature that you could assign a higher weight to. and you could basically, weight those points more heavily than the ones that are, have less or more ambiguous features. Yeah. So yeah, they are those, this is one of those nuts and bolts that I'm still trying to figure out. And they are like so many moving pieces right now and so many parameters I'm trying to figure out how to best set them. But, yeah, that's definitely one of them.

okay. I'll go on so I can cover everything. So just interesting to brainstorm. Sorry. It's just, I don't know if you're already doing this, but. It would be interesting to look at kind of the Hough transform approach of how hypotheses are dealt with. It flips the loop a little bit. You never have the four agent hypotheses. Instead, the evidence tells you which hypotheses are likely, and at any point in time, the distribution of our hypotheses.

So maybe I didn't understand that, but it sounds similar to what we were thinking about using displacements for, It's basically what the columns paper did too, is that, you never would, Yeah, and that's what capsules does as well. I was thinking here, one of the ways you can prove this is that We're working on that idea that they're also in addition that you just you can save certain displacements that you've observed over and over again. And those would be logical things to test right away.

you wouldn't move randomly, you would. so there are two sort of models. There's a model of reference frames and a graph and where the features are. There's also Stored, displacements of components of this model. It's not a, you can't store all the displacements because it's a very large number, but you do store some, and those become, those are things you've seen observed over and over again are consistent in the model, and those things to test right away. That'd be the quick test. if I see your eye, I don't just randomly look around so you can see what else I see. I'd say, oh, I have three hypotheses, nose, eyes, and mouth. three different directions I might go, and then trust those. That kind of thing. These are things we could discuss. I think that's the way to really know. Otherwise, it is a very big problem and we need ways of narrowing it down. That's one way. Another way, I think, is just brainstorming. I know we want to go on, but I think there might be, we might be learning this morphology at different levels in the hierarchy. And so it's a very coarse morphology could be coexisting with finer ones. And of course, morphology is another way of very quickly narrowing down your hypothesis.

All right, so we have to come back to that. Yeah, all of those ideas I, yeah, trying to keep in the, back of my mind or on my next up to do list.

okay, so yeah, here, just to wrap up, things that influence the evidence value. The morphology error, so the angle between the curvature directions and point normals can add and subtract evidence. pose independent features can add evidence. and that's weighted by just a factor, so we don't necessarily need to add those in but they could add evidence for faster recognition. And then there's also an evidence decay factor which I'm playing around with that kind of just subtracts evidence at every time step so it doesn't keep growing and growing forever but it's just the most recent evidence from the most recent observations, and the evidence decay at the moment. looks like this function. So if the evidence value is negative, it's being pushed to it. So it's always being pushed towards zero again. that makes sense. so if I, if one hypothesis has negative evidence, the evidence decay will make it a bit less negative until it's at zero, then there are nothing happens to it. if it's positive it goes down.

yeah. Is it subtracted or multiplied?

it's added in. It's just plus evidence. Okay.

So yeah, this, whole thing, I'm calling it an evidence learning module because it's not really a formal probabilistic framework or anything. It's more like a histories of past evidence that is being accumulated for different hypotheses. The evidence values by no means add up to one or anything like that. but still, it gives you a good idea of what is the most likely hypothesis at every step.

and yeah, it has a lot of other nice properties. What's not yet implemented is voting with evidence, which I'm imagining to add and subtract evidence. and then also what you just mentioned, Jeff. to, if we recognize common displacements that we stored in the model, like moving from eye to eye or eye to nose or stuff like that, this could also add evidence for certain locations in the model.

Make sense? Yeah. Okay, then, yeah, I'll go through one example just to visualize it a bit. So this is the first step. We're sensing these, pose features, so three vectors. This is just the zoomed out view, the learning module doesn't get this view, this is just for us to see. And then this is the sensor patch that the learning module sees.

at the first step, we just initialize the hypotheses, so the first, possible poses. We haven't moved yet, now we moved, the red dot shows where we started, and then the black line is the displacement that we sensed, and then the other three vectors are new pose features that were sensed. How far is this on the actual picture of the cup? So we moved from, can you see my laser pointer? Yeah. Okay, so we started pretty much in the center here, and we moved like a little bit over here. Okay, about that much. Oh no, actually we moved down, so yeah, this is the movement that we call it. Okay, that gives me a sense of it. Okay. Okay, so and then we use this displacement and the new pose features. In this example, I'm not using any other features, no color or stuff like that, to update our evidence values for the hypotheses. And this is a little bit small to see now, but you can see some of the points moved actually off the object. That's the case when the hypothesis was, if I would have started at the corner of the can, and I sensed this displacement. Now, I would be out here. I'm not. Having, I don't have any nearby points stored in my model, so I decreased the evidence value. So now this starting hypothesis has a negative evidence, actually. So how do you know that next red is positive? How do you know that the, cup can, hasn't been rotated or in a different orientation?

Are you not dealing with that now? I'm just, I'm rotating the displacement. That's why the points can move off of the. model of the object that I have. So the hypothesis You could have been here, since you don't know the orientation, but in that case you moved in that direction, right? I'm just saying, I sense something, I don't know where I am, I, can, say, oh, there's impossible points on this object, but I don't know the object, I don't know its scale, and I don't know its orientation. So I move and I'm off the object. I can't eliminate that object, because it could have been in a different orientation or different scales. Yeah, so each point there, I think, those are showing all the possible hypotheses that are still, yeah. You're saying all the hypotheses includes all the orientations. Yeah. Oh, I see. there's a lot of cans that are not looking at. Yeah, that's why that was my high level point is that for agent hypothesis. It's a hugely large loop. Yes.

Yeah, so I'm not rotating the whole can. I'm just, for each hypothesis that I could start out at, so it's a bit small here, each of the gray points, each of the points on the can, for each hypothesis I have also a possible rotation. So I say, if I would be on the top of the can, or on the bottom of the can, the rotation would be 180 degrees along the x axis, for example. So if I test this bottom can hypothesis, I take the displacement, I rotate it by 180 degrees, and move from that point in the model reference frame. And then I look in my model reference frame, is there, do I sense the correct features at this location? And that might be completely off, which is where the blue points are. Those, all those different blue dots represent, also represent the different orientations of the can. Yeah, exactly. Okay, so it's just, okay, I got it. if you give us a sense of, if you do one touch, like you did, how many hypotheses are there for the can?

for example, I think in this example it was around 3, 000 points stored in the model, and then around six to seven thousand hypotheses being tested, except if we start on a flat surface, then we're testing more than that. Okay, that's, good. Big number, but we have ways of reducing that. Okay. And, then again, you can set that you only want to test the, this, the most likely hypotheses. Here I'm just updating all of them for visualization.

yeah, we move again. In this case, we actually moved back to the starting location. So all the hypotheses are actually back to where the model locations would be. Nothing is off the object anymore. And we couldn't really add any evidence. Couldn't really, yeah, we didn't really learn anything from that. Now we move, to the left. Again, we updated hypotheses.

I'm just going to move a bit quicker through this. So while we're moving at the bottom of the mug, we can't really resolve a lot of information. It matches with a lot of, all of the models just on the, that bottom of the cup. But now something interesting happens. Now we move from here to here and the cup model that is in the YCB data set has some weird shape on the inside. It goes up and then it goes up again. I don't know. I've never seen a cup like that, but, that's how it is. They actually took real objects and skin. Yeah. I think that kind of thing. Yeah. So yeah, anyway, so we're actually going up to the first fold on the inside of the cup. So now you see here, this black displacement went. out of that bottom plane and went up. And now you see, these are, a lot of hypotheses are receiving negative evidence and a lot of them are going off of the object. So it's saying, pretty much for all of the objects, like this sequence of displacements doesn't match any of these possible locations and rotations. Only on the mug, we still see some red dots, small, hanging around here that, that still match up.

By the way, I should point out, these illustrations are amazing. Oh, thank you. I don't know how you do this. Yeah, in the Jupyter notebooks, you can also, rotate the Maybe it's some simple tool to make it work, but hell, I've never seen I don't know anything about this anyway. It's very helpful.

Yeah, I'm using it to, debug everything. It's very helpful.

So yeah, so basically every little dot that you see here says if I would have started with my initial hypothesis, then I would be here now, given the sense displacements.

And then it says, okay, if I'm here now, does this match with my model of the object? All the blue dots say, no, this does not match with my model of the object. That's why they have negative evidence here in the cup. In some cases, it matches. Actually, some cases on the top of the rim, that is, if we would have started here on the first fold and then moved up, and then some other cases are here on this inner fold of the cup. We would have started in the middle and then moved up. If you move and then you predict that movement would be outside of the object, then that would be considered positive evidence.

It outside of the object, if you would've predicted you're outside the object. Oh, if you predicted outside the, do you mean off of the object? Yeah.

Yeah, So at the moment, we're not processing observations that are not on the object. So it's like the policy just moved back on the object and then we continue on. Yeah, in the long run, we could actually make use of this information saying okay, yeah, I should be up the object now. That seems to make your life so much harder because You could be collecting evidence all day long, not being on the object. Yeah. It's yeah, no, that, that's true.

Yeah. So especially the ceiling. So that's good evidence.

Yes. Like to I, my Spider-Man, but sometimes I, it might actually be a quick way to. Narrow down between two possibilities. it seems like it's a quick way of negative evidence, right? You're saying, I think what you're using here for negative evidence is a very quick way of proving anything, but, I don't know. Yeah, you don't want to make the space too big. then your search space becomes huge. So how are you determining what your next movement's going to be? Is it just random at this point? Are you, Yeah, at this point it's random. Only if we move off the object, we move back on it, but the rest is random. So for example, if you're looking down at the cup, it's okay, these are the remaining points that are actually still consistent with my previous movements. each one of those has a set of neighboring points that they could move to at this point, and you could command that movement and further narrow down which one of those would satisfy that, yeah, that's one of the main advantages of this new learning module. we can easily use it to, to write some much more efficient action policies that actually move to test hypotheses.

I think, if you think about touch, and I think to some extent vision too, we don't do random movements. we will typically follow an edge or a surface, yeah, just naturally do that. And so you would stay, on the surface of the object and, all your displacements would be valid displacements on the object and substance. And that's a simple action policy. It's just build a system that just traces the edge. You know what I'm saying? Yeah. Yeah. And yeah, this is a really basic system and the action policy is definitely not very good. also we can't like move around the object. So a lot of views are very ambiguous. and we can't really resolve symmetries and things like that very easily in this example, but, yeah, that's in my next steps, definitely, to have some more efficient action policies here. yeah, let me go through this, finish going through this example. So we basically, yeah, keep moving around. You see there are still some hypotheses possible for the mug, for the pose. It's already saying, okay, most likely hypothesis is marked with rotation zero, zero. We have this evidence value for it. nice thing is that this in this learning module at every step we have a most likely hypothesis in the beginning it may not be very accurate but still we have some hypothesis in every step. you see the ones that are still possible are also symmetrical, which is, yeah, something else that's being tested for. We're testing for, symmetry, but it's not super robust, which is, yeah, one of the main problems right now. so yeah, now in this case, we actually detected symmetry, we detected that these poses are symmetric. And, reach the terminal condition and classify this as a mug. What do you, I don't understand this, your comment that there's symmetric. What does that mean? it means, yeah, basically that from this view, like all of the rotations of the mug, Oh, I see. I see. Like this, it's always the same. Got it. The only way to resolve that ambiguity at that point is to go look for the handle. Exactly. And for a lot of the objects in the YCB data set, it, there's actually no way to resolve it. Like the bowl is, it's just inherently symmetric around one axis and a lot of objects have at least one axis of symmetry.

okay. So yeah, this is really nice. And overall from initial tests so far, this learning module seems a lot more robust.

One problem is that it reaches time out very often now, especially when we have symmetry or ambiguous views. for example, if we look at the mug from this side view, it reaches the time out because it says, okay, it could be the mug or it could be the can, or it could be the side here. yeah, this is just one run with different objects. Sorry for not having nicer results yet, but here for almost a thousand steps in those two episodes it said okay mug and master chef can are both consistent with my inputs But wouldn't that have been solved if you move further? Yeah, you can't in this setup, the action policy only tilts the camera up and down, we can't move around the object. But, even if you, so you can't determine the difference between those objects. At all. yeah, I guess if you move to the top of the mug here. Yeah, why wouldn't you do that? Yeah, I guess it didn't do it in the random policy. Again, this has to do again with your random policy. You're just like, yeah, if you had a chance to connect it. It's like you're in a field, and it's all grass, and you walk around in a circle, and you don't go to the edge of the field and see that there's a, a road and a tree or something.

Okay. Sorry. Have you had a chance to connect it to the, the finger? Because doesn't that kind of actually go around? Yeah, I did. it definitely performs better than the old learning module with the finger. I didn't get to run more than one experiment yet, so I don't have conclusive results yet, but One problem with the finger is I didn't get very good object coverage during learning. some parts were not really explored. I think that's where some errors came from there, but Yeah, I see. Yeah, because I guess with the different views, you're densely sampling each view. So it's a bit more systematic with the fingers just trying everything. Yeah, what do you mean by matching is a lot more recognized with the finger. So it's more robust to sampling new points on the object. So before It pretty much only worked very well if we sampled the same points that we sampled during training. but now, yeah, I have some results actually. that's because of the, before you would throw out a result that it wasn't a perfect match. And now you're, yeah, exactly. You're claiming evidence with a hysteresis and therefore you can blur over incorrect data points. yeah, exactly. So yeah, that's exactly the problem. Before, if we get one inconsistent observation, it's out. You can never go back to that hypothesis. Here we can get inconsistent observ.

and still recognize the object if it's still the most likely hypothesis. Okay, so you don't necessarily mean robustness to noise. We haven't put in noise in this, at least so far. No, just robustness to sampling. Sampling, okay. Is there an outside, outside significance on the first sensation in that sense, that if a kind of hypothesis wasn't initialized on the first sensation, you'll never consider that? Starting point. That makes sense. Yeah, that's yeah, one problem. So if you instead had like something where you could uniformly resample that or something like that, if that's a good point. why would it be that way? Why, wouldn't you, as part of your evidence, say, especially, if I, if, I, my search base is still very, large. It would seem like you'd wanna go back and reassess your evaluation. You know what I'm saying? Like when Neil is saying, you always start over again, or at least. Consider that possibility. Yeah. Yeah. And I'm thinking about going back to just initializing like uniformly sampled possible rotations, and then assigning some evidence to the ones that match with the first observation. But yeah, I guess one reason I have done this way so far is because This way we can really get the pose like to the point five degree, if, we wanted to. Or if we, have a good initialization, while if we have to like uniformly sample that's even if we sample in 45 degree increments that's already more than 200 possible poses we need to test. So it's, yeah, it's just a really combinatorial thing, that makes it a bit difficult.

Yeah. Yeah. So I guess, yeah, I feel like you, if the first kind of starting position was adversarial for some reason, then. Yeah. But again, it's like a niche situation that can be dealt with later. Yeah, I don't think it's a new situation because, there's lots of things in the world where it takes you a while to even start tracking at all. the extreme example is the Dalmatian dog in the park, but also I'm listening to a song. I have a whole bunch of notes before I even begin. But then it's into it, I start catching something that I recognized. So it seems like there's a lot of situations where we could be confused for some period of time and my initial observations may be useless.

so again, I guess the way I was just gonna say, yeah, and I guess, yeah, the way I think cortical filters deal with this is just to yeah, every now and then uniformly sample from the hypothesis space and you can set like how much you want to do that. and I think bias that sampling. Yeah. So one thing I was, so two things I was thinking about was either like use the point normal to inform the poses, but then for the curvature directions, which are a lot more, noisy, do more sampling. and then second thing to perhaps use the incoming votes to add more possible pose hypotheses. like if other learning modules think slightly different poses, that we can like also consider them on the next steps.

Yeah, that would be nice, especially like in occlusion or something, you might have I don't know, some, parts of the retina or whatever that I've seen, something in some parts that aren't. Yeah. yeah, let me, go, on maybe because yeah, and also a lot of stuff, unless there's another question for now.

Okay. So yeah, these experiments were using just the morphology. yeah, the mug obviously has a very different color than the can. So if we actually use the features more, we can recognize. those as well and do not get the timeout conditions like the two timeout episodes that just showed. so yeah, that's the case where features like color can help.

and yeah, here better able to deal with new sampling. this is the experiment we have in the README where we have new sampling by moving a different step size and we learned that. So we moving in different increments over the object. And here now we get, 100 percent accuracy before it was about 75. that's nice. it's just a really small experiment. I definitely need to run a lot more experiments to, verify this. And also for noise, I'm not specifically testing that, but I'm assuming that it should be more robust to noise, but yeah. have to test that, obviously. Is it, this, you're dealing with the different orientations and the different objects, are you dealing with scale changes here at all, yet, or no? No, right now this is all, yeah, fixed scale. Okay, that's right.

Yeah, other advantages are, yeah, at every step we have a most likely hypothesis, it's not like we have to wait until we narrow down the possible, matches to one, but we know what's most likely at the moment.

we have a rough idea of the likelihood of different hypotheses compared to each other. it's easier to use this to inform actions and to make informed action policies that are more efficient. It's a nice representation for voting. I can talk some more about that later. I think it could be used for generalization and categories, because it's, yeah, even if my mug is slightly differently shaped, it hopefully would still have the highest evidence compared to, other objects that are not mug shaped. but yeah, obviously still have to test that's speculation.

and yeah, it opens up possibilities for using priors. like just initializing the evidence as not everything zero, but some of the objects may have a prior evidence, evidence based on context. and also for more customized mixing of features and morphology and other evidence sources like displacements.

yeah, so those are the advantages. next steps I'm thinking about right now is, yeah, Definitely adding voting with this evidence based learning model. Can you give us a clue why you think this is a good representation for voting? What are you thinking there?

Yeah, I think it will be easiest after I go through how the voting is working right now, but, Are you going to do that? Oh, I thought you were wrapping up, sorry. Yeah, I'm wrapping up the first part. Oh, okay, sorry, keep going. One thing I'd like to see is really some sort of a noise analysis, noise in the pose estimates, noise in the movement displacements, noise and those are, it's not clear to me how well this would work. Yeah, definitely. Yeah, I mostly ran all these experiments today and I'm still like, yeah. Definitely want to test that. Yeah, it's a big thing about applying this to real world, real sensor data or real things. It's really important to have at least some sort of noise to the actual sensory input. That's what I'm saying. Yeah. sensory input and the movement. and, pose estimates, curvature estimates, all that stuff. Yeah, so so far I'm still working on getting all the hyperparameters and everything and the code right. And yeah, but that's definitely one of the next steps to evaluate that.

And then yeah, I think also, using voting and multiple learning modules should additionally help with noise as well.

yeah. And then another major point would be to have more efficient action policies. that should also help with the timeout conditions because we can actually move to the locations that resolve the most ambiguity and like efficiently explore the object. and then, yeah, short term code stuff is, yeah, figuring out the right thresholds, terminal conditions, symmetry conditions, and all that stuff. Can I, add to your, can I add to your major list? we talked about these, I just want to make sure we don't forget them. one is the, using displacements for improving our action policy in some sense. and the other is, a hierarchical representation of, morphology, which we haven't really discussed, but I think is an important part of this. So just don't forget to put those as placeholders. Yeah, that's a really good point. Yeah, right now, it's still doing a ton of stuff, to represent very complex objects.

For the displacement stuff, I feel like that kind of necessarily requires the intelligent action policies. No displacement is going to be more likely until there's a reason for it from like a bottom up. you could learn it by, just by which things are consistent.

there's various ways you could learn it. So I don't think it's a random thing. Maybe I misunderstood your comment. No, I, yeah, no, I see what you're saying. Yeah, I guess I just meant like for it to. I imagine if we're to really have a big effect, it would be because the action policy is tending to sample in certain ways. And so then you can leverage that. Yes, exactly. Whether that's a bottoms up, like going from saliency area to saliency area, but mostly it would be like, repeated consistently then get stored and the things that aren't consistent get put on.

it's pretty easy to do in the neurons. Okay, it's good enough, just put them in there soon. I think those are going to be, I think those are going to be important to solve this whole problem. Yeah, especially also the hierarchical point, yeah, I would really like to get into more soon. Yeah, I've only thought about a little bit, I don't know if we've talked about it, but the idea is you've got, you've got a model at one level which is very coarse, And yet, at each point in that coarse model, you could have some, a refinery below it. Yeah. I feel like that would be an interesting topic for a brainstorm because at least in my head, it's not clear. Exactly. Like practically how you go about creating a coarse model that kind of, and that ties into kind of what you're actually observing. All right, good point. let's put it on a list for brainstorming. Yeah. Schedule it.

okay, and then another one issue at the moment is that it's still a lot slower than the other learning module. I already added, changed a lot of code to matrix multiplications and stuff and got more than 10x speed up, but it still takes about three seconds per step if we are testing all hypotheses. If we don't test all it's faster, and I think also adding more intelligent action policies and voting will definitely help. But, perhaps there are also ways we can make the code run faster. or definitely there are ways, but. I think we should, I think we should focus on the former as opposed to spinning all this. you gotta speed things up, but that's not the real solution. Yeah, it's just a little slow to run experiments. Okay, But are you able to use the meganode and run, 200 in parallel? 200 cross, Yeah. Things in parallel.

As of yesterday, hopefully, that should be working. That should be like a 200x speed up right there. Oh, hopefully, yeah. But yeah. Yeah, is this? No, we have 256 cores. Yeah, I set the limit to be 128 right now. Okay, We can, change that if we need to. The other thing would be interesting with that is just flipping the loop so it's more like a Hough transform type. thing rather than explicitly checking all hypotheses every single time. I'm not exactly sure how to do it, but I think there might be some potential there. Yeah, I'll have to look into it that more. Yeah, that'd be nice. Yeah, what do you mean with that, sorry, Subutai? I'm just trying to picture it. That's probably a longer discussion, but the Hough transform approach is just you have The space of hypotheses, you don't explicitly go through each of you. As you get evidence, you accumulate, you added to the points and hypothesis space that are consistent with that evidence. And then, at any point in time to make actual inference, you pick the max, the points that have the max.

That's what we did in the columns paper, I'm confused. it's, yeah, it's, similar to the columns paper and, the, capsules definitely uses that intuition. I got it. I'll have to explain it to you. Yeah. It's a longer discussion. Yeah. Okay. Yeah. I'm also not, an expert with transforms. So I have to look into it somewhat. I think, yeah, one of the, one thing that's holding me back from using more matrix multiplications for these hypothesis tests is because. we have to rotate all the displacements by different poses and stuff like that. So it's a bit, and then find the nearest neighbors in the, model and those other, and that can, yeah, that's a bit difficult to do it. I'm not sure how it would work with a transform, but yeah, I'm not sure either. It's just a different, it flips the loops basically.

Maybe it won't be tiny thing, but it's another possibility.

Yeah. Okay. So now you have another, present. Oh, we have one bad point here. Yeah, last point here is just adding a real value feature error instead of binary or just right now and yeah, bouncing the different evidence sources perhaps adding in displacements like recognizing common displacements. And yeah, just figuring all that out.

Yeah, and now, then I would go to the voting but I don't know. How you guys feel if you have more questions or if that was already too much information, or, yeah.

We can look at the votes. I go voting. You vote for voting? Yeah, I vote for voting. Are you ready or were you hoping We say, were you hoping we're gonna say nah? No. I was hoping, you got, are you ready for voting?

All right. this is also implemented, but it's only implemented for the old learning module. Not yet For the evidence based learning module. Oh gosh. But it should be the same principle. It shouldn't be, yeah, I just don't have, yeah, any results on the evidence based one. Anyways, should be same principle. So the idea is, we have actually multiple sensor patches, they may be at different locations, they may be different, size, different resolution, different features that they're sensing. whatever, and each sensor patch feeds into a separate learning module and these learning modules float with each other. Each learning module has its own model of the object, the models may be different, they may be storing different features, they may have more or less points stored in them, yeah, each learning module has a different, has its own model of the object and it gets, its own patch. of sensory input. And then the vote contains, a vote on object id and a vote on poses and locations. Make sense so far. Yeah. Also, we would include scale, but we're not doing that yet. No, she is has, she has different scales, I think, of features, right? that's, those are different scales of the learning patches, but not different scales. Yeah. Not the objects. Yeah, not the objects. in some sense, what do we know when we vote, we do a flash inference, we know what the object is, we know where, its scale, basically how far away it is, we also know its orientation. Yeah. And those are things that we seem to vote on.

so anyway, but we don't have to do scale yet. We're not doing scale. So you're doing orientation and ID. yeah, there's a placeholder for scale, but since we're not detecting scale right now, it's also not being voted on. For now. Okay. Okay, so voting on object ID, how it is right now, definitely open for other suggestions. It's just like first stab at it. are you saying, are you going to vote separately on object ID and separately on orientation, or are you going to vote on them at the same time? Is there one representation or are there multiple representations?

it's they both interact with each other, but it's slightly different mechanisms. Maybe I'll go through it and then that makes sense. Okay, sorry. Yeah. So yeah, for object ID each, module sends out a plus or minus vote for each of the object models. each object that it knows about. Incoming votes to a learning module are added up and if an object's minus votes outnumber its plus votes, it is removed from the possible matches. So one, one issue with that is, it may be some learning module can completely rule out an object. Exactly. No, that's exactly not the case because Yeah, that's what I'm saying. So it's not if there's one minus vote it's being eliminated. It has to have the minus votes have to outweigh the plus votes. Yeah, which is could be a problem because if one learning module is really sure it just cannot be this object. It cannot eliminate. No, I think she's saying, which I don't, maybe it was a confusion. Let me go through it real quick. So yeah, here's an example. each learning one thing, not all learning modules need to know about the same set of objects. So for example, learning module four doesn't have a model of a cup, so it just doesn't vote on the cup. Okay. So they all have different ideas. Learning module zero actually, just in this example, the cup would be the target object, learning module zero actually got some inconsistent input from the cup, so this one actually has a negative vote for the cup, and then all these ones have a positive vote for the cup, and the learning module four has no vote for the cup.

So those are the outgoing votes for learning modules. I'm assuming here that there's an all to all connection between learning modules, but theoretically they don't have to all connect to all. It just, adds up the votes that come in from whichever learning modules connect to, you. And so here, does the green and red, does that signify, just a yes or no vote for the object? So green is a yes vote and red is a no vote. this line is the outgoing votes, and then this line is the incoming votes, so Oh, okay, yeah, there's not a line for outgoing, yeah, that makes sense, thanks. Yeah, so learning module 0 receives 3 plus votes for the cup from learning module 1 to 3, and no minus votes for cups. And it receives 1 plus vote for the apple. LM1, and three minus rows for the apple from the other three, and so on. And then I'm just philosophically right now against this typocoding, but maybe you can convince me Why is that? Because you can't Let's say one learning module happens, you're sensing the apple, but you don't know if it's the apple or the ball, they're both curved, but one learning module definitively senses the leaf at the top. It can say, it's impossible for it to be the ball. But this doesn't allow that. nothing, nobody has, nobody here can, can dictate everyone else, right? Yeah, that's what I'm saying, that's the problem. what I've been trying to do here is In the pilot paper, we definitely had that. it is more, you, but the trick is, you want to be, if you're not sure, you don't want to eliminate anything. But if you're really sure, there's a leaf there.

Yeah. What,? Well, how can you be certain it's a leaf? you can be certain it's not a ball. Yeah, so this is something that I'm hoping the evidence based learning module can help with, because here in this old learning module, we just have objects as possible or impossible. In the evidence model, we can have the evidence for different objects, so it can be a more nuanced vote. We can say I'm very confident it's the apple. I'm very confident. You want to introduce noise, and so if you introduce noise, then your votes can never be completely confident, right? yeah, so I'm pushing back on the idea that I could say I'm certain it's an apple, or I'm certain it's not a ball. Maybe that's not possible. Yeah, maybe 100 percent certainty is never possible, but you can be pretty sure, even with noise, and it's just there's no way to work. It's fine. So you might still reach the right conclusion. Maybe this is more of a consensus based thing, rather than voting. And it's like majority rule. Yeah, no, everyone has to agree, right?

yeah. So not everyone has to agree because Here the mug was negative in the learning module zero and it still, ended up on overall saying it is the mug. but yeah, the properties in general are, what I've been trying to get out is not all learning modules need to know about all objects. Yeah. that's good. And we can deal with some noise if the majority of learning module still consider the object as possible.

And yeah, overall, I was just trying to strike a good balance between optimizing for robustness and speed. So yeah, we could say we get a negative vote for the cup here. So we eliminated and that's that makes it narrow down on something faster. But it also makes it less robust to noise. So that's what I've been trying to balance here. Yeah, that's an important, that's a tricky balance, but it seems like it's also somehow if that learning module zero is, pretty certain it's not the mug for whatever reason. I would argue this one, we don't have certainties. We only have yes or no. So yeah, I would, even learning module zero, remember learning module zero could sense the object multiple times too, right? So So it's doing voting with itself over time, in addition to seeing everyone else's votes. So it just seems like you want, to be able to know that nobody can ever just only eliminate something and say, I have a, my fingers skipped the road, there's something missing here. I don't know. Yeah, but the counter argument here, let's say you have a mug without a handle and you have a mug with a handle. Yeah. One of your fingers is touching the handle. Here, three of your five fingers would need to get to the handle before you eliminate it. maybe, but, perhaps, I grab something, and there's two objects in my view there, or I'm looking at something, and there's a mug and something next to it. different columns will be looking at different things, so maybe one, only one column sees the handle, and the other columns don't, but maybe that handle's part of the mug, maybe it's not part of the mug, maybe it's part of something else. And so I can't be certain yet. yeah, but this will require a majority of the columns to see enough distinguishing features. I think that's right, but that's okay. I think like it's a good idea to do that with where you just phrase it sounds like a good idea. it seems it's just be slow. Ah, necessarily.

Subutai would you agree that if we once we use the evidence based model and don't have like binary votes, then that would kind of address. Your problem potentially. yeah. At that point you could also just, you could have a strength of vote parameter or something like that. Oh, yeah. Yeah. Like I said, that's what I, this is, binary. I forgot about that. Yeah. Okay. But I was just, I was zooming ahead. Yeah, I mean there are definitely cases where you do a single sensation and you know what object it is. but that, but, so that's okay. Even if, what if one of my fingers. when I grabbed the cup and I said, that's the cup, but one finger felt something weird. what that would be, what would I do mentally? I would say, that's the cup. There's something weird down here. And yeah, I would say that's not a cup. No, I know that. So you want to, but suppose that one weird thing really told you, oh, it's my mug because I know there's that logo that's sticking out. that may be, and then maybe you have to attend to that component and then maybe, and then you'd see that, right? But we definitely want to allow, a single sensation to tell you what the object is. this would allow, both this and the evidence based one would allow a single sensation. you could, settle on the object in a single sensation. You could, but there are a lot of cases where it won't, I think. Why wouldn't there be? Because In the maturity, you don't want to, but then without the right evidence based ways, you could say, okay, yeah. So why it seems to me this sort of, most cases would settle on one sensation and I'll see why it wouldn't.

Yeah. I guess it depends on how well distributed the sensors are on the object. Yeah, Obviously, if all the sensors were looking at the bottom of the coffee cup, it's not going to do very well. Yeah. Yeah. But if they're dispersed around the coffee cup, it'll do pretty well. Yeah, it should work. How are you taking advantage of the relative positions of these sensors? That's the key thing here. Yeah, that's the next step. okay, you don't have that yet. Or do you have it? No, that's not in here. So now, that was that. That just makes, otherwise it's just a bag. you know, bag of features, right? Yeah. We know that's not good enough.

Yeah.

now that's voting on object pose.

let me go through the example here. So basically in this example, let's say we have two sensors, two fingers, one is touching the mug on the top and one is touching the mug on the handle.

Each learning module sends out its current possible poses and the current pose of its sensory input. So it's It has the possible locations and rotations of the sensor relative to the model of the object. So that would be the green dots here in learning module zero. That's corresponding to sensor module zero. And learning module one has, for example, two possible locations on the top of the handle.

And just maybe anticipating some, some confusion, just about what we said before, because this is taking account of, relative positions. So it's, yeah, what you're describing now is not doing a bag of features. Exactly. Definitely. Yeah, exactly. To make sure that was clear. Yeah. Thanks. Yeah. yeah, exactly. So that's the next step. Basically we have the sensor module location and rotation relative to the body. And with rotation, I'm saying these three vectors I talked about earlier, the point normal and the curvature directions. And in the next step, I calculate the displacement between the two sensor poses. the sensor pose of the sending to the receiving learning module. So let's say learning module one is sending votes to learning module zero. I calculate what is the displacement, so location and rotation, between how my sensor, like this hand on the handle, is rotated to the hand on the top of the mug. In the next step, this is both, orientation and location, is that correct? Yeah, correct. But why do you need to do this? Why not just do it all in the, in the body's reference frame? then you don't need to do this stuff. I'm, doing all the matching in the model's reference frame, Okay, so that's a discussion whether you want to do it this way or do it with the body reference frame, but. Yeah, so yeah, let me just finish this one real quick. So I have this displacement, location and rotation displacement. I transform the votes using this displacement. So basically saying, I think I would be on this handle, but given that your sensor is displaced by this locational rotation, you should be over here on the model. So now these transformed votes, the yellow dots, are being sent to learning module zero over here, and it might get more votes from different learning modules. And the receiving learning module checks for each of its possible poses if there are at least n incoming votes close by, otherwise we remove the hypothesis from the possible matches. So in this case, the incoming votes are only close by to this one green dot, so after voting, all the other ones were removed because they didn't match with what the other learning modules were thinking, and only this pose is left over. And this is how it looks in the real model. Before voting, all the green and red dots were removed. were possible as locations for the mug, and then after voting all the red locations were removed, all the green ones are still possible, and all the gray dots are other votes that came in from the learning module, from other learning modules. Does this make sense? Yeah, that's that makes a lot of sense. That's great. I do see a lot of competition going on that I don't think no one's able to do. and so that's just an aside. I think it's still good. But, Subutai point out, going through a body reference frame might be much more efficient. but, this, it seems like we're still a step away from how neurons do this, but that's okay for now. Yeah, it seems like the, potentially the body, if you have n learning modules, you have to do, n squared of these, transformations from one sensor to the other, whereas if everything is in the body's reference frame, Internally, then. Yeah, so we could replace this one black arrow with two. One first going to the body reference frame and then to the other. yeah, that's, yeah, could be pretty straightforward. Yeah.

one way to just think about it is more brainstorming is a single module does this sequentially in time and it knows the displacement of the sensor as it moves. It calculates its displacement as it moves and, and what we, ultimately want to do is when multiple learning models working together, they work on the same mechanism, but they're doing it in parallel. that's just a, it's a very simple blanket statement. You want to be the same mechanism doing it in parallel. So I'm looking for that common mechanism that can work in time in one module, across time in multiple modules. I'm reminded of a paper I could show you. Where they show when you activate a single column, that column, information goes up and down in that column, and then as soon as it does that, it starts spreading to the next columns over, in the lower layers, the layer 5, and I'm just thinking it might even be a sequential step. It might be like a single column, gets its input, makes its hypothesis, and, then, and then a moment afterwards, you get other input and say, okay, now here's another input, as if I had gotten it, but I didn't, someone else got it, and I'm going to add it to my hypothesis. It's, that's an alternate way this might work. I'm just thinking out loud, I don't know if that's useful or not, but I think Cortex will do this somehow differently. Yeah. Yeah. so the right now, the difference between the. Update within the learning module and update from the votes is that the input to the learning module is this feature at look at a location and. The features that can be received can be specific to like a sensory modality, for example, also. And they are specific to the features that are learned in this learning module. And this sensation leads to an update to the possible objects and poses. And then the votes, the format of the votes is consistent across all learning modules. It's always possible objects and poses. And those are just yeah, add or subtract evidence. for the poses that each learning module itself represents. While the model in the learning modules can be very different, it can contain a lot more or less points, it can store different types of features, it can, yeah, be very detailed, or not.

And yeah, it can store completely different locations on the object, for example.

Does that make sense? Yeah, I'll just finish up the slides here. So yeah, in, the, in reality, we do this with these complex 3D objects. So we receive votes for each model in a learning module and perform this update. We do this for all learning modules and, yeah, learning modules here, for instance, the one on the bottom has a lot more features stored down here and they may be like storing different features and different resolutions and so on. And also, as you can see the possible locations on the objects within those models will be different. because the sensors are in different locations. So learning module one will have different locations on the mug than learning module two because the sensors are displaced. So yeah, that's why we have to apply that pose transformation to, to send the votes. we do this for N steps to narrow down possible votes and every step we, do an update using the sensor inputs. And then we do an update using the votes.

just a quick result. using voting cuts down on the average number of steps needed to recognize an object. So yes, no vote with voting.

one little detail here is that, yeah, I'm sorry, you're voting on both, you're voting both on pose and in that previous slide, you're voting both on pose and object ID? Yeah, both.

And in terms of processing that, do you take in the object ID vote first? Because it seems like that could save computation time if you particularly with your evidence based model, if you like eliminate an object ID, then you don't even need to do the Yeah, so the votes are all sent out at the same time, and then after that they're being received. So if one learning module never eliminated an object after receiving the votes, It has still already sent out its own votes before that happened, if that's what you mean. but it, but at least after, if it receives its votes, and it first checks the IDs, and let's say there's, with the evidence based one, there's like large negative evidence or something that passes a threshold, then you can just say, okay, we're not going to iterate through all the points in this object. Yeah, because it's not. yeah, definitely. I think one question here, if you, had a single learning module getting five times as much data, how, fast. That would be one learning module taking five steps, basically, right? Yeah, that would, what am I trying to say? So it seems like in the no voting case. It's taking close to 30 sensations for each learning module to, to, figure out what the object is on average. Yeah. Yeah. And so five learning modules should be dramatically faster should be a fifth of it. Yeah. But, and with noise and other things shouldn't really be a, it won't be exactly a fifth. It'll be a little bit. It's not even half. The purple bar seems too high. It's too high. in this example, the sensors are pretty overlapping. get very similar inputs. So it's not five fingers that are all over the object. They still like their input doesn't give that maybe the question is a single learning module, as it goes through 30 steps may cover a larger area of the object than these five modules here in a single step. Obviously, if all five modules are looking at the same thing, you would have no better. But if I look over time they move away Yeah, so I guess maybe that's the, question, is it some artifact of how the sampling is set up currently, which is what you're suggesting here. I think Doesn stick together, those things. That's some artifact of how we're feeding the data into these five learning modules versus something inherently with, I would, yeah, if I would've done this experiment, I would've picked five. Disjoint points on the object. Yeah. Opposed to five next to each other. Yeah. And then that, purple bar would be a lot lower. Yeah. But, I guess as Heiko says, even with a more intelligent sampling or whatever, like a retinal patch, like what's sampled at one point is going to be resampled by. the learning modules that kind of move on to that. Like you can't necessarily get away from that in some sense. No, but you can, recognize the thing in one fell swoop if you're, if the different learning modules are looking at the extent of that.

Yeah, the thing here is again, very simple action policies. all patches move together. If I put them too far apart, then it's very difficult to keep them all on the object. Even right now, a lot of patches are often off the object while the one that's used for the movement policy is still on it. So that's all right. But that's your policy again, your action policies to say, just don't have them go off the objects for starters. Or use the tactile sensors, which you can more easily distribute, like having five fingers, five fingers or just, yeah, the problem with the touch sensor, I wanted to do that originally was that during learning, it's I didn't get it to cover the object as evenly as I can do it with the vision sensor. even the vision sensor, you could just say, look, you've got these objects in non cluttered backgrounds at the moment. you could just say, hey, make all my vision sensors on the object someplace.

I'm not sure if I They don't have to be together. I know they're together on the retina, but from a theoretical point of view, they don't have to be together. Yeah, just from a testing, and remind me again if, is there anything wrong if something is off the object or something? No, the step, is just being skipped for that learning module. Okay, so, that would still work if you had five of these patches that were separated. Yeah, that actually happens a lot in these experiments. Some of them will be off the object, some will be on, and then they just don't update their model in that, in those steps. Yeah, so imagine you're looking at a pencil. And, the pencil is going to some image across the retina, where most of the retina won't be seeing the pencil. most image patches will not be on it. Going back to the issue with the touch sensor, that you don't get a good coverage of the object during training. Could you use for training the vision sensor? Because essentially, we do a morphology based model. So you get the model from a vision sensor, but then use touch just for the inference. Yeah, I'm hoping I can do this with the evidence based model. The model that I did this with, it's too sensitive to sampling, so it just Yeah, it didn't work with sampling new points with the touch sensor. Okay.

yeah, one, one thing is that the terminal condition is something that needs, needed to be adjusted as yeah, so a bit of a question, what's the best way to do it? so some learning modules may have already recognized the object after a few steps, but then some other learning modules may not have, for example, the pose narrowed down yet.

overall, if, we want every learning module to have uniquely narrowed down the object and pose for the episode to finish, it overall takes longer than if we just look at one learning module individually, and one thing we can do today. But shouldn't it be the case, but shouldn't it be case, the case once the majority of learning modules have learned, narrowed down the object, then everyone should have narrowed down the object, right? Yeah, the object, yes, but the pose, no. Why not? Why not?

with the pose, we don't use a majority vote. We just use whether there are enough agree, votes in agreement in the vicinity. Oh, okay. But in principle, since these are rigid objects, if the majority of them agree on the pose, the rest should also agree on the pose.

I'm not sure if it's always the, if that is always the case, especially if we, if some models. Store, for example, the mug was a lot fewer points or, yeah, there's five of them and three of them say, Oh, it's this pose relative to the body. It has to be the case for the other two to the majority, vote.

Yeah, it's not that simple. It's this specific. Yeah, I guess we conceptually, it seems Yeah, I agree. I don't, so why wouldn't, Yeah, I don't know exactly how to incorporate it into the algorithm, but it seems like conceptually that should be there. We're going with majority vote and yeah, even more interesting than that. Remember, at any point in time, a lot of columns, there's a lot of individual system. There's a hell of a lot of columns and only a small subset will be sampling this object. And, but I believe that they voting would occur across all the columns, even the ones that aren't getting any input. that in some sense, the entire section of v1, not all of you want, but some area of the cortical tissue we all know that they all come to agreement, even the ones that are getting input and I can't be voting. They should know because when those columns do get input, they should already be biased by the previous knowledge of all the other columns. So it's, like a, it's like we come to an agreement. It's shared among everybody, even the people who are not voting right now. and so everyone knows the whole system says, okay, I know if I do get input, I know what it looks like, what it should be. Yeah, that's the case here. So if we, if a, module is not on the object, it doesn't use the sensor input to update the hypotheses, but it does use the incoming votes. Great. So I don't, again, you can explain it later, but I don't understand this Subutai comment why all the columns wouldn't have the same understanding, but there's some reason. So like you said, some, all the voting, it wouldn't be equal. So I don't understand that.

Yeah, I think a lot of it also has to do with some hyper parameters. So for example, if you look at the last frame here, all the possible poses are pretty close together, like the possible locations but The question is, when do we say this is specific enough? Do we want it to be one specific location, or do we say as long as all the possible locations are in a radius, it's okay? And then that, is influenced by what's the radius that a vote can influence, and so on.

yeah. Those things play into that a bit as well, because we're not voting on discrete sets of poses. We're voting in a continuous space of locations because every model may store different locations, like these points, they don't overlap.

And actually, one thing I forgot to point out with this picture is, you can see voting is really useful, especially on the first step before voting. We have all these gray dots scattered around this, mug that those are the incoming votes at the first step. And then all these votes are gone at the next step. So pretty much at the first step, like the relative sensor displacement eliminates a lot of impossible locations and poses.

That step is just a voting step, right? Yeah.

So that's, yeah, a nice thing that like, that's not, just the bag of features, but it takes into account the relative displacement of the sensors and with that relative displacements. A lot of locations on the objects are not possible, but you are calculating on a vote by vote basis, you're calculating the displacement of those two votes.

so by five columns, I'm, one of those columns. I'm getting four votes. I have to calculate the displacement, all the other four columns.

individual Yeah, certainly, but one at a time. Yeah.

yeah. So every, learning module sends out its votes and its possible poses. And it's sensor pose, and then the receiving module takes the sensor pose to transform the votes and take them into account. Yeah, I'm just saying computationally, it's a serial operation. Yeah. It doesn't seem like that's going to work in the long run, but it could work right now, so that's good. It's really cool to see this working, this stuff, voting with poses and object IDs. That's really great. Yeah. It's got an initial version working. I think there's a lot of, yeah, like we're saying, there's a lot of, additional things we could do, but it's really great to see something up and running. It's working so well that we get agitated, I get agitated, and I want to fix all the little problems. And yeah, if we actually say that, we, terminate an episode once one learning module has recognized the object and pose, and we just say this learning module, like spreads that knowledge to all the others. Then the number of steps gets cut down by a lot more. So yeah, this bar here is five learning modules, no voting. Then the purple one is five learning modules with pose and ID voting, but all the learning modules need to narrow down to one object and one location and rotation, and then the pink and the, I don't know, lighter pink, are either three out of five need to come to a conclusion or one out of five needs to come to. Conclusion and then, that's actually also a lot faster than just using one learning module.

And yeah, that's, that's it.

Right now. Wait, how can it be fast. That's the one learning model.

Yeah, because I guess. More information it gets like the learning module, like one learning module but it also gets the voting input which can use it to narrow down the. possible objects and poses faster. No, yeah, I get that. That's good. I'm trying to decide between one learning module on its own versus five learning modules without any voting. How come there's such a big difference between them? Oh, because all five learning modules need to come to a conclusion on their own and some may recognize the object faster than others. Wouldn't the one on the right be confused half the time? It wouldn't know enough.

The one learning module? Yeah. Is that one learning module? One step? no, it's, Oh, I see. It's ten steps, I see. Yeah. it's still not obvious. okay. I don't know. If you have more learning modules, it always takes longer until everyone got to a conclusion without the module. I can see that, but I'm not sure why. It's not clear why. on average, it would be the same, but yeah, there could be some, there will be some spread. No, because this is like the max. It's the max, but there'll be some, there'll be some spread, yeah. Like every purple line is like when each individual learning module recognized the object, and it may be much earlier than some other learning modules because, yeah, they get different sensory inputs, so sometimes it's, gives you more clues, the sensory input that the one learning module receives, and then they are faster than others, but it still has to wait until all the other learning modules figured out the object and pose. Yeah, So if you were to, about the max and min for the one learning module, the max, by the time that it might take, that would be close to the 30.

Yeah, from five seeds kind of thing. Yeah, exactly.

Okay, very nice. Thanks. All right, Jason, we'll see you soon. I stopped the recording. We'll see you soon.