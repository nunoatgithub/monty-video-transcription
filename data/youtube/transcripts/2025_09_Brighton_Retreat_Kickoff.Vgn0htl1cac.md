yeah, so in terms of the format, I guess the idea that everyone can pitch a project or two if they want to, the pitch should be like no more than five minutes. Ideally, like one to two minutes to be like, what's the project? What do you plan to do in that week? And why is it important for Monty? People can ask short follow up questions, but ideally we don't get into deep discussion right. By every pitch. And then after all the pitches, we'll form groups around who's interested to team up. So we shouldn't have a project where there's just one person working on it. You should always find at least one other person. okay. Help on it.

and then, yeah, we can also have a group of three. and then you can also, dabble in another project. Yeah, you can, but you should have one main, yeah, you can help out on another project, but it should be just consulting. Yeah. Not, actual. Writing code or something for them. And then on Friday I only sent out the calendar invite for it. We would do like we usually did with like focus weeks or hackathons. every team does a short presentation, kinda showcasing what they did, why it was important, and yeah, the progress they made in that week. And then Jeff, Celeste and Terry will be the judges who deliberate on which team wins the trophies. And yeah, I added on that spreadsheet, a second PA page. It says How to win the prize, which with some basic categories similar to the last time. So there's best in show, most entertaining presentation. So like even if what you tried didn't work out, you can still make it a fun presentation. Yeah. most ambitious. best execution, most exciting results, most impactful, makes community happiest, makes the team happiest, and most creative solution to problems that came up. it's just some broad categories. The prize is like a little trophy, so yeah, don't take it too serious, but yeah, it should be a fun little competition maybe at the end. hopefully every team gets some final results. yeah, so that all makes sense. Cool. Exciting. Okay. And then, yeah, there aren't really any grown rules on when you're allowed to work on it or how you're allowed to help people don't, or sabotage. Sabotage. Don't sabotage other teams.

but yeah, I think that's bit of obvious, hopefully.

okay, cool. Yeah. yeah, Tristan, the TV is not gonna work out right? I couldn't get it to work. Okay. All right. Whatever team gets to get the TV to work. Yeah, it bonus. That's fine.

okay. Should we just start? we can, yeah. I guess just start at the top of the list and then whenever someone wants to pitch that project, we pitch it and if they think it's not that relevant anymore, we skip it. Yeah, sounds good. And should we put the laptop in between Jeremy and Tristan and then people can stand at the front here, so they're presenting progress. so they're staying presenting both the present sending, yeah. Recording and the room. Yeah. I was just thinking so that Ramy can see us, but, or you could just join the meeting from your laptop so that I could just hear you and mute this one That's seeing the whole room.

just to get the microphones what they trying to solve. I was just saying I just didn't know how important it is that Rami sees everyone else when people are pitching. Oh. And that maybe it's easier. It is a bit like kind of the camera in the Menta office was like at the end of the room. I guess it was a mix. I don't know. Yeah, I guess whoever pitches can also just stand over here. Okay. Yeah, that's a good idea. I can see them. Sounds good, right? Is that, can you hear me well? Ramy? I hear you. Yeah, you're a little quieter, but I hear you. Okay, so we just need to project talk louder. All right, so first up is implement object behavior, prototype start. Okay. So yeah, this one is I thought it would be a real fun project for this focus week because it's very much about building a prototype in a short amount of time and not getting too deep into the weeds of it. Not building anything perfect, but getting an idea of what the issues are with object behaviors. And especially since in the past research meetings we've gotten a lot more concrete about how we would want to implement object behaviors. I feel like I have a pretty concrete idea now where I would start and how I would try to implement it. So it could be a great opportunity to just test that idea, see if there are any holes in that logic and plus get some, yeah, first prototype object behaviors working. I think the main issue with the proposal might be that the scope might be a bit big for a week, 'cause it would also involve setting up a test bed and all of that. But it might be actually a good forcing function to set up the most minimal test bed and do the most minimal kind of first prototype of it. yeah, that's my pitch. Nice. Yeah. Benefit the community, tv. Oh yeah. Being able to model object behaviors will unlock. Tons of new, applications and yeah, it's one of the three big milestones in the project, like composition objects, object behaviors, and then manipulating the world. So I think the closer we get to object behaviors and modeling, moving stuff, the more people can actually do with Monty. so yeah, I think it'll be super powerful once Monty has that capability this week. We're not gonna add that capability to Monty, but it would at least get us closer to having an idea of how we would add it. And yeah, to your point, it's quite like a big scope, but we will just, the way this kind of week has panned out and stuff, we'll probably have, at least you have one team with three people. yeah. And that could be one of them. Yeah. And again, it's quite like the moonshot kind of project and not the kind of project like. Obviously if we wanna add object behaviors to Marty, that'll be like a multi-month effort with multiple people working on it to get everything right. But this would be like a very quick prototype to get an idea of scoping that effort basically. yeah. Cool. Cool.

Next up, draft a spec for the CMP, suggested by Jeremy. Yeah, so my thinking here was, when you talk about a messaging protocol or protocol of any type, I think of something that has a spec that describes the behaviors of the system, the message format and how, how it behaves. and I think it would be useful to have, something more concrete, Make implementations of modules and stuff by ourselves in the community easier to do if we're only matching a protocol. I thought it'd be useful to do that here where we're all in person and can talk about it and get on the same page of what, do we mean when we say we have a protocol. what does that look like? yeah. So what would be the kind of outcome of the, at the end of the week, like coming up with a spec or at least a draft, like a written document? Yeah. Yeah. I know that Tristan started one, but like filling in some more of the details of what actually happens when doing voting or doing setting goals and stuff that, like what is it act, what is the actual, like nailing down the specifics of the protocol. Yeah. And that could potentially then become part of the documentation or, like protocol in the code base. Yeah.

So well. Yeah. One thing to maybe think about is just, obviously we wanna make the most of like us being here in person and being able to collaborate in teams and stuff like that. I'm just wondering whether we could have kind of teams where it is one person with one or two collaborators, but only one person who's actually doing the main work. Like one person getting a lot of input from other people. Yeah. Because like it would still be making use of the fact that we're all here. Yeah. But just 'cause I feel like we have a lot of nice ideas. Yeah. And some of them don't necessarily maybe need multiple people working on it. point. Yeah. And so like maybe that one, for example, if you were working on that Jeremy, then like you could still get input from a few people and stuff, but, so it wouldn't be like you're on your own, but yeah, it's just a thought as we're going through them. Yeah. I'd be okay with that. Cool. Yeah. A project that naturally involves interaction with pretty much everyone. Yeah. Doesn't necessarily need to have, multiple people full-time working on it. Kinda I remember in a previous new mentor hackathon, I think Charmaine did like interviews of everyone or something like that. And so it's it was everyone and also just Charmaine. Yeah. people that might make it easier as well in terms of the size of the teams. 'cause otherwise basically we have three projects, two, three person teams and one two person team. yeah. oh yeah. And by the way, if a three person team wins, then we can just order another trophy. Shouldn't be an issue. I just thought. Yeah, they're pretty heavy. So yeah. Bit much to get three trophies here. And then, and what do we do with the third one if the two plus the winner? prototype emitting telemetry data. So right now, every time you wanna do a visualization. You have to do bespoke artisanal changes to the entire Monte framework in order to get out the data that you wants to visualize and put it in the right place and then build a visualization around it. that's fine. But the problem is then if you wanna change it, you have to go back and again, change a lot of, Monte code. The example of this was Rami recently, wanted to visualize some data, so we had to punch a hole through the entire framework, so that the data could come out of where it was being collected.

we have the technology to not do that. it's basically, if you think of how logging works. And you just say, get a logger and then logging.info, and then out of band, all those logs are magically corrected, collected into a spot, et cetera. The idea here is to make collecting telemetry data as well as snapshots, which would be a name for screen buffer that you wanna visualize, for example, using the same pipeline as logging does. so the benefit for the community would be that this is a standard mechanism for selecting what data you want to visualize into your telemetry stream. The way this would work is just like in logging, you can turn on logging per module. you could turn on snapshots per module. So let's say you have, my favorite learning module dot pi. you wanna get snapshots just from that type of learning module, then you just, you would configure, Monty to have like trace level four snapshots for that module. And then, those screen buffers would just, those screen snapshots would just show up in your data stream and you can, and then we will have handlers for, to grab this data and put it in whatever format needs to be done. So just buy, save it binary onto disk or something like that. The other benefit is you are not, this is all will be streaming, so none of this would be like collecting all the data in ram, and then dumping it into disk or something like that. You would be, you would not be collecting this data in ram, which is important as we increase number of learning modules. and it's a, so it's a, it would be a standard way of collecting. Any sort of visualization data that you think you need? it would, you would, every time you write a new module, a new something, and you're like, Hey, there is a snapshot thing that I might wanna collect, you would write it out, make a snapshot like you would do logging. and then it also reduces the amount of codes to modify when adding new telemetry code, because you don't have to come up a bespoke mechanism every time you have a new, object in the code. You just you don't invent logging every time you need to log something else. You just get a logger and you log, you would get the telemetry data thing and you would emit telemetry. That's the benefit for the community. Just way with this cognitive load, the, existing buffer class, probably a lot of that could be removed, after that was implemented. I'm just, because there's a couple things that, like the previous location. It stores it. Yeah. It still needs to store off the observations. So you can update the model at the end of an episode, but you could remove the stats. Yeah. Okay.

And so the goal, like if everything goes well, would be to have that integrated in Monty or would that be follow up work from this week? This is a prototype, so this would be a fork of Monte just demonstrating of how this works and that it does work. And probably grab like one visualization and rewrite it all. Visualizations would have to be rewritten, so probably grab one visualization to rewrite it. Accepting a data stream versus this one J object that everybody expects.

Yeah. It's also interesting you mentioned about it not going into ram, It just writes it directly to disk every time it, stores something. Yeah, just like logs. Yeah. It just gets dumped. I'm just wondering how much of a performance that we're getting when we do detailed logging just from accumulating Yeah. Image data and, as long as you have Ramy, don't get anything. Yeah. But you can't run anything with detailed logging. That's like more than 10 learning modules. Is it? part of that is, I think the Jason file gets stored, but so maybe part of this would be, yeah, thinking about how we store the things like imaging data as well. I just wondering if you could add filters, yeah. in the same way you have logging levels, yeah. You get just, you get filters. So like by default all the Snapchat would be trace level, so you'd have to turn off tracing trace level for that module. But then you can also specify the name of the module. So you like, I want trace, like you could just specify I want. These learning modules. And then even if you turn it on, you could add a filter. So I only want it to come from the first one and then ignore the other ones. So there's always performance penalty for storing all the screen buffers that everybody ever sees, but it's, it, would be just like this standard way of dealing with it like we already do with vlogs and metrics. Cool.

Next one. next one, reference Brain transformations library. This is the thing that Scott talked about where all the data has metadata of what ref, so trans, I'm trying to remember the, current reality problem with it. So reference frames are detached, like the variables that store like a location or oppose are detached from reference frame that they are in. So in order to. Implement any transforms. You have to do two things. You have to do the appropriate combination of low level operations, which are stored in, like transform tools or whatever that, that Python class is called. And you have to remember which reference frame the variable represents. So you have to just pull that out of the air from your knowledge and then assemble it. And then you have a transfer as opposed to the alternative, which could be you have a variable and you just say two body coordinates and then it just does it. Or you have two disc sensors coordinates and it just does it like the API is very different. so instead of like right now we have to specify how to do transformations and we don't have all the information in scope. The idea here is to have a reference frame transformations library that has all the scope attached to the variables that you're dealing with. And you don't, you stop specifying how we just say, what I want to go from this sensors reference frame to the body reference frame because I'm voting and then in the sign of learning module, okay, I have this in a body reference frame, I want it in my reference frame. And you just say what you want and it just does it because all the data is metadata is attached with the variable versus Now, every time you're trying to do that, you have to figure out the how.

so what do you mean by attached to the variable? Can you describe, imagine you have, like a unit library, so you have meters. So you're never handling like integer seven. You have a variable called meters with a value of seven and you can do a dot two millimeters on it. Okay? And you'll get another variable that represents in millimeter, like two millimeters would actually be probably serializing something into two millimeters, a string. And then you'd be like, got it, 700 or something like that. Versus you never actually have to worry about the units because it's always the correct thing. And you have a function where you expect, expect millimeters, but you'll get this generic type of, measurement. And then in your function you just say whatever. You get two millimeters to make sure you're in the right units that you care about. You don't care what number is in there, bill give you the millimeters when you ask for millimeters. You also can, because it knows that it's represented in meters and I need to multiply by a thousand or something like that. You wouldn't, you couldn't accidentally apply the wrong transform for something like, 'cause you assume it's in a different reference frame than it is. PI, it's Python, so Yeah, you absolutely can, but like it's easier to, make that mistake. yeah, and I mean we could talk about this more if we do it, but I feel like there's Yeah, a few approaches you could take with this. Scott and I were talking about it recently 'cause yeah, I feel like this is the most kind of advanced and powerful approach, but probably requires some sort of graph representation of all the different, like spaces and how those are linked. One thing we could also do is just attach metadata to each, variable. And it would basically check for this and it would also make it easier to see, okay, what reference frame is it in and with respect to, but, so it wouldn't solve the problem of oh, I just wanna embody. But it would make it much easier to know what is the series of transformations you need to do to get that.

and that would be just a case of kind of Yeah. Adding a bit of metadata and then changing the transformation functions. But, yeah, especially 'cause you can't always do each transformation if you don't have extra data. what's the, what are the hypotheses? It's not that you can't transform. Yeah. And so I guess that's the potential danger of like in some ways it's nice that we have to think about how you actually transform between these spaces. 'cause it forces us to think is this reasonable? The problem is, it's like the other extreme right now where it's we don't even know what reference frame this is in. But if you basically had, in the same way the kind of robotics notation, the it's this rep coordinate system with respect to this coordinate system that could just be two pieces of metadata. So you could always like print that out if you wanted to know that about a variable. Then if you're transforming them, any transformation function will basically check. Does the bottom pres script match the top pre script of the other matrix? If so, it's a valid transformation and the output is gonna be this pre script and this pre script. If they don't match, then it's invalid. Yeah. that would be simpler. and already a lot better than what we have. But then the advantage of this full one is like when we start having like multi-joint agents or something, it might get really complicated to say okay, wait, how do we go from finger to shoulder? Or something like that. And then maybe having an automated system to do that could help. Yeah. Yeah. We can talk about it in a little detail. If, that project, so then it's wrapper, for customizable interactive visualizations from Rami. I, I could pitch this one. I don't know if we're gonna do it, but I could pitch it. It's, mostly that, users would be able to mix and match visualizations now because, I, have a pending PR for just having the widgets separated and talking to each other through, pops or like topics. So they don't really, they're not, intertwined in the code itself. so what would happen is we could have a wrapper that goes over these visualizations and you could bring in some visualization, like J'S visualization with mine and just put them in separate renders. and you wouldn't have duplicates in widgets that are required for these visualizations. For example, if you like ho J's visualization and mine, they both need a slider for the episode number or something like that. Then. The wrapper would be smart enough to know I, I'll just put one of them, put all the controls somewhere and just put all these visualizations together. So when you change one control, that control will just publish to the topics that are required. and all of them will change at the same time. So you can just design like the UI of, whatever larger, visualization that you want, and it will just bring in, parts of visualizations from other, self visualizations I guess. So that would be the idea. and we could use this in, presentations or whatever. You could just very quickly just define a little piece of code that says, okay, I want this part from here. I want this part from here. and, it will just make up one visualization where you can change them all with the same widget.

Nice. Cool. Yeah, that would be cool.

Yeah, I guess make Lego robot reproducible from Will.

so this one is taking, that everything is awesome teams Lego robot, and then replacing all of the Lego parts with 3D printed parts. So that we can have a link to go and buy a bundle from some reputable 3D printing company to reproduce that robot exactly every time and not have to worry about all the, bits that are difficult to get a hold of. so the project is essentially because this would require like really long cycle times of going to a 3D printer and getting 'em back. This would be just designing the 3D cad pieces and doing as much of the pre-work as possible for getting everything ready so that we could start doing cycles with, 3D printed parts after the hackathon week. the benefits of community is that it's like the first robot that actually moves in the world autonomously and measures things. And it would be really exciting for the community to be able to like, get that and play around with it and reproduce it and make it, yeah. Would you need, experience with making cat models work on this project? No, I don't. I don't have any. So we'd all be learning as we go. How difficult. It's just reference frames, right? we all have reference frame.

Yeah. Are you thinking of making the parts pretty generic so people can configure them in different ways? yeah. As making it more modular as well might be like a good, goal to have, like getting the actual robot bits so that the original one works. But yeah, having it modular as well would be really useful. So you could add multiple sensors and move it in different ways. Maybe have more axes of freedom.

Nice. Yeah. I'm just trying to remember the little robot or whatever it's called. 'cause that's like an open source one from, hugging face, I think. the new photo guy that it's like an arm desk or it is this thing, it's called, I think it's technically called. So arm 100. Yeah. Yeah. But it's 300, or something like that. Or that's for two arms actually. I. I think the main issue, if I remember correctly, is, I dunno if the camera is actually mounted on the, arm. Like I, I think it's more at the base or something like that. Okay. But anyways, I'm just wondering. Yeah. it could be interesting. It is probably beyond the scope of a hackathon, but whether there's like a way to, for someone to implement mon using this and then that kind of opens up other stuff. But Yeah. Yeah, I thought about that. these companies, like they, they seem to change their arm really fast or then go outta business. They difficult to, I wanna, pick one where like in five years you could still do this. Like it's still, the CAD diagram is still, still available. you have an idea how much it would cost? Two, three. It's pretty cheap. Like I was looking into the 3D printing, like ones that just, you send 'em the CAD file and then like weekly you get back the, 3D printed objects and it's like super cheap. So people could, it wouldn't be like a huge thing for people to No, especially with the simple one. It's just like a moving whatever, whatever. it wouldn't be too expensive. I think it would be cheaper than the Lego version. Okay.

All right. Next one. That's it. so if you crossed out extract habitat sim wrappers as a separate project. Okay. So having that some dependencies and python requirements are holding us back from upgrading the rest of the monthly stuff. So the work here to be, to create a separate like a TDP, that simulator underscore habitat project that would compartmentalize our habit habitat SIM interactions. Taking advantage of Jeremy's work that already isolated like the simulator protocol. So this would be putting the simulator protocol into like interprocess communication or some kind of network, communication and, develop mechanism for controlling and interacting with a simulator that comes from a different project by TDP simulator habitat. The benefits to the community, Monty could be installed from pi. Pi. So PIP installed Monty, PDP Monty. Monty could use the latest Python. Monty could use the latest dependencies. Monty could run on arm 64. Native Leaf Mon is be much easier to measure Monty performance because Habitat simulator would be in a separate process, completely separate thing. So whatever you're measuring on Monty, you're just measuring Monty. all the installation problems would be associated with the simulator package, not the Monty package. Wait, is that 'cause the capabilities paper? Flops there? Were they including the simulator in the numbers? No. No. Okay.

no, but it took months of work to do that because Simulator and Monty were in the same spot. Okay. yeah. So we cannot bespoke instrumentation of all of Monty versus just run perf from Monty Sim process. Ah, okay. Yeah. so installation problems would be associated with Habitat sim versus Monty. someone else could write a simulator that works in the same way, but it's a different simulator. So it would be like, it also specifies like an integration point for other people of Hey, bring your own simulator to Monty as long as you're meeting the simulator protocol. and you have an example of how a different simulator works. And then we also wanna do this as part of our, this is also part of our platform 1.0 roadmap. Anyway. So sold me. Yeah. Sounds pretty good. Yeah, it's that darn back everywhere. to the discussion this morning, is there any reason it's like a security concern when Python's support lapses as well for people to install? mon yeah. TPP monthly, I'm trying to figure out how to not have the wrong conversation about this because when we, I don't wanna have to justify upgrading because there's a problem when we, like, when we are not upgrade, we are missing out on, so this expired, the Python we have is from 2000, what year is that? 2009. Python three eight. They have pretty long lifetime Python, stable mostly. It's just embarrassing. Embarrassing anymore. So I think here we go. So it was, start laughing at Python. Python three 80. Created was 2019. So that's not that bad. So we have, missed out on six years of new features to the language. So it's not that it's gonna become insecure or whatever else is, we are running six years behind. We are building the latest next thing in ai. And we are using technologies from 2019 before Chad GT five existed.

it's a hard sell to me. all the library. The other thing is because we're using an old version of Python, all the libraries we're using are now going obsolete. So now mpi. We are MPI version one. The reason we are MPI version one, because MPI version two doesn't support Python 3.8. So now we can't upgrade to MPI two. And it's do we ever need to upgrade? It's like at certain point people will not be able to install Monty because it was gonna be old crap that nobody knows how to run anymore. This is the life of software is we always have to update it. So it's so it's always, I, yeah. To be clear, I'm not arguing against upgrading it. I just thought maybe it's another benefit. Yeah. for this part it's less a security issue 'cause you're not running anything in production that someone can attack constantly. Yeah. It's more that is it supported? Yeah. It's supporting it. What is the user experience almost? Yeah, the library. especially as you get further along and distributions of Linux and stuff like that, start removing older versions of Python, like they're probably gonna remove, Brie already. Yeah. And then you can't even install it up, like building it from source. So whatever. Yeah. Yeah, because an installation problem, as soon as you go obsolete, then any new platform that comes after that feels not obligated to include it. Because it's obsolete. And so you rapidly become uninstall by anybody who has anything modern. yeah. Yeah. Lots of excellent reasons. Yeah. Cool. okay. Implement test bed for computational objects. So Viviane had added this one to this, but I can, pitch it. So we all know Comal objects is something we care a lot about. It's what we've been gunning for a while. but we actually need a way to evaluate it and see how we perform on it. we talked about this last Wednesday. There's a few ways we could approach this, but Omni Glot seems like a really good candidate for a variety of reasons. It would allow us to test that we can have stacked learning modules that are representing different kind of parts of objects as well as these compositional objects. but it would also enable us to, yeah. See if we can improve Monty's performance on Omni Glo. That's a very challenging data set still in machine learning, because you have extremely small amount of data to learn from. People in the community are interested in how we do on these more traditional machine learning, benchmarks. And although, we have lots of good reasons for not evaluating on and things like that. But actually Omni Glot is a pretty good match for Monty because again, of the size of the data set, but also 'cause it has actual like, movement information about how people were moving their cursor as they were write, writing these letters. yeah, it would just be a great data set to evaluate this. And it means after the hackathon we could go back into September armed with this, benchmark and tick off some of these really nice things that like Scott and Rami, have been working on with, for example, the, hypothesis resetting and the policies. and actually see how those perform. I think it's also a cool project because there's kind of scopes expanded further. So one of the other projects you'll hear about in a bit is about having these 2D sensorimotor modules that kind of are specialized for processing kind of textures and logos and things like that. If someone works on that, makes progress on that, there's scope at the end of the week to try and combine them and then say, okay, can we now do the logo on mugs, dataset, and things like that. there's a lot that we could do with it. So then part of it would be to also set up the logo on mug, dataset. I don't think that should be a starting goal, but it could be something to try. the other thing I didn't mention was, yeah, as part of this would be coming up with evaluation metrics and implementing those in Monty. yeah, we talked about, for example, how this kind of prediction error, is a really good way we can look at how well Monty is modeling the world without having supervised labels. Because as we go into the future, we're gonna have less. Less access to kinda supervised information about the world and what Monte is seeing. and so one of the only ways we'll have to measure its performance is like how accurately is it predicting the world? and so this would be one of our, I think our first measure, looking directly at that. Yeah. I guess if it becomes a two or three person team, one person could work on the, logos and Marks data set another on GL and another on performance measures for That's true. Yeah. Because Omni Glo will not be a good data set to test policies, for example. but since Omni Guard is already pretty much set up and mostly they requires like evaluation metrics Yeah. It sounds like it could be so reasonably within That's true. So to try both. That's true, yes. Yeah. And I guess just to give a bit of context with the composition of being able to model composition objects, this again, one of these huge milestones that will unlock. So many more capabilities or applications for mon like before it, it can model objects and void. the one, even if it's a composition object, it models it as one large thing. with it we can deal with pretty much every application has some background and, different objects, within a seam. So we can then deal with that. Plus we can more efficiently model things by decomposing them into different parts. So yeah, it is like a huge capability to add to Monte. Huge benefit.

Cool. What was the next one? Bounded evidence. EMA Neurons. Ramy. Yeah. I could pitch this one.

the reason for this one only came about last week when, I faced some problems with just using the raw evidence, AC accumulated evidence. scores, and these are unbounded. They could just go, they could keep increasing. We've seen evidence go 50, a hundred, whatever. and, there, there are a lot of problems, like when you're trying to find some formulas or like just mathematically understanding what's going on. It's very difficult to work with these, unbounded evidence scores. one, one approach that I took was just to divide the evidence score by the number of steps the hypothesis has taken or like the age of the hypothesis. And while this seems to work, it's not very much biologically plausible. it's not, it's probably not the way we want to go. Niels has suggested some different approaches that are like, more the, like a neurons, membrane, like membrane potential, where it would decay if it didn't get enough evidence and things like that. And I think this project would be more of an invest investigation into how to make, neurons more like how to make the evidence more biologically plausible, similar to how neurons do it in, in, in a way that it would decay and, increase exponentially. it also like plateau, when we increase it. So it would be bounded between zero and one or negative one on one. and we would basically be using something like EMA, exp, exponential moving average or something like that so that we could get the same effect, of that bounded evidence. So basically I think what we're thinking here is that neurons will basically be supported by evidence. So as they plateau towards one, they will keep, if they're getting e evidence, they'll just be getting closer and closer to one, but they're gonna be tapering off at, one, But then if they don't get evidence, they will start to decay just the membrane potential will decay, if it doesn't get spikes. the difference here is that I think the, the Integr on fire model, all of the integrating fire models, they just keep integrating, the charges that as spikes. So they keep adding them. they're not really bounded. they're bounded by the threshold, so they, when they fire, they just go back down. So we just have to think about how much biological possibility we want here and how to compare it to, existing neuron models and, what benefit we'd get from that and all of these nice things. Yeah. yeah, I guess that's it. If you Niels have anything else to add? Yeah, I was thinking, so I feel like there's a cool, a few cool things about this. 'cause this is like Viviane had already added some parameters that. If you select the right parameters, it is an exponential moving average. And so we've already experimented with a bit. so it's like we were already and thinking, oh, we should probably go back towards that. it seems better not really thinking about the biology that much. And then think about it. Okay. When implement, yeah, sorry. No, that's, true. as in that it was bounded and stuff, but yeah, I was thinking about it, implemented it, it didn't work well yet. 'cause we don't have efficient policies, so after we have the efficient policies, it took work much better. But then, and then at the same time, Rami, you implemented this like slope tracker, which was like an independent measure of kind of evidence that we had. And what's interesting is neurons will have, what do you call it, different compartments. Oh yeah, sure. Thanks. okay. Firstly, like probably see that, not really, but that's fine. I can hold it there or we can take it over because Yeah. yeah, anyways, so this is the membrane potential of the neuron. as remember was saying, as you get kinda like incoming spikes, it goes up and it accumulates that. But let's say you stop giving any spikes, it'll decay down and then if it hits the threshold, then it, then it has its own kind of discharge. but this kind of behavior, it's not exactly modeled by an exponential moving average, but it's very close. And I feel like exponential moving average is like a nice 'cause. Having networks that use integrated fire neurons is notoriously difficult to train and learn with. But we basically know already that exponential moving average would work pretty well. Like you say, it's just we need good policies and stuff like that so that we move around enough. And then in addition, when people model integrated fire neuron. They tend to model neurons as point neurons, so they treat this entire complex structure as a single dot getting inputs and having an output. but the dendritic compartments are totally ignored and that's where Numenta was unique for coming up. And then you can have multi compartment neurons where you simulate all of this, but those are even more complicated and like really hard to train to do anything interesting. Numenta was unique because HT M neurons approximate a bit of a point neuron and a multi compartment neuron. If we did an EMA neuron and we had the slope tracker, that would potentially be doing something like modeling the soma and our old, evidence count with the EMA would be modeling the dendritic compartments. 'cause these have, when they spike, they have a much longer time constant where even if you don't get further input, it stays up for much longer. Whereas the Soma, it's a bit more it depends on what's coming in at that moment in time and neurons integrate those two. So it's just interesting that, I don't know, all these different things are converging on what it seems the brain is doing, or neurons are doing. And then part of this week would be as well as implementing this kind of thinking through some of those consequences. 'cause one thing is, yeah, as Rammi said, we don't currently have spiking, we've talked about how as we move towards unsupervised learning, we want to have some sense of okay, these columns are cove a lot, so we need to associate them. So if we actually start having spikes and measuring that, like that would be a natural way of counting that kind of co-occurrence and things like that. it could have some kind of interesting implications. I think it would be exciting for the community. Like I think they would be really interested to see us having a neuron like model and. we already had something similar, but I think it turns some people off when we say, oh, it's evidence count. and it's not clear that there's that connection to the biology. I think we can make that much clearer here while still having, while still not restricting ourselves by trying to simulate the brain. most of the implementation, so you would take the existing implementation of EMA? Yeah. And then add EMA for the slope tracker. Yeah. And then test parameters of how to, like, how to set the parameters for hypothesis, adding a deletion. Yeah. And then Yeah. See how well it works. Yeah. and then there might be a bit of like theory side of just let's try and think through some of the other implications, like the multicom compar stuff and the spiking and if there's anything else, that like we don't necessarily need to implement this week, but are worth kinda having on the roadmap of oh yeah, we need to do unsupervised learning. Now we have our emman neurons. They're bounded. Let's add in spiking and start counting these and then, and see how that works. yeah. I guess my main concern with it is that if we already had the efficient policies, that would be great to do now, but now it will run into the same issues I ran into three years ago and it will not work well. And that'll be like discouraging and it'll not really, you'll not really be able to test the new mechanisms. You still have that issue in theory if you set this time constant for the, background evidence long enough at the limit, it becomes equivalent to the current one.

I don't think so. 'cause you'll have very low resolution if you set the pathway very high compared to the current weight, then yeah, you'll, you have to work on the not point, not one threshold kind of thing. Yeah. That, and you have to adjust that. The longer the episode goes and stuff like that. Yeah. But then I guess all of them would be struggling with that. So it'd be like everything was normalized the same. Yeah. But yeah, it might, be an issue, but yeah. Oh, we can, yeah, that's the project. Yeah, that's the project. Talk about it. Okay. All right. Next up is, Monty Flame Graphs. So this is, just building a quick prototype of making sure we can use flame graphs with Monty in an easy way. Basically, every time you run an experiment, you can get a picture of a flame graph to see where Monty spends its time. so this is perf, perf is standard tooling in the industry. Flame graphs on standard tooling in the industry. So there's no invention here. It's figuring out. Just the happy path for all of us to get those visualizations out of Monty. the benefit for the community is, it becomes trivial to rapidly identify the bottlenecks of wherever Monty spends most of its time. So if anybody cares about performance optimization, et cetera, they can look at a flame graph, see where the most time is spent and be like, okay, we're gonna fix that. And especially if they're trying to do robotics in the real world that, and you're trying to keep up with the worlds Be especially interested in where money spending it time. Yeah. so just having a story just in a tutorial of here's how you get flame grabs and the other thing, That feedback lo loop is, super helpful in being, making effective changes without just guessing and hoping for the best. so you can rapidly like he'll climb to really good performance using this type of tool. What's it with the existing profiler? It's just easier to use or, anytime you add some new code, like you have to update your profiler, like perf just works with any Python code. The profile too, I think we're not integrating it specifically, it's just, it a mix in for the experiment class and then it Yeah, but it's wrappers around all those methods. If we add methods and stuff like that, you have to remember to go and add those wrappers as well. Whereas with perf, it just picks it up for the interpreter. PERF works at a Python level, so like. I don't have to change any, I don't have to remember to turn it on. I just, I run it with the perf tool and it instruments the language itself and function calls inside the language. there's that. That's what the profile does too. I never had to change anything when code base changed. It just profiles how long you spend on each function. Paul, but maybe I'm missing something. I don't know. Like whenever I used to profile, I just added that. Why don't we use that for floppy? Why do we write floppy instead of using the profiler? I thought the profile, like the seconds you spent on each, not for counting flops. Okay. Seconds or milliseconds. Okay. Like the time. So that's work time. Yeah. The flame graphs don't do work clock time. They just do flops or time. On CPU instructions. Like how much it takes.

Okay. I'm not sure which one the profile currently measures, but it was definitely time spent on each function. Yeah, that's generally what you're gonna do with profiling. Like flops is very specific to mathematical operations. Yeah. And even like deep learning, it's become like a thing that they focus a lot on. Yeah. Doing a lot of point operations but generally you don't like you care about Yeah. These other things not pops 'cause it's sort dependent on Yeah. What hardware you have, implementation and all this stuff. Yeah, I mean I read the profile is probably more tedious than this 'cause you get like a large Excel spreadsheet and then you have to analyze it. Whereas this sounds like you get like a nice visual representation of it. but yeah, I guess just wondering, there are other benefits. Part of it's you're using, it's standard tooling versus bespoke tooling that you can use the stuff that's built for using the perfect data turn, play graphs, those things. The profile is also just some library we import. Okay. You have to rewrite anything to use that. You said it was a mix in Yeah, it's just as an experiment class mix in, it's just like it is a, I think it's a standard python profiler library. You imported it wraps around like the whole experiment yeah. You just put, you just use the profile experiment mix in. When you experiment config and then it at the end of the, it takes much longer to run your experiment and at the end of it you get like a spreadsheet with how long you spent on each function. So the other thing with PERF is nothing takes longer. Like it's just. The data is collected out of band by the operating system. Okay. So so this is the thing about if you're run perf there's no performance overhead. Yes. All the data is collected out of band by the Linux kernel. Okay. Versus this stuff is it's this is what I mean, difference between adding more Python to measure Python versus the kernel intercepting calls at a kernel level. Yeah. And so it sounds like for the same reason, it's very, or like credibly simple to set up, like even the first time, just put the word perf in front of your thing. Yeah. Some flags per what we use like in AWS, to compare with floppy, if you remember those. yeah. Yeah. Good. yeah, the only, I think the only, at least in our AWS, besides just adding like perf and then Python run, whatever it was like. There was some security, like we had to set the, level some right? Remember, say yeah, some paranoid level to okay, not very paranoid, but but besides that, yeah. Yeah. So we are using C profile library, for the profiling.

but yeah, I guess that's another advantage if the perf also can also look at flops if we ever wanna look at that again.

Okay. Cool. Yeah. Next project, implementing sensorimotor module. do you want me to do that? Yeah, maybe. Yeah, just it, that way you can relax. Rj Yes, please. Thank you. Okay. So yeah, implementing 2D sensorimotor module has, several benefits. lemme start with what it is. basically the idea is to have a sensorimotor module that's specifically designed to. Both output movement in 2D space. So basically how you're moving on the surface of an object versus how you're moving in 3D space plus detecting 2D features on that surface. So basically instead of the sensorimotor telling you're moving in 3D space like this, it will tell you, oh, on the surface of this object you're moving up down left, right? And I'm, detecting like an edge here. Instead of saying I'm detecting like a point normal and curvature direction here, because there might not be any curvature directions. like if you're on a flat surface, you wanna define the orientation you're sensing there by the edge that's printed on here, for example. and so that's important generally if you're dealing with 2D data sets. Like I implemented something basic for that when we did the ultrasound project, 'cause we got 2D images. you could probably use some code from that for the kind of edge. detection and coverture extraction. and then specifically in the context of compositional objects, this would be useful to model the logo, which is inherently like a 2D model, being wrapped around a variety of three dimensional structures because if you are just looking at how you're moving on the surface of those structures, it'll be con, it will be consistent with the logo model that was learned in 2D. Even though your sensors moving around like a curve or something, the 2D sensorimotor module would represent it the same way as if it was moving on a flat surface. so yeah, I guess the outcome of the project would be a new sensorimotor module class that outputs 2D movement on the surface, thus, features detected. And on that 2D image.

salient showdown, Scott. Okay. I've got the energy to go a cowboy on this one. so I think everybody is familiar based on we research meetings probably, this project that I, I've been working on, I'm just trying to recruit somebody here to help me with it selfishly. But the basic idea is like, this is a vision oriented project where, if you look at, can I go this? Yeah, please. If I pick up this bottle, I'm trying to recognize it. I'm not just going to randomly move my eyes across to any point on this bottle, which is what Monty does right now. It's gonna try to, in a very. Low level way, find the points that are most significant or saline and then just immediately attend to those. And then a few of those, after a few of those, I should be able to more quickly identify what the object is. And it doesn't require any memory about object that's seen or anything like that. So it's not like this model based, jump, which it's another topic to get into. So this is a, it is a model free, strategy. And I would say, there's like a pretty good, I've got like a pretty good fork right now that's like ready to go. And so the idea is essentially anybody who wants to work on it, you should be able to, after maybe a day's work or so, just drop in any kind of, function that'll just take an image. Oh, and from that image I'll output a scaler for each pixel and say, oh, that's a place I wanna look at. That's a place I wanna look at. And that's a place I wanna look at. And so in principle, you could spend a week just trying to find the function, function, find functions, fund functions, find functions that we'll, most efficient, and then, make it a little test bed, which it already works for the YCB data set, but we can make some use existing benchmarks or some maybe smaller ones so we can do it faster. So you should be able to, drop your function in, run the benchmark, see how well it did, like how, what was our performance improvement in terms of the steps, because other we're, that's what we're trying to reduce here. and so that's the basic idea of the project. I think it's, I think it's. The preliminary work to get this project is there. And so I think it would be pretty easy to drop in. and I would say this is also project that could work pretty asynchronously for most of the week.

for remote people could, it could work well. there would be like a little bit of early setup up to figure out how to get my fork going and maybe set up setting up benchmark and stuff that would be good to do, together probably. But beyond that, it's here's my function. Drop it in and try it and go, we can just check in and compare, whenever. So that's my pitch. Cool. If y'all are good with that, I said this, sorry, but yeah, it should be possible in terms of evaluating, just use the standard YCB objects now. Yes. Yeah. So then, no, that's perfect. Yeah. And oh, are you competing with the person on your team or, I think the competition would be who, what is the best algorithm? Yeah. And if you want to make it personal, then yeah. Have at it. okay. So I guess the other sort of addition to this is Viviane, if you're talking about objects, behavior, sence traditionally like in, in animals involves motion. having nice experience working with Salient could be nice for helping em out with that. Eventually when, maybe we wanted to attend to the parts of the scene that are moving for object behavior or something like that. And also that quickly if they're moving Yeah. Important. Yeah. Yeah. It's, so yeah, it's kinda a lot of like little bits like one in other projects where it'd be nice to have, even if we don't build it out right now, like the particular implementation that's gonna work with motion or whatever, but, getting some kind of. Development on that project would be good.

Good. Was that the last one? I think we'll added one. Yeah, I added one more. Alright. Which, just an excuse to show a video of Tristan.

can you guys on the call still hear me? Yep. Yep. All right. Awesome. so yes, this one's called, interpretive Dance of the Neuron. and it's essentially. The output of this project is each member of the TPP staff using neuron on doing an interpretive dance of that neuron. I think this would, in no way, so it probably won't be chosen us by entertainment. Yeah, us by entertainment. Let me give you, a quick example of what that might look like, if I can figure out how to share sound. All right. Here we go. share.

And where did you benefit?

No, actually, but there we go. Yeah. I'm sold. I did get permission. Brain AI mouth, make braining and we can go viral.

TBP team officially lose their minds. Brighten.