so yeah, throughout the week we did a lot of pair and peer programming on Brighton Pier, on the bright and Pier. I was gonna say pier as on the pier, right? Yeah. Yes. Is that picture, literally on the pier or is that picture went? We went there. Wow. Yeah, just that joke. That's great.

we also did some mop programming, which, yeah, basically Niels, Jeremy and I had a three person team, and so that was a, we found it a really cool format where basically we took turns every 15 minutes switching who's programming, and the other people would watch on the big screen and kind of, give input. And it's, it was really effective way to kinda make decisions together and figure out things quickly together. And we had also had a whiteboard to talk through things as they came out. Came up.

here has a short video of Martin learning on the screen. And then, some people, watching contributing learning how Monte learns learning, watching Monte learn. can say it again. There's multi moving around. Yeah.

and then, yeah, we also had daily check-in calls at. 5:00 PM UK time and I think, 9:00 AM pt, with, Rami and Hojae who unfortunately couldn't join in person. Oh, I didn't know that. Okay.

Yeah. Hojae unfortunately got sick last week.

and then yeah, one of the evenings went to a real British pub having some pines and British food and yeah, this is the prize that you'll get to award to the best team, yeah. Yeah. With the proper UK weather in the background It has a, an umbrella and a sun hat because you never know what you'll need. Gotta be prepared. Yeah. Is that, a, Monty Python reference in the corner of the can there. Glorious spa? yeah. You mean the, knight? I don't know. Yeah, I imagine they probably are like unofficially alluding to it, but I don't know. Yeah. There aren't any budget Glorious spam. No, but Mon wasn't the holy grail. yeah. didn't they have a skit about glorious spam or something like that? Yeah. Vikings in the skit. Yeah. You guys in armor.

Scott, did you have, did they give you any trouble in customs with the, you carrying those? The only trouble I had, with the, extra suitcases that I didn't really remember what it looked like after that. Oh, no. And you didn't put a tag on it? Yeah, I have to inspect the numbers, like pretty generic looking as well. Yeah. I should have put like a ribbon. Very generic.

Basic black suitcase. There's millions of them.

The one, the police were standing around waiting for someone.

okay. Yeah. So now the idea is that each of the four teams has a maximum of 10 minutes to do a quick presentation of what they did this week and why it was important. And yeah, just a quick presentation. it's great if you take less than 10 minutes, try not to take more than 10 minutes and for the judges hold your questions until the end of the presentation and then you get to ask all the questions you have. Sound good? Sounds good. I love those graphics. Yeah. Thank you. Question, how did you choose the teams? we first chose the project and then people signed up for the ones they were most interested in, and we just kinda made sure. We got people on each of the teams and tried to mix sort of departments, I guess a bit. Yeah. Yeah. I'm interested in their project, but I'm not gonna work on something about Mark timing. Giving like a two minute warning at eight minutes as well or sure. Yeah. I can give a two minute warning. I think we're gonna be done in four minutes. Yeah. Okay. Yeah. Cool. Let's try. I'll give, it's a lot of work. That's much to show. okay. so we've, were Team Python 3.8 to three point NE thing. the reason being that we are currently pushing a big boulder of a GI up a really steep hill. and the habitat simulator is forcing us to stay on 3.8, Python 3.8, which is making everything slower and harder to get this boulder up the hill. So that was our project. We need to stop Habitat Simulator from slowing everyone down. That's great. Yeah. So this is a pretty simple diagram of all of the problems and things that slow down the engineering process that Tristan put together. And Viviane and other members of the team have had input into this, but it essentially, it's a Riyad street. We are gonna focus this, project, focus on that green blob at the bottom, which if we zoom in, into here, looks like this. and so there's a bunch of boxes that this POC, this proof of concept was gonna try and, fix and deal with.

Okay, so this is the existing architecture, from, our proof of concepts perspective. So a benchmark which is currently constrained to Python 3.8 contains an experiment. Monty doing all the work and the Habitat simulator, which is what's tying us to, Python 3.8. So essentially a benchmark starts up and it builds an experiment, and then Monty starts requesting, patches from the Habitat simulator and then doing things with them and sending movements to the Habitat simulator. we wanted to get to an architecture that looked like this. so we have the benchmark, which again contains the experiment of Monty, but that can now run on any version of Python. we'll have a look at what version we got to in the demo in a moment, and then we're gonna have, we're extract the Habitat simulator out over the network, and make network calls to it instead of it running in the same process as the benchmark. we opted to use GRPC, which is a over network protocol that Google brought out in 2016, and it's pretty good at sending large amounts of data back and forward over the wire.

this architecture will also allow us to start, putting in different, simulators. So Isaac, Jim and c plus or any simulator that comes out, we should now be able to switch them out and use them much more simply.

Okay. So what we did, and feel free to jump in, what we did with a lot of this was Tristan and me peer programming, and, peer program, peer programming as well on the peer. so we did refactoring. So we looked at the existing classes and we found things that just we don't actually need in the protocol that we send to the simulator. So we got rid of a whole bunch of things and, had to go through and look at all the code that touched these and move them around. A lot of refactor 'em. we did pair programming and peer programming as alluded to earlier. and we did typing, lots of typing and lots of typing, adding types to all of the objects. Within our code. So you can see on the left in the red what it used to be and the green on the right, what it is now. and so on the left we can see it's just NumPy array and then 0, 0, 0. And that could mean anything. But on the right you now know it's a eular angle, X, Y, Z. And so those coordinate systems, if you look at this code in the future, you know exactly what it does, which is a great benefit to anyone who comes to our code in the future. it also made our code a lot smaller. so on the left we have a function that essentially clones some data. And on the right with the new typing, we have that exact same semantic stuff happening in half the line, less than half the lines. and so typing also allows us to write smaller and, more meaningful code, which is cool. that what the word blind, that what the word blind me means is it's a, an expression of surprise that comes from an English derivation of God blind me or God, me, I was actually. Wow. I knew that question would come up, so I, we need it. I was impressed. lemme check. Oh, it's not okay. Good ation. so yeah, so this is the code that now exists. this is the class simulated protocol. This is what, this is the python version of how you create, the simulator instance. And it has functions in it. I'm just gonna put one in here. The add object function, which takes a name, the position, the rotation, the scale semantic id, and if there's another object in there, if we're doing something important I don't know, compositional objects or objects, occluding other objects. That's what the primary target object talks about. and then inside there, there are some functions that you can call, take a step. So the next step in the experiment, reset the experiment, close the experiment.

on the simulator side, the GRPC side, there is a receiving object. which is written in a, system called Proto Buff, which is this thing that Google came out with to allow, which is what they use internally to do all their network data transfer. and so you can see there's that re ad object, the step reset closes, removal objects, and then this is what a message looks like. So the thing that will get sent across the wire is essentially set up here. and then there's some more interesting data types. So when you make, whenever observation is returned, it contains an agent observation, a sensorimotor observation, and the repeated sensorimotor observations. And that's it. That's the code not gonna go too much more into code. There was thousands of lines changed over the last four or five days, but it all resulted in this exciting piece over two.

And so the demo is, there's not much to see out of this, but what I want you to focus on is the idea that on the bottom here Oh wow. It looks much more lively on my screen, but yeah, actually it's probably more readable on your screen. It's moving faster than we can read, on the actual monitor. So on the, bottom, on the right side here, what's important is that Python 3.8. So this is the simulator running, it's actually 25 simulators. running, those are just, different numbers of the simulators that are running and they're getting messages. They're moving the simulator around and sending back what the simulator sees, back to Monty. And then the exciting part is up here. that's Python three point 12 and that's where Mon team is running. Python three point 12. This is, yeah, this is where I, yeah. Started, So I'm, sorry, this is the new stuff, right? but isn't three 12 older than three eight. No, it's New York, you think? No, it's why, would it? It's, I think that's, it's 3.8 is became obsolete last year. 3.9 becomes obsolete. oh, I see. This is three point 12, not 3.1, 0.2. Got it. I'm sorry. Yeah. Sorry. That's newer than 3.8. Yeah. And So yeah, so that's it. It just, it's Devaluating. We're 95 episodes in. This is a normal, this is what a normal benchmark run looks like. one caveat to this was we can run on Python three 12 and about 10 times slower. So that's exciting, because everything has to go across the network. And so the serialization of data of essentially just sending screenshots across, the network to between the two processes makes it about. I would say probably 10 times slower than, if we just all run it in the same process. So did you anticipate that? yeah, some level just didn't know how much slower it's gonna be. So 10 x slower is interesting. Hard to tell. is that still make this valuable or, that's, it's a big hit for this separation, right? Yeah. It makes it questionable whether we should adopt it or not. So we'll see next hackathon you can figure out how to make it faster. Exactly. And you can mention some of the other benefits you mentioned during the pitch of any problems with installation are now associated with that. Yeah. Yeah. Just, yeah, lemme see if I can, yeah, I wonder even just things like some of the basic transforms could maybe happen in the simulator. so just. Cutting it off all the benefits that we listed for it. kinda going into what will highlight it, why this would be good, if it worked faster, we could install Monty from, the way people usually install everything in Python pip install Monty. Right now to install Monty, you have to do, you have to use a con environment, which makes the setup more difficult than, what people Python people are used to. We can use later Python, we can use latest dependencies. A bunch of dependencies are updated in this, three 12 version. we can run on, our MacBooks natively. Right now we are acting like we are on Intel PCs. we're lying to the environment, but we could run on MacBooks, natively, which is what the new version does. we can measure multi performance separately from the simulator because now Mon is just running by itself and Simulator is a completely separate process. the big one is any installation problems. So like Monty would be just install Monty and it's installed and all the installation problems we can now blame on habitat of Hey, it's not working. It's oh, but that's, 'cause you're installing the Habitat simulator. So that's Yeah. That's the habitat problem. So that's separates nicely the blame. We, would start getting the blame for it and then, other people can write simulators as will alluded to already also, like installing it on Windows is currently Yeah. Difficult for anyone because of Habitat, but people don't, realize. It's because of Habitat and then yeah, contact us. Yeah, so Habitat is a significant constraint, so this is one way of removing it for 10 times slowdown in the POC. We think it'll get faster. We'll see you, I think you can faster. Is that 10 times? some of those other benefits are quite significant. So is the 10 times, faster, is that overall or just this piece that you tested? it's just a guess based on what I've seen. It's not a, I wouldn't quote that, it just in order of magnitude. I wouldn't, that would, be an entire experiment, not just this section of the Yeah, We'd have to see how to have to measure all the benchmarks, et cetera. This is like hacking through a benchmark to get things working. it's probably to get a representative number that would require more work. I'm just curious if it's 'cause the benefits are great, but the speed down, is pretty bad. So it's yeah. I feel, by the way I'm getting, I get, I'm getting a sort of a weird echo when I talk. I'm not sure if someone's got their mic on or something. Maybe Terry, if you mute yourself, it looked like it was coming from her. Yeah, and I, feel like we've never had to think too much about the amount of information that's being transferred, for this sensory processing and things like that. So yeah, it feels like there's a lot of low hanging fruit that could probably optimize, optimize this, like particularly the images, We are kinda sending across and yeah, the beauty of Monty is that it needs very simple input features and so it's probably excessive what we're sending right now. Yeah. And also with the touch policy, we could send four actions at once and then just get back. And the encouraging thing is we're now living a time where I can play high resolution video games over network using a cloud computer in real time. So like we know that sending data fast at, at human, like 60 hertz plus is possible. So it's like we just haven't done it here, but that can become an entire effort, but it's not impossible to do it very fast.

Got it. Tristan. Excellent. All right. Just a question. You said you're, running 25 instances of the simulator. Yep. Could that be why it's slower, or is that not, that's not why it's slower. 'cause today I learned that when we run benchmarks, every episode creates its own simulator. Oh. When you run parallel and there's 16 CPUs, then you have 16 simulators running.

Yeah. Interesting. Yeah.

Any other questions from the judges?

You wants to go next? I'll go. All right.

So OJ and I paired up on something we've been wanting to do for quite a while, which is to improve our, our model free, secs. So secs are eye emotions, you, dart around your eyes to, to, look for, to scan objects more quickly and identify, what the object is more efficiently. So let's say in real life as a human, you encounter some unknown entity and you need to rapidly identify it, by scanning your eyes across it. 'cause it may be a vicious beast and you may need to run away from it. You may need to react to it. So in reality, give it up what you might do. What's on the left here? You might look at one of, one eye, look at the other eye, nose, mouth. You may, you're not aware of this, but your eyes are gonna be just darting around to these most sa salient features, most interesting and informative features. On the right here is more what Monty is currently doing with the distant agents. Yes. Nice length. Yes. As you can see, it just starts somewhere and then it just is does this sort of random walk in the fixed lengths? It may or may not find anything interesting, informative for a while. So essentially, Monty is a bit like this and where we want Monty to be is more like this. Just make a full, a few, cool, calm, collected motions and rapidly identify what the object is. By the way, who, whose dog is that? Who? I think it was yours, Hojae. And then, and we're just talking about the model free, eye movements. Yeah. because we still have model based eye movements. This is just the, the model free ones. That's right. Yep. Yeah. Okay. So better sound detection. Yep. Interesting. Just to ask yourself the question, how do we know that dog is friendly? It's really, you can tell right away the dog is friendly. what features of the face tell you that side point. It's just interesting. Okay. So the idea of this week is we're gonna have a salience show down. We're gonna try to, ence metrics. Metrics, and we're gonna try to possibly pit them against each other if we can, or de, but definitely pit them against what Monty has right now. And we're gonna see, who, who walks away from this fight and, who's the fastest draw on the west? is this a suggesting you really are trying to get that spam trophy? Is that the idea there that you're, plug? No.

And what was the prompt you used for that image? No. So the very first thing we had to get going was what's called inhibition of return. So if you have a really salient location and you aren't encouraged to explore other parts of the image, then there's nothing preventing you from just staring at that one really salient, interesting part or constantly returning to it. So what we're seeing here is on the left we've looked, we've started our gaze at the center of the cup and that little dark spot is indicating, lower likelihood of returning to it after a couple steps. We've got a couple of these points now. So those are spots that we don't wanna return to. And as you can see, over eight steps, they accumulate and it helps us, push. Pushes us to explore other parts of the object and not, okay. So the, red arrow is where you did, fixate ready and the gray, areas where you're fixating and red, where you went were already, is that what they, the gray actually indicates where you're gonna go next. It's just I see. Part of the visualization. I see. Okay. It's next, but the other ones would be where you have been, correct? Yep. No, so this is just a part of the piping. We had to get going first, to, in order to, start working with the salience maps. But even this alone, we went from taking an average of 39 steps at best with a random walk policy to identify an object all the way down to 21 steps. So just that alone gave us like a pretty enormous per, performance benefit. So it's a good start. So there's a, there's a question there about how you did that, because. you're trying to hit, you're trying to find salient spots, I assume, or, not yet. Maybe you're just, all you're doing right now is just inhibition of return and you didn't really try to improve, you didn't really focus on saliency. at this point there's no saliency. It's just uniform on the object. This is just Got it. Okay. Yeah, because there is a trade off between saying, oh, don't go back there. But, also, like some places you could go are not very interesting. So Yeah. it's a, balance between picking salient spots and not returning for sure. But right at the moment you just, right now you're just saying random don't return. Exactly. Yep. Yep. Or at least don't return for a while because these spots do ease over time. Like you see that central spot, it's like less pronounced after eight steps, so Yeah. Yeah. So you get eventually returned like maybe after 10 steps, but you're not like completely like you're barged from going back. Yeah.

But a, after the inhibition of return, we did test some t non deep learning based sail LNC methods. So I showed, here are the like four CNC methods that we kind focus on. There's a lot more, but we didn't implement them all. One is called spectral residual. So this one, basically does fast forward transform, which mean, we're gonna filter out for, low frequencies and keep like the interesting parts, which, in this case are high frequency areas, usually edges. the bios one on top right is, something bespoke that's Scott made. maybe you can explain that a little bit more 'cause we're gonna focus on it. minimum barrier is like a, recent as in 2014 computer vision method that it was kinda focus on trying to pick out regions, pick out foreground regions from backgrounds. So this is a little bit better for kind of segmentation like. If there's a cluttered background, this is good for picking out areas, in the foreground. And then the last one is id k so this one, this is a very bio environ method that kind of combines, Gabor filters, has implementation related to like centers around differences. yeah. so this is, like a kind of a complicated, more complex bio, based method. next slide, Scott.

Yeah. I've also tried this. Oh, the different, yeah. Muscles. I've tried it out on natural images. just outta curiosity, this is just one example, but it makes a demon dog, basically. So this is Arthur and the, good boy is Arthur. when he is sleeping or low energy, the demon boy is when he's having zoomies. That's what I see. When, which filter, which method is this? spot. Okay. This is ID coke. Okay. Yeah, you can see his eyes pop out though, which is what you want. You want to be attending to that ing. Yeah, but it's And teeth. Interesting. Yeah, the teeth, but also this, there's something above his, right ear, which is not really interesting at all and it's highlighted very much, which is, yeah. Also feels like a binder, some kind of model free segmentation method to help keep constrained motions on the object. Yeah, I had to look really hard to figure out what's going on up there. It looks like it's, 'cause it's blue maybe, and there's no other blue, yeah. That's weird. That's weird. This one doesn't. Yeah. Yeah. Yeah. So for example, we could apply like different Saul methods, like if I apply the foreground background segmentation first to get like Arthur out first and then apply, this I code, then we will focus just on, in the interesting parts of Arthur. So depending on the ency method, but what it picks us as interesting is very. Method dependent. And also, clearly it's just not, this doesn't capture the biology completely. 'cause it's a bunch of artifacts in it. It's just like, why? yeah. But it get, it gets the teeth. That's pretty good. Yeah. Yeah. And unfortunately, if you need to run, so the method we're gonna be working with for the reiter of the talk is this, it's essentially a combination of filters that, are known to exist in early primate visual system. So we've got luminance based filter, depth space filter, red green, poncy, blue, yellow poncy, spectral residual, this one is this bottom right one, favors, Continuous oriented regions. And so it's just this, a collection of six and, we can weight them. There's gonna be a lot of tweaking and waiting and playing around with, but this is our, you describe spectral residual. I don't, I'm not familiar with that. Okay. It takes, so it's pretty similar to the one that she, Jose explained the, what the FFT based one was looking for, like higher or low frequency content that's above the average. Okay. In the area. Yeah. So this one actually needs quite a bit of tweaking the special residual. like especially the one that came out of the box. Open CCB looks a lot nicer, but there's a lot, there's still a lot to be played with here in order to, to get to, to play a little bit better. But yeah, for spectral visual, think about moving across the image. let's say you're going through a line, it'll be high frequency if it suddenly encounters an edge, right? or think about it that way. So, spectral visual is good at kind of edge detection. I'm not sure if that helps. Anyways, go ahead. Are we doing on time? Yeah. Two minute warning by the way. Okay. But I think it's not your fault. I think it's, maybe try and hold back your questions a little bit more and then we can discuss it in more depth in the research meeting. Okay. Oh, sorry, that's me. we did get our improvement from 39 average steps per episode down to 23, which is great. It's actually a little bit worse than, in inhibition of return only. And we thought, maybe the reason for that is because like we might actually get a bit better performance if we pre-trained our models using the same salience driven motions. Then if we use the ones that were, pre-trained on what's the sort of naive spiral scan, which just takes even points all across, it's a very machine kind of artificial way to learn models. So we then went and we decided to learn our models. Again, this is what the default is right now, the spiral scan, and this is what it looks like if you only C co to or you primarily c co to the most interesting points this year is the spam can or the potted meat can if you prefer. and this is what it looks like if you c co to the, the most salient points. And that actually did help. It drove us all the way down to 17 steps per episode. and that's fantastic. That's nice. And one of the things we hope to get outta this is sparser models and, we'll have to get into that, later. But, here's the basically summary of results. This is our new state of the art. So if we just learn our models with a salience model that we've got and do perform inference, we both have. So the top row is what we have currently and the bottom row is what we have now. So we've more than hald the number of steps. That's a confusing way to put it. We're more than twice as quick, at recognizing objects now we've drastically reduced the number of timeouts, which is actually a huge time suck. It's hard to quantify it and put in the same, category of steps, but it's, really significant, change. And then we've also boosted our accuracy a bit. So it's super promising. I'm really excited about this. and I think that's, oh yeah, audio is random mock. Yeah. I think we have a winner. We're very excited about this. It not only do we get what we wanted, but we were able to, to do it in a bio-inspired way, in a principled way. And so it's a, pretty exciting result for us. So thanks for listening. Very nice up as well.

That's great. That's very exciting. You've been holding back these results. Yeah, just every day we're checking in, they're talking about the results and today showing like. Boom, drop might drop. I just, so I'm not taking so much time here. it's interesting. Will and Tristan had this green back, this green theme, strange saying, you guys have introduced the red theme here. Going on now we have blue. Yeah. This, I don't know if we would time for questions or if we wanna move in or, yeah. Do you have any more questions? I was just chastised for asking too many questions.

maybe do you have questions that you'd like to have answered to make a informed, decision about the winner of this, I, don't know Terry Celeste, but I have a question similar, the question I asked of, of Will and Tristan, which is, this is one part of an overall system. This is the model three part, and what's not clear to me how this model three part interacts with the model based part.

This should get us to model-based decisions much quicker. Is the idea, like if it, normally takes 20 steps before you make your first model-based, motion or motion that is, derived from, some kind of model-based hypothesis. This should get us there in at least half that time. And then after we go to model based, do we stop using the Ency based completely? Is that how it works? Or how does it'll continue? It would just go back and forth. It and then there is a place that, that integrates those two in the code. Yeah. Yeah. Most of those steps are model free steps and then every once in a while it takes a model based step, so it keeps switching between the two. And, but it doesn't take a model-based step until it has some kind of hypothesis, got it. In the beginning. If it doesn't have any hypothesis, it can't really make an informed model-based. Sure, of course. Okay, cool. Nice. Thank you.

Yeah. Rami, do you wanna go next? Okay. So this, this presentation is about EMA evidence bounded scores right now. what we have is evidence scores that just grow unbounded. Every time we get a little bit of evidence on any of the hypothesis that Monty has initialized, it just keeps adding them, infinitely until just the evidence keeps growing. there are a lot of, downsides to this. I will talk about these, in a minute here, but I don't know what e what does, EMA stand for? exponential moving average. So we are averaging the, the evidence so that it Okay. Yeah. Yeah. Stays bounded. Yeah. Yeah. so the, I presented a little bit about this before, but right now in Monty, what we have is we have, e an experiment. And experiment is composed of multiple episodes. Every episode gets its own object and Monty is just trying to recognize that object. After it, recognizes it, we move on to another object. We reset all of the hypothesis that we had, that the hypothesis could be just every hypothesis, like a, I guess about the orientation of the object and where the sensorimotor is on the object right now. So we reset all of those hypothesis and we, initialize new ones that match the observation that we get from the new object. this has problems when we are doing compositional, modeling or composition inference. and I think, Viviane will, and Niels might talk about that in the presentation, but mostly, what this has led to is the idea of hypothesis resampling, where every step we're always, resampling more and more hypothesis. And we're also deleting the hypothesis that we're not performing very well. but actually, initializing hypothesis midway of, or like at, any time in an episode does have problems. and one of the problems is how do we initialize it? What kind of evidence do we give it? this is one of the problems here that we're dealing with, is. We could have a hypothesis that's initialized at the beginning of an episode, and it's just, it's not a very good hypothesis, but it has been there for a while. It's accumulating evidence, at a lower pace than another hypothesis that is just added maybe later in the episode. and it's accumulating faster, but when it's time, when it comes time to evaluate them, the one that is, that has been added earlier, actually has high evidence. this is a problem. 1, 1, 1 simple solution would be to just use the slope, of the hypothesis, basically normalized by the age of the hypothesis, just divide by how many steps it has seen. But, there are some concerns about, how, like whether neurons are able to do that, whether they're able to like, keep count of the age of the hypothesis, stuff like that. but it's one of the, approaches that we're also considering. So the, goal of this hackathon was to test, the idea of, keeping the, evidence bounded, in a range. So basically we're doing this exponential moving average, where at every new step we, we get the evidence that, the input evidence, the step evidence, basically multiplied by some alpha and then add to it one minus alpha, multiplied by the old evidence. So it's like we're always averaging and it's always going to stay within the, bounded range of the evidence. this has a lot of benefits. this basically, hopefully, it will solve this problem because it's, the evidence is always bounded. It, just, it's just going to plateau once it reaches a certain point. And I'll show you some profiles of what it looks like on real data. This is not really real data, but simulated.

also another, honorable mention here for, the idea of, membrane potential, where we're also thinking of using, so again, this is also bounded, we had to do a few tricks here to get it to be bounded, but it's basically, also bounded. And the idea is that we would have, we would have to, in order to get this to work, we'd have to, threshold the evidence scores so that we can convert them to spikes. and then every spike is just depositing some charge on the, just basically adding some, input to the circuit and or to the mind brain, and, and then it's decaying when it doesn't get spikes. So it's another, but the problem with this is that it would be, we would have, when we convert, when we threshold these evidence scores, we're losing information about how good the evidence is, or we're losing detailed information about the. We don't really, wanna compare it to spikes, just yet.

I've presented this before. This was the result from hypothesis resampling. Basically what you're looking at here, excuse me, on the Y axis we have the evidence scores, and these are accumulators, as you can see, they can grow up to infinity. and I am switching between, so those bars at the top here, those are basically the episodes and I'm switching between objects. and what's happening is that as we switch, after we switch, we're deleting the hypothesis that are not performing based on their slopes and stuff like that. This the evidence slope and, things like that. But the idea is that we're deleting some of them, and new ones are being added and they keep accumulating more evidence. and they're winning. So this is a, good scenario, but, it's missing some of the details. And, one of the problems is if you're just looking at one of the episodes, not looking at all of them like that. So not really the scenario of object swapping that I'm just displaying here, but just within one episode, we could still have that problem that I discussed before, right? An episode, that, and, hypothesis that has been accumulating evidence for a while, just not getting deleted. and a newer one that has been added halfway, just not to winning, the race at the time. We're, evaluating it here.

This, here. What it, I reran everything with EMA and this is what it would look like, with the evidence course for the EMA evidence course. So the biggest difference that you would see here is that it's, always bounded. So it's not going to grow above, I think 1.5, that's this, or two. Maybe that's the highest step evidence that we would get. So the evidence would always just stay bounded. but it will be, if the, hypothesis is good, it will be above, the, all the other ones or all the, yeah, the, most likely hypothesis would be above the other ones. Just like you see here. The, if the color matches, basically that means it's good.

and then this is this, is more examples of, what you would see with, so these are, this is real, hypothesis, basically few random hypothesis just so that you could see. What the profile looks like. one interesting thing here that, you can see, so on the, left, we have the evidence scores. The, this is unbounded evidence scores, and the blue lines are basically those unbounded evidence scores. And then on the right we have the, the orange line and the evidence EMA, which is just on a different twin axis. and they, when you see, a line here that says four or three or something like that means we are, we just basically moved from one episode to another. So this is the object swaps switch, right? It's a, it has, changed from one object to another. So you, can see the decline in, in evidence EMA and, I'm basically using that decline to, to, delete the hypothesis, right? Which I, could bottom. Sorry, go ahead. The bottom left one though, that one. You, you saw this decline before you switched to a new optic? Yeah, so there's a bit of decline. it's still in the range of one, but the decline increased a lot when it started. just not getting any evidence. But there is, I think, a few of these are actually, might be a little, lower in the, in terms of, slope. I'm sorry, but would you consider that a failure in that case? Because the EMA score went down prematurely and Yeah, it depends on, how you threshold the, I should have, it's my mistake here, but I should have, made these, like a constant limit on the, Y axis. But this is not, this is 0.3 to 0.9. This is 0.6 to 1.2. but why would it go down at all? Why would the EMA decline at all? It's points like these where you have, where the evidence, it doesn't get evidenced. So it would just drop, because it is using the, it is just, it is dropping because of the, evidence not increasing. So that's how it would decay. If so, it's, it is not, it's not just bounded, it's also has a decay factor to it. yeah, it, would decay. so here, for example, it, the step evidence would be zero. And if the, evidence that we already have is like one or 1.2, then it would just decay because of that. So it's getting a, an evidence of zero, but it's at one, so it would end up decaying. And these are noisy experiments. if I, if those are the ones you showed before. And also not always you see points that are well represented in your model. So it's not easy to get a lot of positive EV evidence at every step. Yeah, no, I just didn't understand what the actual algorithm for EMA is, and so I didn't understand it had a natural decay. Yeah. It might be helpful to think it, it's, so it's quite similar to a leaky itegrated fire neuron and okay. That makes a lot of sense to me. Okay. It's leaky in the sense that when you're not getting input, it decay down and, what you call it. But what's interesting is I guess we can, by changing that alpha factor, we can essentially change the timescale for that decay and also the rise. And so one thing we've talked about, that this might even relate to the multi compartment, properties of neurons where you have dendritic spikes that last much longer versus The, the kind of somatic spikes, which are much, so we might end up having a mixture of these alpha constants to get something similar because we've observed that it's actually beneficial to have multiple time constants. but I guess I, don't wanna get off track here, but I was initially the, current system, the blue line, this unbounded increase, that has no decay in it. Is that correct? It just says resets. is that a correct interpretation? it can decay. Is it just decay very slowly? because it's not so much decay. It can get negative evidence. Yeah. But it doesn't I see. On its own. Yeah. So it can get, I guess that seemed to be, it seemed to be that sort of decay was the key, one of the key things that's missing. But then the EMA has that Yeah, We've been wanting to do that for, a long time. And that's, yeah. and just this bounding between kind of, from above to a specific value just makes a lot of things a lot simpler and a lot more interpretable because right now it's oh, if an experiment's going, been going on for 400 steps, some of the evidence values are super high. Versus another experiment, it just doesn't really make much sense. the bounding didn't, the bounding on its own didn't make sense to me, but if it has the bounding plus dec K, it makes sense. Okay. got it. yeah, two minute warning, by the way. okay. I'll be quick. Those are important questions to ask them. Yeah. the, this is a visualization here that I'm j I'm going to be talking about a little bit. mostly the parts that I want to focus on are here you'd see, the ground truth, object in the world, basically what is, what the rotation is and all of that orientation and everything in the world. this one here is a correlation pro plot between the evidence slope and the post error. The important part to focus on is, so every.in here is a hypothesis. Basically, this is the hypothesis space. and what, we wanna focus on is the pose error. so there are some hypothesis that have ground truth, good pose, error, and others that have high pose error. that's just the way Monty initializes those hypotheses. and on the side here is basically the selected hypothesis. If, you notice here there's a red dot, I could click on any of those and I would select a different hypothesis so I can inspect it and, see like the evidence scores since it was initialized until it was deleted, all of that stuff. so all of this information on the right is specific to a specific hypothesis. One of the dots on that previous, plot.

W what I noticed is, when we're running these experiments, this is currently how Monte does things. it would end up choosing, the hypo, like a hypothesis that has a hypo error, like three point 11 radiance. This is 180 degrees, as the, MLH or the most likely hypothesis just based on its evidence, because it ha it has accumulated a lot of evidence. just as you can see here, this, blue line has accumulated a lot of evidence because it has a high age, which means it, it, was there for a lot of steps, and it had, plenty of time to accumulate evidence. whereas we have a, better hypothesis down here, which has a low pose error. And as you can see here, this looks a lot more like the ground truth. but it's only been, there for 21 steps and it has accumulated 22, evidence points. So when you compare 22 to 42, of course the 42 1 is going to win, but, the, this one that has local error is, the one that we want. oops, sorry.

so when we add EMA, that resolves this problem, you can see the EMA, on the, in the orange line. and it's bounded, it's nice. It basically ends up being, a lower score for, this hypothesis that has higher evidence. Even though it has higher evidence, it's, it has lower EMA, whereas this one has higher EMA, but it has low evidence. But this is the one we want because this is the one that has low post error. it ends up resolving these, issues because, just the a MA is, a slightly better metric than just comparing the actual evidence, the accumulated evidence.

I ran a few benchmarks, just to make sure that, everything looks good. A lot of benchmarks actually, but, I'm only going to talk about one experiment. I noticed that, so this is one of the big experiments, the, noisy 77 objects surface agent. And, the alpha here, is basically, one of the EMA parameters. So I noticed that, it, we, kept the same performance. Some, of the alphas, increased the performance a little bit and some decreased the performance. the problem that I've seen is with the pose error, some of them have increased the pose error, like in most cases, pose air has acre increased. and also runtime, has increased, quite a bit. So that's the, I think that's the part that I'm going to investigate more. there is. There are some parameters that could tune this, like the x percent threshold. Basically, how do we know when the terminal conditions has been reached? there are many details here that could be, tuned, that could help with decreasing the, runtime, just like here, the, difference between the orange and the green, without actually affecting the performance. So I think, just more investigation is needed. but EMA is a plausible approach to doing this. And there are other options, but, this is one of the good contestants, I believe. yeah. So this is it. Nice. yeah, hopefully you see this. Want me to pass that over? Oh, sure. Thanks. yeah, Viviane, Jeremy and I, we were working on compositional, objects for our, project. And so this is something that goes back, many years actually, since at least, we reckoned 2017, people had menta and, a thousand brains projects have been talking about recognizing mugs. And very quickly that became recognizing a mug with a logo on it. So much so that it's become kind of something you mentioned a thousand brains Projects are famous for. when ROI joined or when one of our first contributors, helped with the code base, the natural thing for them to do was to pose, with a photo of a mug with a logo on it. so it's it's core to, I guess the DNA of, this kind of effort of developing intelligence. and that's no coincidence because the whole world is compositional. this lo this example of kind of a logo on something like a mug might seem quite simple, but it's absolutely essential to intelligence and how we understand the world, that we understand it as a hierarchy of compositional objects and how they are, relative to one another. and so many of you may know that this is one of the central, or really decentral theme for our research right now is getting compositional, objects working. and so a lot of the cool things that people are working on, including what ROI just showed, including what Scott and Hojae showed is about. Enabling us to do compositional objects well, but we have been missing a way of actually evaluating, and experimenting with compositional objects. Scott had made this data set a while ago that you're seeing right here with kind of objects, with logos on it. but we had no framework, we had no experiments and we had no configs, none of that to enable us to actually, perform these experiments and measure that. so that was really the key aim of this, project, was to, implement that and get a new benchmark, with compositional. and if you don't do that, then we will continue existing as Monty currently does in a non compositional world, which is really sad. And just where everything is a blurry cloud of dots associated with colors, and kind of surface texture. and we don't want that. So that was really what motivated us to do this work. We.

Say something about this. Oh, this is, just a bit of an insight of multi learning about compositional objects. I guess at first, learning about the logo in isolation, very frantically. And then, of a, the first or second day, I think we got it together. Calm down a bit more and, yeah, just move a bit more slowly over that object. yeah, just to peek into, that, Yeah, basically, the kind of learning framework we set up and we had to make quite a few tweaks to the existing framework because learning composition objects has a few more intricacies. what we have now is, split up in two phases. Phase one is learning object parts, but we basically show it the logos and the different objects in isolation. And both the lower level learning module and the high level learning module are learning. And both of them are receiving a label at the moment. So we're giving one a few crutches right now and not going straight to everything unsupervised. And then over time, we'll remove those, but for now we just wanna get it to learn competition objects. So yeah, we show them in isolation and tell Monty what it is and it learns these kind of models and you can see. the lower level model gets like a smaller, higher reso resolution receptor field as input, and the higher level one gets like a larger lower resolution input. So they learn different models, like in the lower level, it's much more densely sampled, and at the high level it's, it learns, sparse models, based on the sensorimotor input. and then, but those are not compositional models yet. Those are just based on sensory input. And so then there's a phase two of learning the compositional objects where we basically show Monte the logo, the different logo and object combinations. we have the same kind of sensory inputs, but now the difference is that the lower level learning modules actually doing inference. We're not telling it what it's seeing, it's trying to infer either like TVP logo or a mentor logo, a disc or cup or a cube, whatever it's currently sensing, and it sends that object ID up to the higher level learning module. So then that one can actually store whatever input it gets from the lower level model into a compositional model. so here's an example that shows, a model of a cup learned from input from the lower level module and here basically a lower level module sense. Mug ID up to the higher one. and in this case, we still give the higher level one a label, which is the label of the compositional object. So we tell it, alright, this is a TVP mug and not just a plain mug. and then we just have a comparison condition, which we call the monolithic models, just to have a baseline to compare to where we don't send any input up through the hierarchy. And they just both learn these compositional models based on pure sensory input. the, sad world, that Niels just showed. Yeah, that actually wouldn't be, it wouldn't be compositional. Just, yeah. This is not composition. So that's why we call Yeah, but you just said the word composition. Alright. So it's basically try to learn the mug with a logo is one thing. Yeah, it's learning composition objects, but not building composition models. It's learning monolithic models. Yeah. And that's what we compare to, to show the benefits of learning composition models. Yeah. And so one of the things we needed to do was to update how we measure, the performance. because initially when we ran these experiments, the, these are each pair of, do you want, each pair of these, lines is a single experiment where we have the lower level, learning module and the higher level one. Initially this would say that it was confused. Actually no. yeah. Saying was confused. and then we, what we've done is we've changed it to where it says it's consistent. It's a, it's consistent with the child object of the actual object that's being viewed. So we're looking at a cube with the TPP logo, but it thinks it's a cube and that is a sub component of that. And so what we had to do was come up with a mapping of, and we have much larger than this, but a mapping of, if you're looking at this, these are what the sub components or sub objects of child children, objects of that are. and then if, the lower level module sees that, or actually if either of them see that, we say, yeah, it's, a child, it's a consistent with child object of it. I don't remember what these graphs are actually measuring or how they're, yeah. Percent. Yeah. Basically compares, comparing the performance on these new metrics and basically showing what we'd expect on the left. It's showing the le lower level learning module accuracy of the monolithic and, but when, or when they compositionally and on the right, it's the higher level learning module when learning monolithically or compositional. Yeah. And what we'd expect is, as you see on the left, in the kind of light blue, the low level learning module, it doesn't know about the TBP mug with a logo. but it does know about mugs and it does know about logos. So it correctly recognizes those kind of child objects. and, this basically enables us to measure this, even though there's limitations in the simulator, that kind of limit our, knowledge about what the, what object is being seen at any moment. So then we were able to, come up with some ways to collect the stats cv, CSV stats and have some detailed JSON logging to, to compare the prediction error, and then stuff with the one db prediction error and look at how those are measured. And then what's the next one? Oh, yeah, Prediction space is a measure we didn't have in the code base before, so we never looked at this before. We basically added a new way of measuring Yeah. Performance, which is you don't need any labels. You're just looking at how well does the learning module predict it's next input. And yeah, it's a, nice un unsupervised way to assess performance, I think. is this part of mine or yours? that's, yeah. And then this is showing our, final test bed. So I mentioned at the start that the, ultimate aim was to have kind of a new benchmark. and so we had to change a lot of things to actually implement that. But this is the final sort of artifact, which is, this would basically go in our documentation, and show how performance is right now in Monty with these, compositional data sets. So the aim was not to solve, doing compositional models. that's all the cool stuff that others have been working on, and that we're gonna be continuing on in September. The aim was actually to be able to measure that and actually run experiments that evaluates that. So that's what you're seeing here. And in particular, if you look in the top right, that we have these different levels. And so that's where we increase the difficulty of the compositional model or compositional object that's being presented. Going from what you saw before with a flat disc with a logo on it. So the logo isn't being distorted by the surface up to objects with curvature. So the logo is distorted. And finally, the kind of famous or infamous logo that's bent halfway through, and how you would represent that and what these results show, oh, generation ago, which is which is great, is, what we would expect. So firstly, if you look at the correct child or parent, if you just look down that you see in general, as the level gets, increases, it gets harder. And so accuracy drops. But if you look at the top, you see the monolithic models versus our, compositional models. And the compositional models get better accuracy, but more significantly, if you look at number of matching steps. They're almost 10 times more efficient at converging to the correct object. And that's because the child's, Lear or like the low level learning module can recognize something simple oh, I'm seeing the disc, or I'm seeing the mug, and send that information up. enable the higher level learning module to use that information, rather than both of them just in their own kind of flat world, trying to always recognize TVP mug with logo. and So in this case is where they're both recognizing the same object? Is that what you're saying? Yeah. In the top two, they're both evaluated level one dataset, so they're looking at the same thing. Got, so that shows yeah, that the, compositional kind of architecture is already beneficial. but what we don't have here is clever policies to focus, our movements on one child object, for example. and we also didn't have, for example, the resampling that Rami talked about, which is much better given that we'll often move from one object to another, and we need to be flexible with our hypothesis base. all of that is benefits that we can start bringing in. So yeah, the model is, the best, the best baseline is the one that leaves lots of room for improvement. so yeah, we briefly, today tried, integrated Rami's hypothesis resampling and actually got to learn some compositional objects that represent, that contain two different types of object IDs in the higher level model. So the LO tb, it rec at some point times it recognized the TBP logo and at other points it recognized the disc. but there's still some bug in there. so if you actually rotate it, the, some of the locations are wrong. So we think there's something being messed up with the locations and that's why it looks messed up right now. but yeah, it was exciting to see that the lower level model switch between logo and disc hypothesis and sent that up in the high level one, integrated that into its model. So lots of, lots of follow up work from this week. But, yeah, we opened the draft PR there. It wasn't as simple as just creating some experiment configs. We all actually had to write a lot of code and file changes, and, also updated our documentation, out of these new benchmarks that anyone can run. Then we also found several bugs and issues independently in Monty that will open independent PRS for, that were not related to this test bed itself, but just, we came across as we were working with it and yeah, the legend of logos and mugs continues. Woo. So you suggested we give you extra credit for finding bugs.

No.

Couple slides to the performance measurements. Yeah. Here it also goes 20 minutes faster. Is that right? Yeah. 'cause it takes less steps. 45 versus 350. That's great. 'cause we made the whole thing go 10 times slower. You guys have made it go faster. So if you make all the improvement, then we can use them and it'll feel the same. Yeah, we can just, we did this week and come out very strong on the other side.

Now, does it, does the results change if you change the color of the logo? it's the, color of the logo, the tech, the font of the logo. Does that make any difference to your results, or is it basically just focusing on text versus area? That's a good question. Yeah. Right now it would be sensitive to the color. it would, it basically picks like the way it's learned, the logo is with a specific color. But, one of the things we have in our, maybe we'll get to in September, is basically having a, sensory processing system that understands 2D things better. And so it would pick up on things like edges, and that would be, independent of color basically, and that, that would be a much better model of something like a logo. But, but yeah, generally with 3D objects, it doesn't rely too much on color. Color can help infer faster, but it can also recognize the same object in a different color. Yeah, since our current processing doesn't really pick up a lot of orientation feed or any orientation features on a 2D object, like the logo, like Neil said, we need to add some extra processing for that.

Yeah. The generated the spam. Can you not using the handle at all? Doesn't understand. Three zero. Put another copyright on it, or composition Yeah. That put a logo on it and then just put copyright.

very impressive what you guys have all done. Yeah.