Ramy Mounir: Yeah, my name is Ramy Mounir, and I'm one of the researchers here at the Thousand Brains Project. Yeah, so today I'm going to be talking to you about the learning modules, and we really think the learning modules are at the core of how Monty learns and infers objects.

Scott talked about them a little bit in his presentation. I'm going to go into a lot more details, and then we're going to also show some visualizations that are really going to give you this intuition of what Monty is doing behind the scenes. So yeah, let's, start.

Okay.

I've just set up an experiment with 3 episodes. Every episode is just a single object at some orientation. And Monty is going to go through the, this is just a, the Monty agent or an agent and Monty is just going to go around the object. Let me run this animation here.

So Monty's just going to go around the object and it's just going to observe or move its little sensory patch. This is just one learning module working, so it's going to move its own sensory patch around the object. And the goal here is to figure out three things. First, the object identity. What that object is it a spam can, or a spoon, or whatever. And then the orientation at which this object is currently at and also the sensor location on the object.

Because this is a sensorimotor experiment we expect that the learning system should also know where it is on the object. Just like if you drop a rat in an unfamiliar environment. When the grid cells re-anchor, it not only knows what the maze is or what the environment is, but also where it is in the environment, because it uses this information to path integrate and do all that fun stuff.

now, the cool thing about this experiment is, that Monty also doesn't know where the object ends and a new object begins. for a potted meat can and then it will just magically switch to a die or a mug, and then it just it just has to figure out, when it did that, or. Just basically all the temporal or spatial object boundaries, which is also useful for learning compositional models. if you're looking at a car and you're you're fixated on the car wheel, and then you go over to the car door, for example, and these are like low-level, objects, then you want to be able to figure out what the spatial boundaries are and how you detect when you moved from one object to another.

yeah, so let's open up Monty's brain and take a look inside. And so basically, this is one of the visualizations I'm going to show. I have another one here, which I'm going to be switching between them, during the presentation. But, we've seen this part before, where the agent is over here, this violet or purple sphere. And we have a step, so we can go through the steps within this episode. The agent is moving and it's also moving its sensory patch on the object. We could also change the episode to another object and another one. Monty doesn't really know when what happens. We don't tell it that we changed the episode or change the object to another one.

One of the other cool things that we could do here is that we can activate a tracer for the agent and for the sensor patch. So if I run this animation it'll basically just tell you these are the parts that we've seen of the object. And yeah, so this is the agent path, and this is the sensory patch path.

Okay.

Let's now go to talk about the second part of this.

how does Monty figure out what is the actual object? It wants to infer those three things that we talked about before. Monty comes up with what we call hypothesis space. Which is just a collection of hypotheses. And we want to now get an intuitive understanding of what a hypothesis is in Monty. let me just, reveal this other part of the visualization. Where I'm going to lay out all of the hypotheses that Monty has about this object. Let's move to step 6 or something. And now forget about the X and Y axes for a moment, I'm going to discuss them in a little bit, but you could just think of all these scatter points as just laid out randomly without any patterns. But the idea here is that every one of those little points is a hypothesis of its own. And we're going to investigate further and look into what representation is being captured into those hypotheses. So I can now reveal the third part of the visualization, and I can select these hypotheses, and it will tell you basically what's in this hypothesis here. So you could look at this right side to figure that out. for example, if I click on this hypothesis over here will tell you I'm seeing the potted meat can, which is the object identity, and I'm seeing it in this rotation, or orientation. That's the second piece of information, and then I'm also seeing the place where it thinks the sensor location is right now. The only part of this hypothesis that is correct is the object identity, but it didn't get the orientation correct, or it didn't get the sensor location. So what Monty does at the very early stages of the episode, or, the very early phase, is it will generate some hypotheses. It will sample them intelligently, I'm not going to get into how it does that right now, but it basically will sample some hypotheses, and then it will start testing those hypotheses. So it will sample some very good hypotheses, for example, some over here, this might not be best hypothesis, let's see.

So it'll basically sample somehow. So this one's a good one, right? So it got the right orientation, it got the right identity, it also got the right sensor location, right? Almost.

Some of those are, when we test them, they're going to prove that they're good, others are not.

we could also sample hypotheses that are not on the same object, right? So we have many of these views here where we have many hypotheses, right? Since the dice, The mustard bottle whatever. So if we go back to just the ones that are on the correct identity, and just to drive this point home about hypotheses. We can also activate here. Look at the path that the sensor moved, right? Because the hypothesis is just going to integrate, it's going to get a movement. It's going to adjust transform this movement so that it fits the allocentric reference frame of the object and then it's just going to path integrate and add this movement to the sensor location. So this is what it looks like. This is the trajectory that it thinks that it moved. We started over here, and then we moved all the way here. Which really just matches what we moved, through with the simulator, right? Started over here, and then we just moved this way. So you'll see that the movements are the same, it's just that it might not be at the same location, or at the same orientation. So if I look at some of these hypotheses at the bottom here with the low pose error. These all have low pose error so they all match this one.

You will see that they look the same, but they are just translated, right? Just translated because it thinks that, yeah, the pose is correct, but the location of the sensor is different. But if I move to something that has a high pose error. Over here, you'll see that the whole trajectory is just rotated, and that's just it's trying to fit it into the allocentric reference frame.

Yeah, let's actually run this, or, let's just replay the animation and see what happens to the to the hypothesis space as a whole, right? Because we have all of these hypotheses and we want to look at what happens there. started again. And... So the hypothesis space, it starts... this is, the hypothesis-based side so basically it starts with a lot of hypotheses, and then we start pruning them. And the pruning is basically depending on the evidence. what happens is that as Monty as Monty moves or as the agent moves, it will send those movements and features to the reference frame, and it will compare it to what the hypothesis thinks it's seeing, right? Because the movements are also applied in the hypothesis. it will apply the movement, and it will compare it to it will make a prediction about what it will see, and then it will compare that to what the simulator has sent it, which is the actual observation. And then it will basically build, if it matches, it will add some evidence, and if it doesn't match, it will subtract some evidence. So the evidence added is really proportional to what it sees. this is what, a typical good hypothesis would look like, right? You have evidence just growing like this. And then we also have the evidence change which is like the slope averaged over the last few steps. And that's what we use to delete hypotheses or add new hypotheses as a signal of adding or deleting hypotheses.

yeah, so let's just run it to the end. We will see that we have over here just a few hypotheses, I don't know, 10 hypotheses at the end, which is very efficient. What we will look at now is what Monty thinks is... which one of them is the best. So we can just set this to 1, this is the top. Top one hypothesis according to the evidence score, the accumulated evidence score. Not necessarily the slope, but just the score. And we can try and look at, what this one is.

this one's actually a pretty good one, and you can see here that the evidence has been climbing since we started the episode. It's just been adding evidence, and it only started, the evidence started only decreasing after we switched to the next object and then we deleted that hypothesis, which is what we would want. So we could also we could look at this hypothesis, and we can see that it's the right orientation, right object, but also, as equally important, it's the right... it knows where it is on the object. So that's that's a very important piece of information. But we could also look at what is the second most likely hypothesis, which would be over here, and you can see that it's structurally the same. It's like a very high pose error, at 180 degrees rotation. But it's structurally the same, which is, that's because Monty focuses more on the shape of the object rather than the texture or the the colors. And that's something that we've built into Monty. You can change that if you want but this is something that is really important to Monty.

we could also look at the third most likely hypothesis, which would be something over here. So that's it, again, it's the same object, just rotated 180 degrees and 180 degrees, or... Yeah, it's keeping the same overall shape.

I don't have a lot of time so I'm going to skip over the die example. But it's getting the correct information. And then these are just... it's the same orientation. But it's different faces over here. Just swap some of them. Because again it's just focusing on this overall shape. But one interesting example over here is the mug. Where if I just run this to the end. Let's just take a look at what the results would look like. it thinks that this hypothesis over here, the green hypothesis over here, this is the most likely hypothesis, according to evidence. if we take a look at it, it's an interesting hypothesis. It's the object is in the right orientation, but it looks like it's rotated around one axis. Which is the the Y-axis, and it would be the axis of symmetry if the handle was not there. Let's take a look at a few more and you'll see that all of them are actually the same. same trend, right? Same pattern, it's just rotating around that axis of symmetry assuming that the handle's not there. what we could conclude that it hasn't really seen the handle yet, and that's what we can also explore by looking at the sensory patch. We could just remove the cup from here and we'll see that it's just been moved around the object. It explored also on the bottom of the object, but it hasn't really seen the handle at all, right? it would make sense that it didn't know where the handle is and it would just think that all of these are very good hypotheses. They've been accumulating evidence as you can see here. Another view that we can look at is, this view over here. I'm just going to talk briefly about this figure at the bottom. Which we're just calling the hypothesis space. If you look at the one with the area under the curve. They highlighted area under the curve. This is the hypothesis space size. at the beginning it grows as it thinks that it needs hypotheses because at the beginning, it didn't have any hypotheses. So it needs to sample some. And then, oh, and then it prunes them, right?

it will just prune those hypotheses and then go to almost a few, 10, as I think we've seen in the previous visualization. But then after some time, we'll switch the object with another one. And Monty figures out that it will automatically figure that out based on just looking at the evidence and the slopes and all of that. It'll just figure out that it needs to sample new hypotheses because the ones existing are not really good enough. So it will trigger what we call a sampling burst. It will just burst some hypothesis over a few steps. And that will increase the number of hypotheses again. And then it will start pruning them. It will happen again when we switch it out with a mug. You can imagine this also working if you move from one object to another sharing the same space.

I just want to show you what it will look like here in this different visualization for the cup because I think it's interesting. Let me just run this again and turn on the hypotheses. So here what I'm showing here is not the hypothesis orientations, again, I'm just showing where every hypothesis thinks its sensor is. there's a lot of hypotheses they think they're all over the object but once we start running the animation it will just look like we'll be pruning them, but it will also leave only a few hypotheses. So let's do that. And... transparency. So yeah, so we've pruned the hypothesis, we are over here right now, and it thinks that there are just a few hypotheses over here. And the hypotheses are all around the object, because, again, it hasn't seen the handle so it just thinks that they're all around. If I turn on the evidence scores you'll be able to see that it thinks that there are basically good hypotheses all over. there are very high hypotheses everywhere. Again, it thinks that this is symmetric. This is what happens when Monty sees a symmetric object, or thinks this is a symmetric object. It will just think all of these hypotheses are equally valid or are good hypotheses.

And let's just to end this let's run this, again. You'll be able to see that as the sensor moves up and down these hypotheses will go up and down with it, right? Because we're path integrating so every movement is just moving those sensor locations. it'll just move up and down with the sensor. Which is what we want to see, right? I think I'm gonna end it here, because I took more time than I should. Just the last thing is if you like these kinds of visualizations there's a repository for it: tbp.plot, and we have a very small gallery here and you can contribute to it or you could just use them to view your experiments that you run. And there are some written tutorials on how to create new ones. And there's also a video tutorial on how to use those. with that, I'm gonna hand it off to Will, who will actually guide you into how to contribute more into various aspects of this project.