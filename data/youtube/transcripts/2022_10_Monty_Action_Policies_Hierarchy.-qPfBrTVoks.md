yeah, so thanks, Ko I'll yeah. Be talking about action policies and how, what we're doing at the moment with action policies, what we hope they can, help with. And in particular with a focus on multiple objects, at a time. So yeah, one of the aspirations for, improving the action policies that we have is just to make recognition more efficient. if you have these objects in your memory and you're experiencing this object, ideally we do something better than just a random walk over the surface of this object, in order to identify it. and for example, making a quick movement, to this point, and this might be either a bottom up driven, approach or a more top-down approach. And the way I've divided that, at least in my head, is that kind of bottom up is anything that the sensory module and the, motor system will have had access to. the top down is stuff that relies on a learning module, so an explicit model of objects either from, yeah, based on previous learning.

and Philip had already implemented this kind of momentum to the touch policy, which can circumnavigate an object by, moving along its surface. and in particular there was this momentum parameter. but I don't think in the time he was there, there was enough time to really explore, the kind of potential use of this to make recognition more efficient. In particular, the default value was not 0.5, which gives you this kind of rambling slow, slightly directional random walk. and so one thing I was interested in was if, we could improve on that, in terms of different, values for this momentum, that gives you more efficient recognition mission.

but I was also hoping to improve on that even further with something a bit more intelligent where it's still bottom up, but it's driven by the kind of features that a sensory module and the motor system are experiencing. And in particular, this was a policy where essentially the, agent follows the principle curvatures, that it encounters, in order to hopefully more quickly get an interesting overview of, the object structure. What do mean by, and what's a principle curvature? So the principle curvatures are the minimal and maximal curvature. so on a co on the side of a cylinder, for example. The maximal will be the curved direction. Yeah. And the minimal will be the flat one. so principle you would follow the maximal curvature or would you? So, the policy does both and that's what the white and the black is showing here. So when it's white, so when it's blue, it's just following momentum. It's just rambling along. When it's white, it's following the minimal curvature. When it's black, it's following the maximal curvature.

and it kind of alternates between those. in addition, it has this, element in its policy where it tries to avoid revisiting previous locations. So that's what you see here in green is the kind of policies starts to move in this direction, but then it realizes actually if I continue going in that direction, I'm just gonna be getting the same information I already have. let's try going a different direction. And all of this doesn't assume any knowledge about what the actual object is. it's just based on this kind of Yeah. Information that's coming in through the sensory module and some knowledge of where you've been, in the past, in the kind of egocentric or en environmental, reference frame.

and yeah, maybe another presentation I can go and go into a bit more detail about the, kind of the technicalities of kind of this. But, just very briefly, a snapshot of some of the results around this. So these results are from, the first 50 YCB objects in two different rotations, two very different rotations, and, looking at performance up to a maximum of 30 steps. So basically, just because the idea here is, this is a bottom up policy, and ideally this is just going to be a first, kind of step. you might do something like this and then by this point you'd expect to have a reasonably good hypothesis about the object. there's a good chance you won't have recognized it because there will be many objects that are similar. But at least if you have a good hypothesis at this stage, then that will then feed into, the next work, which is going to be about top down, policy. And then you can, test specific hypotheses and so forth. So for this analysis, I'm just allowing 30 steps and then looking at performance at that point. What is performance here? What is, sorry. Yeah, so it's, there's two outcomes being shown, both which are good. there's blue, which is, it's correct. So it, in those three steps, it managed to find the solution. And then there's red, that means, sorry. That means identify the object. It identified the object. Yeah. And then red means it's, correct hypothesis. So basically it's, maximum, like the hypothesis it has with the most evidence is the correct object, but it has other objects with sufficiently high, evidence. and basically all this evidence hasn't reached the threshold that it's made a decision. But if you imagine then. Know, once a an object has taken a few steps and it has a kind of maximal hypothesis, about a particular object, then that's where, for example, it could do a top-down, policy executed action to test that hypothesis. I'm still confused. Yeah, I'm confused about that too. So, just let's walk through one of these things. So you've got a bar, you show these bars in the blue and the, red or whatever. Yeah. And, and this is, I guess I'm, just trying to understand what, yeah, what is the, so I'm trying to understand what the difference between the blue and the red are. I, I understand you, you said the red, so basically the red, it hasn't converged, but Right. Let's, it's because it, the one that it, it thinks is most likely if you pointed a gun at it and said, okay, which one is most likely, you have to decide it would be the correct one.

That and a correct hypothesis because it Correct, it's actually converged and it said I'm confident it's the subject. Yeah. There are like some threshold, for it to actually identify an object. It needs to be confident above a certain level and it needs to be sure, like it needs to be higher evidence than all of the other objects by a cer certain threshold. So there's still possible, okay. So if you look at like normally how classification is done, you just pick the top hypothesis and that's your classification and that would be the red one and then blue. In addition to that, it's actually very confident that it's the right one. Is it confident in that it eliminated all possibilities or is just some arbitrary threshold?

I guess it's a relatively arbitrary threshold. as in it's user set. What, sorry, maybe repeat your question Jeff. I'm just trying to understand, is this blue meaning. It's absolutely certain, or is it just above some threshold of certainty? It's just above some threshold. So this distinction of blue and red is arbitrary.

it's like it's depends on where you put your threshold. yeah. But, but I think it's still, I don't know, useful to, to think about Yeah. In some cases you want to be more confident. you, have to have a threshold at some point if you want the, objects who, the kind of system to output one label, but the labels continuously being outdated in some sense. You could, it's okay to say the system is always, guessing. The system is never a hundred percent certain about anything. You could think of it that way and, just like in science, your hypothesis are always hypothesis, but say this is feeding into something else. okay, how much confidence do we need before we're going to act on our hypothesis? we wanna first find out what it is and then we're going to try and pick it up or something. I think, so we, need some sense of kind of confidence about, about the object before we're gonna then try and execute an action. That's true. Jeff saying, yeah, you need to have maybe some threshold, but that threshold is just a parameter. Yeah, it's more of a functional thing because ideally we just have a continuous evidence that like updates every step and it just varies and all of that. But, we have to measure it at some point. So we have to say, okay, is it correct now or not? And for that we can need a threshold. When do we measure this? And, we need to end the episode at some point. So that's why we, what is So there's nothing magical about a particular number. Yeah. What does the outcome proportion number represent? So it's a percentage of what? Of the, total number of objects, experienced mean you're saying. So a as in on about 12 of objects, it got the correct solution. Oh. In, 30 steps. So it's just classification. So, higher is better. Yeah. It's, so 12 of the objects were correctly recognized versus.

So that's means even with all the different tests here, there's a lot of objects we're not correctly recognized. Is that right? Yes. Is that bothersome? Very few steps. the steps are very small, so 30 steps is not a huge distance.

And yeah, I'm Okay. So that's another, it would definitely be better if you obviously did it for longer, but this is why I was saying that I think ultimately we don't want to be just at inference time dragging our fingers slowly across an object we want to You could do much faster. Yeah. Yeah. That skip if you go on the surface. Yeah. but the, yeah, the touch policy I think is, it's constrained to follow the surface. at least, at the start. While it doesn't have a hypothesis about the object. And I think that is a good thing because I think that kind of relates to what would be happening if we eventually have a robotic digit or something like that. That's. An accurate depth reading, but has to follow the surface and not knock the object over, which is very different from an object with your eyes.

when I think about how I touch something, it's, pretty obvious how I do it. And it's not quite like this. if I touch an object, either I'm learning it or I'm trying to do inference, I will typically find, let's just call an edge. Like I, my, this cup in my hand here, I find a sharp edge and I follow that edge around. and I do continue. That's, yeah, that's what I was trying to capture with this principle curvature guy, because I was trying to lay that onto this, minimal and maximal curvature. And it's funny because. It's, I'll, say on this, I'm not, when I'm on the lip of this cup, the, to, go to the maximum curvature, I go down the side, but I don't really wanna do that. I wanna follow the edge, which is, in some sense the least curvature. But it's, still following the edge. So it's not clear. there's not a lot of, it's, you don't really want to go on the flat surface very much. You want, you, you quickly try to find an edge and then you follow that edge around, which it's hard to lay exactly on this, which is a little, this is a little no. So, that you could easily do if you just, so I, set the number of steps that it's going to follow a particular curvature before it's switching to the other. Yeah. And that's to avoid it spending ages just going around the rim. So in some sense what you do is you go around the rim until you get back from, you start, then you go someplace else. And. You get to a point where there's a, where there's a, bifurcation on the edge, and then you follow that one. it's very related to what you did here, but it's not quite the same. So it's interesting. The other thing is it's very clear we do this continu, at least with touch, you do it continuously. You don't stop, you don't go oh, sense move, since move. it's odd because that's not how we've talked about in the past. It's like you continually follow the edge and you, and, it's not a, it's not a saltatory experience. It's, a, continuous experience. and I just wanna point that out. There's a distinction there. I've talked a bit with, I think with Vivian about this, that it is possible that neural systems can learn continuously like this. they don't have to be done. you don't have to do them, always with, move sense. So I just, that's another thing. So if I said I wanna move further. I don't wanna just move my finger on this little area here. I wanna move further. if I have to jump and jump then I'm losing information. But if I can just follow quickly all the way around, then it's not like I'm storing a thousand points. It's more, I, think I can go through the basis of this, how this would work. It doesn't, you don't have to form new synopsis all the time as you move around here. It just adds new if you need it. And, that's not how we have it structured right now. we have it structured now as this point and movement and point and movement. So I can see the problem there because you want, you wanna move further, but the, if you just jump further. Then you're losing the continuity between these points and, you don't know what happened in between. yeah. Yeah. And I think in general, something that will be useful eventually is using movements themselves to estimate principle curvatures rather than having these kind of point estimates. where it's how, a, something like a finger is actually moving along the surface, that when it's constrained to follow the surface is telling us information about, for example Yeah. The principle curvature of an object. and, I think that could be a lot less noisy than some sort of depth camera reading or whatever happens to be anyway. It seems we have a fundamental issue we're dealing with here is the number of points that have to be sampled. it's easier to sample close points, and then you have these accuracy issues 'cause you've only sampled over a small part of the object if you wanna sample the whole object and you either have lots of points. Too many or you perhaps are missing the key features of the whole thing?

if I just touch four sides of this circle, I can't tell if it's, I can't tell if, and I can't really feel the curvature of it. I can't tell if that's a circle or a square. You knows like four, certain edges at 90 degree angles to one another. So I, I'm just pointing out a problem here, that I, think the neurons could, neuroscience could tell us the neuro solution for it, but I think it's just, it's an issue, right? the sample density is an issue here. that's why you're only getting 30. I think that's what, that what. Yeah, I said, I'm not too concerned because this is, yeah. Just to be a kind of starting point, at this point, you would obviously then start testing, likely hypotheses, and at that point you can start making larger jumps. But it's hard to do that if you're, imagine you're doing it with one finger. You touch one point on the object, now you want to try and touch somewhere else on the object. you have no idea what the structure of the object is other than that there's a surface in a particular direction relative to you. But you wanna follow the surface, even if you're just, imagine you're learning it, you wanna follow the surface, but you don't, and you wanna follow it continuously in some sense, but you don't wanna store it. Ian points, yeah, I think, the, continuous versus discreet is a more fundamental, thing for sure. Yeah. I, we might be, but, I don't think that necessarily, like we can still, like you, you can view this as 30 steps, or you can view this as viewing, one or two centimeters of the, the surface of the. I'm just saying that's the continuous service, but that's not really, that 30 centimeters is, not what we do with our body. And it's, very, it has restrictions to it. I think what you're saying is, Hey, I can sample a small part of this, object and get enough of a hypothesis of what the object is. Now I can use a top down action policy. but that's maybe true, although, obviously it's not gonna work extremely well because we still have, the only 30 of the object being a third object being recognized. so that's, insufficient. And also, it's also not sufficient if we're gonna have an action policy for learning the object, which case I have to sample much more of the object. because yeah, I still think these results are pretty pro promising because. 30. It actually had the most likely hypothesis. And saying it's the most likely hypothesis means there were se several other ones that are also possible that may be objects that have a similar morphology. Yeah. Or symmetric versions of that object. So different poses of that object that are in symmetry equivalent. So even, those, if, you, even if you just use the most likely hypothesis and it is the wrong object, it will most likely still have a similar morphology. So even with that, using that with a top-down policy should, in most cases work. so I, all right. I don't take my comments incorrectly. I didn't mean be critical of this. I think these are good results. I agree with that. I'm just anticipating problems and trying to understand how is it different than I think the brain does it. that may not, may or may not be important, but, I'm just, that's the naturally comes outta my head, I think this is not exactly, this is close to what the brain does, but it's, but I'm just bringing, that's what I'm doing. I think it's still very, it's resolved, don't get me wrong. Yeah. So yeah, and so it'd be very simple to change this, for example, so that you follow most of this and, rather than you, you have the action policy follow this whole thing, but then you only feed, let's say, five or 10 observations from across the whole rim to the learning module. Like there, there's kind of simple approaches to make it a bit more what you're describing. But yeah, definitely the kind of distinction between discrete and continuous is, a much more fundamental, change that would need to be, implemented at some point. That would be, and I mean that, I feel like that gets to how the learning module represents things and That would be a fun little side discussion to have, the discreet versus continuous. I don't understand it completely, but I have ideas about it and, and it's, it is be an interesting challenge. It's one of these things I say, oh, I wonder if we could add the screen. We could add continuous learning to the kind of models you guys are already building, which are fundamentally not continuous. so that'd be interesting sort of mental exercise to go through someday, if you wanted to do it sometimes. Yeah, no, I agree. That would be really good. you guys set up, proposals for different directions to go to next or other Yes. Yeah. Maybe just very quickly, just in case it wasn't clear, this plot. on the X axis we have random walk. We have the momentum at not 0.5. So this was the default value that had been used before. And then this was exploring kind of a smaller momentum value. So this is just carrying forward more. And then this is this principle curvature guided policy. Given the number of samples I have and so forth, it's not clear if there's a huge difference between these two, but it's, it seems that at least these are, it's a trend towards improvement certainly over, random walk, potentially over previous might walk. Yeah. Yeah. Could you remind me how to interpret the momentum value? I thought a larger number means you'd go further, but here it looks like you're going, you're not going, it's the, opposite. So of a momentum of one. Yeah, I agree that, yeah. Maybe momentum when I, write that here is it's one over is a poor choice of words. It's, a parameter called alpha. So yeah, really maybe momentum is the inverse. Yes. Okay. It's one over momentum. No, it's a good point. So, one is a random walk. Okay. Got it. Yeah, I'll, I might change that for the next time. If I present one the opposite of the word momentum, you might say something like, yeah.

Dixie, yeah. Scatter brand, just take one minus. I know, but the word momentum a sticks in your head. Yeah. The bigger, the further I go before I turn, it's the opposite of this. It's like this, number represents how, crazy you are, And just quick other, oh yeah, please. I have a question. Is the word you're looking for, what is it? Mean free path, like when you're talking about, atoms bumping into each other. Yeah, You're technically right, Eric, but that doesn't speak to the masses notion. If I asked my, if I asked my wife, what's the opposite of momentum? She wouldn't say mean free all. So I, had a question. Neils, so you're making a jump from these principle curvatures to object identification. If I'm just thinking rather than these, the criticism that you're having to do all these micro steps and maybe store them something is if they're contributing to a hypothesis of some kind of contour and then you're, if you have something that is basically relatively smooth and you say, I think there's a, curvature continuous curvature going here, so there's a contour. And I keep moving and, it's being satisfied. I only need to store that, kind of rock, curvature if you wish. Rather than all the intermediate steps. So as, an intermediate representation before you leap to going, okay, now I understand the set of contours represent this particular object. Yeah. Yeah. No, I think that's a great point. and I, exactly. I think something like that will be important. And I think when there is something like that, then I think the advantage of the PC guided over just the standard momentum will be even more stark. Because Yeah, at the moment, like it, this policy means we sample, like this principle curva going in this direction more often, but it's, it's not in itself really giving us information, the fact that we're following an edge about the, kind of principle curvature. but like you say, that is added information, which could itself be an intermediate representation. I think I, I have a bias against trying not to parametize the objects, and you've been through this before where oh, we can say the cups are made of these certain, curvatures plus this and plus edges and so on. It at least if you're gonna do that, you have to do it in a very, non, no Rio assumptions about what the structures are. You know what I'm saying? It's you don't, wanna get down to a model of a cup is, some predetermined, structures that are combined together like blue arms or something like that.

I think I guess I'd prefer to keep going in the direction you're going and solve the problem. they arise. I don't wanna overstate. Yeah, I think there's a variety of ways, that what kind of Kevin was describing and, yeah. What I was also looking to earlier about capturing. With movement can be represented. So yeah, I agree. We don't want it to just be like a list of three variables that parametize cones or whatever. so as long as we agree, suggest way of thinking about it, then I'm okay with that there. Yeah, No thanks Kevin. yeah, and so just quickly then the other kind of result was just, so if you go back to these kind of red ones where you have the correct, like the kind of the correct objects is your most likely hypothesis, but you have a bunch of other objects that you essentially haven't eliminated yet, and are cluttering your, comparison. And this is what this is looking at here. So this is the number of, potential hypotheses, and then against the different conditions. And again, what is it seems these policies do a better job of kind of, yeah, ensuring that we have a, more. constrained list. not something with 50 20 objects, particularly again, the PC guided. So in this case, when the num, this number of possible hypotheses are down to these low numbers, are you checking that the correct object is actually one of these possible hypotheses or Yeah, it's not only one of them, it's the leading one. leading one would give you that 30 accuracy. But it could be that it's the second leading one, but it's still a, decent hypothesis. Yeah. So that's not what I'm looking at here. That's not, so this is for the ones that were correctly identified? Exactly, yeah. Okay. Okay.

okay. Yeah. as well as what's already been discussed. So one thing that I want to explore is This policy is interesting, but it's biased to, sample these kinds of Yeah. High principle curvature areas. And I think there's a risk that some of these areas are by their nature, some of the, more poorly sampled or at least more liable to noise, parts of the objects in the model. And in particular because this is, this is all using a model that was learned from vision where it did the kind of scanning policy and now we're evaluating it with touch. and so one kind of natural thing would be to augment the kind of vision model, go back in, and then also learn with this kind of policy and visit a lot of these kinds of interesting parts of the object, which I think would, yeah. Help to, boost the performance. I didn't understand that. Did, if everyone else understood, you can go on, but I was confused by that. You're talking about using two different modalities. Yeah. To, so if you think about the points on this graph that are represented in memory. Yeah. lot. There may be areas where there aren't, many. Yeah, it is a lot of points. It is a lot of points. I agree with that. but regardless of, yeah, and then, and that unfortunately is just something I guess we can't really get around at this point, but regardless of whether we're representing with all these points or, not, we want, a good representation of kind of interesting points. I guess the analogy I'm, trying to get to here is the principle curvature areas on these objects when you're feeling them with your finger are a bit like when you scan with your eye over someone's face and you look at high contrast areas like eyes and a mouth, like you're more likely to visit those points both at learning and spend time on those and you're more likely to visit them. At inference, which will speed up recognition, but you also wanna learn a whole object. You don't wanna only learn those things. So what I'm just saying is, the vision policy is really good at getting a broad overview of everything because it's systematic, unlike the, kind of finger policy, 'cause it does this spiral scan, but, it may not be great at making sure we've visited and paid attention to these kinds of interesting areas, like the edges of a cup. so what I'm suggesting is you can do both. First learn with vision and then go in and add points, learn through this kind of policy. and yeah, that's one thing I'm just gonna, plan on for you is, it really something fundamentally different between vision and touch? It seems no. yeah. I mean you you could implement the spiral policy with the finger for example. there's no reason you can't do that, but it's just, yeah, it's, you're right. It mean because you're basically doing the same thing in Envision a touch, you're sampling one point at a time and. and with vision, you're just doing a more comprehensive search and the touch, you're matching, follow a trajectory. but yeah, but in theory I couldn't, there's really no difference between them, right? We're both the same. No. Yeah. If there's something, this maybe off the wall maybe, or maybe it's related to what Jeff was, envision we have, pre cortical area, subcortical areas that help in determining where you might wanna to next. And so that there are objects, so severe or whatever. So you're seeing a lot of the areas of the object, but not only some subs. Few specific points are actually sent for recognition. Are used for recognition. I'm wondering if there's something analogous we could do in the touch realm where you're moving smoothly but you're moving fairly fast and only every once in a while you actually send the data. Yeah. Recognition. So yeah, and, you could use some very low level technique to decide, okay, now am I, now should I send this data point up to, Yeah. Yeah. So I'm redefining the steps in some sense. It's a little bit like what Kevin suggested earlier. You might say Hey, I'm following this curve, this, it's pretty boring. I know what curves are like, and now I get something interesting. Now pay attention to that one. Something like that. Something like that. Yeah, yeah. Yeah. This is something we've discussed and I think you could definitely do that. yeah, we actually already implemented, huh? I thought this technique was already implemented. Yeah, it's, not, not with the touch sensor, but there's generally, it could very easily be applied to the touch sensor where we just send an observation to a learning module if the features have changed significantly. Yeah, whatever that strategy is, it could be features significant, it could be some other strategy, but basically there's some very low level strategy that decides when something should actually go through the whole recognition process. Yeah. But yeah, I think that's, it's still different, from I think what Kevin was describing, or at least how I interpreted what Kevin was describing, which was more, I was thinking about using the actual movement to, represent, yeah, a curvature. But, yeah, I, think you, yeah, we could definitely do that. And, that especially, by redefining steps, with that. Yeah, you could definitely cut it down. Yeah. Because then in 30 steps, you would've seen a huge chunk of the object. Yeah, it's almost like it's, almost as you're moving over the object, this gets, again, back to Kevin's thing earlier, moving over the object is there's the number of bits of information that are useful, changes that is, if I'm on a flat surface, there's very little new information coming in. If it's a curve surface, maybe a little bit more, but not much. When I get to an und unrelated surface's more, if I get to a sharp co a corner where all of a sudden something, the handle comes out of the cup, that's much more, and it's almost like you want the information stored to storage to vary based on how much actually is useful. saving a whole bunch of points on a flat surface is really a waste, it's yeah, I was thinking the same thing. And I can think, I can imagine how this might happen in neurons, and I'm gonna, I have to spend some time thinking about it. Yeah. I don't know, I also don't think it's optimal though, in the sense that if you compare it to what our brain is doing, like it's not like we're completely throwing out the information of what's happening in between, which is what you would be doing if, you implemented this. So I, I feel like the, best solution is gonna be something in between. I didn't say throw it away. I said store the, it's like you're storing less, when you can, but at the moment, storing less would mean a more discreet, sampling of the space or like a subs sample. Which is problem That's saying not what you're doing here. That's, it's not what's happening here. It's like saying if I'm moving my finger on the lip of the cup and it's not a lot of information I need to do that 'cause it's a nice continuous circle, but at one point there's something weird happening and that point, I, tend to, and I have to store a lot more information for that point. so it's just an efficiency of coding scheme. And, Anyway. I don't think we need to worry about it now, but I do think there's potential biological suggest how it works.

Yeah. There, there's something in information theory about, it's like the, value of taking additional measurements where you already know what the result's gonna be like. You don't need to take, temperature measurements every microsecond because the results aren't gonna change. And having, a lot of information densely packed in one region, it's lot of redundant information. That's a, low entropy environment. there's not a lot of, variation. And the value of that, those extra bits that you're using to store that information is not useful. And Shannon's information theory says that there, there's certain bits which are more valuable because they provide novel information. And I think the temporal memory is very good for this because the anomaly detection, basically if you're scanning along a surface, you can go very fast. As long as that curvature is constant, the second that curvature starts to change, your anomaly score goes up. Okay. That was something I wasn't expecting. And that could be a signal that you use to say, oh, I need to slow down and take more measurements in this. Yeah. But the question is, how do you actually store this information in reference frames in a way that is variable length in coding in some sense, right? You wanna say store more bits when I need to, more or less bits when I don't need to.

there's different things going on. One, one is what are you using to decide what your action is? How fast do I move and in what direction? And the other one is, okay, when I get to a place that's interesting, I need to collect more information, then that goes into this hierarchy of basically features I detected a change in features at this location. Yeah. yeah. I see, where am I at on the thing when I got there? I'm gonna stop talking because I got this idea of how this might work. I'll just have to go through. Yeah. Just to briefly mention, so I did also implement some stuff so that basically you can use exactly what everyone's talking about now, where you, use the, either how much the principle curvature is changing or how much the point normals are changing to determine your step size. but unfortunately, yeah, we still have to take fairly small steps along this surface just because of how this all works and because the kind of limited, concept of surface it has and we don't wanna fall off the object and things like that. and in general, I found that it, isn't improving performance at the moment in terms of kind of step size and convergence, and I think it's just because the principle curvature and point normal is still fairly noisy. but anyways, the ideas are or like the kind of the core implementation is there, so that at some point if we wanna return to that and yeah, basically take quicker steps across like a flat surface like this. Slow down in an area like this, that should happen. But if it's a problem with taking small steps with the touch policy, we can still combine it with the feature change policy. So we, the motor policy takes. Yeah. Yeah. It only sends up a, sensory information to a learning module every couple of steps when the features actually change. Yeah, that's what I was suggesting is that you still take small steps, but you only send potentially informative data out to the cortical system. Correct. A while back, Viv and I were talking about the sort of the big differentiation between representing things with SDRs versus just more the numerical.

And there's always been in the back of my mind, this question is, can you build systems like this that do not rely on SDRs? They're just, relying on numbers and and that's still un unknown. but I do think the solution to how brain solves this problem is gonna rely on the fact that we're not sending that, as you move through space, an SDR representing something, whether it's representing location or a feature, that SDR changes slowly. The bits change slowly as you move. If it's not, if it's smooth, if it's, not smooth or it's more sudden, then the bits change faster. And so the, I think the brain's gonna rely on that principle and that principle doesn't apply directly to the kind of num miracle, way we're doing it now. But it'd be interesting, again, I'll say it's interesting, we could try to make it work that way somehow. way I'm thinking, like right now, we take some number, we have to decide, oh, we gonna save this number or not. Okay, we're gonna save this number, put it this, okay, here's the number, put this, okay. But I don't think neurons do that. I think neurons don't save a new set of synapses for every point. They just add a few more when necessary, or more if it's more required. So just getting back to this, there's a fundamental perhaps, dichotomy between the right brains. To handle information in the way we're doing it here. And I'm not saying we have to abandon the way we're doing it here. I'm just saying we have to, if you go back and look at the brain method and see if we can figure out how to solve this, I'm thinking out loud. Sorry. I said I wouldn't, but I'm doing it.

yeah, so anyways, so that's all of the bottom up stuff. Basic, yeah. Next plan is to, look at top-down driven actions to kinda speed all of this up. And also, stuff that will be, relevant for the vision sensor. What about, oh yeah. Okay. Yeah. What about using multiple columns to help drive what's next? So if you, in some sense, having multiple columns is like having multiple, sensory inputs Yeah. Over time, right? You're just getting them in parallel, so that can also help drive for sure. And yeah, I guess one of the limitations there is just, multiple columns at the moment's, A bit slow. and, yeah, so I'm, it's not something I'm experimenting with at moment, but I think that would definitely be interesting and important. Yeah. Is that slow because of just the way we've implemented or is it some fundamental reason? It's slow, it's a mixture of implementation just because Yeah. Obviously finite resources have gone into the implementation so far and also just weird issues like with the Lambda node that are, people are trying to figure out. Okay. it does seem like this, top down thing could work really well. I, was thinking about the picture you started with the, set of, dining utensils, you had. Yeah. and and I said this said, look, they all have the same handle and now you're handle, now you say, okay. I have, I, imagine there's a lot of other objects, not just these three objects, right? So now you say, oh, I'm my utensils, so what are you gonna do? You know right away that, okay, I have to go down the end here and I can predict either, the fork times or the spoon bowl or the edge of the knife. Yeah. And very, so that's a really, that's a great illustration right there, right? You don't have to spend more, you don't have to spend more time moving your finger along the handle. You need to immediately say, okay, let's jump forward and figure out which one of these three it is by, through a top down action policy. Yeah, no, exactly. it's a good illustration right there.

yeah, so that's, yeah. I promise this isn't my room. Did you get a picture of Sophia's bedroom? Is that used to.

What it, yeah, some of this stuff is weird. what are you, just these, like a lot of old stuff here, like old photographs or, old projectors, those are old boxes, Yeah. And some sort old cord phone anyways. yeah, the other, so I'll all that with policy was a bit about, I guess primarily focused on efficiency. but now I'd like to talk about more kind of speculative stuff and in particular, how kind of action policies can be related or, leveraged for dealing with multiple objects, cluttered scenes.

and so when you're dealing with multiple objects, yeah. Somehow you need to have a visual understanding of the scene. As was said earlier, in computer vision, often this kind of takes the form of segmentation, but, we want something richer than that. there's of course, a variety of cues you could just use from, oh, sorry. It should be, yeah. Anyways, not those kinds of cues, that you could, I thought some my British people, that's how you spell Qs. Yeah. maybe, I should just, claim that. No, yeah, like depth and lighting and, texture and so forth to segment the image. And of course a variety of deep learning methods that could help with this. But ideally, we wanna avoid, that kind of approach, and in particular ideally leverage Monty's knowledge of objects to do this, in a principled way. I like that you spelled color correctly.

Yeah. I honestly though, I'm, for the last 12 years I've been teaching my spell myself to, type with British spelling, and now I'm teaching myself to undo that.

yeah. anyways, but before I talk about policy, actually, I think it's worth talking a bit about how we want to represent, multiple objects because I don't think this is something that, or at least while I've been here, has been discussed, a lot. And, but I think it's an important kind of question because, imagine you're sensing this knife and you have high evidence for the knife, your confident it's there, and then you leave the knife and then you go. And then you feel the coffee mug and you start feeling it and you become con confident there's a coffee mug. But of course, none of that evidence in that area of space is evidence against the fact that there was a knife. if all of this is, say, in a allocentric or egocentric reference frame, then you know, both of these objects exist and with some relative, arrangement to one another. but as everything's currently implemented, it's not clear what exactly a single cortical column would do to try and represent all this in general. Yeah. So in general, and I think, yeah, like just saying it would make sense that it, represents one thing, but then, the kind of natural question then is if we might have a in some kind of level in the hierarchy that is representing the actual objects and their relations amongst each other.

but yeah, but this is just something that. I think that's right. That's, that, that is gonna be implemented. and it ties into, the questions of hierarchy. I think it's gonna require a hierarchy, right? It's, it's, say you're gonna have, one region of cortex, which it's got a reference frame that's larger in space than the, one below it. And so whether these are objects in the room or these are features that are part of a larger object, that's gonna be the solution, I think. And, therefore you recognize the, different components and they get located as features in the larger object, which is a hierarch higher level, reference frame. So it's require two reference frames of different scale Yeah. For certain that, and then that seems to be, has to be the answer. Yeah. Yeah. And then I guess then. a lot of times we might not have to then instantiate new graphs for new scenes, but perhaps, obviously there are some scenes that have a certain underlying structure themselves that kind of repeat like, tableware on a table or sorry, dinnerware on a table or messy roots. Or messy rooms. Yeah. Super. I saw that, reminded him of his daughter's room. Even though none of the features were identical, almost certainly. but we all recognize it as a messy room. So that, gets down to the, this other issue that we brought up earlier about, classifications and, similarity between objects that are made of actually different components.

that's, a bit mystery still right now. I think we could do worse than just say, okay, when we're dealing, let's tackle an issue like, like that and say, this is, will establish a reference frame, even if it's a temporary reference frame. where we can then place these objects in relative positions to each other. yeah. and so that's, if we say, okay, we're gonna do this every time for starters, we're going to create every time we can, we're gonna create a new scene and, and then we'll be trying to recognize components and place 'em in that scene. And the next time around we come back and, start again. That wouldn't be a bad place to start. Yeah. and not ask ourselves how is it that room reminded to die of his daughter's room? That's right. Yeah. Yeah. we all recognize that, right? We also immediately that it was a cluttered room and I saw as a cluttered room of old objects, which is an interesting, anyway, yeah. And then, kind of one other kind of element of this is, yeah, when returning to a location in space, obviously it would be useful if the C-level representation helps us, re instantiate or more quickly recognize objects that we saw there previously. essentially a form of object permanence that, yeah, if you return somewhere, you don't have to study the teddy bear again or whatever to recognize it. You expected it to be there. So as long as there's even a small amount of evidence for that, then, then that will confirm. that would come kind of representation. When we talked about the way the, i, was given a little discussion about how I think the neuroscience does this with these feet forward and feedback rejections between regions that would do that. That would solve that problem. It's I saw it here. Now I come back to that location in the scene. I will then immediately invoke the expectation of the object I saw before. so that should happen if we do something similar to that. Yeah. Even in the original Monte architecture, we had that short-term memory module, which is learning exactly the same way, but it's, and it's representing the structure of the current scene. And then, Just not a permanent representation. It's a temporary representation of the Yeah. Scene structure. Alright. I was wondering when that would come back in. Oh yeah, this was before my time. I don't remember. Okay. So that's cool to know that there's a short term memory module.

yeah. That the idea that each learning module has like a hyper parameter of how fast it can build new models. if it, pretty quickly builds them up pretty much immediately, like short-term memory, but then it's also not very permanent. Or if it's slowly built, like very stable in general models. you wouldn't want it to always be, you wouldn't wanna classify it as, temporary. It'd be more like, Hey, if I saw, showed.

I would recognize it, I wouldn't say as, oh, here's another cluttered room. and of course if he showed it to me three days from now, I have no memory of it. so on my point is that, we can, there could be a continuum of yeah, build something up, and then it, the memory fades slowly. if I don't reinforce it again, that's what we do, right? That's, what, most of our memories lie.

and yeah. But in the short term, I'm just gonna propose that we don't worry too much about Getting an a scene level kind of understanding at its most basic, what we first wanna be able to do is just given a series of objects overlapping each other, be able to recognize everything that's there and when we're looking at something correctly recognize, what it is. and then as, and so that, that way this kind of work can operate in parallel to, for example, work on hierarchy, which would be important for the true kind of scene representation. Yeah. It seems like you should be able to do that, right? Yeah. So you should, you need to be able to recognize the objects first. Yeah. And exactly. And, it's necessary for, the next step anyways. So, with that kind of discussion to the side then, I wanted to return to discussing actual policy and how it could, kind of action policies and how it could relate to multiple objects. So just a simple, one that, seems like it would be useful is to stay on an object or at least try and stay on an object, until there's a reasonable amount of confidence of its, ID imposed. Obviously sometimes this is gonna be harder to do, than in others. and this is probably going to be scenes that are harder to parse and visually understand. But, yeah, there's a variety of kind of, heuristics that could help with this, as well as though, top down cues. both based on the model that you might have of a car, you'd expect if you are gonna see, the kind of car anywhere else, you have some understanding of kind of where that would be. and you can also kind of, I'll get to combine this then with, multiple modules and how they represent objects to guide you to kind, certainly kind of sample, for example, in a clustered region like this, but potentially in a more kind of intelligent way. Also realize, okay, let's try sampling over here and see if it's consistent with being the same object or, not.

and similar to the, similar to the, staying on an object until you're reasonably confident. I think, yeah, generally focusing on nearby foreground objects seem to have less occlusion or near in depth again, should, make it easier to resolve a heavily occluded scene. but then, yeah, ultimately there may be genuine ambiguity, genuine uncertainty in a scene. And there you can move your sensors as these people have done, or eventually, we will be able to, interact with the objects. and thereby reduce uncertainty as well. And again, there may be some scenes that in a completely passive sense would be impossible to disambiguate. but this kind of sensory motor element will, would be important to. if you went back to this picture of the car. Yeah. and if you, instead of the car being black and white, if it was green and instead of, the bush being done with all little blobs, it had little lines of green in it too, it might be very difficult. much harder. So it just stinks, reminds me, there's all kinds of clues here to come into play. In this case, color is a big one. And also the general texture of those components is a big one. There, there aren't any lines in the tree as much as there is in the, I'm just saying. it's a multifaceted problem.

and we just have to be careful that we don't make it too hard for us. yeah, and I also think with some of these things. yeah. And I know this is a maybe controversial argument, but, yeah, what some of the things you're describing, they, they can be hard to, yeah, codify and write down explicitly. and so I wonder, this may maybe if this is one area where we want to use a bit of deeper learning, just in extracting some of the kind of features. and that kind of tell us something about okay, how similar are, parts of an object like this and this, in that kind of feature space. which, yeah. I don't think it would be too crazy if, the brain is just doing maybe, which kind of some bottom up, some non-linear, processing. I was thinking, I was think the opposite actually. Maybe misinterpret what I was thinking here is that, is if I'm looking at the car on the left side, yeah. I don't get fooled by start looking at some of the green things and see if they make sense with the car. it's, not like I'm jumping to the right. It's more like, how do I know where I, it's clear I'm looking at the back of a car. I recognize that as a VW Beetle even. And but I'm not confused by the green bush because it's, an easy distinction. And I'm just saying in terms of action policy, it's an easy action policy. In this case, you just Yeah. Because, if you, if I occlude everything to the right off the left edge of the tree, I can still recognize the car. Yeah. And I don't actually need the other end to recognize the car. But once I recognize the car, knowing the morphology of cars, I can actually predict there's gonna be a front wheel there. Yeah. yeah. Yeah. It's really not about bottom up, bottom up cues. That's, but but I think the bottom up queue might be important to, as for saying if the car was green and maybe the textures weren't as different than it, it can be hard to Stay, actually stay in this area, it's just know what to do. to me, this is really an example of a, recognizing this as a car is a bottoms up action policy, right? I don't, the super was saying, I can just look at the left side here. I'm following the black lines are not like the green stuff. And I follow 'em and very quickly I say, oh yeah, that's a car. multiple columns could be voting on that at once and see that's a car. Yeah. and, that's, a bottoms up thing. now obviously once I receive it's a car, then I could test some things. Like it say, oh, that's, maybe there's a taillight. Oh, there is a taillight red. And I think in standard computer vision, it's very traditional to use these like really low level cues like color and, t junctions and what can, what overlaps you can have, but I don't think that's really necessary here. I don't think it is. you just, again, I look at this image and if you don't see what it is right away, which I could argue maybe I did, but then you would just, then you would narrow your attention down and maybe the action policy, here's this, go to these, these high frequency components of the image, which the, what the black lines and just move, my attention around those things. in the vision, our vision system. That would be multiple columns still that's moving around over that rear end there. And I'd say, oh yeah, that's a car.

but sorry, can you say that again in terms of why, you're saying you don't feel like these kinds of bottom up queues would be necessary? You saying the bottom of queues aren't necessary? no. In the traditional computer vision way that they deal with occlusions and all, which there's like specific types of edges and junctions that can occur in Oh, I see what you're saying. Yeah, That you rules of continuation and which is what you edge continuation and color differences and all this like really low level. Cues of when occlusions occur and don't occur. And I don't think we need any of that. No, that's true. Yeah. Yeah. That it would be better. No, I, yeah. Actually the edge consideration, no, I think that's a fair point that yeah, it's, it is much more powerful. what I had to hear with Yeah. You recognize the object and then, you just know from your understanding of that object that it should be there if, the bush isn't there, kind of thing. Yeah. Yeah. one, one question I have is that you have the percept that the tree is in front of the car, that's gotta come from some higher level. yeah. So that would be in the like kind of scene representation. And so that's independent of, 'cause I look at the fact that even if, let's just, exclude the left hand side and the right hand side of the car, you just have that one little blind contour in that notch in the tree. You would still have the sense of a precept of the green object is in front of and somehow occluding that other, whatever that is.

Yeah. border ownership and, yeah. yeah. And then I think a lot of these things also, yeah, there's probably multiple, I think to get something that kind of works and is able to recognize objects in a, in alu scene. Yeah. Starting points that's a required Rio that I had to figure that out before I recognize the car or are you just saying it's something I do top down later or say, yeah. Lots things that they must be in front of that. I would say that's, it's, I don't know which comes first, but I'm saying that the, there is a percept of how to interpret the scene based upon there's a sense of one thing's including the other. Okay. So I think that's a top down thing myself. So if you are, I agree. I agree. Okay. So I don't need to do that. I'm just saying, but that's a necessary precondition. I'm just saying it, it, somehow conditional realizes your, you're able to connect all the pieces, together. Now you're right. You can just look at the left hand side and say, yeah, there's a car that looks like the rear end of, of a VW bug or something like that. Yeah. you can do that. But I'm just saying that these things, whether they happen, one after the other or sequential or whatever, they, inform each other and, become part of the composite, percept of what you're looking at. Yeah. I just wanna point out it's 1120. We've been going for over an hour, on, do we have additional material as well, or is it just this presentation? maybe are we close to your end here, Neils? yes. Yeah. So last slide. yeah, this is just getting into kind of multiple columns. So yeah, you could have, as was discussed, like islands of consensus and, but it may be necessary to use something like attention to narrow, the kind of fields of, columns that are, communicating with each other, to get some kind of consensus there. but yeah, for example, in a setting like this, here it might be where you're focusing high sensitivity sensors in the case of a human, the phobia. but then you might be getting kind of signals of kind of something similar, something hubcap or whatever in the periphery. And then. whether it's through direct lateral connections or first via a level of hierarchy, there might be then or yeah, there could be a way for learning modules to communicate with one another and say, okay, maybe we should move the high, kind of density sensors over to this location.

and just a similar idea would be in, if learning modules in general were very uncertain about what was in a particular area of the, of the scene, then they could again ask these kind of more high density or other modality, sensory modules, learning modules to, essentially come and help identify what's there.

so yeah, all I, think, yeah, a lot of this, a lot of the ideas so far are fairly straightforward and basic. I think the main thing is just how we go about implementing this. And, yeah, that's the next step really. I.

Thanks very much. Good. Alright.

Okay. Yeah. You guys still upward? Sure.

One, one of the bottom up, parts of vision that I think will probably be necessary in the situation, especially with the occlusion, is like the blob and the inter blob cells where one is telling you is this is a contiguous region and the other one is telling you, okay, this is where that, region ends and another one begins. So having that be able to segment the, space is important for figuring out that this part of the vision space is one object and this is something else. Would that, does, is that possible to put that in, into a sensor in your current framework? You talking about blos like in, in v1? Yeah. Yeah. The blob, like the edges versus the, gradation, continuous regions.

I, think I need to review that 'cause I don't Yeah, I'm also unclear how, can we, table that for now? Because I don't think we else understands the blobs thing. Yeah. it's the low entropy, like smooth regions versus the high entropy, like edge regions. That's Could you send a paper on that, Eric, that talks about that? Just succinctly use that issue? yeah. I'll find it. Yeah. I don't remember. I don't remember that being associated with the blob, so I just, I must not be up to speed. I'll see if I can find it. Thank you. Thanks.

Okay. Yeah, so this presentation is maybe a little bit more high level and vague than Nielsen's. yeah. As you already saw in the timeline, it's like these two branches, action policies and hierarchy and action policies has already started. Neil has already been working on, but hierarchy we've really only talked about in, the brainstorming meetings, and I've tried to condense some of this in here, but, yeah, sorry if I didn't capture some of the ideas, that we were talking about, but, yeah. Just really quickly, yeah, what are problems that we would like to solve with hierarchy? So one is, the classical problem that we've been talking about. being able to quickly associate learned models with each other without requiring completely relearning it. So for example, if we have already a model of a cup and we already have learned a model of the NOA logo, and then we, for the first time see a cup with a NOA logo on it, we can very quickly associate the two without having to build a completely new model of this object. And that's the way in which I'm gonna phrase hierarchy in this presentation. Basically that hierarchy has the purpose of associating models with each other.

yeah, I don't know if that's just my interpretation, but, Yeah, let's see. and then, yeah, that kind of ties into the second goal, of being able to represent higher level object part relationships, and more complex object structures without requiring huge amounts of memory. and yeah, I'm gonna make a bit of a detour right now because I feel like this is a pretty nice example to visualize this problem or this property that we need. so basically, I've tested our Monte system on the Omni Guard dataset, which is, yeah, now it's actually two 2D data, and 2D is very nice to visualize this problem. So basically the GL data set is a bunch of different alphabets, and each of the letters in the alphabet is drawn in many different variations. So they are 20 versions, 20 drawings of each letter, of each alphabet. for example, this is Korean, and then, we have this v it looks like an H and then, but the h can take very many different variations. So the two bars can be further away, from each other or closer together. They can be longer lines or shorter lines. And the, horizontal bar can be tilted. so there are many different variations. And the multisystem right now, if we have learned, this first variation of the age, it can recognize it again if we see it and it can recognize it with small local distortions. But if we have these more global distortions, like longer or shorter lines or a, larger distance between the lines, it can't really deal with that.

Basically, just to illustrate that real quick, here are the different versions of this character that is in the data set. And if we overlay all of them on top of each other, it gets messy. And if we want to represent this with one model, we would basically have to give the model a lot of slack. So that would mean, anything in this purple area, goes for this H character. I'm just gonna call it h and I know it's a bowel and it's pronounced differently, but, I'll just call it h for no. but then if we have this model that allows for a lot of slack in this spatial field, then this would also allow for the letter to look like this, like a, it would also allow it for it to look like this an x, it would allow for it to look really funky, like, this. And that's clearly not what we want. We want to preserve the sub components of the ladder, like the two longer vertical bars that are parallel and then one smaller horizontal bar. But, they are, relative arrangement should be possible to vary. and I think this is something that would require hierarchy. So if we do this with hierarchy of how I was imagining it, at least we could have, lower level models. And I know now it's in 2D, but I'm gonna show an example in 3D in a moment. And in general, all our models, will inherently support 3D or B in 3D. But basically we would have, models of lines of different lengths and then, one level up in the hierarchy. These models or object IDs would become features with poses. So basically the longer line would be a feature that would be present here and here with the same pose, same orientation, and then the shorter line would be at this location with a 90 degree rotation. And then, yeah, so this would basically question, yeah. Quick question here on the lower level module models, when it's. Representing these lines. it's not just an edge. It's actually representing it as an object that it, you're gonna recognize this thing under all different variations and poses. It's not just Yeah. it's not an edge. That's a big distinction. It's a complete object. Even though it's, you're writing this as one letter, but this could be like three letters in a word in some sense. it's like CAT for cat. This is, this thing is a vertical, horizontal. Vertical. Yeah.

Yeah. And yeah, like you said, this is definitely not just an edge, it's like a full object model. And even though I'm showing this as an example on the 2D image, it, it is represented as a three dimensional graph in theory or definitely supports it. so all of these are full object models. I think we have to be, when we, if we explain this externally when we do, we have to be careful about that. 'cause that's a big misunderstanding. That happens a lot. and it's a common reason why in, neuroscience, you get these edge detectors and you miss the fact that it might be modeling like a complete Yeah. Object that happens to show up in, as an edge. Yeah. Ex. Yeah. Yeah. I hope it will be a bit more clearer when I show this on a 3D example, but, yeah, I thought this would be a bit easier to follow for at first, and to see the problem. But yeah, basically now we can constrain the degrees of freedom that we have. So basically we say, okay, we need to have solid lines. They need to be like straight lines. They can't have breaks in between or something like that in order for this feature to be detected or this object id. But, then the pose of these features can vary slightly. So we can, for example, rotate the bars a little bit or we can adjust their relative position to each other a little bit. But that significantly reduced the degrees of freedom. And then, two more side notes.

the output of the higher level model is again the same as the input. Again, a feature ID and post. So it would output, feature Id would then, for example, be the ID for this character. And the pose would be the pose of this character if an object is rotated. It works the same as the lower level models. It just outputs the same feature ID with a different post. And it then basically just assumes all of these features will be, have their posts rotated relative to each other. So it works exactly the same as the lower level learning module. Same input, same output, same inner workings. that's like the principle behind it. And then, but because it constraints the degrees of freedom, and now if we take this, these other variations, this would not be recognized as the letter H anymore because this curve line is not one of these features that are part of the H model. This would also not be recognized. It would recognize the longer line as a feature, but the pose relative to each other is wrong. And then this would totally not be recognized. It's interesting. the lo left one, is at lower case H, but you're saying that would have to be represented as a independent Well, this is a Korean Yeah. That, yeah, this Korean doesn't have anything to, but suppose this was English, it's Yeah. It would be a separate model, nor case H would be separate things. It's not, yeah.

Vivian, just to be devil's advocate, if your original h was composed of dash lines Yeah. How would you handle that?

yeah, that's a good question.

that's just aligned with occlusions. It goes back to representing. These lines as complete objects, right? If the occlusions became too much, then you wouldn't see it as a line. If the oc, if it was a, if the dots were close together and there was a lot of them, then you wouldn't have a problem. You'd still see those as critical line. That's fascinating that the brain seems to like automatically have that prior, that like sometimes what you're seeing is just like a known object with ions. Like that way you interpret a dash line as one continuous thing naturally, and you assume that the butch is in front of the car just like that 3D prior. A prior versus That's a yeah. One thing as well, I wonder if, it, yeah, I guess eventually some sort of top down, feedback, might be important or helpful. if the hate, or, sorry, Korean letter was overlapping, let's say another Korean letter slightly like a capture basically. and you were you're trying to parse the low level, but. just like which ones are, what line segments do we even care about, and then the information from the higher level can potentially help with that. and I guess that kind of iterative process is what a lot of Yeah. Different systems try and do for this kind of thing. Yeah. Yeah. And I'm, yeah, still not really thinking of this as like this pure hierarchical thing where we first get the lowest level features and then the higher, and then the higher, and then the higher, but more like an association of models with each other. So basically, we also have this higher level model of the age, which has a certain contour shape.

and that. I don't know if that makes sense how I'm explaining it right now, but, makes total sense.

yeah, I think, also you have to, look at the fact when you have the HI mean you can, okay, the occlusion, you can knock out my, dash line, easily with that. But if, for instance, you can see an h if the only difference is a difference in texture, I think it's the, notion of what constitutes a feature starts budding into the notion of what is a connected region of a similar type. So, this, I think the, this is bumping into this co this the thing we talked about earlier, that there's these two sort of forms of the model. There's a morphology model and a feature based model. And, the way I've been thinking about it, just crudely, is that the morphology model is much more forgiving. It's much lower reso, it's much lower resolution, less high frequency. and that seems like fuzziness or dotted lines or these are all more things that would surface in the, sort of specific feature, component of a model. So that's why you can recognize the morphology of multiple letters that have different colors or different textures or, and so on. None of that matters for the morphology model. they matter for the other details.

in the same way that a coffee cup is a coffee cup, the, the morphology is the same, but if I put the logo on it, that's a specific feature on that particular cup, but it's, it doesn't change the fact that it's a coffee cup. So I think some of these issues that we're touching on here are gonna rely on this idea of these two sort of model components. Yeah, yeah, let me show the 3D example real quick. so yeah, now moving to some more complex examples than like simple characters. here, for example, a bunch of different airplanes. And if you would wanna represent all of these in one model, we would have to allow for a lot of slack in a bunch of directions. And it's just, they vary along all kinds of dimensions. even with hierarchy, I think this is difficult to, do, like the bomber plane is like quite different from, the other planes. And like the X-Wing is also, that's not an airplane. I just wanted to point out, the exit is not an airplane that jumped right, out at me saying one of these doesn't belong. So airplane, a space plane is not an airplane. Yeah. All the other airplane. But yeah, so the standard airplanes have a pretty common structure, and. Yeah, just as an example, we could, it could be represented the same way as I showed with the letter. So basically lower level modules would have models of cylinder and the tube shape and like wing shape, like up, models. And then in the higher level model, we would store the relative arrangement of these 3D models as features relative to each other basically. And that would then make up a generic airplane model. But the relative arrangement and tilt off these features can vary a bit and thereby allow for the different versions of airplanes.

yeah, you know what's interesting? what's so puzzling about this is that clearly we can do what you just said. We can move these around, give 'em different shapes and so on. Yet at the same time, I can identify many of those planes individually. so it's again, this goes back to the classification issue. it's puzzling in the sense that we can, have models that we can really recognize a new plane. It's, but we can also recognize very spec many different specific ones.

it's just, bringing up the classifi, the, generalization classification problem again. Yeah, exactly. And we even do recognize both at the same time. We recognize a specific airplane, but we also know that it is in the general care category of airplanes. and recognize a specific one. I then have to attend a specific features of them. Typically, like the one on the bottom center, I said, oh yes, that's one of those transport plans because the front looks funny or the one to the bottom I said, oh, that's got propellers on it. So it's a, over the wing prop jet, I, it just funny, it, I'm just like speaking out loud. It just, there's numerous problems or in, or interesting things that come up in this analysis. Yeah, Like a functional analysis. All the airplanes have wings that can support themselves in, air, apart from the X-ray. That's not the x-ray form also wouldn't, but I have to know that, to know what an airplane is. I don't have to know that wings, what wings do. a child could play with a toy airplane and not know if they fly.

interesting point. yeah, you can recognize a category without ever realizing why it's a category. Yeah. there toys that, toys you look at don't mean anything but you recommend them. just, yeah. That's interesting. some of these problems like this classification versus specific general category thing just keeps popping up over and over again. It's gonna make you feel like, oh, we have to deal with this sooner than later maybe.

Yeah. but I'm not disagreeing in anything analysis. I'm just yeah. No, I was just trying to think about how we could deal with that. It could be, for example, that the general category model of an airplane loss for a lot of slack in all of these relative feature arrangements and also for additional features to be present, like the propellers and stuff like that. But then if we have a specific model, it associates additional specific features with this or constraints the pose of some of these features. in terms of the work you're doing. If you wanted to avoid this problem for starters, it wouldn't bother me. You might say. Okay. We're just we're trying to differentiate different. Specific airplanes or something like that. and not worry about the general classification problem, but I'm just saying we don't have to bite it all off at once. Yeah. Yeah. even, yeah, even without, the task of categorization, this will give us the advantage of being able to store complex, very complex objects like airplanes in, a more efficient way. yeah. yeah. Arranging these sub parts relative to each other. yeah. So yeah, coming back to problems we like to solve. So on a higher level, basically the key, capabilities we would want to get out of hierarchies. being able to deal with composition object categories and also structurally complex objects in an efficient way.

yeah. And yeah, I, a few slides on how this may be happening. I'll go real quick, but yeah, basically, we already have the bottom part of this diagram. now we have to figure out what is the communication protocol, for the, between learning modules and different levels of the hierarchy. and ideally we wouldn't wanna really touch the inside of the learning module or generally the input output format. and, design it in a way that, yeah, hierarchy allows us to associate models with other models as features basically. So the idea for communication would be to keep the communication protocol. The same no matter where a learning module is in the hierarchy. So basically the input is the same as the output. It is, features a number of features plus a pose, and features at higher levels can be object IDs. they always have just one pose may possibly relative to the body and then lateral voting between, there's, yeah. And then we could have lateral voting between modules who share models of objects. This is unrelated to hierarchy, and those can vote on multiple possible poses. So that can be a list. and that communication is basically independent of what is going on inside of a module and doesn't pass on information about the internal model itself, but only about public id and post that all seems right because I think you're saying the lateral voting point really isn't about hierarchy. That's, yeah, that's something that exists without independent audit. Yeah. Yeah. I guess I just put it because I was talking about communication in general, but Yeah. Yeah. It's good to clarify. Those are two different communication pathways, right? Yeah. What would be the difference between, let's say lateral voting and top down voting? there evidence from above could impact, impose restrictions on what inferences you may lower down. Yeah. I haven't really think, thought about top down feedback at all yet, to be honest. that's definitely something we still need to figure out. It could be thought of as just another, voting pathway. Actually, I. Yeah. Yeah. But they would have to have the, they would have to know about the same object IDs. And if we say that the lower level module has more of the part component IDs, then the higher level module would need to know, like if it votes on a, on the whole object ID, that this is associated with those certain parts. That's the same thing is true for lateral voting, because if you have voting between touch sensors and vision touch modules and vision modules, they may not have exact correspondence of object IDs. It could be a many to many relationship.

I think just part of the answer to this question seems obvious to.

The way I've been thinking about before lateral voting, you're always voting between columns that are trying to infer the same object. Meaning they have models of the same objects and they're trying to reach a consensus that we all understand that this is the same object when the hierarchical voting. That's not the case. And hierarchical voting, you're just associating the location on one object, the higher level object At this location, I have been, I had A feature can be an entire object from the region below, but I had a feature, I don't know what that feature is. I don't know what it means. I don't have a, i, I may or may not even have a model of that thing. I just know that I was given some bits that said, guess what's at this location. So I'm gonna tell you, I can feed back you saying I'm at that location again. You shouldn't have broke the same thing you sent me before. so the cup could be saying on this location, you showed me some bit patterns. I didn't know what it meant. Turned out to be the logo. I'm not gonna tell you. Invo that same, ID, I don't know what it means, but you're supposed to invo it. And so now you have to vote for reference frame for the, logo, and instantiate the logo, which is really in one case, you're voting on the same objects. In the other case you're voting on different objects. You're not even voting, you're just associating different objects at some location. Is that would that address what you're thinking about? Or, so I understand that distinction. I'm just thinking that lateral voting may, in some situations, may have the same properties, and so maybe can we think of it as the same? and example would be, again, audit, multiple sensory modality. Maybe you hear something versus you see something is not necessarily a one-to-one relationship between them. It's more if you see something, it constraints, the types of sounds you might hear. the same thing that the object up top, but the higher level will constrain the objects that are possible down the. Biological difference too though, right? It is, from what I understand or recall, the lateral connections are layer two three to layer two three. Whereas the top down connections, I think are layer four, back down to layer two, three. it's, or at least the connections go that way.

Six, six to layer, there is anatomical layer. There is layer six to layer one. Yeah. And then going backwards and going forward, it is primarily layer, two, three to layer four. And the lateral connections, it's more than just layer two, three to layer two, three. It's also layer, there's to half layer five. So there are, it's, this is the way that, that it was done back in the, paper by, Connections in hierarchy. They said, what, how do I decide two regions from the same regions 'cause the same cells in the same layer connecting to each other. So layer five is connecting layer five or layer two three is layer two, three, that, those are considered al. So those are the same level in the hierarchy. And then, anything that's different cellular layers like layer three in layer four, or layer six to layer one, those are hierarchical connections. So I, that's in my mind is the way I continue to think about it.

I think as a mechanistic point of view, maybe you've got a point that perhaps the same sort of similar kind of mechanisms going on. I'm not sure. Sure. I don't know either. I'd have to think about that. But just hypothesis is if, can you say.

Whether it's acts more as a gating mechanism as opposed to a voting mechanism. In other words, if there's one thing, is it, possible for it to say, despite the hypothesis that you're coming up with down there, I have this overall gating thing that negates all that. In other words, it's a conditional rather than a, I think, I don't think you wanna throw out the thing at the bottom because that would be total confusion. Be like saying, I think I'm on the coffee cup and the guy, the bottom of the bottom and you should be seeing a logo. And the guy on the bottom says, I'm seeing a car. that doesn't make sense. That would be like, something's wrong. I think what it does do, Kevin, is that the top down feedback says it biases the lower level region. It says you may not be certain what you're looking at, or maybe you're not even seeing anything yet, but I'm telling you what you're gonna see, what you should be seeing. So you can use that to, to both predict what you're gonna see or, to refine your, con your narrow down, your confusion. okay. I don't disagree with that. I'm just trying to think of if, the lateral is, is voting based upon various pieces of evidence, whether the top down has either some primacy or might be able to, I just think of how we, in, in the dendrites project where the dendrites basically conditional what you're going to see at other levels, whether it might have a more, a different. I don't know, halfway mechanism, but I don't think there's a primacy to this. you've got different parts of the, cortex are saying what they see, and if they're incompatible, then the brain really wants to make 'em resolve that incompatibility. It's not, it is not gonna say, just ignore what you're saying, I'm in charge. I don't think it's a, I don't think it's a primacy. I'll just put it that way. okay. So let me, give you an example. If I'm searching for a particular book, that I know has a red spine, I will exclude a lot of other things that I'm looking at. And, that filtering mechanism, it says, you at the lowest level I can say, you're seeing, coffee cups, you're seeing all these other things, but I can tune my vision so that I'm only looking for something and that's the only thing I'm allowing to come onto my, perception. I think that's true. that is oftenly true. In that case, you'd be saying, okay, you should be, I'm telling you, be biased to look for this book. You should be looking for the book and then the lower region will try to fit its inputs into a book hypothesis. and, and I agree with that. but if the lower region's not seeing a book and it's seeing a coffee cup, it's not gonna say, no, it's, no, it's too bad. It's gonna say, no, I'm not right. It's not right. It's not right. Keep going. Yeah. It's interesting at a mechanistic level, the way we implemented it, they're very similar. Gating and voting are basically done with almost identical mechanisms. Put the S. Yeah. Oh, that's good. I like that. So you don't make the hard distinction. I was trying to get away from a hard distinction about gating I, the primacy of gating. I didn't like that. But as a continuum it works. Yeah. Its all basically all just dendrites. apical dendrites do have slightly different behavior than lateral, basal end. So that's maybe something there, but I dunno. The other, another comment I wanted to make here is you have things represented as discreet object IDs as and this is something that the comms paper as well, but we've all often. Thought about that as not being so discreet, but being more of an SDR R where there's some Yeah. This gets you back to your question, can you do all this stuff without SDRs? Yeah, exactly. Yeah. How far, can we go before we have to go like version two of money, everything we have to do, I don't know. Yeah. It's 42 versus object ID 1 0 3 as opposed to thing feature, vectors or overlap SDRs that have overlap and things like that. Yeah. Yeah. And I think the communication protocol should be relatively unaffected, regarding how we actually represent the features and the posts, like we could represent them as SDRs, as Well, it's a question of similarity, like object IDs implies just categorical things that have no inherent similarity between them as opposed to distributed representations that have some notion of. Yeah. Yeah. That's interesting. Yeah. Yeah. we've had this discussion over and over again about, do we have to use SDRs and if we do, how far can we go before we have to use Zoom?

I'm fine continuing without 'em, but, it keeps rearing its head every once in a while. Yeah, that would definitely be a, good reason to use them, to be able to express more similarity, between objects. Interesting. it's interesting the question, could you do a hybrid system? Could you have a system, very high level thinking where, you can inject SDRs where you need 'em, but go back to numerical, stuff when you don't, or you knows, represent categories too. So that's.

Can you have a system where part of it's running on IDs and numbers and the other part's running on SDRs and to all SDRs. we're now, today we're all numeric, we wanted to do both. I would just say just use SDRs. it's a big, it's a big, it's a big thing off it. Yeah. incrementally for example. Yeah. Sorry. It'll also make things slower. Say what you gonna say. Yeah. For example, we could keep everything in the learning module, how it is right now, and then just represent the object ideas and SDR that, expresses similarity. And since at the higher level, the object ad basically becomes a feature, the learning module automatically takes into account the similarity between the SDRs. So that would be a hybrid system, That would be, great. I'm, it's not clear to me you can actually do that, but if you can, that would be a nice, that would be a nice. It would be really nice to not have to throw out everything and start over again. So if that could be done, that'd be great. It's not clearly how you do that, but we have our company meeting in less than five minutes. Yeah. one more slide. So maybe I'll just go over that real quick if we have another minute. yeah. We got five. yeah, so just how would this look like in the learning module? So slide before was the communication. So each learning module learns three models as we already have it, no matter where in the hierarchy. higher level learning module can use lower level learning module outputs. So object, idea and post as features. could also combine this with direct sensory module output. And then the lowest level learning module could also in theory learn complex object models like an airplane, if that would be needed, but it would require a lot more memory and training and would allow for less. Category generalization. And then the last one is, how would learning work? just make a point on that. It's not clear to me. Don't have to explain it to me, but just away, I don't why we're allow for less categorization. I'm not questioning it. I didn't jump out mes oh yeah, that's right. I don't know why. Basically like the example with the age that basically we can have, we, it allows for small local distortions, but if the, if global parts of the object are distorted, like the bars of the age are further apart or closer together, then it can't really deal with that. Okay. I think, I think there's other ways that we potentially solve that problem. Just leave it at that. Okay. yeah. And then and one big question that's still very is how a learning would actually work in the system. Like how do we decide where, what is being learned and yeah, that's the pretty big question mark for me.

okay. This is my favorite slide. it's, it doesn't seem as mysterious to me, so maybe we can talk about that if you want.