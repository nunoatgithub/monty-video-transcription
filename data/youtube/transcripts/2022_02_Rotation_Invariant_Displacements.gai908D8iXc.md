Okay, this should be pretty quick, like five minutes probably.

without discussion. Takes five minutes. Worry with a simple diagram here.

okay. So basically this is an alternative to, to using the 3D displacement that I used previously to match graphs. So before in order to match graphs, I basically used the difference between the previous x, Y, Z location and the current X, y, Z location. The displacement between those two points in space. And this would, and that displacement is not rotation in variant, which means you can't recognize the object if it's rotated and using this representation. why, could, why wouldn't it be? Because the theory, if you rotate the object, you rotate the reference frame. And you're just saying, if I don't know how I rotated it, it doesn't work there. the, it's assuming a priority. You don't know the reference. Yeah. If you don't know the re frame Yeah. Given that the displacement and relative to the objects reference, then you can't do it. yeah. Okay. But if you calculate this place, it's dependent on the object reference, then you could, okay. Yeah. Yeah. If you would have that knowledge, then that would also work. But I'm assuming that you don't know that. okay. and, with this, it would still, work without knowing. Knowing the, that reference frame. Okay. so let me just go over real quick. so here in the, that's what it says in the paper where they introduce it. So for two points and one and M two with normals and one and N two, we said D, which is, I would say the displacement that I previously used equals M two minus M1. And it define the feature F as the length of this displacement. And then three angles. First angle is the angle between the normal of the first point and the displacement between the two points. So it's this angle here. I hope you can see my mouse.

the second angle is the angle between the normal of the second point and this, displacement between the two points. And the third angle is the angle between the two normals.

and this, those four numbers basically denote the feature between those two points in space a second. It, just, my intuition thought, said that the third angle is, determined by the first two angle, but apparently not. So let me see if I understand that. The normal to the, to distance, D is just a, linear distance. No. D is a what? What is DD is a direction. yeah. D is basically, the displacement, but for the feature we use the length of this displacement. So it's basically how far is, 0.1 from 0.2.

so the distance between the two points. So then you're taking the, the angle between, why, but why do I care about the distance? I'm just trying to calculate angle between the normal md I don't care what the, length of the. Yes. So basically if you look at this picture here, the idea is that, these three, point pairs would all have a very similar feature, attached to them, or they would have the same feature attached to them. But for example, if you move this bar a little bit up, so let's say you have this point and this point, the normals are still pretty much the same. So the angles would be pretty similar, but the distance would be different. So it would have a different feature.

Oh, I see.

one way to look at it is that if D is very small, you have a representation given whatever the angle difference between the two normals is of one curvature. But if d is very far apart and you're not sampling anything in between, but you're, but the thing is, that then the curvature would be less because you have a greater distance over which to swing that normal. Yeah. yeah. the bigger D is the less knowledge you have about the shape on. It's this, as D gets smaller, then you, get this d approach to zero. You approach the instantaneous cur of the, figure. Yeah. And I think I, get it. Yeah. And if we would have a sensorimotor that samples in equally spaced intervals, then D would always be the same. And we would just, it, wouldn't be super informative. But if, D can be different, like for example with Cades, it, it can have a lot of information in it.

if you are, if d was along the, edge of the shape, then it would be very informative. I could say, because, not a straight line D but if I want a, if I, wanna, every, point was equally spaced along the curve, then D would vary a lot and it would capture them. Yeah. Yeah.

Okay. Yeah, so, that's the basic idea to use those four numbers to quantify, the, yeah, instead of the displacement, between two points. So basically that would be the attribute of the edge of a graph. And using that instead now solves the problem of detecting objects from new rotations. So on the left side, this is detecting five different objects rotated the same way that they were learned and on the right, detecting them, in a different rotation. It's pretty much, the same performance. So it, it works, quite well. The bigger question would be if we can assume that we have knowledge about the point normals. So do we know essentially. Yeah. How do you call it? Perpendicular?

the case where, vision and touch seems are very, different. And, we with touch, you would know this in some sense, right? And if you think, imagine your finger moving around the, object. You, inherently know where the surface is well to your finger, right? It and sometimes whatever patch your skin is touching the object is, it's, gonna be at a tangential point. And so you would know this I think of touch, I guess if you know the local 3D curvature, then you have the normal. Yeah. Yeah. But how would, I mean isn't this to determine the local 3D coverage unit? yeah, I think a touch sensorimotor can pretty naturally determine it. And then the question would be if we could also determine this from a patch of, vision of vision I just talked about with that too.

Wouldn't need these points for teachers, but could you use that to get this, these normals, you were talking earlier about the curve possibly. Yeah. Good. Tricky thing in the vision, right? so it depends, it depends on things, right? This is the weird thing about it. And, it, Yeah, we might be able to learn this in vision. And now, you know that we talked about that too. you know what if you had touches your brown truth and then you just, you're getting these visual inputs and there a lot conditions. They're weird stuff going on. Could you, do it? I don't know. yeah, you could use your touch to train the vision. Yeah. Touch is your, so we could just say, assume we know this ground truth here. Could you train the vision system to imagine just we have an object and we have ground truth, so we don't have to touch, we just have ground truth. Then we could say, oh, could, given this visual input, could you actually learn these normals? you can also use movement to train. once you have a ground truth, then you can move and see how it changes. Because as you move, you know how, angle changes, but the lighting, the input will change. Yeah. That you could, but still in a very complex way. That's hard to imagine, In the visual point. If you have a point cloud, you can have the point normals and the other work we discussed before, we assume we have the point cloud. but what's different in this? the point cloud doesn't give you the normal. No, you can't. you can approximate it, but if the point, are really close together, then you get a good curvature. But just from a single point, I don't know, maybe, it's not too hard to do. I dunno. Yeah, so I'm, using a point cloud right now to get the normal, so, I thought you said you have your question, how could we do this? But now you already have a way of doing it, that assumes that we have a point cloud, which might not be the case if we are just moving a finger along the edges, then we only get one point at a time. We don't have the whole point cloud to estimate the curvature. So it seems to me a finger moving along the edge is, it? Intuitively feels yeah, I know exactly where I know what the edge is. I know it's sharpness, I know it's orientation. I can, sometimes I'm sensing the normal, you know what I'm saying? Yes. So we should be able to get the normals from just one sensation instead of all the, the whole cloud of sensations. I think it's, I think it's also possible to cheat a little here if you want. I'm not sure if you can cheat, like I asked myself when I'm actually touching something. Is there really one column in my court that's active? Probably, almost certainly not. I don't know if I can ever actually just get one column touching something. Because the column, the receptive, the, on the sensory organ, it's, they're overlapping. And so you touch something, you're gonna have multiple columns getting input and they, they have short term connections between we, it is very possible that, we are able to, integrate several columns instantaneously together. More information. I don't know. My point is it seems like, I definitely have it. It's like I touch something. It's like I know what the curve, I know it's not just, oh, there's a dot. It's oh no, there's an edge and around it and it's oriented this way, and I can feel all that. yeah. Yeah. I was also thinking, it, using multiple ne neighboring columns could solve a lot of these, problems like detecting the edges and the curvature. Yeah. And it, And again, it's clear that we are able to do that. it's, I'm conscious of it. I can tell you what the edges and what the surface is. I just feel it. We alter, so it's clearly there. so we don't know exactly how it's done, but multiple columns could do it.

it's also in the cortex. The lower layers have a, broader receptor field than the upper layers. So the lower layers would be looking at a larger part of the object, and the upper layer is looking at a smaller part of the object. And so one could argue the lower layers are now determining this curvature because they're, they've got input over a broader area, but the upper area, is determining the feature you're gonna put at some point.

there's lots of tricks like this, right? And so a single column actually like multiple columns in the sense that it's detecting over a broader area. and, then it's gonna integrate that information. Then you can say, at that it can turn curvature. And then you say, oh, at the point of the curvature, here's the feature. That kind of thing.

Thing about touch to me is, you actually have two modalities simultaneously. the fact that you, your finger is impeded, it can't push beyond a certain point. You can just move your arm and your hand with your finger in contact with the thing, and you can trace out the course shape. But on the fingertip, then you have the fine. Detail the texture and confirmation of the local curvature. So you're actually getting a multimodal input of space, with touch. Yeah, I was arguing opposite. I was arguing that the, that that the curvature is detected by the broader input and the feature, let's say the, texture or something is detected by the smaller input at the top. But, it is, I think they can con, they can confirm each other. maybe, that's, I know that's true for vision. I don't know about touch actually, but it's probably true. I think it's a, I think it's your general property cortex. anyways, we can put that aside. We, can just assume that we'll have some sensorimotor that can, if you had a touch sensorimotor, we could tell you this information.

and I don't know about the visual world how easy it's, but it's, if you got a point cloud that's working, then I guess it's. Yeah. two more notes just in general. So even though those features are rotation and variant, we can still extract the pose of the object, once we recognize the object because the matching procedure basically narrows down where on the object we are. And once we know where on the object we are and we know where in space this point is relative to us, we can, calculate the pose of the object relative to us. that's one more side note. And the second one, I'm not sure, but it might make it easier to add more slack for. the sampling problem because if you just have the x, y, Z displacement, it's difficult to say if you, do you wanna have more slack in the X or the Z or the Y direction? Could the displacement be longer or could it be more rotated? here, those could factors are nicely separated out into four variables. So we can say we don't care so much about the length of the displacement matching Exactly, or we don't care so much about the angle. it would be easier. And here's just one example of, so the, this model was stored in memory with a hundred points from the surface and then here that are 200 points from the surface, with, different features and connections between them, and it does it, At least for a short time recognizes the mug and then thinks, okay, it's not anything, it's not any object that I know anymore. But, that was just a quick test. But I feel like with those four variables, it could be easier to explicitly add slack and either like length of displacement or certain angles. So it could be somewhat inva to changes in scale, for example. if you have more slack on the displacement, you could have larger or smaller. Yeah, I didn't even think about that. Yes. I think it might allow for scale if you allow explicitly only the distance factor to vary.

Yeah, And, it's funny because it, seems like every time I think about that, oh, that'd be nice, but. But sometimes the scale varies in direction. what if I showed you a coffee cup that was, instead of being circular, it was oval. And, that, you get it right away. So that's not it's okay, it's stretched in one dimension about not in the other bench, and, but you immediately recognize it and you could immediately learn it. and it was like, so it's weird. It's like you can, it's like it's not, I'm just pointing out that scale isn't just scale. Distortions occur in non-uniform and, it's weird. But I feel like with those distortions, it may be important to consider hierarchy and the features as well. Because, for example, if you have a very distorted cup or like the Dali clock or something, if you'd only feel the morphology, you would probably not recognize it.

if it's highly irregular. Yeah. But, once I've learned it, I would, I, could very quickly learn one of these distortions, right? Yeah. I recognize it as a cup right away, but I could definitely learn this object and I would then know that it's a cup and I know I could put liquid in, I know I could grab the handle. I know how to drink out of it. It's all those other things that come along with it. I wouldn't say, oh, this is something completely new.

yeah. But I was thinking about how much distortion can we tolerate? So here are three examples. Oh, you put these up.

Put them up. And so, for example, I, similar to what you said. The ring is a little rotated. It's all tilted. But, for example, the last one, if I rotated this way, it doesn't feel like a cup anymore. It's more like a cartoon character. Maybe the face, it could be a face, I guess I'm saying when we say how much can we tolerate, are these objects that I'm actually learning now? Or like I, let's say you learned a cup Ry and now I see this thing. Yeah. Yeah.

so to questions what I label this as a cup, I would definitely see it as, I would see it as different yet somehow the same.

it seems like a pretty good technique in general, right? Viviane, the, point pair features. Yeah. Yeah. I like it. it, it's a very nice description of. How two points in space relate to each other? I think two. Two planes relate to each other. Yeah. Yeah. Two points with rotation attached to them, or Yeah, two. Two. they're normal to plane, so there's really two plain, yeah. Two, two tangential planes. yeah. So I think it has a lot of nice properties and it's solves some problems. yeah, I just wasn't sure if we can assume that we can get those normals that there would be like the major difficulty with it, I think. But I feel like it sounds like we can, right? Yeah, you can if we have the point cloud, but let's say we don't, you have a partial point cloud from a sequence of observations. You can get the normals for the point cloud. You could train a network to also retrieve the normals from a partial point. Cloud wouldn't be like perfect, but at least you don't have the assumption of having the environment cloud. Yeah, I mean in some sense it, it feels like either whether I'm touching something or looking at something, I'm gonna learn the shape of this thing. In some sense. I have to be able to determine these surface normals. how else could I do it? It seems I can't think of any other way. It feels like I have to do that and I'm able to do that. I'm looking at this chair in front of me right here, and any point I look on this chair, I have a sense where the surfaces are on the edges are, and I can, describe where the point cloud is on that. I can do it with one eye closed. so it seems that somehow that information is there, so you should be able to do it. Yeah. It is also a variant that I presented two weeks ago, which was, instead of the normal, it was like three displacement, rectus in sequence. And instead of a very similar way, you get the angles between them. So it's normal you, so it is two displacement rectus to define your plane, and the other one is. And yeah, you don't need normal in that case. Yeah. Just displacement.

pardon? I'm confused. Aren't we trying to determine the displacement? Let's say you have to dismiss, through observation. Let's say if three in sequence, but I thought this is how we're gonna determine this displacement. No, this requires a displacement too. Yeah, because you need the distance between the two points. So this, but that's not, that's just a, to me, the displacement is both proposed. That means it's not just the distance. It's a, it's the angular has to be the 3D distance. Yeah. This, but this is just a linear distance here. It's not three dig distance. Oh, It's, a normal distance. yeah, just a scaler. Scaler. So anyway, it seems like this issue to learn the morphology and object, it seems like we have to be able to detect these normals.

The boundaries on an object. And that's really the plane change. The plane, the normal, just normal that, so that sounds like it's pretty safe technique to use, right?

Yeah.

Yeah. I feel like if you can't do that, it makes things very difficult. yeah. You can't tell the curvature of a surface or to be able to, so is this like a worthwhile idea to pursue further, any objections or alternative ideas?

'cause I guess then it could, it would be interesting to look, how we could extract those normals from like a vision patch or a small subpoint cloud, of a touch sensorimotor, for example. I was confused. Didn't you just say that we have the point cloud, we can extract these, right? And don't we have Yeah. So yeah, if we have, so right now the touch sensorimotor always just gets one point. not, it would need to get like several points, in close proximity. and then we can, get the normal from that. Yeah. but don't, at each image patch, don't you get a whole bunch of pixels with depth.

depends. So how I was using it before I just get one point in space, oh. But, but you could, it's there, right? Isn't it? Could get multiple Yeah, it's not, yeah, it's, all there. Yeah. The system is getting depth at every pixel. Yeah. But I think you are using the mesh directly, right? Not using the sensors. Yeah. So right now for the, cloud, I just showed, I, just used the mesh directly. But doesn't the mesh have a little angular? Yeah. That surface you can have the surface, right? You have the surface and you calculate the normal. Yeah. so that a real system wouldn't have access to the mesh. Correct. Oh, we're getting the mesh because we're cheating habitat. Yes.

What meant if it's operating on the patch, our depth patch. Then you have depth that it picks Yeah. Surrounding pixels and you capture room.

Sounds doable, right?

No, it's just a question of noise, how much noise it could tolerate. I guess you try that would be something to find out. Might be right. Good. It also, the, issue that was, shown that if you have a, plane that's nearly normal to your view, your, sampling is much coarser in the Z dimension, for that surface than it is for the, something that's normal to, you're, however you're capturing these, these Z coordinates. So yeah, the mesh is, is almost a ground truth there. But if you look at what you're getting as a sample Z image, you have to be careful about which points you associate near to each other. I didn't follow that, Kevin didn't, I thought that if I was looking at a plane, that's normal to me, that would be an easy one. You made it sound like it would be harder. I, imagine the hard one is, let's say I'm looking at the, I have a cube and the cube is almost, I'm trying to look at the, Viviane had this last week, the top of the cube was just going off in a very, slight angle for me. And, therefore he really couldn't stand how many points on it. You're gonna get fewer samples off of, that plane of the cube than you would that one that is normal to your viewing direction. Yeah. So what, so I'm saying is that when you say that I'm just gonna take this cluster here and average, construct a normal from the XYZs, from there, you have to be a little bit careful about how you choose them. Yeah. Because you could be on a corner, in which case it's ambiguous, for instance. Yeah. working on this idea, I'm working on, I've been thinking about things like tubes and faces and things like this, and it's, really interesting, when we, when we, it is as I look at an object and I rotate it in front of me, and I'm learning that object, it, it seems like I have this real bias towards, remembering the surfaces. if I, look at a cube. I remember what's on each face of the cube, but I, it's not like I, I look at the corner of the cube at the corner on towards me and say, okay, that's something I'm gonna learn. It's I don't learn that. it's no, I just rotate this to the next space and I rotate this to the next space. So I know it's a cube, but therefore I'm not even gonna try to learn the displacement. Looking at it from an angle. I just think something with a face, we remember face like straight on from the side, and this is why we're really bad at drawing faces of angles because you don't, you never, you don't really store it that way in some, my point is that when we observe objects to learn it, like if I'm gonna learn a cube, I'll probably learn these features on one face and one is pretty much, orthogonal to me. And then I'll rotate in one features in other face. I don't, I won't try, I, won't really memorize what it looks like from a corner. It, it, seems like I don't do that. So my point is, but you need to at least recognize the, boundaries, Yeah. Maybe not the corners, but, you need to know where the edges are, just to define anything. Yeah, it, yes. So the theory I'm working on right now is that, that in general, the basic way we learn displacement of features is in 2D and, yet it's, these, this 2D displaced set of features is actually wrapped around a 3D morphological object. And, it is easier to, we try, we wanna get those features into a 2D sort of presentation for us, so we can consider, do better at recognizing the relative positions to each other, not when they're really skewed and them, but just rotate the object to see that.

you know what I'm saying? It's if I'm learning what's on the coffee cup and I'm rotating the coffee cup, I don't look around the edges. I look what's facing me and then I keep rotating to what's facing me and rotating what's facing me. So it avoids this problem a bit. I don't try to imagine what's around the corner or just really skewing me how just rotate till I say clearly. So anyway, I think there's righters ways to get around those problems. I learned model. We've also capture those edge cases. Kevin means you're talking about heuristic averaging to get the normal, but we're thinking more of learning a system that maps like partial point cloud or sequence of image to normals and then the learn to capture those. Yeah, what I said, you have to be careful what I, meant was in the con, if you, there's gonna be some context you're gonna have to recognize to know how to weight those contributions from the points. But it's a learned system I'm talking about. No, I understand. It could be, learned. I just said it. It's, it, doesn't pop out, out of, a simple analysis, on, in, in a diagram. That relationship, I agree, could be learned. It's just not gonna be, it's gonna, it's gonna take some training to be able to learn that. and not so much edge cases if you're, pardon the expression in, the, histological sense. But they, they are, things that will become salient and then you use those as anchor points to, to, decide how to, go with it. But I'm not disagreeing with you. I agree that those could be learned. it's just that, it's, mathematically, it's if, I was to do it by hand, it's not necessarily straightforward.