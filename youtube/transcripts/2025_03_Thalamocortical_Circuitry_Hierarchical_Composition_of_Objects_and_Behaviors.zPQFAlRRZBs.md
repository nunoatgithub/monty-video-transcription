yeah, there wasn't a kinda specific plan for today other than just, obviously continue the brainstorming, the various kind of tracks we've been discussing. yeah, I guess there was, some write-ups that, myself and Jeff had shared. And Viviane you mentioned you had some other stuff, you might be interested in talking about what kind of hierarchal behaviors. She, yeah, but she also po together one side.

Oh, nice. I do wanna talk about your, Viviane, your proposal about, the one you posted today about, driving inputs coming from Yeah. Orientation. I don't understand that, that Yeah, that's something different than a slide I put together, but yeah, we, it sounds like it, but I, seen it, I've been thinking about this topic a lot, about these higher tical columns and what drives them. And so if you, I don't know if we want to jump right into hierarchical behaviors, but we could, but I was. Sometime today, I wanna go back and understand your proposal better. Yeah, I'm, up first starting with that. Yeah.

Maybe you can bring up that today. That might be better to start with because, yeah, let me load that up.

oh, are you doing that? I'm not sure what, what's driving everyone else thinking, but, in my mind we have a lot of uncertainties about what happens in, we have a pretty good idea what's going on in the, first and second sensory regions. and, but the higher order regions are still a mystery a bit. And, we're trying to preserve the, come up with a language, this, is how I think of it. We're trying to come up with language. It describes what all cortical columns do. And yet that language encompass higher order regions and things we think they might do, which we don't really know very well. And that's the general goal here. And, I've been exploring one idea, which is just an idea that, maybe all cortical columns get, some sort of sensory input, which would be helpful 'cause that's how we describe our columns. We, last week we looked at the basal ganglia, whether it could provide that kind of input. We never, we didn't reach a conclusion on that. And then you have this proposal Viviane, which is I'm not sure. I think I understand it, but maybe not. Maybe we can discuss through it. I wasn't talking about this particular diagram unless you wanna start with that one. Yeah, I just thought I'll show this one first because that's like kind of the, the thing we, I guess all agree on. so I just thought this was a nice diagram also for everyone else who isn't as familiar with sensorimotor around receptor fields and. all that. and actually I'd never seen it visualized like this. So for me it was also nice to see, basically how, layer six can, first of all, we have centers around, ganglion cells as input to these LGN relay cells, which then get pooled into a representation of, an oriented edge, which then drives, neurons in layer six. but then the nice thing they show here is how input from layer, so sorry, layer four, I meant to say, but then how input from layer six can actually modulate these responses and turn this receptive field. It looks like. basically, I thought this, did they show it? Did they show it? Turn the receptive field? I don't think that did, it. this was just showing it, this is showing, enhancing it, quote unquote. yeah.

yeah, enhancing, I think, I, think this idea of changing orientation is new to us. I've never, I don't think anyone else has ever talked about it before, but if I'm wrong about, that'd be great.

yeah, I've also always seen it described as gain modulation, right? Just enhancing. it's more just like suppressing the alternative, representation. So if you look at the lower right here, you see the before and after it's, a sort of cleaner.

you could read those words, to the preferred orientation.

but I think they're just saying, Hey, there's an orientation and it's cleaning it up, or something like that. Yeah. Yeah, it's not exactly like as strong as what, we're, no, not even close. they're basically, yeah, they're giving a minimal role to the thema of just shuffling the papers and straightening 'em up a bit. it's like it's supposed to do anything useful. but it's not, it shows the, arrangement of things there. yeah, also it makes, no, this picture makes no, no distinction between layer six A and layer six B, which we know there is a big distinction between those.

yeah. But yeah, the main reason I was looking for a picture like that was just for visualizing how the centers surround, cells get then pooled into an edge representation. I Oh, I see. So the arrange the arrangement down there. Yeah. And then I just thought it was nice that it also showed the layer six backward projection, and just doing some population.

Yeah. Anyways, to the main. Diagram I wanted to show, the resolution is a bit low, but I hope it's okay.

maybe Eric, lemme just share it in a different way. I dunno if the resolution looks as bad for you, lemme just for me. I can read it myself. I, about other people.

Let me, one second.

Maybe if you want. just while, while you're finding that I could just, following up on that, like initial Oh, you got it. Okay. Yeah. So yeah, basically, actually, can you see my mouse cursor moving around? Yeah. Yes. Okay, cool. That's really nice. I can move my cursor on my laptop, on my iPad screen. anyways, so this is the, classical one that we all agree on. We have direct sensory input going through the thalamus and to a cortical column, layer four. and that can be modulated by the layer six B projection. so I think we all agree on that one. but let's be careful. Let's be careful about that. That drawing, that diagram is, there's two of these projections up there and this is the one that's defining minicolumns, right? This is the one that's defining the orient. Yeah. So we're not dealing with Yeah.

With orientation. So it goes up there and it activates a set of minicolumns for different orientations of, that are observed on the retina. Yes. yeah. That's why I'm drawing the error here, at the upper part of therefore. Just wanna, be clear. Yeah.

and then another thing we, I think also all agree on is that, this direct sensory input can also go to hierarchically higher region, like V two. so the three columns shown here are supposed to be hierarchically arranged with the one on the left being the lowest. There was just a knit detail here. I'm not sure we resolved it, whether or not that green arrow going up to column two actually came from the LGN or it came from like a next region up in the Pul.

there was, I used to always think of, oh, it's gonna be coming from the next lambic region up the, but I think some sort the paper, but you can go with this from the, from a higher order of thalamic, nucleus now. But I like that. I'm just saying it may not be right. So yeah. I think that's what I read before that. Yeah, maybe, I think there was some stuff about this in the, shallow, brain hypothesis paper, for example. Oh, good. I think. But yeah, I thought someone showed a paper recently that said, oh, a V two column gets input from L from LGN, and I was like, oh, damn. I don't, that doesn't make sense honestly. I, my memory is that we found evidence of both that it was like, oh, that, it's it goes from by LGN and higher water ones, but probably probably for more, like the, more, whatever you call it, like abstract kind of the sensory input, the more likely it is gonna be some sort of like higher order thalamic nuclei. Oh, okay. All right. all So that's a little detailed. Okay. By the way, try to remember that. Maybe it comes from both. All right. Sorry to interrupt. Keep going. yeah, no worries. Yeah, I think actually also in the Thalamus book by Sherman and Gallery and also a lot of their papers they talk about. How the thalamus is arranged or divided by where it projects to like, so every neocortical region project, right? that was my working assumption for, decades. but then we saw one paper, which maybe the one that Neil says, and so it said, oh, LGM projects to B two two. I'm like, oh shit, that's, but that don't make sense. But let's keep going this way. We can just be, we'll just ignore it. We'll ignore the, other data point from now. Okay. So now we get to the new part that I don't know if you agree with, which is the higher, I didn't, understand that the hierarchical region that doesn't get direct, that doesn't get input from a sensory, from a sensorimotor. and this adds a lot now, but basically, let's start with this one 'cause it's a bit simpler.

basically, this one could get. The orientation that the, child object is and the parent object. That's what we talked about before. How, we can get the child object orientation and the parent object orientation and Theus can calculate them together to get the orientation of the child object relative to the parent object. And that can then yes, become the input to the minicolumns here. And it could either be the only input it gets here if it doesn't get sensory input or it could be an additional input it gets, if, this column also receives direct sensory input. But exactly the same mechanism that's drawn here, that it's basically the orientation of the child relative to the parent, which is similar as the orientation of the feature relative to the parent object. it's just that in this case, it's a orientation of an, of a child object instead of the orientation of a sensory feature like an edge. and I think so just so what, so this third column. If it's not observing something from the retina, for example, how we've assumed it's inferred an object because it has to know the orientation of the object to pass back to the ths. but how's it observing an object? how's it inferring an object? you can, if I knew its orientation, it, I mean that the last, the right most orange, yellow hour. Yeah. So in the beginning is that hour is the orientation of this column to, typically we say it's the orientation that layer six B is the orientation of the object relative to the sensorimotor itself. It's a all sensorimotor gear. Sensorimotor. Yeah. But this, case, we don't have that. This would just be a hypothesis in the beginning, same way as, in the lowest column. This is also just the hypothesis in the very beginning, orienting this, incoming feature. It's the exact same mechanism. It's just that the incoming orientation of the features is not an orientation of a sensory feature, but it's an orientation of a child object. But we the same way as we have to do it at the lowest level in the beginning, we don't know what it is. So we sent down a hypothesis and rotated, oh, but what am I, modeling? If I'm not getting any? It's, like I've cut it off with the knees up there. what's this third column? it's, and locations of child objects relative to each other. So for example, if, it models a face, it would model orientation of the mouth, orientation of the eyes, orientation of the nose and locations relative to each other. Okay. But in that, case, in that exact example, we would move our eyes to look at the. we would fixate on the eyes and nose and the mouth of, and so there would be, there'd be a motor output of this column directing, or directing the eyes to move, to those different features. Yeah. Which, but I'm not, you're saying I'm not getting any input from the retina, but I am still driving the retina. is that what you're saying? I'm still moving the retina, but I'm not getting any input from it directly. Yeah. It could still have a motor output that moves the retina to another child object on the face. But the individual components, like the eye itself and the nose itself, would be recognized in the lower level column from the sensory inputs.

Yeah. I'm still trying to internalize this. I'm, intrigued by it because, for the reasons probably you're, you like it, but I'm still struggling to. internalize it. yeah, the reason why I like it is because it's, basically exactly the same mechanism as we use down here. And we already talked before about how features that come from the sensorimotor are pretty much treated the same as features that come from L three to L four hierarchically, like the child object, he just becomes teacher on the parent object in the same way the child object orientation just becomes, like a mini column input on the parent object. Okay. So now, you made some, comment like there's no reason to have a spatial pooler in this third column. I'm, not sure why you said that. oh.

Yeah, I guess it depends on, where, we say the spatial pooler would be located. But I was thinking of the pooling happening actually in the thalamus where we have all the gang no surround cells coming in. I don't think so. Think the edge representation To me, the patient pool is always the double bookcase cells in that define the minicolumns. So yeah. So let's put that aside. So let's put that aside for a moment. There's happening, so one of the things I've been thinking about, and I didn't really write it down or articulate it, but yet, but I was trying to imagine how we learn abstract things and I was working on the, on a hypothesis that, that abstract things are still tied to some sort of sensorimotor abilities, sensors. And so when I. When I learn mathematics, when I learn anything, even like right now, we are defining abstract ideas about the cortical columns and we're using pictures to do it right there. There's a visual element to, these are very abstract concepts. We're talking about spatial pooler and these connections, what they're doing. So we're doing abstract thinking right now, and yet we're presented with a visual input. and I know that when I think about things, I have this sort of visual, mental model of 'em, even if I can't always articulate exactly what, it is, but this sort of, this visual aspect to my thinking, spatial arrangements of things so that I was questioning whether like abstract thinking is really gonna, is really gonna be based on the same sort of sensory based, concepts we already developed, which would be consistent with Mount Castle's proposal and the common learning algorithm and all this stuff. So here it would, still be grounded in sensation.

this is why I like this, because you're, I was having trouble making that work, but you've only, you haven't separated the vi you haven't separated the sensorimotor or vision from all of this. You're just, you've still got the motor sense of vision, but you've gotten rid of these sensory input sens of vision. Yeah. or at least directed sensory input. But like the child objects are ultimately grounded in Oh, of course. But this, particular column on the right is not getting, we still getting movement information. Like we, we still have relative locations in that. So, this is the thing I was confused about and I think I wanted to really understand because it smelled right. It was, I, thought originally you were divorcing this completely from sensory, like from the eye, but you're not, we're still moving the eye. We still have motor commands, we're still got a reference frame that's moving around. but we're just not getting direct sensory input from the eye. And that's a very simple idea. I didn't understand.

yeah, so confused by this be we would still get a motor input going everywhere. Like it would go here and, I think maybe the first time you said this, you might've used some language which strip me up with something like you might've said oh, we no longer have to have input directly from the eye. And I'm like, wow. Okay. But that's not true. We do have input the eye, but it's motor input. so it's a hybrid as you pointed out. Yeah.

Okay. I didn't understand this. I think I understand it now. At least I understand enough. I can think about it myself and play around with the idea. it's intriguing idea that all. By this idea that, like we learn mathematics, we learn anything, we do the, we make diagrams like this. And even if I learned something through language, I'm, I was coming up with examples where I could teach somebody something and through language without a visual input, but I typically would, or at least often I would describe it through language, visual scenarios.

or I would, the language would describe something physical to teach an abstraction, so the question is, are all of our high level thoughts really grounded in, perhaps even just vision? Because I'm not sure the same mechanism would work well for touch or sound. it, may be that all of our abstract thoughts are really, derived from essentially visual processing of some sort. why would that work? for touch. I was trying to come up with examples where I could think of learning abstract thoughts through touch. It's like hard to think of, I feel like, I guess used lot as well. what one at a time. I'm sorry. Go ahead, Niels. Just that I feel like we use more somatosensory descriptions a lot with abstract concepts as well. if you can grab a kind of something mentally or, yeah, I don't know. I, it feels like the key thing is, more like if there's movement involved like Yeah, sound and definitely like smell, at least for humans, there's not that much movement involved. So it feels like we don't in general have that much kind of, it seems like ference frames. But I was trying to come up with real examples where I'd say either I'd learn through, touch an abstract thoughts through touch, or I would communicate it to someone, by describing touch sensations. Maybe there is, I just, I couldn't think of it. Still have abstract thoughts. So maybe it's just that if you're not born blind, you rely more on the visual sense, but if you are, you still learn, you're still matter sensory. maybe, you're right. that's certainly true, right? maybe if you're trying to describe it, a texture or something like that, like fur or something, it, yeah. But yeah, do you say oh, I guess you could say this mathematical concept is fuzzy, but that could be visual too. yeah, exactly. but that's really, like I, I ran into a wall with my thinking or, yeah, Maybe. But even that, I'm, I picture, I guess maybe in the end, maybe I should say it is envisioned, maybe I'll back off on that. white people can think abstractly. That's, that was the point that was just made. but perhaps what it is really is that all abstract thoughts are based, or derive. From physical models of the world that, we can sense. that's where it starts. Yeah. So I could, it's easier to imagine, right? You could imagine.

In that case, it doesn't matter if you touch it or see it, as long as you're building up the same model of the world, it may, sometimes it's a lot easier to build those models visually much quicker. 'cause you can glance around tons of things without touching 'em and see their arrangement. or blind person can't do that. they have to physically touch things.

okay. So, that's a, bit of an abstraction, at least for me. Might be helpful to think about. Just think about, oh, the world we start with and then hindsight, it sounds obvious, but it wasn't obvious upfront. We start with a model of the physical aspects of the world, and somehow we continue to build abstract concepts with the idea of. Them having a physical structure underneath them.

and so in the case of vision, even abstract thought could involve, visual, like these pictures we're all looking at right now, and movement relative to that image. It's, we're, again, we're gonna ground all, we're gonna ground all abstract thoughts in sort of physical models of some sort. I guess that's, even that's not new I, in some sense, can I describe that in a thousand brains? A little bit like that, but yeah, I feel like that's something we've thought about. Yeah, I just didn't, I just didn't get, I'm, still struggling to get a deep understanding of this, right? It like the, I can say the words and they can sound right, but I'm just trying to get a, really deep gutteral, oh yeah, now I can know exactly what's going on. I'm not there yet. I maybe somebody else. You guys are, I'm not there yet, but this, is helpful.

this is really helpful for me to think about this way.

yeah, and I, guess just one more layer if it helps in any way to add to it, is that here, like the same way as you already mentioned, we also can get like feature input, like color and stuff here. We could also, in the higher level, we would get the object ID as this kind of layer three, two layer, four projections. This is right, this right, And maybe for context, this is what I was trying to get out with my, like what I had written that what is the analogy for that blue arrow as opposed to the green arrow for the lowest level of sensory input. Like why are some things these kinds of rotations versus some things are not? and that's what I was.

Yeah, I like as in some things are driving inputs and seem to be transformed by the thalamus. that's what, Can we, the kind of rotations are, can we back up, can we back up? Show the picture again. Your face is nice, Neil, but I like looking at you, but I don't look at the picture.

I got rid of it too quickly. Sorry. See, I don't even wanna talk about it until I see it. I don't even wanna talk about it until to see the image of it. Yeah. So, just that, I, think we had some, clarity that Okay. The, kind of rotation input in, let's say in the higher hierarchical area that that's, the green arrow, that's the rotation and then the blue one is just a bias in input. It's not going to, that's not the main driving input, the, L three to L four. And I. It makes sense if there's the, same thing at the lowest level of the system that you have kind of things that are orientations that are more important for robustly recognizing things. And then you have things like maybe color, that are, less important. And maybe there's not a clear distinction. Maybe sometimes color is more of an edge, maybe sometimes color is more of like, you'd have a colored edge and maybe sometimes it's more like a particular texture or pattern. Yeah. But that's at least what I was trying to get at with that. Like maybe one of those is the model free and that's like any of these kinds of perceptions. And it could also be like emotions and things like that, whereas, The other one is the, more kind of what's derived from center surround and becomes orientations. So this, I think everything makes sense except for the blue arrow on the left. so right. So let's say the blue arrow on the right, these are SDRs. which are, the, they represent, let's say an object or, behavior. Maybe we haven't done that yet, but let's say it represents an object. It's not, it's, just, it's independent of its particular orientation. There's no transformation has to be done on it. It's just a bias saying, yeah, I'm observing this feature in column two, I observing this object in column two. That should bias a sheet of what you should be seeing now in column three. and that, because it's an SDR that you can't rotate it. There is nothing to rotate.

but up to now, the, as we've been working on the assumption that every input coming from the retina and therefore every input coming from the thalamus is a center surround receptor field. this makes sense. But I guess, yeah. Does it have to be, or could it be, some things are center surround and those are what become the, so let's, let me just fill this up. The moment. My understanding of the research says that every cell that goes through the thalamus is a center surround. Receptor field, the ganglion cells from the retina are all centers around receptor fields. now that could be wrong, but that's what I've read. But it could be wrong, but I'm like, I like it. I wanna, I'm going with it. There's something really nice about the center surround, I try to write that up, is that it represents the center round cell only active if there's something happening there, there's no point in reporting stuff that's not happening. It's a center found, so a uniform field of, illumination on the retina will not activate a centers round cell. It doesn't matter if it's color or black or white, doesn't matter. Rods cones. so that's my understanding. So in that case, it's hard to explain the blue arrow on the left, which is coming from the thalamus and, projecting to the sort of the mid or distal dendrites of the layer four cells. Again, the green arrow drives which columns are active, which minicolumns are active, and that's, basically the encoding, essentially Models are all, models are basically orientations at locations and then the blue arrow is helps you differentiate. it's a bias, it's not a driver. So that Blue Arrow is not a driver yet. How is it that a center around, IT field would make sense for that input? It's, very, it is substantially different than the blue arrow on the right. let's put it that way.

yeah, I think, yeah, it would be interesting to revisit that, the anatomical evidence and, also I guess how it fits in with maybe other inputs coming to L four because there's so many different, everything from, Potentially superior calculus and things like that. But at least because one of the paper I, one paper I found it didn't have that much detail, but it was basically exploring how color blobs respond to, in animals when they view like a uniform field of color, which in the same way that you can have an entire visual field of color, you will still perceive that color. even though there aren't, any edges in it. They basically were exploring how that was triggering, blood cells. I'm just interesting in that because my understanding is that if you would, if I were to immerse you in a complete uniform color field, the moment it appears you're aware of the color, but over time it fades away. Is that Yeah, for a long time? Maybe. Is it a long time or, you know what, my point is that, oh, not like seconds or probably not even minutes. I don't know, maybe, Anyway, the, the, current, the dogma is that everything that goes into the, everything that's like a information content neuron that, or axon that goes to the cortex comes through the theus. there, there are no exceptions to that. There are things like matrix cells projecting to layer one and other cells projecting into layer one. But basically everything else goes through the Salmas. And as far as I know, everything that goes through the thalmus is centered around. So those could be wrong, but if they're right, we'd have to understand how it is, what's going on. For those blue arrow on the left, we just, I remind, I said this last week, we used, at Menta, we originally thought, oh, that, that. The, all the cells in the minicom would all be driven by that blue arrow. and so the synapses on the layer four cells would be near the cell body, but they're not, they're far from the cell body. So, we know that whatever's coming from the retina going to layer four cells is not driving those cells. so I'm just, it's, not new. I'm just reminding myself of that. yeah. Yeah. Maybe like one interesting complication as well, which I think we've touched on briefly but haven't discussed recently is just, that at least in the case of color blogs, like they tend to, they're defined by not being an L four, like they're above and below. which Interesting. You probably need to switch to Neil's screen, by the way. There's like both of, okay. Yeah. Okay. I can see it. Yes. Mine didn't switch automatically. Okay. Yeah. Thanks. Do you mind if I stop sharing my screen? Because I don't think I can switch to, I'm sharing. do you still wanna see my No, that's, okay for me. I had a couple questions though on Vivian's diagram. Okay. Yeah. I was just quickly showing this, but yeah, then we can go back. The column don't exist in, so what, happens in layer four? They just not color sensitive.

It's a good question. I don't know if, yeah, honestly I don't know enough, oh, by the way, because I know there are, often they think cells are insensitive to color, but then they, the more they study them, the more it's oh, this is a neuron that responds to edge and color. so I don't know how that kind of fits into this. all I know is classically the color blobs are basically not found in L four. this is dry eight V one. And is it clear from this diagram that four C beta is not color? is it color only starting five and six? Is that Yeah, I, think it's meant to be five and six. okay. I appreciate, that's interesting. ambiguous here. Interest. That's pretty interesting.

But I don't know. sometimes they just put it through. Yeah. I think this is meant to be the color blob. Sometimes it's everywhere. So it's, who, who knows? Or, I, remember once I shared this one, this makes it look like it's only an L two L three. yeah, it, almost feels like it would be worth going back to the original papers and understanding what is really going on with color blobs. I really like this idea that, if you think about an input is, centers around, then you could have, a note that basically they're gonna detect changes of some sort or some sort of frequency change. something, it's not uniform and therefore, you would have the same thing for colors. So then I, and I wrote just yesterday, I think I wrote that, Hey, one way to think about the color Bobs it's just like another sensorimotor. It's like a whole nother sense system that's co-located and moves with the, other sensors. So the color Bobs could be just like a modeling system that's based on color and and there's a separate modeling system that's based not on color, but they both all work on edge detection or change detection doesn't really matter. there's two parallel systems running. That's a really nice idea. I liked it. I'm just saying that I still like the idea of the centers surround is this fundamental element that we're dealing with and everything's built up from that. And it could be centers surround color, or it could be centers around non-color. I don't know, but let's go to Jose's question about this diagram. Okay. Yeah. Sorry, I'm switching, not switching topics, but, questions about a couple arrows. So one below the centers around rf. So the light, light green going from the eye to the higher order thalamus, is that.

Is, that, no, I guess we've talked about like rgc, like retina going to the LGN to V one or V two, but I, wasn't aware of, direct sensory outputs from eyes going to HOT. isn't that what we just discussed earlier, Hojae? We were saying, we, we know that V two gets info from the retina, right? Yes. and so the question is, so the retinal ganglion cells, do they project to LGN or they project like to, a cortical, thalamic region that's dedicated to V two? Yeah. V two and in the pulum. oh, I thought the idea was that LGN projects to V one and V two instead of, that's, we were discussing, earlier the.

just review what we said may, maybe you didn't follow it, but the classic description as to say, as Viviane mentioned in Sherman and Gil's book, is that the retina would project to a region of the pul, which would object to V two and the retina would project to lgm, which would predict to one. And then Niels mentioned that there was a paper that suggested V two gets it from both LGN and higher emmic nucleus. So I think the arrow, the green, the light green arrows, it's shown is the more standard dogma, of, what's thought.

I think, and so where an LG an LGN to B two projection was, newer, but as Neil suggested maybe copying too, but we didn't understand that. Does that make sense? yeah, that, makes sense. What. Yeah, just, for literal, we just have to go back into the literature to check it. But the basic idea whether or not this data is now here or we move it over to the right, it's still right. Input that goes into this, V two column. Yeah. Yeah. Yeah. And I guess I was gonna point, anyways, so that, okay. That's my, that's, yeah, that's my fir the second question is the red arrows, the movement, is this, is it like simplified in a sense that like the eye movement will go to the superior curricular first and then go to thalamus and then the columns? Or is it that the movements go from eye to the thalamus, especially lower order? I thought it would, I guess go to the higher orders and then go to the columns instead of stopping at the relay cell area. This is representing, the, par, the Magnus of the cells. getting input from a, the receptor fields are very broad in the eye, and they're representing movement of the eye. Okay. They only, So there, this is movement of the eye and, the movement of the eye. Those cells do project directly to the thalamus and then to cortex. So the eye movement itself is a, okay. It can, will be detected by the cortex, the eye moves. Those cells will say, Hey, the eye's moving in this speed in this direction. the, but there are also other sources of movement information like we can, For example, the movement output from the column that goes, to the subcortical regions has this oscopy that then goes to thalamus and then can get rotated and provide information. But, there's other ones too. imagine the, the vestibular system. It has strong projections to. Lg, LGN and, the pulmon and the vestibular system is another way of knowing that there's movement going on, the head's going on or the head's rotating or something like that. so there are other things that could provide signals for movement. but I think all you have to think about is that those relay cells are representing movement and, they're going through the thalamus because that movement has to be changed. Its orientation based on the orientation of the object to the retina. And, but the source of the movement command could come from the eye. It could come from the superior canicular, it could come from the pulin arm. Okay. There's lots of places that could come from one of the earlier diagram, the diagram we have in the paper. The hetero paper shows that it shows multiple sources of movement information going into the, thal.

it. The movement of the visual stimulus as well, is that, accounted for in the red arrow or, the visual stimulus meaning like the object itself is moving? Yeah, something is moving. because these cells, that would be the input to, that would be the shorter receptor field. Magnus cell would then go to layer three, for the behavioral model.

remember, so the same, magnocellular cells, if, they're pooled, if they're collected over a large area of the retina, that would mean the retina's moving. If the, It's the whole, if the entire image on the retina is moving, we're gonna say the retina's moving. if it's just moving in some smaller part of the retina, we'll say the, retina is not moving, but the object is moving, or something out there is moving. And that would, that's not shown on this diagram is Viviane just said. That would be, a. A another orange red hour going into the layer between layers two and three.

and that's our, that's part of our movement. That's, our movement model that's saying, Hey, this thing is changing the behavior model. The thing is changing. We can learn the model of its behavior. 'cause we see it's, changing. I know this is a lot to keep in your head at once, especially if you don't think about it every day. But, but it does, hang together pretty well.

I have a, question, about the connectivity between, V two and V four. So between VV one and V two, it's like a one-to-one connection between columns and the hierarchy.

not, exactly, but, it converges at V four and so the blue line there would be like multiple arrows from lots of other columns. Okay, there's different connections. We can talk whether they're converging or not.

this is really the best looked at in the hierarchy paper. Perhaps what we've shown is the compositional models require columns that are one-to-one. That is they, if I, want to know what I have to on every location on the coffee cup, I have to know what is the child object that might be there. And I have to do this at a location, basis. And so two columns, one hierarchy goes above the other, can say at this, wherever this column is, looking at wherever this column is looking at on the cup, this column's looking at the logo at some point on the logo. And those can be correlated. but some of the productions should be broader. If I'm, that blue arrow that's going between the second column and third column, there are many columns in, V two, which are simultaneously. Voting on the same object. So that blue hour doesn't have to be a one-to-one column. It could be for many columns in B two to many columns in B four. and that's the, and you do see that kind of, you do see that sort of spread. Okay. does that, make sense? It does, yeah. Thank you. And why? By amazed it can make sense. It seems so complicated.

What's next? Any other questions or sh should we, should I stop sharing? I guess one thing I just thought it would be interesting to think a bit more about like movement in abstract spaces.

'cause I was just thinking it, like how important the direct motor input is to when it gets more abstract.

because almost like maybe in that case we could have this kind of motor output signal, the brown error, but not send it subc critically to actually control a part of the body. Yeah, exactly. Or at least we can inhibit that control when we want to. Like I think it's quite common that when people are like thinking in abstract spaces, like you move your eyes a lot and you might gesture with your hands, but if you need to, you can also force yourself not to move. But, but I guess also I dunno what yeah, what would be driving in this case, but like you could imagine if the high level column is in some abstract space, if you move through that, you're gonna activate. Different kind of neurons in like grid cells in L six that can recover new representations in the layer below. Like you can move through that, space.

and so I was just thinking s proposal that there is no abstract, but that we're still moving. Oh. Jeff, you're quiet or it sounds like you're far away from your computer. I was, but I had my headset on, so I was making a cup of coffee, but can you hear me now? Is that right? Yeah, now I can hear you well. I was only 10 feet from my computer. I'm using my earphone.

so my point, what was I gonna say here? I, think, lemme turn my video on.

no, going back to our early discussion when we were talking about Vivian's proposal, I was saying, oh great, we're still, movement is still in visual space. the features are not, but the movement is. Yeah. but now you're saying, but that's different than we used to think about it. We used to think, oh, movement in apps, spec space. but I thought we got what the current proposal is. We don't have an AppSec space. We have visual space because I'm confused. Yeah. I guess I just thought it'd be interesting to talk through that a bit more and just think about it with some concrete examples. Okay. But now, we're back to Okay. that's abandoning what we just discussed earlier. I'm not saying we should abandon it. I'm just saying I, I think it would be, it's okay. It's okay, but I'm just, but, it's just I'm just trying to think. Yeah. I'm just thinking it's an alternate hypothesis. no, we haven't settled on anything. So an alternate hypothesis that, that this movement isn't related, the one we discussed earlier is that even abstract thinking is I. It's gonna be movements that'll be, visual in some sense. you're going to, or, tactile. But the point is, there's still a visual, there's still a physical movement, co correspondence, even in higher level regions and Yeah. Which is different than we've talked about it before where we've said, oh, concepts live in an abstract space and, movements in abstract space, are not physical. and I get, yeah, I try to make up some examples of that in my book. but I could be totally wrong. So I'm just saying if, I don't mind talking about abstract movements, but it is, I just wanna point out it isn't alternate to the picture we have in front of us. It's not the same. It's like saying, okay, no, let's, keep, going. Let's not abandon the idea of abstract movements yet. Which I'm fine with. I just don't know how to make sense of it.

yeah, I guess it, it's like this is definitely not a well. Formulated, thought or whatever. But it's something I think we've talked about before is how you can imagine flow and, like movement at the lowest level is, you know, just these kinds of bit patterns changing and there's a, yeah, like a flow to that movement of like where things tend to be moving together and things like that. And you can imagine if the input is from some hierarchically lower level, but it's still from a fairly abstract object, that object can change. And that change in the object could be like a form of flow. So for example, may, maybe two examples to talk about. Like one is the, learning of the concept of a parent versus a child and the kind of movement between those. I guess when you first learned about that maybe you were a child in a room and your friends came over with their parents and then there was this discussion about oh, that's so and so's parent or something and, you're like circadian back and forth. And so in that sense, it's like initially in visual space, but maybe in this higher level cortical column, it's getting, lemme add a little bit here, lemme interrupt here. Because I think a counter example is when we really wanna learn about family trees, we almost always resort to visual images of family trees, genealogy charts and all this and stuff. Because then you can visually see the relationship between these things. So I would argue it's hard. The word do you need to, do you need to be exposed to that, to understand the concept? I feel, like it's, it's potentially more just that, say the third, so there's four columns here, the third column here. Is at various times representing child and at various times representing parent like a parent person. and it's changing between those based on what you're thinking about or what you're perceiving. And so as far as the fourth column is concerned, there's a movement in the form of a transition between those. I'm not following this, I'm not following this. Those, lines are getting crazy there. I'm not even sure who wrote those. Did you, Drew, did Viviane drew this? Oh, I drew this as a Viviane was trying it as, let me make my argument here if I want. Go ahead. Yeah, I was just gonna say, the circle's still supposed to be the thous. I was just drawing, a version where there's no motor input from the sensorimotor. But I agree with your point that even with abstract concepts, we usually learn them visually first or in some structured way that we actually sense. But then it seems like inference or thinking about it can happen without any actual sensory movement. So things like this kind ground arrow can like Yeah. Mentally move we can, discuss, let's, look, you can still have visual movement without actually moving your eyes, right? I can imagine moving my hands about actually move my hands. So the question is not that we actually physically move, the question is. Am I using the same process and as if it was visual space? Now let me, let's talk about, it's interesting because my grandmother was big into genealogy, and so the only way I could learn about my relatives in the past is looking at these genealogy charts. It's way too complicated otherwise. But even when I think about it now, and if I think about, oh, I have a pretty small family, so I have to go up a generation like my grandmother's had sisters and, I have a sort of visual picture of it. It's almost like an org. It's almost like a chart, like a genealogy chart. It's like I visually picture them horizontal and vertical to each other. that's, how I, that's how I think about these relationships. I always have the, I always have the older generation, visually higher than the younger generation. it is literally, in my mind, there's an image of sort of a, I wouldn't say it's a graphic, super graphic image, but I have this relative position. It's almost like visual where I, can only think about these things when you have. Generations are going up and down and, people within a generation are going left and right. Yeah. and certainly like math, I think most certainly myself, it's really hard to understand mathematical concept unless it's like visualized well. and most things, whether it's calculus or, vectors or Yeah, all kinds of things. it's very much like a visual, right kind of language here. Here's the funny thing about it. if it's a simple thing, I don't need to really have it spelled out visually. I can imagine it visually, but when it gets complicated, like this image we're all looking at right now, damn, I, wouldn't be able to have this conversation if I didn't have this picture in front of me. It's like there's just no way. I could try to visualize like, oh yeah, but then I forget what's over here, what's there, There's people, so let's separate A, there's people with AIA who don't think visually at all. I don't know how that fits in. How does that work? what, are they like, how do they know? They don't think I, I think they can be like engineers and stuff. Like they can be, but how do we know they don't think visually? What do we, what does that mean? They report not seeing, any, visual imagery in their mind's eye at all. They must have some imagery. They must have some, kind of, some You must have concepts in your head. you can't, it's a, it is not like a, just a, yeah. I wonder whether it's almost like a, something more about the nature of consciousness. Like maybe, those, representations still exist, but for some reason they're just not ly aware of it. How do you know they don't have it? Do they have any? but that's the thing. It's, through self-report. That's, what I mean. So do they are they have, do, might.

I don't dunno. Yeah. Do you know me, sir? Oh, the, is this a something you can diagnose and say, yeah, this person can't do things? Yeah. It's a pretty well known phenomenon. It's a whole spectrum of they, they say people are on a spectrum of, from very visuals of zero. But, what is, there a deficit? is there a, some oh, if you're, if you have this condition, you can't do certain things or you make mistakes, or is it just purely a, self-reporting one? Like when they're, are they just as capable as everybody else and they just say, I don't have any visual inputs or visual imagery?

I'm not sure. I'll look. If there's no deficit, then I don't trust it. I mean, I have to, think about oh, it's visual, but it's not really visual. It's just like, with the, or with the parent child relationships, I say, oh, I, imagine the parents are above children. That's an arbitrary way to think about it, but that's the way our charts usually show it. And therefore that's how I've come to internalize it. And if these people, if I ask them what their parent-child relationship, they must have something they, they think about just doesn't pop in their head somehow. I can't imagine. Maybe they just don't. Yeah, they would be able to draw a family tree or something. I would assume. if they're engineers, they must be able to draw concepts and kind of project that models back onto paper. And it's interesting. If I had to draw a family tree, I'd have to think it would be much harder for me to draw it upside down. It'd be real easy for me to draw it to classic way, I'd, oh, I put my grand grandparents at the top and I start going down like that. If I had the other way, I'd have to think each step of the way I'd to go. Let's say, okay. I feel should be careful about generalizing from our experience of literacy and seeing charts everywhere. if you are literate, working, living with your family and you didn't draw charts or write or don't know writing you, I, you could still very well conceptualize your family. But literacy not read and it probably better than we can read. No, wait a wait. Literacy doesn't mean you don't understand pictures. Literacy doesn't, you don't know the words. I could still, I think in a literate person could no problem at all understanding a family tree. In fact, they've probably seen 'em, they, they see signs, they see everything. They, get around. They just don't know words. What I mean by illiterate is not even bothering with science, just conceptualize being embedded in living with your family. You just understand who's who without having to draw trees, is what I'm trying to say. But you must, but you probably, have a sort of impoverished understanding.

it'd be very difficult to, I, I don't know. just, I, okay. look, we, you, but yeah, you still, your point, introspection is dangerous, but it's also quite useful, so we shouldn't abandon it it is quite useful. and I, guess it's just whether, yeah. Is there still some kind of visual input coming in when people learn about or think these things like. Again, like it would be probably very hard or impossible for a child in this kind of society that you're describing, Tristan, to learn about the concept of family and parents. If they only ever encountered parent, children and parents in isolation, like if it was like, yeah, there is a parent or something, and that was described, but they never saw them in the same room together. Like that concept that, oh, there's some relation between these things would probably be very hard to grasp. Whereas like by seeing them together and looking between them and I don't know. So it's age one's older than, yeah, exactly. So it's, and, one like their behaviors tend to be stereotyped between each other. The parent tends to go pick up the child, scold them, whatever, have that sort of patriarchal relationship, or maternal relationship. And then, so let's, yeah, let's review. Can I just review? I think we all agree. I mean if, I hope we all agree because that. Knowledge is all gonna be, this is corridor theory. Knowledge is all gonna be structured in reference frames. And now the only question is, are those reference frames, grounded in or derived from, physical, sensors? and, so when we said already, we said someone can build the same models through touch through vision. So it's not, it doesn't have to be just vision or just touch or something like that. You can, build the same models of the world through these things, but there is a, structure to it, and there has to be a structure to it. It's impossible to think without structure. It's impossible to know things about structure. So the, this is like a, this is an axiom. It's a, I don't think it's questionable that knowledge is structured in reference frames. And we're just arguing about are those reference frames somehow still related to physical sensors that we use to learn about the world? And so the que one hypothesis, yes, they are. And therefore we'll have visualizations or we'll have a we'll visualize things. Even if I don't have eyes, I will still visualize the relationships between things that I've learned. 'cause we all learn the same model of the world in thumb sense. Or, the alternate hypothesis that no, the reference frames used in sort of high level abstract thinking have no correlation at all to the physical world. and movement and sens and inputs don't, there's, somehow just completely abstract, which I find appealing. or maybe something Yeah. But, or maybe something in between. 'cause it feels like, yeah, like with say, a family tree, like it's important that you, or like it's very useful to look at it and perform those eye movements, but the precise eye movements are a lot less important. In terms of like the physical arrangement of things, it's more about these abstract, like it's the abstract arrow that connects them. It's not their, my point is my point, their relative placement, which is very different from how you understand like a physical object. the, relative placement still exists even if it's not visual. That's my point, was like in matter how I think about parent-child relationship. Yeah, definitely. I, put the parents above the children, whatever that means. it doesn't mean they're physically above it, it's just some sort of imagination that there's an orientation to these relationships.

I guess it's interesting how we learn to have that more abstract kind of understanding of movement that it's oh, it doesn't matter exactly. Kind of where these are displays relative to each other. It's it's this, maybe, still what kind of makes the relationship abstract. it still correlates to physical world. Like, it's an up and down, which is Yeah. Which is, it's a physicalness to that. It's not, I don't have a relation, we can say it's a parent-child relationship, but I imagine it, or think about it in terms of a physical relationship. It's even if it's on a piece of paper or just parents are above children in my imagination, and that's how we teach it. so maybe a, middle ground, proposal. since, like Neil says in, abstract space, it seems like the movement is a bit more constrained or slightly different than movement in physical space. So I. I guess the most extreme case would be there's no physical movement to learn, these abstract reference frames. And the way I would implement that is by taking the layer five motor output from the lower column and sending that to layer six at the abstract model column. and basically layer five of the lower column would learn how to send proper movement commands to move through this abstract space. or the layer six would learn how to interpret these layer, five motor commands. But just remember the layer five outputs, the layer five outputs as it's just typically described, always project to someplace physical movement related.

yeah, so I guess that's where the middle ground solution comes from. is that potentially, like this would be the most extreme case, but then that is really hard to learn and would require I. I don't know if it's realistic that I'm not sure what is, I'm not sure what this case is you're talking about. yeah, basically learning, an association of how layer five movement commands could be interpreted in an abstract space. That, but, if, we believe again, it could be totally wrong and we believe that layer five cells always project no matter where they are to some place that's related to physical movement, then, I have to be, then that output can't be just purely abstract. It has to, it's gonna relate, it's gonna relate to some physical movement someplace. unless it's, 'cause you could imagine maybe from an evolutionary point of view, that was always true. And maybe now in humans, maybe, I dunno if there's evidence of this, but if you look at prefrontal cortex, maybe almost like the, part that goes on from the, the kind of subcortical structures as either atrophied or. Chronically suppressed or something physically, because it doesn't matter anymore. Alright. So, that was the question, right? I said this is the belief what we've read is that every cortical column projects subcortical to, a motor behavior system. It could be wrong, as Sherman told me, he says, everywhere we've looked, that's where we've seen it and we've looked lots of places, but we haven't looked everywhere.

and you could argue that maybe those projections exist, but they don't do anything. I don't, that's, a very weak argument. So it could be that these projections don't exist everywhere and that's incorrect. But if they do exist everywhere, then it's hard to argue that the behavioral output of a column is somehow completely abstract and has no relationship to some physical movement of something. It's just, it doesn't mean you have to do the physical. you don't have to actually execute the physical movement. You can just think about it.

yeah, so that's where I would say like the middle ground solution is it's really useful to support this learning of abstract spaces with actual physical movement, like looking at a, family tree drawn. and there, there might be the possibility to send an actual motor command sub critically, but as Neil said, maybe that's, atrophied or it just doesn't happen very often as you're thinking through abstract space. so I don't you could just in inhibit it, okay, but inhibit it, it inhibited, it still means it the physical movement and you're just not doing it. I'm okay with that. I'm good with that. Absolutely. That happens. There's no question there. I can imagine. I think again, that kind of fits with how, like when, if people aren't holding themselves back too much. Like right now, I think when you're, thinking you do a lot of physical movement, I think that happens invariably. I can look it up. I'm pretty sure there's some studies about eye movements and people recall and stuff like that. Yeah. I think people, will eyes move while they're thinking about stuff. I maybe, the, even if they didn't move it, I don't think it matters. the point is, yeah, there's lots of be any behavior I can imagine in all kinds of behaviors. I can imagine picking up my coffee cup right now and what it's gonna feel like, but I don't have to do it. I'm still imagining it. I can imagine what ringing up to my mouth, it's so the neurons that I would fire when I do that are going through their motions and I can just suppress 'em. I can just run the system without actually doing something. so that's not an issue. I guess really the, real issue here is, are there true absec spaces that have no correlation to. Physical movement, in the world or physical or changing positions in a physical, in some sort of a space that corresponds to the world space. Or that can be a space that can be explored by rep, by sensors. think that kind of space. Very hard to learn for us, or Yeah. Was that okay? Say that again? I think that kind of space would be very hard to learn for us, I would say it's more the opposite. That the abstract spaces are usually a simplified version of the physical space. Like it's a more constrained way that you would move through it.

Like the family trees, you have very discrete relationships, between them. But again, But I imagine I, when I think about family trees, I imagine them literally like images of family trees as opposed to. I don't imagine where my grandparents sat in the house versus where my parents sat in the house. it's I see this sort of abstract, it is an abstract representation of, a family. You've, we've put it down onto a, picked a bunch of boxes and narrows.

what time is it here?

I we're not gonna reach a conclusion. I'm gonna state what I'm gonna work on and then you guys can, I'm gonna just throw it out there, see if VI has suggestions on it, and then other people can take different approaches at this point in time, I'm gonna work on, so the idea that Vivian's proposed, which is that, That in sense, the movements still correspond to, in some sense, physical movements. and that we might be imagining these physical movements relative to images and so on, but they're still physical movements. But then the, but there's no sensory data coming in, so I'm gonna use the, the, I can, switch from learning something with it by looking at images to then thinking about it without the image in front of me. But I'm still, I've established a reference frame from the visual image and and I can work from it from there. So I'm gonna, I'm, that's not the hybrid approach, that's the, it's motors all motor and sensorimotor is sometimes sensors and sometimes, internal representation. Yeah, it's basically like in the drawing here, what I tried to indicate with the dash arrows is. When we actually just think about this abstract concept, those are not active. Like we're not sending a motor output and we're not receiving actual movement information. But instead we can move through the abstract space like mentally, But right. During learning, we would still use those, to learn that abstract reference.

All right, I'm gonna go with that. It's, a hybrid in some sense. Movements still related to physicalness, but the things you sense, the blue arrow at the top may be, we may not be, there may be no sensory input to a layer for, or layer three at this point. No, nothing equivalent to the blue arrow. The real mystery between me. What is the blue arrow on the left that doesn't fit in this scheme Very left all that's how I'm gonna think about this. I think it's a promising approach. It may be wrong. but if I'm gonna spend some days thinking about this, I gotta have at least one, one proposal start with. So that's the one I'm gonna start with. and other people can take other, I, wrote down some short questions for very kind of generally neuroscience questions that I might post on the group in case, anyone's interested in, looking up, or I might try looking up some of them. But everything from, do you wanna show it right now or, do you wanna just post it later? yeah, let me, maybe if I gimme a minute just to clean them up and then I can do you want me to share my one slide while you're cleaning them up or, oh, yeah, Let's go. That I forgot about that. Don't want, don't wanna understand. Yeah, it's, really more an idea to throw out there again, maybe something to think about more.

Yeah, I don't know if I need to show this, but I Oh, nice picture. You go back there. Wow.

yeah, I, was, I tried to clean up the hand-drawn, diagram that I put in the object behavior, write up. So this shouldn't be anything new. This is just a kind of not hand drawn version of it. but in case we need an image to talk about stuff, I can pull this up again. but yeah, basically just showing the object model in green arrows with the static input and voting on object id, and then the behavior model with the purple errors, with the changing inputs, with the additional time input. And then again, we can vote on behavior id.

and they have two separate reference frames that we move through in against Synchrony. And then what we talked about before is that. like object ID can then become a biasing input to layer four in the higher cortical region. But what I wanted to talk about a bit more is, what happens hierarchically with behaviors. So can we actually have compositional behaviors or is there like a similar projection of the behavior ID up here? so I have kind of three thoughts on it that I came to for myself for now, but I don't know what you guys think. I couldn't think of any good example of compositional behaviors where it's actually important to compose behaviors of other behaviors. But there are lots of examples of behaviors being applied to compositional objects. So we can have a compositional object and different parts of the objects can have different behaviors themselves. But I, just couldn't conceptualize a compositional behavior composed of other behaviors. So I'm just putting out this claim that there are no compositional behaviors and behaviors are simply applied to compositional objects. So basically, if you look at this picture, there's just the green error going up here, and then we have compositional objects being modeled here and up here. We would then again, recognize behaviors at locations on those compositional objects.

I guess that's maybe a radical claim. I don't know. no, you caught me out on something. I wrote the shared to Viviane where I started saying, oh, here's a set of tasks like to make coffee, the one that Niels always uses. And, and she says, that's not really compositional behaviors, that's just a series of tasks that's oh, you're right. so I, and then I try to think of, I could think of any composition behaviors and nothing popped into my head. So I feel like it maybe depends how you define compositional behavior. I think it's clear that there's yeah. Situations where you need to decompose things or you understand the behavior as a series of things.

I don't know, just like the way a phone works, like turning it on is like the first step to being able to then access a series of things. Going into an app is the first part of being able to access a series of things. Each one of those levels is associated with different behaviors. but I don't need to understand all of them at the same time, or interact with them, all of 'em at the same time. What I would define as the compositional behavior would be taking the behavior ID that we recognize down here and applying it to a location in the behavior model up here. So in the, pH style. By applying you mean learning and association or? Yeah. Yeah, basically saying at this location in the parent behavior, I learned that there's a child behavior. So I think in the phone example, I would again solve it as having a compositional object with different behaviors. So like opening the app on your phone would be going to a different object id, which then has that's the behavior of going from one object ID to another, like your phone home screen to the app home screen. And then that app home screen has a different behavior associated to it.

maybe the make, maybe I feel like the coffee example, so the like behavioral state at least of the coffee machine is on, is a useful thing to have in the higher level planning kind of behavior or whatever of like brewing coffee. but it's not necessarily compositional 'cause Yeah, I think that's still being passed up, that information. But let's, I would assume, let's, Viviane I think mentioned the key element earlier. When we think about compositional objects, we say there's a child object assigned to locations on a parent object. So if we're gonna think about composition behaviors, there'd have to be a child behavior assigned to a location on a parent behavior. I can thi I just thought of, I, I could, I made one up just sitting here. Now imagine I have a, there's a little, a disc that has a interesting pattern on it. Say it's a letter A and this disc has a behavior that it'll spin around counterclockwise 360 degrees and then stop. And when I see this disc, I know it has that behavior. I, oh, there's a little spinning a and there's, now I could say that's spinning. A is now I'm gonna attach it to the upper arm of the. Or say the bottom of the stapler or the upper arm of the stapler, say the upper arm of the stapler. And when I lift the upper arm of the stapler that a spins, and I can put that little spinning a on a door, and if I pull the turn the door, handle it right above his little A and it spins. So those are, examples where the, little spinning a has its own behavior. when I see it someplace new, like on the stapler arm, I don't, have to learn its behavior. I already know that it spins 360 degrees. but I now know that it's located, it's part of a behavior of lifting the stapler arm.

yeah, so I was thinking about a similar one, and originally I was thinking, okay, so I can solve this with applying behaviors to com compositional parts of this object. So the A has this one, the spinning behavior and the stapler has the opening behavior and the A is the child object on the stapler. But now you're bringing in an interesting component of where the child behavior is dependent on the parent behavior. So you said if you open the stapler, then the A spins and only then, so yeah, right now I'm not sure anymore if, yeah, maybe it does need compositional behaviors. there's a, there's an, that's an example where the spinning A is located. It, its, behavior is I, the key for me is I don't have to relearn the spinning. A, once I've learned that behavior, I can assign it to a new thing that has its own behavior. that's the key, that's the key to compositional. You don't have to relearn it. You just B well. Okay. So if o only that part, I feel, I still feel like I can solve it with com, having compositional objects and then applying, behaviors to the child objects on those composition objects. But the main part that made me question, whether we need it now is that you said there's a dependency between child behaviors on the same object. So like the parent behavior would need to learn these dependencies between those. Yeah. This is why I feel like the definition of compositional behavior is important. 'cause I think basically we can all agree that there are objects where like you will have one behavior of a child. You'll have another behavior of a parent and you'd be interested in associating those two potentially.

but yeah, like one way of describing that is just compositional objects where each object can have a behavior.

So I, don't think like anyone's denying that's important. And then the, question is how the behaviors might interact across the hierarchy, which is Yeah, Maybe a bit more of a gray zone. Yeah. like it, I, we're all just it's hard to follow these conversations, but I just realized a behavior when I talked about the spinning A, the behavior is not the spinning a, the behavior is a, is spinning the way we define behaviors. It's just, it's the changes in the morphology or, of something and the actual details. oh, there's an A is independent of the, spinning behavior, the behavior is spinning, and I've applied it to an object. So now the question is, does spinning in one section of a hinge as a hinge thing, that's a behavior. Something's moving at some angle. We might call it a hinge. It's just the movement that's, we're encoding. We're not quoting what it is, it's just the movement. And I have another, which is a spinning movement, but not a particular a it is just can do I want, do I associate spinning movement with that? 'cause as soon as I say it's an A, that's spinning. It's an object that has behavior. It's not, it's no longer just the behavior itself. All right. These are very complex things to think about. I'm, arguing now that maybe I was wrong with my spinning a example that maybe it's, really composition. I say not, that you brought up the spinning. A example. I'm changing my on composition behaviors.

I think it might be useful to have, the ability to, associate a child behavior on a location of a parent behavior in the sense that we can learn, especially to learn, the timings of when child behaviors start and stop and. Dependencies between them. So I guess for a real world example, instead of the spinning a, we could think about a car. I mean on the car it's a pretty complex one because the car has like lots of behaviors at lots of different locations, but some of these behaviors are dependent on each other. So if I turn the key in the ignition, that's one behavior, the key ignition turn. But that, what is, that starts Other behaviors have a key maybe. Yeah. Maybe a better example is like the one that Scott gave a long time ago with the lamp switching, like the, there's a switch on the lamp that we can turn on and off, like up down, whatever. That's like a, I mean we can call that just switching. It doesn't need to be switching this. The button, whatever, switching is the behavior and then, the parent behaviors, like whether the light bulb comes on or off or something like that. Yeah, I think, we're confusing two things here, which seem similar, but they're really different and Yeah. One is true compositional structure, location to location. there's a behavior at some location relative in the same reference frame as the parent behavior. Yeah. and there's a whole nother series of tasks which falls into the make the coffee cup, or make the pot of coffee or turn the light switch on where you're trying to correlate and learn causation between behaviors in one place and the behaviors in another place. At first, it sounds like there's a saying, but I'm, I, but if I, if we say that the behavior, a child behavior is defined by, that has to be signed on a location by location basis to a parent behavior. If that's, our limited definition of child-parent relationship, which is what's suggested in this, we're talking about in this diagram in front of us, that's very different. That's, more limited. and I, although I argue though the spinning a was an example that showed that it exists and now maybe I convinced Viviane, but now I'm thinking like, oh no, I got it roaring. Perhaps the, spinning A is an object that has behavior, not just spinning. so switch positions and Yeah, we switched positions. We'll talk again next week. Yeah. but I think this is key, whoever posted the Wozniak, was it Wozniak or some of the make the coffee pot. Oh yeah. Wozniak Coffee Cup Benchmark. that's what Neil, that's what Neil's been using too. So it's oh, great, maybe that'll become our thing. But that is not, I think it's really, arguably that is not compositional behaviors, at least not in the difference we're talking about. Yeah. That, is something else. And we don't even have the language for it yet where we're trying to, we're trying to get a, this, the world in some state, and we're gonna use lots of different behaviors, to do that in, that's different and that requires temporary memory. the, you have to remember the, what you've done so far and where things are and, I don't think it's gonna be, yeah, that's definitely more complicated because it's motor, it's really like policy and planning and stuff. it's a combination of behavior and, changing the state of the world. I'll argue it looks really complicated now, but we're gonna come up with a solution that isn't complicated, is, it'll be as complicated as our behavioral model or something like that. it, it'll be a, there'll be a basic mechanism. I'm betting there'll be a basic mechanism that explains all those things. so we start with very complicated examples that are befuddling, but the goal is to find the simple underlying, methodology the brain uses for these and apply it in lots of different ways. So I think that's gonna happen. Yeah. so I guess one parting thought, on the topic of behaviors in hierarchy. so I've been lately just I kept thinking about, okay, I. All the time we're thinking about modeling object behaviors, but I keep seeing link like smashing objects against each other and hearing what sounds they make and taking a pencil and drawing with it on it. So like lots of, things he learns about how objects interact with each other. And, but I think like object interactions will naturally come out of the same mechanism basically that at higher levels in the hierarchy. So where we learn more scene like representations, we can learn object behaviors that represent like behavior models, that represent interactions between objects. So if I smash two objects together, what sound do they make? If I move a pencil over a paper, what, how does the paper color change? if I pour water into the cup. So this is, this is what we were just talking about. this third bullet is corresponds to making coffee pots, right? pots of coffee. It's, a combination of things interacting with each other. And, I think you just said you think this is gonna come out of our behavioral, I was arguing a second ago, it's gonna be a different way of thinking. It's not compositional behaviors. you already said there's no composition. No, I'm not saying it's compositional behaviors, it's just behaviors being learned on compositional objects or like a scene representation. And I, I don't think it falls into the bucket of trying to, output actions to achieve a goal or put the world into a certain state. It's more about learning a model of how objects interact with each other. So just like seeing Yeah, but you have to do, you have to do that to make the copy pot, right? You've had to learn how liquid flow and how lids open. Yeah, that's part of what you need to learn. But it's, I would say it's represented as a behavior model the same way we talk about behavior models within a one object.

I don't know, it might be something different. My first guess would be something different. I would say behavior models on an object. Are they structured things they can be like, staplers and, things that move and change, running.

it seems to me like these kind of interactions between objects are, it just feels something different to me. yeah, I guess it feels like in general when we've been talking about things like the stapler and stuff, we've been deliberately trying to avoid discussing things like causality and like how things are interacting. Like the staple has always been like the arm always moves on its own and all this stuff. And, so that's the kind of missing piece is this what causes the system to change? what causes object behaviors and how do you learn those associations?

Yeah, I wasn't really talk, I'm not trying to bring actions into the picture right now or actually how we move th things through different, how we move an object, but more just about the stapler, if you push it down and it actually puts the staple into the paper, but it wouldn't put a staple into a, wouldn't block. that kind of knowledge. like how does it interact with another object and where would that be modeled?

here's how I've been thinking about this. the how we interact with world, all these complexities, things, lots of things we don't understand. And I said, let's carve off one piece of it. The one piece of it that we could start with was how do we model behaviors of objects? That was just a small piece of it. and it was a very challenging problem. But it was a small piece of the bigger issue. It was just like, oh, I think we're gonna have to need to know this. I know we're gonna need to know this before, before we can go on and tackle something else. This is like a, foundation component.

So, I, we got a pretty good idea how we learn behaviors. So I think it's right. So now we're debating like, okay, what's next? It's okay, what's the next thing if I'm gonna make the coffee pot? I certainly have to know how the lid of the coffee pot goes up and down. I have to know how the faucet on the co on my sink goes up and down and turns to get, these are things I have to learn in behaviors, but it's, that's just the ground level, entry level here. So now we're just debating like, how do we, pick about the rest of the problem? No, I wasn't trying to bring up another problem. I, was thinking about it for the past weeks and I thought it was another problem, and the only reason I brought it up now is because I thought. I thought that we can solve it with the same mechanism. I'm not talking about like, how do we manipulate the world? How do we interact with the world? How do we learn causality? I'm just talking about instead of learning the behavior of one object, learning the behavior of two objects interacting by learning behavior that's applied to a more of a scene representation or like a parent object that represents both objects that interact with each other that certainly that could be true. but it gets pretty complicated really quickly. oh, a rubber ball hitting something is different than a, a ball of yarn versus different than a steel ball. And, throwing a round thing at something is, it's getting complicated now. Like it's, these attributes matter. It's not just the behavior, it's not just like some change. And we've always defined behaviors as some change in the features and their orientations of, objects. I don't know what it is now, how does we learn, how does Link learn? rubber ball is different than a steel ball versus different than a, baseball or something. Yeah, I guess that would be like, how I was thinking of that is that basically the object model ID would inform which kind of behaviors can be applied to it. So if I infer the rubber ball, that means I can only infer the kind of bouncy behavior and not the metal ball smashing a whole ground. But we haven't discussed how we actually learn that. and, the sort of combinatorial side is interesting, like speaking of like computational behaviors 'cause it's like how a rubber ball interacts with like a jello ball or whatever. Like you may have learned about a rubber ball and, you've seen that in like normal settings and you've learned about jello, but somehow you can predict how to a certain degree, what's gonna happen. Even though you've never seen those two objects interact. So it's like the combination of their behaviors? no. We have a, we have, another week of brainstorming coming up after the hackathon, and it's only two months away, I think. the last week of brainstorm we had, a ta, we had a topic. The topic was to understand object behaviors. And so it was helpful to know that in advance so you could prep your brain for it and, make it more likely that we succeed at that. I'm thinking, I'm asking myself what should be the, topic that we're gonna try to solve two months from now. and we are dancing around it here. I'd love to say, oh, make a pot of coffee. that seems too ambitious for the next two months. Maybe. Maybe by the end of the year. But right now that seems too ambitious. So I'm throwing this out because I'm wondering what would be a good topic. To yes, this is meaty, it's self-contained, it's self, it's like object behaviors. We could define what that is. How do we represent it? That was the key. How do we represent object behaviors? we don't even know how we learn 'em yet, but we do know how we represent them. So that was great. so the question, what would be the topic for two months from now? I'm throwing that out just 'cause over the course of the coming days, maybe someone can suggest or we can, it'll become clear. That to me is a, goal we should be shooting for is what topic we want to solve. And then, we'll have a better chance of solving it at that time.

Yeah. It still feels like we, the object behaviors picture is incomplete and we've done a little bit of a tangent talking about features and stuff, but it feels like it was important to develop some clarity there and then hopefully we can go back to more kind of core object behavior discussions we were having and it'll be more certainty about as in like the stapler example, like we, I feel like the last time we were talking about that, like we, we had ideas and stuff, but then it was like, okay, actually what's L four doing? What's L three three doing? What, we didn't, I feel like that's why we've gone on this tangent. I think what we did do is we, resolved how to represent behaviors. I think that's very clear in my mind. We didn't resolve how that representation of behaviors translates to predictions about physical observations, like what the stapler cup will look like when it's rotated up. Yeah, I guess that's, there was like missing pieces. Yeah. I, would say that's the next big one. Like how, do we actually make predictions when we apply a behavior to an object, to a new object? and yeah, I think generally we have a pretty good list of open questions. Collected in, in that document that we can pick up whenever, I mean it starts with like, how do you make predictions, but then it goes all the way to how do we use the behavior models to output actions. Yeah. which then goes more into the making a coffee pot example. I think I'm, hopeful that, we could solve the, prediction of the Coffee of the Stapler top based on the prediction, based on the behavior model. That is not gonna take two months. I think that can be solved well before that our next meeting. So maybe we should work on that, or at least some of us can work on that. I don't think that's big enough for me. I want something meatier. It feels like I, maybe I'm wrong. Maybe it's gonna be a really hard problem, but it feels ah, I can't be that hard. It's all the pieces are right there visually, I can just somehow make it work. anyway. I it's a good point. We didn't finish it up, so we should finish it up. I'm hoping we can finish it up soon and then we'll have a meteor topic to deal with, in two months from now. Yeah. I think in that case, using behavior models to change the state of the, to figure out how to change the state of the world would be a big, topic. that would tie into causality and learning about that and Right. But we need, but I need, a very specific example. changing the, I think the lamp ones are good for that because, 'cause you need to learn the causal relationship between the switch behavior And the light behavior. That's you. That was it. It's super simple and that we did, we said that before and now that you mentioned it. Yeah. We said yeah. when, it came up the first time. He said, oh yeah, that's a good simple causality one. that's change the state of the world. Oh the state, we want change is the light's gonna be on. That's the change. That's our, task. We started practicing drawing lamps on the whiteboard.

okay. So I'm always conscious of autonomy, not that I have to run, but it's already after 10.

we do have to finish up the how behavioral models in interact with object models and, and then that has to be done. And then, but we also can move on to the lamp model, the switch and learning causality, something like that. Does that make sense? Yeah. And I think talking about the, that example might actually help us figure out also whether we have compositional behaviors or not.

I think also thinking about how the, behavior predicts the morphology of an object is also, might be related to, might also play into that.