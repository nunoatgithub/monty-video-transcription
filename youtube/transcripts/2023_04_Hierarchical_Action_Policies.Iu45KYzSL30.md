Okay, nice. So yeah, this is my effort to distill down some thoughts around hierarchical action policies. It's definitely not clear in my head yet, so this is very much going to be just like an amalgamation of stuff. But yeah, hopefully that's, the point that we'll talk through this. And also, it's not like these are really, new ideas, it's more just about trying to structure how we think about this and, yeah, just think through it. And it relates also, to, for example, Rajesh Rao's presentation. There's definitely a lot of similarities to that as well.

but yeah, I thought, yeah, so in terms of what the aim of all this we're doing is, these hierarchical action policies are really about yeah, flexible agent behavior and efficient agent behavior, that can really kind of impact the world, and, yeah, handle arbitrary, novel situations and, and things like that, and, I think we all have kind of an intuition about how hierarchy helps with efficiency and flexibility, but, yeah, we can maybe just work through it a little bit.

but yeah, at the same time as we're doing this, we talked previously about how it feels like we need to, also understand better how we're going to represent object behaviors. And I think that is true, in order to really come up with meaningful, hierarchical action policies. so that's a lot of what I'll talk about today.

and I thought just like a first kind of useful thing would be just to give some kind of basic definitions, just to make sure we're all on the same, coming from the same point.

and, yeah, in particular, so action policy, this one's pretty basic, yeah, basic set of actions to take to achieve a goal. this is typically a condition on the state of the agent and the world. reinforcement learning is methods to learn an action policy given reward signals. And there's kind of two main flavors of reinforcement learning, model free, which has been very popular for many years, where you basically just learn an action policy directly from the rewards, that are associated with actions you take, but you don't actually learn a model of the world to, use in the future. You basically just learn okay, this action was good, so I'm going to do this action again.

and then more it can still be state condition, so it's not Right. Yeah. So you still use the sensory input, but you just don't have a model of the world. Yeah. Yeah. That you could, roll out in a mental simulation to, to decide okay, how's it going to respond? but yeah, more relevant to the approach that I think we were all, thinking about is this more model based reinforcement learning where you explicitly learn this. model, which is often called a transition model or a transition function of the world. and then, you also can learn a reward model, that basically tells you, how valuable being in certain states are. And then with those two, so understanding how one, like what causes one to move between states, and then understanding what rewards are associated with particular states, then, it's very easy to develop policies on the fly and adapt as necessary. if a wall gets, or a blockage gets put on a road, you can, refer to the model and find a way to, to go around it.

and, Yeah, and in general, the kind of holy grail is, to learn, these models in an unsupervised fashion, and then, you can leverage reinforcement learning as necessary, when, there's states that are important, that are useful, but the hope is to really yeah, learn the transition model, first, so that you can, leverage it later.

Before I go on, I think you touched on this here, but.

I've always been uncomfortable with the, reward reinforcement learning with rewarding, because it seems like so much that we do is, you mentioned getting around an obstacle. It's not really a reward based learning system. It's, it's a model system, right? I'm a model, I'm trying to get to some state. And I use the model to get to that state. I don't have to, I don't even have to learn that action policy. I just use the model and execute it. And then, next time the road won't be blocked. So I guess I'm, all I'm stating, and I think you're getting out of here, but the whole idea of a reward model, reward system, even for a model based, reinforcement learning, I think it's a bit of a red herring and we shouldn't focus on it, right? We really just say, we have these models, how do we achieve various results based on, given a desired outcome, how do we do that? And it doesn't have to be reward based at all. I don't know if you agree with that or not, but I just want to make it clear. Yeah, no, I definitely agree with that. And, yeah, that's what I was trying to say here was that, like, when you At least from my reading of the literature, you rarely see studies that just go into transition model and learn the transition model. There's so much focus on reinforcement learning generally when it comes to policies that it's almost always baked in. That's the only reason I brought it up because in your first sentence here it says, along with a model of rewards, Yeah, because that is how it's defined, even though, because yeah, but yeah, so in a moment I'll specifically drill down into the transition model, but yeah. But not necessarily, like often you can learn a transition model without rewards, like you can learn the transition model using prediction errors, for example, but then learn the policy of how to go through how to best go through this transition model using reinforcement learning. But you could use the same model, and then apply some other planning algorithm on top of it, and you would have never used reinforcement learning. Yeah, so No, for sure. And that's what I mean by unsupervised learning here. I'm just, from what I saw in for example, with hierarchical reinforcement learning, there's very, they almost always talk about reinforcement learning, even though, like you say, it's not essential, and in fact, we would ideally only use it at the last point, but it's often yeah, thrown in there. You don't have to use it at the last point. it's, if I have a sufficient model of the world, and I'm just trying to get to some state, I, don't need a reward, it's just, I don't know, we've, I think we've said enough here, but I think we should really focus on this model based transition model, I agree.

yeah, just the kind of classic example of this from neuroscience is Tolman's work with rodents where basically they would explore maze like structures. And, given no reward, and then, if you took the, those, and they would explore it seemingly haphazardly, but if you then took those mice that had a chance to explore the maze, and then, gave them just like a single reward, they would very quickly learn, or be able to move through the maze, to reach their reward, in comparison to ones that kind of were trying to learn how to get to the food from the start. And it suggested that they built this sort of world model. This kind of cognitive map, and that's what they were leveraging. And I think again, a lot of reinforcement learning and this kind of whole, action, yeah, sensorimotor, action based, learning is very focused on an agent interacting in an environment. What I think makes Numenta unique is we're trying to capture this sort of concept, but at every level of representation, that we're saying, this isn't just something that's oh, how does an agent interact within the environment? It's like, how do we understand essentially everything, including objects?

and yeah, we basically want a cognitive map for all of the models, that are going to be relevant. And compare, drawing the parallel to Monty here, basically learning the maze is what we do right now, learning the object models in Monty, and then the reinforcement learning part. would be saying, okay, I have this model of a cup, and I want to achieve the state, how do I do this? And then achieving the state would give you the reward, but it has nothing to do with learning the model of the cup that's been defined. You don't even need a reward. you have to know that you've accomplished the goal, but in the computational sense of it, basically just telling you accomplished the goal.

the fact that, do we lay down memories of how we solve the problem? Yes, we do, for the most part, but not always. In a maze like this, surely I would remember, oh, I've solved it, next time I'll be quicker, manipulating the stapler, I may never manipulate the stapler the same way again, it, I might do it a different way next time, I don't know, it just, it doesn't always have to have this reward signal, and we don't even always have to learn our solution, we can. But sometimes it's a one off solution, and I'm done, and it keeps going on, Anyway, I think we've said enough about that, but I have said enough about it.

but yeah, so the kind of, typically this, this kind of model of things is the term to transition model or transition function, and yeah, it's basically the kind of set of states of an environment or an object. those states, and then the associated transitions, that occur between those states, and how they happen. so often it's conditioned on some sort of action.

and it the simplest would be a deterministic mapping between states. So you're in one state, and you deterministically transition through a series of states. Obviously, that wouldn't be very interesting. or you can have kind of a simple matrix of probabilities that basically tells you what is your chance of randomly going from one state into another state. But yeah, more often it's a kind of condition on an action and it takes the form of some kind of arbitrary function, which may be probabilistic.

And it can be hierarchical. So I did find some kind of discussion, for example, of this in the literature. because we were talking about, presumably, yeah, others have thought about this and, I thought there was, yeah, a particularly useful paper from, Matthew Botnovich, I think, from 2014, where, yeah, he talks about hierarchical, reinforcement learning, hierarchical transition models. Is that the one you sent me? No, I'll mention that one as well. That was, the one I sent was the one that was linked in, Rajesh's paper, which I also did look at briefly.

Yeah, I'll come back to that one. So I didn't read it, it was all math. It was, and it's very long. Yeah, sorry, I should have looked at it closer before I sent it to you. It's alright, I spent only a little bit of time on it. Okay. I'll let you read that first.

but yeah. And then the other thing is often this action is described as the action of the agent. and again, as I was mentioning, often this kind of literature, it's all about an agent moving in an environment, like a room. but I think it's useful and I think it's fair to take a more general view of this kind of action as just an influence on the state. It could be kind of anything from gravity to the influence of another object, it could be the behavior of another agent, flapping its wings, and we're observing that, or we're, intuiting that it might be doing that, and then of course it could be, an action of the agent itself.

and then, yeah, and arguably maybe also internally driven, things like the elasticity in a rubber band, although maybe this is captured fully in, the state itself. But just the kind of idea that it's, it's not like everything is just, completely passive and the only way that anything changes in the world is by the agent going out there and interacting with things. things are much more kind of fluid than that. there are many things that can influence other things, transitioning through states.

and yeah, and so this, yeah, feels like a natural way to start to connect the kind of object behavior, with the kind of representation that's going to support, planned actions. Where kind of, yeah, hopefully once we somehow learn this kind of transition function, transition model, represent it in a kind of useful way, then that can support really powerful, action policies.

. and then just again, because partly this is because the terminology out there, yeah, I figure we might come across it and it's just useful to understand how it all hangs together and how it relates to what we're trying to do. So there's also this notion of hierarchical reinforcement learning. And basically, yeah, this is standard reinforcement learning. It can be model based or model free. and you're trying to learn action policies, but where these, action policies can abstract over time or states.

if you imagine in this diagram, you have to take this series of, kind of actions down this tree to reach a particular goal state. you might learn a way to lump these together so that this first one is okay, I'm opening the door. And this one is I'm getting out my car keys or something like that, rather than the minutiae of motor movements that you have to do, in order to achieve those kind of sub goals. And, of course, that's much more efficient to plan, it's much easier to do credit assignment, and all this kind of stuff. but, again, it's, yeah, you don't necessarily need the kind of, if you're just trying to achieve a reward, sorry, a state, then, as you were saying, Jeff, The reward learning part maybe isn't necessarily that, that relevant. if you just know you need to get into a state and you know the structure of the world, you can just get into that state. the funny thing about that diagram, is that if I think about, actions and action policies, this says there's one path to any endpoint, any end state. But that's not true in the world. There's many paths to get to there. I was looking at these lines saying, oh, do they connect? No, they don't. This says this is an ideal solution, but in reality, you should be able to get there from anywhere. it's yeah, you can have that end state, but there's multiple paths to get there. so this kind of diagram doesn't capture that. I think you're arguing about the chunking of actions or the hierarchical nature of these actions. I think that's a good illustration of that, but it doesn't, it doesn't capture the nature of the problem completely.

Yeah. And then, this is just another diagram trying to show this kind of abstraction that, yeah, you can do these kinds of temporal abstractions over a series of, actions, where you can do more kind of a state abstraction, so chunking that, okay, what's the high level concept that we care about, the high level state, it's the agent has the key, and so forth.

but Okay, yeah. The state abstraction seems somehow more relevant to me than the template. It's yeah, the state abstraction is the first thing you do, I think, right? it seems like the first thing you do is you say, Yeah, I think it's definitely, we're already, doing this because obviously, yeah, again, we don't want to just, know models at the highest level of detail or whatever. and yeah, we want relatively, Abstract models of the world. so that's something I guess that we're already doing, with heterarchy.

but yeah, what's interesting is if you look at these papers, generally a lot of the literature is focused on this kind of model free. if it's, whether it's hierarchical or not, a lot of reinforcement learning is focused on model free, but particularly this, the hierarchical reinforcement learning. Basically because learning hierarchical world slash object models. is, not necessarily that easy to do.

yeah, as you were saying, Viviane, there's unsupervised, signals, predicting the next image or whatever that you can use to inform them. But I think it partly just relates to, the kind of, represent, the kinds of ways that a hierarchical model is being represented. As far as I've seen, it tends to be fairly shallow, or, yeah, relatively simple systems, which I think the next time, we do a brainstorming, I can maybe get into, learning how, a set of particles interact and stuff like that, with kind of a world model.

but yeah, but of course it can be model based, and that's maybe where we'll come in.

so then the kind of last definition just to mention is this term options, which again you might come across. And my understanding is basically just a term for this kind of chunking, of, actions together into kind of a more temporally abstract, manner. And so an option would be something like open door, and then if you select that option, then that's going to initiate its own subpolicy, and play out. until it's done. You're saying this is the term that's used in the literature? Yeah. So it doesn't imply a choice, it just, it, you're saying it just says it's like a, an abstract, action that may be done different ways or something like that? When I think of options, I think oh, you have three options, A, B, C, which one do you want? I guess you, yeah, in this case, you do have, so you could open the door, you could lie down on the sofa, or you could turn on the TV, or whatever. Those are your options. I don't really know why they're called options, it's, but anyways, but then, then once you select one, then you're going to have a different policy that sorts out actually executing that.

Okay. I don't know, yeah, Viviane, do you have a sense why they're called, Options, oh, I don't know if she's frozen. Viviane's frozen. She's frozen in a good pose, so she looks good. very pensive. You're frozen, Viviane. Yeah, sorry. No, it's okay. Yeah, do you have a, a deeper sense, Viviane, of why they're called options?

yeah, I also, I'm not sure why, but that's definitely a term it's like in, in the reinforcement learning book from Sutton and Bartow and they have a whole chapter on it. I guess it's just to distinguish it from the term actions, even though they are kind of actions, just higher level actions. okay. Okay. Yeah. Not true. No, that, that makes sense. Thanks. It'll probably take me a while to absorb that. Okay.

so yeah, so that's all the kind of definitions out of the way, just so we're all on exactly the same page now. Sure. But yeah. Yeah. Yeah.

Just then, to take a step back, so the kind of challenge is, yeah, we want to learn this transition model of objects across multiple hierarchies, these kind of object behaviors. But not only that, we need to learn how motor systems would interact with these, with the transition model, to achieve a desired goal state. So it's this kind of intersection of object behaviors and action policies.

Which are clearly related, but also not equivalent.

I thought as a first thing, I would try and just walk through how I'm conceptualizing the transition models at the moment, and how kind of it maybe ties in with the sort of cortical anatomy as we've been recently discussing.

and yeah, this is going to be focusing on object behavior, and it's not, going to get into kind of hierarchy yet. So if we just imagine that we have a model, a learning module that has learned a model of a stapler, we have these different layers. If I've, for these layers that I've just had one color, it's because I don't necessarily have a good reason for subdividing the layer, but maybe that's something that will come up in the discussion today. But, and kind the term, can I just, again, the word transition model, I wanna try to understand what you mean by that. why don't you just rephrase what that means in a sentence? is that how the morphology, the stapler changes? Is that what it means? So it's how, the state of the object changes, with the environment over time. Okay, so it could be But the state might capture quite a lot of things. but it could be like the stapler were going up and down. It could be the icons changing on the screen. So that's, that transition model is really, it's the state of the object and how the states transition between each other. Yes, but it could also be, it could also be where we are on the object. Okay. Why would that be a model? That just seems to be a temporary, So the model is, tells us how, transitions take place. So it's not that the location on the object is a model. It's just, for example, grid cells and our understanding of path integration. would be part of the transition model, because they tell us that if we, move through space, then our location will be updated, where the location is part of our state.

that's confusing to me, because to me, there's a model of how my, sensor moves, my finger moves through space under different motor behaviors. That to me seems very, largely independent of the model of the stapler itself.

yeah, so it's just all about staplers, yeah, I guess that's where I just, I feel like they are related because, yeah, maybe if I just go through okay, all right, you had a question mark there, I'm not sure why, but I was, I had a question mark too, so maybe, yeah, how I was most thinking of it originally is that the transition model is, It's basically how actions move you through the state space. okay, so those actions would be relative to the object, that really doesn't matter which finger I'm using or anything like that, right? it's like the action would be specified in the model. It's The top of the stapler has to be pushed down relative to, something like that. I can't separate out this model of the stapler, which has these behaviors and does various things, or the model of a smartphone, and I can separate that out from how am I at this moment, where my body is, and how I'm interacting with it. Because, I can push the stapler down at my elbow, it doesn't really matter. Yeah. Yeah, I mean in its simplest form it's basically saying, if I'm in this state and I take that action, what state will I be in next? But I'm saying that the action should be in the models the definition of the model, not the definition of how my fingers move into space. yeah.

Okay. But so I feel like it should maybe be both, as in, yeah, so humor me for a moment. I guess if, so if the state is, capturing a set of things, so the state is, it's more complex than just where we are on the object, and it's more complex than just the state of the object, like whether it's open or closed. It's, it's basically a set of these things. for example, it could capture, yeah, where we are on the object. The sensed feature, we're experiencing, the object ID, and then, and the object pose, and then, I think we, arguably also want some kind of object state type, representation which might be wrapped up in this L2, L3, Yeah, open versus closed, subject to gravity, just, it could be, fairly flexible.

okay, if I have a learned behavior, my default behavior is to use my index finger, push down on the stapler, and, maybe that's a, fits your model there, that's, because it matters where my finger is. That's not the way I think about it. The way I've thought about it in the past is more like the stapler has its own model, how it behaves, and then I can apply my body to it in different ways. But we can go with this for now. It just, it doesn't, it feels like those should be separated to me, but you can make the argument otherwise. Okay, yeah. Yeah, I can maybe they're still separate, but I guess, I just feel like they're entangled as well, let's just say there is some transition model that somehow captures, all of this, like basically how all of this is going to change given some, the current state, the current set of, all these kind of state variables and the, and some action.

and yeah, go ahead. the variables you showed on the slide before. They would not necessarily all be in layer five, it would be more like they would be represented in other layers, and layer five tells us how to move through those given actions. yeah, no, for sure. Yeah, so X, XT is in L6, sorry, I should have made that clear. XT is meant to be in, L6, FT is meant to be more in L4, OT and PT are up in L2, L3. Okay, yeah. None of these are in L5. Yeah, just because technically that would all be part of the transition model.

Basically what you're saying is L5 is just telling us the routing between Yeah, how to move between them. Yeah.

yeah, so it's more this model itself that's maybe in L5 rather than the states representations.

and yeah, and this transition model itself is also subject to change. It isn't totally fixed. Okay.

so just then going through some examples. So the kind of simplest action would be a movement through space, which would, yeah, affect, kind of XT plus one. this could be via a direct connection, and this could just be hypothetical, like a predicted, we get a predicted location and then a predicted sensation. or if this, action actually gets executed in the real world, then via sensor movement. then the actual, FT plus one, like the actual sensory feature at the next, point is going to change. this is how the kind of state variable, the kind of, the state representation is changing. and this can also, these changes can also then influence, for example, the object model. If, suddenly we decide, okay, actually it looks more like, This mug that I'm on, and so forth. So that's a simple way that actions are changing the overall state of the system.

and the transition model here is very simple. It's basically path integration or something like that.

but a more kind of complex action like persistent downward pressure on the stapler head or something like that could affect, the state of the object, and put it into, stapler depressed, mode or whatever. and here the kind of transition model is definitely going to be more complex. in order to capture that.

and yeah, later I'll eventually hopefully talk about what that, how we might represent, how we might capture a transition model that could do something like that.

And then lastly, the object state. for example, if the stapler is opened, or of course if the ID changes, can influence the transition model. we'd expect the transition model to be different, probably from different objects, at least in many respects, but also the state of the object might influence that. so basically how your actions are influenced in the world, and that could be as simple as when the stapler is open, if I'm on one location and I, go to another, I'd expect something different versus if it's closed.

but of course a more complex object as you interact with it, that might make new affordances available, When you close the stapler, that's when you can actually use the stapler. When you open the stapler, that's when you can load it. Etc. and I think here then the transition model is even more complex, because then suddenly, the whole kind of notion of space and all this kind of stuff becomes a lot more flexible.

but yeah, but I feel like it's all, like all of these things are kind of part of this, this possibility of kind of actions can happen. The system can change, and the state of the system can change, but different parts of the, this kind of state may be affected, and they can also influence one another. So it's not as simple as just there's this environment where we are moving through it, and then there's actions that we're taking.

it all yeah, feeds back.

Yeah, that was, that part.

Jeff, in terms of what you were saying, do you feel like it's fair that, if I understood you, it sounded like what you were saying was like along this line, this kind of example, that kind of actions are affecting the state of the object ID. Yeah, I guess so, yeah. Or like the object representation, the object's level state. Yeah, first of all, I like this diagram a lot. I think it, it's a good thought provoking picture. I don't think it's necessarily clear in my mind all the different things you mean by it, and I probably have different interpretations of it, but it's a good way to start thinking about it.

yeah, if you want to start brainstorming a bit, I can do that, but if you want to keep going on your presentation, I can, wait. Yeah, I could, I could just do the last bit and then hopefully maybe, like, why Yeah, then at least it's clear what I was thinking.

oh, just before I forget, it was really bothering me. So when I was labeling this, I wrote kind of feature detectives and feature location binding cells, the feature detect, what was the name of those cells that you said are very, they have a specific tuning response, which they they fire fairly consistently. And then so they could Basically, you can determine the selectivity of a microcolumn.

what was it? it's not bipolar cells. that's Oh, they are, but there's a They are a flavor of bipolar cells. and they go by the term, oh gosh. They're not a type of stellate cell or something like that? no, not the stellate cells. oh gosh, I'm forgetting words as they get older. But I've always had trouble with this word. I have a picture of what they look like in my head. They're really skinny with the dendrites and axons, very limited in size. And sometimes, some papers just call them bipolar cells. But I'll just, find distinction and, Okay, yeah, no, it's not that important. It was just, it was bugging me. I should know it. it's a word that not everyone uses. That's why it's, it's hard, to keep it in my mind. Yeah. If you want, Yeah, if you think of it. If you a reference to it, then just send me a link, or send me a message and I'll try to remember to send you a link to some of them. I'll do that, thanks.

what the actual name is not universally agreed upon. Okay, yeah.

Yeah, so then, of course the whole point in all this was to bring in, hierarchy, so then okay, yeah, how might Hierarchy play into this? So again, if you just humor me for a moment. So if you imagine yeah, okay, we want to use a stapler. So we move to the stapler in the office space, when we need to depress the stapler. So that's the change of the stapler state. If you imagine there's a stapler level representation of the object. And that involves moving to the stapler head and then applying downward pressure on the stapler head. So a change in the stapler head state.

maybe more complex one might be, make coffee, where again, it's beneficial to have hierarchy where you skip over or have these options that you pick amongst. and, once these are fulfilled, once the, A machine has water once that state is achieved then you can then move on to turn on the coffee machine. and again this is changing the machine state and then in order to do that you need to press the start button which is changing the button state.

oh, sorry, I meant to break down the slides. let me just quickly do that. Sorry, that's a lot of text. You can just tell us to ignore it.

yeah, I guess a lot of, hierarchical, action policy stuff that I could see was talking about, What do you call it? you send down these actions and then those produce sub actions and stuff like that. But I wondered whether, to a certain degree, we also want, more kind of top down feedback that's passing a goal state. and so what this, might then mean is, basically the transition model would then be conditioned not just on our current state and, the kind of current action, but, what do you call it? oh, sorry, this should be, yeah, so this should be our current action is conditioned, not just in our current state, but also on our goal state. and basically this would continue until, the transition, sorry, until the state, of the, the system achieves the kind of the goal state. So the transition model is not the same as the policy, right?

yeah. yeah, sorry. So that's why I should have a separate, line for this. Okay. So it would not be output. The output would not be the next state. It would be the next action and it would be independent of the transition model. It would just be, yeah, using The transition model is one of its conditioning elements.

Yeah, No, I, I was getting a bit tired maybe when I wrote this one.

but yeah, but basically, but they then the thought is that, okay, so you pass this goal state to a learning module and then it's basically going to, kind of transition through this by initiating the, whatever policy it has. until it reaches, that goal state.

and that final state, it depends on how it's specified, but it might, involve, probably most likely something about the kind of representation at the L2, L3 layer, the kind of object level representation, but achieving it might involve some kind of interim location, XT, and with the example of the stapler, so maybe, you're passing in this goal state, okay, we want the stapler to be depressed. So then, you, this learning module knows that in order to get it into that state Is that what the black line represents? It's the desired goal state? Is that what that is? That's the top down feedback from the higher level learning module. You're now drawing it, you're now drawing it like a neuroscientist after our last meeting, so Laughter.

Yeah. And then this is meant to be a apical, Okay. But, yeah, then this learning module because of its, transition model of the stapler, it maybe knows that, okay, I need to get to the location, which is the head of the stapler. And then maybe at that point it needs to, pass on, the kind of responsibility to an even lower level learning module, or maybe at that point it can finish things, but if it needs to pass, down things, maybe then it passes, okay, to the stapler head model, you now need to be, you need to be in a state where you have a force, Going down on you, something like that. And then this, the staplehead model, then takes over. and I guess then, yeah, if the learning model is basically making progress in kind of achieving its, its target state. then it just sends motor outputs from L5 and those get executed. But if it's, for some reason, because of its kind of insufficient model of the world or, or because, of the level of hierarchy it's at, if it needs something else to be achieved, then it basically sends a target state to, another learning module. And, so that's where perhaps like making coffee, you'd progress through this and then, okay, you've achieved filling the. machine with coffee, and then you go to the next step, but now you need the machine to have water. So you're going to pass, a goal state to other learning models, say, okay, the machine needs, water, and offload that. Yeah. I like the notion of sending goal states. instead of actions, it's similar to what we have right now, only that our goal state right now can only capture the location of the sensor. And then basically we would add more to the state. So like the state of the world is the location of the sensor, but also location of and pose of the object and state of the object. And then basically you could say, yeah, I want to make coffee. I'm gonna tell you, I, I need the coffee pot in this state. So coffee pot at this location pose, I need the hand right at this location and pose I need. And then those can say, okay, to put the coffee pot there, I need the hand to move in this location. and then basically those goal states would be translated into action sequences somewhere else. I guess my main question would be then like, where. Would the goal state turn into actions? And where does the policy actually live? Is it also in the column or is it then somewhere else? Yeah. No, it's a great question. Yeah. Yeah. So that's, yeah, exactly. And, yeah, so basically, so it can also L5 in this is, because I looked, I think I saw something that L5 can send top down, connections. so the kind of, That would be where it's sending goal states and then, but it also, has this policy that can just generate actions. so the policy, yeah, would also exist, here, but it just Just a quick, I have a lot of thoughts, but I'm not aware that, L5 has, hierarchical projections down. I think it's all layer 6. But I could be wrong, I could be wrong. Okay, I'll need to double check. there's so much literature here that it's easy.

But I, if all this makes sense to me in some sense, I'm hesitant to interrupt you because I want you to get through it. But the way I'm thinking about this is imagine a single learning module has to be able to do an action policy that and has to be able to put the stapler some, it has to be able to take a model and put it into some state, because every, if it's going to happen in the cortex, it's going to happen in every learning module, and we can then add hierarchy on top of that. And one way to think about it is that, I agree, the top down, if someone says, somebody has to provide this learning module and says, we wanted to depress the stapler, and then this learning module would say, okay, you're telling me to get into some state of the stapler, and, and I know it's current state. and then it can say, where is my finger, right now? And then I, then generate a behavior based on my current location of my fingers. It's like wherever my finger is right now, I'm going to generate the behavior, correct? So to my mind, it's L6 represents where your finger is. Somebody's trying to, tell you what state you want to be in, and the combination of where my finger is and the model state, then generates an action in layer five, which says, okay, this is the specific action I need to implement to move my finger to do that goal and move the model to that state. So it's an interaction between This is what I was saying earlier, layer six in my mind is not really part of the model, it's just, it's the contemporaneous position of your sensor or your body, and and you're saying, if you want me to put this in some state, I can calculate what I need to do to get it into that state, given my current position, and that's going to be calculated in layer five. That's how I viewed it. But again, imagine now, imagine I have, ten fingers on the stapler, and I'm holding it in both hands. And I have ten learning modules that are all modeling the stapler, they all have a model of the stapler, they all know the same, state of the stapler, they all agree we're in this state. And then I put a top down request to the whole layer, meaning all ten learning modules. And they all could calculate how to move their specific finger to achieve the goal. And, one of them is going to do it, but I could do another one. I could, my point is that there are now ten fingers. There's ten different places, ways I can move to push the stapler down. And each one can calculate their own specific behavior that they need to do. everything, I can do it, I can do it, and so we can't have them all do it, they have to somehow pick one, I don't know how that happens, but my point is, again, I can have, I have lots of models of the stapler, and I provide this goal to all of them, and, one of them has to actually implement the behavior that's required in that simple example, but they all could, it's yeah, I could do it, you could do it, I could do it, they can do it, but I'll do it, type of thing.

I'm just, doing, I'm just saying words how I'm interpreting what you're saying, Yeah, no, I, yeah, I think that's exactly, it, and, yeah, and, yeah, as you're pointing out, Viviane, obviously, yeah, we do need to also do the action at some point, and yeah, clearly there's like a balance between what you call it, Passing the kind of task onward to someone else versus doing it yourself a little like I guess middle management in a company or something like there's only so many times you can subdivide a task before someone actually has to do the work or whatever and you know maybe pressing a stapler isn't a great example because yeah once we move to the target location then you know maybe we can just generate the action or whatever at that point. I think one thing that's, yeah, clearly important here that I'm not, showing, but, yeah, maybe, next time we'll, can try and dig into is, we'll also have, models for, complex, motor systems, or whatever, on a hand, whatever, I know we've talked about that before, and clearly that's involved as well, in terms of, actually executing the, action, but, but, yeah, but so just to yeah, why, not just send actions? yeah, why send kind of goal states as well? I think yeah, consistent with, what you guys are thinking, yeah, basically this is what It's just easier for the higher level learning module to know nothing about the lower level learning module. It basically just has to say, get, can you, can someone get this for me? this is what's, needed.

and separates out knowledge of sort of necessary states, so that the coffee machine button is pressed from the actions that can achieve it. often, talk about how you can press stuff with an elbow or whatever, that's independent of, the necessary state.

it also feels, yeah, this would be easier to represent when we've satisfied the top down directive, because then basically the current state will equal the target state, and that's, if that representation is L2, L3, then that's being sent forward, so then we know immediately whether, we were successful or not, whereas if you send an action, how do you really know, whether. what you wanted to happen has actually, I'm not sure actions are sent down at all. my assumption walking in would be that no actions sent down. It's just desired states and But what about, the motor output from L5 to that's a little weird. it's pretty simple. Imagine I have a two level hierarchy, and the top one says, we want to depress the staplers, and then the lower level says, okay, one of us will do it, and we'll generate the behaviors. Okay. what are the, motor outputs of the higher level region doing, how are they expressed? the way that Sherman and Guillory write about it is that those layer 5 cells just project subcortically and, and they do something. Here's, one possibility, and stop me if you're not following this.

The they've argued that everywhere they look, and they make that distinction, they don't say, they say we haven't looked everywhere, but everywhere you look, you've seen these layer 5 cells that project subcortically. That doesn't mean they all project to the same place subcortically. for example, the motor system we have in our bodies is outside of the cortex, is already hierarchically arranged. We have, nerve centers in our spinal cord, we have them in our brainstem, then there's the cerebellum, and there's, there's a series of, stuff going on. It's in fact, many of our ingrained behaviors are already hierarchical in some sense, or they're complex. And so a level two region may be saying not move the finger. it may be saying something more higher level, like telling the whole motor system below, I want that the lower level motor system is already maybe capable of interpreting some of these high level commands. you know what I'm saying? It's it's not like layer 5 and region 2, high regions and lower regions are projecting to the same cells. They may be projecting to a, sort of a complementing hierarchical motor system that already exists in the, old brain. And, and so all they're trying to do is saying, if I can find the correlation between my output and some other system down below, I'll do it. a higher level region learning module, layer 5, may project some place to the, in the basal ganglia, or something like that, and the lower level one just projects right to the spinal cord. that kind of thing. And, yeah, and they're all subcortical projections? does any of it go to, motor cortex or anything like that? alright, but motor cortex is in the cortex, so I'm not aware of layer 5. Again, it's not, I'm not aware of anyone saying layer 5 cells project, I'm not even sure I've read anything other than what Shulman and Gillibrand have written about this. So they've written a lot, but I'm not sure who else has written about it. But they say those layer 5 cells, the ones that we're talking about, the intrinsically ones, the bigger ones, they say they all do the following thing. They, have an axon, it splits, one part of the axon goes out of the cortex, someplace in the old brain, and that's, quote, related to behavior, and the other split goes to, is passed to the next, is passed back up in the hierarchy to the region above through the thalamus. Okay, but there's no indication that those particular layer 5 cells, the ones that project subcortically, go anywhere else. There are the other layer 5 cells which seem to be voting. Those cells do not project subcortically, but they do project to other layer 5a cells elsewhere in the cortex, broadly. So there seems to be a voting layer going on there, and then a Motor output layer that only exits a cortex or passes up the hierarchy. That's it. So that's what they've written. I don't know if it's true, but that's what they've written. That's helpful. Thanks. Yeah. But, I don't think, basically, it's consistent at some level with all the things we're talking about. We just have to refine some of this.

So anyway, I argue that there's no reason to pass down behaviors, while it certainly doesn't look like it's being, it doesn't look like it's happening, for whatever reason. So I thought about this, this idea, this bothered me in the past, and so I said, hey, it does, there is evidence that these layer 5 cells do project, To different levels of hierarchy in the old brain. it may be that the cortex is just trying to model your existing body's behaviors like walking and reflex reactions and so on that are old brain behaviors, and the cortex is mapping its hierarchy somewhat onto the. The hierarchy of the old brain so that a high level region of the cortex can give a sort of a high level state goal or behavior goal to, some part of the old brain and a lower region of the cortex would project into a lower behavioral section of the old brain. Yeah, does that make sense? Yeah, no, that makes sense. Yeah, so, we can, basically express all hierarchical behavior in the cortex as just target states and at the lowest level then like something in the motor cortex would, for example, say my target is that my hand is in this grasping state over there. And then that could be sent down subcortically and actually be turned into motor commands. But remember, layer five cells themselves never Actually innervate a muscle. And it's all, in some sense, I don't know, I'm just going to state that. I don't know how to interpret that. But there's never a point where a layer 5 cell says, contract this muscle, it says, the way I interpret that is, the layer 5 cells project to some subcortical motor section, and it associatively learns to say, when I'm in this state, the layer 5 cells are in this state, I want to associate, I want to be able to invoke these subcortical cells, when they're in this, if they're in an equivalent state. it's always, it's never directly driving musculature, it's always, Just trying to influence the old motor system, to do something. it's learning to pair its representations with something that already exists elsewhere. That's how I've interpreted it. I think that's a good way to, an interesting way to think about it. Yeah, that's a nice abstraction. That's what the evidence suggests. Cool, yeah, I'm glad you guys are on board with sending states down, because, yeah, I was worried maybe, that was going to be contentious, but no, it sounds like we're all on a similar I'm going further, Niels, I'm saying now that we're talking about it, I can't recall that I don't think anybody ever sends any motor actions, they, send in some sense, some The representation that says you should do something to get in the right state. Yeah, achieve this state, yeah.

but yeah, and so just, yeah, quickly wrapping up, so yeah, basically if you send that then it's very easy to tell when that's, if it's been achieved, and then you, a higher level learning module then knows, for example, it can continue, if it's carrying out a complex series of actions, then, it knows, okay, the coffee machine is on because. The state of the coffee machine is now what I was asking it to be in.

And then, similarly, a low level learning module can maybe quickly fire back if, a target state is completely outside of its possible state space. it's just going to say, look, I can't help, and if everything is saying that, then you would, more quickly realize, okay, I need to, at a higher level, try a different approach.

and then, yeah, and then this kind of, I think, connects to this idea of what if it's all kind of states? One of the kind of thoughts was, okay, can we unify the sensory and motor representations a bit more? And basically, if the, motor outputs in some sense are target states, then they're essentially analogous, to the, sensory or object representations that we're sending up through the hierarchy, the kind of L2, L3, representations. So that kind of means that they're not as, yeah, they're fundamentally linked.

It's still mysterious exactly. We don't really know yet how we represent object states. But, we do, we're saying, everything is in control. However it's represented, yeah. We don't know how it's done yet.

Okay. Yeah, I was just thinking about that, because, actually, if we put it this way now, the output states and the input target states will have the exact same format. it will be like, objects at poses and sensors at poses. And then, In the hierarchy, it's not that big of a problem because, yeah, you just get the target states from the higher level hierarchy and then you try to yeah, you send back at what state you're at, and, but then the main question that popped up in my head is, where does the targets, the highest level targets come from to begin with, like, where does it come from that I want to make a cup of coffee now? Yeah. So I was thinking, yeah, I was also thinking about this, it's almost philosophical, but I feel like, at a certain level, like the hypothalamus and stuff like that and the amygdala, like just like basic needs, like hunger and sleep and stuff. So my, my suggestion on how, at least how I think about this is like. The same way we thought about just object modeling in general, we start off by saying every single cortical column or learning module has to be able to model an object of some level of complexity, then have to model everything, but it has to be able to build a true sensorimotor model. And, and so we just focused on that one module, one learning module. How does it do that? I'm working on that still this week. And then, we said, oh, we'll introduce hierarchy when we need to. And the first time we really introduced hierarchy was with the, logo on the coffee cup. And initially, we spent a lot of time, I spent a lot of time trying to figure out how a single learning model could represent the logo on the coffee cup. And I eventually gave up, and I said it has to be hierarchy, and now we have a model of theory about how the hierarchy can do that. And so the same thing here with action policies, it seems to me we have to solve, first solve what a single learning model, how does it interact and change the behavior or the state of an object, all on its own. We have to understand how we can do that but it will be limited and then we'll have to introduce the hierarchy thing to overcome some of those limitations in the same way we have to overcome the limitations of a logo on a coffee cup. Now, we never, I don't think our goal as here at Numenta at this point in time is to worry about where does that top level goal come from, because you're right, Niels, it could be coming from, all kinds of behaviors, I'm hungry, I, I'm covered, having that, someone else's sandwich, I don't know what it is. I don't think we have to worry about that, we could, I'd be happy just to say, here's a stapler. Our goal is to, push it down and have a stable come out, or our goal is to open it up so we can put more stables in. I'd be really happy if we could do that. We don't have to worry about all this other out of the loop stuff that goes on at the moment.

I would avoid that. I'd just say, we could just make an artificial system where we just tell it the state you want it to be in and see if it can do it. That would be one thing I'd suggest. Yeah, I think that's an interesting point.

yeah, what, what is the kind of the breaking point of one learning module, where we need, where we actually need the hierarchy and, yeah, I already gave you a simple example of the multiple fingers on the cup where there is an example where I have to, I can impose a desired object state on a set of learning modules and, and, that requires a hierarchy because In the end, I have to decide which of the ten fingers is going to do it. It's not really, it's not really a hierarchical composition. oh, I think we're about to be up against our time here. It's not really a hierarchy. it's not really a, hierarchical composition.

we're not breaking down a series of behaviors. It's more like hierarchy is also necessary to do something basic that exists in one model, but I don't know how I'm gonna implement it. There's multiple ways of doing it. and that sounds a little bit to me, Yeah. Okay. Which finger in everybody is the gonna press stapler? That may be. Somehow similar to oh, I need to get around this obstacle, what behavior, who can get me around those obstacles, I don't know. can I, before we exit here, I thought this was great, by the way, I want to make one more point, if I can. You and I briefly exchanged a couple of messages about this, Niels.

there's some of the, behavior, like making a pot of coffee. Some of those behavioral states are like, move to the coffee pot, or move my hand to the coffee pot. Those behaviors are going to require the where pathways. And, I prefer not to deal with that at the moment. it would be better to say, okay, I'm touching this stapler with my hand, and, and the fingers are on it, and they're all working in the coordinates in the what space, where we are in the coffee cup, and how do I move my fingers to implement something. Whereas if I say, oh, my hand's at my side and my coffee cup's on the other side of the room, then that's going to be, we can't do that in a single learning module. We can't do that even just with a hierarchy of what modules, right? the evidence suggests we have to figure out how to navigate to some place first. We could, we should try to avoid tasks that are like that. We're just chatting. we should, my, I think we should try to avoid that at the moment, Yeah. that sounds good. Yeah, I actually, yeah, I was probably asking because I have reservations about the, what where pathway where, or I think it, it's obviously, not something I think that would be controversial at Numenta, but just like the whole what pathway just implies that there's no location information or reference frame type stuff in the ventral stream. and that's obviously not true, but the evidence, the early evidence of the what where pathways is, a person sees an object, if their what pathway is damaged, they can see the object, they can't identify it, but they can move their hand to it. And once their hand is on the object, then they know what the object is. so they know they have to move to some place, but they don't know what's there, but once they get there, and they touch it, they go, oh, I know what it is, and they can manipulate it. we don't, we want to skip the part, oh, how do I move my hand to the object? We just want to start right at it. My hand's on the object, now I want to manipulate it. and that doesn't require the where pathway as much. I think we have to stop here. I'm still slowly working my way through a bunch of papers, working on the sort of the, the detailed model of the column and how it could relate to all these things. So hopefully soon I shall have something to present on that. I'm just letting you know, I'm, excited to work on this problem, so I'm working on it, but slowly.