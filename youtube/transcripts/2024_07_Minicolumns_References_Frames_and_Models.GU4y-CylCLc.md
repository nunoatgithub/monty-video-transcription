So if it's all right with you, I thought what I'd do is I try to summarize, sort, put all the different ideas we had yesterday and make 'em seem more normal. Like consistent. And the more I thought about it, the big insight or the big odd thing that we came up with a long time ago, but we're also dancing around yesterday and maybe we mentioned it, is that the idea that there's two types of models in a cortical column there's a morphology model and then there are composite model. And this really confused the heta for a long time because clearly there are composite models. you can't argue there aren't composite models, things, both of other things. And we can rearrange 'em. But then there's things like circles or an egg shape or an oval that have no components. Yet you recognize 'em, right? They have a shape, but they don't really have any features of, it's I can't point to, oh, this thing is here, this location, this thing is this location. So that was always a really a problem for us. So if we just adopt that idea that there's two types of models, how, does the composite model relate to, like the feature model that we talked about? Same thing. Same thing. Okay. So if it's, got, if it's got, I think about it like in a primary sensory region, there's no sub components. Yeah. But there are things like color or texture. And if you consider those as components that they, do not related necessarily to morphology, but there's something at a location that could be read at this location. I don't That's what you're getting at. Yeah. I guess I'm asking, so if we have a model that's purely morphology, so it only, contains information about the point normals and relative locations. Yeah. Test multiple parts, like it's the model of a human with arms and legs. Would that be a composite model or morphology? I don't know. you can tell me. Let's go through this conversation. Okay. Yeah, go ahead. Yeah, because I feel like we have a approach to this that, but yeah, I definitely want to hear the, my initial concern with this, it feels like the morphology models are only at the bottom, Whereas I, I feel like we had a way of describing it where more morphology models can also be composition. I don't, I'm not, I wouldn't, I don't think of it that way. I just think, okay, here's a column and how would a column do both of these models? Now maybe I didn't think about oh, what would be four column? How would it be have morphology models? I don't know. Yeah, I think the, just the way we talked about it before was that there are two models, morphology and feature model instead of morphology and composite model. that's the same thing in my mind. Okay. You wanna call it feature model? That's fine. It a feature can be a component or it can be alright. One is like point normal at locations. And your other thing is something specific beyond point normal at that location. Yeah. Yeah. I would call you that composite. If you don't wanna call, if you don't wanna call it feature, fine, but we started using the word composite elsewhere, so I look you'll, see in a moment here, we think there's two types of infant to layer four, one to finds of point normal and the other fine. What is at that point, normal, maybe nothing. Yeah. Maybe it's just point normal. Yeah. or it could be a coffee, it could be a logo, or it could be red. in my mind, those are all on a continuum there. It doesn't, I don't make the distinction. If you don't like to use, if you want me change that to be a feature model, I'll say that, but the feature can be other objects. yeah. Yeah. I agree. Would you like me to call it a feature pro? It would be more clear for me. Okay. Sure. yeah. I don't know. I don't, how, about I just write it right there in slash feature? Yeah.

Conceptually the important, the idea is to keep the distinction, right? One is just there is a point, normal edge or something. It could be internal edge too. It doesn't have to be just external edge. And, but there's no other detail beyond that. And and the other one is, oh, there's some detail at some location that's different and there's a whole bunch of different detail that might be there. Features, yeah. Including whole other objects. Yeah. you have a concern about this? No, I, because I'm also remembering now how we talked about with the kind of, direct connections to higher levels of cortex. That would basically be what creates the morphology model in the higher level. maybe, I don't know. I haven't even, I wasn't even thinking about that yet. Okay. Yeah. That was at least one possibility we discussed where so yeah, with the person how I, so you would have a broader, more course point normal, over a bigger region of space. And then, Then you get this kind of more coarse grain morphology model. Can we, just, as opposed to like thinking about, so that gives you like the stickman but then you could also zoom in on an arm and then you would have the morphology model for an arm. I'm still, I'm confused about this. Okay. Can we put that, we can come back to it. Yeah. Yeah. Sounds good. I'm a little bit stuck in my own level. I remember last year we had all these pictures of stick figures. Yeah. And then they started dancing and then there's this like Elvis Presley dance that I really want to get to that, but yeah. Sorry. Can we just, I want to cover what we talked about yesterday, I think. Yeah. The idea we had yesterday and then I come back to this, I wrote it in there as hierarchy that, that's your co to tell me about this problem again. Okay. Sounds good. Yeah.

So in my mind it was this idea. There's these two different types of models. they're, really separate. You can have a morphology model without any features that would just like a circle or egg shape, an oval or maybe it's a cylinder. I don't know. I don't know what the extent of it we'll come back to in a moment. 'cause I have questions about the capacity morphology model. And then you can have specific versions of that. You have a morphology with specific, features and then it becomes unique. I could have a cylinder with handle or similar without a handle. I could have cylinder with the logo or someone without the logo.

to me this is the, this is I know we've talked about this fast, but I've forgotten about it came back to it, a clarity in my mind that's driving all the confusion that in the conversation we had yesterday. before I go on, is there any other thoughts about that before we go on? That there's these two types of models and then we can work from there? The issues. So the features would be, more like colors and the features I'm gonna describe in terms of the anatomy, the features come in to layer four and they has a large variety of what they could be. They're basically saying there's something at this location that's not a point normal. And that's also the thing that would explain, like recognizing the fruit phase because we have the morphology of the face, which is still a relative location of eyes and nose and mouth. Yeah. But then the features are just different. it's just different object id, those. The sub handle is still, I don't know yet. Okay. where the border is between morphology and features. It's a little hard to say at the moment. I think it's a, I think it's a, a fuzzy border. Take something like a word.

A word is composed of a set of letters. So it's clearly a composite feature based model. It's like these letters in this position or, but once we learn to read, we also turn, we, we create a morphology model for the word, and I can read a word without looking at the individual letters. I just recognize the shape. and you can fully, I can drop things out. As long as it's got the basic all, shape, I'll still see it reading through. I won't even notice if there's something missing, and you recognize it by shape. So you, there seems to be like some water, there's some sort of spread between these. it's not necessarily like holier.

so that's another issue.

I wanna get back to what the i, the idea from yesterday. Okay. So the idea from yesterday was, and maybe we had this before. I don't remember. We, so this requires, first of all requires two sort of separate reference frames. You have to have one that. Very unique for the unique optics, and you have one that's less unique that you're can use for the, morphology optic. but they're tied together and they're not independent. They have to be updated together. It requires two types of input to layer four. And we do see that in the codex. There's a, there's one would be DD to determine that point normal, and the other would be representing whatever that feature is at that point, if there's one. And so if you look at the inputs to a cortical column, there's an input to, basically that goes onto the, the dendrites of the, the distal dendrites of layer four cells. And we used to think oh, this would be driving, you'd have to drive the activity of the cells would be on the proximal den, but it doesn't look like those are on the distal dent. But we also see a layer, an input to layer below layer three, the border. And I've always believed, and I don't have, I, there is evidence for this, but I'd have to go back and look at it again. There are these cells I was mentioning yesterday, these bipolar cells, the specific ones are called double bouquet. I couldn't remember that saying that seemed to, be they, these cells very much, very skinny, very branchy dendrites to go down one mini column. They're, controlling a minicom of some sort. And so my basic, my guess out in what's going on here is that this input here is determining the point normal. The point normal is basically the edge orientation of these minicolumns. So these cells, these double bookcase cells are complex cells. They have a lot of synapses. I'm, the argument here is that the spatial pooler is actually occurring at the level of double bookcase cells. They are essentially saying, I'm a, my whole mini column is active, but these guys are doing this, the spatial pooler operation.

and that's one piece of it. so that was required sort of two types of, two types of features. essentially this would be like color and this would be point normal information. This would be, the, from the previous region, this would be called, logo and this would be like the edge of the coffee cup or something like that.

then we talked a lot about two different reference frames. It's quite from two different reference frames.

and, and that was a real challenge. We talked about how do you have two reference frames both simultaneously updated. We talked about two possible ways of doing that. one was to use phases that is the same set of cells at one phase at spar sales base, it's dense. We also talked about having, multiple sets of cells where, some of those are dense and then I'm below that. There were sparse versions of that. I'm just, I'm going with that one at the moment. Okay. this one's a little bit harder to understand. This one might match. I don't know. It may not be right, but that's the one I started going. Partly because you can now see, parallel construction here, which is nice. Doesn't mean it's right, but it's nice. In the lower layer, this would be, this is layer five, and this is layer six. I'd say say six A, this is layer four here. the input to the, there's a sensory input to the, to right at the bottom of layer five, right above. Just, it's very analogous to the one that's right here.

I don't know, but I'm gonna argue that there are double the K cells that control the upper regions and double K controls of lower regions. There's literature on this from a long time ago. I haven't looked at it in a long time, but there is, there's some knowledge about the connectivity of these double K cells. So I'm just going with the idea that the same thing is happening down here in some sense, that you have this input here, which is defining your set of grid cells, the classic grid cells, however they get created. there might be, like in the tank paper, there might be six guitar modules and each one is act, there's a cell active in each one is Ingram. And then the idea that below them, below the active ones, there was a more sparse representation. Somehow, we talked about different ways of doing that. One, each little slice or the, random SDR learned that was another way of doing it. The idea here is you got two sort of reputations. You'd find that the standard grid cells and then you'd be these very sparse representations, which would be difficult for most researchers to see. and then the idea here is you have, the two models are defined between the, black connections here are basically saying, Hey, based on this, point, normal and based on, the crude location of some sort, the, not the very, not very sparse location. I could define a model of a morphological model of an not. There's some issues with that, but that was the idea that we came up yesterday. And then there was another set of connections would be between the sparse representation here and the sparse representation here, which would be the model of Unique object, that has that in morphology, right? So same morphology, but now unique. So I can have a lot of unique versions of basic shapes, basic morphology. So that was a new idea yesterday that you'd have these two types of, our classic model between location and feature. It's really now there's two of them, and there's two rep, two reference frames and there's two sets of features type of things.

so that was pretty cool, I think. yeah, I really liked that. we talked about it before, like having the morphological feature model and how that could work in layer four and lower level three, layer three. But it's a nice idea how that could tie into, I. I didn't, I don't think we had that idea before. Yeah. I think this connectivity, that's a idea. I think, I don't know what you think, Niels, you went there yesterday. So in terms of the connectivity or Yeah, like that's how they could connect and that's why it's, but the moment just ask, don't ask yourself, does the neuroscience look like this? Does this make sense logically? Yeah. But yeah, because we were talking before about the, like two models and I think that definitely makes a lot of sense. Yeah. and, and I think what we were talking about before with the sparseness of the lower level, I think that's particularly important because like for example, even just like how we would implement in Monty, like we don't wanna just double the number of models we have of everything. Yeah. But because the morphological model is gonna do a lot, and then you could imagine the, kinda more feature model is just going to do like a few. A few points stored in it, basically. So yeah, you might move quite far before it even predicts anything. The feature model may have nothing, to say the feature model. Only something to say when you get to a feature. Yeah. And it would nicely, fulfill that requirement. Number two that we'll want to have to relearn, like you say, the whole morphology again, just because we have an object with the che, but different features on it. this is a very speculative idea here. it's theoretically driven, but really speculative. But the good news about it is that we can probably test it. there's probably literature out there that this makes very specific predictions, about the connectivity between double bouquet cells, and I know I have read at least one paper on that in the past. what would you look for there too? I would, I'd be looking for con this sort of separate connectivity here. There's a, what do those double B case cells project to? and where do, what's their distribution in the upper and lower layers? What do they look like? this, basically says there's these two separate types of connections. Be we, might typically classify people say, oh, it's between layer six and layer four. That's your classic view, right? this is not, oh, it's a little bit more subtle than that. It's these cells are different than these cells, and these cells are different than this cells. And there should be connectivity like this. and, that basically is a new idea. And I never searched for literature on that specifically. So we might see some papers which says, just say that this is impossible. It doesn't look like that. Or, people have te people have measured the, the receptive field, the, response properties of these cells. And there's some literature there. It's not a, it's not a big, but there's literature there. And so I, I'm just saying that. Wild as if, you're a neuroscience you say, oh, this is completely speculative. And it is speculative, but it's dramatically derived and not used to that. So there's a lot. If we believe this is true, we keep, I wouldn't have never thought this idea, this just two models of, five years ago, just crazy. But, we worked our way into that, believing that in That's true. Then there's is a mechanism that seems there aren't too many mechanisms that would work, and this is the only one we come up with is only one I can think of. So then there should be, then there, there should be a literature out there that supports it or invalidates it. so I'm just saying it's testable to some extent. It's not one of these things you say, oh yeah, it could work anyway. No, this is very specific predictions about these connectivity and even, these cells, the idea that you have grid cells, but then you have other cells that are sparse versions of the grid cells that in the grid cell literature you might find that too. there's all these. There's these, there's all kinds of grittiness cells, but this is a very sort of specific prediction that we're looking for this very sparse, connections or cells that these cells on a particular object would always become active in the same location on the object. So basically a play cells, yeah. Biology thing. Think about that. I don't know. They, and so then we don't need like a whole, our place cells are play cells more sparse than good cells. I don't know if like by definition, but just as in I think they are, if you only need to encode like a few locations, if you don't need to densely encode Yeah. Space, then it makes more sense to almost use the grid cell enc coding of space and get the path integration and everything. And then you just have these additional cells then code okay, there is a unique feature here or something. I think the idea that these, all these have, these look like place cells. Yeah. Yeah. Did you have any more thoughts in general on how we could get these sparse encodings of location? we talked to, we talked about the two methods yesterday. Yeah, I have no more thoughts about that. Okay. Alright. in some sense, if we could prove that it is, there is a sparse representation like this, then I could say, exactly how is it derived? Isn't that important? Just like exactly how grid cells are derived isn't that important. It's useful to know and just explore, the holes in the theory and, it'd be more important to know that these cells exist than exactly how they're derived. yeah, but maybe I don't, I didn't think about that further. I'm just, I was we, put so much crap on the board yesterday and I was all over the place and I couldn't remember things. and so I'm just clarifying what we said yesterday, I think. yeah. Yeah. And and this was the big insight at the end of the day was that these two different, I. Models could exist simultaneously with a denser and sparser, thing. And we had to come up with a way that when grid cells are updated, the proper, sparse version of the grid cells has to be up tested, do path integration. Two, I found that more comfortable just thinking about the two cells per layer type of thing. Or, but but maybe the other method you said, which is you could pick random would work too, but you'd have to learn it somehow, Yeah. That has always been a challenge. How do you upgrade, update a, a sparse SDR for location. But, if I thought of this as many each, e many different good cell modules and within on the cells moving based on path integration, then it would work. I, could spend more time thinking about that. It would be fun to say, oh, can we come up with a mechanism? How would it play out with those phase distributions and so on. But anyway, I didn't think at the moment, to me this was just like, yeah. And then, just then playing out like the examples we were talking about yesterday, the soda can versus the mug.

yeah, 'cause I guess just this whole thing about, like something like a handle, which is morphological.

how we kind of factor that in. Yeah. because like one of the first examples we talked about, it was oh, there's a cylindrical kind of morphology object. And then, and then it wasn't clear. Are we like transitioning now away from a morphology model when we are then on the handle? Or is it just this gets that back to the question that came up earlier. Yeah. I don't know. There seems to be a. lemme just bring up the first issue I had here, which is the capacity of the morphology model. How many things can you represent? Yeah. In the capacity of, and in this scheme, as I've shown here, it seems too limited. if you think about, if you, think of upper, the upper layers, every mini column is they typically classify, oh, these minicolumns represent edges that, orientation edges. And, so you may have literally, they may only be representing eight or 10 different orientations. You as you move through the columns, they cycle around. and if you take, there's evidence that's not right, but let's, if you accept that and there's very few, okay, I can say, okay, I have these point normals at this eight different locations. Now how many different loca, how many different locations could I use? And if I just use this sort of, one level grid cell for thing, there's not that many. If I take the tank paper and there's, six grid cell modules, but they're all going in sync. There is like maybe 30 different points on that grid cell. I don't know how many of the revenue a hundred doesn't really matter. Basically, I only have a hundred different locations that I can really represent by that. So I just took the pure dense es, the grid cell model and the pure dense. I don't think upper layers is problem. The problem is the lower layers, but I just took grid cells and I say, oh, how many point, how many different locations can I have? It's pretty limited. So I can't really learn too many morphological models if I stick to the dense grid cell. Yeah. what could be a way to do it is, so if we think of these grid cells implementing, transition models, that's like a term we talked about two years ago I think. So basically saying the grid cells, so how you normally you think of grid cells as encoding 2D planes, right? But then let's say. In the neocortex, we can have grid cell modules that can encode like 3D shapes. Okay. Or like a cylinder. And they encode that if you go 360 degrees around the cylinder, you're back at the original location and stuff like that. that, that would mean we would need custom grid cell modules for the different basic shapes. So we wouldn't wanna have, why would really cost the good on trees? Basically, because they need to know, like the basic properties of geometrical objects. So you would have one, like for a cylinder that knows that you go around, that you'd have unfor sphere and they're not that many basic shapes.

just to, so you know, the path integration properties on these three dimensional objects. But then I think it, it would work because, this is like high school art knowledge, so don't quote me on that. But if we say the cup is if you draw things, you often compose them into like these basic shapes. And then a cup would basically be a cylinder. Plus like a half sphere. Like a Yeah. Or a Taurus. Yeah. so we would have one grid cell module that models how you would move along a cylinder and on the, handle you would use like the Thors grid cell module, to know how you would move through this kind of 3D space. So I wouldn't wanna, wouldn't wanna have multiple grid cell modules for this, but could the same grid cell module, what you're suggesting, the grid cell aren't just encoding your location? They're also, there's, they're somehow tuned to the action because they would need to know some special path integrating properties that you only have on those shapes. Like in 2D on a 2D plane, you wouldn't have the fact that if you exit the 2D plane on one side, you enter it on the other side again. But, it does do that, but just put it, I guess it's like. I guess you could model it all of these with a 3D, just like a 3D coordinate, good cell module. But then it would be a separate thing to know like what actions are valid on a particular object. So like you would still get path integration if you just had the cylinder model. Yeah. So in a generic 3D by the way. Yeah. I guess the alternative would be like what, I think that's what you're saying that to have just the 3D grid model. Yeah. That models 3D space. And I guess I'm just saying maybe it's not a three, not instead this issue, I, think this issue you come up with, it also exists in 2D space. a rat will follow, there's these mazes and they go up one arm and they go down another arm and they come around and they come back to the beginning again. They often, they show that they model those are like separate spaces, but whatever. But the point is they don't really know they're back in the same spot unless they observe the same, Imagine that they done. Yeah. Thought it's like, one of the key things, but only c integration only works within a well, just that it's noisy. It doesn't really work long distances like that. Oh, okay. Yeah. Imagine yourself, imagine you're walking in a big house in a, a hundred rooms and you're wandering around and you went through one room and you come back. You might be surprised, oh, I was in this room already. I, you didn't say, oh, I'm expecting to see the, that particular bedroom again, because you really lose track. you don't really, so you're like, you, build your map and, within some local area, the path integration works. But as you start going further and further field, you really don't, it doesn't work too well. And you, could come back and you might be surprised you're in the same room, or you might be in a different room. you wouldn't be certain where you are, at that point. But maybe that's more of a scale issue that they don't work at that large of a scale. Like with 3D objects, you usually don't have them. My intuition in and from the literature I would have is that, that these idea that you come back to the same place again, that the grid cells wouldn't know that necessarily in this sort of, in this sort of three dimensional circling topic. I ideally if you are, if you're receiving, like updated signals when you get back to the same location, the same grid cells are active. But it's just, if you're relying entirely on like path intubation on dead reckoning, yeah. Then that's not guaranteed. But if, you're going around the mug or the house or whatever and you're getting updated information about which room you're in, it's, this is really has to do with the fact, what do we do about edge wraparound? That's the only issue here. The only issue is like you run off the one edge. You have to know you're coming back to the same edge on the other side. You don't seem to get that in a two dimensional plane, even though I'm arguing it does happen. 'cause you don't really know where you are. so this is okay, yes. Does the grid cells know in advance that they're coming back to the same location and through path integrations? My intuition says they would know that. You'd have to learn that. You would.

it's too much to ask them. In fact, all the evidence we have for grid cells so far is that they really do best in two dimensions. They don't really work well in three dimensions. I remember the bat papers. We read those. Yeah. But that's why I'm suggesting that we don't have this approach where the grid cells are just general three, 3D modeling units. But instead we have this approach where it's more like a 2D plane with the added knowledge that we end up. So I would say the, I would, my intuition would be is a 2D plane without that knowledge that you've wrapped around and that you discovered, that you've wrapped around and you say, oh, I'm back to where I am again. yeah. But I guess so if we, for example, have a cube. The grid cell module would model more like a space like this, like how you fold a cube out of paper. Yeah. And then, but that's, only one folding. Like this way, that's another, it's only one folding. There's other foldings where, you know, you might not, you might wrap around the square on the bottom there. You know what I'm saying? What, no, like what happens? I go this way. this one has to go someplace too. And this one has to go someplace too. Yeah. So that would be the extra knowledge that the grid side would have to encode, basically. Okay. so this is, that's what, for example, delete George's, where it explored, but he, took essentially like HDM type stuff and then extended it to like just learning sequences where you learn every transition possible, like you visit every edge.

And, so that's one way to learn it. But then you're not getting the benefit of good sales, which just out of the box, generalized to novel, path. Yeah. Yeah. But I, wouldn't want, want them to learn this like for discrete transitions, but like continuous space. And basically what I'm saying is there are not that many basic objects. So we wouldn't learn the, have to learn this for all the possible objects in the world. We would just learn this for a sphere. so I think that the system has to make no assumptions about what the basic shapes and objects it's gonna learn are. And it just has to do with any, I personally, I think this is, I, don't think this is the way it's gonna go. This is, there's too many weirdnesses about this and, grid cells don't seem to do this. And we also have to match it on the anatomy, right? I don't have, but didn't you also say that they, don't work very well in 3D space, right? They don't, it's almost like you're doing 2D and then 2D, and somehow you're patch 'em together. Yeah, that's what I thought that but No, but it's take, a die, right? Like a, duck, a die from playing games. most people have no idea what on adjacent surface, what the number will be, but there's a pattern to it. They're all the same. and no one learns it. They don't learn to do what you just said. It's almost like you see a face and then I see another face and I see another face. But I think it might be an, a different issue there. 'cause with the dice, like how you move around it and where you get like morph. If you just look at the morphology, not at the features, you know how you would move around the dice. And then if you go around four edges, you're back in the original space. I think the issue you're describing is that you don't learn the features on the different edges and which edge they're on, but that might be because it's a symmetrical object and you don't really know how to anchor them on the dice. So if you have a, if I have unique, if I have a unique representation in the morphology model for every, for every location on that, why, path integration should tell me what's on the next phase. But maybe you don't have a unique representation here because it's symmetrical, so you don't know which phase but they just argued there that I did with the unfolded. Even the features don't tell you the orientation of the dice. Like you need to look at two faces to know the orientation of the dice, because if you have won't even then you won't know what's on the other faces. You just don't learn it. you just don't learn the transitions from the face of a cube. But if you have the coffee cup, which is not as symmetrical, when I started the Momento logo, I know, like I could take a blank coffee cup and put them back on there like they're on this one. Like I, I would know how the logos are placed on this cup. Yeah. I didn't follow that argument, but, like I would be able to draw the same feature structure on this cup, from memory because it is not symmetrical. I know the handle is here. But there's top and bottom of the cup. again, yeah, I've always been really surprised how, again, like you can't really, if I asked you to draw the cup, you can draw it like that. But if I asked you to draw to some angle like this with the letters, it's really hard to do it. You just can't recall that. It's like you don't have a map of what it looks like when it's an angle like this. You only have a map from plane reviews. and that's just seems to be an observation. I'm gonna go back. I still think there's a capacity issue here. E even if you're right, Viviane, that there's different models for different shapes, which I'm lo to accept the moment. I don't how that gets represented up here. I just don't know how it gets, where are they? How many are there? who decides what they are?

the world could be composed of, one world could be composed of one set of shapes, and another world could be composed of another set of shapes and who knows? yeah. One kind of related thing that we discussed once was like having almost like a view sphere where it's like good cells on like generic 2D sphere surface. And then you're always like looking in and projecting onto that. So it's kinda like this, but it just means that you don't need different ones for different objects. yeah. and that again would yeah, maybe fit with the dice. if you just can't really orient it, then your views spear is just gonna be a bit of a mess in terms of which specific features on which face and stuff. But I remember there were some limitations we felt with that. So I would much prefer that we make no assumptions about the basic shapes of the world and the system learns them, whatever they are. And it's gonna have some capacity limit and then it's, and not, and you'll learn whatever it can from the statistics of the world. yeah, we might, I'm not saying you were born with these, you might still learn about then it's like it takes you, you don't really learn nuance after a certain point or it takes really, I guess one question is, can I do this with the same set of cells and learn all these different morphologies up to some limit? And I think what you're arguing is we have different sets of cells with different morphologies and that's the part I have trouble believing. it seems like you have one set of cells that are gonna learn some morphologies or some limit to them, but there's no assumption about how many and what the different shapes are. yeah, I think you could reuse a lot of the cells. You would just need some kind of modulating cells that tell you to restart at one point, once you go distance. But why did we go that part? we got a system right now that could say, okay, the problem with this one is limited, but in theory it could learn any morphology. it is limited. so to me the, to me the issue is capacity, not, like differentiation of shapes. apologies. It's really a capacity issue. if I just take grid cells on their face value and there's no evidence that there's different sets of grid cells for 3D or for cylinders or whatever. As far as I'm concerned, as far as I know, I mean it's just, this phenomenon could be explained in different ways, but just one thing is that it's very difficult for you to learn a totally new shape. if you have a cloth that's wrinkled up and let's say that's a static object now it's like a weird. Abstract or statue. Is the department, is it a complex shape or is the department's a novel shape? It's a novel shape. Okay. So a folded a piece of cloth is a very complex shape. yeah, but I, a simple shape. I feel like there's no simple shape that we don't already. Oh, I could models, I could come up with one. it's interesting. I always find it interesting that egg shape is a shape we know. But if I was never exposed to eggs, I wouldn't probably, the head is egg shape, Do you know the grebel objects? This is yeah. This is like a, in some psych experiments, but it is, it's like these novel 3D objects that people have to learn. Yeah. even a two dimensional object, they can learn. They have, yeah, they can. Okay. And they definitely learn better than machines, but they all come from generic objects, yeah. So now here's a, I just drew a two dimensional shape, right? I don't know that. I don't recognize that. But I think if I studied it for a little bit, I would recognize it. It would be all of a sudden I can instantly grab, after I've seen it for a few times, I practice it though, it become second nature to me. Maybe this is the way, some face mask look or bicycle seat or something. I don't know. My point is, I think it's easy to come up with novel shapes that we don't necessarily already have some sort of memory for. even I gave the example earlier about words, right? You, kind of Abhi a lot of words. You just recognize 'em by their shape. And I think it's well documented. But, I check on that again, but it's clear to me I do that I don't, that's why. And so that are new shapes that you didn't, weren't born with. They're just the shape of a word. and as long as you got really close to that shape, you'll recognize it as that word. So I, don't think there's some pre preordained amount of these things. It feels to me like the morphological objects are continually learned. It's like everything else and there's a limited capacity, but. My problem here is that if I just do with the dragger I did right here, the capacity is way too limited. Yeah. it doesn't seem right to me. but just to be clear, like that's not really a new issue, is it? if I, this me, this mapping between these layer and this layer that's new today or yesterday. Yeah. But the, fact that grid cell modules aren't really sparse enough for No, that's not a, that's not a reference rate. most people, most people don't think of 'em as, as representing unique space. Yeah. they just, they don't think about the idea of a unique location. that I thought wasn't that Eli Feet, at least that was like she had eight. I thought that was what inspired men's work. It wasn't. Or you get like a, that was at least, okay, that was one. We have lots of grid cell molecules. Yeah. And then it gives a single unique location, right? And so we've adopted that here again. Yeah. by saying of these extra cells below, it's, it, it is just, we adopted that idea. It just, we couldn't get the map to the anate correctly in any way. and so this is a variation on that could be mapped to Anate, right? Because there are not enough GR cell modules, right? If, I, as in the tank paper, oh, there's six brizo modules and they all seem to be moving together. they're not mapping independently. So really, even though you have six, that's, it's really, I have one. And, and then where are all the others? I need 20 of those. And they all have to anchor differently. I couldn't have any evidence for that. So this is better because at least here, they're very sparse and you wouldn't, expect, you wouldn't find grid cells. You'd be more like plate cells, like you said. they, you wouldn't find, they don't look like these lowest green cells don't look like grid cells. If you probe 'em, they just don't look up it. This maybe, just finding this to get background. You could go through what we talked about yesterday, why these are sparser again, why they sparser or like how they would get sparse. How they would get sparser. Yeah.

Sure. I don't remember what you, I, the black ones would look like grid cells. The black ones look like grid cells. Yeah. And then you were here for the tank paper. We showed the little six modules. And then remember we talked about there were some missing ones every once in while the missing one. And also how you could scan above or And perhaps the missing ones, you might be capturing some sparsity just starting to get some sparsity below some level. so one idea was that, you have a grid. you have a grid cell module, one of the six, here's the cell acts like a grid cell. And underneath it are a bunch of other cells that are much sparser. Basically. They're like a minicom. And you'd, if, there were, like, imagine we, he had six grid cell modules and there was an active cell in each one. Yeah, one possibility was you go down a little bit and now maybe there's another layer of here that's very similar.

but the difference here is only, let's say, two of these modules are active. there's two cells, the others are not, active. So now you've, this is a, six, choose two is 15. So there's 15 combinations of ways you can choose pick two cells.

and that you might have if you just keep going down there could be like, you can think of these minicolumns here. There could be like a whole bunch of these layers each. If I have, 15 of these layers and each one, I'm picking just two of the, out of the six. So they, each little layer acts like a grid cell module with only two cells active. Then you've got, it would be like, this 15, it would be 15 to the 15th. Number of possible combinations of that. But each one of these layers, these cells would move around just like these cells would move around. They're not, anchoring would means which of the individual cells, which of the individual six modules get an active cell? That would be the anchoring process. so each of the 15 layers could anchor differently. I'd feel bit wonky. I have to admit that's a, that's got a weird idea. How would that work? But at least it builds on the idea that you might have sort of a minicom structure down here, which we do see. And, and you're picking a sparsity. But, but, actually when you, do the path integration, it's not like I'm randomly picking in each one. I'm actually, this active cell is gonna move and this active cell is gonna move. Yeah. so that was one idea. This, if it worked, this solves all the requirements. Viviane had another idea, which I never really understood. Sorry, which, which is somehow you have to start with the same basic premise. And then she thought oh yeah, but I could just consider this all big, but, and I could fix some sort of random sparseness down here. Something like that. Yeah. And I didn't see how that worked because it didn't do past integration on these guys. But maybe explain, I was basically saying, you or originally picked random sort of SDRs that are active when a specific state is, when the grid cell module is in a specific state, you associate that SDR with that state of the grid cell module. So then the grid cells still do path integration. So when they are back in that state, the same SDRs being This is the idea that you don't really wrap around. You just have to, you just have to remember, you'll have to recognize back in the same spot and then, says it's the same thing. Yeah. so they don't really do path integration in a sense, they just keep randomly changing until you come back to the same spot. Yeah. But then, and then, but then one thing is, we, use, I don't know how it would work with. I think it also works with, what you just explained, but that we have a different random SDR depending on the object that we're on, like that the location is specific to the object. that happens here too. Yeah. it would easier to do it here. Yeah. You just pick a random one. I just, think the path integration component of this example is wonky. You're not really doing path integration. I think what you're arguing is the path integration still happens in the Yeah, exactly. But then I guess the only thing is the capacity issue not still a problem? Because now you need to have, like the grid cells that are active need to be sufficiently, you need to activate the correct SD down there. Yeah. Basically, I think what you're saying is that this has to be activated. The correct one has to come from up above. Yeah. So in the beginning, if we don't know which object we on, it would activate a union of SDRs. Then that gets narrowed down. it throws away the whole path. Integration idea, just, yeah. Path integration would still be done by the grid sum. Yeah. But down here you have no path integration down here. It just randomly random patterns associated with the specific object. Yeah. But implicitly by learning, an association with the Right, it's a lot of learning. Yeah, that's right. Every point has to be learned. Every single point in that unique representation has to be learned. And so that to me is not really during path integration. It's like a romance path integration. It's okay, I'll just remember every location in the world and when I come back to the same location, I'll remember it's the same one. but I, but it means as I, if I move in some direction, I don't, if I don't have a, if I don't have this, if I pick a random thing here and I have a movement vector, how would I know what the next random thing is supposed to be if I don't have this feedback from above. I wouldn't, I can't predict the next proper location unless I have somebody, I don't think it would work. Yeah. I, think, now that I see it again, that your, proposals is probably better, I just think that problem we would also have with your proposal that we need the object information to know which as, which first, I think you would, need it to anchor, but once you've anchored path integration works, without any further ado, it just, it would just say once I've anchored, once I've imagine I have 2020 of these, or 15 of these little one layer grid cells, but these are sparser. once I pick which of these cells in each of these, which of the two of the six are active in each layer, then path integration works. I can just, if these two dots will move around and these two dots will move around and these two dots will move around and nobody has to, I will accurately predict to the next location, even if I've never been there. I would correct. I would always get to the same, same SDR. Yeah. So basically it would keep the two that are active in each layer within that square field, and then just move. they would, we would change, you're right. This the thing as you move this, the active cell moves changes, right? The active cell changes. Yeah. And it could even, it could move completely even within this whole six thing here, could, it could move over to the next one here and so on. these are artificial boundaries.

yeah, you'd have basically just, I would pick two cells. It would be, it would essentially say, and that way you don't always need the top down feedback. You don't, ever need it. It just needs the anchor. You just need the anchor. Yeah. So in the very beginning, it would still be like less sparse, more like a union of possible, I don't know what the very beginning would be, but you need the anchor. Yeah. If you did before you anchor, I don't know what happens, but. yeah, that would be, that's nice, right? You need an anchor. Yeah. Nice shape there. I'm trying to draw an angle. So I need to take like a 3D drawing course at some point. Practice drawing on three DI wanna go pick mine. Make last year drawing forks this year it's drawing the cubes. So it, I just wanted to mention, 'cause yeah, I feel like this reminded me a lot of, I know you've talked about a lot or I think some of the things before where the, you have the grid cells and then the columns, the minicolumns are the movement vector cells that yeah, that's, maybe, but, just, I don't know. That doesn't fit into this scheme, I don't think though. what I was just wondering was like, if we need these potentially enable the, like the general 2D grid cells. the top layer is like the gen two degrees cells. Yeah. There's the gen two degree cells. These are 1D vector cells, but I guess these could have two properties. One is there's a lot of them, so they could be sparsely active. but we, and then they could do path integration at least over local, coordinates, potentially, like over small distances. Why wouldn't they just work like any other grid cells? what we were talking about with they only kind of work in 1D Oh, I'm, I'm, not, I'm giving up on that one right now. The, okay. By the way, that, that thing I talked about, many comms you get of the cells that are all in different phases. Yeah. Other people have argued that actually there are different dendrites in different phases. Oh. So that you don't have to have different cells in different phases.

it's confusing. Okay. So in today's conversation, I've abandoned that idea from, but just put it off to the side. and so I'm not trying to accommodate that here. Okay. I guess I was just thinking, I don't know if this helps in that somehow, the active grid cells here are help re-anchor, these to, specific representation that was learned and then we do path integration over short distances here. no, but then, this is a different scenario. You, that path integration. this was my phase transition. Yeah. That's not what we're talking about here. When I move this, the, this cell would stop becoming active and this cell would become active and then this cell becomes to just like up here. Yeah. Yeah. So movements move. In this exam, this, proposal movements are just like grid cells. sure. They, move horizontally in their plane, if you will, vertically, don't they? The phase is vertical. We're not doing phase anymore. Remember? Take, but I'm trying to, okay. Example, all the cells are active at the same time, and then which one? the phase shifts up and down. Yeah. So it's not, yeah, I'm trying to go back to that. Okay. maybe we can incorporate the two. that would be really nice. I was just wondering if that helps in terms of, like thi this is a more sparse code, and then so like potentially has what we need to sparsely, reactivate or like uniquely encode a location. It can at least do some path integration, but maybe it's more limited over, over local spaces. But then maybe this is what helps us get into the right space, if that makes sense. Like, you always want to be, if this code is gonna be meaningful when you're at a particular location or an object, you need to reinstate the particular phases that are active. If I want, if I think about cells having phase as a derivative of the grid cells. Then I would need to anchor the phase. Yeah. That would, the anchor would be anchoring the phase, which then reflects itself in which grid cells are effective. Yeah. remember those phase cells are not grid cells or they predecessor the grid cells.

just bear that in mind. Yeah. just to clarify, in drawing in the top part, there would be a cell active in each of the six. That's what they observe. That's what tank sees. That's what grid cell people see, right? Yeah. They see, what they just remember. Imagine you don't see a single cell active. You see a single cell most active. And then in the direction movement the, a little bit less active then you know, a little bit further. it's like the, it's a phase procession. Remember like this fires and this fires, and this fire's fire and this fire. So you have a, you'd have a series of cells here that like on each phase they go bing, bing, and the center of it would be the middle cell. that's why the grid cell location of the ocean is fuzzy because, a anyway, it is complicated, right? But, but yeah, so I guess the general, I'm not suggesting the, problems, but just if it's somehow that this brings the, higher d path integration, which higher density? higher dimension. Higher, 'cause this is only in the far one, the, no, the, top, the grid cell. The true grid. I know why. Higher dimension, but, because it, sorry, because it's more than one dimension. we didn't agree if it was two or three. oh. oh. Okay. You're now, you're conflating the whole idea that good cells represent 3D. that, I'm just trying to be open to, I guess that possibly, but then, and then this is like local path integration.

which, plus sparse, location code moving it.

so yeah, some, something where okay, so for example, at a particular location, a kind of, these ones are active and then you move some distance and then another sparse code is active, but you're moving up. But that wouldn't be the case if in terms of the grid cells, would be the next one over the next mini column over become active.

that's a good point. Yeah. Yeah. Again, remember the actives. I think we should, here's what I'm, I keep wanting, but Okay. But yeah, if. Yeah, I, see what you're saying. If it's this, the same exact structure here where it's like the double bouquet cells, but if it's not like the double bouquet cells, then they might be different. This one's in that plane and this one's in this plane. So there's a whole series of questions about how grid cells get derived. and it's, as it's a very complex topic and so what I'd like to do is compartmentalize our issues compared to those issues. So my com comparation is saying we have a dense SDR and A sparse SDR, right? Yeah. And they both do path integration, not unlike the random thing, just like even in the sparse one moves, this, what I propose here is a variation of what we had in our paper that it affect, that, it's a variation of that, but one where I can say maybe it's compatible with the observations because we don't see lots of these good cell modules down here that are like anchoring differently, but we, this thing would make it work like that could fit the biology. It's the only way I can imagine how it fits the biology, but at least gives me a way to fit the biology to say, yes, I have a unique SDR and a less unique SDR, less unique representation location. So I'm just happy with that. I don't need to word the moment too much exactly how it comes about. Okay. Because I guess I have a possibility. Okay. I guess just to say, I guess one last thing. I guess the only concern was like, this needs all the hardware to actually enable the grid cells. I thought could, and it does, it was this idea, which I feel like you've discussed before is, more like, this is the hardware for the grid cells, but it can be reused. it, it basically does more than just enable grid cells. Like it also gives you, you then a substrate for, then you see as far as location. Then you'd see, as you go down to the cord, you'd see a bunch of cells that all affect the same grid cell, but at different phases. I'm not sure that's observed, would they, because this is the grid cell, this is the. Vector cells. So they're different types of cells. So it's not it's not like this has the same receptive field as these. the problem with those phase cells, someone else has to read them out.

alright. look, imagine, you're right. Imagine these are all active at different phase. Yeah. they could be read out up here because maybe all these activities coming up here and these guys detect the, set that are in phase. Yeah. The specific location. So that could work. The problem I see with that is then you would expect to see, alright. They wouldn't look like were, they would look like just, they would look like. They would look like just, cells that are going in a particular direction. they're motion sensitive cells. Yeah. Going upwards down. And that's actually not, going up and down, but meaning every cell in the minicom would all be like moving in some direction. Yeah. Different phases and every cell in the next moving column, minicom, yeah, so the, active cell would move up or down, but yeah, the direction that the cell would be different, not active. The in phase cell would be moving up. Yeah. Okay. There's, they're all active. Okay. Okay. So this is when it's the, so, interesting. If you look at the, going back to who, because, sorry. When we talk about phase, sometimes I think we're talking about if you imagine this as a, frequency band, then you could be the phase within that, or it's phase relative to what, I was talking about is that like a background, the idea was that you have a set of cells, they're all firing at the same frequency. They're all the, basically the frequency is based on the velocity of movement in a particular direction. So they all seem to be of, The firing rate represents the movement in that direction. But if you look at 'em, they'll have different phases over the beta cycle or the beta cycle, right? So they all look like they're all firing together, but their spikes are offset, 360. But can you also just have a sparse code where, one cell is active and that cell, like it's essentially 1D grid cell? Where, or no, because, that I, to go back in the grid cell theory, but then as you move up through this, that's not, yeah. if you're moving in this direction, then this cell would be active and then this cell, and then this cell, but no, but not the mechanism, how to do that. the point of the whole point of the thing is that the grid cells, the, a grid cell has to look at all these all the time. Yeah. And pick out which one is in phase with the, beta, gamma rate, whatever. What is it, beta.

Same to me.

okay, then, yeah, if we can't do this, then this is a problem because then this isn't a sparse code and then we need some sort of like temporal, right? Temporal. it is, the interesting thing is if all these cells are active, like I just said, and then, and you're taking all those cells and feeding them up here, in essence, it is a sparse code because it is very sparse. One of these cells is saying, I'm only gonna respond to everyone who's in phase. Everyone's in the same phase, part of the phase cycle, and therefore it looks like a sparse code. Yeah, even if we have a temporal code. Yeah. that's, that was the speculation that whoever wrote that, you need something like this to get to work. and it's, and there's a lot of questions that extremely through my head right now, but. If you look classically going back to Huble and diesel, they see cells that are, that they characterize the cells in the lower minicolumns as being directional sensitive. sensitive. And there are some papers that show that directional sensitivity is different than the directional sensitivity that c up here. So we consume, that's like a movement thing. So these kind of cells or all these cells that are firing in the same direction is what Albu observed. I don't know if they ever looked at the phase. They probably didn't.

but they did observe all the cells under me column down here seem to be, the ones they, the ones I report on, right? Remember, there's only cells they don't report on. The ones that don't fire or fire really rarely, they only reports are the ones that fire constantly. And there's a whole bunch of other cells that are not firing constantly. it's only maybe less, less than half the cells have these properties, but that's what they report. so there are some set of cells that have that look like this. they all look like the mo they all velocity sensitive to a particular direction. they didn't report on a phase shift, but if there was a phase shift that would pit somehow into the grid cell theories of how grid cells are dry. And then you could take then these cells. When I look at this projection up here, if I just look at the population activity, it doesn't look like a good cell at all. It looks like, it looks like a bunch of bunch cells like this. but if I were to, recognize which ones are in phase at any point in time, it would be very spar. It would be like, in some sense this is a different, this is in some sense these are almost like reading out, location. so imagine I'm one of these cells here and I have a, dendrite and coming in on that dendrite, is, or all these different, axons from here? they're nearby coming nearby. At any point. And then I, and I far, and I wanna learn, to recognize the pattern. I would learn the ones that are, the ones that are actually firing in phase. and the other ones would look like rephase. So I would say, oh, 4 absence to these. And so in some sense, this dendrite of this layer four cell is acting like a grid cell. It's saying, oh, I recognize a specific location.

this is a specific location. 'cause these cells are all peaking at the same time. And then, and therefore that's the only one I'm gonna pay attention to. yeah, a place cell. it's it would be like, but yeah, basically it responds to a specific location. It responds to a specific location. Although these cells themselves don't look like they're responding to any specific location. Yeah, they don't. And I guess the only thing is just then I feel like, but this is still important. This layer is still important because yeah, these can't do path integration over. Larger spaces very well because they're only, each one is only integrating in dimension. they, that they do work. if they were constantly active, they would do path integration because the, you can think of it as the, quote, the active cell is the one that's in phase with the beta cycle.

which ones in phase with the beta cycle tells you where you are in along some dimension in space. But that's what I mean. It is only local in that 'cause we talked about the example of if you start going in a different direction or even if you just did something SIM simple, not going in a square, you're not going to necessarily reactivate the same ones. But that's what this, So this kind of re-anchor these, these periodically, these have to be path integration.

Either these re-anchor them or I guess the features that you're observing. This is a very interesting idea.

I, don't, know if we, how much time we should spend on today, but it's a really interesting idea that I take the idea of the minicolumns representing this.

and, there, there are a whole bunch of series of problems here with this. this, these cells only make sense as you're moving in this direction. Yeah. And when you start moving another direction, these guys don't represent that even fit the opposite. If I go in the opposite direction, the phase doesn't shift down here. There's another set of cells that represent this direction. So you have to like, you have to anchor these cells phase wise and then start tracking the distance. So some sense, this mini column of cells here represents where you are. The phase represents where you are in along some distance in a particular direction, as long as you're moving in that way, or, moving one way. But if you're moving the other way, they don't represent it. The other guy represents where you are. So it's not, it's like a one di, it's a one dimensional grid cell, but only in one direction. In the other direction you have to have a different one dimensional grid cell. maybe these are really interesting topic. I'm really interested in pursuing this idea.

I'm just wondering, have we dealt with the basic issues here well enough? This is like a detail, how you would implement this. Now, it's a really interesting idea that, you would not see these as grid cells. You would see these as just direction oriented cells. and those all get passed up to here and these guy recognizes 'em as unique location codes. That's, I like that idea a lot actually, that, the actual detection of the in phase cells might occur up here. it doesn't explain then why do I have grid cells down here? 'cause the grid cells have to do it too, right? If there are real grid cells here visible. then, these don't look like grid cells. These look like, almost like place cells here. actually, because this is, this cell will become active at a specific place with a specific feature.

these would look like directional sensitive cells. And, I still have to be able to generate, why do we have grid cells then, if we have, if this is the predecessor to grid cells, these are not grid cells and these are predecessor required predecessor grid cells, we have to generate grid cells. And what's their purpose? I feel like they, I don't know if this would actually work, but it just feels like their purpose would be to it's like a recurrent connection with, I agree. That's my opinion too, that these are some, these are a way, this is like a temporary memory, Yeah. This is where we are right now, guys, so you know, you're gonna forget where you are. I'm gonna tell you where to invoke the right thing. it's there's some sort of like a holding place. It's not very high resolution. That's the interesting thing about it. It's it's not very high resolution. but, and I guess with, I don't know if you have predictions from L four going down to the minicolumns in the L six. if the, if I get that. that connection would, our theory is that, would bias that, that you would have to, now what would happen here, you have a cell that's active, which for specific location, with specific feature or a set of cells that represent that. And now you're projecting that down here so that, because this is gonna be a fairly core signal telling it, which, or like this is updating, predicting which one should be active. I'm just thinking There's gonna be a lot that, there's gonna be a lot that are predicted. So almost like the same, this is sparse population activity up here. Yeah. And this sparse population activity is sufficient to specify exactly which cells down here should be active, or in this case, which cells should be active in the right phase. It would be like, like it would be like, okay, this pattern says you, have to be in, in the peak of your face right now. Something like that. The other thing is, it a bit easier to simultaneously encode multiple hypotheses with this? I don't see how you could, because, because you could have different phase that are all can't, but you can't, like the two cells, you can't have two cells, either Is a cell is in phase with the beta or it's not? it's I don't see how you could have multiple cells that are So you're saying it's at the peak of the beta? Yeah. The idea is that there, there's, yes. they peak at the same time as the base frequency. Therefore, and the other ones don't. And therefore, yeah, anyone's looking at both of 'em. I suppose you could try to, you could try to have multiple cells here basically at the same base. Frequency is peaking at the same time. But I don't not, it seems like the mechanism would be like, no, we're gonna force these guys to be phase distributed. That's the, there's gonna be a mechanism that forces 'em to be phase distributed. these double case cells could do that beautifully. they send an axon down here and all you need is some sort of delay in, literally the physical delay going down could do it. it's interesting to, to know what the axonal propagation properties that these are. Maybe it's you, maybe they're slowing enough to do that. I don't know how you could do multiple hypotheses here.

if you had, if you had multiple po well, something would happen. What if I. If I had multiple SDRs here, I'd say I had two active SDRs here. I'm certain, I have some non a union up here, and I first bring that union down here. it's gonna try to, it's gonna be conflicting data. It's gonna say, some of these cells will be like, Hey, I should be, in sync with beta. And this guy says I should be in sync with beta. Who's gonna win?

if, these two SDRs appear were in different locations, meaning, on the morphological optic or different locations on the morphological optic, then you'd have different minicolumns. That would be you like this one, one SDR would be trying to set one set of minicolumns in phase, and the other would be trying to set the other set of minicolumns in phase. And then you would have two hypothesis. you'd have, two sets of minicolumns one, one set to, represent. The phase one SDR r and the other set that represent the phase or the other side. So you, in sometimes, if there were, if they weren't overlapping, they didn't have the exact same location and the morphological object, then you could have a union down there. If they're on the same location on the morphological object, then you'd have to pick one or the other. You mean the union would be activated by the layer four representation? Or imagine I just have narrowed down two SDRs because I thought the connection, the screen connection is an inhibitory connection and the layer four representation just narrows down the cells. Is that you're saying you thought that because that's what the, neuroscience says. Yeah. Yeah. Is that true? I thought that was also nomans papers before that, the layer four, six connections in, if I remember it right. Okay. I, wouldn't think that, let me see. And that may be true. Remember going back to the Thompson paper. She showed, yeah, in Thompson it says, strong reciprocal inhibitory connections between layer four six and layer three. Five. Three and five. Does that mean it's only in inhibit through large basket cells and double bouquet cells? I was, I don't think that could be true. And also at the same time, yeah, they might be excited. Excited too, because be the Thompson paper. I remember she showed those, the axon rising up here and forming lots and lots of synapses up here. Yeah. I think I, and you're saying those are inhibitory synapses? So here, that's not the figure.

yeah. Least in the mento code. it's exci. The, main inhibition is implied in the, yeah, that's one here. So you have this, these are all axons coming up here. And from all these synapses up here, big screen. Yeah. yeah. I was confused at first why the, layers underneath the dense grid cells that also follow the same grid cells. do you understand now? I'm leaning more towards the, the way, the, other one on the left there. Yeah. because I was also thinking it's more like community college, the way we have the layer four and layer three, the I, I, the important thing is that, and I had to convince that path integration has to occur automatically, even for sparse representation of space. it just has to, that's how you have to do it. You have to, if I, in some location on unique location, on object, then I move, I have to always go to the right, the same one. You can't learn. It has to just, that's the whole point of path. Yeah. anyway. it is interesting the idea of combining the, phase procession thing, on the left there, as an ultimate way of doing what I'm suggesting here, which I thought this kind of wonky, but it would work. yeah, this is a little bit better. because it was still also in code, the, way that bit cells move from one mini column to another, on the top of the dense. It is like this guy here would have to say where, what mini column would go to. Yes. And then, and then it would encode the path integration in the, in which cell gets, although I don't know how it, deterministically set. It's like I, what if I move this many column to this many column? 'cause I'm moving direction.

no, I'm moving in some direction, this, guy has to move. See, there's no longer a correlation between these cells and these minicolumns. That's no longer the case. and that this is not, these are not related to that. Actually at this point. they're totally separate. They're not related to the, and this one moves from here to here. that, that mini column doesn't move from there to there. No. that's, not the way it works. it looks like that the way you drew it, but would it really be like this? You have, you've got a set of, you just have a set of minicolumns here and, then you have a, layer of cells up here.

these are your grid cells. These are the grid cells in here, but these grid cells do not correspond to these minicolumns. They have no correlation at this point in time because when you move, if I'm moving in one direction, these minicolumns aren't changing. It's just that which cells in phase are changing. whereas these cells do change. these are moving around on the grid. These are just the in phase, cells moves up and down so they're not, driving this the same way that the okay here driving, they're driving this, but not in the calm basis.

I'm just helping, Mohamed do something. So I'll be here a couple minutes. Yeah.

yeah, I know it is a bit confusing, but think about it this way. I'm moving in some direction. The grid cells, the active grid cells change, but the all it means is, but if I'm moving that direction, there's a cell in this column. Mini column represents moving in that direction. The minicolumns is active though throughout the entire movement in that direction, and all I'm doing is moving the in phase cell up and down.

and because it's a velocity controlled oscillator, so it's, fir a little bit faster, so all the cells are going a little bit faster and therefore, which ones in phase of the beta cycle changes? So when you move in a particular direction, the phase of the only the, several minicolumns are active and it, and, but as you move the, which minicolumns active doesn't change. It just says, okay, I'm moving in this direction. Therefore, which of these cells become in phase with the beta? It changes. yeah, that's a, we assumed that the phases on the minicom. that's, a, that was that proposal. That's what Nielsen proposed. okay, I made this proposal. He's trying to bring it back in. and if we don't assume that, then, but then, they just don't do anything. Then they're back to then we're back to this. Then we're back to this idea where the minicolumns represent, Locations and a grid cells may, and so which Minicom comes active would, that's this idea. That's this idea. That's this idea. Then this is the way I understand.

yeah, I'm still a little hazy on the, phases and the, this, if this idea is you have a set of grid cells here, and then they, these minicolumns are, they are not, they are a predecessor of this, but the minicolumns are not tied together. This is a different set of cells. It doesn't tie to minicolumns.

it just, each of these, grid cells are, are detecting, patterns, in phase patterns down here. but it is independent of many columns. I was hoping it was as simple as we know, a dense layer and we're just trying to, incorporate context into them or, that's what this idea was, right? Yeah. I was hoping it was, so talking, talking to Ram here, there's, it is confusing and this idea we're pursuing you, which I really like a lot. if you have a layer of grid cells, those good cells are not the, they're not, they don't have any correspondence with the minicolumns beneath them. Like a good cell that's active doesn't mean the minicom below it's active. So that was confusing around, so it's okay. Yeah, because over here that's what I was suggesting. Yeah. This is where the, each cell up is like a good cell, but it's a sparse SDR. This is more, each minicom represents uben is active during a movement in a particular direction. So as you move in a particular direction, the active minicolumns don't change. yeah. All the changes that since the velocity of the cells, the frequency of which they're firing. Increases with speed, then which cell is in phased with the beta cycle, changes. That's all. It's, so I guess, yeah, it feels if this would actually work, if it does help, yeah, re-anchor the vector cells or like movement vector cells periodically, that would at least help with, because that always seemed like the main limitation of that approach was like, what was that? You couldn't path integrate over like slightly larger distances or I'm confused by that. I don't know if that's out of the below seems to me. I thought that's what we were talking about just before.

maybe you can ignore it then, but, yeah, no, I, what we're trying to look, we, it's go back to the big picture. The big picture is we have to have a sparse SDR and we also have a sparse SDR.

that represents location and we propose one mechanism for doing that. And that mechanism doesn't really have, doesn't really have capacity for the less sparse one. It doesn't have enough capacity. So I don't that we hadn't resolved actually capacity of the morphological model, but now we're talking about the details of how you would get the more sparse model. And this is an, a way of getting the more sparse model that might fit better with the biology. so take the grid cells outta the picture right now. If you could just assume that any point in time you can anchor which cell is in phased with the beta, with the beta cycle, then as you move, layer forward, read that out. Or we lead as we'd read it out as a sparse SDR, meaning it's the cells that are in phase right now with the Ben cycle or sparse.

so it solves that problem. it doesn't tell me at all how I get the. How do I get the, less sparse sdr r for the morphology?

I don't have an answer to that. As in Yeah. 'cause you're still concerned that one layer of grid cells isn't Right. Although it's less sparse. It's not sparse enough. and then I guess that's one layer of grid cells is not based sparse at all. It's just like one out of 30, a few morphological objects then less of an issue. Yeah. Like the, yeah.

I'm, debating why I bring up another topic related to this. Did you wanna have a look at that Thompson paper about the, activity or, I think we brought it up. It was just, you weren't gonna argue that it was negative. It was inhibitory connectivity. Ah, did she say that?

I am not sure if that can, oh, I think that's from the 2010 paper. Let me just, search for that one.

Oh, that is okay. That's it.

there's a general rule about critical connectivity, which important here, but is relevant here. Why, like to little bit, I just saw inhibit connections. So here it says, in, in contrast to the relatively weak excitatory input from layer four to six, there appears to be relatively strong inhibitory input. Oh, alright. So totally understand, the, The layer four input is on, on is like on distal dentin rights or something like that. And so if you look at the, soma and you see how much depolarization it has, it's like very little. so they say, oh, it's weak. Yeah. But they're not taking in account of the fact of integration and DDRs. You follow what I'm saying? Yeah. So they'll say, oh look, I measured the response. it's weak. It doesn't do anything but it, that's what it looks like until you get the right, until you get a dendritic spike and then it's not weak. Yeah. and the, I forget, are they mostly based on measurements or on anatomy? Because I think I also looked at the anatomy like connectivity. why would she say relatively strongly Relativity is, yeah. Here even then here's a, general rule of cortical connectivity. A general rule is the bone you have a, cell excitatory cell. It is projecting to some other place. There's a bunch of cells here, there's a bunch of cells here, and it's gonna make some connections here. The general rule is it will, there will always be almost, not, I say always, but very often, this projects to an inhibitory cell here, and it projects to excitatory cells at the same time. And the inhibitory cell, these synapses tend to be right on the soma or nexus Soma. They, that's why you might say they're strong. Yeah. you're not, recognizing a pattern. You're activating inhibition, global inhibition here. Yeah. and These guys, would you want these guys to respond very quickly and on almost any input? So it's not like this is detecting a pattern. It's oh, have immediate inhibitory response where these will be under distal dice and it'll be look like a weak response. Yeah. That, that might match with this sentence here. So layer four, double K cells with an adaptive firing pattern, bundle of fine descending unmated axons and more distal dendritic targets. Also in a layer six, this again, double of K cells with an adapting firing pattern. A bundle of fine descending un axons. And more, is that a sentence? Yeah, sentence. How do I par the sentence? It feels like there should be a comma somewhere, something. I'm, not parsing that sentence. I guess what I'm like the more distal than targets, For, like basically therefore double bouquet cells going towards the distal dendrites. In layer six is how I, read it. Let me read it again. Layer four double bouquet cells. Oh, this is the verb then. What's the verb? What's the verb? on my, I thought that's, no, that's, an attitude. I think it's also innovate, right? You, it's okay, the life four cells, phi abundance action and more distal also. Yeah. Also innovate layer six. It's just describing them. Yeah. That is a weird sentence. Okay. that, you know what that sentence is saying? That these cells right here go to the distal dendrites. They go down to layer six. Yeah. I'm not sure what cells in layer six they're talking about there, but they're saying, I see an axon from those cells coming down to the layer six that. Yeah, that's the exact data that I was saying you would wanna see. Yeah. yeah. That's nice. And also for what it's worth, you're talking about what is the delay in case that Yeah. Unmyelinated. Unmated and also fine. 'cause the, narrower they are, the slower they are.

that might be a delay. so those, I don't know what he says also because, 'cause I'm, saying they must, they need to a activate, these guys mean it says they also activate these guys. So they, these things here, we want the slowness here really. We want the slowness here because we wanna, we, here we want it here. alright, hang on. Yeah. The only odd thing is that this is under inhibitory inputs. two layer six. So I'm not sure if this are inhibitory. Okay. So these double case cells, yeah. these inhibitory connections are reciprocal layer five inter neurons projecting to layer three while layer six into neurons. Project layer four. I have careful, here's another weird thing about these. I think the double bouquet cells sometimes act as ex excitatory cells and sometimes act as inhibitory cells. They have this space to 'em. I don't know how they do this. What? Yes, it is. I forget, there's some, it was weird and there's some explanation for how they do this.

it just stuck in my head that they do that, somehow. Okay. okay.

I still wanna just get back to the, this issue of capacity, I think. Okay.

let's say that, let's say that these grid cells are not actually, the ones that project, they aren't the code we're gonna use for our morphology model. They just don't seem to have the information for morphology model. I, they're just too limited. what I need is something that's, that's less dense than the d less sparse than the sparse one.

I, I just, I need a little bit more capacity. You, how do I phrase that right?

maybe these grids hold are acting as we were just talking about a moment ago. They're really acting as this buffer to keep, track of where, which cells should be activating point in time type of thing.

I don't know. I'm, it's like I need a, I need a less dense, a less sparse and a sparse. I need a less sparse and a sparse with separate connectivity between the two. I was hoping it would be these double bouquet cells.

but they can't be just grid cells. Grid cells can't in their classic form, don't have enough capacity. Do we have to have the classic grid cells there? I just assume the grid cells exist because the whole idea that the whole idea is we didn't have anything how brain, how the cortex could represent location in a reference frame. And grid cells seemed to fit the bill well, at least the grid cell mechanism as a whole seemed to fit the bill. So we just assumed there'd be grid cells. and people see grid cells, so they're there. It seems to be, but they see them. it's only in the FMI study, but with abstract concepts. but it's, no, they also, there was a Chinese lab that saw 'em in B one and, that's one. And then other people dismissed it like, oh, I can't believe they're there.

Remember that's, there was the fact that this Chinese lab sent me their, papers said, look, we found him in, you want an S one? And and then when I talked to someone, was it, I forget who it was, the guy, he definitely had a meeting with Tim Barons. And was he the one who said he dismissed it? Yeah. I think he was arguing at least that, they weren't in the egocentric or, like object centric coordinates or something. But I dunno if they like actually tested that. I what I, a few people I've talked to about it, they did I can't be right. it doesn't make sense or whatever. And I'm like, oh, come, don't dismiss it. Try to work it out. I think there were at least some concerns about the statistical measures they used, but I don't know. I believe they're gonna be there. They're gonna be everywhere. yeah. I'm just saying if, we can, figure out of the different mechanism like these, like maybe just these 1D movement vectors. And then I think the data that there is for grid cells in the brain might be, we might be able to explain it in different ways. Like it's just moving through conceptual space that they measured there. Yeah.

I don't know why you wanna bring in conceptual space. That just confuses me when you Oh, I, I just mean if we find a solution that works without the classic grid cells, it's not like this experimental data. I do need a solution that has path integration of, I have two different pop the current hypothesis. We have to have two different populations of location cells.

one is, one would be for morphology and one would be for, feature based models. Just going back to that assumption, I'm just thinking more like one or two is like, how much of an issue is it if, Yeah. to your point, Vivi, but okay, we have the good cells, but let's assume those are just for stabilizing and like re-anchoring the Yeah, The vector cells. If we just use those vector cells for both of those, you mean the and down the lower layers, like the, yeah. The phase that's ultimately like the location code or whatever that reactivates both double bouquet cells for morphological features and then L four cells for, features. But it's just we, will, we'll in general, we might have more morph, like morphological features will be dense, whereas features will be sparse. what, but it's ultimately the same like substrate there. here's one way it could work.

I'm just thinking if that's actually an issue. is what's an issue just like reusing, like just thinking in terms of Monty, like I. We have a model, and then at every location there's a morphological feature. But at some locations there's a, a feature, a non morphological feature.

and then it would be like a separate thing whether we like Fuse. But wouldn't you, need a, we're assuming there's a unique location for unique objects, an object that's unique with the, and then there has to be a non-unique location for the morphology or less unique. I don't like when I'm on the coffee cup, I have a coffee cup with a logo without a logo. Two DentiCal coffee cups. Yeah. I wanna be able to use the standard representations that com. There's just be a common spatial representation for both of them, but a unique one that is only useful for the ones for logo on it. so I wanna have two of these sort of, is the, I don't wanna have to, I want all of the cylinders to have this, all the coffee cups have the same, is the, ID feedback enough for that, like from L three L two, I don't think so. I think I need two.

I need to have one be a sparse version of the other one.

and again, in, in terms of like how we might implement it in Monte, it's I. You have the same graph, but when you have a strong hypothesis that you're on a particular object, there will be unique location, unique features that you will predict at a location that you wouldn't have predicted otherwise. then you have to some way of knowing, okay, you have to some way of, here I'm on loca, on locations, on a cylinder, but after some way of knowing that I have a unique location on the cylinder with the logo, and I have to have, I just have to have both of those. I have to have both of those, it has to be unique. 'cause, another cylinder with a different logo or a different site feature needs to have a different spark. SDR for location. So like in Monte it would basically be like having, one morphological model of a cup, but then having multiple possible feature maps for that cup. So and. For each feature map, we still need to have a mapping of where the feature would be on the morphological model. So how do the locations, they have to track together, right? yeah. So when we are, let's say we, we know we are on a coffee mark, we're on, the body of the coffee mark. and then we have multiple possible feature maps that could be on there, no matter logo and a different logo. Yeah. Whatever. So in the feature map space, we need to have a mapping of which locations on the coffee cup morphology correspond to which location in the feature map space.

yeah. Although the beauty of the doing this with SDRs is that I just create a sparse version of the first one and it works. Yeah. And in the multi implementation, it would also be pretty straightforward to, to make the mapping or to make it work, but. Yeah. Yeah. I'm just thinking in terms of Sure. It's, okay. Sorry. Yeah. How would it maybe work at a neuro level? Rami, I feel like you're good about reminding about the top down feedback. So I'm just thinking about how, if you, once you have an ID here, whatever the IDs are there, like you have a unique location for the gen general morphological reference frame, like coffee cup. But, and then that has to be shared against all coffee cups, right? Yeah. And so let's say you're at the location that is associated with the logo on one of the coffee cups. That's going to send some positive signal here, but only with a sufficient prediction. Maybe it gets circulated like, Spars it based on a particular object. yeah, I was just thinking if you are, if you know what you're on that object, then together, that's enough. And then those ones would, and assuming you get the input Yeah. okay, let's say the Memento logo has, yeah. It has two logos. So you saw the logo on one side, you go around to the other side, you're, you're on this object. Okay. Because it, 'cause ultimately, it doesn't make sense to predict you're on the new mental logo unless you know you're on the new mental logo. I can, even if it's on one side, it can predict when I turn it, I'll see it. It's, but, until, it's the Numenta logo, we don't want these details. I've seen it once and I rotated. I still know him on the ment. Sure. It is persistent. Yeah. Yeah. Okay. It doesn't matter if Okay, but my point is, it's the Numenta logo, the top of the logo. Yeah. And then so you go to the location where the logo is expected, you're going to get the prediction of that feature. Which is going to be biased by the id if you're on a different, cup now without the logo, then you have the same location active. That's because, that location is associated with the new mental logo on, in one object's instance, that is going to send some bias up to L four. But in the absence of the bias from the ID representation, it's not gonna be enough. I, think I didn't follow it, but I think what you're trying to say is we're gonna have a single reference frame and, the prediction's gonna, I'm gonna make the unique prediction based on the object id. Yeah. I don't like that. I don't think it's gonna work. Like I, maybe I'll have to try to convince you that it's not gonna work bad. Okay. It doesn't seem like it's gonna work. it's, I thought that's what AP Denverites were all about. that doesn't make sense. Wait, come on. That's a, curve ball. Aprils all about.

so how would we know that this is a unique location on that object without having the object id, it wouldn't, I think the proposal is you'd have a totally different location representation for when it is the location on the momentum mug versus location. How would it be a totally different, because I need to do pa I continue to do path integration while I'm no, the sparse code, the, additional, isn't that what you were suggesting? I would still both would be a subset of the dense code. So there would be, I guess similar in some regards or is that, are you proposing a subset of dense code or are you proposing that just like in this ID tells you what to do somehow? Oh, no, I'm not talking about the id. I'm still saying yeah, but, I think Ramy was asking like, what is the alternative? And I was, I was trying to paraphrase what we had been talking about before, which was that we have the dense grid cell code, the sparse say grid cell code. It's the sparse can cell code, which is, we'll have a unique location only for the mento logo at that. When and when you're not, when you're not there, what's the location representation. How do we go from a dense to a sparse one? where's the sparse one and where's the dense one? I thought that was the two populations we were talking about before in L six. Yeah. But before I had it as the grid cells and, yeah. Yeah. That's what I'm talking about. grid cells aren't sparse enough. They're not dense. they're too dense. They're not, gonna work for us. I'm confused. I thought we got discussed. I was just trying to paraphrase the previous argument. I wasn't saying that was right. I was just trying to say to Rami, because we were talking about how are we gonna capture this? I was just talking about the ID. Ramy was saying, if we're not using the id, what are we using? And then I was saying, oh, one thing we were just discussing before was the two populations of grid cells. Okay. Alright. Oh, okay. That's what you're saying. Then we're back to where we were before. Yeah. so we don't have an id, we have a union and that and the, and this sparse, grid cells, I guess until we know which object it is, Yeah. If, there were, two cups that both had logos in the same position and we don't know which mug it is yet, and then there's, I think you would have a union of the locations and that were associated integrating on multiple. Yeah.

My summary of this is what if we, if, we're going for the, this idea down here where the meaning columns are just phase distributions, then we have a solution that works for the sparse code. It's, it works. I can code that in layer four, recognize that pattern. I'd have to recognize different patterns going in different directions. I'd have to think in this direction is, if I approach a point this way, it's gonna be a different pattern code this way, but I could, it could work.

I don't have, now the, code for morphology I need, a less spars one for morphology that follows the same rules.

because the, first thing I said, we have to have two models that share path integration. And so we, that's bottom line. We have to have two models, one for morphology and one for specific ones that follow the both through path integration and, the morphology model is accurate in different unique objects. did I miss something? Oh, what's this proposal? Now I thought this was, that there are still grid cells in the right, but we're not using the grid cells. We're not using the grid cells for The idea is that grid cells are there to, to, as a maintenance function for the other cells. They're not actually encoding anything. They just don't have enough information to code anything. So we couldn't use those two steps. we, if we tried to, then we have a capacity issue. Yeah. The capacity issues that there's just, it's, we literally take the tank paper and you say, oh, really, I just got, how many different locations can I encode in a grid cell module? I don't know, 150, whatever. It's, that's, it doesn't work. I'm just thinking if the top down feedback would, the ID thing if that helped as well in terms of Yeah. Path integrating over multiple objects. 'cause that was like another issue we discussed yesterday is like, how do you test multiple hypothesis at once? So basically if we assume that there's just like one, like it's not separated into morphology and feature. In terms of the location representation, like for mugs, we just have a common one. Then naturally when we move through that, we're gonna be testing all hypotheses because the location isn't unique by the way. But then what's unique is the activation that will happen in L four when you get the, ID bias, by the way, the ID has got a problem too, because we have to have two separate models. I have to be able to have an ID for the morphology model. I have to have an ID for the, feature model. I can say that's a circle, right? If I need to be able to identify that and pass it on to someone else.

but that feels like maybe a separate issue for I don't know. It feels like I need to have two.

I have the same issue here, but it is like I have to have an ID for the common object. I have to have an ID for the unique object. A convenient way of doing that would be to spars the ID for the common object.

should we maybe just collect the main issues right now and then take a little break? Sure. just add to the list. Yeah. I don't know. I actually don't think we're too far away from having an answer here. I think it's pretty close. Yeah. I think it have some really good ideas from here.

the best summary I can do is what I just said a moment ago. We have to have two models that track together from path integration and, the, it seems like the surest way of doing that is to have a sparse version of, of the morphology model has a set of reputations for. Features at locations and you, wanna spars the locations and you wanna spars the features. That would be the way, the simplest way of doing the two model where you have, we also get the capacity of the morphology model. We don't have to, we don't have to have, we have to have more of the, I'll make a statement, we have to have more of the feature models than we have to have of the capacity models. Because the feature models, many of 'em share the same, the morphology models, so many of the feature models share the same morphology. Therefore we have to have higher capacity in the feature models than in the, morphology model. But they also might be more sparse, as in like more, they encode less information in that each feature model is probably a few points of information that needed coding. Whereas the morphological model is needs to densely encode. I guess that's true. Morphology, right? But even the feature models might need to be dense. Like you wanna have colors at all, all locations in the, but like that would, I'm just thinking that would be like, where you could, I, it feels intuitive. Like you could easily learn like the, the detailed morphology of an object, but to learn the color at every location. If it was like, let's say some crazy pattern, like you probably wouldn't memorize that. Yeah, I dunno. Or if, a cup was covered in hundreds of like logos, you probably just aside some sort of like generic, right? You wouldn't say, here's what this is. You wouldn't memorize each. Yeah. it's an interesting question. We haven't really addressed that question. oh, if the whole cup is red, I don't have to learn red at every location. I just somehow, yeah.

maybe you do because at different columns are looking at different points and they're all learning. It's red. I don't, I think the, the new idea today, revisit it, but new idea today is going back to this, idea on the bottom left here as a way of getting, sparse coding of vocations. And, and I only reason I like it is because that's what they observe. That is known to be observed in cortex, right? You've got minicolumns of orientation movement, preference orientation in lower layers. So that's, we know cells that look like that or they are, no one's reported that they're phase shifted, but that would be a prediction of the model that the cells in the minicolumns be phase shifted. what we don't have is, a denser model.

we don't have a good working model of the denser model yet.

Yeah. And, but at time, yeah, I'd like to just try and think through maybe later, like more about ways that we could reuse the same substrate. the, I appreciate there might be issues with it, but at least that is one solution. If we can find a way to it of instead of having two separate populations, if we can somehow reuse the same population right. For the reference frame, then we all have the population, the problem from like a neuro, biologic. but you do want separate connections. You wanna be able to say, I need to make a unique prediction. I think they still have to be separated. they could be the same substrate, but they have to, the, it seemed like the models have to be somewhat separated. I don't know. Can I, so we're gonna take a break now. Can I throw out one last thing before I take a break? Yeah, sure. This may be a total red herring. but in vision, in a V one column, what is the, now I'm looking at, remember this has to do with the slabs. Remember the slabs, Here's what you in V one. if you move the, if you move your probe in one direction across the quote, what is you see the minicolumns change in orientation like that, right? So in this direction, you see that and they cycle through again and again, and you go in the other direction. I didn't do it. That right? You go in other direction like this, the minicolumns all have the same orientation.

and so why?

Why is it that these all minicolumns all seem to have the same orientation? They don't seem to be different from each other, whereas you go in this direction, they're all different from each other. But so you're talking about something different from the penetration. I'm just saying if you look on the surface and you say, okay, the minicolumns, all the cells in the mini column here, this is the mini column, and they stick to penetration like this. All these cells respond to the same orientation, right? Then you go to this mini column, all the cells respond to the same orientation. So as you move in one direction across the cortex, yeah, the orientation changes continuously. You move in the other direction across the cortex. The orientation doesn't change at all, is it? I thought they were like these pictures with the pinwheel color there is, right? So this is a. This is a well understood phenomenon, but then you overlay the pin rail on top of it. Okay? Oh, you just, if you look at the hub and vi little iceberg, ice cube model, you just go to Google images and you'll see it, right now, images. and, let's see.

forgot that test.

yeah, maybe that one. Yeah, right there. Okay.

maybe, so right here, this pictures right here, see these, if you go on this section, the orientation's changing, and then it's the ular, like when you go across this right here. So ular dominance, this is left eye, right eye, but within this little rectangular block here, there's multiple minicolumns, all have the same orientation. Oh yeah. So I think you might even see it better here, perhaps.

and then you still have the blobs and blobs.

This one up here.

oh, alright. So very small picture. maybe it doesn't show up there. anyway, you can look up, I'm pretty sure this, I'm sure of it is the way it looks like you have a, they call it a slab because a slab of minicolumns, not many, but a slab of minicolumns, all the same orientation and the next slab over has a different orientation.

I've always wondered about this. It's what the hell's going on there? there's some redundancy there, but I don't think it's redundancy. It's a, it's some part of representational scheme. I don't know if this applies to anywhere else. This could have to do with, this might have to do purely with three dimensional image viewing, like trying to get perspectives. There's a whole bunch of things that could have to do with absolutely nothing to do for a problem here In this picture, it looks like it might be a spatial frequency that's encoded along that direction. I don't, is that what it is? I don't know. Maybe they just suggest that, oh, that looks, I didn't know that, when was this modified ice cube model of Cat B one column, but I don't know. it could be, I can share that paper if that I. Oh, what does that mean? Because you have, there, they show the, that direction and the ocular dominance columns, but then within one ocular dominance columns. Oh, they seem to assign spatial. I guess its like sinusoidal gradings. Why would, they would be like, why, would you do that? Course ones versus fine grain. Why would you want that? okay. I said it could be a total red herring. It may be a total red herring. It's maybe something specific to vision. 'cause vision has, we have the 3D, perception of depth. We have, viewing at a distance you have to calculate the distance of things. There's a whole bunch of stuff going on. Vision doesn't occur anywhere else. so certainly doesn't occur us not sensory. So maybe we shouldn't pay attention to that. I, was come up with other ways that you, the reason I thought of it, here's the reason I thought of it. I said, oh, I, could, use this other dimension here to, to create, a different. Way of creating, unique morphological models.

It is it gives me two variables. it gives me, which cell is, is active in the minicom, but also I could tell which of the minicolumns are, something about this dimension. I was just trying to think of other ways of increasing the capacity or change the capacity of the, representation. Unless it's probably a red Heron probably shouldn't have bought.

Yeah. Okay, we're gonna take a break.