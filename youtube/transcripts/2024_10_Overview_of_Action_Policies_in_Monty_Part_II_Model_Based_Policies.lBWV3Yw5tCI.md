Perfect. So yeah, today I'm going to be continuing the overview of action policies in Monty, and yeah, I think although we'll record this so that we can, share it in the future, the emphasis is definitely on the discussion part, so please definitely stop me and either ask questions or raise objections or Whatever it may be. No problem.

That's par for the course.

So yeah, so continuing on from last time, I'm just going to talk briefly about a few kind of, points here around, I thought it'd be useful to just talk a bit about the kind of core anatomy, and then a bit of the philosophy of learned versus hardware, hardwired, policies. Then, finish up the kind of discussion of model free policies with some discussion of abstract spaces. And then get into the model based policies, and then finally hierarchical policies, and kind of future, directions.

so yeah, we talked a bit about, last time on how kind of model free policies generally map onto subcortical structures in the brain. whereas the model based ones map onto, cortical structures. So I just thought it was worth, Pulling up a figure so people can see, the kind of basal ganglia where they are, in the brain, as these are generally, the kind of regions that are most associated with, model free learning, they have a significant involvement in kind of motor, or like action initiation and control, and there's these, kind of significant dopamine projections for the substantia nigra, which are, implicated in reinforcement learning. yeah, type learning, whereas, for model based By the way, on the, just by the way, on the previous slide, you didn't highlight the, the cerebellum, which is, the bottom, bottom right there. That's very heavily associated with learning and movement, but, Apparently mostly small movement changes and timing changes and, but it's a huge part of the non model free learning system, although it seems to be doing fine tuning of lots of things. Just want to point that out because if you saw a person listening, they're going to say, wait a second, you're talking about, you didn't mention this at all. And then you have, yeah, like the brainstem down here, which is where the actual motor signals Go out and affect the muscles in the body. And including spinal muscle. And then the thal is involved in everything. And the spinal cord does reflex reaction. there's a lot of Right. Pretty much the entire body is a model free learning system, Yeah. And then basal ganglia get projections from the cortex. And then, my understanding is the recurrent loop is then backed by the thalamus.

And then, yeah. Model based is, the model where going forward is where essentially you have these object models learned in each cortical column. and, you can think of these as represented across the. fourth in the sixth layers of each cortical column, that's where the model generally resides, but of course there's many different elements to a model, and it seems L5 is probably, the most kind of important region in terms of, kind of action planning, for each column, because this is where you get a direct, motor projection and this is, two, two subcortical structures, so both a connection that goes to the thalamus and then one that goes to, brainstem, nuclei generally. but what's interesting is, that this occurs in every, column in the brain, essentially, regardless of whether it's prefrontal cortex or, ostensibly sensory cortex or motor cortex, this is a very prominent, projection, and this is quite unusual in terms of how, motor outputs are often formulated. That you have this kind of direct projection from all levels of the system. Yeah. It's worth pointing out that is really the only direct output from the cortex. this is it. It's motor. So we think about it as a sensorimotor system. The only output is motor. there's some, obviously there's a bunch of other sort of, modulator connections like that, but this is viewed as the, output from the cortex.

Yeah. And then last time, I guess we were talking about kind of separation of model free and model based and maybe it's obvious, but I guess what I was just trying to get at with the example of, like riding a bike or whatever is that, if you were trying to teach your child to ride a bike, the initial stage of them, like getting onto the bike, And being told, oh, you need to pedal, or maybe them seeing an adult, pedaling and just having some basic idea of what to do is probably more of a model based thing. But then, yeah, the kind of fine, motor coordination that's required to actually balance a bike and that kind of innate knowledge that comes over time, is more of a kind of, model free type. So I guess a lot of actions, end up involving both, kind of types of policies.

so yeah, so that's just revisiting the model free versus model based with a kind of focus on the anatomy. Another thing that I think is just worth touching on in terms of the sort of philosophy of policies and how we're approaching them in Monty is a lot of what I've described that we've implemented might come across as like hardwired policies where we're we're trying to almost cover the entire space of any policy, the system might need, and, build that from the ground up. Which would be reminiscent of good old fashioned AI, but that's definitely not the intent. The intent is to design a few key policies, that we believe are important. And then these can be almost seen as primitives that, more complex policies can build off of. So that not everything is totally from scratch or totally needs to be learned through some sort of random exploration. but that, the kind of system has these starting points. that, it can, it can leverage, and yeah, we touched on this briefly last time, but that could, for example, include kind of state switching of, the kind of model free, policies that's controlled in some, top down manner with a more model based policy, as well as parameter adjustment, yeah, with the state switching, you could be like, okay, start following the minimal curvature, now switch to the maximum curvature, Or, with kind of parameter adjustment, you can imagine that sort of momentum in terms of how much you either persist in moving in the same direction or move randomly, if you were just trying to sense an area over and over again because it's a noisy input and you really want to clarify the nature of that information. That's a very kind of simple, adjustment that could be made, but if you're just trying to quickly get as much information as possible, over a broader area, then it's natural. You might do more of a sort of high momentum, movement. Yeah. It's interesting to think about, hardwired versus learned, and we have model versus model free. A lot of the model free learning, it's learning, right? There's stuff that goes on, like riding the bike, it's a model free learning. So some of the cortex has to deal with, it has to deal with a body that's changing. It has to deal with a set of Model free action policies that are changing over time, and our bodies grow, right? We get longer limbs, we have injuries, whatever, and the cortex, I just want to point out that the cortex doesn't deal just with hardwired policies, it deals with the other learning policies that might be model free, but they're still learning, and so it's just worth bearing in mind that the cortex has to deal with this, the world it's interacting with, the other parts of the brain it's interacting with are not static, so there might be some really hardwired things like reflex reactions. But a lot of what, most of what's going on subcortically are also changing over time, and the whole system has to settle on that, the whole system has to constantly adjust. it's just something to keep in mind as we think about this. Yeah, Yeah, no, that's an interesting point. We, from Monty, we could have some hard wired stuff that don't learn, but, just, remind people that. Yeah. Remind, reminding myself.

okay, nice. and then, in terms of finishing up the models free, model free policy discussion, I just thought it'd be interesting touching on abstract spaces, which we didn't get around to last time. so I think in general it's fair to say abstract spaces are really just defined by abstract actions. It's not like there's anything that special about the space. something like a family tree or kind of simple multiplication kind of fits naturally into, some space, some subset of 3D space. But what really defines it as abstract is the movements that you are doing through that space. So You know, in a family tree, there's these discrete, relations like parent and grandparent and grandchild, which kind of move you around this space in very, very defined ways, and similarly with kind of mathematical operations.

and last time I talked about, these kind of, model free policies that we've implemented for, for example, following principle curvature on, object surfaces. And so it's just interesting to, this is speculative, it's not anything we've implemented, but it's just interesting to think, about what sort of analogies there might be in more abstract spaces.

and I think it's, useful if you imagine that, generally, Information is clustered. I'm confused, Niels. Are we talking about abstract spaces in the model free action policies? Yeah. Yeah. How, that seems odd to me. Yeah. I would assume that abstract spaces would always be model based.

by the very definition that they're not real world things that we could have evolved with. yeah. So why would you consider them part of the model free space here?

just thinking of it as simple heuristics of how to move through space that could apply just as much in abstract spaces as they do in I guess, I'm challenging that it seems to me that the abstract basis would be almost the definition of something that has to be model based. Yeah.

do you disagree with that? you're, it's my, my spinal cord can't know about family trees, or no mathematics. And, I agree with that. it just seems I guess, I can imagine, that. there would be ways, for the, cortical structures if it's, useful to, to learn simple heuristics about how to, process, More abstract, inputs and Yeah, but I can imagine, in, in terms of the model free subcortical stuff, it's very, physical, right? you got your finger touching something or you're, trying to balance or you're walking or you're, you reflect reactions to physical things in the world. but when it comes to like abstract spaces, I can't imagine what, a subcortical relationship could be. It's there's nothing physical about it, right? It's all in the model. It's all in something I've learned about the world that isn't physically, manifested in them. Like the parent child relationship is not like something I can detect physically. It's something I have to know or learn about. So I guess I'm just thinking if action in abstract spaces is more like essentially how you move your attention through, through that space. Wouldn't that be cortical? how could that be subcortical? I guess I just don't get that. yeah, I guess it's, come back to whether there could be some sort of more model free type stuff as well in cortical circuits. Okay, all right, all right, so that would be the yeah, so I'm not arguing that this would be, this would happen in subcortical structures. But I guess what I'm just arguing is that, generally the simpler of the two of model free and model based is model free. So if the cortex is capable of model based, then maybe it uses some kind of more model free heuristics. that's an interesting question. That's an interesting question. Is there, are there model free action policies in the cortex itself? And I've never thought about that question. My initial assumption would say no, but I haven't thought about it. And because whatever it is. It has to work across all modalities, across everything, right? It has to be, up to now, everything in the cortex, everything in a learning module, cortical column, has to be generic enough to work for everything, every kind of thing you might do, physical things and abstract things. and now we're saying, oh, is there an action free policy that similarly Works. and it's an interesting question. I don't really, I'll, now I know where you're coming at it., it could be in the cortex itself, but it'd have to be something that applies everywhere. So maybe that's what you're talking about now here. Yeah. and yeah. So this is very speculative, so maybe this is, yeah, this is totally wrong, but. I guess what I was just thinking was that there's, there seems to be maybe some kind of symmetry between, what we intuitively seem to do when we're, exploring, an object with our finger, and what we might mentally be doing when we would explore a more kind of abstract space. So if you imagine, that the kind of dimensions of, of information is often it's clustered in kind of high density, regions, which might be if, they're Gaussian in each dimension, it's, has a ellipsoid type, structure, then you can think like following the minimum and maximum curvature, on this, manifold. is essentially following the kind of local principle components, like the, dimensions of most variation, and so to make that a bit more concrete, if you are thinking about someone's political views in general that might fall into this kind of, two dimensional space of authoritarian versus libertarian and left leaning versus right leaning, and Again, this could be wrong, if there's some sort of analogy to how your kind of finger is, notices the curvature of a cylinder and then finds those two directions most interesting, both the kind of orthogonal direction, maybe there's something similar in these kinds of abstract spaces where You would explore primarily along these dimensions, when trying to understand someone's political views.

it's a bit of a stretch. It is a bit of a stretch. If I were to pursue this further, I would say, I would say, if I'm going to come up with some model free action policies in the cortex, I'd want to see that the same policies apply to vision, touch, and hearing.

not just political persuasion, it's if it's got to be in the cortex, it's got to apply to everything. what would be the equivalent to vision? What would be the equivalent audition? It's not, you couldn't consider the abstract spaces, but it's got to apply to other things too, which are, universal. So it's just something to think about. yeah, I guess that's what I was trying to get at with this analogy of kind of the surface of, manifolds in abstract space and the kind of surface of objects, or, you can also think of like in vision, you have these principal axes of the object, which seem to be like the dominant sort of directions and the kind of the points of symmetry, around which kind of the object tends to be reflected, and that's often how we just tend to understand objects.

So yeah, maybe there's some similarities there. and then, if you're studying a metro line, which is a graph, in general, your eyes are going to either follow minimal variation in perceptual space, which is, just the line color. so okay, that might be just subcortical, but then, or more abstract space, you are following some, line name. But yeah, I can also see how that can be put down to more of a model based, policy. Maybe it's not, maybe it's not that relevant. it's also, it's, in our work, of course, we've got to get the basics of, Monty first has to solve, all the sort of, physical world interactions. so it's fun to think about, it's fun to think about abstract spaces, but it's not our first thing to go after. Yeah, that should be, if, anyone's watching this video somewhere, please watch the first one first, which gives broader context. Jeff points out the abstract space is definitely the more just niche stuff that we're thinking about for the long term, not, the current work.

but yeah, maybe that's a good way to, segue to model based policies, unless anyone had any other comments or questions.

Cool, as we touched on last time, the model based policies are going to, leverage learned, models of objects, in order to make more kind of planned, deliberate action proposals, and we really have, two that are currently implemented, in Monty, and again, this is a situation where we're crafting these policies, but they could be used as primitives, for more kind of general policies and, represent. Is this, are both of these basically all around learning and inference? yeah. when I think about, oh, we're talking about action policies and the cortex, I'm thinking like, oh, how does the cortex achieve some results in the world, the movement? But we're not talking about that here, right? We're just talking about like, how does the cortex. Learn objects and how to infer them, is that correct? No, that's correct. That's a good point.

yeah, not yet changing the state of the world. Yeah.

so yeah, and the key one of these is this hypothesis testing policy, which is the one I'm going to focus on. and really this is about how we can efficiently move to features that disambiguate, object and identity, and or pose.

and the kind of concept, if you imagine, we have multiple models in our memory, like the fork, spoon, and the knife, and we have some, observed object we're feeling along the handle of this knife. We, at that point, don't know what object it's going to be, but we'd like a policy that would enable us to efficiently move to this part of the object that's going to, very quickly disambiguate between the three of these.

so imagine, yeah, the way we're going to do this is what you can call, local graph mismatch. and I'll talk later about how kind of hierarchy can fit into this. But, essentially, we're going to take the most likely object hypotheses we have, and use the most likely poses we have associated with those, and look for a mismatch in these, aligned graph regions.

breaking down that a little bit more. Imagine our two most likely objects are a spoon and a hammer. The true object we're observing is the spoon, and these are the kind of graphs. And right now, our kind of object models are quite simple because we don't have any hierarchy. So we have lots of points, and all these points are associated with this kind of morphological feature of a point normal with some, principal curvature directions.

and we're going to have some most likely poses associated with these, and what we're going to do is transform essentially internally in the sort of mental space of the learning module, both of our most likely, these two most likely hypotheses by their most likely poses, and that will give us some alignment, just naturally based on how Monty works and how it It naturally aligns objects based on its hypotheses, and when you have these two graphs aligned like this, the algorithm in its kind of current version will basically just find the nearest neighbor pair across the graphs, and it then takes the Euclidean distance between those and finds the kind of maximizer. So the It's basically going to be the nearest neighbor that is most distant across these graphs, so you can see like I don't think that means how's the nearest neighbor most distant? Yeah, so basically if you take Every point in the red graph is going to have a nearest neighbor in the blue graph, so this one will be that, this one will be that, and this point also has a nearest neighbor, it's all the way over here, so this is all the nearest neighbors. So it's got those three nearest neighbors here, the three dots in the tip of the spoon. No, but we're only comparing to blue. We're, because we're comparing across the graph, so we take so you got, the blue, what's the closest to the blue dot at the top of the hammer? Is that what you're saying? Yeah, but I'm, going through all the red points, and for each red point, finding the nearest neighbor in the blue graph. huh. So, yeah, so this, dot's nearest neighbor is this one, this dot's nearest neighbor is this one, and this dot's nearest neighbor is this one, but if we take the distances to their nearest neighbors, This dot's nearest neighbor, that, this dot has the largest distance to its nearest neighbor. So that's what the most distant nearest neighbor is. And that, the maximizer, that point, it's basically the point we want to test. And I'll talk later how this can also apply in abstract spaces or like feature spaces. Which is a more natural comparison. What is the sort of the everyday logic behind this? why did you pick this? I'm not doubting it. Yeah, yeah. the reason I picked this is essentially it's it's this idea that, okay, these two things are aligned, but how are they most different? and they're most different where they don't align. And that's basically what this is looking at. But a more natural way to measure this and test this is, But I'll talk about later, if you have hierarchy, and let's say you have a representation of a spoon head, and you have a representation of a handle, and then for the hammer, you also have a representation of a handle, and you have a representation of a hammer head, you're going to do the same thing, but if you compare them in feature space, the handles are going to be very close together in feature space, but the hammer head and the spoon head are going to be very far apart in feature space. So that's where you're going to go to. So rather than it being Euclidean distance in physical space. That's right. So you're looking for the biggest, maximum difference between these two models at some point. And yet, so in the physical, non hierarchical model, it's a little linear, Euclidean distance. But in object models, it's, It doesn't have to be physical distance at all. Exactly. In the abstract case, how would I define the big difference? Just different features then? so for example, SDR difference. So The SDR representing the hammerhead and the spoon would be more different than the SDR for two different handles. So you're baking this assumption that the SDRs are conveying we have some way of representing feature alignment, whether that's with SDRs or something else. But yeah, I'm assuming features aren't just random points in high dimensional space. Yeah, okay. I have a quick question. This might be obvious, but, so this works when we have the, when we have already learned the spoon and the hammer and those points are in memory for Monty, right? in the unsupervised or in the, like from scratch case, Let's say we like explore the handle and we only have a knowledge of the hammer, but the actual object is spoon. In that case, we don't have the whole graph of the spoon. And maybe this doesn't apply to the unsupervised case, but like, how would you think it would work in that case? Yeah, no, it's an interesting question. Yeah, yeah, I should have clarified. Maybe this is only for inference once we've learned many objects.

you, can imagine, maybe doing something similar at learning where it's I think I'm like, I only know about the hammer, so I think I'm on the hammer. But, let me move to what's the most interesting part of the hammer? The hammer head because of, other objects I do know about, but let's say I don't know about a spoon. and you might more quickly, just with the way objects are structured, I think there's going to be similarities in terms of, like, where the most interesting parts are and things like that. but yeah, in general, this isn't set up for, use during, learning. for the learning part, you really can't use the nearest neighbor concept. all you can do is say, hey, if all I know is a hammer, I think I'm touching a hammer, and then all of a sudden I feel something which doesn't fit the model. yeah, I didn't, yeah, it's only, I guess if you already knew some objects, maybe that could be used to bootstrap how quickly you learn or something, but yeah. I think, that learning is going to be, we have to explain both of these at the same time in some sense, the learning and the inference. so, generally you're going to, for whatever reason you're going to sense something which doesn't fit the model you have, and then it becomes an interesting question is how do you decide it's something new, or an extension of the existing model, those are interesting questions. Yeah, Yeah, no worries. Yeah, and I guess maybe to answer your question from earlier, Jeff, if your most likely, hypothesis was the hammer, and your second most likely was the spoon, then the algorithm would start with the hammer, and for every point, it would look for the nearest neighbor in the, spoon graph, and then, in that case, it might be this point, which the most distant nearest neighbor in the spoon graph is this one, so there's some kind of asymmetry depending on which object you are starting with as your base object. Okay.

It's the idea of, look, if you're searching through objects for nearest neighbors, certainly it would work. It seems hard to understand how the neurons would do that. they don't, they can't sort through, search through lists, you know what I'm saying?

and, I don't think I have the answer to this question, but the whole, one of the things, the reason we love the union property of SDRs is because it allows you to do that without sorting through lists, it's like it somehow allows you to deal with ambiguity without saying, okay, let's try all of our hypotheses. Yeah. I don't know what's the right thing to do for Monty, but, clearly neurons can't do that if I'm understanding what you're saying correctly. Yeah. At least, yeah, the way we've implemented it. Yeah, okay, I just, yeah, no, I agree with that. Yeah.

yeah. And so in practice, what this kind of looks like is, so you can imagine, so this is the spoon as it's oriented in the, real world. this is the agent, the finger, which is touching it here. It's, I've done a few steps. It's on the fifth step. So it has some sense about what object it's on, and in particular it thinks it might be the knife or the spoon. And then it's going to, this is in green, these points is in, now it's in its mental space. It's going to align these based on the most likely, poses of these two objects. And that gives us what we were expecting, which is that the spoon and the knife are both here. the, heads. and then it's going to use that, most distant nearest neighbor, and that gives this red point here, just like on the back of a spoon, as the kind of, place it should, test.

and then the policy then, the kind of lower level parts of the motor system essentially receive that as a target location, and then, the agent, moves there. And then in the kind of current implementation, it moves there instantaneously. It's teleported to that location. and then it, it can then keep moving around from there.

it seems like what we have here is a hybrid, which might happen between a single column and multiple columns. A single column, I'm just thinking neurons here, I'm not thinking Monty. Yeah. Okay. So in a single column, let's assume it can't check multiple hypotheses, it can't go through a series of objects and saying, what's my nearest neighbor to these other objects. All it can do is. is move along and until it just, your fingertip can move along until it discovers something that doesn't fit the current model. And as I say, something's wrong. But interesting, if I have multiple columns active at the same time and assume that they know where their relative positions are relative to each other, and, I say, oh, I think I'm on the spoon. And so now all the columns say, not all the columns, even the ones that aren't receiving input are saying, okay, we're, observing a spoon here. And that column may say, in my current location, I'm not on the spoon, right? Or maybe I am on the spoon. So the point is a distant, another column could be observing a distant point of the spoon or the hammer. And if it says, okay, I think this is a spoon. And, I say, I know I am, I shouldn't be observing something because I'm not on the spoon, given our consensus. Okay. And yet I am on something. I hit, I, it's like something beyond where the spoon is supposed to be. I'm detecting something. then I have my ability as a column and say, hey, we ought to go over here because, it doesn't fit right. I'm saying, I shouldn't be seeing the spoon. I shouldn't be seeing something here and I'm seeing something. Let's go look over here. so this kind of, in some sense that's, I'm trying to imagine how neurons would do this, what you're proposing here. They would do it probably closer to what multiple columns would be doing. They would, be doing this, instead of saying a nearest neighbor, it would be like, a testing different points on the object simultaneously, including ones that shouldn't have anything. And, then somebody says, you know what, this isn't right, given that I'm, we're supposed to be seeing a spoon, I'm seeing something else. And then, visually, our attention would naturally be drawn to that, or tactilly, too, if I'm grabbing an object with my hand, and multiple patches of my skin are touching it, and one of the patches of my skin doesn't fit right, it's just that isn't what it's supposed to feel, your attention is mainly drawn to that part, and, and, and that sort of achieves what you're doing here, I think. Yeah. So it would be Yeah, although Yeah. Okay. I guess in the, kind of single finger setting, we would also generally be able to do this, like work out where to test. But you would, I think you wouldn't test anything until you ran into a problem. if I think I'm on the spoon, I'm like, yeah, I'm on the spoon, I move my finger along. I don't say, hey, I'm not proving I'm on, I don't, know, you might, just say, I think this is the spoon until you find out it's not the spoon, as opposed to saying, hey, I don't know what I'm on, it could be the spoon or the knife or the fork. it might be that. I don't know. I have to think about it. It's an interesting question. Yeah, but I think it is, it's interesting about the multi LM, because I guess even if you had one finger, you'd probably still have multiple columns active, and if they inhibited, if there was some kind of mutual inhibition, so some of them were like, okay, I'm going to have this hypothesis, the other one's going to represent this other hypothesis. And so they're both kind of similar points in space. I don't think that would be the case. If you have, a single hypothesis, that would be voting across the columns. And so everyone would say, Hey, you know what? We all think, the voting, the way, the way we've implemented the way I think it works. it's a, it, very quickly settles on a answer. it, doesn't like, it, it, doesn't, it very quickly settles on the answer. It wants to come to a hypothesis. That's it. And, as soon as it does, then the different columns could say, this isn't right. I think about it. I don't know. All right. I'm just, I'm thinking out loud. Sorry about that.

I think, I just, maybe the general thing to think about here is to keep in mind that what does one column do versus what does multiple columns do in the same region versus what's going to go on in a hierarchy. And those are our, two basic, those are our three basic scenarios we have to understand. Single column. Multiple columns working together, and then columns in a hierarchy. And so some of these action policies that you're describing here might be mix and match of those different scenarios. I'm not objecting to it, I'm just thinking out loud. Yeah, so yeah, just showing some kind of other examples of this. if you have a mug versus a handleless cup, this process might propose that you test the handle here. Here, the knife is the most likely hypothesis. And so distinct, to distinguish it from the second most likely in this case for, poses this, location.

and I think, yeah, I alluded to this earlier, but I think one nice, The kind of element of Monty is just its kind of rotation equivariance, where it it always accounts in its hypothesis space for the rotation of the object, and the rotation it was learned in the real world, the kind of, like initially, the sort of like base rotation is totally arbitrary, Here you can see, if you plot the rotation of the internal models of the knife and the spoon, oriented as they were in the real world, then, they don't align, but when the, system is sensing it, it can sense the, orientation of the handle, and so it just naturally, aligns these in its hypothesis space.

and then you can see just how, over a course of inference, the kind of hypotheses, eventually you might come to the point where you are convinced it's the mug, or you've eliminated, all objects but one, but you might still have some uncertainty about the pose, and you can also use this process to, infer that. oh, I'm still leaving the slide there. a better, yeah, better example is here. early on in inference. You've sensed the handle, but it can be ambiguous whether the spoon is oriented that way, or it's mirror reverse, and so you might test a point like this, but then later on, you might just have some very subtle uncertainty about the pose of the object, but, is it, 10 or 20 degrees in particular rotation, and then so bringing everything together, this is showing the model free, curvature, but, following policy with the model based hypothesis testing, and so what you see here is first the, in white, the, finger, the surface agent is following the minimal curvature of the handle, so it's moving to the end, and when it gets to the end, it, has a high, certainty that it's looking at something like the spoon or the fork or the knife. and then it performs a hypothesis, based, jump to the head of the object. When it does a hypothesis based, jump, it has one hypothesis in mind. It says, oh, I, is this a fork? Lemme test that. Is that what? Yeah, that's it. It's not doing, yeah, it's not doing fork and knife at the same time. It's saying, okay, this a no, it's saying this is my most. So here it's it's most likely hypothesis of the knife. It's saying, okay. What else could it be? It could be the spoon, but I think it's a knife. So I'm gonna test it's knife based on the thing that's most different to a spoon.

yeah. Okay.

mentally it's, it feels like if I do, if I, it feels like if I'm touching something, it's ambiguous. Then I say, oh, it could be X, and then I test X, and if it's not, I say, oh, it could be Y, and then I test Y, as opposed to, I don't feel like I'm, testing them all at the same time, or jumping around, but rapidly, I, would, I explore one hypothesis, then I get to try, the next hypothesis, then I try the next hypothesis, until I get the right, which I think is consistent with what you're saying here. yeah, I think so.

You used the second most likely one to inform how to test the first one.

it's actually the second most, likely, I don't, know how you, how do you determine the second most likely? I missed that.

It's just based on the evidence scores. Oh, I see. Based on your evidence. if I have, a whole set of cutlery, each all have the same handle, then they're all equally oh, guess maybe some, maybe I use forks more than I use knives. Yeah, they might all be pretty likely. So you might compare it to any of them, but you are already very certain it's not your cat or like some object. I'm just saying it's, there is, there may not always be an obvious thing.

there may be a set of things that are equally likely at some point in time, all the cutlery has the same handle, and I may be four different types, so I, just, there's no one that's more likely than the other, and I just would have to pick one. but I guess the system would, wouldn't have a problem with that, would just say, okay, they're equally likely to pick one. Yeah, that would just be random noise, which one to pick.

yeah, this is just showing that this also works with the distant agent. So here the distant agent, because it's this kind of ball and socket, eye like agent, it kind of moves around at a point, but then it can jump in space to then look down at a different part of the object, which is what you're seeing here. And then after that point, it jumps over here to then look down at the handle.

And, whenever it arrives at a particular location, then it starts doing its kind of, random walk over that area.

This is, again, the distant agent, where it first tries to test this side of the object, the spoon, and then it tests the other side.

and then you can plot what are some of the hotspots for testing based on this policy. And, as you'd imagine, it's extreme edges of the objects, like rims, and handles.

and again, with the Things like the fork, it's the two ends of the object, so that was more qualitative results, so I can just show a few quantitative results, these are, this policy was implemented a while ago, but just showing that kind of when we implemented this, what was the effect on the system, and so all these results are when we, learn and test on all 77 objects in YCB, and where we have, noise in the data, so this is showing, for example, the amount of location noise, that there is, and it's worth just mentioning that maybe, what you were asking about earlier, Jeff, clarifying the kind of correct, or like the model is correct about its, most it's, it's final output versus when it just gets the most likely hypothesis. So if you imagine that the, true object is mug in, scenario one, the, mug has the most evidence, so it's correct in its most likely hypothesis, but it's only in scenario two where you have this significant difference in evidence. That enables the system to, converge, and so in particular what this is, You're saying that scenario one, it wouldn't converge in scenario one? it hasn't converged yet. Oh, okay, but it would, wouldn't it, eventually? Potentially, yeah, it should, eventually, but it might not always. But, the reason I'm mentioning this is because when we report the results of Monty, sometimes we just look at whether the most likely hypothesis was correct, and sometimes we look whether it actually converged to the correct hypothesis. It should always Yeah, it should always converge eventually, but when we run experiments, we set like a maximum amount of steps just because we want to keep the runtimes at a reasonable time. And then that way, sometimes it just times out and we say, all right, you had enough time. You didn't have enough certainty to make a classification. I guess even with lots of time, if there was like, noise in the data, like if it just couldn't disambiguate them well, it might never fully convert. I think, often just going with the most likely is often good. imagine I have a cabinet full of coffee cups and one of 'em has a chip on it, on the, a little defect on the side. And so I know that one is a separate item, object, let's say, for example. I don't know, maybe. Maybe there's something else different about it. But I reach into the cabinet and nine out of my ten coffee cups are the same. I pick one up and I just assume it's the, it's one of the, one of the nine. And then, and that's good enough. And then maybe later I, my finger brushes across the, broken part. I go, oh no, this is the broken one. My point is I don't have to search through all my hypotheses all the time. I, I. At some point you just say this is good enough and I'm going for it, until I accidentally discover oh, no, there is a difference. Yeah, and that's the strength of this evidence based approach, that we can act at any point in time and just use the most likely hypothesis, like we don't need to 100 percent recognize the object, we can just act on our current hypothesis. But right now in this kind of research stage, we wanted to really get to scenario two as quick as possible, which is why we want to have these efficient policies that quickly get to a high confidence on which object it is. So scenario two would be, in your example, trying to move to the location where your chip on your coffee cup is and actually see if it's there. it's interesting because there's a scale here. Obviously, I don't want to confuse the coffee cup for the fork. that's not going to work. But confusing the coffee cup for another one that's very similar to it or a fork for one that has maybe a slightly bent tine. I don't want to spend all my time, in life, looking at all the minutia. it's hey, let's keep going until we discover otherwise. and, and then when something happens that's unexpected, I go, oh, shit, something happened. So I guess I'm saying if there is a spectrum of when scenario one is acceptable and when scenario two is. Yeah, this isn't meant to be a criticism of scenario one. It's basically just a preamble because I'm going to show some results from its accuracy in this setting when this is sufficient, and its accuracy in this setting, like when we require it to be this confident. it's also like when people do benchmarks on object recognition or something like that, they might say, did you just, did you determine this particular version of this object or something like that? And maybe that's not what we want to do all the time. only, it's in a practical system. You wouldn't always need to go all the way to scenario two, obviously, and the objects you're showing here, you would, but in the case of a fork or the bedtime or one that's a little bit different, you wouldn't necessarily have to. Okay, thank you. Yeah, no worries.

so yeah, with the distant agent, so it used to be before we had this policy, the distant agent was essentially confined to its starting location and looking around in that space. So we saw quite a significant improvement. in, inaccuracy there, in this is, doing what, this is, recognizing all 77 objects with noise. I'm sorry, what are the two scenarios there that, with, top down policy? Sorry, so red is, yeah, red is the kind of baseline, where it just moves around randomly. The blue is where it also can jump to the desired locations. But here, there's a smaller, and on the next slide I'll show a bigger difference, here is where we're not required to converge. So that's the scenario on the left. We just need to have the correct most likely hypothesis, and you see there's, a reasonable improvement, but there's not a huge one, because. in this case, often, our hypothesis was correct, we just were never able to look on the other side of the object to confirm it.

but in this case, we have to, we, it's, the second scenario, this one, we have to converge, for it to be considered correct, and here there's a much bigger improvement, because, It's very hard for the distant agent to converge when it's confined to, one side of an object. Often the disambiguating feature is just not present. and so there was, yeah, like 45 percent increase in accuracy. Yeah, and with these model based policies, the main measure that we want to improve there is the number of steps. quicker convergence. Ideally, we would have good accuracy in both cases. I think this is more of an artifact of the distant agent only getting one side of the object, like only one view of the object if it can't jump, and then it can't disambiguate some objects from each other. Yeah. Yeah, and on that note, it does significantly speed up inference for the distant agent.

and so this is just showing the number of steps to convergence, where each point is an episode, and the cross is the mean.

and then, for the surface agent, it was a smaller improvement, because, if you imagine, the surface agent was already able to fully explore the kind of surface, and yeah, when it wasn't required to converge, it's only a four and a half percent improvement. I don't know. Where you do need convergence, 14. Because the surface agent can move around the object? Exactly, yeah. And the distant agent can't. Yeah, but I think it's worth pointing out, 14 percent is still a pretty significant improvement. if you consider, a typical benchmark, and particularly, the closer you get to 100, the more you start having ceiling effects, where I think the big issue, you're already at 80 percent with the baseline, Yeah, but, and similarly, I think there's kind of ceiling effects with the, so there's only a bit of an improvement with the speed for the surface agent, and I think partly what this is There are just some objects in YCB that are very difficult to disambiguate. potentially they're only different based on color. And, the surface agent, if it doesn't have access to color, then it's never going to converge, even if it can jump to different points.

but it just gives a bit of flavor of the kind of quantitative effects of having that policy. It's interesting to think about in the YCB data set, lets say you have two objects that are only different by color. If the surface agent is a, a finger, then there aren't two objects. It's just one object. it's they don't know the difference. That's something that, we recognize, but yeah, it's currently not reflected in how we measure accuracy on YCB. But it would be really nice to have that, yeah. It reminds me of when I talked about auditory objects, like the sound something makes, and, like the sound a cup makes when you put it on a desk. there's lots of objects that can make that same sound, so it's not oh, the auditory column would say, Oh, that's a coffee cup, or that's a phone, or that's a plate. It would say, oh, that's a sound of a certain type of sound I recognize, a ceramic hitting a countertop. And yet, so it doesn't, know about those other classes. It just, it doesn't know anything about coffee cups. I just know the sound. I recognize the sound. And then we can associate, through voting, that sound with a particular object, oh, that could be this object. So it's similar to that in the sense that. A touch object which doesn't have color wouldn't know different objects only if they're by color. It's not like it says, oh, I can't tell the difference between the two. It's just that they're the same. And, so voting, the visual object, the visual, system would have to distinguish between the two. I, just think it's interesting to think about when we do these, as you said, as we do these, tests, to bear in mind that it's not always appropriate for the system to differentiate objects. That are undifferentiable in the particular modality we're dealing with. And that's not a failure of the system at all.

Yeah. Yeah. And speaking of failures, so there's just some, I think, interesting cases. It's not necessarily a core failure, but just quirks in how it works at the moment. for example, we don't currently use kind of absence of a sensation, so like negative information. We haven't yet implemented how to, like a way to. update the evidence for objects based on that, it is in our pipeline to do that, but what that means is if we're currently on this part of the handle, and we go to this point in empty space, because we think the spoon or the knife are oriented that way, and we don't see anything there, the system just moves back to where it was before, and it doesn't use that information in any way, but actually that information tells you a ton about the orientation of the object, or the initial ID of the object. The fact that you were expecting there to be something there, and there wasn't. So do you view this as just something we haven't done yet, or something we don't have yet? Yeah, exactly. It's just we haven't implemented it yet, but we definitely think it's important. Instead of getting the failure case A, we might call it, work to be done.

the word failure seems pretty, hey, it's broken. Fair enough. Yeah. It's, not a failure. It doesn't recognize the sound in an object because we don't have an auditory sensor, right? Yeah. And then I've alluded to how eventually you want to do this more in like feature space and when you're calculating the distance rather than just Euclidean distance between points. and so that means it's like in some instances, like it might say oh, this handle is more different than this handle. Like it focuses on this part of the object rather than the. The head of the fork versus the head of the spoon, because physically these are more separated, even though clearly these are the more kind of interesting parts that are going to tell you something different.

and then something I just noticed in terms of the efficiency of the policies, is often the same locations are visited, and that can just be a bit inefficient. I think you can imagine kind of some model based ways in which, you might avoid. revisiting the same locations again and again.

yeah, it's interesting just to think about personal. When I touch something, I often do exactly that. I touch the same things over and over again. It's like a fidgeting, if I'm holding a pen in my hand, and I'll often just move my finger over the exact same parts of the pen over and over again, play with the clip or something like that. I'm just, this is just an observation. I'm not sure what the, why the brain is doing that.

But I often do that. It seems like my cortex is sitting there just like testing the same thing over and over again. I don't know why, but it does. It's not like I'm thinking about it too much, but like I'll be thinking about something else, but I'll be fidgeting with the pen and just repeatedly touching it the same way or twirling it the same way or something. You're starved for stimulation. I don't know, I'm starved for stimulation. Maybe, but it feels Yeah, maybe, the cortex doesn't want to do nothing, it's I want to do something, okay, I'll just keep doing this over and over again. But it's not like I'm really actively testing, I'm not consciously testing something, I'm not consciously sitting there going, Is this still the pen? Is this still the pen? I'm not conscious of it, maybe that's what the columns are doing, I don't know. But it's interesting that, I don't know if it's meaningful, I just make that observation. How else would you learn? Behavior and changes, right? you do it. Yeah. But once I know it, why would I do it over and over again? Yeah. something I do is some pens have a little clip that you can, it's springy, and if I have one of those pens, I will often stick my finger under the edge and just push it up and down a little bit, and I'll just do that, constantly.

it's, clear, the need for that is clear in adversarial situations, when you have an opponent working against you, you always want to verify that things are the way you think they are, because through actions not of your own, they will change in surprising ways. Right, but that makes sense, but it doesn't really make sense in the case of the pen. I've been holding this pen for 20 minutes, I've held it every day. I don't know. Yuri Ivanovich says it's the same mechanism for all the things, so maybe it's just a degenerate case of that mechanism. Maybe. Yeah, maybe. Degenerate case. There's a mechanism sitting there going, I got nothing else to do, so let's keep testing this. you basically look that there's a subtle reward involved in, in it. And then, In an extreme case, you would have OCD, where you keep repeating the same actions all together. there's some pleasure in it, right? Yeah. There's some pleasure in verifying that this is the pen that I know. Or just the tactile sensation is a reward in itself. I don't know about it feels to me You don't have to be goal oriented for everything. it does feel like when I do this fidgeting, I'm actually it's not just, I do the same thing over and over again. It's not like I'm just moving my finger all over the place. It's I do the same behavior over and over again. there, there is like a certain pattern, I think, to fidgeting in terms of it's often mechanical things, or I don't know if it has some relationship to, yeah, reward associated with I don't know, tool making or something like that. snapping a stick or something like that. it feels more similar to that than Any possible random thing, which is like touching a flat surface or something like that. There is something salient about it. I'm, fidgeting with the salient component. We don't want to spend too much time. if I just sit and talk to someone and I'm sitting across the table from them and talk to them, we know that the eyes are continually saccading between the eyes and the nose and the mouth and the ears. And it just, three times a second, for 10 minutes, as long as you're looking at them, that's what you're doing. And that's basically, again, just looking at each, I don't know, looking at the same features over and over again, maybe they're changing. I don't think it's worth a lot of time on this, I just, it's interesting to point out that somehow there is a, I think Kevin's right, there's some sort of reward for verifying this hypothesis and you just keep doing it and it could become OCD at some point. some people have these kind of tics when they're trying to concentrate on something, and it can either be, Sometimes they listen to certain music and the rhythmic patterns in the background, or some repetitive motions, are it, it triggers something in, in the process of concentration that somehow is reassuring or reestablishing something. We're getting pretty far afield here, right? I agree. I just want to point out that I just observed this hey, I do move my finger all the time over the same things. But, I agree with you. We don't want to change this in terms of what you're talking about here. We don't want to do that. yeah, I guess just in terms of people who are actually using the policy, just to be aware that, yeah, at the moment it's implemented in a way that the agent kind of teleports to the location it wants to go to. And so you can imagine the kind of analogy for this is We essentially have the kind of higher level policy goal state of being in a particular location in the environment that gets passed down to, say, more subcortical structures, which would actually initiate the movement through space, simple walking or whatever that's necessary to get there, but to avoid having to implement that and coordinate that with, the kind of realities of the environment. At the moment, we just move there instantaneously. I think that's right. the cortex, basically, if you think about the output of the cortex, I always imagine it's a, the inputs of the cortex are sort of movement vectors, you go in this direction for, at this speed for this amount of time, the output would probably be something similar. And so the output would say, okay, I want to get to some location, I should move in this direction, and, at some, speed and time, but it's actually not doing it, it's just sending that signal to somebody else who's going to try to do it, so in the eyes it's sending a signal to the pericalliculus would, which actually knows how to do those things. so I think, in fact, if, we're just, it's basically saying go here and, let somebody else take care of it. there's nothing embarrassing about that at all. That's what we should be doing. Yeah, I think that will tie into, the second part, the goal states, and can yeah, Talk about that later, like how Yeah. Output goal states and then the subcortical areas can take care of executing and getting to those states.

Yeah, and then I guess just the final point on the local graph mismatch that's being used is just how when we bring in hierarchy, you can imagine this being a lot more efficient. So it's still the same idea. You have learned models aligned, but they're most likely poses, but then you do distance and feature space to determine where to test. This is not currently implemented, but actually we could add this Quite easily, once we have our scene level dataset fully implemented, and, with using Rami's SDRs. One other alternative that ties into, Maybe like making it more plausible, like it would be easier to imagine how the brain would do it. And it might be more efficient as, just storing these hotspots that you showed earlier in the model of the object. So for example, for a face, we would just store that eyes, nose, mouth, that kind of hotspots to go. And you could either calculate the hotspots, calculate these a couple of times from the graph mismatch. Or you could infer them whenever you make a movement and it gives you a lot of evidence. or like makes you recognize an object. Then you add that as like a high salient point in the model. And then, yeah, like Jeff was no, definitely. Yeah. And Jeff was alluding to earlier, then you can like, then you're just testing an object. You don't really have to like, this definitely feels like the more deliberate thing of Okay, you're given a really hard task of I don't know, you're at a dinner party and there's a game where you have to test what's in a cardboard box, and so you're really thinking about the possibilities, but in most day to day life, yeah, like you said, you're probably using a more simple heuristic of just oh yeah, I tend to feel here, and then that tells me what it is. Yeah, that's a good way of putting it. this requires some cognitive resources to figure out where do I go best to distinguish these two objects? And you need the whole model of the objects to calculate that. Whereas just having some hotspots in the object model that are good points to go to, to recognize that object would be like a much more efficient way just to have a general policy for each object, Yeah, thanks for bringing that up again, though. I think it's a while since we talked about that. like having minimal distinguishable features, a set of those. Maybe. Yeah.

I think. Wait.

Yeah. I think one thing we have to bear in mind, if you go back to the picture of the, that one there. Like you said, oh, okay, so the, whole feature, we're going to, we're going to look for this feature, the knife end as a feature, right? That's the idea here, the hierarchy. Yeah, exactly. So in this example, like we, we've talked about how with hierarchy, you could use fewer points to represent objects. and so already this is going to be much more efficient because we're not comparing thousands of points. Thousands of other points. The thing to bear in mind on this is what we discovered about hierarchical compositional structure to begin with, is that the key to solving that problem was to say that a feature doesn't have a location, like the hammerhead doesn't have a location, right? It has many locations which are all part of the hammerhead or all part of that feature. And, we do it on a location by location basis. I, don't, this is in the paper we're writing, so the, there isn't a location for the hammer head. It's like you could go to any point of the hammer head. As long as you know which point of the hammer head you can say it's supposed to be a hammerhead here. And I happen to be at this location right now where, I mean that location now. But you'd still test it. It we have, I just wanna remind everyone, we don't wanna get in the habit of thinking like, oh, a feature has a location in space. It doesn't, it has many locations in space. And, like a composite object built of other objects. So objects have no specific location. They are, they're located on a point by point basis, relative to other objects. It's a No, that's a good point. I, made this overly, simple. You didn't do that wrong. I'm just pointing out that when we think about No, but I think it's a good point in terms of how we represent it. Because, yeah, we don't want, as you say, like a single point. It's oh, that's where the hammerhead is. We did, this is the big lesson, but we can still, use fewer points, like we don't need to, to Oh yeah. Oh, yeah. Cover the thing.

it's almost if I was moving from, let's say I was moving from the handle of the, to, to up to the end to see if it's a fork or a spoon.

it doesn't matter actually exactly where I land on the fork of the spoon part. I it, as long as I know where I, as long as I know the location. And I know what I'm expecting to see, I don't have to go to the same location, it's just any location on the spoon part or any location on the fork part will work, because, it's a subtle thing, not everyone understands it, it's a subtle thing that's really important, we'll just have to keep that in mind all the time.

Yeah, and then just on the distance and feature space, so I am assuming here that, for example, we're representing objects, features like coming in from the lower level in the hierarchy. With something like SDRs where there's some overlap based on the similarity of the objects that have been learned, similar to the work that Rami was doing where, you saw this clustering and then so you can imagine that, in SDR space, this mug is very far away from the spoon. and, but, with hierarchy, maybe you'd have this kind of concept that the two handles are nearby in SDR space, but the two heads are distant. it's, I feel I, I still feel uncomfortable with the SDR overlaps, right? I, maybe it's like Einstein being uncomfortable with quantum mechanics. So he just couldn't accept it. And he just tried to spend the rest of his life trying to prove it wasn't right, and maybe I'm like that, I've just gotten too old, I can't get it, but I still, want to write, just raise it, I still feel uncomfortable with it, so I have to, I don't have an alternate solution, so I'll have to think about it.

yeah, no, that's right, but yeah, I'll try not to, we're going forward with it, so you're baking it into the system, Yeah, but I guess we're baking a lot of things into the system while they work. Yeah. It can't, change. this is a pretty, it's a pretty fundamental idea, whether it's going to do this or not. And the, fundamental idea is what is the nature of representation? And that's pretty basic. So anyway, I, I'll go, I'm not going to stop him. It seems, it works. If it works, it works. I'll just be a curmudgeon.

yeah, in terms of, yeah, multiple objects, I'll just briefly touch on this. We have this move back on object policy, and this uses information. And so when you have multiple objects in the environment, basically the learning module just tries to stay on the object. It believes it's on until it recognizes it, and this uses information about our kind of most likely hypotheses, as well as where the learning module is on the object, so it's also model based in that sense. And yeah, our multi object setting currently looks like this, where you have a target object, like this pink Lego block, and then you have some other objects placed around it in space, and then you're cicading over the objects, but you may move on to other ones. Thanks. And, yeah, the kind of basic principle is as you're moving over the actual object, so in this case, the potted meat can is the one you're starting on, the evidence for that object is building over time, but if you suddenly go on to another object, in general, you're going to get a bunch of observations that aren't consistent, so evidence is here on the left, and so that's the steep decline in, evidence you're seeing here. And this is reflected, so on the x axis, these lines are indicating what the ground truth of what the sensor is actually looking at. So here it's looking at the pot and meat can, but here it starts looking at other objects.

and so the kind of learning module can internally detect change in evidence and retrospect that, okay, something's very different now. I'm now sensing something very different from what I was confident I was on before, and then initiate an action to move back to where it was. what should the goal be? Is the goal to say, I need to keep going back to the meat can or should the goal of saying, oh, okay, I'm onto a second object and I'm building a composition. Yeah, so we can also do that with hierarchy, but here the assumption is we want to like, because then, yeah, exactly, then you just say, okay, I was on a meat can, but now I'm on something else. And then in the higher level, learning module, it's going to now be representing that, okay, at that previous location, there was, And now we're looking at something different. There will be at least some instances where we want to figure out what we're looking at before we start looking at other things. and if it's a learning module, you can't learn hierarchy, right? There's no concept of oh, where is this object relative to the other object? Yeah, so this There are kind of two objectives. This one just shows detect, this plot shows detecting when we left the object that we were previously on. Then we have two choices. Either we reset the evidence and we start detecting a new object, or we invoke this policy that Niels mentioned and go back onto the object that we were previously on and try continue recognizing that object.

and it's a bit fast, maybe, but this is showing that in action, where we start on the golf ball, do a few cicades, and then get on the mustard bottle, and then immediately move back, because we realize that's something different.

and this gives us about a 10 percent accuracy improvement, when we move back on the object in this setting of kind of distractor objects that are on either side.

Cool. So then, that gets me to model based policy in abstract spaces. so in general, when I was thinking about this, again, it's very speculative, but it feels there's the kind of point Jeff was making before, there's, tons of instances of model based abstract tasks, but in general, when I was thinking about them, they don't really, they're not really directly analogous to classification, but there's definitely similar principles to this idea of like mentally rotating and aligning different representations and yeah. What you call it, focusing on the differences and the similarities, but particularly on the differences. you can imagine, for example, you have some taxonomy, structure for how to classify species. one might be based on, DNA analysis, another might be based on fossil records. And so there's two different, taxonomies. And if you align these in representational space, there'll be parts of this tree like structure, which may be. The exact same. It doesn't matter which system you use, it's going to, classify the species in the same way, or there might be, regions in this, structure that are very different. And it's, if one of these systems well and you're learning the other one, this is the system you want to focus on to understand it.

or similarly, imagine you're reading a paper, let's say on a machine learning technique, and you know it's a kind of sparsity paper.

the introductions are going to be similar, to jump to the early figure that shows their kind of main technique in a kind of abstract form.

and of course, these don't have a direct analogy in physical space. It's more of a kind of slightly abstract space of kind of the structure of the paper.

but then when you're scrutinizing the actual, algorithm, you're operating in more of this kind of mathematical space, where again, you're looking at the kind of similarities, aligning those, almost superimposing them on one another, and then scrutinizing the, the key differences between them.

that was everything I had on the, abstract space, unless anyone else wanted to talk about that, but then I was just going to briefly talk about, hierarchy.

yeah, so far we're switching to hierarchy. That picture says we're going to hierarchy in terms of, interacting with the world as opposed to inference and, Yes, yeah, bringing in hierarchy for policy. To our goal oriented behavior.

Yeah, and generally for most interesting things where we want to change the state of the world, we probably need some degree of hierarchy. but we could use, we were just talking earlier about hierarchy in terms of inference, so it plays a role there too, but we're really talking now, we're going to talk about hierarchy in terms of goal oriented behavior. I'm assuming, given the picture. yeah, exactly. Again, this is much more speculative. this is stuff we've had brainstorming sessions about and talked about, but it's not something we have any super concrete proposals for. We can imagine, for example, you want to make coffee, this kind of process can be broken down into these, hierarchical spaces, day planning, kitchen space, coffee machine space, and within each one of those, you might have a particular action you need to do that can be further decomposed.

and so far, the approach we're, thinking, or that's evolved is to have kind of goal states that a learning module can send. You can either send that to a low level motor system, Which would initiate a model free policy, and that's essentially sending stuff to subcortical structures to take care of simple tasks. Or it can send it to another learning module, which is more of a hierarchical model based policy.

and, that way, with the latter, you can break down complex actions into goal states that can further be decomposed into, by lower level object models.

And, so going back to the cortical anatomy, I mentioned the, direct motor projections, but, and those go from L5, but, L5 also, receives, top down feedback via the apical dendrites in L1, and there's connectivity between L5 and L6, so Maybe there's some circuit in there that can support a higher level cortical column saying, okay, you need to initiate this particular, type of, or you need to enact this particular kind of goal state, and then where appropriate, a lower level learning module can then, output its, requirements to the subcortical structure, which as you were saying, Jeff, is like the only actual output in the cortex, and so the only way it can actually change the state of the world.

This is a, it's a fascinating area. I'll let you finish, maybe, because I'm assuming we don't really understand this very well, but, it's, funny because, the, classic motor output goes up the hierarchy and the red lines here, which at first made no sense to me. You think just like in the coffee making example, you want to decompose actions going down the hierarchy. and, but the purple line there really is in some sense, that is your feedback. And so that has to be, if there's, if we're decomposing things going down the hierarchy, it's got to be the purple line there. And maybe it's like layer six, we're talking about it's like a location, so maybe region three says this is the location I want to be at, and somehow the column below it says, okay, that's where you want to be, I'll help you get that up. And then it says, oh, I want to be there. And again, in, in the, way we're talking about it in the, paper we're working on these three columns are co-align, so we're all looking at the same location in space, but that's not always the case. So it's, very confusing. I just wanna point out that these are some of the parameters we have to deal with, but it's very confusing. but I feel we can solve this problem. I think there's a really clean, answer to that. Maybe you've got something else to present. No, not really. I, yeah. The only thing I was going to say, yeah, in terms of like non. So just on this picture, I guess like it shouldn't, it might give the idea that we want to have a direct motor connection back to layer five, where basically higher level column tells the lower level column exactly how to act. But. Actually, it, just tells the lower level column the state it wants to be in, and then the lower level column uses its models to figure out how to get to that state. So we might not need that pink direct connection. it might be going through like layer four and layer six and whatever else weren't there to figure out. So that's correct. think about it. If you watch the purple line, the major connection from the purple line is up in layer one. And so it's basically saying to a whole bunch of other columns below it. You know what, this is the state we want to be in. This is this is in some sense, like this is the, you, all want to be in this location, or I don't know what the state is yet, but you wanna be in some state. The, little synapses and layer six A are more for the comms that are actually co-align. But you're right, we're trying to, the upper regions not telling the lower region what to do. It's saying this is the state we wanna be in. And, and it says, okay, I'll try to implement that state for me, and then it tells the guy below, okay, that's the state I'm trying to be in, and it tells the guy below, region one, okay, that's what I'm trying to be. But it's still really confusing, because all these have a motor output. One of the questions I've always had, we label these two red lines as motor output, but, is there a, are they equivalent, or is it more One is somehow, hierarchically higher than the other motor output. Do they both go to the same motor system, or do they go to some hierarchical representation subcortically of the motor system? if they were both going to, to the, superior colliculus, would one be, Implementing a higher level superior colliculus function and the other the lower level superior colliculus function. I don't know. Yeah, I guess you can imagine how, this might be the, kind of scene level representation of the kitchen, and you're trying to make coffee. And It sends, the goal state of having the coffee machine on to a learning module that knows about coffee machines. maybe this direct motor output is more like moving through space, as in walking. And then, so this one will get you towards the coffee machine, this will more direct your arm to actually turn it on. I think, yeah, I think there's a whole bunch of things in flow here. But yeah, there's obviously a lot of complications. this is like one of the things that bothered me for decades, like how do we understand decomposition of motor behaviors given that the cortex is the sensorimotor system and that it's output? And I felt like I never really felt I had enough information to go on to answer this question. But now I feel like we do with the idea now that this idea that we, come up with is that, the hierarchical composition is a point by point, compositional structure and, it's not an object to object compositional structure. Sometimes it isn't, sometimes it isn't. But. Anyway, with the new insight about colon, columns, and being able to represent the same space location in two different columns hierarchically arranged, I feel like all of a sudden this is the clue that is going to help us unravel this problem. So all I'm saying is I feel confident that we now have enough information to really come up with a very solid, compositional, hierarchical, goal directed, or model directed, theory of motor behavior. and it's, we have a long way to go to get there, but I don't think it's going to take that long. we have a lot of things to solve, but I don't think it's going to take that long. I think we have the information we need now. Yeah. And in terms of like non aligned receptive fields, like we were talking about earlier, a natural idea is that with this, kind of decomposition of hierarchical policies or hierarchical goal states, You would recruit the motor cortex where necessary, for, more complex motor actions where, these are essentially just learning modules that are specialized in, coordinating the movement of a body part. but yeah, how, what those anatomical connections look like and how exactly you coordinate that, it's all, obviously. Of, complexity there. Yeah. It's something we can work on and, maybe, it's something we can solve when we get together in, in January. How about that? Yeah. I feel like this would be a perfect, subject for the brainstorming week. It'd be great. Great topic to think about beforehand and really try to spend a few days just really working on it. we made progress on the whole hierarchical composition. Viviane and I worked on that, we made progress on that a while back and maybe we can do the same thing here.

Yeah, and so just bringing this together, so this is this overview diagram that you'll have seen where you have the, outside world with a mug on a table, hand, an eye, those are sending to the sensory modules, you have these motor modules, and then learning modules that are modeling objects. and then red are the kind of motor projections of some kind, and so the kind of model free policies generally exist in this domain where it's, just this direct sensory motor loop, and, yeah, not involving, to any kind of significant degree, the cortical areas, the learning modules, and it's worth noting that the input to a sensor module and an output from a motor module is not compliant with the cortical messaging protocol. but all these other solid lines, going throughout the system are including the projected goal states. And then, a model based policy is more when you have a learning module involved with, coming up with a, action that it's going to send to this kind of more subcortical motor system. And then a hierarchical model based policy is more when these learning modules are interacting together, these top down connections.

And then the last slide. I'm going to get a call. I'm going to get a call in three minutes, so when I do, I'm going to have to drop off. So keep going. Okay, yeah, this is the last slide, but just to say that, yeah, in terms of future work, so I think the quick thing to do that we've been wanting to do for a long time is just, better model free kind of policies for the distant agents, so just the cadence sailing features and the kind of medium features, so it's grayed out, is more exploration focused policies. We focused a lot on inference, and those, again, wouldn't be too difficult to implement. And then the more complex thing, which is why it's grayed out, is the hierarchical policies for the multi step goals, so things like setting the dinner table and all that kind of stuff. Making coffee.

But, yeah, as you said, that might be a good topic for the coming, by the way. you can solve it before then, great, but It's actually, my experiences, you have to think about these things for a long time and then they gel before at some point. Yeah. Yeah. That was a good summary of everything. Yeah. Thanks, Niels. That's pretty cool. Yeah, no worries. We have a sensorimotor system, we have to figure out the motor part, so this is where we are.

Excellent. Now we gotta do the parting and stuff, but if there's just one question I could ask, has to do with hypotheses and estimating pose. as I understand it, the current hypothesis system is about just object identity. So let's say I've figured out that I'm on a cup. Yeah. So it, does also test for pose. I, I tried to show that, but maybe I didn't make that. We have that in the code today? Did we build that? Yeah, I can show you a specific slide, that was, this one, so disambiguating the pose, so in this instance we know it's spoon, but it doesn't know the orientation of the spoon, so it's It first tries, so it has two, but basically, but in this case, to say it was a spoon, it already had to have, the orientation was either, it could be off 180 degrees. I guess the question, the more general question is what if the spoon was rotated 90 degrees? How did you recognize it? Or how did you come up with the hypothesis? We don't know for certain yet that it is a spoon, but we are, like, it's a lot more likely than the other objects. Like you could imagine, we've already sensed the top of the spoon and that area is not consistent with any other object that we have, maybe. I guess, it's not clear to me, it's how did you decide on the particular orientation when you started? I have no idea what the orientation is. Yeah, but so here it's felt the handle. I don't know, I don't remember exactly how I got this figure. This might not be the most realistic one, but because, if it's only felt the handle, how does it know it's a spoon? But yeah, to Viviane's point, let's just say it's How did it even get to that testing orientation, right? Because it's felt the handle, so it knows that the handle is oriented like that. But the handle could be reflected on that point. but what if the handle is not introduced to that? The spoon couldn't be oriented like this, because it's felt all along here. And so it knows the handle has that orientation. I guess the question is That orientation is consistent with this orientation. When you start moving, you have one point of observation. Then you don't know the orientation. No, yeah, but we're not already This isn't after one point. This is after multiple points of, But so when did So how Where along the way you were deciding the orientation and the potential object? You had to be testing different orientations, is the question.

yeah. So at the moment, it already starts with, many hypotheses for the possible orientations, if that's what you mean. yeah, it starts with I think that was the question. It starts with an exhaustive How did it do that? I'm not asking you to, I think that was the question. Yeah, sorry, Scott. Can you make a hypothesis driven jump? let's say we already know we're on a spoon, for example. can you make a hypothesis driven jump just to resolve the pose? Okay. Yes. No, it's a great question. And the answer is yes. So that's what it's trying to do here. So in this case, it's doing a hypothesis driven jump to this red location to try and tell if the spoon's oriented like that, or if it's like this. And then here it's, trying to disambiguate like a more subtle difference in the pose. is the spoon like this, or is it like this? And so it's jumping here, but no, yeah, it's a good question.