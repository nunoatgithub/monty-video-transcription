All right, welcome to the third video in the core video series. And now we're gonna dive into some more depth on the actual implementation and how we want to implement these modules and the communication protocol. So here I have a couple more notes to make. The idea is that in this core video series I'm gonna do some intros for all the older videos because It's been a while since we recorded them, and since then we tried a lot of things, we figured out some issues, and what we talk about in these videos might not be exactly the same anymore if you look at the codebase now. And we might have figured out some things as well that were open questions back then. So I'm just trying to give you a quick intro to have a bit of context, trying not to go into too much detail here. But hopefully I'm giving you a bit of information that helps to follow along and understand what we're talking about. So one big thing in this video is we talk a lot about features and pose of features and communicating poses and features. And when we talk about features, you might have a different idea of what we mean with that than we actually do. So features in computer vision are often like 2D edge detectors or some pattern or something like that you can recognize with a kernel function. When we talk about features here, features have three dimensional poses associated with them. And at higher levels, features are entire objects. So one of the core principles of the thousand brains theory is that each cortical column can model entire objects. And that means if a cortical column outputs something which adheres to the cortical messaging protocol. It outputs a feature, but that feature represents an entire object and its pose. So let me give you a quick example. Let's say you're trying to recognize this Numenta coffee mug and you have two cortical columns stacked on top of each other. The lower column models the Numenta logo and the higher column models the cup with the logo on it. So the lower level column gets input over time. It doesn't see the whole logo at the same time. We have a sensor moving over the logo and it's recognizing this logo through movement over time. It recognizes the features in the logo relative to each other. And once it recognizes the Numenta logo, it outputs the ID of this, the ID of this model as a feature, plus the orientation and location of it. And that becomes the input to the higher level model, which models the coffee mug. And in that model, the logo is a feature at a location within that model.

that sounds a bit complicated. I'll try to be brief. The main point is that a feature can be an entire object, a 3 dimensional object, and it has a 3 dimensional pose. How would that work at the lowest level? actually, the features that come from the sensor module. How do we get a 3 dimensional pose there? So what we do there is we use the point normal and curvature direction that are being sensed to define, the 3D pose at that location. for those of you who don't know, the point normal is essentially if you draw a line orthogonally out of the surface of the object, that's the point normal. on this coffee mug, the point normal would be like this here. If we get to the rim of the cup, it turns like this, and then on the inside it's the opposite. Thanks for that.. so the point normal is always orthogonal to the surface, and then the curvature direction are two more orthogonal unit vectors to that point, normal. So they point in the, direction of the highest and lowest curvature. So on this cup, it would be one of them would go up like this. So here we have no curvature if we go up the mug or down the mug, and then the other one would go around the mug like this, where we have a high curvature. So if I'm sensing the mug here, the pose would be defined by the point normal, which looks like this. One of the curvature directions looks like this, and the other one looks like this. And those three unit vectors tell us the pose of the object. at that location. And that's essentially the input that goes into the lowest level learning module, and that's associated with the features that come from the sensor module. for instance, we might get color, or the amount of curvature there, as well as features, and then the pose are these three vectors plus the location. And speaking of location, in this video we also talk about how in the brain there is no origin, grid cells don't represent an origin or anything like that. In our implementation we do have an origin, just because we're using Euclidean reference frames. But the origin we're using is arbitrary, so the whole object modeling and also recognizing objects works no matter where we set the origin. The only thing that matters is how features are located relative to each other within that reference frame. Okay, what else do we have? another thing we talked about in this video is, matching objects using displacements. and this is what we first implemented, and we tested it, and it had some really nice properties, like we could recognize objects irrespective of the object's location, orientation, and scale. but it also had some very big drawbacks, the biggest of them being you couldn't really sample new displacements. that you hadn't stored in the model, and if you wanted to store a lot of possible displacements to be able to generalize to new paths on the object, you have a combinatorial explosion.

we implemented a different approach, I'm not going to go into detail on that one, that will be presented in later videos.

what else do we got? Oh yeah, at some point I mentioned the idea of implementing a policy that essentially uses the internal models to move to locations that resolve the most ambiguity. And this is what's implemented in our code base as the hypothesis driven policies, or if you're more used to reinforcement learning terminology, model based policies. essentially, we're using the internal models of an object, plus the hypotheses that we have, to inform where to move next. for example, if I'm on this mug, I'm not sure yet if it's a coffee mug, or a cylinder, or a cup. I would saccade or move to the handle to resolve the ambiguity. and we got that implemented in our code base now.

what else do we have? Oh, one other point is, we don't implement separate where and what columns in the current codebase. A learning module can do both. It can do what a where column can do and what a what pathway can do. So this is combined in a learning module. That should be all for now. If you have any more questions and things are unclear, please just leave a comment and I'll try to answer it. But for now, just focus your 150, 000 cortical columns on this video. Cheers

I guess this is it. I was asked last week to hey, we need to, we need some more definition on this idea. And I don't really want to call it the AI bus anymore. I don't want to call it the CCP, so we need an answer for that. So I'm just not gonna call it anything right now.

one of the questions was what is, can we be more precise about a function of a module that didn't get input from the sensor? And so I tried to do that and, and I went through a series of thought experiments, some of which I'll share with you today, but what it ended up leading to is a set of, definitions that I'd like to propose. the biggest thing for me was clarity on a bunch of old ideas or ideas we've thought about over the years. and there are various forms at different times, and we never really could reconcile all these things. And we have this big grab bag of ideas we've focused on, different ways of doing things. And it seems like I have much clarity how all those come together now. And there's a few new ideas that come into it too, but primarily it's really about clarity and definitions. And so we have a common language and understanding what's going on. So I'll start just by jumping in some definitions. the first one is the word pose. we've been using it, Ben is using it in his document. We might not be able to say that you're testing stuff, but, I want to make sure that we're all using it in the same way. So Marcus and I like this way. So we're going to talk this way. and Marcus says, this is a common usage of it. So in this case, a pose. is the relative location and orientation between two things. So it's not just the location, it's not just the orientation, it's both. in a three dimensional world, we could say that's six degrees of freedom, right? Freedom of location, freedom of orientation. and, the brain is going to be calculating pose a lot between objects. It's just going to be constantly saying, what's the pose of this to this, we've had some discussions about how we want to represent this information. and I, we can have a long discussion about that. because you could think of it like Cartesian coordinates, you can do polar coordinates. The brain uses cellular codes, which is different than, like integers and, distances and angles, but it looks closer, maybe the brain is perhaps closer to polar coordinates but using a cellular code. So we, but at the moment we don't need to worry about that. We're just going to say we can specify the pose between any two things and we're going to assume the world is three dimensional. I don't know if the brain makes that assumption. I think the brain actually doesn't make that assumption, but we can make that assumption right now. So we have a three dimensional world and the pose of the relative orientation between any two things. So I can say, what's the pose of this pen to this coffee cup? I can say, what's the pose of this pen to this mouse? I can say, what's the pose of this pen to this room? and and so to do that, you have to have a, some sort of shared reference point. or shared, you don't have shared reference point. To do that, you have to have you just have, each object has to have its own sort of reference point.

Okay, that's what pose. Any questions about pose? Pretty straightforward, right? we're going to use the word body. It's a little weird to use the word body, but we've been using it already. It's just a shared reference point, related to your physical presence, right? you can just imagine there's some point in your body that's a reference point. and, and then I can say where is the pose of other things to that. So I can say where is the pose of my finger relative to that reference point, where is the pose of my toe. Cool. where is the pose to this object to that reference point, I could, it's just a, one that's shared, among a bunch of things. It's just, it is the sort of reference point for your body. There'll be other shared reference points along it.

all right, so now we can think about, a sensor. now, we're gonna define a sensor as, you can think of it like a patch of your skin or a patch of your retina. It's not the entire retina. It's not all your skin. It's a small patch.

key about the sensor, sensors have a location and orientation. So they have, they can, you can, just say, what is the pose of this sensor to something? I can say, what is the pose of the sensor to the body? I can say, what is the pose of the sensor to something else? But it is a, you can think of it as a point, a location, and an orientation. it's typically, it's a small patch of your skin, where you have a small patch of your retina, that kind of thing. But. we'll, get a little more detail about that when we talk about how it's processed. your body consists of a whole bunch of sensors, that are moving around, as like mine are right now. my eyes are moving, and my, the different parts of my eyes are looking different directions, and I have tens of thousands of sensors, at the moment, and they're all moving around. and they all, have, you can say they all have some location and orientation. And I could say what they, I could tell you, what is it relative to the body? So let's just start drawing that here. I'm going to, for lack of a better word, I could draw like a 3D, a Cartesian coordinate. But I'm not going to, I'm just going to say, this is a representation of my reference point here. So that's a location and some reference location and orientation. We can call this the body.

for now. And then I can have a sensor, which is just some thing out here. I don't need to specify what kind of sensor it is. It doesn't really matter. it's something that moves around. And so this is the sensor. And, I have a lot of those. There, there are different modalities, some can be skin, some can be touch, can be hearing, doesn't really matter. I got a bunch of them, they're floating around in space, moving around. and the first thing we're going to say is, we're going to talk a little in a moment. We're going to just, I'll just jump around randomly here. One of the things we're going to assume, I didn't write it here, oh, up here, I wrote it here. Yeah. Is, that our brain always knows, the pose of the sensor to the body, always. All right. I'll, I might have an object out here as we draw an object in green, something I might be sensing. That's a kind of independent of the body, a pen, paper cup, whatever. And, These sensors are moving around, but the important thing is at any moment in time, the pose of the sensors to the body is known for every sensor.

and I can explain that in a moment, but to do this in biological terms is quite difficult. If you think about, like, how does your brain know where your finger is relative to your body? you might have to know where the finger is relative to your joints, and your arm, and this, and then you calculate all these things, and, there's some error that's involved in this. You might use your proprioceptive system. There's a whole bunch of mechanisms. That the brain could use to figure this out, we're just going to assume that we know it. In an engineered system, we can say we know what it is, it's just here. and so we don't have to worry about that calculation, which is a sort of a biological calculation. and, so we're going to assume that every, sensor's location did, pose to the body that's known.

now, what does the sensor do, So I said it's a patch of skin or retina like that, they have location and orientation. We're going to define another thing here, and this thing's called a feature. And, a feature is something that a sensor can detect.

and the general property of a feature is it's got some measurable property about the world. And it too has a location and orientation. So regard, just think of it, it's a thing out there in the world. Whatever it is, we're going to assume it has a full location and orientation to it. So I've drawn it in the same category here. Now, so I might be seeing something and I say, Oh, at that point out here, there is some feature at some orientation.

and then, yeah. So when you say orientation and location reference point is the body? No, it could be any, it just, at this point I can say some, I can say the pose between the sensor and the feature, that would be, that's a legitimate pose, right? in that view, that pose, the sensor, the feature is on top of the sensor, but the direction is up.

no at this point, all we can say is, all we can say is the sensor has some location and the orientation, and the feature has, a relative location orientation to it. That's, the definition of a pose. This, is fundamental to the whole orientation. It's relative to the sensor at this point, to the sensor, right? You could think of the sensor as having some, a tip of my finger is some location in space. And some orientation at that location. But the orientation, that's part of the transcript. it's, all relative. As far as a point in the location. Think of it just like a, it's just a reference frame. So I can say, it's like saying, it's like saying, I'm going to arbitrarily decide this is north. Then I can say it's east, right? But it's arbitrary. I can just But is it arbitrary for every sensor or for the body?

that's my question. Everything has this reference frame. the body is a reference frame, if you will. Yeah. A sensor has its own reference frame. The feature has its own reference frame. Everything has this property. You could say everything has its own defined north. Okay. You could say everything has its own defined north and its own origin.

this is a pretty fundamental important concept. So if it's other than it's confused about, if anyone can help me explain this better or a different way of explaining it. I think what, what, he's asking is whether the feature has a local reference frame. I think you just answered that. It, does. I'm not using the word reference frame here, but, But, because reference frame implies, maybe it implies Cartesian coordinates, it's, basically when you said they had a pose. There's the, I think there's ambiguity when you say the word pose because it's relative to something. you could say there's a feature I only know the thing about the feature is that I detected, I know the location, but I don't know have the local. Coordinate system for the thing, everything you can use the word reference frame, everything has a reference frame and you can think of those three axis is moving around, right? So if I say, here's my reference frame, then anybody else has a reference frame that I can, then I can calculate the pose between the two. But it's an internal thing. If this feature were to move, like this, its reference frame would move, and then the pose to the sensor would change. if I just rotated in this spot, in any of the three dimensions of rotation, it would have the same location to the, section, but the orientation would change. And if the sensor went like up here, it might have a different location to the sensor, right? But if you thought of that as a reference frame, everything has a reference frame. And it's arbitrarily chosen initially. Like we used to say anchoring your grid cells or something like that. Just arbitrarily chosen what's north, and, where, what's zero if you want to call it that. But the north of the sensor is different than the north of the sensor. Yeah, Everybody has its own local reference frame. And these reference frames are moving around in space. Is there anyone else confused by that?

Speak up. I just want to clarify something. It doesn't matter if we have a universal coordinate system, right? Because everything is centered with respect to my body. My sensor is with respect to my body, and that's a known transformation. And the features that I observe with respect to my sensor can also be directly related to my body. Yes, Wherever my body is, that's the, center. Let's let's, I want to clarify that even further. The concept of pose does not require a universal reference frame at all. Nothing. It's just two objects, two reference frames, and they're relevant, and they're pose. That's it. I can determine that no matter where they are in the universe. It doesn't matter. The pose between two objects is determined. There's another object we're calling a body, and then it's going to have a, it's going to stay a special role, but it's just another object. There's nothing else. It's another reference frame. It's no different than this reference frame, than this reference frame. It's just another reference frame, and we're going to have, and we're going to be calculating things. We're going to use it as a special way, but it's just another reference frame. There is no universal anything in this, system.

Does that help? Okay. I started with this because I thought there'd be a lot of confusion about it and there's a lot of confusion about how you implement it and I wanted to avoid that, although I'm happy to talk about how neurons do this, which I think is really interesting, and, but how we would do it is also pretty interesting, but it doesn't really matter. It's the concept of a pose that's important. And the concept of a pose works in any number of dimensions. So it doesn't matter, we'll just assume it's three dimensions at the moment. Okay. So, but this idea now, this is a key insight I had here that I didn't understand before. that features also have a full pose relative to a sensor or relative to other things. There's an entire reference frame associated with a feature. it's not just, something. Now, imagine you have a sensor and like with vision I can say, oh this, and I have a reference frame for my sensor, I can say the feature is at some point In distance and location relative to me, but also it has an orientation at that point. and now some features, some sensors don't really give you enough information for that. for example, I might just have a sensor that just detects something solid here, but nothing else. I bumped up against something or something out there, which is my thing can't pass through and I know there's something there that will still work in the system, the, in that case the feature detecting really doesn't, it doesn't have a full sense of orientation I could. If I were looking at a surface, I could be rotating the surface like this. I wouldn't know the difference, and something like color may not have any sort of orientation to it. The most important thing about a feature is it's at some pose. it's a relative to other things. And, and this is really confusing because when you think about sound and sight and touch, you might, you may not realize that, but this is a key part of the whole thing. We're just going to assume there's a reference frame attached to every feature. And some features is very obvious why there is other features, it's hey, it's so fuzzy what that reference, the orientation of it, something you can't really detect it. But the general case is. There is a pose of that to this, or there's a reference to an attachment feature. If I have my finger touching an object, then, my finger generally is right up against the object, and so the, distance to that, or the, location of that feature, from, my finger is really almost coincident with the finger itself. but that's just a, that's just a special case of the, of this one. It's just saying, oh yeah, this sensor can only detect things up a certain distance away. By the way, it's been shown that if you wear like a thick glove, your perception of where you're touching is on the outside of the glove. it's not your finger, it's on the outside. It's like when you use a tool, so it's not that touch is always right on the object. it's, you have a sensor at some distance from that sensor and some low, some pose to the sensor there's a feature.

Okay, any further questions about that before we go on? All right, a few more seconds.

So one of the thought experiments I had was interesting. I was holding, this pen in my hand, this Muji pen, and not looking at it, but just running my finger, touching some feature on this pen, like maybe the bottom of the clip, the top little edge here or the side, and it's, and this is, in hindsight, it's not surprising, but it certainly was surprising when I thought about it carefully is that, I have a sense for what I'm detecting. For example, oh, I'm detecting this edge on the side of the clip. And I, of course, my sense is where is that? I can sense it relative to my body. I know where this is. the same touch of my finger here, It changed when I moved over here, I saw it's a different location relative to my body, and here it's a different location, but it's the same input on my finger. and then, and when I rotate my finger on here, my perception is the edge is still oriented in the same position relative to my body. So just by changing the orientation of my sensor doesn't change my perception of what I'm feeling, this feature is at some location relative to my body. I also notice if I roll my finger over, over a feature like this, I'm actually invoking different sensory patches, and yet I sense the same feature at the same position relative to my body. So I'm invoking different sensory patches, different sensors in some sense, we're actually now actuating this thing, and yet I have the same perception about where it is. oh, no, it's the same feature. Some people I remember back in the old days and old office used to run my finger on whiteboard I said, oh, how is it? I had the same sensation all the time, even though my thumb was changing. That's a reference. That's a little time and I also noticed it doesn't matter which part of my skin I can touch it with my finger or my other finger Or the back of this finger or with my arm or whatever I get the same sensation so you can think of it this way You Now, just imagine I'm going to have a whole bunch of, now we're going to start drawing a system. I wonder if I should give myself some more room here.

I'll just erase this for now.

Imagine I have, I'm going to draw like this.

And, okay, these little rectangles are sensor patches.

So these are different parts of my skin, if you will. It's got tens of thousands of them. That is feeding information into this processing element here.

All right, and so as I touch the pen with different parts of my sensory app, these individual columns, these individual modules are being activated. Yet I have the same perception of the, of what's going on out in the world. So we're going to assume that, inside these modules, they know the pose of themselves to the body. So models know the pose. module to the body.

And, so they, if you have my body and so I'm, I basically know this pose, I'm sensing something, I'm sensing a feature from the sensor and it doesn't matter, it doesn't matter which sensor is actually sensing that feature. and then what pose we're sensing the feature, if they're all going to report back the same thing. So basically what you're going to calculate here is, you're going to calculate the pose of the feature to the body. So that's one of the goals of these things, a module, it receives, an element receives input from a sensor, and it always knows the pose of the sensor of the body. And goal number one is determining the feature and its pose to the, first of all, determine the feature pose to the sensor. So it has to determine what this. poses, it knows this pose, and so it can calculate this pose. It's pretty simple. so goal number two would be to, determine pose feature body and this is why I'm, that's why my, what I'm aware of is that output of this thing, which is the feature to the body. That's why it doesn't seem to be changing as I'm doing this. And, and it does seem to change if I do this because it's moving relative to the body. So now, essentially now, if I touch an object with any of my sensors, any of my fingers, in any order, what I'm going to be determining is a series of features all relative to the body, right? It'd be like, oh, I can say what's the pose of this feature, and then what's the pose of this feature, and what's the pose of this feature, that kind of thing.

And now they would occur sequentially over time, as I touch one at a time.

Let's say there was some communications information here. I'm not, I'm called this a bus. I'm not going to say how this is working. I'm not even going there. I'm just saying I'm able to bring these all to some centralized location.

And, I'm going to put some kind of, memory module on here, something that can learn. And what it's going to get is a sense of features relative to the body, at poses relative to the body. And it's going to get those over time, and I could store those here. I could just remember them. So these are features to the body.

That set, as long as this object doesn't move relative to the body, that set defines the morphology or whatever this object is, all relative to my body at this point in time. So this set of things here, this is, it just says, yeah, okay, that's all the features that are out there that I've detected, they're all relative to some central point, therefore it defines the object, it's a definition of the object, but it's only in this case exactly at where it is relative to the body. If it moved or changed its orientation, then all this information would be useless. Is that clear?. So there's a different sensory modality of the same, when they say the term of feature. What does that mean? I want to leave the sensor modality out for the moment, because there's lots of different modalities, and they all measure different types of qualities in the world, right? And, and when I said a feature is a measurable property at some pose, I don't want to, I don't really care about what the measurable property is yet. That's my question. It doesn't really matter yet. So right now I only care about morphology, It's going to be more than that, but we'll just put it in the bucket of other things that it knows about that feature. No location, at that location. the feature has A pose and it also has attributes. I could say it's green. I could say it's rough. I could say it's I can even you'll see in a moment that I can even give much more definition to what that feature is But don't worry about that moment. The most important thing is it has a pose And there's some other qualities, and those other qualities will help us, but the qualities I get from vision are different than the qualities I get from sound, are different than the qualities I get from touch, are different than the qualities I get from echolocation, are different than the qualities I get from, whisking. It doesn't really matter. That is a distraction. It's helpful, it's going to be really important that the sensors are able to detect different types of feature qualities. but if I were to look at this feature and I were to touch this feature, they were both my eyes and my fingers would both report this feature at the same location in space in the same orientation, but they'd have other qualities different between them. The eyes might say, oh, it's blue, and the finger might say, oh, it's cold. And they don't, I can't share that information between those modalities, but I don't want to, I don't want to focus on that. That's not the important part of what a feature is. The most important part of the feature is its pose, plus additional qualities that we can make up as we want with different sensors.

That's a, again, a very important distinction. In my clarity of my mind that don't focus on those other qualities. It's really about the pose of the feature that matters.

That's a great question. Anything else? Before go on. Okay, so we can call, if you want to do a little neuroscience here this is part of the where columns at the moment, and this might be happening, half the entorhinal cortex, which is which I think would be the lateral staff would be the where equivalent right so that most, I don't know, it's hard to say. Yeah, it doesn't matter. Now, but of course, this is, all in some sense of, egocentric or body centric space, right? It's all representing these features and correlatives of body at any given point in time. We don't really want that. we do want it sometimes, but if I want to learn a model of the object, I want to learn the model of the object not relative to my body, but relative to an internally consistent point of view. So we can now define, an object. As, also having a common reference frame, and so just like the body had a common point, we're gonna find out, we're gonna make one up, but it doesn't really matter, we're just picking, pick a point, to do this. And, and now if I could define these features relative to that common point, then I would have a model of this object that's independent of the body.

we talked about anchoring, head direction cells, or anchoring grid cells, things like that. It's basically saying, I'm going to pick a point that says this is how I'm going to orient this. this is the reference frame for this common point for the object. Now I can relate all the features to that common point as opposed to the body. If I relate it to that common point, then I have essentially the definition of the object. The object is a set of features. all relative to a common point, a pose a common point. That is the definition of an object. So how do we get to there? we can get there pretty easily if we know the pose between the body and this common point here. So there's going to be a pose between the body's point and the object's point. If we know that, which changes as the object moves around the world, right? But if we know that, then I can then go back and forth between a, a body centric model and an object centric model. and so I'll just call this, this is a transformation that occurs from, it's the pose of the, based on the pose of the body, to the object, and We shift the camera to the right a little bit, please. Oh, yeah, sorry. Yeah, I can.

And, so now I can take every one of these features and transform them to a set of features over here, just by doing this transposition here, right? It's pretty straightforward. And now I have a list of features relative to the object. this assumes I know the pose of the body of the object. I may not know that, but if I just started out and I said, okay, this is stable and I'm stable. I'm going to learn this. I'm going to randomly pick let's say I randomly choose. A reference point, if you will, for the object, just like I ran into Joe's body. I randomly choose one here, and now as I go around and touch these things, I build up this list of features, that are all relative to the object. This is getting close, closer to what we did in the columns paper. where we had a set of features at relative, locations, on an object, but we didn't have the concept of orientation, which was essential. We missed that, and we amazingly discovered it only once Luiz made the robot simulator. It was all crap. We missed that.

now, this is all good, so now I've learned it, learned this thing, but what if I didn't know this now? Later on, I come across this object, and it's now in a different position, in a different orientation, And I'm trying to infer it. And I have this model that I learned earlier, but I can't, I don't know this transformation. If I knew the pose of the body of the object, I'd very easily be able to go and figure out what it is, but I don't know this. And so I can, determine these, I have a feature relative to my body, another feature relative to my body, another feature relative to my body. How do I figure out which one of these is over here? Some features are, just to give you a flavor, some features have such unique qualities. That I can immediately identify the object. for example, let's say only one feature in the world was pink, and I see a pink feature, then I go, oh, I know exactly where I am, because there's only one model over here that's pink, and I can look it up. so knowing some quality of the object, but, generally, features exist in lots of different places, and you can't assume there's anything unique about them at all. There's just a rearrangement of them which is unique, in which case, I can't really use that. I have to have some other way of doing it. So this is a hard thing to determine, given some observations here, how do I figure, basically I have to figure out what is the pose of the body to the object. And that's not so easy to do, just given the feature here, how do I know? we've come up with a second way of representing a model of an object. And the second way is the graph. that we've talked about a lot here.

and the graph basically says, I have features, but I'm gonna, I'm gonna learn the pose between the individual features. so I'm going to learn the pose between this feature and this feature, or between this feature and that feature, and I'm going to represent that in this graph. And that's what Marcus started proposing a month or two, a couple months ago. And we've worked with that idea. So that's a different representation. Now the nice thing about this one is the following, if I, let me clear up my diagram here, Okay.

We're going to remove the idea of the sensor from this, we're just going to think about the body now, because we've just abstracted away the sensor. I detect one feature at this location, one feature here, and I detect another feature here. And, in the, in this model here, by Blue, we've learned this, we've learned the pose between these two features, right? That's part of what the graph does. It's just saying this feature to that feature, that's part of what the definition of it is. Excuse me, the reference point. I don't, at this point, all, at this point, all that matters is that the edges of this graph are independent of the object reference point. Independent of object reference point. It's a pose of feature to feature. but the origin of that. It doesn't matter. There is no origin. It's just feature to feature. Each one has its own reference point. And which one has its own reference? Yeah. Every feature has its own reference. Everything in the world has its own reference frame. So think of it there, the angle and the distance of that blue line does not depend on where the common, the angle. The angle does not, but they must have their own reference. Yeah. this guy, everything has a reference you, can take. This is feature one. And it has this reference frame. And this is feature two and it has this reference frame. And I can calculate the distance and the angle between them.

it just, that's all you need to know. and so we have been able to now. So that will also, Help illuminate the set of. Yes, exactly. Exactly the point. The beauty of this system here is, I can't look at a particular feature and say, Oh, where is that over here? I can't do that. I don't know the pose here. However, I can look at these two features here and count from a body, from my body centric point of view, knowing that this is a feature of pose here. This is a feature pose here. I can calculate this blue line.

And that's, we've had a lot of discussions about this.

So you can calculate this blue line. and now I have something that's much, it's not, a guarantee, but now I'm looking for, I'm looking for, objects over here where I'm not only looking at some feature, but I am looking at two features in relative poses to each other. And that's a very much more discriminatory thing, right? Now, all of a sudden, that's really narrowing down the number of objects in the world may not narrow down completely, but it's certainly narrowing down a lot. And as I go through and calculate. And I, if I, started looking, I don't recognize the object with these two glances, and then I can do another observation and calculate another pose. And eventually I'm going to narrow down this, I'm going to figure out which object I'm in very quickly, might take a few observations. but I, it's guaranteed to work in some sense, because. Even if you have to do five observations, eventually you'll only find one object that actually matches all those poses. And, and so then I know what object I'm looking at. And once I know what object I'm looking at, I can calculate the pose. And now I say, oh, I inferred what I'm at. So it could be as simple as one observation, because the feature is completely unique. It can be as simple as two observations saying, they're not unique, but the relative poses of these two features are unique, or I just keep going until I get enough relative poses that I find a unique object that meets those poses. Wait, you're also just kicking the can down the road now. How do you recognize a feature now? Good question. Can we come back to that? Yeah. Yeah, this to me assumes a discrete set of features that you know. And in a real object like a coffee cup, you don't necessarily know, you may not always pick the same thing. let's come back to that. Okay. That's a very subtle point. I want to get to it later. And, and I have some observations, but I don't have all the answers, but I have some, I think I have some important observations, which will open up a lot of discussion. So let's assume you know that. Okay, for the moment.

so this is, now notice here, I have a system and what we've designed, a system that can learn rapidly, we learn by sensing one thing at a time using any one of an array of sensors, like in any number of sensors, but I can only observe one at a time, because I can only send in one at a time to this guy, but if I do that, I'm able to build, an object model in a way that I can infer it later by a series of observations as well. And, and that's pretty cool. Notice there's no voting going on here. At least no voting on object ID. I am, But isn't, eliminating the candidates a type of voting? not in the kind of voting that we've talked about, where multiple columns are voting together, right? There's, this is not, this is just an internal, you can Yeah, this would be within the list of candidate objects. But this is all one module here. there are no two, there's no, you'll see the voting as we wrote about it in the columns paper is you get multiple models of the same thing. And, but also within a column, if you just look at a single cortical column, as you get more sensations, you're narrowing down. But that's still, going on here. I don't call that voting we could but I think that would be confusing The way we describe voting in the columns paper and elsewhere is voting between models Two models, two modeling systems. I'll get there in a second. So it's the same operations with the, it's the same algorithm? I don't know. That's an interesting question. Is it the same operation? interesting question. I don't know. But voting in the sense that we wrote about in the columns paper does not exist here. We illustrate in the columns paper it's three different columns that have complete models and they're trying to agree. That's not what I have here. I have one model, And I have multiple sensors. I'm assuming you would have eventually multiple what modules. Yeah, let's go, we're going to go there next. Okay. We're going to go there next, okay? So at the moment, I'm thinking this is like a fast learning system here. And this you might think of entorhinal cortex, right? It's learning very quickly, you're saying, I'm building up this model very fast, and I'm going to recognize it again, like, where are the items on this table? And then I can come over here and say, oh yeah, it's the same table, I know where that item is. So I have a system here which says learned. Learn a model of the things in front of you very quickly, and and then you can infer from, you can build it up from any sensor, and you can infer it from any sensor, and you can recognize this object, and make predictions, and so on. There's enough information here to solve a bunch of modeling problems, but it's only one model of the world. There's only one model of the senses you have here. Now this is, this is all leading up to the next extension of this. So is that okay so far?

Wouldn't it be right to say that, the list of possible objects also gives you an implicit policy of where to look next. So going to the point that resolves the most uncertainty, basically. I suppose if you go there, I, you're, that's a mechanism that, that I didn't want to, I didn't think about here. if I'm trying to infer, like one thing we haven't talked about here at all is the, motor output end of this, like how would I direct these things to go someplace like this guy might say, Hey, It's just in a very abstract way. You might say, Hey, you know what? We're confused. But if you went to this one, if you observe this location on the object, then we'd be, we would know the answer. In which case you could say you could tell one of these people, go observe that location, tell me what you find. So that's an outer loop sort of intelligence. Is that what you're thinking, Viviane? I think that's what you're thinking. Yeah, exactly. there's a lot here I'm not discussing right at the moment. There's no motor system here. I'm just saying if I make these observations, if these things move, this is what would happen. But there is clearly going to be much more going on top of this. But I'm going to go in one direction first. You might answer that later. I think, towards what Subutai asked, but we're assuming in this model there is certainty where feature is relative to body. What happens when we add uncertainty? I don't assume there's certainty. There's always, there's no certainty, to get. you could have a noise associated with each other. Not just, not even just a noise. Take, for example, nothing is ever precise in the world. There's no infinite precision of location. If I were to think about, let's jump into biology for a second, right? One of the ways that biology represents location is these vector cells. And a vector cells you can say you have some point which is some sort of preferred orientation. I'm showing it to you in 2d Okay showing 2d and that around now you have a bunch of cells that are active around that and you say this cell is active and it's at some angle and distance from this point and then you might have another cell like this. We've seen this diagram. These are vector cells, and then you say this cell is active it's at this orientation, but it's the one speculation is that as you get further and further out. These, things get bigger, right? So if I were to say, oh, this cell is active, then it's anywhere out here. I don't really, but from a point of view that this guy, there's no further distinction says that's a location and it's a fuzzy space. it's not an error in some sense. It's just the resolution of space gets smaller or it gets less as you get further out. so there's a, distinction between, this is a real subtle point. There's a distinction between. My certainty that it's this location. And what does that actual location represent and space actual physical space might be quite large, but from the perspective of this module, that's all one place. There is no further distinction side of that, which is, that's, I have a cell represents that's what it is. So it's a subtle point. I don't want to go there. I think one thing that's important from a centrist point of view. If I, if my fingers touching something, it's never going to be like, I think it'd be over here. That can be over here. No, it's going to be right around here, or to my eyes point of view, if I'm eyes poking, pointing here, it's not like the feature is going to be behind my back, right? It's it's going to be somewhere in this direction. It's not oh, like the union of ideas where this. feature might be relative to the sensor. There's going to be some, a cloud of possibility, but it's not going to be like, oh, it could be here, There's no union of orientations and there's no union, could be union orientation, but there's no union of location. It's the sensor knows, oh, this general direction, it's not like it doesn't know that. But what if it's conflicting? what if it's uncertain? That would be really weird, right? What if all of a sudden, Your eyes are looking this direction, but for some weird biological reason, they start detecting things over here, you'd be totally confused. It would just like the whole system would fall apart. So I think there are some physical constraints here. it's interesting question. I don't even want to go there. If you want to go there, I think I want to stick with what I've stated so far. Are there other questions about this so far? Because I want to keep going into the rest of this here.

I think the concepts have to be clear. The implementation details, maybe not, but the concepts should be clear. Okay, these are like where columns, but the theory that we've had in our papers and the theory of what we know about the brain is that they're also what columns associated with where columns. And so what I'm going to argue now is that associated with each of these, that each of these can also build a model, very similar to how this builds a model, almost identical. Okay. The difference here is, these models, if I think about a brain, these models are going to learn very slowly, and there's a good reason for that. we don't want these models, these would be like V1 or S1 or something like that. You don't want to learn things really rapidly here, because you're going to see in a moment that other things are going to rely upon it, so you don't really want to do that. What you want to do is say, I'm going to learn models of things that occur consistently over time, for a long period of time, that I know are going to be around for a long period of time. Not some temporary arrangement of stuff, not some thing for today, not for tomorrow, but more consistent reality, things are consistently in the world. and so I'll learn these will learn slowly and but they're going to do the same thing as over here. You're just not going to be rapidly learning A good example to think about is when you learn the letters of the alphabet. When you first, as a child, you learn the letters of the alphabet, the letters don't look like anything, they're just like gobbledygook, right? And, and someone has to say, there's this thing called the down arrow, and then there's like another thing like this, and we call that a B. And so you look, oh, I have this feature plus this feature, that equals a B at some relative location. And then, you learn that another letter is yeah, this is a D, it's like that, but it goes this way, okay. Okay. And, but over, and so in the beginning, you have to really think hard about this and try to learn your letters, you have to think really hard about it. and they have to practice over and over again, because it's not obvious, they can't just see it and recognize it. They have to see that these components and their sort of ability, and you practice this over and over again, and slowly you build up these models down here, of what these things are, and, sometimes you, can learn it quickly here, I can say, oh, today I went to kindergarten for the first time, I learned, the first four letters of the alphabet, I learned that quickly here, but ultimately we're going to want to learn it down in here. and you just have to practice over and over again. And so I'll just leave it at that for the moment. So these guys here, now we have multiple models. Now, how would they build the model? Exactly the same way as here. They have the same information. They have, imagine, I am now, one sensor at a time is getting input, and that sensor posts up here, a pose of the feature to the body, right?

and, that information is accessible to all these people. And so everyone can learn a model at the same time. I, my fingertip is exploring an object, but the next sensory patch is over, all can have access to the information, they can be learning the module, the thing at the same time. so I'm only touching with this finger, but other parts of my skin could learn the same model, because they all have access to this pose, the feature relative to the body, they can convert that as a pose, as a feature, they basically can build a model from that. It's the same information that this guy's using and the same information that this one's using. but we're going to do it slowly, but that's an implementation detail. but just saying we can all learn. Now, here's a very important thing. When you're learning something new, you can't grab it with all your sensors. that won't work. That only works when you've learned it. And when you're learning something new, you always have to narrow down to some piece, thing you actually can recognize, and then do a series of those and build up your, graphical model.

So, only after you have, you'll see at home, only after you have many models can you grab them with one hand and look them in all their eyes, but in the beginning, if you see something really odd or new, you just have to look, you have to attend to each individual features in order to build up that model. So that's, imagine that's what's happening here. So now I have, models here.

just like the model up here, there's really no difference. It's just multiple copies of it, of the same thing up here, but these, in some sense, these are a little slower. and, and each one now is, associated with a particular sensor. So this, I've used a lot of resources here. I've just replicated this for every sensor patch. I now have a complete model, which is like really expensive. That's why primary and secondary sensory and motor cortices are so big. because I have lots of models of the same thing, down at one level, in some sense like that. before I go on, any questions about that?

You don't have the what columns? This would be a what column? Yeah, but you don't have them collaborating on? I will, in a second. Okay. So now this is where voting can come in, right? We talked about voting, as you have multiple modules that are, so imagine I have some input here, and I have some input here, or some input here. After we've learned all these things, and maybe after we've learned the letters of the alphabet, and now I have a bunch of sensory inputs coming in at the same time, Each of these systems says, Oh, I see a feature at some location. I can now trying to figure out what, my model, what, I'm recognizing. Everybody has uncertainty about that because they only sense one thing. but they can now vote in the way that we've described it. In the, in the, columns paper, they now have the ability to vote, and so they can say, okay, we all have some features at some locations, and now we have enough information together that we can say at a glance, we can all touch the cup with our fingers, we can all look at the letter with multiple columns in my cortex, my visual cortex, And with one presentation or fewer presentations, I'm able to resolve which, which object, which graph I'm looking at. What do you gain from this? The first thing you gain is much faster processing, right? Prior to this, to recognize an object, I'd have to thoroughly go through the features. That's how I learned it. Learning requires that. But once I've learned it in this system here, I still have to do that to recognize the object. I still have to go through one at a time, building, seeing which of the graph elements, and it would take multiple presentations to, to figure out where, what object it is. Here, I can do it with one presentation, one, potentially one, but certainly fewer than this. We made that point in the Columns paper. That's one advantage of this system, but at a great cost. Because I now, have many copies of this model. Yeah. What's the information passed on the green lines? I don't know yet, actually. I'm not thinking about that level. All I'm thinking about at the moment is that there is enough information here to resolve it. so we can say, there could be the union of object like things. But, that's how we talked about it, this is the voting mechanism, we have in the columns paper, that would work, but I don't really know yet how to incorporate the orientation, relative orientation component of this. I don't know if you thought about that Marcus. we've said, your fingers know where they are relative to each other, and that helps you know where you recognize the coffee cup. I haven't worked out those details yet. Okay, let's just say though, there's, we have one part of it, which is the voting mechanism in, In the, in the columns paper. That's an important part of it. But even that, we'll talk about object state in a moment and it doesn't represent object state. So I don't know. Can we just leave that? For sensations there, as you've shown that, yeah, you have four choose two six pairwise things that you can calculate.

the blue lines, what you call relative positions of the features on the object relative. You said four, choose two, four. Choose two. Yeah. All para, oh, all pairwise combinations. Yes. Yeah. So you have six. Six of those blue lines now, you can send back to them. Yeah. You know what those, six of them. Yeah. I think, that's right, I think with the So that's information you can use to Yes. Narrow things down. I think, again, I'm going to put that into the implementation detail here. I'm just trying to figure out who knows what and how, what you can do with it. so there is enough information here to much more quickly resolve, what the object is. And as soon as these objects are in the graph, then you can work out mechanisms to make it happen. but I think there's a huge amount of biological evidence to support this idea. one of the biggest problems I've spent my life trying to understand is how it is we can recognize an image at one glance. where I knew that vision was not like that vision is always and everything is it's it's like moving your sensors and in touch. You can't even imagine what one glance is, but I can say, yes, it's like grabbing this coffee cup, but it's not an image. How does that work? How can this grab be similar to this grab? How does that all work? this explains that. It's like it says, here's how you can infer an object with multiple sensors in one sensation event. even though the object had to be learned sequentially, a feature in sequence. You can't learn an object just by staring at the letter, you have to look at the individual pieces, you can't learn anything like that. So that was one advantage here, very fast recognition of something.

just a little bit of an aside, You're going to build, okay, this is an aside, it's not important. If you're going to build a practical system, the speed advantage of this comes out of cost of memory, and training time, memory. what if you could, in a brain, the fastest you can do one of these things is like 100 milliseconds, like how quickly can you move your finger or your eyes or something like that? But in a computer, you might be able to do this really fast. And so you might be able to infer this object with really fast sequential observations. cameras out there and they say, okay, I got rid of it. but in the brain, things are slow. So this is, this gives you a real advantage. if I, if going from to understand something with one glance versus having to look at three or four, which could be half a second, then that's the difference of life and death. But that's all. You're going to get something else from this, okay? What else do you get from this? So imagine now we have a, we have another, modeling system here. This is going to be secondary cortex. This is primary cortex, all right? And as before, this module here is it's, it thinks it has a sensor. It's it says, I have a sensor. It has this, but let's for the moment assume that sensor is not a physical sensor. What it's sensing is the output of these guys, right? So this is like the, you're passing up the hierarchy.

and this is something a little fuzzy. I haven't worked through all the details here yet. But, think about it this way. This sensor is, again, trying to figure out there's some object in the world at some location relative to my sensor.

This green thing here is an object in the world. it's, everyone agreed down here that, hey, this is the letter A, or this is, the coffee cup, or whatever it is. This is a, this is an ensemble of features, and it doesn't matter where these sensors are moving around on the object. I could be moving my fingers like this, or my eyes can look at different points. The output of the green line is stable. It says, yep, that's coffee cup, yep, that's the pen, whatever, that's the letter B. And so this stuff is all changing, but this stuff is constant. And so this says, there's a B out there, or there's a coffee cup out there, and I can, it's, it doesn't, it's, position to this sensor isn't changing, even though my, sensors were changing, but the position to this sensor is not changing. It's just there's a doesn't matter. My eyes move around. The B is that location. I don't imagine the B moving as I'm moving my eyes around or I don't mention the coffee cup moving as I move my fingers around. you can think this isn't sometimes a sensor where the feature is a more as a, composite object. It's getting this input from these guys, and the feature is composite, a more complex feature, a composite feature, if you will.

and. This guy itself cannot know any of the details. If it's saying, this is the letter B, it doesn't have any knowledge of what the details of a B are. It just says, I, this is a feature that I detect consistently, and we're going to call it a B. or just, it's going to be what it is. But it doesn't itself, doesn't know anything about the detail. it's just That's all you can say. It doesn't know any further spatial discrimination in, it's there's a feature at some location and pose relative to me, so it knows the pose of that feature, but there's no further spatial discrimination possible. It's a very kind of odd concept. It's these guys have further spatial discrimination, but this guy doesn't. Says No, it's just, it's that thing. 'cause I've seen this pattern and this pattern's coming in and I get another pattern and I get another pattern and I can, now I can build up a model of whatever. So these were, if these recognize letters of the alphabet, they may be here. I'm recognizing words and right. And words are certain combination of letters in certain position relative to each other. And instead it says, yeah, I recognize the word dog. But at this level, I don't know dog has a D and an O and a G. It's just a thing, that I observe. Down here, they can say, yeah, I know a dog is composed of a D and an O and a G in relative positions.

So this led me to the idea This is the first time, we talk here a lot about composite objects and, how you represent, things as part of things as part of things. And I've always was hoping that composition could be defined hierarchical composition can be defined within a module. and this view says no, you have one level of composition in any particular module. the letter, the letter B has these components that are in this arrangement. But that's it, can't represent another level of, abstraction. I have to go up here to have this guy say, you know what, I know these, now, these components, I can see that they are arranged in a certain way, and I'm going to form a representation of the whole. So this is the idea that, that. There's one level of composition at each level here, and it's not like this guy can have a model of a, a word and then also a model of a letter and then the individual components underneath it. I don't, maybe that's true now, but I'm going that direction, which kind of implies a limit to how many levels of composition you can have based on how many levels in the hierarchy you're going to go to. here I have, one level of composition down another level of composition and then I got another level of composition.

but this is the first time gave me a sense like intuitive things I knew what to be happening but for the first time I had the language to describe what a higher level cortical column is doing. It's detecting a feature, which, in this case, is something that, it's an output of these guys. It's at some particular location, which now forbids this guy to know any of the details. It just doesn't know it. it can't. and most of the time, my conceptual things I'm thinking about are pretty high up here, and I'm not aware of all these details down here, but if I want to drill down, I can. there's some way we can come back down here, and we can, and I don't know how that would happen yet. But at least I have a language to think about thinking about it, we come back down here and say, Oh, what is that actual feature? Where should it be given this and given the pose and so on. By the way, the. Yeah, this is it.

that's the, that's all I want to say first up here you're going to have multiples of these doing the same thing as well.

And they're going to be voting as well.

at a more abstract level. You can see now why you don't want to have this thing changing very often down here, because if you change this, then you're changing what features this guy's on, right? This guy's based on what this guy's outputting, so if I fundamentally change all the letters of the alphabet in some sense, then all my, then, there are different set of features down here, then a different set of objects, excuse me, if I want different sets of objects down here, then, then this guy's going to be completely confused. But you just can't do that. So this guy has to be pretty damn stable. otherwise you have to retrain the entire hierarchy and this is we see this kind of problems if you've been trying to learn You know something that's fundamentally new Components it can be very it can be difficult. I can never learn chinese And now I have to learn to recognize Chinese characters, which I can't, but I could look at the features and try to remember them, then I might be relearning all these modules down here and I'd be forgetting other things and, the world can get complicated that way.

So, that's why these are pretty stable. It also, it implies that this object you recognize down here may not actually be real objects. It could be things like letters that again, there's either we're trying to do things that are statistically occurring often. this is just an idea. I haven't thought about, but what is a cylinder.

a cylinder is a set of, features that sort of, that transcend any particular object, but they might be a consistent set of filters, features that I detected that there's these round things that are similar, so I might learn something like a cylinder like feature here. It doesn't actually have to be a thing that you can name. It's just that there's a statistically correlated, relationship with features that I see often, and therefore I'm going to learn to represent them as one thing. and that doesn't have to correspond to, something physical like a letter or cup, it could, but it could also correspond to things that are just sub components, types of shapes or, things like that, because it's just going to be statistically learned over time, so that changes that.