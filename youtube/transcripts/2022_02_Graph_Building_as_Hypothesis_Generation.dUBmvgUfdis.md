so, um, I have three kind of goals in this presentation. I want to talk about an upcoming challenge, discuss one possible way we could consider tackling it, and, um, some ideas that that solution, possible solution led me to. So, um, there are many ways to think about Monty and I kind of just wanna introduce another lens through which we can view it and chat about it.

So here's the problem. Um, for our initial demo, we're basically assuming we know the boundaries of objects. We have an object in an empty. Void. And, um, we basically get a label that says, are we sensing the object or are we sensing just the void? So that's fine for kind of getting the ball rolling, but, um, it's a challenge that we'll need to solve sometime down the road. And, you know, if we can solve it later, then we can fall back and all the stuff we're developing for the initial demo. So these little green things are supposed to represent, you know, a label that says, all right, I'm sensing the momentum mug. And these red observations are not on the mug. So of course, in reality, uh, the observations will look more like this. You just have, you know, little bits of sensory input and you don't know what they belong to.

So, um.

It's not as simple as that. Um, but I'll let you keep going. I mean, I mean, if you think about, oh, I have this image and I'm probably in the process image, it feels like that if you're thinking, again, three dimensions, whether you're dealing with touch or, or three dimensional vision, then it's, the problem isn't quite like most people think of it. Well, you'll you'll see plenty of examples of that. Yeah. Okay.

Um, yeah, so this is, this is the problem I wanna solve. So, um, another way to phrase this is that just based on this image that you see here as an example, um, from just one static image, I can't really tell where one object begins and one ends. I can't reasonably say that these are two objects versus one. Um, but using a depth sensorimotor, um, or even a touch sensorimotor, I could reasonably build something that looks like this as a model of my sensory input. Now if we take that real sensory data away and see this from the model I view, this is what we're left with. So, um, yeah, this is my point. I don't think from just this sensory data, you can know that these are two objects in that one.

Now, how do we know in reality that they're separate? Uh, one way we can do this is, uh, if we actually run our finger along the object to start sensing it, you're gonna move the object a little bit and you're gonna notice that one object moves and the other one doesn't. And when you see that they move independently of one another, you now have some data that suggests these are two objects, not one. But if I'm moving my finger over this object, I, I can't move it over both objects at the same time. So the, the confusion only comes about when I jump from one to the other, right? I mean, or, or are you assuming I have two hands out there. One touching one, one touching the other. If I'm just moving my finger object, I won't, I won't occur. I won't run into you. I wouldn't even know about it. Um, so I don't need to move it. Right. I, if I'm just following the contours of an object with my finger, um, I wouldn't know about the other object unless I go down and put the table and up on other side. Yeah. Well, so that's, that's what I'm assuming actually, is that like when you, when you sampled the object, you sampled both with your finger. 'cause they're, they're touching in the image that I presented, but this is an image. I'm saying if you're touching, that's not the case. Right. Um, Jeff saying like, if you, if you touch this and, and it moves, you don't know that this has, well, touching this, I'm just saying I don't have to have it move. I'm just saying when I'm touching this, like this, as long as I'm doing this, there's no, I'm not even touching this one. So I don't even know that it exists until, and the only way I might do this, I come down on the surface and go over here and then go up on this surface. Right. What I'm trying to say is, in this example, there's no space between, it's like this. So they're really right next to each other, like they're next to each other, because that's not what it looked like in the picture. Okay. I should have made that clear. The, the idea is that you can't, you don't know if they're glued together or not, but they're continuous, they're touching, they're part of the same. Okay. Well then, then why do we even think they are separate now you're saying, oh, well, because one's moving relative to the other one. That's what I'm saying is from, just, from just raw sensory data without seeing the object's behavior. You don't know that they're separate. You, you could just say same. Okay. But Okay. But the image implied that there were like two objects on the table. But you're saying there's really are sort of glued together in some sense. Yeah, they're right next to each other. They're not glued together, they're just touching. Yeah. But you know, one could argue like the stapler, the top moves well to the bottom, but I don't think they're separate objects. Right. So even that's not sufficient here. Um, so maybe that's like the second point. You need to understand the.

Yeah, that, that is kind of the, the, the point that I'm making is that in, in the stapler case, and also in these two objects, right next to each other case, um, the behavior is, uh, so I'll, I'll even challenge you on that, on this because it seems like, you know, when I first started learning things, you know, um, I don't, we don't learn things that are like this. We literally isolate things as much as possible, um, so that we have no confusion about it. Like, I'm teaching you to read letters of the alphabet. I don't show you words. I won't show them. I mean, we show a letter on a white space and there's like, that's it, right? There's no, there's nothing other, I don't show two letters butting up against, so that's the A and this is the B. So in some sense, so the question is, you know, once we've learned those things, there's an inference problem. But that's not the same as saying, oh, these things, these things are writing against each other and I have to learn one from the other. Because inference is different, right? Inference is like, oh, I, some sensations I can, I jump out as Oh, I know what that is, but this is not part of it. Um, you know what I'm saying? So I think we have be careful here. Um, you know, are we talking about inference or learning? Because I don't think we can learn the world very well, especially when you're young. Um, as a series of things all, you know, bunch. We need to isolate them. Yeah. Okay. So for, for this presentation, let's think about, you have basically no knowledge of how the world works. So you haven't learned anything, you haven't learned anything, or I'm kind of thinking about how we're gonna train Monty and, um. You're right. Maybe one way to train it would just be to isolate objects and not deal with this. But I think that's what, I think that what it's almost like to have to do that, uh, in some sense. I mean, you can have multiple objects around, but your, your idea that these are actually physically touching each other is, is, makes it difficult. Right. Um, because how do I know that these are not one object? That, I mean, that's the challenge that I'm I'm, well, I'm saying, I'm saying that maybe it's an unsolvable challenge in terms of learning and that you just don't wanna try to learn the world like this because it's not gonna work well. Okay. Well, it may, that may very well be the case. My goal today is to present a way to learn despite this challenge. Okay. And I'll see what you think about it.

Um, so to me it seems that by, you know, if you see that the two objects move independently of one another, that would be a signal that it's two objects and not one. So, um, with the exception of things like staplers, right, with the exception of things like staplers, I, this whole presentation, I don't, I haven't not, not yet integrated the idea of object state. Okay? But what I kind of think is happening here is that you basically, you have a model, this graph model, and it's a bit like a hypothesis. This is how I think the world works. And implicit in having that hypothesis are predictions that your model makes. Um, and you can use actions to test the predictions of your model. So let me run through what I mean. So again, I'm gonna be pick here. Uh, if I have a model, it means I've already learned something, right? What have I learned at, you know, a moment ago I said, okay, we're gonna assume we learn nothing. We didn't know anything. And now I have to learn a model and I got these objects touching into how am I gonna do that? But, and I said, Hey, if you already have models of the world, that different problem. Yeah. So, I, I don't mean models that you previously stored in memory. I mean, online. I went around and sensed some stuff, and so now I have a model that I'm building up on the fly. But I don't know what it is. I don't, I don't, it's not, I wouldn't call it a model. I, I have a set of observations that, right. A model to me is something you've learned, right? I mean, that's how I use the word. It's like I've learned it, you know, I've modeled a cop, I've modeled these classes. It's, um. It's not just a, a set of observations. I don't understand yet, so, okay. But what, what would you use for the term of, uh, you know, that collection of displacement? So I know the relative, I have a set of observations, a set observations. Right. And now my set of observations cannot make predictions because I haven't learned anything from 'em. Right. It's just a set of observations because I haven't learned this in the past. I don't have, how could I make a prediction about the future? I mean, you've still learned a graph, a short term graph. Well, then you've learned something. I mean, either you've learned something or haven't. Right. Well, then what if, if that's learning is building a short term graph, what would you call it when you, okay, if I were a short term graph, then the only thing I can predict or the, the observations I just made, I can't anything else because it's, if it's not stored. Best I can do is like some sort of temporary memory of like, I've been doing this, this, and this and this. Um, if you wanna call that a, a model, fine, but it can't make, it can't make predictions about what's next because it has, you observed it, it doesn't, it doesn't have a bigger model. Typically I have a model of something and then I observe some subset of it, and then I can infer the entire object, right? Um, so in this case, I don't have a model. I just have a set of observations. And I suppose I can make predictions about the observations I just made. If I said, oh, I saw A here and then B here, I can predict they'll be back, but I, I can't predict what would go here. I can't predict what's C over here. How could I, there's nothing, there's no model for that, but, but even if you see a new object, you can still form hypothesis about that new object and how it will interact with the world. Uh, if I, if I a new object, if I don't know what it is and I just, I don't have an object. All I have is an observation, right? All I have is a sensing observation. So, so again, if I haven't learned anything in the past. I don't know how I can make predictions or have an hypothesis about something. How, how about this? I, I don't know what this object is. I just have this collection of observations. Yeah. These relative displacement and the prediction that you could make is if I pick this thing up and move it, the whole thing is gonna move with my hand. It's, it's sort of so simple that it's a little ridiculous, but, um, that's, that is a prediction that you could make with this model, I think. Uh, okay. Um, sure. I'll go for that. Okay. So here's what I'm, I'm getting to is that when you try that, you're gonna see that now you've got two objects instead of one, maybe here's, here's my phone and a eraser on top. I pick it up and I move it. So I think that yes, you can construct counter. I have, well, that's the point of these arguments, right? Is that pro and probe. Um, you're right. It doesn't, it's not, it's not a foolproof strategy. Okay. But it is a, it is. It is possible to knock the eraser off the phone still. Yeah, yeah, it is. Okay. So now I have different sensory input and I even have that there's a, my death Sensorimotor shows there's a gap between these two things. Um, the idea I'm kind of trying to get to is, uh, this iterative loop of a, like building a model. It doesn't have to be of a known object, but it can be gather instead of model. I'll say gathering a set of observations, predicting how they should behave, and then taking an action to test if my prediction is correct or not. And then using the outcome to update the model or the, the observations.

Um, I don't think anyone is terribly sold on this idea yet, but as a comparison to how we tend to train deep learning, I think it's a pretty interesting alternative. I. Instead of just training with cro, um, and doing something like the following, the loop would look more like I take a bunch of actions to sense the world. I'd store those observations in a buffer of some kind. Um, and then I make predictions and select a new action in order to test my predictions, um, actually execute the action and then I update my belief based on that test. Is there any other action and test that you think would work other than I'm trying to separate these two objects and move one and the other one doesn't move? Is that, are there any other sort of predictions you're thinking about or is that it?

I have not thought of others, but I'm, I'm guessing this idea would extend to other things. Oh, no, I'm trying to understand because, you know, select and action make predictions. Those are very generic terms. Right. And, and I was objecting to it because I was saying, but you really can't make predictions about world that you haven't learned anything. Uh, and then you came up with the example. Oh, well, I like to predict that when, you know, I'm trying to do separation. If I move one thing the other, because the other thing move, I, okay, that works, but it doesn't seem like a, I don't know if there's a more general version of this. Uh, so I just be careful because we're using words that sound like there's a model to make predictions. Um, but it's not really, not really a, I guess the hypothesis is that a bunch of things, I sense now that they will, if they're separate objects, they, they'll be separated. And another example, squeeze something. Okay, squeeze this. But how does it, what does that tell me is, does it, it tells me these observations I had flexible. Yeah. But does that tell me figure ground separation or object separation. What do, alright. I'm just working on Ben's hypothesis as well. I mean, if you squeeze it, everything that moves along with it is part of the same, could be part of the same object. They're connected. I guess that's a little bit, but yeah, it would be similar though to the idea that it's the behaviors of objects that you need in order. It's, it's not just like what the object is, it's how it behaves. Um, so you can kind of, like, you don't, you don't know the language coffee cup for this thing. You could, but you could still learn how it. Assign a label to it in your own kind of language and know how it works. You can know how it works. You need a model, right? You just, you just gotta have a model. I mean, some sense, you're, you're arguing, you take a temporary model by assuming a bunch of observations are all together, and then if they, and if one moves and the other one doesn't, then the assumption is that they're not altogether. That's pretty much the, the gist of what you're saying. But it's not a general purpose prediction. Right. I can't predict how any of these things gonna, I can't predict what the other side of this coffee cup's gonna look like. I can't predict what, you know, other things because I don't have, I just don't need prior knowledge about it. I just have always saying, I got this current knowledge of just one viewpoint and I haven't built up any other stuff. Yeah. I would maybe just think in terms of not predicting parts of the object you hadn't seen before, but I would think about predicting behaviors of objects. But again, what other behaviors could there be other than. I mean, objects object behaviors have to be learned too, right? That's part of the models. So, uh, I really, it's, I'm not dissing it, I'm just saying what I, one thing you pointed out is like, I, it's not like behavior of the object moving the object's, not a behavior of the object. It's just like, Hey, if I move this thing, it's something else doesn't move, then I might assume that's separate. But it's not really a behavior of the phone that I moved it, it's just, I don't know what to call it. But it's not really, oh, phones don't move like that. It's like, I can move anything really. It's just so, it's just, it's, it's a more point solution to this problem, which is not making it bad. I'm just saying it's a point solution problem, which, which is if you, if you move something and other things don't move, you could, you could make an assumption that subset of the graph is separate from the other ones. That's, yeah, that's exactly what I have so far. You, I think you might be right, it could just be a point solution, but the problem of occlusions and knowing where objects begin and end is still. A pretty big problem. So even a point solution to that, I think it would still, I see occlusion sliding back in the image thinking about images, right? If you think about, again, if you think about the alternate to this, I'm not saying this is wrong, I'm saying the alternate to this is I start off by learning about the world, um, by continuing moving my sensors of the object, whether it's my eyes tracing something or my fingers tracing something. And um, and as long as the objects are relatively isolated, like I picked this up in my hand and I do this, I'm not gonna get confused where the boundaries of this thing are, right? I'm just not. And, um, I can learn this model really well. And now when I see it in a scene with something else like run, do well, I, I, I have a, it's a different problem. It's not. Then now I have the ability to say these, the displacement belong to a cup. These are displacement. So I focus gun, they look like a phone and I, they're not, they're not the same thing. I ordered blur models for these two things. And so the figure ground separation. As it stated, typically is assuming you don't have a model, all you have is some sensory data and you're trying to figure out where begins where the other doesn't. But if you do have a model, then it's easier, much, much easier. I just do this. Oh, that's maybe interesting question here is I an example, you pick something and touch your eyes over, right? You don't, yeah, right.

I don't try to teach you what the letters of the alphabet look like by showing you a paragraph or text. I mean, I would still say that that fits squarely into this because one of the points I am trying to make is that you can't just sit back and passively observe, you know, images and actually decisively say how objects will behave or where one object begins. Right? Right. By actually picking it up and manipulating it and sensing it, but then, but then my argument, then you build the model. Right? It's, it's, and so you, you start with the model, then the, then the separation becomes easy. But the way you started the process, I'm thinking about this. You're saying, Hey, I have this image and I can't tell where one is and one isn't. 'cause I don't know anything. I haven't learned a model yet. So this are a big distinction. Do I have models of these objects or do I not? And um, well, if I don't have a model yet, am I still allowed to pick up an object and manipulate it? It. You can grab something, even don't know what it is, and you can touch it. Reach your hand in the black box and run your fingers over this, or you can look at it. Of course, that's how you learn exploration. Like you explore this, I pick it up and the process, it falls apart.

Well automatically isolated a sufficient amount of exploration models. Yeah, I I, I mean maybe you could say if you already have models of other objects, you could generalize behaviors of objects to new objects that you don't have a model of yet. So if you don't know about any objects in the world yet, this might not work. But if you already know that they are solid objects that you can pick up and manipulate in certain ways, and then you see a completely new object, you can apply those behaviors to that new object. Right, right. But you don't model yet. But that's not a figure ground separation problem. Right. That's a generalization of problem. I mean, Ben, start over with the saying, how do I tell where one thing begins and one thing ends? Um, so yeah, what you said didn't make sense, but it's a different problem, I think. Right? Yeah. I mean, I feel like this is a more general framework than just for, uh, figure background separation. I mean, it's pretty gen, generic, uh, general framework that you can use to solve a lot of different problems. Yeah. Right. I agree. Totally. I mean, that's the whole, that's the whole model building we've been talking about, right? That's not new. Uh, I'm, I'm just reacting with Ben said, which is start off by talking about we have no knowledge and we have, we're, we're presented with something about the world and how do we determine where one big thing begin and one thing ends, right. That was a question. You, you, that was Yeah, that's where I started. Yeah. And, um, but one of the other things was just, you know, adding a different way of looking at Monty, the stuff you've already laid out. So some of this is gonna seem very obvious, like the idea that you pick up objects and sense them to get mo to build a model in the first place. Yeah. That's not new. I'm saying that, um, I'm kind of proposing this loop here as a, a different way of thinking about it. You sense data to generate hypotheses, to take actions to text them. Okay. And then so I, that to me is, is sort of the whole model building exercise here, right? If I, if I open that up to the more general scenario where I say I have models of things in the world and I pick things up and I, uh, and I act on, I make predictions. That's what the models are all about, right? Yeah. Uh, and so, and every time you do any movement, you're testing it. Not explicitly. You're not saying like, I need to test this, but anytime you do anything, the model predicts what you're gonna get. And if it doesn't get it, then, then you know something's wrong. So that is the framework of learning in a thousand brains theory, I guess, or a court of column theory. Right. So I, so I agree with that. I'm not sure now if that's new or not. That seems like what we've been saying all along. Yeah. So now, so now I'm gonna, in the next slides I'll take this stuff, which is not new. Okay. And I'll just add a little bit of spin. Okay. But Lucas, you have something? Yeah. It just gonna say maybe that's not the right is.

Model based enforcement plan. That would be the most direct comparison. It has, has nothing to do with it, but model based are just data. Okay. So I think that would true for any model building system, I think. Right. Anything that has a structural three dimensional sort of like model that involves action selection that involves, well, well, it, it basically involves the idea that your sensors are moving relative to the, to the object. That's how I think of it. I'm not sure if it works in the enforcement, but if, if you are, if you are moving your sensors and your actuaries relative to the object, then you're gonna make predictions. The model will make predictions and then, then it'll test. It just it's nature. Yeah. And even on a higher level where we know from child psychology that children actively generate hypothesis and test them about new objects and they, they are, yeah. Working like little scientists basically, when they see a new object experimenting and they don't just randomly interact with it to figure out stuff, they actively test different hypothesis and then, uh, very efficiently figure out how objects work so well, I do that. Do you have a specific example in mind when you're thinking about that? Yeah. For example, there were these studies, they made this machine, I forgot how they call it, like, and then they had like little blocks that had different colors. I think they called them blurbs or whatever. And then the children would figure out which colored blocks activate the machine in, in what combinations and which don't. And then if, if they would glue two blocks together, they would have to like. Do some kind of combinations of them to, to turn them around. I, I, I can send some, some papers on that. I'm just curious what kind of, I, I, I just didn't all the time. Yeah. It's like a machine that lights up and produces some noise when you put different colored blocks on it, but they, would they, you're saying when they showed 'em a new version of it, they'd use previous learning to try to figure out if it works the same or works differently? Yeah. Yeah, exactly. Yeah.

Okay. So briefly respond to your point. Uh, Lucas, my comparison here is not really to bash deep learning or say there's no example of deep learning that incorporates these ideas. It's more just to say, um, this feels like a cool idea. Actually, the idea of like going around and building models of objects that feels very different from like. Couch potato image classification where you just feed the model. Lots of examples, so, oh yeah. Kind of an embodied learning situation. It should be more like feed forward, pure feed forward processing versus an interactive sense that that would be a better comparison. I just point out that that's on the rights, not really new, so, well, it's certainly not new for us. Right. This is, this is the core of, not even for like the community like this. The idea on the right is, well just what model do they have? Do those model based or they do, they have actions and sensors moving and like that.

Okay. Um, now thinking about actions, um, I wanted to try and just talk about the advantages and disadvantages of getting to choose actions first, the disadvantages. Um. It's complicated, and we don't yet have a whole theory about how actions are generated and executed in the brain. And so if we put a lot of effort into solving action selection, this could be a complicated or expensive task for us. And Jeff, a couple weeks ago, you pointed out the possibility that we may be able to be successful in our first application without introducing that complexity. So, well again, you know, actions are, are, you know, I put 'em on a spectrum. You know, simple spectrum is your actions are, you're just tracing the lines of something. It's an action, right? And just following things. Another action would be saliency, right? You are looking at something, your eyes move towards something that's a little different. Those are pretty basic actions. And then there's the actions like, oh, if this really is a coffee cup, I should be able to unscrew this lid. Let me try that. That's a wholly different scale of, that's like model based selection. The first two examples I gave really weren't model based. Right? And I took this a bit out of context. You were referring specifically to object manipulation when you said this. So I was saying, I was saying we could build a useful system that worked on the first type of action selection, which is not really model based. It's the model tells you what to expect, but the the model isn't driving the actions. The model isn't saying here's what to do to achieve some goal. Like, again, think of this, the idea that I might think this is a coffee cup, therefore I predict this top should come off. Therefore I think it doesn't pop right up. Maybe it should screw up. Those are really high level motor based. There's nothing to indicate there's a screw on this. Nothing yet previous knowledge about how things work says it either should pop off or shouldn't screw off. And if not, then I'd say it's not a coffee cup. Um, whereas just like, oh, I just picked this thing up, I'm learning what it is, and I move my finger around it and learn a model of it that, and then I can predict where it is and, and so on. That doesn't require a model to learn that those actions don't require a model. So, um, you know, when I was saying earlier, we might be able to do systems that aren't capable of like, oh, let's unscrew the top type of thing. Right? But they stick with the movements. And movements are actions. You move, your ant is controlling.

Does that make sense? You're looking at me like, I'm just kind of paring it because there's a couple parts, there's, there's a spectrum of actions and then I'm thinking about what distinguishes Monty from other things. One part is that it's a model building, obviously model based reinforcement learning. It's a model building system and built on moving sensors, which there aren't many, so Right. Most, most of machine learning isn't like that. Exactly. That's the number one thing. And then how the models are, you know, how models learn and how sensors get integrated is other components. And ultimately we have the idea of ability taking actions that are very sophisticated, like seeing them generality between objects top. So there's a spectrum there of things. Right. Okay. Well, yeah, this, this presentation is clearly lacking some nuance with respect to No, I just, types of actions and you know, I think that's a general idea, even in neuroscience that. There are movements of the, the body that are, that are top down generated and there are movements that are bottom club generated and, um, both have to be handled properly.

Okay. Um, some possible advantages of being able to include actions and, um, I guess here maybe we'll think about going beyond just being able to move my sensorimotor, um, like move my eyes left and right, but thinking about being able to interact with objects, um, it provides a much, uh, okay. So first of all, speed. Um, in the example I I gave earlier, if you're just kind of receiving images of objects, you have to just wait around until you see, uh, an example of, you know, two objects that are next to each other from an angle from which you can tell that they're not the same thing. And, um, until you get that you don't know. If you get to actually manipulate objects, you learn this immediately. I think that was sort of your point, Jeff, initially that Yeah, well yeah, if I can just pick it up. I just learn instantly what this object is. So speed of learning is increased, I think, by being able to interact with objects. Um, and you know, if we run with this hypothesis testing loop idea, it's possible you can get in touch with the world faster by kind of quickly testing hypotheses. Um, it also might lead to better generalization. So you might, by interacting with things, you might learn their behaviors. And, um, just point out that ju Earl has this levels of reasoning hierarchy and suggest that you can't go beyond level one without being able to interact with objects. So, um, there're clearly advantages of being able to manipulate objects. What is level one associative reasoning?

Uh, levels two and three are interventions. Um, and then level three is causation. And so, again, I just wanna be really clear on the first Pro one there, everything in mind built is built on the idea of actions for learning faster, right? That's the entire premise of the thing. Um, so I assume what you mean here is more top down interactions, model based interactions or, um, I mean.

That's the way it works. Um, yeah, I was thinking more about, um, actions like manipulations and things like picking up objects. It, it still would seem like it would be really hard if all I had was, you know, a camera that I could move around the room or something, but I can't actually, um, touch these objects or move them. Well, I mean, obviously you can touch 'em. Uh, I think may, let's make a couple of distinctions here. One is, like we talked about, oh, there's this object out there and I can touch it with my finger in different positions, or I can look at it in D positions. The next step is, oh, I'm now lift this thing up. I'm holding in my hand and now I'm moving around. That to me is a subtle difference between the first one. It's not really substantively different. It's, it's, you're still basically just moving your sensors around an object, but instead of moving around the object, you're holding it and changing its orientation to, and I think complex, but that's still in this category of. Um, I'm learning what this, this, the, the morphology of this object and what it looks and feels like by just, uh, by just sensing it, right? And whether it's stationary or I pick it up, it's really not that much different. Then the other thing is, well, there's a difference, you know, like, you could pick it up, pick up an egg. They gonna pick it up like a, a level. Well, you'll learn something by picking it up.

Yeah, but my point is this object here. If I, if I'm not, if I, if I was restricted to learning by moving your thing finger around it like this or, or I, I'm holding my hand and moving my finger like this and I'm like, while I'm doing it, I'm changing the angle. To me that's the more complex kinematics problem. But it's the same. You're basically learning the same way. You're learning what sensations occur at different positions relative to the object. Right? Um, you just have to translate it to different orientations. But that's, I I'm pointing out is that that is a very small step from just having a stationary. But when you start, that doesn't require really any, it doesn't really require having a model of this object to learn it. I can just start learning the model by moving my extensions over it and, and feeling what it is and, and, and I build up this model. Um, but now there's a separate question here is if I've already learned models of other objects and now I see a novel one, how do I generalize to the previous thing I learned? And that in some cases, well, even, even just something, how do I even learn that this thing has a removable top? That's a, that's a different type of model. We, you know, we have to solve, like the model's not a static object, but it has behaviors. And now if I have behaviors, how do I apply it to novel things that might have similar behaviors? That's a whole different level of manipulation and action. Again, those are top down cognitive, cortical action generation versus the just touching, moving your finger around is really, is much less. So. Okay, so I'm, I'm, I'm hearing you, there's, there's a big difference between unscrewing the lid or, or you know, pulling off the lid type of actions or seeing commonality between two two, a new object and an old object. Okay. Right. That requires a model. Um, it's almost like this separation between like a seman model and a model, but even the distinction that you were pulling together of feeling around it when it's static. And then lifting it up, you actually are learning more about it. You learning more about the hef, the volume. Yeah. But it's a, it's a, it's a modest difference. It really is. Uh, but you, that would be enough to identify maybe. But let's, let's not focus on that corner case of, oh, this is really heavy and therefore it's surprising. Just imagine it's this cup and, and there was no surprise about his half. It. I, I, I, I don't think we should focus on that corner case. You know, I, I can pick it up and maybe it falls apart. I mean, yes, I could run something good during that. Um, but well, it, it, it aids in in identification, separation. Yeah. But let's, let's assume there's nothing like ice, just this object that sitting here and move my fingers. I pick it up and I move my fingers around, or I turn it like this, as opposed to walking around. Those are very qualitatively the same. Right. But I, I'm not trying to make a distinction. I'm not arguing with the distinction between the behavioral model we're talking about, which requires previous experience and recognition of where the similarities are. So, yeah, there's, there's, there's, and I don't know whether that goes, that, those three levels of reasoning, whether that fits in cleanly to those three levels or not. I, I don't think so. Well, I, I could, I I, what I've looked at in the past, I didn't see how it worked, but maybe it always, um, maybe some can make that argument, making sure what, okay. I wanna, I wanna, now that I've gotten feedback from everyone, I wanna frame the way the rest of this presentation should be interpreted, because, um, I was actually anticipating that, um, you would be, uh, hesitant to have all the object manipulation types of actions, uh, included right away. Uh, and I was, I was gonna be in favor of including all the object manipulation kinds of actions. Well, okay. But the way you phrase it here is so open-ended and generic that it fits what we're doing right now. Actions, faster learning, active learning, hypothesis testing. That essentially is the static object, moving your finger over it described Monty. So I think what you're, if you're arguing more for, we wanna, um, use previous models of objects to make hypothesis about the behaviors of new objects like cognitive action, selecting model based action selection, right? Where the models are, determine what to do based on some goal or some active hypothesis testing. Um, then okay, I'm interested in that. Right. I just think the language, it's, it too loose. This describes exactly what we're doing already. So, um, well, I'm, yeah, I mean, I'm, what can I say? I'm in favor of what we're doing already, so I, I was actually trying to be balanced, but I, I'm. Really on the pro side here. Were you thinking about what influence at this point?

Uh, still, still training, like you should exactly. Like Jeff said, you should be able to, you should be able to acquire your first model of an object by like, reaching out and touching stuff and manipulating it.

Publicis testing. And this you Orly or you also apply previous? No, I, I don't think so. Can I, can I go on there? I want get to just like this, this, uh, I don't think this presentation has as much merit as, um, as I was hoping, which is fine, but it's alright. There's still, there's one kernel of an idea that I do wanna still get some feedback on. Um, okay. So, um, what's the word? I can't read it. Yeah, so I was asking myself. Um, what are the benefits of being able to like, manipulate objects and everything, um, and can we get some of those benefits if we just have like sensors and we're allowed to just change the orientation of the sensorimotor? Um, and what that led me to think is, well the two things that you can play with if you want to do better in that situation are, um, the prediction and prediction error components of, uh, of your model.

So, um, here are four ways you might structure predicting your next sensory input. The first would be kind of a brute force approach.

I'm sorry, this slide is supposed to be hidden.

Oh, we're, we're cheating? Yeah. Let me, let me go to the next, this, this was a draft slide Miles. Okay. Brute force. Okay. Okay. So yeah, the first one's supposed to be this brute force approach. Um, this is what you kind of might do with deep learning. Often it's like, I have an observation, I have an action, and I'm gonna use that to predict what my next sensory input is gonna be. Based on what? Based on the observation and the action. That's all we get. So it's just a pairing. So it's, it's just saying, I've learned this pairing. Um, there's no underlying model. Is that what you're saying? That that's a model. Right. That it's a very simplified model. It's a model. Okay. It like the stupidest model ever. It's like, you know, it's like saying, you know, okay, fine. It. There's no underlying three dimensional structure to this model, but this is very, this is commonly used. Okay, fine out there in deep learning. Yeah. So I'm just adding layers of structure to it. So next step is I take my collection of observations and displacement and I put them into a graph, and then I take the graph and then an action, and then predict my next sensory input. Yes. That's what we've been doing so far. So now it occurred to me that like you could take this even further and say, actually I'm gonna predict what my model will be at the next step. So I take in a graph and an action, instead of predicting my sensory input, I basically predict in terms of the, the graph and the displacement of, of these observations in the environment. So I could, I basically predict, you know, and move this thing over. I predict this kind of mesh object moved over and from this I could try and.

Learn the mapping from the state of that object in the world, back to the sensory input, and now compare the sensory inputs. And then the even more extreme version would be, I predict my graph, I get sensory input and I actually construct a graph from the new sensory input. And I only do the comparison, the error in terms of the models, not actually the raw sensory inputs. I'm lost on that one. So let me, uh, well, okay. Can we just, can we just leave that text up for a minute? I wanna read it more carefully. Okay. Um, so even you're using predicted graph model is, there's a whole bunch of assumptions what you're saying. Have a graph, which means right.

Right. Okay. Can we say just a, a collection of displacement. Okay. But that's a model. That's that's what it's Okay. But it's, it's like a model without necessarily a label yet. Okay. But a graph is defining a structure of something. Okay. So, so model then, but it's a new, you haven't seen this object yet. Oh, is that right? That's, that's kind what I'm thinking. I just have, um, I thought you said the graph plus action equals new graph as opposed to, but I assume the first graph is something you'd already learned. Oh, I was thinking of that graph is, um, what you got from your sensory input from like going around and sensing, this is our short term graph. Short term graph. Got it. Good language. Uh, then, then you say predictor number two. Um.

It's just a graph.

What would you distinguish, how you distinguish, predict one and two? So, so one is, it's just the same as, so one, one, I'm thinking about it right now, maybe this is wrong, is I have this cylinder and I'm, I'm constructing a graph of the cylinder, so I have some expectation of what to see. And I know about cylinders already, so I have some expectation now I hit this handle, now I, oh, it's not a cylinder anymore. Now I can predict this entire shape of this thing. So that's another graph in some sense that I'm Okay.

No, I was more thinking of like, um, you have this, you, you have, um. A short term memory graph of an object in the environment. And I predict that, for example, if I apply a force to it sideways, I'm gonna predict that my new graph is gonna be basically the same thing. The graph doesn't change. Yeah. Right. Nothing changed unless that graph is an object centric coordinate. So that graph doesn't change when you move it. Yeah. I, I think that may be different. Yeah. I, I was assuming that kind of get to know something about where it is in, I guess you could try and say it's in relation to the body. So that would be a new pose in relative to the body of that same graph so that, yeah, the graph itself doesn't change, but it has a pose relative to That's correct. I shoulda, um, I was Yeah. Running at the last minute to make these slides. It should, you're right. Exactly. Just say same graph, but new pose relative to the body. Okay. Okay. Right. Well, that's a really important distinction because we, we create a new graph for all. Um, we just moved an existing one and. It's a new post to the body, but really isn't graph and, and I'm not even, I guess you could say that there might be a prediction about that. If I say everything right, I could predict Now these things are, but it's almost trivial. You really haven't learned anything. Right? I dunno if it's trivial or if it's a subtle point because what have I learned by moving some unknown thing to some other position, rigid body that just before Okay, fine, fine. What I'm saying though, though, is that you, it's, it's a different way of making predictions. Instead of making predictions by just going from, uh, graph and action to sensory input, I'm now going from like graph in action to, um, new pose of the graph to body, and then from that to sensory input. But it's in some sense you haven't learned anything new at all. I mean, Kevin says you learned that it's a rigid object fund, but other than that, if I, if I see a set of things in some arrangement, I don't have no model for it. I have no expectations. There's no additional knowledge I have. It's just some temporary thing. And, um, and now I move it to another place and, and now it's a different puzzle to my body. I haven't really learned anything. What, what have I learned other than it's rigid. Right? Well, okay, actually I want to even make another distinction. You're not executing the action yet. You're predicting what's gonna happen when you take, okay, so yeah, I make a prediction and then prediction becomes true. What have I learned? Wouldn't it also depend on the background? Like what's fine? But now you can go back to the original premise put up there like figure ground separation type of problems. Right? But I don't, I mean, I'm not sure if we're still trying to solve that or you're making a broader point about models here. Um. I think you, Ben's trying to make a bigger point than just like, how do I separate one object from another? Yeah. This is a bigger point. This is an idea about different ways to structure prediction and error. Okay. I, I'm cutting. I'm gonna argue that there's nothing gained in this, this, this particular model version. Top right version. Yeah. How about the bottom right? Well, let's talk about the bottom right. Um, so what I have a, a graph, which is again, a new graph novel. I've never learned this, right? So this, this is a new time I presented this thing. Yeah. And it again replaced like the stuff on the right, the predicted graph with just graph in new pose relative to my body. Okay. So lemme pose into the body. So I, I moved the object, um, mentally you haven't actually moved it yet. Okay. It's predicting what it will be after you. Okay. And I, and then what am I, what's my new graph? Uh, new no build model. That, that function should also take the graph, right? You're updating the model, a graph and then, but you know, there's no update the graph because. The graph hasn't changed, but I said, when just 'cause you moved, it doesn't mean that the, the relative positions of things haven't changed. So the graph itself hasn't really changed at all. There is no new, it's still the same or set of components. Um, are you moving the, the graph with the sensorimotor in that sense?

You're moving the, the object, but you have a different perspective, different sensations. Well, he didn't say that. I, I mean, he didn't say he moved the object and flipped it over. So I see the backside of it. I don't need to move. I mean, I, that's a different type of thing. You say, oh, well here I have a model, this film, but I don't know what the back of side looks like. I can rotate it, see that and rotate, see more, have no observations. That no observations for me is sensation. But he didn't say that. He just didn't move the object. So I'm not sure if that, if I just take this and I don't change its orientation, my body, if I just move it. Now I'm observing it over here. I haven't learned anything new. Well, you were saying from the different angle, but well, have I changed the angle of the object to me? Or if I just moved it? That's, that's a big distinction. If I have this same here and I move it over here, I'm not saying anything new about it. There's no new information to be had by it. There's no way I can learn anything new out it just 'cause it moved over here other than this Kevin points out together. Um, so if I rotate it, but then I can just rotate it right here. I don't need to rotate it over here. I mean, yes, I'll learn new things if I do this, absolutely I'll learn new things when I do that. Let me try and clear this up. 'cause the slides are not the clearest I admit. What I mean is I have this short term memory model of the object. I choose an action. I don't execute it yet, but I'm gonna predict where the object is gonna be relative to me. So now it's over here. And now let's pre pretend while it's in this new location. I do a bunch of observations and I sample what it looks like here and I build a graph. But is it, why would the graph be different when it's there than the graph over there? Yeah. So that I, I actually, I don't know that that really has any merit, but the next slide has a visual Okay. Of what I was talking about. Okay. And it's possible it still actually will. 'cause I still think this idea is kind of cool, even if it, uh, well, I, I, I'm trying to tease apart what's, what you're actually gonna learn from doing this. Uh, now again, if you're trying to do the, the, the figure ground separation issue, I agree that you could learn something from that. Right. Okay. So we're going back to that.

Okay. Basically, yeah, the, the idea was to cast error prediction error, not in terms of my actual sensory input, but in terms of two models. So this might be the model that I got from a bunch of observations while the object was in one pose. Um, I predict that if I, you know, grab it by the handle and move it over, the whole thing should move over to this new coordinate. Um, what I observe is that in fact this stays here and this moves. And, um, now I take a bunch of observations of this on both of them. Yeah. And build, um, build a separate short term graph of the two things. Of the two things. And now I compare my observed model, basically what I actually see after I take the action from the predicted model. And so I have to learn that this stuff in red, I predicted this would be here and it wasn't. And meanwhile, this I didn't predict would be here. And it is. So I have to learn the correspondence between those two. And I can actually also go back and take my initial observation and with this new model. I can explain the error and basically correct. But isn't, isn't the only thing you can learn here is that these are two separate objects that don't move together. Is there anything else I can learn about that? It, it doesn't seem, it's more generic than that. It just seems like that's the only I can get out of this. It's like, oh, well these components didn't move when I moved it and these components did. Um, uh, honestly, I haven't thought it all out yet. I was trying to just get some ideas on the table today, but I, and I did think that this might extend to notions of object state, like the state were, well, that's an interesting idea. Um, but to me that, so that would be the interesting thing to explore, right? That, that would be the interesting thing to explore. Uh, how are you, how, how you learn models of objects that have different states. Um.

So that would be a, I think that's a, a more interesting problem. Um, I'm trouble imagining, but that could be just my imagination. How are you gonna do that without first learning a model of not just a temporary, your contemporary observation, but active learning model. Something not staple. Yeah. Maybe for a state book case you have a bunch, again, you mentioned a bunch of observations all around the object, and you do a bunch of manipulations and essentially you learn as a constraint. All the, those observations move because the stapler, like it's just a h Yeah. How you do that in a model, like a graph model's, hard to, I mean, I thinking about it, it's tricky, right? I have an idea how to go about it, but it's really hard. Um, well if, if you're doing more than just moving the object, if you're applying forces to it and all of a sudden the stapler snaps over, like to say Yeah. So you're toying with it Yeah. To see what it could do. Yeah. Right, right. But how do you actually represent that in the model's challenging. Right. Um, well, I mean that's kind of the hook here is that you basically, you learn the, the difference between two models. Like I snap it open, I basically build a new model and then I do the model diff. So instead of, you know, the, the red cylinder that was here and there, I learned that like this part of the stapler that's wide open corresponds to the part that was closed a second ago. But, but in this case, they don't correspond. Right. Right. Here you have two cylinders. They're not corresponding. They're totally different things. You don't.

Example on the board and they're project Right. Those are, that's how you it. Right. The back cylinder is not part of the front cylinder. Correct. So they that you don't wanna learn that as behavior of the two cylinder object. That's not what it is. So I'm gonna push back a little bit. Okay. In, in a very general sense. You can, there are two solutions to this. You could say it is one object that has the behavior that these two parts move completely independently. Well then it, they can't be completely independent because then it wouldn't be one object. Right. Unless they're somehow, I'm, that's what I'm saying though, is it's, it's a learning that it's two, learning that it's one object with this behavior of total independence is equivalent to learning that it's two objects. The way the objects will behave in those two models will be the same. I think I, I, I don't get it. I'm missing it. There seems fundamentally different. You know, a stapler is one thing and it, it has a, it has a constrained set of states. It can be in. Um, and I learned it as one thing, and I have to learn what those states are, and I have to learn how those states transition from each other. That's something, it's all part of one model of playbook. Here. I have two separate objects that can move completely independently in the world. They're not one thing it, and otherwise they say everything in the world is, you know, causally tied to everything else, which can't be so, so here it's a totally different thing. I, here what I'm saying, basically say this isn't one thing. You are two separate things. And I can't build a model of 'em, a single model of 'em, because they're two separate things. Well, what I, I, Jeff, really, I agree with you, but what I'm trying to do is use the same type of thinking to explain object states and boundaries between objects. And so by learning that there's no physical connection between these two things, by learning that they behave and move independently of one another, that, uh, that inference to me like. Again, I see this as a way of separating two objects. It's a way of saying, I can differentiate, object these observations into two object A and object B. I agree with that, but I'm totally, totally extending it beyond that, I mean, I don't know that we're gonna make progress until I think about it some more. So I think it's fair to say, you know, if, if what you're seeing is this, this, the sensorial impression you get, there could be two potential hypotheses. They're connected to it or not. And so do I, if I care about it, do I wanna take an action to distinguish that? Then move on from there, there, so, so you don't necessarily, you know, the, if the, if the sensorimotor is giving you, if the sensorimotor is giving you site sensorimotor, in this case, giving you ambiguous information and you wanna tease it apart, you have to take an action to do that. Otherwise, it still remains unresolved. So it's part of explaining how, what it is in front of me and how this, how the, how the composition of what I'm seeing is. I agree with that. That's right. Right. That's the object separation problem. Right. So, but that's the thing is that in the light of sensorimotor ambiguity, you have to take some action to resolve it. Once you've done that, you know how to classify it and then you can move on from there and learn more interesting things about it. But it's, it's still a, a case where if you're presented with something that's from a sensors point of view, a conundrum, you would like to be able to resolve that. 'cause otherwise it's ambiguous. You don't know if it's connected or not. Right.

Uh, and this is a solution problem. Well, he, he's, he's trying to present a, a, a simple example. Yeah. But, but again, it is a problem of object separation as opposed to, uh, where I'm trouble is, is how does this extend to learning object of behaviors in a model? I think that's a different thing. Right. That's my point. That's why this is a point solution is doesn't, Ben said he thinks it's a way of getting to the latter one, the other one, but, which is really a hard problem. So I'm just making that distinction. I'm, I'm, I'm pushing back on the idea that this is a general way of learning object behaviors. If the synagogue set of actions limited that you have is to move an object, that's all you can learn. No. Even if I, I can push it, I can do anything. No, no, I'm right. Okay. But the point of it is saying that there are more complex things to try to tease out the behavior thing. Right. So. In, in with those more, the increased, uh, richness of actions you can do, you can learn more. Yeah, but I don't, and Andrew, these set of premises, I'm arguing you can't, these set of premises are that I do not have a model of the object yet. All I have is a temporary, uh, constellation of features I have, I haven't learned anything yet. And, um, and that, that, that I can, that, and that's the fundamental issue I'm having. This idea is that you can't really, I have to learn a model of object before I can make predictions about behaviors of that object, right? This is a trivial behavior. I move it from here to here. The object hasn't changed. There is no behavior, the object that's just moving something. One location to the hard part is how do I build a model, this thing when it's actually doing things, performing or.

Um, and that's, that I don't see can be done. I, I think you have to start with a model, that structured model to, to do that. You can't just do, you can't learn behaviors to something that you haven't learned. What, what I would like to do is, go ahead. Go ahead. I, I wanted to riff on something you presented earlier, which I thought was interesting. Okay. I mean, basically I, Jeff, I've, I've taken your points to heart. Um, I still feel this faint intuition that I might be able to get somewhere with this. So what I'd like to do is go back to the drawing board and see if I can make this idea work. So what do you think the core of the idea is?

Well, there's one core thing you presented that I liked, I wanted to riff on.

But what I, do you want me to go back to the slide? No, that's okay. Um, I think one thing I like is the idea that what we're doing is predicting graphs from which we predict sensations rather than predict sensations directly. And I thought that was, I think there's some interesting directions to take that I do appreciate that. And I don't know, I'm, you know, maybe we've thought about that before, but it's sort of a very crisp, a clear way of stating and, and yeah. Way I, it's the first step on a hierarchy of abstraction, I would say. It could be. Yeah. Uh, but I'm thinking like, suppose you're, you know, you start sensing, you know, some shape, right? And we built up a, a set of displacement in a graph. And in our experience, we have a whole bunch of different models that we've learned in the past, right. And now we're sensing here and we can immediately.

All of us here can start predicting that there's actually a, a completion here. There's some sort of, based on everything we've learned in the past, there's like a graph, a way of filling out this graph that we can already predict and then we can start sensing it. But maybe now we start sensing it going this way. Okay, now it's not this graph anymore. Now everyone can probably predict some something. I dunno what I, is it a prediction of a graph or is it illumination of, uh, of, of possibilities. Yeah. Maybe it could be a union of graphs, right? Yeah. That, so then you're not really predicting graph, you're basing, it's not the union of thing, but it's not, you're not necessarily predicting a sensation directly, which seems harder predicting a union of graphs or a specific graph based on everything. No, it seems like a little more trackable. Well, it's, it's a, it's a, it's a subtle, it's a subtle, subtle, it's a subtle, it's a subtle language, but it's. It seems more tractable algorithm make to me. Well, it's more than just language. I think it's, um, like you said, I, I just thought that was an interesting, I dunno if this has language or thing, so I like that idea too, but I thought of completely different. To me, this is just narrowing down an possibilities. I have some, I have some data evidence what I have. No, I think it's more than that because it may be that, let's say you've learned about cars, now you're seeing a toy car. Yeah. It's a little bit different. It's distorted a little bit. So it's not exactly predicting a union of things we've seen before, but it's specific to what you've seen today. But, but, but do I see it as a car? I see it as a car that I've inferred it to a previous graph. Whether it's different or not by, oh, no. Yeah, I'm, I'm, I'm assuming you're using previous graphs to do this. Okay. So, but it's, it's specific, it's, it's sort of molded to the observation. So what is the new model on predicting in.

Either this graph or I think it's the fact that there's object. It's, it's, that's say more than that. It's, uh, it's more the shape based on what you've seen before. Right. Predict different shape, but it's fit, it has to the scale and deformation has to fit what you've seen already. So that, that's the slight distinction. So it's funny because you say if it's a toy car, but I've recognized it as a car, so I've already mapped it to a graph that's by definition as I wouldn't see as a current. And now, and now I'm making, maybe I'm making novel predictions based on the fact that that's a toy car versus a, a real car. Yeah. And it's smaller. It's, uh, but my predict, but am I predicting a graph or am I saying here are some attributes. I have a model of a car and this one's different, and I'm able to extend those somehow. Those, it's like scale. Just take scale. It's small, it's not a real car, it's small. So I have different, I somehow have to be able to scale my model. Yeah. So it's a, to me is that, predict my riff on this is that we're actually predicting a new graph. I see that that's, that has smaller displacement now. Well, I wouldn't think that, because it seems to me that, just to infer that it's a car to begin with, I had to use an existing graph with, and somehow recognize an independent of scale. I, I have to say, I, I already recognized it's a car that's small. Yeah. And so it's not like, but then in order to be able to predict the sensations, I, you're not necessarily using the previous car at a different scale. Why not? 'cause this is a new object. Now you are building a new model. Okay. But it's not my, you know, Lamborghini I don't have in my driveway. I guess it's interest. This one, it's interesting to say. Um, are you predicting a new graph or are you basically saying, I have an existing graph of cars. Uh, I now realize this one is small and different in other ways. Um, I'm now gonna create a new graph, uh, which has these different attributes so I can, and now I'll recognize it as a toy car. Yeah. Because I predicting graph, I'm just a new one that's similar to the old one twist. Well, these are, these are predictions because you haven't since Yeah. But there'd be the predictions based on what my previous model of cars was. Yeah. Yeah. So I'm not sure I'm predicting a graph. I'm still making sense. It, it's a graph because I can predict, I'm predicting new sensations of new displacement. Right. But it's not a new, am I predicting a graph or new sensations? Displacement. I'm, I'm saying that we predict a graph instead of the predicting the entire graph from which you can predict sensations. Exactly. It's a subtle distinction. Uh, I think it's, yeah, but it's, I'm not mine. You're sort of instantiating the whole thing. Unless not I, because now you can tell me, oh, what would it be here? What would it be there? It's, it's like you're predicting the whole structure. Guess, I guess, to predicting a graph. It's, it's sort of maybe instantiating. Yeah, it's instantiating a graph. Right? It, I need to predict something. I already have to know what it is. Maybe instead, but we haven't sensed it yet. Oh fine. That's alright. I got it. But it's, I'm not predicting something. I can't predict the graph. I've never learned before. That's impossible. So here's kind of a flip side of this. Um, here's one reason I don't think it's just about predicting sensory input. Um, you don't really notice like blinks in your visual stream. You don't notice the small micross your eyes are always making, um, you don't even notice the ps. You don't notice pica, it's fine. So, um, I think there are lots of examples how the brain sort of filters sensory input. And so I don't know that it's really comparing like two, like a predicted sensory input to like an actual input. It may actually be kind of comparing like. What I think the world should look like to like the new model that I have of the world. But in the end, the only way you can test your predictions is by sensory input. There is no other way to do it. Right? There is no other way to test it. So you're gonna, in the end, you're gonna get sensory input and it's gonna run against a sensory prediction. I could predict a graph that's like hallucination, right? I'm predicting a graph and I don't bother checking it with reality. Um, and I can just imagine in my head, oh, that's all good, right? Right. But I'm not really testing it. Um, yeah, maybe instantiating a graph is a better word, but you know, it is, it is not anything you've seen before, but it is using information you've seen before. You can kind of impute a new structure. Yeah, but it's gotta, so what I've been thinking about is it has to, there's some, there's some core components of the old graph that have to be transformed into the new graph. Yeah. And, and that, that is some, that is some attributes of the displacement that are, it could be something completely bizarre, you know, like I could say it's a horse with the head of a man. Yeah. And like, um, and maybe you've never seen it, uh, before. Um, but I can still construct the graph in my head of what, and I can make predictions about it. Um, so it, it's a new thing. It's not anything, it's not a subtle change on what you've seen before. You're just Well, that's a good example. That's a good example of. Um, I wouldn't even call it predicting a graph, I would say. Yeah. Predicting is not the right word. That's, that's what I'm focused on. I think you can instantiate a new graph by combining two previous ones. Yeah. I could make a graph that actually has my cell phone attached to this coffee cup if it was really glued there and I pick 'em up. All right. That's the new thing. Okay. I wanna live with it. Um, right. But I'm not, I, yeah, so I think this idea of sort of instantiating new graphs completely, or a union of graphs, I thought that was kind of an interesting Okay. Take and it's very, to me it's very could imagine writing code for that. It's very clear. Okay. Um, so I agree with that. I think that's, um, that's, that's one of the problems. Our graph, right.

I've made that example before, like you can see a coffee cup that's a different shape. You still need a coffee cup, but now you can remember that one uniquely. Or I see a cat that's like other cats. This is a unique, I need to be able to create a graph of that unique one somehow. That one.

Um, can I, can I run with the hallucination idea for a second? If, if we think we have stored lots of models of objects, like I have a model of everyone in this room and, um, you know, maybe somebody like gets up to leave the room. I don't, I, I could start predicting, I'm not predicting sensory input per se. I'm just predicting, uh, the location of Jack that's gonna be close to the door in a minute, as an example. Um, that is basically a hallucination, but yeah, I'm, I'm, I'm kind of wondering maybe that's actually what's happening and you don't bother correcting it until you collide with corrective information. Of course. How could you, how could you correct it until you, there's no other way to do it, right.

Yes, we can, we can hypothesize about future things that haven't happened. But, um, like I'm wondering if like, you actually, what you're experiencing is the, is basically the hallucination at all times until you're, until the prediction isn't correct, and then you kind of have to Of course, of course. I think that's, that's right. I mean, how else could it be so then that you're, you're like predicting, um, it's, it's, but none of it's real until you've tested it. Right? You can't, you can't, yeah. It's a hypothesis. It just like, okay, I, I'm imagining that I would sense this if it actually true, but I haven't sensed it yet. It, it'd be like saying, I'm imagining that when I reach down this under this chair, I'm gonna feel these levers. Right. Um, well, until I reach down there, I won't know that, but I, I imagine it, I don't think that, it doesn't teach me anything. I haven't learned anything yet until I actually reach down there and say, oh, what does it feel like? All these levers there or not? Um, so I think that's what the models that you do then, that they let you run through scenarios and imagine what would happen if you did this and if did that. I'm just arguing You're not learning it from that. Well, you're not, you're not validating your model. You're just testing. You're running through hypothesis and saying, well, you know, this could happen. That could happen. But, um, I haven't updated my model yet. Yeah. But now, now the idea that I'm proposing doesn't have to do with utility. It's, uh, just a interesting idea that like what you actually experience is kind of in the model world and it's actually, it's often in the model world, and it's only kind of in the sensory world when you pay attention to your sensory input specifically, or when you're sensory input. Is not consistent. I just Totally, I agree with that a hundred percent. That is, that is, I, I, I agree with it a hundred percent right. You, it's, it, it's no different than me being saying like, well, I'm imagining that the, there's a mental logo on my coffee cup here, and, um, I'm gonna assume it's correct until I, you know, rotate the cup and I don't see it, then I know it's wrong. But, um, I haven't learned anything. It's just basically a model makes predictions until you sense some you don't, you know, then they're just predictions. Yeah. You live in hypothesized world, right? And until you get, just otherwise I didn't experiment. Like with, with, uh, virtual reality tasks. I first took at 360 picture of one room, so I can like, look around the room and experience it. Different room. Looked at room. It feels like you're actually in that room. And when you take off where? Where you're in a different room. Yeah, different room in room A. I took the picture, I observe it in room B with the glasses and I take off. I'm still confused. I'm sorry. You're in another room, but you're wearing the glasses. Yeah. So is it you seeing different room or you actually seeing the first room? I'm seeing first, how do you, if you're just wearing glasses, how would you know what room you're in? Yeah. It feels like you're in the other room. Right. But how else? Yeah, of course it fits. Like you're suddenly transported in the other room. Oh see, uh, it's kind a weird sensation. Yeah. So you suddenly adjust your place, place and you put it and all of a sudden you snap back. Yeah. So there's whatever path integration you did is being fooled. Yeah. Maybe that's, yeah. But but back to that, you wanted to, you wanted to promote, generalize this, I put challenge to. Represent like the of freedom, for example, the state. Well, how do you represent its behaviors? Right? That's a hard problem we're working on. We, we propose a partial solution once it's not complete. And so, um, that's a hard problem. I don't see how this addresses that problem.

I have to have a model. The model somehow includes the behaviors of the object and the actions which made those behaviors, you know, happen. And I'm still gonna try it though. I'm still gonna go back and see if I can get it to work. I'm still interested if a solution. You also have others not physically to.

There's no attachment to that. There's a cable somewhere. We know that.

It feels like you don't perceive it as one object, right? We do. I know. I mean, I don't, I don't imagine these two things in some constellation to each other in some position and say, oh, there's this one thing here. Right. But I build the behavior. It's more like if I, if I do this button here, there's some, cause there's causal relations to some other behavior elsewhere. Um, it doesn't feel the same as a stakeholder open and closing. It does not, but it might be, but it doesn't feel that way. Maybe it's just more of an excuse myself, minutes to do. Sure. It's just approach the like al relationship, which is generally Yeah.

Yeah. Probably. Why fascination. There's a al relationship that is outside their physical bounds and they flip a switch and they see something happen and they'll sit there and flip it for quite a while. Just, you know, it's like magic. The fact that I can have this, they do that. I'll tell you right now, I'll do that just for spinning something on a rod when they're really young and it's like, it's like magic. They go, look at this. It keeps spinning, you know? Yeah. I, something happened. My granddaughter's doing it right now. Doesn't have to be, doesn't have to be like switches and lights.

Yeah. Well, ambitions. So I think it's ing she's learning some sort of action policy about spinning it, right? Yeah. Um, and and also the fact that if she does it again, the same behavior's repeated, you know? Yeah. Okay. We only have a couple minutes, so I wanna hear what Lucas' Point was. Yeah. Can I go back?

Yeah, I'm just, so the brute force approach, when you call predictor, that's actually a model. So you have a model, you got observation action, you have next action, and then you use the error to update your model, right? That's the predict what you're proposing. We do have an error, but that error cannot be used. Model. 'cause you already know how to put the new model. So what's the point of there, there to only allow you to, um, update your predictor. But why do you need to predictor? I was, I was thinking the opposite. You, you, the, the point is that you no longer update the predictor. You do the updates purely in the model space. But why do you need dairy? But the, is the model, right? Something you just said. Well, I thought the graph was the model there. Right? But there is no graph in that one. No, there i i I'm looking at the bottom right. Oh. Oh. I thought you talking about the one because you already know how to build the new model. The error is not gonna play a.

That error would be there for the predictor alone, which predicts given modern action. And why is the new model? Um, I, I hear you. It's the exact opposite of what I had in mind. How, how can you use that error predict model? You can't, I mean, it's, it's exactly what's on this slide. You have to learn that the stuff that you didn't predict, which you now see. Is the same stuff as stuff that you predicted that you don't see. You have to learn that correspondence to learn how course, so you go back, see a little green thing in the far left, go back to one side, please. This is exactly the step you have to take, right? You already have the graph so you know what your new graph is like, so why? Why do I need this? Yeah, that's, I see your error here, but why let, it's, it's explain. It's explain. You have two observations. You have the first observation from which you made the prediction, and then you have the next observation, right? All right. Now I basically use both observations and update so that I have one model for both observations, right? That's what this error is supposed to move for, right? But what are you gonna use it for?

Ins instead of predicted graph and observe. Graph. From this I derive. Unified graph that explains both observations. Maybe error is not the right term from it, but like you place this line with unified model of both observations, would you accept that I about, oh, I mean, you might be right. It might just be, it might actually not be doing anything. Yeah. Yeah. That's, we have to think about more, but we, but I don't see a distance like formally figure out. Okay. Thanks Ben. Thanks. Thank you very much for all the, despite discussion, acrimonious conversation. It's good to do these things and it's good to argue with each other, so I, I really enjoy, absolutely.