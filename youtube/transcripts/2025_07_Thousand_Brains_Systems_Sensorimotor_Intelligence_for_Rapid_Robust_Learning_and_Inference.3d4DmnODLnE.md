Yeah. the official title of, one of the two papers that we are publishing right now is called Thousand Brain Systems, sensorimotor Intelligence for Rapid Robust Learning and Inference. But as Viviane said, we often have referred to it as the demonstrating Monty's capabilities paper for DMC for sure.

and this paper is really a culmination of a huge amount of work, I'd argue around three years. going back even further though, to, when Jeff and together with researchers at Numenta laid out the principles a Thousand Brains Theory, those were what, Jeff wrote about in his book, A Thousand Brains. these ideas were then taken to, both at Menta originally and then the Thousand Brains Project, to try and develop a system that actually implemented these, concepts. And, that is what became TPP Monty. so the first kind of code implementation of a Thousand Brains System, that is a system that implements, the concepts and, the algorithms, described in the Thousand Brains Theory.

but we had that kind of system there, and we had, benchmarks that we ran internally to look at it, but what we really wanted to do was to compile an overview of all the things it was capable of, into one kind of paper, at least all the things it's capable of. today, of course, we're constantly working on new features and the like. and so that's what kind of became the, DMC paper.

and it's been a, a huge effort from really everyone on the team, but particular shout out to, Scott and Hojae for all this work, that they put into this. And, the kind of main things that we really wanted to emphasize in this paper was the capabilities of robust inference, rapid inference, and rapid continual efficient learning. which is a bit of a mouthful, but we'll break this all down, shortly.

It's also maybe worth emphasizing that kind of what I'm presenting today is gonna be focused on the results in the paper, and in particular, trying to give an intuition around the experiments we ran, what sort of metrics we used, what the results were, and any kind of implications of those. but I'm gonna assume you have a working understanding. If, you're watching this, I'm gonna assume you have a working understanding of Monty. and if you're new to Monty and new to the Thousand Brains Project, these are the kind of resources I would recommend starting with to get that kind of background knowledge first. so things like the December symposium, we did last year, documentation on our readme io and the methods section in this paper for kind of a especially detailed description.

But, very briefly, just as a kind of broad reminder. So Monty is an architecture defined by a series of, semi-independent learning modules, that work together and receive information from sensorimotor modules and output goals via a, motor system. these semi-independent units communicate via a. Kind of common communication protocol we call the cortical messaging protocol. And this is defined by features that have a pose in space in a shared coordinate system, as well as other features like things like color object ID and so forth. And so this gives an overview of what Monty System looks like when it's perceiving something like a cup here. And the input Monty gets is from a very small region of space. So you can think of kind of the receptive field of any given learning module being defined by the sensorimotor module that it's getting input from. And typically this will be a small patch relative to the size of a total object sensorimotor. Modules responsibility is, outputting, pose and features. And so that's the C MP signal that gets sent to learning modules. And then the learning module is where the pose and features are received. these are then learned in, reference frames. we'll talk briefly about learning later, but as the system moves, it builds up a representation of where these poses and features are sensed. and then these same reference frames can be used during inference to develop hypotheses about on which object and where on which object, the system might be. but it can also then gather this information to have a hypothesis about what, the most likely ad it is, of the kind of objects it's perceiving is, as well as generate kind goal states. Enable it to act intelligently in the world. So basically, how does it want the kind of state of the motor system to be in order to achieve a certain goal?

And this is just showing that the, this kinda up here is just showing that the learning module changes over time as it receives new sensory input and outputs goal states. So that's a whistle stop tour. I said, please check out those other resources if all of that was kinda unfamiliar. But this paper was really about, starting with these, principles, sensorimotor modules with reference frames that perform learning and examining what is the, broad range of capabilities, that this gives you.

And before I talk about our first results with inference, it's worth just emphasizing that, Monty as a Century Motor system is all about moving. And whether it's learning or performing inference, it's constantly on the move. and this is what this kind of animation is showing in here. Just an example of Monty as a finger-like system. You're seeing a trace of its movement going along, the handle of something and then jumping over here. And so during learning, this might consist of a policy that's particularly good at densely sampling an object. if it's familiar. And with this kind of camera like agent we have, for example, we would look at an object like this mug doing a, scan. And then Monty would develop, part of a model of this object. Then it might observe it again, for example, from a, if the optic rotated from a different angle, and then use its observations from that second encounter to update, its memory and during inference it also moves. But in general, it might move in a more unstructured way. Certainly does in the case of, this paper, because here the goal is different from learning here the goal is about rapidly recognizing what objects it's seen rather than developing a new representation. and so you see for example, this kind of path as the system moves over the surface of the object before jumping over to, the handle, of the kind of the mug, as well as then the kind of rim.

And, kinda one of the first figures showing actual results in the paper is just trying to give you an intuition of what kind of imprints in Monty actually looks like. so that's what's, this is showing here where we have the kinda internal model, that Monty has of a mug, as well as a representation of where in space its sensorimotor module actually is, at that point in time. So what it seen is, as you can see, it's a extremely ambiguous sensation at that given point. That's equivalent to if you were touching this mug with your finger or if you were looking at it through a straw. and then on the right is the kind of evidence values associated with different objects that Monte knows about. And so what you see kinda at the top is that the first step, it initializes some hypotheses about what object it might be on based on what it's just seen. And so given this kind of red, color and the curvature of the surface, this seems to match reasonably, quite well with, the side of a mug to a certain degree with the side of a bowl, and not particularly well with the side of a white golf ball. Then over a series of movements, Monty integrates that, kind of movement that's taking place, to update these hypotheses and then receives new information. And over a kind of a course of an, episode, it then narrows down its hypotheses about where it might be. And so you can see how it gradually eliminates the bowl and quickly eliminates the golf ball as possible, locations. And then in the case of the mug, narrows down in space where it might be, based on what it's observing.

And the, dataset we used for all of our experiments was the 77, objects in the YCB data sets. This is a collection of household objects such as like a fork, banana screwdriver, and so forth, that have been 3D scanned, and made available. And during learning, Monty is presented, in general unless kind of noted otherwise. These objects at sort of 14 canonical rotations, and what we mean by that is the six rotations corresponding to the faces of a cube, as well as the eight rotations corresponding to the corners of a cube. For a total of 14. and you can see we have this kind of distant agent that does that more kind of scan like policy, the representation that develops across these views. And then the surface agent, which kind of moves more freely across the surface of the object, develops representations all over the object when given a single view. but because it does these narrow paths, it also benefits from multiple exposures to develop a good model.

So that's the kind of preamble. I should also maybe say that. Yeah. maybe it makes sense to leave kinda any discussion to the end. obviously if anyone on the kind of research team or otherwise, kinda notices I, I left something out, feel free to jump in and mention it.

So the kind of first main result in the paper is looking at inference and in particular the robustness of this. And so those same, 77 objects, that Monty has learned on are now used to evaluate the performance of the model. And we were looking at classification accuracy. So does it correctly recognize what object it's seen and also rotation error between measuring degrees. so how well is the predicted rotation? How well does that align to the actual rotation of the object in the environment when Monty sees it? And this very first kind of base condition in this figure, although it's the simplest, it's worth pointing out that. Although the object is presented at one of those 14 rotations that Monty has seen before because of the way kind of random seeds and policies work and so forth, the sensory path that Monty will follow on that object is actually different. and so it still needs to generalize, at least to a certain degree in the sense that it's not getting the exact same observations it's seen, before as it moves over the object. and as you can see, the accuracy is extremely high, close to a hundred percent. And the rotation error, the distribution shown, in this violin plot is, very low. But, we obviously wanted to make things more interesting. so that's where we then started introducing, noise. and we added this noise to a variety of features, of the model, or rather the kind of, perception that, Monty is getting in informed the CMP signal. That was things like the location of that, signal, as well as the color associated with the color feature. And you can see how this certainly starts to obscure some of the finer grained, details of the models, things like the rim, as well as the general shape of the handle. but fortunately Monty, still shows it's robust to this in terms of the accuracy and rotation error. It's not much change. The, noise is not just applied to color and location, which is visualized here, but also to the point normals and curvature directions and to the amount of curvature that is being sensed. And yeah, basically all the features that come into the learning module Yeah. That are important. No, good point.

But to yeah, really test, Monty's limit. We then introduced new rotations, so these are rotations that Monty has not seen at all, at any point in training data for any object. as well as, combining those new rotations with sensorimotor noise. And then finally adding in a new color where we basically set all the objects, all the observations that a Monte gets for any object to this kind of intense blue color. so it loses any kind of visual texture information it might otherwise get. from the object surface, you can see that there is a, kind of some drop in performance, but it's worth emphasizing that, this is a data set with 77 objects, five novel rotations. chance performance is very low in the order of kinda 1. and, Monty in general is doing quite well on this, given that even for a human, you can imagine that if you start, kinda obscuring information like color as well as adding noise to, the kind of locations of where things are being sensed, that becomes very difficult. Things like the fork and the spoon, the prongs, the fork become this kind of cloud of points, things like that. And there's also certain objects in the dataset, these series of cups, which are pretty much only distinguished by, their color. So the fact that, Monty gets such a high accuracy and, reasonable rotation error actually quite, encouraging. I think this is, doubly because all of this noise that we're adding at inference time is out of distribution. which is to say Monty is never trained on any of these perturbations in order to increase its robustness to these. and this is in general a very challenging thing to, to achieve, robustness often relies on a degree of interpolations. as long as you've densely sampled the space and then test something in between those points, then you can generalize. But here we're really seeing generalization that goes out of, the distribution that Monte has seen before.

So, yeah, so this is the first, result we were really excited to, to share.

and we'll talk a bit more about how this relates, how is Monte actually achieving this and how this relates to this concept of kind of, shape bias and, using shape to recognize objects.

in order to understand that a bit better, we then wanted to look at the kind of internal representations that Monty develops, and how these, relate to different objects and, the shapes of those objects. So in order to do this, one thing we can do is look at the evidence values associated with objects. So what I mean by that is. Monty, when it's sensing an object, it has a series of hypotheses inside each learning module, that increment over time based on movement and the next sensory observation. and that's what we're plotting here, is for a particular learning module. It has these, different hypotheses, and you can see how they're increasing over time. And, for the point of view of this diagram, this kind of gold point is, the point at which the spoon head is sensed. And so the, learning module becomes quite confident that it's seen the spoon as opposed to, something else. But you can see up until that point, the amount of evidence it has for these objects is quite similar. and these objects all have, as we'll get to in a moment, an elongated, narrow portion of their, shape. And so what I'll show on the next slide is we can look at these evidence values across a series of experiments, and in particular kind of see if mon is recognizing and actually sensing one object like the spoon. What other objects does it have similarly high evidence values for versus what objects does it have very low evidence values for?

And that's what is shown in this figure of the paper where we cluster, the different objects, these 10 different objects based on the evidence values that Monty has when it's recognizing a particular object. And what this shows is that. Quant naturally clusters, these objects based on shape or morphology that humans would also naturally, identify in these. So you have these kind of cutlery objects that are elongated with this, end to them. You have these box-like objects, and then you have these cup like objects. And, it's important that to emphasize that Monty doesn't have a kind of objective function with some supervised label that encourages this kind of clustering. This just naturally emerges, in the kind of representations it develops, from how it the hypotheses that it has when it's recognizing a particular object.

It also just kinda shows some of the things like, the dec cup here is a largely or is a yellow object as is, the most part the sugar box. You can see they are very far apart in this kind of clustering space. whereas the, mug like objects, are much closer. And then this mug, which has this kind of distinct handle that seems to lead to a more significant separation from these other cuffs. So there's a lot of almost, semantic, consistency it seems, in this clustering.

Then the kind of significance of all this is that, this emphasizes that Monty, uses, or is sensitive to the shape of objects. And this is almost certainly the kind of source of its robustness to noise that we saw earlier, including that situation where it had an entirely new color. Because if Monty is using global structure, and even if we perturb locations a bit, even if we add an entirely d different color, even if we rotate the object entirely, none of those, perturbations are changing the global, kinda structure of the object. Its shape. Obviously, if we added a huge amount of noise to those locations and it just became a random point cloud, then Monty's performance would dramatically fall in the same way that, that to a human, it would no longer look like, the kind of original object it was.

And this is something that's worth dwelling on because, recognizing objects by their shape and using that as the primary, basis of recognition is something that humans do naturally. but it has not been, property has emerged naturally in deep learning systems in the sense that already back in 2019, it was noted that, if you took a image like this cat and replaced the texture of the image, with something else like this elephant skin, but preserve the kind of global shape, deep learning systems would confidently classify it based on the texture, not on the shape. and this beca became referred to as this kind of texture versus shape bias. fast forward to today and despite training on, Virtually the entire Internet's worth of, kind of images, huge amount more data than, than humans, are exposed to. in the same way these systems still show this kind of texture shaped bias, obviously. Monty is looking at objects in 3D here, but, prior work with, kind of reference frames in 2D grid cells showed a similar effect where when you use reference frames with a sensorimotor system that moves through that reference frame and kind of binds observations to locations, you get this much stronger kind of reliance on shape and the kind of global, the global arrangement of features rather than this kind of sensitivity to high frequency textures that, seem to correlate with object classes and therefore seem to be picked up by deep learning systems.

I should maybe, sorry, clarify that. So this figure here, what this is showing is the degree of texture bias. So over here is classifying entirely based on texture, and over here is based on shape. And you can see humans are essentially entirely based on shape. And this is just showing where different, deep learning systems fall on that spectrum.

and this is, an exciting result even if we haven't gone all the way down of applying. Different textures to the objects like, was done in this prior study. this is still an exciting result because of, what it implies and, what we can look at in the future. Because adversarial examples are these kind of bizarre instances where you can add a imperceptible amount of, noise, targeted noise to an image such as this. And a deep learning system will confidently classify it as something else. and just like that texture bias, this is something that scaling deep learning systems, giving them more and more data has not solved. and it's almost certainly related. There's already good evidence, but it's almost certainly related to, shape bias because of course, by adding this, kind targeted noise, you are perturbing, the pixels in a very high frequency way. But the global shape to a human has remained entirely unal altered. it's not something we looked at in this paper, but, you could imagine there are ways you could create an adversarial example for Monty. But what that adversarial example would consist of would be rearranging the locations of the features, to convince it that it's seen a different object and the end effect would be changing. The actual identity of the object is something else. So essentially something that would also fool a human and therefore, a totally different, type of phenomenon from these adversarial examples.

so, far, we've been looking at robustness in this, sort of umbrella of noise, but that's not the kind of only. And, various kind of types of noise and perturbation. but robustness goes beyond that. And in particular, it's really important for a good representation to have these properties of what are known as equ variance and in variants. And basically, equ variant means that it changes a representation changes as, the state of the world changes. And variant means it doesn't change. And for an intelligent system to be useful, it should have a mixture of equ variant and invariant representations under different conditions. and I'll try and make that a bit more concrete. So you can imagine if you were looking at these, pencils, although the one on the right has a different color and a slightly different shape, it's shorter and so forth, and a bit, wider, you still recognize these as pencils. And so your representation of a pencil is inva to these, changes in details such as color or certain changes in shape. and you would also be robust to, for example, seeing this pencil in a dark room.

but if you turn the pencil, like this, these two different kind of ways of holding it in a hand, your representation of where the pencil is in, in space and how it's oriented, changes. So that means that representation is equ variant. On the other hand, if you rotate it on the kind of long axis of the pencil, then as far as you're concerned, it's basically in the same orientation. And so those rotations are symmetric. So for certain rotations, you want the system to be in variant to recognize symmetry. but for others you want it to be equ variant. That's a, it sounds simple because it's something that humans do naturally, all the time, but it's actually a really difficult property to bake into, a learning system. And another really exciting, result, with Monty was that we found that these kind of symmetry representations emerge essentially naturally.

and in particular, you can imagine having an object like this, cup made up of, points, and it can have a variety of rotations which are inherently ambiguous. So these, different kind of ones here are showing the same model, but rotated by these different amounts, represented here. And as far as a human is concerned, these look like they're the same rotation. but a rotation like this looks different. What's, interesting about Monty is as it's recognizing an object and sensing it, it has these hypotheses about the potential rotation of the object, and those are informed by what it's sensing. So it's, this is a detail that's maybe best necessarily if you look at the paper, but it's not like Monty has a list of rotations that has experienced before and it's only going to look at those, that list of rotations. It's it senses the object based on that. It can, have a hypothesis about where it would be oriented, but as it's moving across this. It's going to have a series of hypotheses that are all valid, that are all getting sensory observations that are consistent with the model, with that rotation. On the other hand, the hypothesis associated with this rotation of the object would not be getting consistent evidence. And so the evidence value associated with this would quickly decay and decrease. What happens in Monty then is if it has a series of these kinds of hypotheses that are persistent together for a long time, so for one object, a set of rotations that are all persistently active, it then eventually reaches a kind of threshold, a state at which it says, okay, I believe these objects to be, symmetric. and so this was what we called sensorimotor symmetric because the way that symmetry is, determined, is through sensorimotor exploration of the object. and Monty can continue sensorimotor exploration as to as long as it would like in order to verify with greater and greater certainty the existence of symmetry.

in order to then examine this a bit further in the paper, we wanted to use an additional metric. So I mentioned earlier that we use rotation error, generally when reporting how good Monte is at predicting the rotation object. That's because it's very intuitive metric. but a kind of common metric in the literature for measuring more symmetry type, properties is something known as the chamfer distance. Which you can basically think of as a kind of metric for the distance between two sets of points. So if you had a point cloud, like this blue one and this orange one, you would basically for each point in the orange one, find its nearest neighbor in the blue set of points, calculate that distance and then do the same thing for all the other points. Sum those up and then do the same thing for the blue one, comparing to the orange. and what you'd end up with is a metric that's high for two kind of sets of points like these and low for two sets of points like these.

What, the kind of advantage of using this metric is that if an objects, model is kinda symmetric, and then you apply different rotations, as long as that symmetry is true, then you would get a, low cha distance. And so that's what this figure in the paper, was looking at. So what you're seeing here is a single example of this particular object, this green cup, excuse me. This is its ground truth rotation. And then, there are a series of rotations we kinda report here. on the left is the rotation error in degrees, which again is what we generally report throughout the pa paper. And then on the right is the cham for distance. I was just describing the kind of different types of rotations we have here are what's called min. So this is of all the rotations that Monty has a hypothesis about. this is the one with the smallest, rotation error to, the ground truth one, the most likely hypo hypothesis. This is just the rotation that happens to have the most evidence at that, given point in time. Although in general, what we would find, if you were to look at the evidence values for these different rotations, they would all be extremely similar. and then a symmetric rotation here is just ano an example of another rotation in these, in the set of symmetric rotations. According to Monty, it's just one of these other rotations, that is deemed symmetric. that doesn't happen to be the minimum one relative to ground truth, and it doesn't happen to be the most likely hypothesis. And then lastly, this is just a random rotation, which Monty does not believe to be symmetric. And what this shows is that we get a, very low cha distance associated with all three of these rotations. when we apply these to the kind of learn model consistent with those having some true symmetry, and consistent with human perception. whereas the random rotation, the kinda purple bar here, the champion distance, does not on the other hand, this figure also shows that the rotation error, can vary wildly, depending on, which, particular rotation is, reported. and so this is why in the paper we actually report the minimum, rotation of this set. In order to essentially, capture the, true error associated, while avoiding the kind of, lack of intuition that the champ for distance gives, but as a additional sanity check. So this is from one example, we, ran the same metric over many different objects that Monty sees. and again, looked at the rotation error and the cha distance associated with the, these kinds of different hypotheses that Monty developed, and in particular the ones that it believe were symmetric. And what you see here is, again, across all of these different examples, the cha distance associated with kind of any rotation that Monty de symmetric is very similar to the kind of one with the, minimal, rotation error versus the, ground truth, which kind of supports the, metric we ultimately used. So that was getting into a lot of details about how we measure symmetry and that sort of thing. But I think it's worth taking a step back and just emphasizing, how exciting it is that, that Monty developed this. And again, this wasn't something where we have a specific objective function where Monty needs to, report symmetry or where it has access to ground truth models in order to measure, its ability to detect symmetry. This is just something that emerged naturally through having a sensorimotor system kinda move over an object. Try and recognize its rotation and the hypotheses that it was developing.

and this is, a really, great, attribute to have in the system in terms of kind of long-term future capabilities. So you can imagine going back to that pencil example, that depending on its orientation, if you're learning to do something like writing or erasing, that will depend a lot on the orientation of the pencil along certain axes, along certain kind of, rotations of symmetry. On the other hand, if you again, spin the, pencil in your hand, that has no effect on the ability of this pencil tip to write. And so it's really important that every time you spin the pencil, your system doesn't say, oh, this is a new rotation. I have to now relearn how to use this, particular object. in this particular case, it's about kind of behavior and how you interact with an object, but this also applies to compositional representations where one object is part of another object and so forth. we were really happy to see, this kind of symmetry emerge, the way it did.

So that was a lot about, robust inference. next I'm gonna be talking about rapid inference, and how quickly kind Monty can recognize what it's seen in the world. And it's worth, kinda starting by just emphasizing, biological perception is inherently about movement, and When we look around the world, only a tiny fraction at the center of our visual field is actually in high acuity. if our, that part of our eye, the, kind of foia, if it had the same visual acuity just a few degrees out, we would be legally blind in terms of, the amount of sensory impairment that would give us. So we're hugely dependent on being able to rapidly move our eyes, but of course, this applies to other, modalities, like our hands and so forth.

this is, naturally baked into to Monty, but up until now we've not talked a lot about, how Monty is actually moving. and so one of the other kind of exciting things, that we kind we're showing in this paper is how it can use, policies to act intelligently, and a mixture of what known as kinda model free and model-based policies.

So model free policies are where you are just taking the kind sensory information coming in and without an explicit model deciding how to act in the world. and so in the paper we look at this kind of surface curvature guided exploration policy where Monty kind of follows along, the kind of surface of an object, maintaining contact with it based on what it's sensing. And also when it identifies areas of prominent curvature, like the rim of this glass or the kind of bottom of this cup, it'll, it, sometimes then follows that for kind of a period of time. This was our kind of model free one. And so this is the kind of, thing that, older parts of the brain can be kinda responsible for in humans, but can still be, quite powerful. but the other kind of policy we look at is this model-based one that we call the hypothesis testing policy. the reason it has this name is because Monty essentially has a series of hypotheses and it's going to act in a way to test those hypotheses and eliminate one of them. and so what this figure in the paper was showing is, this is the actual object that Monty is sensing as it is in the world. So in this case, this spoon, it starts on this part of the, starts on this part of the handle, moves along the bottom of the handle. And then at that point, Monty, or rather the one of the learning modules in Monty, outputs a goal state to move to the head of the spoon, in order to disambiguate it from another object. How does it decide to move to the head of the spoon? This is where it's making use of its internal models, hence the term model-based policy. And in particular, Monty has, at this point a strong hypothesis for the spoon in this particular orientation and the fork in this particular orientation. And it can, mentally compare these two hypotheses and through that determine that the kind of largest distance between these models, the kind of, biggest anomaly between their structure. Is, here at the kind of head of the object, at the kind of tip of the spoon. And so that would be the best place to, move. You can imagine if it moved to say, this location, the neck, which it hasn't explored yet, it's going to observe the same thing whether it's on, the fork or the spoon. And so that's a less useful observation to get next.

And I've shifted the figure over, and then this is showing the next part of that figure where we, plotted the evidence values that Monty had, internally as a function of the kinda step in the episode. And when it takes, this kind of goal, driven step. So you can see that the hypotheses for most of these objects are, that have a similar shape, a similar morphology like spoon, the fork, the knife, and a marker get a reasonable amount of, increase as, time is going by. But at this point at which, Monty can executes this, gold state, it suddenly gets this kind of divergence in the hypothesis because now all of the observations after this point are corresponding to this head of the spoon, and are not consistent basically with a for or a knife.

what this then, next figure in the paper is showing is that same thing can also be done for poses. You can imagine rather than disin, vigorating, two different objects. If Monty is pretty confident it's on the mug, but it's not actually sure where the mug is or how the mug is oriented in space, it can use the same principles to, to yeah, infer, this. And in this case, the two hypothesized rotations, that are deemed most likely result in different orientations of the handle. And so based on one of these hypotheses, Monty's going to attempt to move to that handle. and you see again that once it goes to that location and executes the skull state, all of the hypotheses associated with incompatible observations, suddenly start losing, evidence.

and so we then looked at what effect this has on imprints as a whole. And in particular, if we kinda look at accuracy as well as number of steps in a total episode, we can see that with, baseline policy where we're performing more like a random walk, without kind of any, kind of consistent direction or being informed by, curvature. This is the kinda model three policy here. and also without, the kind of model based goal states, we get a reasonable accuracy, but we get a lot of instances of timed out episodes. These are instances where Monty still had the correct hypothesis at the end of the episode. So if it was sensing the mug, it believes the most likely object is the mug, but it's in a low confidence state about that. likely because it hasn't seen a feature that would without a doubt disambiguate it from other objects. And we set a max of 500 steps for each episode. And so these kind of times out episodes are what correspond to these kinds of, episodes appear that essentially continue until the end. On the other hand, if Monty becomes very confident about what it's observing, it can then terminate, the episode and, declare this is the object, I believe it is. And that's what we see here is a significant increase in these kind of converged episodes when we introduce the model free and then the model-based policies.

and that's reflected over here in terms of the kind of number of steps associated with the, episode. It's interesting that, we already get such strong performance with the Model three policy, that the model-based policy only gives kind of some incremental improvements in inaccuracy. it's not, a huge change, at that point, but, we're still really excited about kind of implications of this policy because of what it means in terms of, acting intelligently in the world. So it seems to be the case that for the benchmarks we have at the moment, being able to efficiently do something like move to the head of the spoon. it doesn't result in dramatic change in improvement, but as you introduce more noise, or, need to act more quickly, to carry out certain goals, then this kind of distinguishing, benefit is likely to become more apparent. and one of the kind of other elements about the sort of model-based policy that we're quite happy with was all of this learning of the model is taking place independently of kind of explicit rewards. that is to say, Monte is just exploring these objects and learning about it in the same way that if you were in a new, city, or if you were given a new object to hold, you would, very quickly learn its properties without necessarily being told by someone that you know, you're going to be, paid or have some sort of task to complete, that requires you, solving that it's just this kind of natural, ability to quickly learn about the world. and this fits really well with what was observed by, Tolman all the way back in 1948 when he studied learning in rats, where if you put them, in a maze, they will just naturally learn a lot of the structure of this maze, even when they don't have a reward such as food. and so this kind of is key really to, if you're going to be able to develop representations that can enable flexible behavior, because the reality is. Reward signals in the world are sparse, and rarely perceived. what you really want is a system that naturally goes out there and just learns about, what there is.

So that was all the stuff about rapid inference and, policies. the next bit I'll be talking about rapid inference and voting. Voting is this particular algorithm that, Numenta had originally proposed in the, context of, cortical columns and has then been adapted in Monty and also, evolved to, enable it to work well.

and to motivated or, just put it into context. So we've been talking a lot about the importance of movement and how, biological perception is inherently, sensorimotor. It's all about, movement really. But give, with that said, of course, we are also able to integrate, inputs from, multiple sensory organs, either from different parts of, for example, our retina or from, for example, our hand, our, our eyes. It's not the case, that we actually perceive the world in kind of total, straw world, way like this. and so the question is, okay, how do you integrate this information in a way that still gives, robustness, and enables more rapid inference?

The voting algorithms, fairly involved. and so I won't go into the full details here. there's hopefully, a helpful description in the, method section of the paper, that goes into the details. But, at a high level, you have different learning modules that are receiving, inputs from different sensory modules. So for example, a left hand and a right hand here. And each of these learning modules has some hypotheses about what objects it seemed. And with voting, they're basically able to share those hypotheses. But one important thing is when those hypotheses are sent, we account for the sensory displacement between, the sensorimotor modules, when transforming them to be usable by the other learning module. This is important because we don't want to just vote on say object, it just that Okay, I think I'm on a mug. Are you on a mug as well? What that would result in is a bag of features type, form of recognition, where as long as they're sensing locally something that could be a mug, they will all agree it's a mug, even if it was totally scrambled. and that's exactly the kind of more sort of texture bias, lack of shape, robustness, that humans don't have and that we don't want Monty to have. And so that's why there's this, kinda use of the sensorimotor displacement, when, carrying out voting.

Practically what this means is that, you can have kinda multiple sensory patches. So for example, here you have different sensory patches ranged in an approximate grid. and those are each seen a different part of an object at a given point in time. They're each connected to a learning module, and then those learning modules are able to communicate with one another via voting. and by each scene part of the object and also sharing their votes, we can dramatically speed up inference. and so this animation is just showing how at any given point in time, a particular learning module can be seen something different. no learning module is seen. This, is a kind of viewfinder view that's just for the, kind of benefit of, us as the experimenter.

And when we're doing voting, we can parameterize how many of these patches we have. So you can imagine at least in this kind of vision, like case, this is the size of the grid. so the number of C modules and, associated learning modules, the number of pairs of these. And here there's five. in this example, there are eight.

the other thing we can parameterize is how many of those learning modules must converge for the Monty system as a whole to converge? So we don't necessarily want to wait for all of, the learning modules to say I'm confident in what I'm seeing, because some of them may not be getting much input. And so in this paper, we just, set this two. So we need two learning modules to, converge for Monty as a whole to converge. We then look at what kind of agreement they have on, the classification. And then the experiment we did was to vary the total number of these, essentially the size of this, grid.

And so that's what's shown here where we see the number of steps, until convergence as a function of the number of learning modules, and exactly as we would've hoped. we see kinda a, very rapid, reduction in how many steps are required, for the system to converge, as we add in more learning modules. almost immediately eliminating many of these timeout episodes associated with 500 steps, and getting down to close to, we actually have a minimum number of steps that are required before an episode can converge. And, it essentially reaches that point.

at the same time, we don't want this rapid inference to be at the cost of accuracy. And so this plot on the right is basically reassurance that we can achieve this without, having this kind of reduction in robustness. If, as I mentioned earlier, we use a more, more naive algorithm like that bag of features, we would almost certainly see, a decrease in accuracy as we started adding in these kinds of, non meaningful, hypotheses, being shared.

to then wrap up the kind of discussion voting, I think it's worth pointing out that, this is a great thing for Monty to have to be able to benefit from a multitude of sensory inputs coming at once. but it's important to emphasize that while Monty benefits from it, it is not reliant on it. So in the same way that you can feel a coffee mug with one finger or look at the world through a straw and still recognize and understand what you're seeing, Monty can operate in this kind of single learning module, regime without issue. It's just that it benefits from, having these multiple inputs. And this is a really important kind of, not really assumption, but more like defining characteristic of Monty, that we want to maintain. Because no matter how, no matter how artificial the setup may be, if you are in the real world, your sensors are inherently going to be limited in terms of sampling information. You're never going to be able to sample everything at once. So the sooner you get out of that frame of mind and accept that information needs to be integrated over time, the more you know, I believe you converge to architectures like Monty, where movement is the kind of central motif of the system.

Alright, so that was, we talked first about robust imprints and then about rapid learning. Sorry, rapid inference. now, the kind of final main section of the paper was then looking at rapid, continual and efficient learning. yeah, a bit of a mouthful, but I will break that down.

it's worth noting that in this part of the paper we do compare to, VIT, so vision transformer networks form of deep learning, architectures, to really ground the results. we're not trying to imply that deep learning doesn't have useful applications we use, deep learning systems ourselves all the time. but the point really with these results is that, Monty is a fundamentally different approach, with some really unique advantages, which is what we're kinda excited to talk about today.

for the kind of first thing we looked at was, how rapidly Monty could, learn what's sometimes referred to as few shot learning in particular, how quickly can it develop good representations given only a few observations of a particular object. So in order to evaluate this, we take, all the objects in the YCB dataset, and we present each one at a fixed number of rotations. for example, when, the system either VIT or Monty is given one object view across all YCB objects, it will just get a single view. If it's given two views across all YCB objects, it will get two views. Then so forth to 16 views, up until 32. And after each of these conditions we look at, given that amount of training data, how well does the system perform both in terms of classification, accuracy and, rotation error.

What we find is monthly learns extremely rapidly getting so Monty here in blue, 50 classification accuracy after only a single observation very quickly reaches close to, its kinda optimal performance. The systems we compare to are VITs under a variety of conditions. So the strongest performing VIT is this pre-trained one, which only needs 25 epochs of training to perform at its best. but this model has been trained, pre-trained rather on 14 million images scraped from the internet and with labels. So classification of household objects is definitely not out of distribution, for this network. and so it's, yeah, telling I guess that Monty, which, knows nothing about these objects, can achieve essentially comparable performance. And in a moment, we'll see, firstly how they differ on the rotation error, but also, how Monty does in terms of computational efficiency. if the VIT is trained from scratch. for 70 feet of five epoch. So it's seen, although it's only getting a certain number of rotations, it's revisiting those rotations. many times we get better performance, but still, significantly below, Monty, which importantly only sees each object once. and so in the same way that humans can learn extremely rapidly, we don't need to, pick up a new object and look at it close to a hundred times before we can recognize it. Monty really just needs to see, something once. And so as a comparison, we include a v it, with one ePEP of training. And this is the only it that gets the exact same amount of data exposure at Monty. And as you can see, it performs, approximately around chance. it's just not nearly enough data for a deep learning system to learn.

A stark difference appears when we look at the rotation error, which isn't too surprising because in this case, even for the pre-trained VIT, it's, never had an objective to explicitly, predict rotation error. and so for the kind of network to develop disability, it's just not feasible, with this amount of training data. But again, Monty, shows this really rapid learning, as it sees more rotations.

Understanding a bit more where this generalization comes from. And one of the kinda key aspects is what I was talking about before, that Monte naturally infers the potential rotation of the object based on what it's sensing. So if it senses, this kind of corner here, for example, at the start of an episode that could be consistent with the spam can in this orientation or in this orientation because there's this kind of similar edge here.

those kind of hypotheses will both, exist and then Monty will move over the object. But importantly, this hypothesis of the kind of can being inverted is not dependent on having ever seen the can inverted in the same way that you could recognize an object upside down that you have never seen upside down. by just inferring that it's in that orientation, Monty is able to, do the same. And so after seeing only a single rotation, only a single view of each object in the dataset, Monty already gets 50 classification on a dataset with 77 objects. and the, these are shown at novel rotations. the kind of other element to this is that of course many objects have, natural symmetry that is, even if we present different views, some of their kind of, aspects will be similar to the view that, Monty or the VIT saw on the first one. But it's really important to emphasize that. Without this handling of rotation, generally a system cannot just rely on this sort of natural symmetry of faces to do well. and so that's why we, don't see similar performance in the BIT, trained from scratch.

It's also maybe helpful to understand how is Monty able to learn, so quickly. and so I think a useful intuition for this is to think about, to conceptualize the reference frames as Matt. And in the same way that if you were traveling around, city, let's say you were a tourist visiting Rome, and you went to say Peter's basilica, you visited there and then you travel, you see kinda a bridge, as you go east of the city and then eventually, reach kinda another landmark. As you're going through and seeing these different, parts of the city, you will just kinda associate what you have seen with that location. This is something you'll just do totally naturally and extremely quickly. with kind of spatial navigation. A lot of this relies on, an slightly older structure, in mammals, the hippocampal complex, but the kind of principles are the same that you are performing movement and then essentially binding information to a location in a map. In this case, the map is the reference frame that the learning module or in humans, cortical column, is using. And It's very unambiguous where, the system has to update its representation when, learning something new because of this use of a map. And this is going to lead to other benefits, which I'll touch on in a moment in terms of continual learning, and computational efficiency.

longstanding, problem in, machine learning, deep learning in particular, is this issue of catastrophic forgetting where if you train a system, on kind of one task, and then you use, for example, deep learning network with back propagation of error, and then you gen kinda get it to learn a different task, you will perform all of these global dates to its weights, essentially entirely. Forget about the, task it, it knew before. and this is the kind of catastrophic forgetting, phenomenon. Continual learning then is the kinda inverse of this. If you are resistant to catastrophic forgetting, you were able to learn continually. and this is something that we, found, in Monty when, when we looked and it was actually something that we expected to be there simply from how the kind of system works.

so to more concretely show what the, task setup we have is, so I'll start by describing it. So we, in order to, assess continual learning, we have the 77 objects and. We break this up into kind of 77 tasks. And so any given task, Monty will learn one object or the VIT will learn one object given all of its kind of canonical rotations, those 14 rotations I discussed before. And then we're going to evaluate on all the objects that have been seen until then. so in order for the system to do well, it has to both learn about the objects. It's just seen as well as retain a memory and information about the objects it saw before. We then learn on the next object and then repeat.

and so to show this kinda more concretely you can imagine. So if the first object is the Lego dataset, kinda learn it on 14 rotations. The rotations aren't shown here. and then we're gonna infer with some novel rotations and see how the system performs again, either the VIT or Monty. For the, second task, we're going to learn on the fork, but this time we're going to infer and assess kind of performance on both the Lego object and the fourth, because both of those have been observed. The third task, it learns the banana, but it has to then, perform recognition on all three of these objects.

So with that kind of line out, and you see here the actual performance that we observed. So on the Y axis we have the accuracy on all observed objects.

how the system kind of performs is dependent on, how many objects it's seen. Then on the X axis, we have the number of objects that have actually been learned. And what we see with Monty is what we would expect, where, it does really well, to start because it only knows about a few objects. And so it quickly, decides that, the object that's observing. And then over the course of the, kind of task, it gets a little bit of interference because naturally it will learn about a new object, say the peach, that is similar to an object that already knows about the apple. And so that's inherently going to make future inference more difficult, but it is not catastrophic, which is, what we see with the, pre-trained VIT. so we used the pre-trained once since it was the most, the kind of strongest performing network in the previous, rapid learning task. And, for the very first task, it only has one object it can possibly classify as. so it, achieves a hundred percent, but then almost immediately it, begins consistently overriding its weights to predict the new, object that it is seen. and then obliterates information, it has in its weights about recognizing other objects.

And this is shown in a kind of, alternative view, where, we have the number of objects that have been learned and then the target object. and Along the kind of diagonal is the performance, on the current task. And so in general, you would expect if the system can learn the current task, the diagonal should be green. And that's what we see, for both models. But if we look below the diagonal, this is the performance of the system on all the objects that it's seen, before. And so what this kind of shows is that basically Monty is able to retain, a memory of many of these objects. It seems like there's maybe certain objects that are just difficult for Monty to learn for various reasons, perhaps ambiguous shape, and so forth. Whereas the VIT again is catastrophically overriding its weights and shows almost no evidence of being able to recall these previous objects. It's learned, and it, I think it's worth just emphasizing the importance of continual learning. as a task structure, it's, especially even this extreme case we consider here where each task consists of an all, only a single object. So deep learning systems are inherently dependent on very kind of contrastive learning in order to develop representations, in order to distinguish kind of two objects needs to see one and then the other, and essentially learn to separate those representations and do this constantly in kind of a shuffled order. But of course, when you're moving through the world. the statistics are, constantly changing personally and so constantly leading to the problem of catastrophic getting. but the kind of inputs are also highly temporarily correlated. if you were learning about an edible fruit, and you were a hunter gatherer or another animal, you would, naturally find these kinds of things clustered together. You're not going to get a lineup of all possible fruit that you might want to eat and all fruit that might be poisonous. and then be able to hold those side by side and compare them. and so being able to learn in isolation, just given what's in front of you, through, movement and, what we explored with Monte, is, really crucial to then understand how Monte is, why is Monte robust to continual learning? I think it's, again, useful to consider this, math analogy. So let's say again, you are in, Rome, you're going around, you're seeing some new things. If you then see something new and you are making a note to remember that, you're in Rome, so you're only going to update your kind of internal representation of, your kind of map. You're not going to make changes to all these other maps you have for other cities, that may be partially or are very familiar to. Targeted what could be called local or sparse. Updating to the model is exactly what Monty is doing, when it sees something new. and this means that we don't get this kind of issue of catastrophic, forgetting. as we'll see in the moment, we also get some benefits in terms of computational efficiency.

and in particular, we see some really dramatic improvements in, the computational efficiency of Monty when it comes to learning, when compared to these VIT models. and actually some decent, performance as well, in the kind of domain of, imprints.

And in order to look at the, kinda computational efficiency, we quantify these, this with floating point operations. So you can think of this as, any kind of, as the name implies, floating point operation happening in the computer. And so a variety of algorithms, whether it is a VIT, a deep pointing system, or, it's, how we've implemented Monty. These algorithms need to execute these operations on a computer in order to carry out a task. And so we're basically just counting how many of these, flops in total are being done. we're not concerned with how many happen per second. Just to clarify, sometimes flops with an s refers to per second. Here we're just counting the total number of floating point operations.

At learning, we see a huge, difference between Monty and these other systems. so we have training floss along, the xxs here, and this is a logarithmic scale, going up to 10 to the power of 20, where Monty is all the way down here at 10 to the power of 11. and this is all the learning that Monty does. The VIT here, which if you recall from earlier, did significantly worth both in terms of accuracy, classification and also, rotation error. still uses, a huge amount forwarders magnitude more, flops in order to perform learning. The pre-trained VIT, which was the only one that could do comparably on the classification task, uses both a, far more flops in the kind of fine tuning stage, but particularly staggering is the amount of acute needed for the pre-training stage.

so this is where, yeah, Monty really stands out in terms of, computational efficiency.

if kind of inference, we can do kinda a similar comparison. and what we show here is a variety of kind of different VIT models of different sizes. So generally as you go along the right towards, more flops, these are VIT models that have more parameters and, often more parameters and deep learning results in better performance.

and these are either the kind of pre-trained variant, if they have the filled in circle or from scratch for Monty. Then we look at, a system that has a random walk. So this is the kind of distant agent that is kinda like a camera performing a random walk in all our VIT comparisons until now. And also here, this is the agent we use, even though this is definitely the worst performing, Monty system. The reason we did that was to make the comparison fair because the VIT sees the object from one side. and so Monty also sees it from one side. However, Monty is a sensorimotor system. It's much more natural for it to engage its policies to act intelligently in the world. And that's what we see here with this, system that has access to the hypothesis test and policy. And what's, really nice is even the baseline, does extremely well in terms of kind of the accuracy versus flops straight off. particularly with the rotation error. But when we add in this policy, rather than making the system more complex and resulting in more flops, it actually dramatically reduces the number of flops, while also improving the accuracy and reducing the rotation error. So it's just a, nice demonstration of the real kind of benefits of bringing in these kinds of sensorimotor concepts, into, yeah, learning, intelligent systems.

to then understand the flop efficiency is a very similar concept to before, at learning. again, if you, are in Rome and you're laying down a representation, you're only going to do that, laying down kinda a memory. You're only going to do that in your map. That means you don't have to update all of the other firstly. And, where, sorry. And I should say, you know where in Rome you are as well. So you don't have to make changes to all these other locations on your map, and you don't have to make locations to all the other maps for other cities. So to very, a local, again, very sparse, change to the system. That means, very few floating point operations. and during inference, what tends to happen is, even though Monty has a relatively broad, search space of hypotheses that it's considering, at the first sensation, it generally considers all 77 objects relatively, possible because, if you just felt the surface of something you wouldn't really have a good idea of what it is you're touching. And also many rotations are gonna be possible. so it gen tends to begin with a large search space, which does involve a lot of, floating point operations. But as it senses more of the object and becomes more confident, it can quickly rule out, a lot of hypotheses. And that's visualized here, where, this is now during inference, you're trying to figure out what city you're in. Because of the, landmarks you've already seen and the movement you've done, you're pretty accompanied somewhere in Rome. You're just trying to figure out where, and you can eliminate, all of these other hypotheses as ones that need testing. And so that significantly reduces, the amount of, flops that accrue.

I think to conclude this section on the sort of rapid, continual and efficient learning, I think it's, really important to emphasize that these are all kind of natural consequences that emerged when we were working with sensorimotor learning with reference frames. So it wasn't the case that we carried out some laborious or targeted optimization to extract or bake these sort of properties. It just naturally emerges from the way, learning happens. And I think this is one of the reasons, we're so excited about the direction that Thousand Brains Projects is, taking and, the progress it's making. Because all of this was originally informed by, hypotheses about how the brain, works, how the cortex, where it's based on no neuroanatomy, no neurophysiology. We then kinda implemented the system as said to kinda be able to achieve some basic properties like recognizing objects and very naturally, properties that have been, very difficult to achieve in, machine learning from, shape bias to, symmetry detection, to continual learning, to efficient learning are all naturally, popping out. So we think that's, A good indication that, we're, approximately on the right track.

I think it's also worth emphasizing that lifelong learning is, a huge thing that, we want to be able to develop to have, intelligent systems. And often this focuses on the problem of continual learning because that is such a, kinda major hurdle, that hasn't been overcome. But I think it's important to emphasize that continual learning and avoiding constructive getting is just one element of that. Being able to learn rapidly so very quickly, and also being able to learn efficiently. So not needing to use, entire GPU clusters every time you want to train your model, is extremely important to unlocking that capability. And it's, exciting that Monty seems to, show evidence of kind of all three of those, already, this, the images here where it gets to show some lifelong learners, some adults. And then this is just, an example of a totally novel object you've never seen before, but probably after a few glances, you already had a kinda reasonable representation, of what it was. You didn't need to compare it to some other objects that you may have seen before or sleep in order to initiate recall type, contrastive learning or do any of these things. you were able to just, learn it. Yeah, almost instantaneously.

So that kind of concludes the, results, that we kinda discussed in the paper. I just thought I'd very briefly discuss, what we talk about conclusion, which is, the future research we're working on at the moment. One of the key things we're really excited about is compositional objects. So this is something which the Paper Viviane is gonna separately be talking about, in a bit. our hierarchy paper, which looks at neuroanatomy, has, a lot of interesting ideas for how the brain might be doing this. And so in the same way that Monty is a manifestation of the Thousand Brains Theory from a couple years ago, now we're taking some of these, kind newer neuroscience theory ideas, and bringing them into kind of the latest version of Monty. And this is where we're going to be able to develop, more complex, more interesting, hierarchical representations of objects, a love, like a mug with a logo on it.

and then the other kind of thing we're working on, the main other main kind of, research we're doing is on object behaviors. this is more in the, theoretical stage where we're still figuring out exactly how we believe the brain figures out and represents objects that move and objects that can be interacted with, and so forth. But we think, we've made some interesting progress and look forward to sharing more with everyone soon.

Then the last thing to, mention is, all this has been very kind, fundamental research, but of course our long-term hope is it's gonna have a variety of beneficial, applications downstream in the real world. everything from agricultural robotics to, the kind of energy, sector and kind of ensuring that, equipment, is kept, maintained to things like medical ultrasound. these are in general instances where you may not have much data, where again, the distribution of the real world can constantly change. you cannot, rely on features that may change as a, result of lighting and, wind, rain, weather, so forth. You really need to rely on things like shape all these things that kinda Monty was showing in this paper should really, be useful in kind of these downstream applications. So that's something we're looking forward to.

Thanks very much for listening.