Great, yeah, today I'm going to be talking about, action policies, and, how they exist in Monty, looking a bit, covering mostly how they are now, but also looking a bit more towards the future, and it's, too much to probably cover just in this meeting, I plan to talk about, the different types of agents we have, right now, as well as these kind of two, entry level policies. So utility policies, and then, which I'll explain what that is, and then input driven, sensory driven policies, and then, the next meeting I can talk about, these other ones. And then, throughout this, I'll try and touch on, We're relevant kind of abstract space and how that might fit in.

and yeah, really the, kind of core aim behind our work on policies, in the, kind of short term is, really about getting robust and efficient, object recognition working as well as pose recognition.

and so we want Monty to act in a principled way, obviously not randomly. and, for example, be able to handle things like similar objects. Which might be ambiguous and solve that ambiguity as quickly as possible. and ideally those decisions should also be arrived at quickly and without kind of huge computational, costs.

but, yeah, it's alluded to with the abstract spaces thing. We're, ultimately Monty is obviously not just about 3d object recognition and similarly the policies are not just about, enabling 3d object recognition. And While we're working on this, we're also trying to have a view to the longer term aims of Monty and how those will fit in. You also, you had it grayed out in your previous slide, but perhaps, beyond this, it's really using action policies to affect change in the world, right? To bring about goals, things like that. Which is most, most people would think that's, hey, that's what, that's why we move, right? As opposed to, oh, we move to learn and, so let's not forget that. I know you did it on your previous life. It's a good way of maybe summarizing kind of the existing policies are really focused on this kind of perceptual element. but yeah, in the future, as you say, it's going to be super important how you affect change in the world. This is this is like the backgrounds that we have to get right before we go on and do okay, this thing's going to act in the world. Right.

so first just briefly a word on Monty agents. I'm going to assume kind of some background awareness of Monty, but, whether you're reading the documentation or, those who recently joined the team, you probably be aware that there's the kind of surface agent and the distant agent.

and we have this separation, but it, you can, you might start wondering what does this separation really mean? Because, Monty is of course an artificial system. So if it's implemented in the real world. There's nothing to stop us from, putting a camera on a robotic arm. And indeed, the way that the sort of surface agent, the sort of finger, if you will, is implemented at the moment is actually with a depth camera that just maintains a very close distance.

and so it can also get, color information.

so I think it's useful just clarifying, really, the differences, are fundamental. and I guess the way I'd try and break it down is you can think of the distant agent, obviously think of an eye. and, how that moves in space and how that samples information and, of course, that can sample very rapidly, through saccades, it can move, from one side of an object to the other side of an object almost instantaneously, and with, peripheral vision, for example, you get a broad overview that could, maybe at a coarser level, give you some information about what's out there, and then you can choose to sample that information with, higher fidelity, either through focusing the, distant agent sensor, the, kind of highest, resolution sensors on it, like the fovea and the human eye, or perhaps through, moving your hand. or some other sensor to that location, to sample it in more detail. But you also, with the distant agent, you just get one view of the object, so there's a lot of self occlusion of the object compared to the surface agent. Yeah, exactly. Yeah, so it has some I didn't understand that comment. Could you explain that? if I'm looking at an object, I just have one angle at it. I can't just move my eye to the other side of the object, like I can saccade. I could move the entire agent to the other side of the object. But the eye can't just like change. Oh, are you saying that I can move my, I can move my, sensor if it's like a finger, I can move it on the, around the other side. But yeah, if it's my eye, I have to move the object to move my body. Yeah, exactly. But moving my body is this a slower, more cumbersome way of moving your hand, right? It's the same basic idea. And I guess a Monty agent system could have like cameras located around a room or something like that. but yeah, just, yeah. At least it's more common. I think it's fair to say that, yeah, self inclusion is a more frequent problem for a distant agent than. I just, the word self occlusion confused me. just that the object is occluding parts of itself. So you can't just see the backside of that. Got I thought you meant like the monty is occluding the object. The object is, in some sense all sensors. So you, if you don't move 'em, they can only sense one spot. and then this movement to sense more spots. And some sensors are harder to move and take more time than others, but they're all basically the same, right? your finger can't touch the front and the back at the same time. It has to move and neither can your eyes. So it's not a strong distinction in my mind. Yeah. Yeah. And then, that distance, that separating space, of course, if, an agent is in some sort of hazardous, potentially hazardous environment or. it could be a very simple hazard, just if you're moving around an object, you risk knocking into it, you risk disturbing it, things like that. the distant agent has a much simpler kind of actuation, space, because it just moves as it chooses, and it doesn't really interfere with the, the environment, or put itself at risk. But of course, that also has its limitations, so You know, you're limited to essentially air propagating modalities of information and sensing, whether that's electromagnetic radiation or sound, and of course, if you're separated by that space, then you also can't, directly manipulate it.

and yeah, I'll just keep going through this stuff, but obviously ask any questions or stop me if anyone has any comments. with the, the surface agent, again, it's, intuitive to think of it as something like a finger, but, maybe a robotic finger where you could add things like, a camera to it, you still have the, essentially all the modalities that are available to the distant agent, and then you also unlock some, texture, which, require touch, Or you can imagine having more sensitive, ways of measuring depth and temperature and things like that, given the proximity. and of course by being next to the object you have the ability to, manipulate it. But in general that does lead to a much more kind of constrained, complex movement, kind of space and, you can't just arbitrarily move around, then you start colliding with objects and, so that needs to be a much more deliberate thought out. and even the kind of limb or whatever system is supporting, the surface agent tends to be more complex than, just like a ball and socket, kind of system for a, for something like an eye.

and then of course, as we're saying that, eventually these things should work together, as I mentioned earlier, you can imagine a distant agent getting a quick, overview of a scene, quickly parsing the information there, and then using the surface agent to both get more, detailed information as necessary, reduce things like occlusion, and then potentially effect changes, in the world.

I'll say it again, it seems to me that these agents are, from a practical point of view, you can think of them differently, but from a theoretical point of view, they're just points on a spectrum, Yeah. even like touch is interesting, because sometimes you, I've experimented with touching something with a, with holding a stick in your hand, and what can you do with that? And so that, that's a touch at a distance, and, if you manipulate something with a tool, like a screwdriver or pen or something like that's also touch at a distance. And there seems to be this spectrum of proximity and distance, difficulty moving, what you can sense, what you can't sense, but somehow they're all on a spectrum to me. It's, yeah. Yeah. It's also not the only two possible agents. these are just two possible action spaces you could have. And other people might have, a car, and the action space there is, acceleration, and, steering the wheel, so it's definitely not, these are the two only options. It's just And it seems, what we really, ultimately, be shooting for is an uber understanding of action policies that are okay, there's a spectrum of things here, cortical columns all work the same way. And, on this particular set of parameters, it's going to look like a surface agent or a distance agent. But somehow there's a common theory here because humans and other animals are extremely flexible about what sensors are and how they move them and in different ways of doing things. So it just seems like there's a, it's not really a hard, differentiation. Yeah. No, it's a good point. yeah, worth emphasizing.

yeah. And and then we also have kind of types of policies, that we've implemented and again, trying to come up with some classification. there's maybe three broad types. There's kind of utility policies we have. These can be thought of as experimental tools that they're not really what the model is doing in any kind of sense of intelligence or anything. It's more something we enforce on the model in terms of how it moves, for some kind of experimental purpose. And then there's input driven, just low level sensory information driven policies. You can think of these kind of like reflexes. And then there's learning module driven, policies, which are more kind of concept of kind of model based, reinforcement learning, where there's a model of the world using, used to drive action. I think the, the, latitudes seem to be like the big two, right? And, in particular For sure. the reason I'm just talking about this one is because This is the one I was kinda alluding to earlier when I said, I don't know if we've actually talked about it this much in meeting. I don't that meeting. I've nevers heard it before, but never heard of it. I think for that reason it potentially causes confusion and people are kinda like, what's this and why does this here and, things like that. So yeah, it's not something that exists in the brain. for sure. Okay. So the input one is interesting because. It's basically anything that's not cortical because there are subcortical structures that also initiate movement and reflexes is one form of them, right? the reflexes have a sensory input and then there's a, neural system that moves the body. so there's really there's things that the cortex controls and there's things that the cortex doesn't control and it finds out about. it's, oh, I guess I moved, and that could be like, hitting your knee and having to do a reflex reaction, but it could also be the cerebellum, deciding to do something, it's pretty close. Yeah, I was torn about this use of the word reflexes because Yeah, I don't mean it in like a strict neuroscience sense. I mean it more in the colloquial sense of I would call it, I would call it, it's basically learning model driven and non learning model driven because either the cortex initiates it or the cortex, someone else does for whatever reason, and the cortex finds out about it. And it has to go, okay, what do I do about this?

In that latter case, it's like just part of the environment almost.

to the cortex it is in some sense. Yeah. it's, and going off it, go ahead. Oh no, yeah, you go ahead. There's, a lot of research that shows that when a behavior occurs that the cortex didn't initiate, the cortex will still think it did it. It'll say, oh, I must have, you're, mentally, you're, don't think cortex, but you've, you as a human will say, oh, I must have meant to do this because, because I did it, and in reality the person had no conscious knowledge that they did anything. It was either posed upon the body or some subcortical thing did it. But they'll make up a story like, oh, I guess I really wanted to move my leg like that, but it's not true. So the cortex looks at something and says, this happened and maybe I did it. It's just inching observation, that's all. Yeah. And I guess in the like reinforcement learning terminology would be like model based versus model free policies where with the input driven ones, you don't really need a model of an object to decide how to act. and then also what we have right now, like one of the input driven policies, for example, is to follow principal curvature, which is basically we're just looking at the sensory input, how does the curvature look like? And then we just follow that.

That's, for example, something where the learning module might say, it might be aware that we're doing this policy right now. It might say, okay, I don't have a good hypothesis right now, so let's just follow the curvature. but it, we're not using the models inside the learning module. or it could be that's something that's done by the cerebellum or something, right? Yeah. The cerebellum, this is, But cerebellum says, oh, I'm on a surface, I move along the edge, yeah, so we don't need the cortical columns or learning modules for it, but it's not necessarily only reflexes, it might still be like some separate movement. that's what I was trying to get at. Yeah, I guess that's why I also tried to avoid the model free versus model based, because I was thinking, based on the models that we develop, That we have access to in the learning modules. I think we will probably use some model free type, elements, in there eventually, as in just learn shortcuts, you can imagine you could slowly think through like a learned graph, or you could have some heuristics that I've learned that kind of still leverage that, but combine the two.

But, that's it. Yeah, but it's hard to, I think, yeah, again, everything's on a bit on the spectrum, but, on this, not learning pedagogical driven motions, we have free reign to do whatever, however we want there. we're not constrained to biology, we're not constrained to different types of, whatever works is good enough for me, at least, because, these are biological hacks that the body has come up with to figure out, Reflexes and, even something, like I've mentioned many times, when you're walking, how you place your feet and how you balance it and all that stuff, that's not, none of that's the cortex. And, we, shouldn't try to burden the cortex with things that, that it really can't do, we're not very good at. So we can do anything we want for these input driven non learning model policies.

Cool. yeah, I guess it's just a bit more kind of clarity on that, but as we were just saying, input driven, this is where the decision is going to be based on, external information, essentially alone. Yeah, the, learning modules might still be inhibiting or disinhibiting kind of these policies in some sense, but the information that's necessary to make a decision about what to do is just based on internal, external information coming in. And these are generally quick, simple rules. whereas if it's learning module driven, then it's some level based on internal kind of learned models. and it's, generally going to be a more complex methodical kind of deliberation that's determining the action. Good question. Yeah. External relative to what are we talking about the cortex? Because it could be internal, machine status, which was internal to the machine, but, not external to the environment. Yeah. in general, I meant it here as like external to the, like meta agent, in the sense that it's, since a lot of these have been driven by subcortical structures and things like that. I think that's a question. I think you're saying external could mean subcortical structures, right? It's, yeah. Other parts of the brain can be doing this. It doesn't have to be, outside of the body. It doesn't have to be homeostatic mechanisms, for example, that are outside of the cortex.

That's like I talked about walking. Walking turns out to be a very complex process. It's not too terribly, maybe it is simple, I don't know, but that's, it's it's a, there's a motion generator, action policies for walking and balancing that aren't cortex, but they're not like, you don't walk because, your knee was poked, So I interpret externally cortex. Yeah. Yeah. Yeah.

Yeah. Okay. But it's your presentation. Yeah, I feel like it could be either. The reason I'm just emphasizing external to subcortical and cortex is just, yeah, if we're imagining that most of these policies are implemented in subcortical structures, then obviously the information they're receiving is external to them. But, but yeah, but it could be from other parts of the body.

and then, what the policy is actually aiming to do, it can be inference focused, and that's most of the policies we have at the moment. It can be focused on learning, exploration, and then not put here, but obviously it can be focused on, changing the state of the world. and yeah, most of the ones I'll talk about fall in this camp, and I'll emphasize if they happen to be more, the second one.

yeah, with that then I will get on to the utility policies, the first one.

so yeah, I guess the best way to think of this is it's a policy that's in some sense independent of the Monty system in terms of kind of the intelligence of the Monty system. It's there in our code base, in our kind of setup as a tool for the kind of scientist researcher. To, study Monty's capabilities and debug it and all this kind of stuff. And they're usually called in the pre episode function. So before any information is sent to the learning modules, it's just right up utility to get the environment into the state that we want it to be in. Yeah. No, I'm confused. And I thought the utility policies would include like. Just how to move your finger around the object or something like that. no, yeah, it's even more kind of meta than that or something. It's like, how do we, it's almost imagine you're doing an experiment with a mouse in a lab, the, scientist is first going to move the mouse into the correct cage before, the experiment starts. And, we can't just leave it to the mouse to jump around the cages and things like that. but the mouse is a physical entity in space. And so in the same sense, as soon as Monty is instantiated in the simulated world, it needs to move to a particular location, and that's something we want to enforce on it. But it's not really something that Monty is thinking about. it's funny, they call it a policy then. it's a Yeah, that's fair. yeah, maybe that's a remnant of the way it's implemented in the code, but yeah, in general, it would be nice to disentangle it more. because I take the word policy as this is the How our system is behaving and, not Oh, me putting the mouse in the, on the, on the, wheel. Yeah, I think we call it that because we use the agent's action space to move to the correct location to start it. Yeah. But, yeah, we shouldn't confuse it with the actual policies that the agent uses during an experiment. Yeah, and there's a way we could improve that, so for, yeah, and we can talk about this later. it's getting into the weeds, but yeah, at the moment we're using, we're getting the agent to shuffle forward, for example, until it satisfies some criteria, but we could, Very much use a more external action space where we just set the position of the agent and things like that. All right. Just be careful of the word policy. Yeah. And use it when most people think about it.

Okay.

so yeah, as well as Viviane was pointing out, that generally these are called in the pre episode, before information is sent to the system. They generally make use of the kind of viewfinder. which is a, I'll show on the next slide, but, that's sensory information that's typically, or that isn't sent to the learning modules. and they also make, use of this semantic sensor, which is a kind of feature in Habitat that basically tells you the ground truth of what you're looking at, you're looking at X object as opposed to a blank wall or the vacuum of space or something else. so this is privileged information that we're generally not, or trying to avoid giving to the Monty system.

and there's two main ones, GetGoodView, which is used by the distant agent, and TouchObject, which is used by the surface agent.

yeah, this will be familiar to most, but just as a reminder, the viewfinder is this, if you have something like the distant agent, it's this zoomed out view of the scene, and what's visible. And then there's the kind of zoomed in, small kind of straw world view of, of the objects and sensory information coming in. This is what's actually sent to, learning modules and what's used to build up models of the world. This is here, again, as really a tool for the experimenter. who's working in Monty, if you either want to debug things or just understand the performance of the system. but we can also use this viewfinder, as I said, for these kind of utility functions.

and so getGoodView, which is used by the distant agent, essentially just tries to move forward, while facing the target object, until you have a majority of pixels in this viewfinder, on the object. as opposed to empty space. but then that's conditioned on not getting too close to the object. and yeah, for this, it makes use of the viewfinder and the semantic sensor and the kind of end effect of that. So you can see this is an example of where at the start, or sorry, this is maybe a better example. These two were at the start of a, episode. We may be quite far from an object, but by moving forward, until you have this good view where a significant portion of the input is, filled by the object, then we ensure a good starting point where we can make kind of small saccades over the object to explore its surface. and this is simpler to implement than trying to predetermine exactly where the agent can be because then we risk, for example, starting inside, the object and things like that.

and then the touch object has a kind of similar core purpose. ultimately it's about getting the agent to be sensing the, the kind of object of interest at the start of the episode. but because it's the surface agent, it's ultimately about getting it onto the surface, or in particular, this, it's a depth camera within kind of a short desired distance. And so this kind of image in profile is showing a mug where we have this. It's, representing the distant agent, and it begins the episode here, moves forward until it's touching the object, and then from that point forward, touch object has been satisfied, it then hands over to the kind of, the more internal input driven policies to then follow the surface and things like that. Niels, what does the blue line and the red dot represent? It's just meant to give, a sense of orientation. So you can imagine, maybe this is the base of the finger and this is the, the finger itself.

I think so. What's odd about it is it seems like you'd think the red dot would be where it's touching the object and the blue would be just, showing its orientation. That's fair. Yeah, I guess I thought of it more as like a joint, but I could see that. Oh, I see. I see. You think of it more as like the there's a red spot, that's the spot, you think the spot would be hitting something versus, Yeah, yeah, does that make sense, Will? Yeah. Anyway, yeah.

but yeah, this touch object is actually also used if the agent has fallen, off the object, which does sometimes happen due to, I'll get more on the kind of touch agent or the surface agent, later, but, it can be a way of reestablishing contact with the object, if that's essentially lost.

And overall, there's kind of some subtle differences, which I won't go into all of them, but, only GetGoodView actually makes use of the semantic sensor, but both use the viewfinder, TouchObject can actually search for an object, even if it's not visible in the viewfinder, it will look around everywhere in space, basically until it sees something, and then move towards that object, whereas GetGoodView basically assumes the experiment has been set up so that the target object is somewhere within the viewfinder field, and then it orients, or is it and then, as the name implies, in practice, the touch object tries to get closer, although that is, defined by a parameter, whereas the distant agent tries to maintain, a minimum distance away from the object.

but I would say that the distinction between these two, or the fact that they are implemented so separately, is probably a historical quirk of how these were Or the time at which these were implemented in code and things like that. I think a very simple improvement that we could do and something that would make the code much easier to understand is to really consolidate these into, yeah, a more kind of unified, set of utility functions. and one thing we could do as part of that change is, Scott shared some, nice findings from, a policy he's I've been experimenting with that as an alternative to the get good view, to essentially fill the input image with the desired object, but also not clip it off, at the edge, which currently sometimes happens.

and then in general, to this whole discussion about kind of utility functions, utility policies, I think it'd be great if we actually separate out what is really utility from, something a model can use, because as I mentioned, the. touch agent currently uses the, model to or the touch object to get back on the object if it loses it. That's really something that starts, that happens after the experiment has started. and similarly, for the learning module driven policies, which I'll touch on next week, the agent is generally teleported to a location and then refinds the object that's currently using these, but we obviously want to limit the use of things like the semantic sensor, once an experiment has, has started. Just one note on the semantic sensor, as far as I remember, the get good view policy also has an option to do it without the semantic sensor, where we basically, estimate the semantic information from the depth information. Yeah, no, that's a good point. and then, yeah, in which case the only place we'd have to use it is in the, what you call it, like the multi object setup. But I think in general, it's very fair to use it during. Like the pre episode, there's nothing at all wrong with that, but then, yeah, to your point, we can just make sure that those, that we separate that out and that we're only ever using the depth estimate, when we do the, once the experiment has started.

yeah, were there any other questions? I remember, Tristan, yeah, you mentioned that, you wanted to clarify some things about these policies. I don't know if that helped or Yeah, this is helpful. Thank you. Distinctions are useful for everybody to know when they look at the code. Yeah. Yeah. And I wrote up some notes, that I can also make sure we document somewhere, because yeah, I don't think there was really anything written up to clarify these differences. Might be good to put those in the, the glossary as well.

and then, yeah, so that's the kind of utility functions, utility policies. so now I'll talk about the, input driven ones. and. What I was going to say, yeah, these kind of mentioned are defined by using some sensory information to, that's coming in to, make a decision, but no kind of learned models of the world and things like that. and so we have the base distant policy. The spiral scan actually doesn't use any input, I think. Does it not do moving back? I think, isn't that a separate policy, the move back? maybe I'm forgetting. Yeah, I was actually, I was wondering about this. Definitely not a model based policy, so yeah, we can definitely put it into this bucket. Yeah, I think, it might be worthwhile revisiting all the language here, because even the learning module policies are input driven. In some sense, it, has to know where it is. It has to know how the object things are moving. I don't know. It feels we used to call them the bottom up and top down policy. And I think like that terminology, I still want the model based and model free terminology because, that kind of emphasizes that the, learning module based on the learned models, I like to, I would think of it as cortical and non cortical or cortical and subcortical or something like that. In the brain, it's, it, all policies are driven by neurons, even the reflexes too. So I would think, oh, it's a cortex or it's non cortex, but, that's, neuroscience. But I think just thinking about the language, because we're going to be living with this language for years, everyone's going to be, everyone's going to have to go through this stuff. model free and model based is definitely, more familiar. Maybe. Yeah, As long as we acknowledge that, yeah, model free can also exist within the cortex, but I guess there's nothing about saying model free here. I'm not sure. Is that true? I don't know. It could model free. I would say so. yeah. I'm. What would be a policy? There's evidence that, Give me an example. just things like, riding a bike and things like that. a lot of that. That's not cortex. Exists within the cortex. Doesn't it? No, I think it's mostly, basal ganglia and spinal cord and things like that. Yeah. Yeah.

this is why, learning how to ride a bike, someone can't tell you how to do it. it's, it's you don't, even know, most people don't even have any idea how they balance a bike. it's intuitively weird. So I would argue that's an example of definitely non cortical. It's, it's some basal ganglia or cerebellum or something like that.

I would, I'll push back on that. You might be right, Niels, I don't know. Yeah. But I would challenge that. The way I view the cortex is the cortex doesn't really have any non model based policies. It's, yeah, I think all that, other stuff, walking, riding bikes, balancing, reflexes, these are things that the body does and the other parts of the brain do. And yeah, I'm, not sure why I'm remembering. Yeah, that, but I think, yeah, it's, if I'm thinking of anything, it's just like supportive functions or something like that. So it's. Yeah, it's probably not really relevant. It's a clean break. If we could think about the cortical, the learning models as pure model based systems and they don't really have any other stuff to do with the world. Because all this other stuff is very, riding a bike, it's this is the strangest thing, to turn left you first have to turn right. And, people don't even know this. And, it's it's just stuff that happens outside and the cortex doesn't really understand it. Anyway, I would really think we should try to keep, we've got the learning modules. Which are really pure model based, and they don't, and then there's a lot of other stuff, which is, I think you do a good job here, pointing out there's utility things we have to do, and then there's, reflex things, and other things that happen, but I would think all those are non portable. that's how I would think about it. Yeah, I think I like us making that distinction, because that will make it easier also later as a rule of thumb, if you have a policy that doesn't require a model, try moving it outside of the learning module. and yeah, and almost all those policies will be either modality specific or environment specific or, not general purpose, right?

I don't, I don't, I'm not super familiar with the words model free and model based. I know if, there's a problem with that, I, they sound good to me, but, I'm not advocating for those, but that, that basically does capture what I'm trying to say. Yeah, I would say they are pretty good. And then. Yeah, I would only change it if we have a very concrete example where we would need a model free policy inside of a learning module or a cortical column, right? in the brain, the one potential exception I can think of, and I mentioned this many times, is the extra layers in V1. we don't know what they're doing.

and, I've always speculated that they're, trying to deal with this, determining depth, from, it's from two eyes that, that are distance and, but, that's the only place in the cortex where I can think of, oh yeah, there's a weird, some weird extra machinery here for vision. everywhere else it looks the same everywhere. So it's it seems that there wouldn't be any non model policies in the cortex. Yeah. just a line, wouldn't model conflict with like deep learning models, like the actual weights and of something?

It's like what, in terms of language, in terms of confusing people? Yeah. Yeah. Like maybe calling it a learning module policy.

But we use the word models in Monty, learning models, learn models, that's inherent to our Yeah, in some ways, I think, yeah, model based is, it's good in that, because yeah, that's an established term, and when people use it in reinforcement learning and things like that, they do generally mean a similar thing to what we mean, which is, you have some explicit model of the world, for example, you're simulating the results of GoMoves or something like that on a, on a kind of modeled board, it's not just like some implicit function. Yeah, I agree. I think the way that it's used in reinforcement learning is, pretty much what we mean here as well. Yeah. Every learning system has models. Everyone. And they're just different types of models. We have a sensory motor model with, locations and orientations where, deep learning doesn't have those attributes. It has different types of models.

I think, that's, how I've been using it generally. It would be correct to say model. We just have to make sure that people understand that our models are different.

It's not your model. Cool. I think this is only the third, maybe the fourth time we've changed these names for these, things, but yeah, I think every time it's probably gotten more accurate and better. So it's probably a good thing. Maybe it's finally clear.

nice. Oh yes. Thousands of people are going to be trying to learn this stuff. and, if we spend some time now clearing it up, it'll make it look a lot, it'll the benefit to humanity will be great, so it's worth going through it. Probably tens of thousands of people will be trying to learn this stuff, Can I, like I was thinking about how to unify, the distinction that we have for surface and descent and also these different policies, like utility space, I'm just using the language from your slide, utility space, like reflexes and learning module, and this was just going off from the utility space where we use, viewfinder to try to orient ourselves towards the object, but, maybe, and I think we were leaning towards, maybe that's not. We shouldn't, call that a policy, but maybe, we can even apply it to, flush this idea, but, we have an agent at different states, so it's in, a searching mode, or, a navigating mode, or exploring mode, and I guess exploring would really depend on the kind of sensory or whatever kind of actuators you have, but, I, the, The current way that we try to do use viewfinder would be when the agent is at a searching mode and hence, it will try to plan out a path to get to the object, I don't know, or like maybe there's some policy, maybe this is indeed a policy, it's just a policy at, searching mode, like maybe we can think of like agents having finite states, like I don't know very much about finite state machines. Thanks very much. Yeah. But yeah. Yeah. No, I think that's a fair point. I think, yeah, I think searching is a good way of describing it. And, but, and then I guess the one thing is we could have searching policies that fall under kind of model free, where it's it's the agent that's just using the information coming into it. so basically like sensor patches. and it's using that to look for something I feel like. There's still a risk, or I think we should ultimately still separate those out, and just make sure that, if we are using this kind of privileged stuff, In general, we don't want the viewfinder kind of anywhere in, in Monty in the longterm, and also That's the main thing, that we don't want to use this privileged information during an experiment. we could say, oh, we have a sensory in the periphery that gets like a larger receptive field at low resolution. And then we might have that connected to a learning as well. But right now, So they are getting information that isn't given to them during the experiment and we don't want them to get during the experiment. So that's the distinction in my mind, but we do have these kinds of states, like we have the matching phase and then the exploration phase, for example, that's got, goes into the direction, Tristan, that you were interested in, what's the direction we want the code to change. And I think in the future. We do want it to be so that the agent can actively switch between different policies and pretty much learning module would say, Oh, I recognize the object. now I want to use my exploration policy and learn more about the object. And that could even be model based. It could say, Oh, my model doesn't know much about this part of the object. So let's move over here. Or it could be model free. Oh, I just want to scan over the whole object, with a spiral, whatever. There's a lot of literature about, Both in terms of biology and machine learning, but in biology too about, about, we do have these sort of overall modes of, hey, if you don't, you only want to explore when you're comfortable, you're in a safe environment, you're not at risk. And so there are these sort of emotional states we can go through between, yeah, okay, I'm willing to do explore, which is inherently a dangerous thing, right? If you're exploring, you might trip up and hit something bad.

so I think the idea you could have some sort of overall, the whole system is in an exploration phase now, the whole system is trying to, just infer, or the whole system is trying to, deal with something that's happening that's unexpected, I think that would be a reasonable thing to do, because it looks like biology does that.

Yeah.

Yeah, no, I think this kind of stuff, if you think about it, maybe in a deployed system, you might have deployed systems that do no exploration, right? They're just like, Hey, we've trained the system. It's now we want them all to work the same in some sense. And, don't go off and try to do something on your own.

If you've got multiple sensors and actuators, does the whole system have to be an explorer mode or Hungry mode. I'm just thinking as I, I'm a perpetual fiddler, so I'm sitting here fiddling and listening. Yeah, so I can say I'm exploring this object, and I don't know what listening is, that's interesting. I, I've asked myself this question about, there's these behaviors that I do, you call it fiddling, where people flip a pen in their hand, I clip, I will put the top of a pen off and on. And I wonder, is that, really cortex? Or is that, because you don't, often you're not even aware that you're doing it, you're not thinking about it, it's just off on the side happening, you're tapping your foot, you're You know, I don't know, even if you scratch an itch, that's usually not the cortex, right? So I think, I've always thrown the fiddling, scratch the itch, tap your foot off into some cortical thing. The cortex is not involved in that at all. that's where I've thought about it because the cortex can observe it and say, Oh, look, I'm tapping my foot or I'm scratching my back. But the, policy to implement the scratch is probably not going to work. I guess maybe in general you can have one subcortical and one cortical going at a time. Like you can be thinking about something while brushing your teeth or something, but it's hard to yeah. You can switch between them with like short windows, right? I would say my fiddling is, constructive. I build little things, so I think I'm giving it some attention. All right, maybe, yeah. Or here's another example. You're driving a car, right? And while you're driving a car, you are talking to someone. that talking to someone can be very exploratory. You can be trying to understand what they're talking about. You might be visualizing things, you might be probing them.

and yet driving the car, clearly, a lot of that is cortical, right? Almost, the vast majority of it is, a lot of it is. So there's, an example where you have one modality that's definitely cortically driven, model based, control, goal oriented, and then another part of the cortex is exploring. So there's, I guess you could have both.

Yeah, I was just thinking about that the other day, like, when you're driving a car, your eyes are saccading all the time, you're recognizing all these objects and where they are and how they're moving. You're also steering the car and still you can listen to a podcast and in your head have all these abstract concepts and think about all these, a completely different scene in your head. Yeah, unless something happens visually unexpected, or something happens in the picture of the car, then you immediately stop thinking about the podcast, and your intention is to completely focus on the car situation.

one way I've thought about this is that, we have these many levels of the hierarchy in the cortex, and things that are really practiced can be learned at lower levels. and therefore they don't require the upper levels of the cortex to pay attention to them. You don't have to, like, when you're driving your car, you don't really form episodic memories of, the car that was next to you one moment, and the next car that was next to you another moment, things like that. It all just gets washed away. So a lot can be happening on these, if you think about a big hierarchy, imagine that some part of the lower level of the hierarchy is driving the car, but the other part that's thinking about the podcast, it's going all the way up to the, hippocampus, and you're forming episodic memories about it. And. And that's where your sort of mental focus is, where the other stuff is just on autopilot until something bad happens or unexpected happens and then it flies right to the top of the stack and says, let's deal with this issue. so I'm just sharing personal observations that perhaps may have some value. Yeah. Yeah. And then just in terms of Monty to wrap it up, because Niels, I don't want to take over.

We have implemented so that you, there's always just one agent and it might have multiple sensors attached to it. but in the future, we want to be able to have multiple agents that move independently of each other, like multiple fingers that are moving or like a hand, two hands moving. and each agent could have its own policy. And each agent might change its policy at different points in time. So we want to have that kind of modular flexibility. We can start simple here, but ultimately we want to get to the point where we have agents who are like humans and being able to do things and think about other things at the same time. We'll get there. so it can be confusing because right at the moment we just have to, can this finger recognize this coffee cup, As I start thinking about podcasts at the same time.

yeah, just, talking through the kind of model free policies that we have, at the moment, the kind of simplest one is for the, distant policy, distant agent, sorry, where it's essentially just making small, saccades as like a random walk, and so this is entirely, internally driven in a, model free way. I guess the only kind of sensory information coming in that can then influence this is if the, system goes off the object. It essentially just reverses the previous section.

And moving on, the object is determined from the depth image. So we have some heuristics of a sudden change in the distribution. In depth, Images. And this is just showing again but in a different view where you see the mesh and then you see the kind of essentially points that are centered, or, that are sensed rather on the, object, and, this just shows up. Almost looks like a random, almost looks like a random walk, is it? It is a random walk. Is that, is there a reason for that, or is it, we just want to make sure we cover it? why wouldn't it just follow the line for a while? Yeah, so this is the simplest policy. Okay. It builds up from here. Okay. But yeah, but what I'm trying to show here is that this is very inefficient. Okay. Okay, yes, that's what it looks like. I agree. and you can add kind of some momentum to this, random walk to make it go, more in one direction for a while. but it's still a random walk. and then there's the spiral scans policy. This is the one that's essentially learning only really, or like it's entirely designed for learning. And this was a way to ensure that we sampled, points on an object densely. Okay. And so you can imagine it literally does this kind of square spiral, going outwards, across, the surface of objects, and this is again the distant agent that does this, and then the base surface policy is one that, yeah, touches the object and then moves, along it, while always Remaining oriented to the point normal. So I'll show what that means in a moment, that both here in terms of how the kind of finger follows the curvature of the object, plus if you watch this video, as it gets to the lip of the object, it curves around and then follows the top.

That's really nice. Did you make those videos new for this presentation? I don't think I've ever seen them. This is an old one, but yeah, I haven't shown it in a long time. It was way back when I was debugging some issue with the surface policy.

but, yeah, so just trying to understand this a bit, better. So imagine you have this surface of the object in blue, zoomed in, and then you have this, finger touching it. And we start out oriented with this point normal. it's, it's a recursive, or iterative algorithm, but just for the purpose of this kind of, we start there, and then to follow the surface, basically take a kind of tangential step perpendicular to this point normal, then move forward until we're touching the object and then orient to be parallel, with the point normal. And then, repeat. But of course, I've shown these steps very large in order to make it easy to visualize. you can imagine if you make these steps quite small, then that's essentially what your finger is doing as it follows the surface. So it's not actually, it's because I was saying, I wouldn't actually remove my finger from the surface. But I think I would sense that if the pressure is reduced, I have to move in a certain direction to keep a constant pressure, which is essentially the integration of what you're showing here. Exactly. Yeah. Yeah. So as the kind of distance becomes extremely small, then it And also, only every fourth observation is sent to the learning module, so only the ones after the, reorientation, right? yeah, so Again, with the whole kind of like subcortical, it's only some, yeah, there's a few filters, before it gets to the cortex, but one of them is just that, yeah, a lot of this is just the kind of finger orienting itself, and that's not information that needs to be said. this idea that the finger is following the surface and automatically adjusting the it's depth to stay on the surface. I can imagine that's all subcortical. the cortex could say right in this direction. but what actually happens in the actual movements and, would be reported back to the cortex as opposed to cortex saying, Oh, I'm following a curve. It would be more like, Oh, I've moved. I moved in this direction and it looks like it's curved as opposed to Oh, It knows what's going on this kind of, micro adjustment that's happening here is almost certainly not Cortex. Yeah. And in, in our system, it's not, it is a model free, sub, sub LM, system. Yup. Great. The surface point normal is estimated by the sensor? Yes. Yeah. The sensor module.

the sensor module gets the depth dimension and estimates the point normals. Okay. Great. Thank You back to your question earlier, Viviane, about we only send every fourth point or something like that. That's not today's talk, but that is a major topic for us is in the cortex it feels like, I've made this argument many times, the cortex can learn all those points and it doesn't really take any more memory. whereas where we're doing it now with discrete points, it does take more memory. so that's a sort of an elephant in the room I'll have to deal with some time. We have another thing, which I'm not talking about because it's not really policy, but it's like the feature change sensor module, and that's more of a buffer or whatever that It determines how far you have to go before. It's trying to be efficient. Yeah, and that's both in, physical space and in perceptual space. Yeah, I think, but I think cortex does it miraculously, cortex, you could train on different points and it's, anyway, we talked about this before, it may, it's a, it's potentially a looming problem, I don't know, what we'll do, maybe you've already dealt with it, sorry, keep quiet. No, sorry, but as it, excuse me, stands, this will follow the surface of the object, but it's still a random walk. in terms of the, kind of directions that we are moving tangentially, either it's an entirely random walk, or you add some momentum factor, and then you get these kinds of curves. but at this point, there's nothing to follow the significant curves of an object itself, which is what this follow up policy, that implemented, which is called the curvature informed, surface policy. And so this was designed to follow the, minimal and maximal, principal curvatures, on objects, which is what you see here in yellow, where it's first, following the maximal, then the minimal, then still the minimal curvature, or you can imagine the same if it was moving along the handle. So this is This was a, trying to get a policy that would intuitively capture the idea that just instinctively our fingers will tend to, move over these kinds of curves, when sensing an object, rather than just moving randomly all over the surface.

and just as a reminder, so the principal curvature, At any point on kind of a 2D surface, these are the orthogonal points of maximal and minimal curvature, either they're flat or negative, and so this provides us with two directions that we could go, one in which kind of there'll be a minimum amount of change and one in which there'll be a kind of maximum amount of change.

and in effect, this can be You know, it also just occurred to me, by the way, it just occurred to me, this could be related to how we learned the orientation of the object, right? In some sense, those two things, maximum curvature and maximum flatness, could almost always align with how we Overall, for an object, how we orient it, I don't learn the cup at some, 30 degree angle, I tend to learn it where the vertical surfaces are flat and going straight up and down, and the curvy ones are, I just think it's, somehow it's related to, also to the orientation. Yeah, no, for sure. I think, yeah, and in general, I think it's, Yeah, like the kind of, it's almost like the principal axes of the object. That's the way to put it. I think Biedermann had these theories around how we represent those, but, yeah, that's how you can derive them from sensory information.

but yeah, this is just showing, what this can generally give you, which is, you follow the kind of side of a cup and then go around the rim when you reach that. And then this is just showing against the finger as a kind of stick. Moving along it, and as you see, we're still orienting to, to be, perpendicular to the point normal, which, is more noisy or more kind of, rapidly changing in somewhere like the rim of a cup, so that's why they're it's going around like that, but.

and then just going into a bit more detail in general, this policy tries to alternate, following the minimum curvature and following the maximum curvature, and then sometimes just not following either, because if we just keep on one forever, we risk just doing kind of a loop on the object, And then, similarly, if, which go off and these are undefined, this is something where I'd argue that this is a hack, because at this point, it's, this was like fully, model free, fully, input driven, whatever, and so this was a way, preventing us from just, repeating ourselves endlessly, I think, the natural way to resolve this is to have still this kind of same principle, but then there would be more of a kind of model based, knowledge of okay, I'm returning to the point that I was at before, let me explore in some other direction, and that would be more of a top down signal. That sounds right.

but yeah, this was to get it working in practice, and yeah, so this is just showing, for example, there's a lot of places where to the human eye, not here, because this is perfectly flat, so that's why there's no kind of defined, principal curvature directions. But there's a lot of points like here where it's the human eye, you'd think, okay, it looks like the principal curvature direction should be defined, there's the minimal curvature around the edge, the side of the bowl, and then the maximum curvature here. But, but that due to noise in the kind of point clouds and how it's estimated and things like that, often it's, not actually present, to our sensors. But still you see here it, it takes a kind of random path with some momentum, gets to this kind of small edge on the bowl. It follows that for a while. Then it goes in this direction for a while.

Yeah, and just in terms of future Monty work or plans, this is a great example of where we would ideally want these to be three separate policies and then have the learning module be able to switch. Like switch between them. And right now we just have hard coded random switches between them or triggers. I was thinking, it's a good point. I was thinking like, oh, I would probably. Follow the white, curvature all the way around, and, mentally is what I would probably do, and then I said, now I'm back to where I was, let's go in a different direction, which would be a, changing direction based on the model I've just learned, Right. Yeah. Or may already even know. Yeah.

if I haven't, assuming I'm learning this thing, if I know, if I have previously learned this and I'm just doing inference, I might follow the curvature until I have a hypothesis that I could test, oh, this could be the And, and now the best way is to head down the side to find out, but during learning I don't, I might want to just follow the, just imagine the coffee cup, I might want to follow the entire rim of the coffee cup, but I've never learned this object before, and, and then, okay, back, to where I am, now let's go another direction. All right, but anyway, looks, what you've done is great. I also was thinking about like developing some kind of internal model of the, general bounding space of an object. if I have some kind of, I might try, if I have an object like this. to, imagine that there's like a sphere enclosing it, and I might want to try to go around the object in such a way to capture all the angles around it. And then in this way, I can develop some sense of the overall boundary of the object. But what, that's what we're doing, isn't it? that's what the model is, mostly, isn't it, Scott? it's, a, but it's defining the morphology of the object, right? And we're, that's the goal here in some sense. I'm suggesting like not actually following the curvature, but following, if I had a sphere that was projecting a line that just did this, like a sphere with like lump, I don't know how to put this. Basically I'm trying to find the exterior, boundaries of the object. But so again, I guess this would do that in two sweeps. So we would, with most objects at least, A lot of, at least the manmade objects have yeah, two, two directions. So like the one you were just holding that tin, I just think if you found yourself on the edge, then you would go around that way. that would be minimal curvature on that side. And then you'd be like, okay, I'm back where I was and now the model base could say, okay, switch calling maximum curvature, and then just keep going. And then you'd eventually do another circumference. That's what you mean. Yeah, I'm trying to picture why I might need, may or may not need curvatures to do that. I imagine that I could do this without following curvature.

you can just do like a random, like with momentum kind of thing. You could do it, you could sample it anywhere you want, and you're going to end up learning the same morphological model either way. I guess we're just trying to do it efficiently, is that? Yeah, I think this was inspired by just, our own observation that when we touch an object and try to infer it, we usually follow like an edge or the rim of the cup, and those are usually the tasks of the minimum or maximum curvature. And we don't want to do the, we want to do this intelligently because the random walk is very inefficient. Yeah, and I think in general it is. More efficiently, imagine you're suddenly on the handle of the mug, if you're going to move in a random direction, that could be any direction, you could end up doing this like weird spiral around the handle and just go round and round. It makes much more sense if you just follow the minimal curvature and then you just, you immediately get the general shape of the handle. So I think in general, it is like a, kind of to the whole point about primary axes. Like with a lot of objects, if it's like a, some sort of ellipsoid, or even a cylinder, like in general, following the minimal curvature will quickly get you from one pole to the next, and so give you the overall shape of the object.

so yeah, I think, I don't think Scott was suggesting a random policy, but from how I understood it, it sounds a bit like it would be, it would maybe be like a spiral scan policy for the touch agent that kind of scans around the object and in a sphere, And, that could definitely be, like, an efficient way to cover the entire object surface. It just doesn't seem very natural, what we would be doing. Oh, I see what you're saying. It's like you're walking into a 3D scanner. Yeah. It's just, it's, got, it's moving this, scanning element around this sphere and measuring everything. Yeah. Yeah. I think, yeah, again, we're, yeah. Ultimately, the, we can have all kinds of weird sensors that do things like that would all fit within Monty, but if we're trying to build embodied agents that actually move in the world, they're going to be constrained by physical movement issues. so from some limitations of sensorimotor learning, we can do that sort of 3D scanner, but other ones would have to do this sort of action policy where you're moving sensors along surfaces.

Yeah, I was trying to imagine, if I pick up a brand new object, one of the things I want to find out first is what the spatial extents are of this object. And once I have some idea of the overall spatial extents of it, Then I have more information, more ideas about where I ought to explore next to fill in the particulars. let's, break that down in a couple parts. The most extreme, which is what we're doing here, you didn't pick up the object, you're touching it with one finger. And that means you have to move everywhere to, to learn what this object is and you can't. if I pick up an object and I've got, I've got hundreds of sensor patches on my hand that are touching the object simultaneously and, they know their relative, we don't know how to do this yet, but we know that the brain does this, they have, they know their relative positions to each other and so even a single grasp of a hand tells you a huge amount about the overall extent of the object. Yeah, basically this is if we would have five learning modules that would touch. All around the coffee mug, after just one sensation, they would vote with each other and immediately narrow down the hypotheses to the objects that have that size and shape. And you would immediately only have those few hypotheses, and you'd have the complete models of them. So then you could use those to inform the next movements, like it doesn't have to be a separate process to estimate a bounding box or anything, it would just automatically come out of the Like possible having these given imagine, imagine you grabbed this cup or any object with both hands. You now you almost, you have a really good idea of the boundary of the objects at that point. yeah, there could be something sticking out between two of your fingers that you didn't touch. But, the, interesting challenge here is that we have to develop Monty with this single learning module focus, which is very primitive and seems wasteful. And it's we have to come up with all these things that Niels is talking about. Ultimately, that's not the ultimate system. The ultimate system is going to have all these different learning modules working together. but we have to start here because this is the foundation.

so I'm just, reminding us, this is not where we want to be. This is, where we're starting.

just, like when you look at something with your retina, you're sensing different parts of the object instantly, different parts of the retina are detecting the edges of that object and you immediately have a sense of its boundaries. With one glance. but if you're looking at it through a straw, then you have to move all of it, just like here. Same thing. Yeah, and I guess just one other thing to add, I feel one other kind of potentially useful thing about this kind of heuristic is just in terms of sub objects or sub features. I think in general you're more likely to efficiently find those, again, something that people will often kind of sense that might give them You know, tell them whether they're sensing a mug or a can or something else like that is like the rim of an object. And again, if you're moving along and then you encounter the rim in this case, you're going to then follow it for a period of time. and get some sense of it. If you're just doing like a, yeah, I don't know, just like a general sweep across the object, then, yeah, it's just you bump into each feature, but you don't really get a chance to explore each one as you go, on the way. Yeah, and I think what you're bumping up against here is that this is also a single learning module in, in the sense that there's no hierarchy here, therefore there's really no, there's no object composition at this point. Yeah. It's just a bunch of features at locations. Whereas in the coffee shop it's always been a difficult question like, oh, what are the components, is it just a single level, object or is there a compositional component? Clearly, at some point in our lives, handles and rims become compositional components, so do vessels and, cup cylinders and things like that. But we're not even dealing with that right now. We're just dealing with the There's some morphological shape that has no subcomponents yet, and, we're just trying to learn its morphology, and what features exist at different points. Because I went down a line of thinking where if a cup wasn't a single object, but compositional, then you've got multiple surfaces, you could have a model for each surface or distinct cluster of things, like a rim would be a model on its own. Yeah. It's not clear. It's right. Then could you, drive your exploration policies based on predictions on those sub modules? if I, see a piece of a rim, then I could instantly predict what I'm going to see around it. let's jump to something which is really clear. Let's say I draw a picture of a face and you've got two eyes, a nose, and a mouth. each of those is a clearly identifiable sub object. And so now I'm saying, oh, if I see an eye and a nose, I should predict where the other eye should be, where the mouth should be. And I can go check for it. those are really clear. The coffee cup is a very unclear, what are its sub components, and, it's just always been a problematic one for us to think about, so the moment we're avoiding it in Niels presentation right here. But ultimately, many, things in the world are almost, the vast majority of what we do with the world when we're learning new things is we are recognizing objects we already know and doing compositional structure. That's 99 percent of what we do, day to day, moment to moment. this is more like a child just starting to touch the world and learn shapes and things. so we're avoiding that, right? We're avoiding that issue with the coffee cup right now. It's too hard to think about.

Is it too big a thing to avoid? No, it seems pretty important. Sorry, maybe I'm going down a rabbit hole. no, we're not avoiding it. In fact, the paper that I'm supposedly writing with Niels and Viviane is all about compositional structure and how it works, and I think it's extremely important. but on the other hand, a single learning module or or a single, a bunch of learning modules that are all at the same level, like in a single region, cannot learn compositional structure, and we have to do both, right? We have to, so I, think we're, not avoiding it in terms of oh, we're going to hide it. It's more okay, let's get started. Let's get these policies on a single module, a single learning module, down, because we have to understand that. and then we can also talk about how we do, the face and other kind of, truly compositional structures. And I, in my opinion, what will happen is we'll figure out, Ultimately, that'll lead us to, what do we do in the in between spaces, is a rim a separate object, or is a handle a separate object? Yeah, and unfortunately, or hopefully once we get time to it, Viviane and I had some discussions at the last By the Bay for, like, how at least in the current implementation of Monty, we could start, adding in learning of kind of sub objects like this. that should probably work pretty well.

I can't remember, yeah, if we ever, discussed that as like a broader group, but yeah. My intuition would be if you started at the lower level, like of cylinders and rims, then your problem is simpler, like predicting a flat surface and having a policy that can explore a flat surface, or is it easier to start from the bottom up would be my intuition, I think almost the opposite. I think what we're doing right here is really hard and, that's where I'm having a lot of data problems. I think it maybe depends on what you guys are meaning by simpler, as in I think, yeah, I think it's true that it's, more efficient if you have hierarchy like in the same way that kind of voting makes inference more efficient, but I guess Jeff's point is he just wants to get it working first, like independent of because first we have to learn the subcomponents and then put those together and stuff. So it's I guess getting the system, before there's any sub components working. if it's a handles a handle system, like the communication and the Yeah, or if the handle that's the lowest object, then it's a handle. But like how you get that system that understands the handle working, in that case, if that makes sense. I just thinking like mugs don't form other objects easily, but if you started with planes and circles, it's, a simpler, like they form other. Simple objects. I don't know, I could argue that mugs do form, we'd look at the, the table setting, you got a plate and a mug and a fork and a knife, whatever, that's a compositional structure where a mug is one of the components. I think in my goals, it's a fair criticism to say, it's easy, it's, once we can easily say how we're going to learn the morphology of an object that's not compositional, that's what we're doing right here, then you can say, oh, it's not, it's easy to understand how we learn the composition of an object where the components are all pre learned objects, and the hard part is the in between ground, like this mug is a handle a separate object or not, and what, it's clear to me that when you start out in life, you don't know that handles are separate objects, and if a child is first time ever presented with. A mug with a handle, it wouldn't say, oh, the handle is a separate object, it would learn it all in one thing, and then later on says, oh, this handles, I have similar handles on other objects, and therefore I'm going to somehow, separate out handles. We don't really understand that yet, but there's this interplay between the levels and the hierarchy. I'll let that happen.

I'll just reiterate, this is the approach we've taken to break it into two parts. start with a single learning module, then add multiple learning modules at the same level, and then do compositional structure at different levels. Okay. We're definitely planning to look into hierarchy and make modeling compositional objects in the next months and definitely a lot more. that seems, yeah, that's much more exciting. But I think if you jump to that now, you're just going to get it wrong, that's my, my, my opinion. We have to understand the basics, that's my opinion. yeah, I guess one thing to add, because kind of part of the intent of this presentation is to inform people who are using these policies, I just wanted to make sure I covered this point, even though this, I'm hoping to change in the future, but, With the, this curvature informed policy, it also has this kind of evasive maneuver, step where it keeps a trace roughly of where in space it's been recently. And if it feels like it's going back there, it basically instead takes a step that's going to bring it away from it. This is clearly the kind of thing that would be much better, implemented as like a model, based policy, where it's the learning module that understands, okay, I'm, starting to return to where I was before. I want to guide myself to go away from it rather than the kind of subcortical structure keeping track of all that information. But again, this was a kind of hack to get this working well, in the kind of current setup.

But, but anyways, that's shown in here in green. So you can see here, the system goes down here, it's following along here, and then at this point, it starts pulling the maximal curvature. in black, it's hard to see, but going up, but then it's going to just return to somewhere it's already been, and it, it can tell that from, a quick kind of check. And then, so it turns around.

And you could imagine in other applications, the opposite policy might be useful, if we have very noisy path integration, we might actually want to revisit the same feature again to make sure we adjust our path integration. That's a great observation, yeah. Yeah. Because that's the only way you can really, ground yourself in the right location is to go back someplace you've been or observe something you've seen already. Yeah.

It's also like any heuristic, there's going to be some object that it's going to fail horribly on some, like you can design a menacing object that will trap you in some pattern of your heuristic. I think the learning part is, needed. Like the model based part gets you away from that. Yeah, no, for sure. we say model based. There's a whole other topic we haven't really delved into too much, which is, the sort of the idea that the hippocampal complex is learning models, but it's a, it's more of an episodic memory. So the, so if I were literally, touching something for the first time and I was exploring it and I was, thinking about it and concentrating about it, okay, we're moving over here. I'm imagining the hippocampal complex is keeping a trace of where you've been, and it's just if you're wandering on an environment, it's going to keep a trace of where you've been, whereas the models that formed in V1 or S1 or something like that don't do that, so there's some complexities here. Where you could say episodic memory is a temporary model. It's like saying, okay, I'm going to remember the actual path I just took, but it's not really part of the model that I want to ultimately learn. So it's the model I learned in the episodic memory of the hippocampus, which is a trace based model, and exactly how did I get here, where did I go, how did I turn. The model we wanna learn in the, V one or S one, or it's not like that at all. We don't wanna have any memory of exactly how we, learned the object or how we traced over it. So that's another complexity that is certainly going on in the brain. and we could think of this. Evasive, avoiding revisit. You could just throw that into oh, okay, there's a part of the cortex or part of the brain that's doing this. I guess I was thinking of it more as like short term memory.

But that's a lot of like prefrontal cortex. it could be prefrontal cortex. Okay, fine. Whether it's prefrontal cortex or hippocampal, my point is it's not V1, right? It's not, but it might be hippocampus or prefrontal cortex or whatever, stabilizing some representations in V1. Maybe. Maybe. I could be. You're right. I don't know. I viewed it more like episodic memories. Yeah. Literally, if I was, for the first time, touching an object I've never touched before and trying to learn what it is, I would have the ability to recall mentally recall, oh, I was already over here and I need to go someplace else, I'd have that, and therefore I had some sort of short term memory trace of where I've been, and whether that's prefrontal cortex or, enthorhinal cortex, I don't know, but I don't think that's what's going on in, in the module. I, I don't think there's any evidence that there's a short term memory in these things. Maybe there is, I don't know. I think it's, again, this is in the gray area of okay, I think it's totally fine to have a policy like this. just don't think that this is what all learning modules do. Yeah. It may be, implemented in a different type of learning module or a different part of the brain or something like that.

and then, yeah, so just in terms of how to improve this, kind of some simple stuff, as mentioned, I prefer to have this more as the, And, model based, because, at the moment I'm storing locations and things like that in the code and everything associated with the, what's essentially subcortical motor systems.

and then, yeah, as we were talking about before, model based switching of the curvature guided policy. At the moment, if you look at the algorithm for this policy, it's really complicated, and I don't think it needs to be. It's just This was how we tried to hack it together to be, to avoid a lot of the adversarial, conditions. But to your point, Michael, there's always going to be adversarial conditions that aren't satisfied. And having more principled, model based, decision making would, help with most of that.

and then, yeah, I guess just a general kind of input driven improvement that we've talked about for a while that we want to implement is, saccading to salient features. where, and this gets a little bit towards the kind of search policy as well that you were talking about, Hojae, and I think, Scott, you were asking about this the other day, but basically the distant agent, it makes a lot of sense that we would have learning modules that perceive kind of the periphery with some low resolution, and then, based on some condition of kind of salient features, We can then orient towards something that's interesting. and, yeah, so this has been on the kind of wish list to implement for a while. it'd be nice to, do. Yeah, and that kind of points out again, these nice curvature following policies are all for the A surface agent. So the distant agent is pretty random walk. because with our eyes, we usually saccade from point to point. We don't really do smooth pursuit and follow a curvature. So yeah, I think it would be very nice to find some efficient policies for the distant agent as well. Yeah. Yeah. At the scale they're at, the moment, the random walk is almost like micro saccades, just Your eye, drifting across an object, so not very efficient. Yeah, it's interesting, if you were trying to learn something really foreign, so imagine I don't read, Chinese characters, right? So when I see a Chinese character, printed character, they all look similar to me. they don't jump out like, like they would to a native reader. And, so if I wanted to try to recognize one of those characters, I would, use very small movements of my eye. I would look at, okay, there's a little, bar going this way, and there's a little bar going this way. I would, in some sense, I would go down to these, I'm not sure I would follow, I wouldn't follow the curvature, but I would jump down to recognizing individual, lines of that. Say this character has 20 different strokes in it, or 10 strokes in it, I would then attend to each of those strokes separately, because that's the object I can recognize, a little line segment, that's the only thing I know, and so I guess I'm saying is, as you get with things that are less and less familiar, You end up getting closer and closer to following, even in vision, you get closer and closer to following the edges. You're not, but you're jumping to the lowest thing you can recognize. But in that case, it would be just a line segment at some orientation and location relative to the character. so only when I recognize the entire, learn to recognize the entire character, with multiple, multiple, points of my retina. then I can just, jump around and saccade from character to character or from, whatever. Yeah. I think with vision, it's still usually that you jump from location to location versus this kind of smooth walk. Yeah. you could, in fact, you can't, you cannot do smooth pursuit unless the, there's an object that's moving across the visual field and then smooth pursuit is automatic. You can't, you have no mental ability to do smooth pursuit. If you try, it doesn't work. It only works. So it's a, I believe it's a superior colliculus thing where the cortex has no control over that. but my point is it's not smooth pursuit, but it's also not jumping all over the place. You're, in some sense, there's always, the minimal thing I can recognize is always some edge or line section that if I didn't have that, I couldn't possibly learn anything. So given, like I use the Chinese character as an example of something which is basically just a bunch of little edges. And, and so I jump between those, because that's the thing I recognize, in some sense I'm building a compositional object of little edges, little lines, I'm just pointing, it's, we keep going lower and lower in smaller and smaller movements until we recognize something, and, Like in this figure, look at the windows across the street, those four paned windows across the street. I recognize those windows, and so I don't, I can just glance at any one of them and I'll say, oh, that's a window. But if I've never seen a window before, then I would jump down and say, oh, this is the vertical line, the horizontal line, and there's four of these little blue things, whatever, you know what I'm saying? It's but now that, once I know a window, I can just look at it and go, that's a window. I don't have to do it. But if you don't know what it is, then you're going to go deeper and deeper into it.

Is this a good time to take a break because we're already over time a bit? Yeah. Yeah. Maybe we'll stop here.