Nice. So yes, everyone maybe saw on, slack the plan. Today's, Rami is gonna present some slides and we can discuss, ideas around that. And then, if there's time at the end, we can just open up to more general brainstorming.

okay. Yeah. Guess whenever you're ready.

And do you want us to ask questions from the start, or would you prefer us to wait for a bit? I think it's fine. Yeah. Go ahead, ask questions. I like this is, mostly. me not being sure that my understanding is correct. And I think the best way is to just have this interactive and, yeah, make a mistake. Just let me know that this is not how we think about it. And, cool. All right.

can you see my screen?

Yep. Yes. Okay.

I'm gonna be talking about modeling object behaviors, and, I'm going to be proposing a slightly different way of looking at it. It's the same, I'm gonna be starting from the same, current theory that we have, but it's going to, change in some details. And, yeah, I'm just hoping to have some discussions on this.

yeah. So let's start with the current theory right now. we have, Movements that are coming from the thalamus, relayed by the thalamus. And, they, from Magnocellular pathways, they, some of those cells have, larger spatial receptor peels that are going to, represent the sensory location or the sensorimotor movement.

Excuse me. So something like this, and you're looking at a stapler and you move your eye. but there is also another signal that is coming from, Magnus Solar Pathway that also has, smaller receptor fields and it's just representing the, patch movement. So you're still looking at the same location, you're not moving your eye, but the object itself is changing or its morphing some in some way. Could be also color changing or, that's why you have Patch's movement. I think we, the last we talked about it, we were opening that up to basically change. Something's changing at that point. Not necessarily moving. Moving would be a big part of it. In the staple it would be moving, like the light on the traffic light's not moving, but it's changing. Okay. yeah, so it's something changing there, but it's not related. and just to be, and just to be clear, it's not that the patch is movement, it's the observed feature that the what the patch is observing. The feature is changing feature change. The feature is moving. It, we're not talking about the patch moving. Okay. you agree with that? Yeah, I agree. Yes. because we don't really, we can't really observe the movement of the patch. We just can observe that. and that's not the important thing. The important thing is that the object itself is changing. Yeah. Okay. And, the features themselves are going into, They're coming from a different, pathway, from the par cellular pathway. And these are represent, they're not representing change, they're representing the actual features that we're looking at.

so in order to build models, we are, the current theory is that we have two reference frames and, we, so we have a, an object reference frame. And this is, both of these reference frames are representing locations in physical 3D space, and they are tying, or they're learning, one of them is learning the features at these locations. and the other one is learning movements at these locations. And they're both synced, and they're both using the sensorimotor movement to know where we are.

and the object model and the behavior model, they're learned. They're basically, this is knowledge embedded in the associative connections between the features and the locations and the, movement and the locations. So the model itself is in the associative connections, and hope I got that Right. But again, I'll be, I'm gonna be precise Ram in the language the, they don't represent, although they are, every location you're observing at some physical location, the two reference frames are representing, on the left is representing the space of the object, the set of features, the static feature, excuse me, one on the right is representing the set of static features. and the one on the left is representing the space of the changing of the features. And although they are co-align in physical space, they, they do not represent physical space in the sense that they would, re-anchor on different objects or different behaviors. so I just, it's not like a. For someone who's new to this, you wouldn't wanna say these are representing, these reference names are physical space or they're not in some, egocentric space, or they're in the space of an object or, in the one case or the space of the, of the extent of the changing features in the other one. Okay. It's just subtle language, but is important to, I try, I find important to, to keep using the correct language on those. Yeah. And I, think, this is the, part that I'm, I may have some confusion on and, oh, or maybe you're proposing something different, but, okay. So which case I should be quiet. it's, you did say current theory, I'm not proposing that we're gonna change the theory. I'm just saying that, okay. This is where I'm starting from. Yeah. It's a, hard if you're not, if you haven't spent a lot of time with grid cells, it's a hard concept. but they're really, I. they anchor, right? And so when they anchor each time you're anchoring for a specific object or a specific thing. And so they're anchoring for the specific object for the stapler in this case. the one on the right. And then on the one on the left, a similar set of grid cell modules is anchoring, for, the, movement, the changes. And, and that, that's basically all you can say about it, Yeah. Okay. so I find it useful that, we dig a little bit deeper into the reference frames because maybe this is the part where I need to, be better understand. So I'm, just gonna talk a little bit about reference frames so that, make sure that I have the same understanding. so my understanding is that a reference frame, is a, some sort of coordinate system. it's a, it. Metric ordinance system, I think as you usually refer to Jeff, it doesn't have an origin, so it anchors to different objects, but it does not really have an origin. We, they're, not related in the, objects are not related in space relative to each other. what I'm going to show next is not the right way of showing what the anchoring is, because we don't really go in space and look at these objects like that. I understand that the representation of the location itself changes, or when it re-anchor to something, it's like it's REIT initializing to, or it's initializing to that, space, so that there would be no conflict between those locations. But the, but, we're not, really moving in space looking for these, objects like that. But this is like the closest thing I could, in the way it's implemented in Monte. It is not the way it's implemented in the brain, my understanding. in the brain we have these grid cell modules and they are, they end up creating a sparse distributed representation space. But in Monty, we're not doing that. My understanding, we're using more traditional, Cartesian coordinates, but we prepend, a sort of a class modifier, where we essentially just put this in some other space. So when we talk about the reference aims, we really are talking about two different implementations. And, we have to be careful because I'll be thinking about the brain's way, and you might be thinking about, Monty's way, and, and they're not the same, although I've been assured multiple times that it works just fine, that they're different, but we should keep rac They're really quite different. The way you show the picture there, I think more corresponds to the way Monty does it. It's not, it doesn't really correspond the way, grid cells do it. Yeah, yeah. the, like I said, these, there, there's, they're not related in space like that. And once we re-anchor, it's not like we're, moving to a different space, but it's just, I understand that the representation itself, the grid cells, changes the, it's like all of these are like randomized and initialized in a way such that it gives us a new space to represent the new object. yeah, I think the important thing is that the loca, like the location coatings aren't shared. So a location SDR on, the coffee mug is unique to that coffee location on that coffee mug, and we won't get that same location on a different object. so I guess generally, how you put it on the slide, like the definition that a reference frame is a coordinator system without an origin. I would say wouldn't be right. It's more like grid cells don't really have an origin in the way that they encode space. But usually how people think about reference frames is that they do have an origin, and that's why we have an origin in Monty. 'cause we're using Cartesian coordinates. and that, that coordinate space requires defining an origin. but then we use that shorthand Jeff mentioned that we prepend like a class modifier, so that every object has its own location space and it's not like zero look, coordinate 0 1 0, exist in two objects.

okay. It's weird in, in grid cell space. It's not only don't have an origin there, the, potential size of grid cell space is so huge, enormous. size of the universe, bigger type of thing that, once you anchor, you can have, integrate around the set of points, but they're all SDRs and it's not, like X, Y, Z, but you'll never, ever travel far enough to get to another objects part of the world, you know what I'm saying? whereas we think about Cartesian coordinates, you can say, oh, I'm gonna put the reference frame for this particular object up here in this distant corner in another one, in this distant corner, or something like that. But it's not like that in brains. it's really a, very clever but totally different way of doing it. But I'm not sure if that's gonna impact what you're saying here. I'm just pointing out the difference. Okay. yeah. Thanks.

and, this same thing that Viviane was talking about, that the locations are unique to the object, Whatever location we define with respect to the anchor, object is not going to be shared by the other objects. but my understanding is also that the movements are shared. the deme movement is shared, Okay. okay. So go right or go left or whatever. This is gonna move the location the same way. it'll, they both will have to integrate properly. Okay.

okay, so I'm just gonna go, now into the more, speculative parts. the, this first part is the same. We, already know that, this is how we learn features at locations. We just look at, we are getting some patch features and we move through the, objects, reference frame and, learn the associations like that. On an object, but these are, nice animations. How did you make them? Oh, I've learned, some transitions in, in PowerPoint, it's called, it's all done in PowerPoint. This is what PowerPoint. Wow. So is that occluded? is there a stapler moving rather than a window moving? In that case, this is the, this is, like a stapler object moving behind a window. Yeah. This is what I'm doing. I'm cropping it and I just move. yeah. Okay. Nice. Wow, that's pretty impressive. That's, again, I don't wanna be too nitpicky here, but the latest thinking we've had, at least, latest thinking I've had is it we accept this idea that, you have the center surround receptor field. Then, you wouldn't have an input that's all black. You'd this, the bottom dot there, the bottom, it's half white, half black. that makes a lot of sense. But from the biology point of view, not from Bonnie's point of view, from a biology point of view, it's not clear that you'd have any input at all when you're looking at a solid black area or solid anything. It's really what, the, features are always gonna be non-uniformity of some sort. And the, it, when we see a red circle, we're really actually only, learning the edges of the red circle and, and everything else is implied in between. So it's just a subtle difference. by worth mentioning perhaps. This is why we're, always saying it's edges at locations or orientations. Always. Yeah. I've come to, I've come to think of it really as non-uniformity. It's just 'cause that's what, center surround receptive fields do. They just, if you give 'em like any kind of uniform pattern, they won't respond. But if you give 'em something, any kind of non-uniform pattern over their receptive field area, they will respond somehow.

yeah, it's kinda interesting thinking about that in terms of 3D objects because if you talk about images, it's easy to picture how Yeah. Like a circle. It's more efficient to not try and learn all the points in the middle of the circle because it's all the same stuff. you're just gonna learn the edges. But what does it mean when we have a 3D object where, essentially every surface is in some sense as meaningful. I guess you could have like corners corner 3D corners and stuff like that might be more interesting. it's, pretty speculative, but if you follow the logic, then you say, if I have a flat surface, yeah, that would be uniform. But if I have a curved surface that's not uniform yeah, there's some information there, it's not all the same. if I can detect that curvature, then it's non-uniform. So in that case you might, you get to the top surface of a table. You wouldn't learn all the points on the table if, you don't have to touch all the surfaces on the table. yeah. Yeah. Maybe even if the curvature is constant, then that can also be maybe Approximated or, yeah. I don't think we understand it well enough, but Yeah. But yeah, it's hard to map on like edge detection as it happens with a 2D projection on 3D. Features? No, I think like currently what Mon uses, I, I'm not saying it, sorry. When I say it's hard to map on, I'm not saying it doesn't apply or it doesn't make sense. It's more just, like we, we've talked about like how we might get like 3D features from, and like where they might emerge, I guess a couple months ago. But I think it's still like the classic neuroscience view of what like Thalamus is doing as it projects to V one is still hard to map onto, like, where are 3D features coming from? Because that's normally not discussed. It's, normally just We have this 2D field of, this is the first time I've heard us discussing 3D features, or at least I forgot.

when we were talking about like motion parallax and, or, parallax, binocular vision and stuff like that. Maybe. Yeah. Yeah. We found some evidence that at least in mice they've observed that like motion parallax seems to lead to very early representations of depth. Representation of depth is not necessarily the same as 3D features, right? A feature is Sure. A patch, of a sensorimotor patch. The way I've been thinking about it is you take a sensorimotor patch and if it's got any kind of center surround receptor field, so that could be a part of, a flat area of the retina or the flat area of your skin, within that patch is if there's any non-uniformity, then you'll have an input. So you can just follow the logic from that. so obviously there's no 3D features on the retina, but, I guess I, we could probably go down a rattle hole on this for hours here. yeah. I don't wanna distract, I keep coming back on it. I, love this idea that you could just think about center surround, receptive fields, because if that's true, that sort of answers all these questions, you could just keep going back to the, what, kind of represent with the center surrounds receptive field. what would be, what would make her respond and what would not make her respond? Okay. I probably said enough in this. We haven't let Robbie get to his main point yet.

Okay. like I said, we just learned the model this way. But my question here is, do we also want to learn? I, my thinking is that we should not be, we should not want to learn, multiple models, for that object at different behaviors. As I, we should not want to build associative connections of the, of the features, as they move, like every, at every behavioral state, we are not building a new model. and this is my understanding, that we're building just one model of the stapler, and then we're applying behaviors to it. But we're not learning multiple models, of different behaviors for the same object. we didn't actually, reach a conclusion on this Rami. We certainly don't want to learn a new morphological model for every position of the staple top. That would be a, we can't, we don't have the time to do that. we, but on the other hand, we can infer the object in its different positions. Oh, we can talk about that. And obviously if, a object has two main positions, maybe it's like always open or always close and then move between 'em, then you might say, yeah, I'll learn the open one and the close one. And I learned that they move between 'em, but it seems we, it seems impractical to try to learn everything in between. This is very much what we were just talking about in the, morphological model. We said, Hey, if you, got edges where things are, you know where non-uniformity are occurring. And then you imply that. the surface of an object continues, even though I'm not sensing it. in between here, it could be the same thing. I've got, I have a set of movement vectors, a set of, I'm representing the behavior of this, of this top of the staple moving. And I might be able to generate, an image of what the staples should look like at any point along that trajectory, even though I haven't learned it. it's yeah, I, can fill it in and say, yeah, if it's moved this much, this is what it's gonna look like. Not that I've learned the morphological model of that. Okay. We haven't, we haven't figured this all out. So I'm, talking if we understand though, but we don't understand though. But these are, reasonable assumptions. Okay. yeah, like I think in general the feeling was it may be somewhere in between where it's yeah, we, may need to learn some kind of important key frames, almost. But, but yeah, we don't wanna have to like densely sample that space. yeah. Okay. yeah, we can maybe talk more about like path integration later. 'cause if that comes up Because I think that also is relevant. The, reason I'm, bringing this up is that because I've, I think that there's a lot of parallels between learning this and learning the behavior reference frame. And I think that whatever rules we come up with here is going to transfer, to, to the behavior reference frame. and I'll discuss what I mean in a second here.

I'm going to assume here that we don't want to learn this because this is going to be a lot of associative connections. It's like learning a new model for every behavior. a new behavior for every model, and then you have a lot of models. So it's, it is just, impractical to do that.

the other side of this is learning movements at locations. And, what I want to discuss here is that, we are getting patch movements. We're not, now, it's a feature change and we're learning it at a location.

but the, we're being anchored here to a behavior and we want to anchor to this behavior regardless of the object, that we're seeing.

we, probably, I think we agree on this, that we probably don't want to anchor to a behavior and an object at the same time. because then we get into a combinatorial problem of we will have, be every behavior has different objects, and then another behavior will have a different, object. So it's like we cannot anchor to both at the same time. We have to anchor to a behavior, which means we have to, but, couldn't, we have two separate reference frames, and they can anchor. At different times or at the same time. But whether one of them anchors or the other one does shouldn't affect each other, at least in the current setup of the idea. Sure. But, how does a why would we wanna have a reference frame that anchors to a behavior at a specific, for a specific object? are we anchoring? No, we don't, we, don't, think about it this way. You have the world of behaviors, which is just how things change around changes relative to each other. And you have a world of objects, which is how, where features are static, relative each other. And when you first learn a behavior, it's always gonna be associated with a particular object. I don't, I couldn't, I haven't been able to think of learning a behavior independent of an object. I agree. So you're gonna learn someplace and we can associate. The whole behavior with an object. But the behavior has its own reference frame and the object has its own reference frame. But those two, we can associate the behavior Id, let's say with a location on the object. but now I can apply that behavior to other objects, right? So we don't want, the behavior definition is not tied to the particular object, although I can, as I can associate it with a particular object, right? I can see of state booths have hinges and refrigerators have hinges and doors have hinges, but water bottles don't have hinges.

that kind of thing. yeah.

yeah. So yeah, this makes sense. to me, would we be able to learn. disassociation from, the interaction between the behavior reference frame and the object reference frame.

what we represent in the behavior reference frame is an, abstract behavior state, but it's associatively linked to the object in the object reference frame. instead of actually representing re relative locations in the behavior reference frame, it, it, must be associated linked because when we see the staple, we know that it can open and close. I associate that behavior with the staple. Okay. So there had to be an associated link between the two at some point.

but so that doesn't, sorry. No, go ahead. Yeah. the solution we proposed so far doesn't really talk about the associative link. It's basically just. To be able to recognize behaviors and recognize objects and recognize them independent of each other. like Jeff said, they're basically two separate worlds. And if you sense a specific movement, you can recognize that pattern of movements. And if you sense a particular object, you can recognize that pattern of features, but they don't inform each other at all right now in the current proposal. But, like Jeff said, we need to be able to do it, but I don't, as far as I remember, we haven't proposed specifically where that would happen if we have an association between Object and Behavior ID and Layer two three, or if we have an association between the two reference frames. s so, we haven't discussed it, but it's obvious it has to happen.

I, it just, it seems obvious to me that has to happen. another possibility that we haven't discussed any of this as maybe just said. you might even think of perhaps I'm just brainstorming here, that a behavior typically is smaller than the object and perhaps the, compositional mechanism could be used for that. You can, you could learn that this object has a, in some sense a child behavioral object that is, is applied at one location, one orientation. so that's another way to do it. You could learn between two regions, two, two columns. In fact, more talk about this seems pretty good, two columns and two hierarchical regions. And the behavior is a child of the parent mythology of it.

Okay. Yeah, I think that's interesting. It's like the sub object behaviors we talked about before with yeah, I'm forgetting the exact example we had, but where it breaks off. yeah. But keep going on this idea, the problem, a behavior, first of all, a parent object can have multiple behaviors, right? We talked about staple having a, deflection plate behavior, the hing top behavior. There's other behaviors, adding staples and so on.

and, and each of those behaviors is exist at some location and some scale and some orientation to the parent. that's exactly the language we use for compositional objects. So it seems logical to think Hey, the same mechanism for compositional objects of the, logo on the cup would work for the behavior of the hinge on the stapler. I'll, I've said enough about it, but that's a pretty interesting idea. I, completely agree. And I think there's a, lot in common, in how we learn, the positionality and object behaviors. I, think that the, it's the same me learning mechanism and, I try to talk about it a little bit in the writer, but I don't think I did a good job. anyway, so I'm just trying to make, comparisons here between learning features at locations and learning movements at locations by saying that in the features at locations we're anchoring to an object, and we want to represent, that object regardless of what behavior it's, it's at.

we don't want to learn multiple models, of that, every model would have a separate behavior. Also in, learning movements at locations, we're learning a behavior, we're anchoring to a behavior, and we want to represent that regardless of what a model is, exhibiting that behavior. Because we are anchoring to a behavior, we don't really care what the model is. This is part of recognizing the behavior. If we want to recognize the behavior regardless of the object that's exhibiting it. And, I think it's, probably reasonable to assume that, how we get there is, a different story. How we learn that is a, different, story, but also, it hints at what, how I'm thinking about these locations because I think these locations are somehow associated, linked to physical locations on different objects, that have different morphologies. a laptop, top is different from a stapler top. So in my thinking that. A location in the behavior reference frame needs to be linked, associated, linked somehow to different physical 3D locations that describe different objects such that it could apply these movements on them at the same time. a and by saying that, it also means that the locations in the behavior reference frame is not necessarily in physical 3D not necessarily physical 3D locations because they, need to be associated with different objects that have different morphologies in, in, in, physical space.

but aren't they still locally, spatial, like they might anchor at different times to different parts of an object, but if a sensorimotor moves in the behavioral and in the morphology space, like once it's anchored, is it, it would still be the same. Like it would still be kinda moving in 3D space. I think it starts this way. It's when we are learning, when we first learn, when we first observe a behavior of a stapler opening or closing, I think it starts this way that we learn it on a stapler, but as the be, as the behavior becomes more and more abstract, we move towards a definition of a location in the behavior reference frame that is not, tied to the movement. It's tied to the, to moving along the behavior trajectory. what I'm imagining is something like this. We, I'm saying scale behavior here, but we, I know we haven't agreed that scale is a behavior, but, we're, anchored to some behavior, like a scale behavior. And the, locations here, the, these red dots are basically, moving along the trajectory of the behavior. We didn't learn it this way, it became this way. The, lo the definition of the location has morphed into this so that it could represent different objects at the same time. are you saying that. You think that the behaviors aren't learned initially in the behavioral space, but learned in the objects morphology space? Is that what you, and then later it gets, learned in, it doesn't get transferred. I'm saying it, it is learned. it is originally it can be learned tied to a stapler, morphology in its own reference frame. But the definition of the location in this behavior reference frame will change over time to represent multiple objects. the location, no, wait, Let's, be careful there. the behavior, the space of the behavior, I don't think it's ever gonna change.

you can associate the behavior at a particular location on a particular object, but the behavior is defined by its behavioral, by, its grid cell space. Just an object is defined by its space. That once I know the object, all the points that I'm, the object might occupy are defined. so up to now we've had this sort of like a iron clan rule that once you've, at least in grid cells, but once you've anchored, you now anchoring is synonymous with recognizing the thing. So once I've anchored in the objects morphology space, I know what the object is and once I've anchored in the behavior space, I know what the behavior is. But the beha, but those locations wouldn't change. that's how we've been thinking about it. If you're suggesting they do change, it's not that the location changes, but the meaning of the location. So what I'm, suggesting here is that at the beginning of learning, as not scale, maybe let's go to a hinge behavior at the beginning, finding a hinge behavior, the. We are, we're learning that behavior specific to the, to the stapler. And it, this is how we know how to identify this behavior. It's, it is on a stapler as we start abstracting it more and more, the location here cannot be represented in physical space in the morphology of, the state. let me stop you there, Ramy. 'cause the way we've been viewing it, is that imagine you have the, every, the definition of an object is static, features at position, or poses in some reference frame. And the definition of a behavior is changing features at poses in a reference room. Now, from the very beginning. If I have a changing feature, it's only gonna get stored in the behavioral object. And if I have a static feature, it can only get stored in the morphology. they're separated from birth. If it's changing, it's gonna go into the behavioral space. If it's not changing, it'll go into the objects morphology space. So I think that's, if I understood what you said, that's counter to what you were saying.

no, I, agree. I just, I don't think it's counter, I, think what I'm saying is that the behavior reference frame locations need to be abstract enough to represent multiple, yeah, but they are from the start. they're the, locations in the behavior space. Only makes sense in the behavior space they have, no, actual correlation between, they're not tied in any way to the locations in the morphology space. No. Or, like they're associatively bound. They could probably, but, it's not like they, yeah. As you say, it's like a separate, the definition of the, from the very beginning, the behavioral object, the behavioral sequence is its own set of SDR locations and, and it never changes, but, and I think in a, or just from what you were saying earlier, Rob, because it feels like in that behavioral space, like having, it's still important that it's like a space. Like obviously there's temporal dimension as a complication, whatever, but like you can still move through that space with your sensorimotor and that affects what you predict. but I agree, we need to be able to, for example, like almost scale it up or even deform it, maybe to apply it to different objects, but it feels like that's maybe more about where you bind and also maybe like how large the behavioral space is than if becoming, I don't know, somehow more abstract or but Neil's that, that all that gets answered. If we take the idea I had a moment ago, which is that, that, behaviors are child objects of, larger morphology objects. if they work the exact same way as the, as the logo on the coffee cup, but it's the hinging behavior on the stapler, then we know for the, logo on the coffee cup we have to do a scale and orientation and changing and, morphing. And we came up with the answer to that. it was, hard, but we came up with the answer to it. So I think that could apply here too. I can say I have a behavior, say hinging, but, when I apply to a particular object, I'm gonna do it in the same way I applied the logo to the cup. I'm gonna do it, I can do it on a location, basis. I can learn those associations as I need to. and therefore the behavioral model can be applied to, a someplace on, a parent morphology model. Yeah. Yeah. I really like that idea because, but first, we already talked about how behaviors can be recognized at different scales and orientations and locations, but the one thing I was still wondering about was. Deformation. if you imagine the stapler top is curved and moved up and down, you can still recognize that hinge behavior, but it's in different locations. But then Niels reminded me the other day that in a hierarchy, we already have a solution for object deformation. Like we can learn the bent logo by just, assigning the logo, location by location basis to the cup and assigning it at different orientations on different locations on the cup. So that would also be a solution for assigning like a bent behavior to an object quality. That that's exactly what I was saying. Yeah. Yeah. I was just, yeah. Putting it in my, I'm just, you guys may have thought of this. I can't remember we discussed this, or, but I'm just saying yes, that's the same thing. maybe you knew that. Yeah. Yeah. That was a threat on Slack, so you might have not seen that. I, in the last few days.

I'm still a little confused about the locations and the behavior reference frame. and so let me ask it in a, different way.

These, locations here, even though they're completely different SDRs, because they're in different reference frame, they're relative locations, they still, e even though we don't store any, features or, orientations or, whatever, in this here, but the relative locations between them still in some way defines the shape of a stapler. No, it doesn't. No, it doesn't. I yeah. Something we've not addressed is maybe that the assumption that movements could be scaled by different amounts in different, in these two reference frames, or at least, yeah. actually, I don't think you need that. I, think you just need it. When you adapted to a new object, you would scale it. by the way, let's just go back to what mommy just said. Mommy, you said that. Even though they're not, the behavioral model does not represent static features, it still defines the shape of the stapler. I think that's what you said. Yes. Okay. I disagree with that. Okay. You could, first of all, the behavioral model wouldn't represent anything that's not moving. So if there's a part of the staple that's not moving, it's not gonna say anything about it. Okay. it, you can have a behavioral model, which is just a deflection plate. Okay. That's not gonna tell you what the, what a stapler is or what a staple shape is. It does nothing about that. It just says within this local area, a set of changes can occur and I can learn that set of changes over time.

it really is a subset. Of the whole stapler, just like the logo on the coffee cup is a subset of the coffee cup. It's not the whole coffee cup. It just, is that right? I agree. I agree. the point is it's still a subset of the locations on the stapler. it, it, at any point it might be applied to a subset of locations of staple. Just could, it could be applied to your laptop, it could be applied anywhere there's a hinge.

and, that's because these reference frames are synced and they, the, relative, locations in both the behavior and the object model aren't the same? no. No, not, still. that's how it's learned when you're learning it. If you, I think if you looking at a physical object and it exhibiting behaviors, sure. but again, think about the stapler. There's a reference frame for the deflection plate behavior, and there's a reference frame for the hinge behavior. Those are completely different reference frames. And at any point in time on a particular staple, I could apply the reference frame for the deflection plate at some point in the reference frame for the hinge at another point. But now we have three different reference frames. They're really unrelated to each other, only in intent, in, in a particular object related.

and my point is that, and when you recognize them, you can also recognize them in different skillset. Like you might recognize the h learn the hinge on the stapler, but then recognize it in a totally different orientation and scale, on a different object, like on the door or on the laptop. so it like when you recognize a behavior on a new object, that reference frame, like how you move through it, might look totally different than how you move through the objects reference frame because you're actually recognizing it at a different orientation, location, and scale.

Okay.

It's still not clear to me how the relative location was similar to the stapler, but it's, but we can recognize it on a different object. I know, It's the, it's, we're still storing movements, but these movements are defined at specific relative locations. A rock. But maybe if it helps with like a, stapler versus a laptop. So when you learn the stapler, yeah, you moved five centimeters or whatever from the kind of hinge point to the tip. and that was what kind of stored that relative movement both in the, morphological and in the behavioral model now. And, the, kind of recognition of that behavior is this kind of movement of this hinge type thing. When you're then seeing the laptop open and you cade to the tip of the laptop, that's like moving most quickly is the end. That's where the kind of hypothesizing in the scale of the behavioral model would basically say, oh, okay. I think the behavioral model is actually larger because it's mapping onto this larger object. And so the location, which is the tip of the behavioral model, is gonna be bound to the kind of tip of the laptop lid and the, corner, which is now separated by large distance. And this kind of ties into what we were discussed the other week where Viviane was pointing out, we probably wanna be able to transform the, movements separately for the behavioral and the object models because they may have different rotations and they may have different scales. Yeah. Yeah. I think it's, what you're saying is that helps, makes sense.

it is, It is useful or it is not useful. It's necessary that if we haven't seen a hinge behavior before and we learn it on a stapler and we're going to recognize it, an object, that object, a hinge behavior, it needs to be applied in, on a morphology that's, close so that we can still, anchor to the same behavior. But I think that by anchoring, both morphology, both the both hinge behavior, both, the stapler and the laptop to the same behavior, I think this is going to change, doesn't have to be close. Remember Rami, at the last, retreat, I brought up the example of having a chair where there's a little piece in the back leg that had a hinge on it. It was a totally made up idea.

There's no, the chair is nothing like a stapler, and it's only a little part of the chair. So I, think you have this, you have to just think of these, it's completely separate objects. The, behavior object and an apology objects the set of each of them. And they can be mixed and matched in different ways, at different locations, different scales and different orientation. I think it might be maybe a bit of an issue of like introspection and trying to think of examples, where you're applying behaviors to morphologies.

I think I, I get your point of, and I was first thinking like that too, that yeah, we are still defining those changes at locations and those locations. Tell us a bit about the shape of the object. But, your picture is accurate that the behavior model is just these red arrows at these locations. If I look only at the behavior model, I wouldn't be able to say that this is a stapler. but the fact that they are at these relative locations mean that I will only recognize that behavior if I see changes at those relative locations, and I won't see them on objects that don't have a solid straight thing that can move like that. So if I am looking at a ball, I will never recognize the hinge behavior because I can't see these changes on a ball, but as it's like un, unless it literally splits in half and basically develops a straight edge that could then map onto it. Yeah. But yeah, yeah, when it's a solid ball, it's like I would see these changes relative to each other, how they were stored in the behavior model. But if I'm not seeing them, I'm not recognizing them. But it's independent of the actual object itself. It's just what's physically possible to observe.

Okay. I think I see your point.

I just think that, yeah. Okay. Yeah. Nevermind. But do you wanna talk a little bit more about the abstract stuff? Because I, I think we're, we're trying to at least explain how we're currently thinking about it or whatever, but I still don't want to get that, to get in the way of you talking about, yeah. Your ideas. 'cause it, might build on that in, in some way or, show us a problem with what we're thinking. I, this is how I was, before this conversation. This is, what I was, going to propose basically is just, the, we anchor to a specific behavior and the movement becomes abstract enough that it. The locations here do not represent, relative locations on the specific objects, but rather, a location in the trajectory of the behavior. the movement would ba basically be moving it into an open state or a closed state. The, I can go on, and explain more about this, what, I mean here, but, and is that a space where like a sensorimotor can still move through to make predictions?

it becomes, so the, sensorimotor will still move. let me, lemme finish that. the, yeah, please. These are the, end of the slides. These are these few, four slides, I think. so I'm just going to contrast behavior, reference frame and object. Reference frame from the way I see it. the object reference frame here on the right, this one anchors to an object. And the behavior reference frame anchors to a behavior. And by saying that the, they will anchor to that regardless of the other concepts. So an, an object reference frame will anchor to an object regardless of what behaviors this object is. is exhibiting, like what kind? it could be different colors, it could be different states, or scale. If we assume that scale is a behavior, and this is a desirable feature, we want to factorized the, behavior from the object representation so that we, first of all, we don't build too many associative connections in the object reference frame. but also for recognition, we, want to generalize such that we can recognize this object. If I take my stapler and put it under the sun or if I put it in a dark room, I still want to recognize it. So this is a kind of variability that we want to recognize, that we want to. Cognize regardless of, for the behavior reference frames, it's a similar idea. We, we want to anchor to a behavior regardless of what object is exhibiting that behavior. So we need to have this idea of an abstract, abstract locations that can represent multiple different, objects. they, can associate with multiple different objects in physical 3D space, but the location itself and the behavior reference frame is, more abstract than that. so this is the, this is what I mean here, as we move through, this is probably not, a very accurate representation, but as we move, as we change the location in the, behavior reference frame, we move along the trajectory of the behavior.

this on the right is the object reference frame. This here on the left is the behavior reference frame. The behavior can change the object, the be well, what's in the middle here is like the superposition of both or like the, some sort of a conjunction code or something like that. like in the toman, icon. Bu it's the, it's both of those. In order to actually represent the object at a specific behavior, we need both locations. We need the location from the object reference frame, and we need the location. In the behavior reference frame, the behavior does not really represent the object in any physical space. It just represents the behavior, but it can be applied to any object. And then the object reference, the object locations can be applied to, that same object, but on, on multiple different behaviors. So you can change the behavior, but it's. changing the behavior doesn't really change, your sensorimotor location, for example, like here on the other side. Sorry. Yeah, no, sorry. I'll let you, yeah. On the other side, you can change the sensorimotor location and you can move on. so you can change the sensorimotor location without actually changing the behavior. You can, the behavior could be, an open stapler and then you can change the sensorimotor location on it. You're still detecting that it's a, it's an open stapler, but you are changing your location on the open stapler like this or, a closed stapler. It, doesn't matter because the sensorimotor, the, sensorimotor movement only controls location changes in the object reference frame. The behavior, the, movement of the object controls the location in the behavior reference frame. So you can have movement here, but you don't need to have movement here. the superposition of both, or the associative connections between both is going to give you finally the, to be able to predict basically, where on the object, given the object and the behavior. but we still need to move in the behavior reference frame too, if we're moving the sensorimotor so we know where in the behavior model we are, right? So we know what changes to predict at that new location. If the, if the behavior is not changing, I don't think so. So if it's, an open stapler like this and you're looking at the stapler and you're just moving across the sensorimotor, it, moving across the object with your, with the sensorimotor, you don't really need to know anything about the behavior except its location in the trajectory of the behavior. I, I know it's an open stapler like this. I can. I think what Viviane was saying is that while you're observing the behavior, you have to, you actually have to observe the locations that are changing and there's multiple ones.

your sensorimotor pads can only observe, we'll say one at a time. Yeah. you need to observe. you have to be able to, if I'm observing a change, I have to know where I am in the space of the behavioral model and therefore I have to know my location in the behavior model. that's why we And that location, yeah. Or like where you are in the trajectory space is a function of your location in physical space and your location in time because the stapler is gonna be having a different behavior at towards the end of where it closes. oh. It opens from at the very start. And that's because you're at a different location.

As well as a different time picture when the sensorimotor movement goes into both of the reference frame and you move through both of them in Synchrony using the sensorimotor movement. I'm sorry, can you repeat that? The question, what, you showed in your picture on the first slide where you have the sensorimotor movement go into both of the reference frames and you move through both of the reference frames in Synchrony, this one or the, very first slide? the very, very first slide.

This one or, yeah. Yeah. The sensorimotor movement at the bottom that goes into both the objects and the behavior reference frame. And remove like the sensorimotor moves. Remove in both of the reference frames, because otherwise if I'm, if I have the opening stapler and I Cade from the top to the bottom, I need to make the predictions about those changes that I'm next going to see.

so yeah, this goes back to my assumption here, that the location in the reference frame is a, it becomes an abstract.

it becomes, it re dislocation now represents the whole behavior, state. So you, would know be, now you have a, morphology of the object and you have a location in the behavior reference frame that represents what state it's in. So as you move here, you can predict what the rest of the object will be like. because you're learning, this location here is not, is, it's not at a physical location. it's a location on the trajectory of the behavior rather than so you're basically changing from changes at physical locations and then you. You changed that to actually pulling over all of the changes to define an object state and then moving through that state space.

Yes.

Why are you just starting information, the local changes? how do you still make predictions about local changes? If you get rid of that, you, can still make, the way I described it here, this state here is just a behavior state. We have the, model, and both of those describe what these, the, actual state of the object is.

it's an open stapler or a closed stapler. You, can make predictions, using the, associative connections between the behavior reference frame and the object reference frame.

You, have a, it's like a superposition of these two codes or whatever that gives you. A full view of what, what's happening.

So for example, like with the stapler opening, imagine as it's opening, there's something occluding your view in between. And then so now you're gonna move your eye to where it's no longer gonna be occluded, and you're gonna be able to make a good prediction of what that movement is gonna look like at that point. Because that's a location that like, because you're, predicting a, state, like a location in the behavioral model, but that location is a function of your location in space, which is where you path integrate with your sensorimotor movement. And then you're like, okay, I've learned a, feature changes at, a particular location.

yeah. But, so yeah. How, would you predict something without something like that? I guess you wouldn't need to. it's just a, it's the same way when we, When we learn an object model, we're learning features at locations, and we know we can predict what's happening here because we have an object model. When we have a behavior model, we can learn what's going to happen somewhere else because we know how that behavior affects the object. but how do you know you've moved to that location in behavioral state? I've made a aka how do I know to expect a particular point in behavioral space? I could be anywhere in behavioral space.

no, you're still tracking your behavior. but the, this is where it's a clued, so I, have to make a prediction at a future point or like a new location. What, what exactly is occluded? The object is included. the movement of the movement of the stapler head. Of course, if the movement of the stapler head is occluded and you have not anchored to the right behavior, then you cannot predict what's going to look like if the, and I'm not saying it's secluded the whole time. It is just I think this was an example Jeff brought up previously where like a stapler iss opening and then let's say at 45 degrees, it starts going behind like a little screen and then it's gonna come out again.

but like, when it comes out, when it comes out again, you, don't predict that the stable head is going like, it's going in the kind of exact same like feature movement that was seen, the feature chain that was seen at this point in time, and space. That's not what you predict at the other end. You predict a, new one, which is a function of, where it's moved ahead in behavioral space. you're anchored to a behavior even though, so if, I'm looking at, can you see my, camera? yeah. If I'm looking at this stapler and, I, I occlude part of it and, I, I still move it. I, because of the movement that I've seen from the part that's not ud I can predict, the behavior, I can anchor to the right behavior and I can predict where am I on the, behavior. So that gives me still the full model of what's happening behind the occlusion. It, you can still predict all the points behind it because this here represents the behavior state rather than the behavior at the locations. the, movement at every, location. Maybe I'm not describing it very well, I don't know how do you even still re behaviors in this solution? Like where do you store the changes at locations to recognize that there's a hinge behavior going on? Where do I store, the. The changes at relative locations that we have in the objects reference frame?

it's the same, it's the same theory that, we already have. It's just that I'm still thinking that the location does not become, the relative. these, locations will, morph, will change to become, locations of states. And we can move between these states rather than a location on, as on, in, in physical 3D. They, don't move by the same, way that when the sensorimotor moves, they change the same way. It's not a, it's not a location like that anymore. It's a, location of, in the trajectory of the behavior, but it represents the, I think we're a bit confused with what the states are now doing in the behavior reference frame, because those are not changes anymore. Like when you say the, it is in the closed. State, and it's not moving and you're moving, on it. That's not like nothing in the behavior model that because there's no change happening. The No, they still store changes at locations. It's just that, for example, if you've only seen the behavior on the stapler and the laptop, then the location is going to be the full, stapler, top and the laptop. Laptop. It becomes, this point becomes the, whole, top of the, object. and then you're storing a movement at this location, which represents the, top of the, of the stapler and the top of the laptop. And as you move in the behavior reference frame, it, it represents the, it, the, change in the location. It is just a change in the, the, in the state. Basically it becomes a, this change in the state. Yeah.

Ramy, I wonder if all this confusion is coming about, 'cause you have a maybe a, different idea of what we mean by locations and you, because sometimes you say physical locations and, nothing in the behavioral model and nothing in the, physiology of the morphology model or physical locations, those locations and those models are, all, both of 'em are all abstract. It's just relative to each other. And then of course, an object will appear someplace physically in the world. But I'm just wondering, I'm hearing, I'm trying to figure out what's con what's confusing you or what's, what, the issue you're trying to get out, because I don't, see that issue. and I'm just wondering if you have this, that you have this just a bit different idea what we mean by the space of the morphology. It's not a physical space. It's not a stapler could have a big stapler, a small stapler, right? I could have, it's a, set of locations relative to each other. and that's all it is in, the morphology model and the behavior model. That's all it is. And, after that, they're nearly identical. They're the same process going on everywhere. I'm just trying to, I think we're beating around, I think there's a more fundamental misconception or a difference of opinion about something here, where you think somehow think the, stapler space is more real than the behavioral space. And it's not, they're both equally as real or non-real. They're both as abstract or not abstract, however we wanna call them. is that possible you're thinking along those lines? yes. I'm thinking that they are different. maybe this is what we need to address. I don't, as I view it, they're not different at all. They're virtually identical. It's just that in one space you're storing the changes. In the other space, you're storing static features. So changing features and static features, but after that, they're identical type of spaces.

and the fact that you keep saying the word physical space, I would never use that word, physical space or very rarely I have the space of the object, which is, it's not, it's really self, it's self-referential relative to the object. It's not, when I define a model of an object, it's not physical space. it's, it is model of it's stapler space, and it's all relative to the stapler, but nothing else physical about it. and the same thing with the behavioral model. It's relative to the, there's a, space with which is metric, but the different cha where things are changing in that space is what I'm storing in that model.

Yeah, I'm just thinking that it might be a more fundamental problem here. Maybe, it is. Maybe said, maybe that is right. yeah, I, think, yeah, so yeah, you probably pointed at the, problem. My, my confusion point is, way what this tells me is that, first of all, this is a really hard concept. these are real, I, it took me months to understand grid cells. I read paper after paper. I, people try to explain to me, it took me months until I got it. It's, and maybe I'm slow, but it's hard and the concepts are really hard. and maybe what this tells us we need better, educ, we, we need better way of explaining some of this stuff.

the, I, don't know. I'm just saying it, it could be an education issue. I think it also, yeah, I'm curious 'cause. Firstly, these were really nice slides, I think. Nice discussion. I agree with that. I think helpful for, me to also understand better I think our current approach. yeah, I'm curious if if you go away maybe after day and like also just thinking about what we were at least arguing where you can like scale and location by location bind behaviors, but that at least in our head right now seems sufficient to do a lot of this. I think it would be helpful if you do identify issues with that to then bring in concretely then how you see that helping with this. 'cause I think that's also part of the challenge is like right now when you describe this, I think we're more seeing it as taking away stuff, like being able to have, integrate with the sensorimotor moving and stuff. And it's not clear what, it's adding, but, maybe there is something that, that we're missing.

yeah. I think one other thing that this, maybe highlights, or at least that makes me think, that we need to talk a bit more about is that in the behavior model, there is actually another space, besides the like reference frame of locations. We have the temporal dimension as well, which is. The state space, how I, if I understand you this is maybe a bit like the state space you're describing, where through time we can move through different states of the object. And that's an open question that came up before. If we can actually path integrate through state space, like if we have a joystick, we know how to go from wherever to wherever in that state space or if those are discreet sequences and we have to learn a bunch of different sequences that we go through. And then maybe a, second related question that I, forget if it was maybe you, Scott or Jose, but when we were talking about the, diagram last time, and the temporal input going to the behavior model, whether there's also some temporal dimension to the object model, where, like maybe what we mentioned earlier that we can learn certain key states of the objects morphology, like the closed stapler and the open stapler.

yeah, just that, not a topic for right now probably, but that there is actually the second space in the behavior model at least, which is the temporal state space. Yeah. Which I agree is under kind of visited or under-discussed or whatever in the kind of solution so far. Like how Yeah. is do we have key frames in time as well? Like how do we, represent that without having, again, like another explosion in how many representations we need?

yeah. One or, oh, sorry Jeff, you're on mute. Yeah, I'm not too worried about that. I think it could be, it's worth exploring, but the, the behavioral state is, is pretty sparse, I thought, compared to the object space, the morphology space, and, it's only where things are changing. And they, don't change forever. And, the same location doesn't usually change twice. it's so I'm not sure that being too dense. I, am concerned about how we learn it. I, and then in our, retreat, I was really, I kept bringing this up, like, how can we learn? I can't imagine the final state, behavioral space populated properly, but I can't, I don't see how a single sensorimotor patch could learn it. A single sensorimotor patch can't observe all of it at once, so maybe if I could, try to at least my benefit, summarize several things I've heard here today. First, I've heard that that really smart people like ROI King, perhaps maybe I don't want prejudge this, but maybe you end up with an incorrect idea about what the space is. And so we need to address that from an education point of view. Second, we, I think we came up today for the first time, the idea of, representing, behaviors as children of morphology objects. I don't recall talking about that before. and I think that strikes me as a really great idea. So we should explore that. And, and that addresses a lot of the issues we're talking about just as Neil summarized. And then the third thing is we still have this issue of filling in. if we, have, we can't learn every edge of a, every surface of a table. We can't learn every, face part of a face of a cup. and we can't maybe fill it and learn every single point in a behavioral space. We have to be able to fill in between, we have to, be able to now. Take the knowledge of what we do have and fill in the consistencies in between, an old idea. But I think we never really fully addressed that.

those are my walkaways so far today. That's just my personal summary. Yeah. Yeah, I think that's a nice summary and, also just yeah. Two other things I think that came up was like just kinda acknowledging again, that we've talked about that we might need to learn like multiple morphology models, at least like maybe a constrained set, but what is that number? How is that a function of the complexity of an object? And, I guess like ties in with the filling in thing you were talking about, right? I think it does. And then, yeah, and then the, in the temporal dimension. yeah, I had a brief thing I wanted to talk about in terms of path integration. unless there was any, more in your presentation, Rammi. No, that, that's all. Thank you all for the feedback. It was really great. Yeah, thank you. That, was really good. Those are great. Those are great images by the way. Really nice presentation. Thank Yeah. Easy to think about that. Yeah, same. yeah, it, is just a silly introspection thing, we use introspection a lot and it was just, the other day when I was, bouldering, you have this concept of when you're looking at, the kind of climb, sorry, lemme pull up an image, that you try and read it. So as in like when you're standing on the ground looking at all these handholds, obviously it's this kind of mess, but rather than just jumping on it and trying to climb it, what you try and do is picture yourself moving through, kind of space and actually doing the climb mentally. And I thought it was an interesting example of kind of mentally simulating a behavior because it's really hard to do. and in general you can only, you can only do it really over kind of short horizons and if you're going to do it at all, you have to anchor after a certain kind of horizon and then go from there.

anyways, it just got me thinking that and I, mean we've talked about this before I think, but just like a kind of resolution to this whole question of like path integration versus not path integration is we probably can path integrate in, a lot of behaviors, but only over very short horizons. and I think, I guess an interesting aspect of this, I think it fits well with the biology of grid cells, which is that I think the more grid cells have been understood, the more they've realized they aren't a very good metric of space over large distances, that they quickly become noisy and distorted and all this kind of stuff. But they're quite good as, local metrics of space. and so as long as you don't expect to be able to, in the case of sensorimotor movement path, integrate very far with accuracy and maybe also in the case of behavioral movement, move through a behavioral trajectory very far before you reestablish where you are in that. Then, you can do some path integration, but it's local. and yeah, it might, make it me easier to learn, like we've talked about how like in a local space it's more easy to densely sample it and almost learn it as like a series of sequences.

so yeah. I dunno, if this is an argument for it being a, like being the, temporal aspect, being path integratable as well. Yeah, I think so. But just again, over kind of short horizons. so it's yeah. I don't know what it means to be path integral in time. I, don't know what that means.

yeah, in time it's taking different trajectories path, maybe path intergroup, gradable in state space. So if you imagine a, an can be a different states, so like the stapler can be open and closed, or in between. But then knowing how to get from one state to another, you observe how it opens, but maybe it's more like a, it can also move curved to the closed state. So being able to take a new path to go from one state to another. Yeah, it's not really through time. It's both through state space. Okay. Yeah, because as you say, time is linear that once you're on the trajectory of time, you can't go back, move off time. Yeah. We're, Or go outside of time. Time travel. Yeah. No, I think you're right. Maybe it's more like behavioral space. Maybe a better example is like a stick shift in a car. You can, you, know the different states it can be in and from every gear how to get into every other gear. Even though you maybe in the beginning have never done a specific transition. Like maybe you never shifted from Gear One to Gear six. but you still know how we would get there. I, you might have to think about it a bit more.

I think you're second nature going from one to two to three. But yeah, you probably have to think about it, but, it's not like you have to have stumbled across it to, and experienced it to be like, oh yeah, there is a connection between these. You can infer it. I have one question for you, Neil. When you bouldering, do they have a harness on you? No. Oh. but that's 'cause it's very low down. Oh, yes. Should we allow this behavior? I'm not sure this would be acceptable in a thousand Brains project. We don't wanna have, after the Yeah, we did have a lot of injuries that, you meant I know that's right. And I, this is crazy. they're more hardcore climbers. I can promise you, I'm more of a scaredy cat than, than Ben and Lucas, I'm not doing anything too crazy.

and one, one funny side note, when you gave that example of kind of scoping out the, route is that usually you actually do the physical movement while you're looking at it. often at least like you see people go like this and imagining for sure they go, but they actually do it on the ground, just moving their hands how they're planning to move them. So maybe that's an as well in that process.

Yeah, and I was thinking 'cause yeah, in general we're gonna have to get better at dealing with noise as we move Monty into the real world. So like even just path integration in space for morphology models. Like we can't assume in the long term that's noise free. So I was like, is that an issue if we can't do No, it's like over longer distances and stuff. But I don't, shouldn't I actually, I think with no, exactly. And I think with like courser models for example, I. Like that would deal with kind of like things like large dec codes and stuff. that's what I've used this example many times. I'm sorry for using again, but this idea that you're walking in a room at night and it's dark very quickly as I leave my bed, I can just sense the uncertainty increasing. it just, it's oh my God, now I have no idea where I am. But then, as soon as you catch a single thing, an edge of a door or something, then immediately know where you are again. you can just observe that, it's a very inaccurate, noisy thing, or even medium short distances. But I think it's not a problem as long as you're able to re-anchor with a single observation.

I, think it's something we might not, we might wanna include in. I was gonna say, I don't know if you've heard Jeff, but Rami's working on some cool stuff with kind of re-anchoring and re-sampling hypotheses, at the moment. Sounds great. So yeah, that'll be I think, a nice. Addition to Monty.

I think, today's discussion for me is it reinforce, even though Romy is poking holes in it, I think it reinforced to me the, validity of the idea that these two, models are really parallel. And, and now we've introduced the idea that they might be parallel in hierarchy as well, but it's not a hierarchy of behavioral state. It's a hierarchy of a behavioral state on a morphology, which kind of addresses some of the things that Romney was trying to get at. Like how do you apply this to a particular model of morphology model. Yeah, that also fits with, when, before we talked about like how behaviors would, be communicated in the hierarchy. and one of the issues I had with having like hierarchical behaviors is that the behavior model ID isn't really a change anymore. It's like a static feature. So it would make more sense to assign that to the morphology model in the higher levels region. So I think, I really that idea of using that as a solution for, assign assigning behaviors and being able to deform them and assign them on different locations on an object. Same way it's so interesting because the region, we just said, oh, let's apply what we know. Said, oh, there must be a hierarchy of behaviors and they're gonna be, they're gonna work the same way as the hierarchy of morphology models. but it was a very slight twist on that. And, I think that's a, I think that's actually a quite good advance for today. That's how it feels to me. Yeah. So however we got there, thank you, Ramy. I think it's a great, it's a great, progress. Great. Yeah, you can, be our foil, you can be out there just, poking holes. rubber duck. Yeah. I thought it was also actually really nice explanation of where we are so far, the first slides right. And a really nice visualization. I was actually thinking that this might be a good first video to release if we release this idea. at least the first part of recapping, I think it was at least helpful to me to go through the basics of, the reference frames and also the, the current theory that we have. I think, we still need to go through the basics again, because this, there seems to be some confusion about it.

and, I'm not sure what the best way of doing that is.

Other than I've said it multiple times. these, are very interesting conceptual, very challenging conceptual things they absorb and and so that's personal observation about myself. And so we should expect that lots of people are gonna have trouble understanding this stuff right from the beginning. Oh yeah. Yeah. Maybe one thing I have trouble for sure my hand up, I think, I don't know if someone should wanna do a review of grid cells and good some anchoring and so on, that might be useful at some point. yeah. And I'm also wondering, how much we've talked about how we think about scale. 'cause I know we've had brainstorming sessions on that kind of back in 20 22, 20 23 when it was just Viviane and you, but I don't know how much we've talked about that recently. And so that might be something, I don't know if we're taking for granted. Is it? we have a, like I, think we have a reasonable idea of maybe not the biology is something hand wavy about, or, like unclear about, phase. but at least in terms of if we were to implement it today in Monty, I think we have a reasonable idea of how we would do it. And it's, 'cause it's not that different from rotations. I, would still be interested in going over that again because I think, there was actually only one in depth presentation that Jeff gave about it in the, in the idea of how it might work in biology. And I was, sick that day. So I, yeah, maybe that would be a good topic for, yeah. yeah, it's, a pretty simple idea.

I say it's simple because, it may not be simple to, to convey.

I maybe I'm not prepared to do that today, so maybe we could do it another time. Yeah, it's also the kind of six or, sorry, not six o'clock, 10 o'clock. It's fine. yeah. But, but yeah, like I would be happy to, if we did wanna do a presentation on that, I'd be happy to talk a bit about how, I think it could make sense to integrate into Monty, if you wanted to talk about the biology or something. yeah. But, just one last note. if we do review grid cells and, all that, maybe also, a couple of slides towards the end about what is our current theory for, I know that we, we, don't have an actual theory for that yet, but the, how to apply this to an abstract space or what do, locations in these, I had a, I had an interesting idea about abstract spaces and it's really just a half bake idea. maybe even less than half. Okay. But.

it, starts with sort of a definition of what is an abstract space. It's when you are building a model of something that you can't directly observe. So in that regard, V one or S one would never build an abstract, concept, but V two could, because V one is getting, a, an object that d from V one in, terms of the hierarchy, right? We're talking about, compositional structure. V two is building a model of a, it's model is based in a physical, has a reference frame that's updated with physical movement. So again, I'm updating the reference frame with the physical movement, but the things I'm assigning to those locations. some of that information is coming from V one, which is no longer a direct observation. It is, an abstraction of what V one thinks it's learned. V one says, I think I know what these features together mean, and maybe they represent a letter E and I'm, but then V two is getting this thing and says, I have no idea what the hell is that B one sending me? It's just some Abhi, it's just some SDR. And, but I'm gonna build models based on it. And so this, I think was the, a while back, I said, I, felt that abstraction begins very, early on in the cortex. And I think this is what I thought of at that time. I couldn't remember it at the time. It needs a lot of flushing out. But the, but it's marrying two things. One is we might have to have a, space and, or morphology of a space. That is some physical stuff. if I'm learning mathematics or physics, I might draw a picture of an atom or I might draw a timeline, or I might do this or that. So I have to have a, we talked about this, do all the, are all abstract models based on physical, some sort of physical model, let's say. They are. But the, population of the model is stuff that has absolutely nothing to do, but, it, it's, derived early from someone else. It's, it's no longer an observation of the world.

the concept of, I don't know. a, i have to come, I don't, wanna go speculate further. Just listen, just lemme just let that sit on it for a bit. That abstract, models are basically any model where the features are not directly observable in the world and they had to become from something else. And, there needs to be a lot more elaboration on this idea. But it seems pretty, it seems like an interesting way of thinking about abstraction. in some sense it comes down to what is the meaning that we assign to locations in that reference frame, right? In V one, I'm observing something. There's nothing else that being assigned to location in V one other than something that's coming from the retina. And lon. There's nothing other than something that's coming from, your skin. But, there could be some exceptions to that, but let's not go there. but as soon as I'm going up the hierarchy and I'm doing compositional structure, now a column is building models of where the features are unknown or already abstracted from the world. They're not, they may not be real or they certainly can't be observable directly. I can't observe directly. I don't have a sensorimotor of which directly recognizes roaming. I don't have a rummy feature detector. you are an abstraction of some sense based on a lot of other features and other things I'm learning. so anyway, it's just a start going on that path. We, immediately jumped to like mathematics and physics and quantum physics, how we understand these things, right? but I think we can, we start with the very basics of I described it and you might be able to work your way up to that, understanding. So that's just a food for thought. Yeah, I like that, that framing because I, I feel like usually when we talk about abstract space, we actually talk about the like, location space. So whether we can have, non 3D or higher 3D or different kind of space, right? So like actually defining it by the features is, an interesting way of looking at it. I guess just to resurface it again, I'm currently sold on the idea that everything is represented in like 3D or less space, but, one idea that, I think you presented Jeff a, a while ago was that potentially, space could be more generally framed as each mini column learns a one dimensional movement vector. And then the way you combine them, you can learn different types of spaces, and how to move through them. It's a great idea, although I'm not sure it has any validity.

it, it felt this idea just presented was going the opposite direction, like you said, it's like going, hey, space itself is not abstract. and I'm not willing to say that. and yeah. I also don't know if they need to be incompatible or I feel like abstraction could partly be about the input features and could partly be about space. You, but like the space, let's think, about the example you use all the time, is about a, a, a family democracy. Well, family. Oh, okay. relationship. one way to think about it is that the way you've, we've have, and you have presented is that there is some space that's actually not physical in any sense, but it's representing the relationships between, people and the other way of saying no, perhaps it really is just you're drawing a picture of a tree and that's what we're looking at and that's how model, and, and then somehow the abstraction comes about from the features and how they are, and how you do composition in that space. it's a fuzzy idea, but it, was it touching what I think Vin was just saying that, maybe space really is always two or 3D and And yet the abstraction comes about not in the change of the space itself, but in, in the features and how they're organized hierarchically in them anyway. it is past bedtime for you guys. Yeah. and, I gotta get going, I think it, it might be a good time to stop. I don't know. Is that all right? Yeah, no, sounds good.