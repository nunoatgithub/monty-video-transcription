All right. So this is a really high level overview. and yeah, as I said, I don't have experience with real robots, but only with simulated, robots.

for that I can reference somewhat papers or work groups that work with actual robots. But I thought, Since we don't have robots either, it's, not that applicable anyways.

but yeah, if you want me to go into more details on a specific topic, I can also do that at a later time. So this is really high level.

okay. So just as a short overview, that we're on the same page with all the terms, for sensory motor learning, there's an environment and an agent and the environment, is in a certain state, which the agent receives and then the agent decides on an action and performs this action in the environment, which leads to a new state, such that there's like a closed loop between action and perception. And then the environment can also emit rewards, or, punishments. depending how you define it, and the agent can use that to learn, to perform optimal actions. And in many applied situations, or in general in the real world, you don't have access to the true underlying state of the world. But actually just have a partial observation of the world, which may be noisy or wrong or missing information. so with your eyes, you can only see in front of you, not what's the state of the room behind you, for instance.

And this can be described as a Markov decision process, which is defined as a set of states, all possible states of the world, a set of actions, a set of all the transition probabilities of, going from state S to state S bar performing action A, and then a set of all the rewards associated to these, transitions.

And the goal of optimal control is to find a good policy pi, which is a function of, often it's a probabilistic, so it would be a probability of action A given state S. And this policy should optimize the future reward. Often people use a discounted return, which means that Immediate rewards are valued higher than, rewards set up far delayed into the future. Okay, that's the general setup. Do you have any questions yet? Yeah, one question. It seems like this formulation is already limiting what's possible. look, the field could, if like everyone in the field uses this formalism and you're already limiting the types of possible solutions and things like that, that you might do. What are you thinking of when you say that?

again, I don't know if this is So it's a Markov decision process, which means that you're only looking at the current state and you're not considering the set of states you were looking at before. Yeah. you're not, we used to call these high order sequences and stuff like that. So that's, it's just, okay. Markov decision process implies that? Yeah. Yeah. It basically says that Markov property, which means that the future is independent of the past, given the present.

And there are approaches in RL that try to relax this property since it obviously doesn't always apply. and also techniques like using recurrency or memory, try to relax this property a bit. and, there are also other formulations like POMDPs, which are Partially Observable Markov Decision Processes, and there are some extensions of this out there as well. That would be a big restriction, I know that. Just listening to it, that would be a huge dis In some sense, much of the state of the world would be hidden from you, much more of the state of the world would be hidden, because the past is some sort of a state, or your attentions, or what you did is some sort of state you can't even observe, at this point. And, that would be very limiting. Yeah. a common trick, for example, in reinforcement learning is that People give some of the past observations as part of the current state. So for example, with Atari, people often give the last four frames stacked together as the current observation, to go around this a bit. Yeah, these are like very very analogous to just sequence learning and some of the other stuff, you can give the last few states as input. But of course, in long, range temporal dependencies become hard. It just explodes in complexity as you add more and more.

time. So that is just one of my observations. One more, just a note to everyone that this is, we don't always have to think of it this way, but this is just a common way people look at it. Yeah. Yeah. Yeah. Current solutions. have broken that Markov property more often than not. And it's very, usual to use things like LSTMs to pre process this state and LSTMs will keep a memory. So it is a restriction of the mathematical framework, but I wouldn't say it's a restriction of reinforcement learning since most people are going to break that in a way or another, even if replay buffers are breaking the property.

yeah, yeah. And in general, you usually can learn long term dependencies in the value function that is learned. So you, after learning, it's not like you always have this extreme short term memory and can never really learn long range dependencies. It just takes a while to do that.

because you have to propagate the reward through the whole timeline of the value function. Yeah. One other, it's phrased as finding a good action based on the state, but I guess you could include the agent state as part of it too. What is, here is phrased just as the external state, but there's an internal state as well that you could base the action on. True. Yeah. You hear the state is coming from the environment. This is literally just what's on the screen. Yeah.

So the state could include also an internal state of the agent. Yeah. Yeah. You don't really see that in this picture.

Okay. then solving these optimal control problems, and this is a really high level overview. So If you want me to go into these details, it's like an entire textbook.

yeah, just really roughly, you can divide it into value based and policy based methods, and also at the intersection, there are methods that use both, like actor critic methods, for example. And in essence, value based methods, calculate or estimate a value function, which is, how good is it to be in the current state? it's the expected future return given the current state, under the policy of the agent. I don't know if you can see my mouse. Yeah, okay. and then they use this value function to define the policy basically.

if I'm in the current state and then I look at all the states I can reach from here, which is the state with the highest value function that I can reach from here, and then I perform this action in between, and this is then the policy. And then in policy based learning, the policy is optimized directly by parameterizing it, with parameters theta here, and then performing gradient descent on the policy parameters with respect to some, objective function, usually also some form of, the reward.

Yeah. So that's the general distinction between the two. And then within these, there are many more subclasses. So value based methods here, I just drew up on, on two, two more dimensions. So width of update and depth of update. and if you look here on the right side of width of updates, so dynamic programming and exhaustive search, those two, require a model of the environment. So let's say exhaustive search. basically means you go through all the state action transitions until you reach a terminal state in every branch, calculate how much reward is accumulated over each of these branches, and then select the highest one and perform the action that is in the, branch with the highest return in the end. And in order to be able to do this look ahead to the terminal state, you need to have a model of the world of all the transition probabilities. And this you, for example, have, can have in chess, where the, they are pretty simple rules to formalize the states of the world.

but in a lot of, real world applications, you usually don't have a model of the world, but rather learn from experience. So from actually going out and performing actions and collecting experience, and then you can't go through all the different options. You can just go through one option. dynamic programming is a weaker version of exhaustive search where you just do a one step look ahead for all the different options. And then the experience based learning methods are, for example, Monte Carlo methods, where you, where you do an experience episode until you reach a terminal state. And then you calculate the returns for all of these, states, and then update the values for all the states that you actually saw, but only the ones that you actually saw in this episode and not all the possible options that would have been there. And in TD learning, you actually update your value function after every step. So you do one step, you look, okay, what reward did I expect to get? what reward did I actually get? and then you calculate the TD error and use this to update the value function. And then in between there are different other blending methods like TD lambda, where you take more than one step but less than to the terminal state and you can also weigh these states in the value function update so that The most recent states has, have the highest influence on how the value function gets updated. What does TD stand for? I'm sorry. Temporal difference. Temporal. I, have a question. your description of value based was pretty intuitive as, how good it is to be in the current state. The policy based one seemed a little more abstract. I was trying to figure out how they differ in principle and implementation. yeah, let me get to the policy based, Oh, okay. I'm sorry. I'll put it real quick. okay, just as a brief mention real quick, when we go over to deep reinforcement learning, it can be applied to these, all these different methods, and the basic difference is that instead of using a lookup table for values for each of the possible states of the world, We now use a function approximator and in this case a deep neural network as a function approximator, which takes the state as an input and then outputs a value of the state. And now to the policy based methods, this is now also that the policy gets parameterized and this can, for example, also be a deep neural network, or some other, parameters.

and one classic example for policy based learning is reinforced by Williams. and here you basically have this, gradient of the current policy. which is the eligibility vector together with the, so, it gets divided by the current probability of the action, under the policy, which just makes sure that, the size of the update is not dependent on how often the action gets picked in general. So if it's like a really common action, you don't want it to influence. how much, you update the parameters, and then gets multiplied by the, the return that was collected. So again, here you collect some experiences in this case, and then you calculate the gradient of the policy parameters and use that directly. So there's no value function that is being estimated here, but you just calculate the discount counted return, which is GT, and which is basically the sum of all the rewards that were received. Discounted by how far it was away from the, time step t.

Does that make sense? so it sounds like the underlying metric here is, rewards rather than, value, I'm just trying to tease apart in my head the difference. I understand that you're at this optimization. And there's some control of how things are updated, but I'm still having a hard time, tearing my mind away from the notion of how good it is to be in a particular state. Is it just the horizon it's looking at or, trying to understand what a policy would look like rather than an abstraction of it. Yeah, so in this case, you don't really estimate how good it is to be in a certain state. But you basically have, this is now a function and you put in the current state and you have the parameters and it just outputs you an action. You can imagine this as a deep neural network, for example, and it has some advantages. For example, in some cases it can be much easier to estimate a policy directly rather than estimating a value function. For example, when you have the game Breakout or Pong, you, in order to know which way you want to move the paddle, it is much easier just to look at, okay, where's the ball right now, and then which is the state, and then, learn a policy that moves the paddle relative to the ball, instead of having to learn, How good is it now to be in this state? And when will I actually be game over and what is my high score right now? And then from this infer, infer the, policy. It's possible to do it, but it's more straightforward to do, to optimize, the actions directly instead of having this value function in between. And also here, you can actually learn a stochastic policy while here it is deterministic. And some, in some cases it can be important to have a stochastic policy. For example, in games like poker or, rock, paper, scissors. if you have a deterministic policy there, it can easily be exploited. It seems like you could do stochastic in the value base to the value. It doesn't tell you, you have to go to the highest value one. You could just, yeah. You can, add, noise to it, saying, okay, with probability, with probability epsilon, I will take just a random action instead of taking the best one, according to my value function. And, that is used to have also more exploration.

but it doesn't, actually. It can't actually assign probabilities to different actions. It can't be like, okay, these two actions are 50 50 and all the other actions are crap and I don't select them ever. something like that can only be modeled by this.

so back on the, policy side of things, it all, in the examples you cited, it almost sounds like you were learning rules of how to process a particular situation. But you gave the example of Pong. that there is some kind of association between, I want to move the paddle relative to where the ball is or something like that. Is that kind of where the policy is sitting?

yeah, pretty much. So it's always difficult to interpret if you're working with visual input, but for example, a lot of robotic stuff, you just get a few numbers of the angles of the joints, for example, or distance to the ground. And, it learns pretty straightforward rules when it needs to increase velocity and when decrease.

Okay. Yeah. So in some sense, it's a higher thing where it's trying to learn the operant rules of the situation rather than blindly following, not blindly, but, optimizing. saying I happen to hit here and this was good. So let me just remember it as opposed to being able to generalize what it looks like with the policy based one. There is the possibility of learning general principles as opposed to the value based one. It sounds like, it's trying to become an expert at doing something.

Yeah, in, in, both of them can generalize if you use function approximators.

deep learning, for example, or other functional approximators. But if, you use, for example, lookup tables, like these methods were, were originally developed, you can't really generalize to unseen states. Okay. Thank you.

Okay. And then the, last one I talk about is the actor critic, which has a little bit in between. both. So there's an actor and a critic, and the critic is using value based learning, so it gets the state from the environment and produces a value estimate, using parameters w for example, and then the value estimate is compared with the reward that is actually received. And, the TD error is calculated. and the critic then tries to minimize this TD error. So it tries to make as good predictions of the reward as possible. And the actor also receives the status input. it also has parameters, but it produces actions as output and uses, policy based learning to update. The policy to produce the actions, and it actually tries to maximize the TD error because it wants to produce actions that lead to states that were better than it was expected.

But in this way, it looks like the critic doesn't impact anything. Does it impact the actor at all? Or the actor uses, because the actor is not using the value function. Yeah, because of the value estimate that it produces.

So the actor needs to use actions that are better than the critic expected them to be. okay.

And for example, PPO, which was a really popular method, falls into this category and also A2C, A3C, trust region optimization, policy optimization. So a lot of the state of the art methods at the moment are in this category.

And then just to mention, there are also some other methods, especially for if you have an offline learning situation where the data is not generated by an agent, but it was already collected before. for example, by a human, or if you don't have a lot of data in general, there's imitation learning, or we like behavioral cloning, and then also inverse reinforcement learning, where you try to infer the reward function from, state and action, examples. Okay, that was the really rough overview. Are there any more questions? No, that was very clear. Thanks.

Yeah, if you like more information I've made like a little reinforcement and crash course at the beginning of this year and it's on YouTube if you want to watch anything in detail and I also have a short chapter on that and my thesis that I posted in a slack channel.

Okay, now to the challenges.

So this list is not exhaustive, but some of the bigger ones are in here. And the first paper here, how to train your robot with deep reinforcement learning. I would recommend if you want to read up a bit more on challenges with actual real life robots. And then there's one blog post. Yeah, it's not a paper, it's a blog post, but it's still pretty well cited for a blog post and it's, I think it's really well written and fun to read, and there are still discussion about it on Reddit and saying all the critiques raised are still valid in 2021. Things are incrementally getting better, but I would say that no serious breakthrough happened. Challenging fields like robotics manipulation are still far from being satisfactory.

Are these part of the blog post? No, that's a part of Reddit thread about the blog post. Just that it's still being discussed and still seems prevalent, most of the issues that are talked about in there. Vivian, who wrote the blog post? Alex Yopam?

I can share the link. Yeah, we just share the link and that in slack. Yeah, I can do that.

Okay, and then, just to start it off, one quote from Andrej Karpathy. I think he's Head of AI research at Tesla right now. Not sure. supervised learning wants to work. Even if you screw something up, you'll usually get something non random back. Reinforcement learning must be forced to work. If you screw something up or don't tune something well enough, you're exceedingly likely to get a policy that is even worse than random. And even if it's all well tuned, you'll get a bad policy 30 percent of the time just because. And I made a pretty similar experience with that, over the past years. Sounds like our experiments. Yes. I'm trying, can we go back to that? I'm just trying to parse that a bit. I'm just trying to say, why would supervised learning be like that? And why would reinforcement learning be like that? I don't, I'm surprised to read this. So I guess. I don't have an intuition why that's the case. Is there an intuition for it? I will go over some more in the challenges, but one basic problem, for example, is in reinforcement learning, you don't get the true label. So supervised learning, it tells you what would have been the correct answer. But in reinforcement learning, it doesn't say you should have performed this action. It doesn't even tell you if the action you performed was good or bad. you might just get a reward after a thousand more steps because you reach the goal but you don't really know. But isn't that just restating the difference between them as opposed to why one just makes it so much harder and you can just focus on the wrong things and not and do much worse than random reinforcement learning. I mean up front would you have expected that? I mean that I guess that's the question when I've been. I think so I think people Tech TV learning was done. I don't know, 70s or 80s by Sutton and Bartow talked about this stuff even back then. And another, I think another big challenge is that in supervised learning, at least typically the, whole data set is fixed. whereas in reinforcement learning, as you're changing yourself, you take different actions. And so the training data itself changes based on how you do it. So there's this double loop, which makes it even more complicated. So those like I don't, you don't generally hear this. This is a critique of reinforcement learning. it's reinforcement learning is all you need, that kind of stuff, right? it just jumped out at me, really? Is this how everyone thinks? Most people don't think this way. It sounds very much like how humans go on. Yeah, you can get rewarded for the wrong thing and you go down the path and continue to get that reward. But you generally don't screw up worse than random. it's Anyway, I have the impression, maybe it's just a very, uninformed impression, that reinforcement learning is through this, this, gold standard of techniques. And this says, oh no, this is terrible. Yeah, so it's yeah, reinforcement learning has also a lot of advantages and a lot of unique applications. So for example, for reinforcement learning, you don't need a huge label data set, interacting, and usually you just need to define a reward function, which may be a lot easier than labeling a big data set, for example. Yeah, but this all implies it doesn't work. So what's the advantage of something that, you know? it just says you have to work, you have to work hard at it. So someone like DeepMind can do it because they have tons of resources, but a random lab might have trouble with it. Okay. Interesting. It's a very interesting statement. Yeah. And also why there are few applications out in the world. Right now it's really, hard to deploy a system that doesn't.

I didn't catch all of that. Lucas, you said now it's really hard to deploy a reinforcement learning system. So there are a few applications. So there are a few applications. Yeah. Okay, I didn't know that. I think the nuance of the last sentence, at least for me, is that it's, less about you training a bad policy that ends up being very bad at the end. It's that the system that you train has no idea where it went wrong. I think Vivian will probably get into the credit assignment problem later. we have really no idea how to fix it. You don't know where you want to go. Okay. Yeah. Yeah. I can, I would get into a lot of these things, show some examples.

yeah, just as a short overview of some of the bigger challenges, there's a sample inefficiency and a Subutai mentioned learning from dynamically changing non IID data, reproducibility and hyper parameters. specifying a reward function, a credit assignment problem. So where did I actually go wrong? Which action actually contributed to the reward, and dealing with delayed and sparse rewards. Then the classic exploration versus exploitation dilemma. then there's a generalization and, simulate, transferring from simulation to the real world. And then in the real world, there's, of course, also safety.

And yeah, I'll just go through them. first, for sample efficiency, there's this, really classic 2015 paper. from DeepMind, that first really got DeepQ learning on the map, solving all these Atari games with, human level performance, but, human level performance in, the sense of the score, but not really in the sense of how much training is needed. So here they write, they trained for 50 million frames, which is around 38 days of game experience. while the human performance they measured was after two hours of practice, which is, quite the difference. And they also write, we did not perform a systematic grid search owing to the high computational cost, which coming from DeepMind is, yeah, I think, must be a pretty high computational cost.

It is pretty sample inefficient. Also the experiments I was running with the obstacle tower challenge would, could run for multiple weeks to, and still be learning and improving.

it, it takes a lot of samples. Here's another example from that unity posted. this is a pretty simple game. I think it's called bubble shoot or something like that. They shoot the bubbles and when you get more of a color that disappears something, but still it. It takes over 60 hours to solve on once with one simulation and if you use more parallel simulations, it gets less time, but it's still quite a long time for such a simple game.

and. This, this long training time is for one due to the number of samples that are needed and the rewards can be very uninformative. So it takes a while until, the rewards are actually helpful for learning. If you only get one at the end of a whole episode, whether you reach the goal or not, for example, there are weak inductive biases, which is the same for all deep neural networks. And then also you have to take into consideration that you need to simulate all of this to collect experiences. So you don't have a huge data set already that you can just throw onto a GPU, in a big batch, but you have to collect these experiences one step at a time, which yeah, take takes time and, is often not optimized for GPUs. Is it thought that these inefficiencies might be related to catastrophic forgetting, or is that a separate issue?

I wouldn't say so, because usually the general setup doesn't change that much that it would lead to catastrophic forgetting.

So with, with our paper, we're doing multi task reinforcement learning. So this is like learning one task and then imagine doing this for learning two very different tasks. And there are all these problems that compound there because each task is telling it to do something completely different. And that's where you get this interference and catastrophic. Yeah.

So there are a few solutions proposed to this. And, some things work pretty well here. For example, reinforcement learning, fast and slow, they propose, two solutions, episodic memory and meta learning. So episodic memory basically means using memory of previous experiences to solve the current problem. So looking back, how did I solve the problem in the past and then using this information. and then meta learning, it's basically learning how to learn. so you learn how to learn in order to be able to solve each individual task faster, in, the future, which, I guess partially is the reason why humans can learn these games so fast. Compared to the AI plus, model based reinforcement learning. So humans already have a model of the world that they can utilize. We already know how balls move, how physics work, while the network has no idea about this. and if you do model based reinforcement learning in general, you don't need to collect experiences. You can just use your model and think about what are the options. And compare them. Isn't that a funny way to put it? the way I would view it is like, Hey, we, brains use model based everything, right? We build models, we solve every problem. So it's calling it model based reinforcement learning is weird. It's it seems like you can just go all the way to model based learning.

yeah. So, there is just a difference whether you actually have to learn the model or if you're given the model. So for example, in the case of chess, you are given the model, you have the rules, you just need to evolve. chess is such a weird example, right?

it's such a constrained system. in general, some problems where you're trying to move things and navigator, solve a puzzle of something. It seems like model based learning is the way to go with model based learning, not just model based biases. It's just interesting that you, that, that, seems like the answer to most, how humans do all these things, build models of the world and how they work and reapply them. Yeah, And, some of the more recent breakthroughs have been by, actually learning models of the world and utilizing them in the reinforcement learning setup and that actually does have some results already showing that it's very useful. Is it wrong to me to think like all of our work is basically all about model based modeling? Yeah, it's a it's about learning models and then you would basically have to add some kind of reinforcement learning to use this model then to decide on actions. Yeah.

Some would argue, and I agree that if you have a large enough replay buffer, that's very akin to having a model. You're just storing that model in the form of samples. And then when you want to know a particular transition, you're just going to get the sample closest to it. Yeah, that's like the episodic memory. That's pretty large info between like memory and model base. If you have a large enough memory, then you just have a model. Yeah, just remember everything, right? That's kind of brute force and not very efficient either, right? Yeah, in that case, it would be also more difficult to generalize if you just remember. It would be harder to generalize, but if you can have it in a way that generalizes, like you can, for example, have a Cluster based retrieval approach. And then you just get whatever is closest to your experience. So I think there is a very blurry line and I would never call DQN a model free. method, I think it's in between.

Yeah, I guess it's just that it, in those terms, it distinguishes that it doesn't take, it doesn't learn the transition probabilities of the world. And then when it needs to decide an action, it doesn't look ahead, multiple transitions. in order to make this decision.

in a sense, it has a model of the world in the, weights, but it doesn't have this explicit transition model of the world.

Does that make sense? Yeah. Not that you mentioned that. Do we have, are there models that use the replay buffer to do that, to roll out?

You, mean you would have to have a pretty big replay buffer then, yeah, I'm just wondering if someone went that path.

Yeah, I'm not sure. Okay. I think about one of these game based things like, Breakout, right?

what am I learning when I'm learning how to play Breakout? I can learn to play the game in just like a, 10 seconds. But I'm not fast enough or I'm, someone explains the rules of the game at the plate. If the game was very slow. I think I can play a perfect game right away. I went up to practice. It's understanding how to solve that game seems trivial to me. what I'm really learning is how to do it quickly and it's because it's running at a rate that I cannot do. I can't think too much about. I have to practice and practice to get faster. But the problem solving itself is instant. I position the paddle under the ball before it hits the bottom. I'm done. And, so I think we have to be careful here, too, about mixing up what we're learning, right? We are not learning how to play the game when I have to practice two hours. I'm learning how to get good at, how to do it fast enough, which is a different type of learning, it's dealing with the slow, the slowness of brains and how we can get around that. Yeah, that's a really good point. It's not fair to compare the deep learning Atari versus the human Atari because it's just humans. It's about speed in the computer. The speed is not a factor. Yeah. And the whole thing stops until the network makes a decision and then you get the next state of the game. Yeah. But again, really think about it. I don't have to practice to learn to play that game. It's, once I have to know what the game is, so I'm gonna tell you, but that's it. You already have a pretty good model of how trajectories of balls work. Exactly, that's my point. the model is already in my head about, okay, a ball's gonna hit something, it's gonna bounce. all done already. And when he hits the bricks, surely blow up or do whatever they do. but really, it's there's really the only learning is just getting fast. And of course, they designed the game to get the limits of human ability in terms of speed. that's the whole trick that you're trying to get faster and faster at and get more, But actually, it's not a problem solving game. It's, just a speed reaction game.

There's a lot of games like that, right? You can, that, that's what you're really learning how to do. But the actual, no, there are other games that are really, you're solving problems or you don't know how to solve them, what chance. That's the chess one that has a star agenda. Yeah, chess is more complex. Yeah. Tetris kind of sits in between. you can position the things, but they don't do it well enough for you. This is the thing I've always had, obviously reinforcement is a key part of learning. I'm not going to deny that. In humans, but so many of the things that I do, you might think of robotic actions. don't really require this sort of strong reinforcement signal. learning how to pick up an object and how to manipulate something or use a safe. it's not gamified in any sense like that. I didn't just have to learn these tools because I've always felt that, relying on reinforcement learning as the key, the goal of the key learning strategy for practically everything seems really off base. Thank you. It's no, I need it. To me, I've always felt like it's all about model based learning. You need to learn to model the world. Once you have a model of the world, you can solve any problem if your model is good enough. I'm not, that's not to deny that there are reinforcements based on, that reward doesn't play a role. But I always felt that the ratio between reward learning and model based learning has always been skewed incorrectly. and the brains are basically all about learning models. I'm just saying the same thing again. Yeah, that's why I find like these curiosity based approaches so interesting too, that you basically learn in general. Model of the world, and then ideally are able to use it quickly to perform different kind of tasks. And I think it's a really promising research directions to try and learn very good models and then just utilize them with very few reward signals or none at all to perform. if we're going to look what the, what the meant is going to do uniquely, we have to focus on that because that's our strength, right? Model based learning. Definitely. so we need to, we have to be careful looking at robotics challenges and tease out ones that really are suited for the techniques we have, over the model based learning and not suited for the ones that require speed or, reward functions all the time, things like that. so that's the spin I'm going to be putting on all this stuff. Yeah. Yeah, I would agree. Okay. And sample efficiency. Just, it gets to that, right? If you have a great model, you don't need many more, you don't need any more samples. You've already lost a lot. Yeah, and they talk, they reference this book, Thinking Fast and Slow, I think, and Like that there's fast and slow learning and basically learning the model would be the slow learning and then the fast learning would be just taking the model and applying it to a task and solving it pretty much immediately. Yeah, that's why I didn't like the basic thesis of that book because it was making this distinction which I felt was in most cases unnecessary. where, oh, there's two separate worlds we live in. No, we have a model of the world and that takes, as you just said, it takes a while to learn, but then you can act quickly based on the model. It wasn't like, oh, there's two different worlds we live in and two ways of solving problems.

Yeah. Okay. So let me go on to the next challenge. So learning from dynamically changing non IID data. so generally when training deep neural networks. In theory, we always assume independent identically distributed data, which means if I pick a sample now, and then I pick the next sample randomly, it's independent of what the previous sample was. And that's clearly not the case when you're acting in a continuous world. and This kind of violates this basic assumption of deep neural networks, which adds an extra complication when you're using deep reinforcement learning and can lead to unstable training. And then also, as Subutai already mentioned, the quality of the policy determines the quality of the data that is being collected. So if you once have a bad policy, it's really difficult to recover from it because you don't get any good quality input data anymore. That you can really learn from. so you can really get, get stuck in a bad policy. And, one solution is commonly used for the, non IID problem is experience replay that you basically have a policy, use it to collect a bunch of experiences. and then you take random. batches of samples out of this, buffer and, replay them. They use replay from, this. You're saying you actually, don't play, replay in order. You take a random sampling of the episodes and mix them up. Did I hear that right? Yeah. Yes. Each episode might have its own order. You might have the last thousand games stored, but each game has a sequence to it. So you would pick a random game. Oh, you play the game in sequence, but not I think so, yeah. Okay. Sorry, go ahead. You calculate the discounted rewards in sequence, and then you have the reward, the discounted reward associated with this experience sample, but then you can mix it up and it doesn't matter anymore because of the Oh, I see. I see.

yeah. And this is one of the key tricks that actually got this, 2015, Q learning paper off the ground. The method, as far as I know, was already there for a while, but people couldn't get it to really have good performance in combination with deep neural networks. And then using this trick was one of the keys, they But, still even with that, it's, difficult and still you can get stuck in a bad policy, but we live in a dynamically changing world and we can learn. So there must be a way.

Okay. Then there's reproducibility and hyper parameters, which is related to the previous issues.

little changes, in the high performers can have a large effect on the performance, and also because of the. sample inefficiency. It can take a lot of computation to actually tune the hyperparameters. And then even just a little difference in, in the random seeds have very large variants. So here, on the left, this is actually from this blog post where he ran 10 runs of the same parameters. Everything is same except for the random seed. And three of them never got off the ground. And the others, still have some variance in here, and this is from a 2019 paper comparing some different, state of the art algorithms, and you can see some of them have a huge variance, and if you imagine trying to tune hyperparameters with this, it's really difficult to do because If you have the same hyperparameters in two runs, one is at this performance and the other one is here, it's, you, it's really hard to really say which parameter was actually better without running multiple runs of the same parameters and this takes a lot of computation so these problems compile on top of each other. is this kind of sensitivity to initial conditions? also result in sort of a susceptibility to what do we call, random, small perturbations in the end result, adversarial attacks. Yeah, adversarial attacks. That's what I was looking for. Or is it not, once you get a good model here, is it good or is it, or, is this sort of susceptibility continue on somehow? Inherently in the system, everything is susceptible to adversarial attacks. brains aren't really susceptible, not in the same way. definitely deep learning networks, no matter how good they are, they can be susceptible to adversarial. But I guess I knew that was true with deep learning networks. I didn't know if that was true for reinforcement learning. But they're using deep learning. I, just didn't know if that somehow solved that problem. the brain is susceptible in some senses, though. Would you not argue, we were looking at a three dimensional shape in a whiteboard, right? Yeah. That's in some senses, a matter of serial attack, right? Perceptual attack. Perceptual attack. I don't think it's, look, it's, not Wouldn't call it. No, I wouldn't say that at all. It's hard to see it as flat though, right? yeah. No, that's almost, that's actually the beauty of the whole thing, actually have a tunnel. first of all, I think that's an a, a plus, not a minus end. All right. And second of all, by putting it on the board, I don't see it as, a school bus. it's that's true. Yeah, yeah. it's not the kind of stuff you see in there. It seems like the solution here is clearly just to pick the best random seed and only publish with that. Yeah, there's a startup idea right there just to choose the best random seeds and sell them to people. I call them magic seeds. Magic seeds, exactly. We've heard the story before about the magic seeds, right? That's basically what the world of finance is. Oh, this is very fascinating. Okay. Let's, let's keep going. And just a remark, we've been seeing this in our experiments now that seed only works, when they say see there, if you see the whole thing, but even if you set the initial conditions, like you have this exact same model, exact same weight. The problem is such that with this interdependence between the data generation and the modeling that even just after a few steps, they're going to be widely different. So we're still going to have that variation, even if you set the initial conditions to be perfect, to be exactly the same. Yeah. I think there are like three seats you need to set the seed of the environment, the seed of the. network weights and one other one. I don't remember exactly. Yeah, because there's a lot of stochasticity. So you got to sit, you got to set the seat for all same thing you're going to do. Then if you're running on GPUs, you, there's inherent variability in the GPUs that, that you can't, you just cannot set. Unless you set it to some deterministic mode, and then things are slow. There's always some injuncture. This again seems very much related to the issue of model based learning versus non model based learning. Because model based learning, it seems inherently gets around many of these problems.

it's just that you're, obviously your model is pretty good, but your model gets good over time, and I just don't think, I just, I don't know how to describe it, but mentally, I see that the model based learning solves these problems. because you're assuming it's limiting, model based learning is limiting in the sense it says the world has to be like your model and the world is different than your model, they don't work at all. But if the world is like your model, then they work really well. We're here, without model based learning, you can learn anything, but it's very fragile. It seems like inherently that's the case. Yeah, I think, if you just talk about model based learning for reinforcement learning, it's the case you have less variance, but if you actually need to learn the model from experience as well, I think you still have this problem because you still need to Somehow sample this experience with some kind of I disagree think about all the work we've been talking about recently, where, we don't, the model isn't just learn from experience the model there's an inherent structure to the modeling system of the brain that makes assumptions about the world. And you're not learning that. It's like the brain has, it already has built in an assumption what the models of the world are going to look like. The structure of it, the, dimensionality of it, how movement relates, how information is stored. So the brain basically says, the world's going to be like, it's got to fit into this model. I'm not learning this model from scratch. I'm just filling in the details. Yeah, that's why I think there's a big difference in learning a model. Learning a model, yeah, you're gonna, you're gonna go through the same process here and you're gonna have the same kind of crazy that's gonna happen. Yeah, I think if you add more inductive bias to the model and don't just have a neural network with random weights in the beginning, I think it's, solve some of the problem. I guess you would still have to learn a policy which you need to start at some place with. And of course, And they show also the policy. What is it? Given our recent research on, we've been talking about how brains model things in the whole recent discussion. Where does policy fit into that? What is the policy? I actually don't get a sense for policy, what that means. It's just choosing the motor command. And so we don't really talk about that. We haven't done that yet. We haven't done that. How do you, what motor command do you do next? That's policy. That's what policy means. See, this is, I want to get to. This is why I asked you to do this presentation. I want to understand how to bring in motor behavior to all the recent work we've been doing on, modeling. And, and I want to understand what problems we should be trying to solve by introducing policy. So if our work doesn't have policy yet, like what should what problems should we be solving? This is an uber question or not a question based on why we're talking about this what policy what problems should we be solving? Therefore we can define the policies that You know, we have to judge the performance of the system or introduce, introduce behavior into our models, because we have a struggle. You, you said yourself, and we talked about the how all the models are in the brain. You said all the information is already there to solve robotics problems. so we just need to know what kind of problems to solve. I'm just speaking a lot. So Vivian, I want you to respond to Jeff, but I want to get in the queue for a follow up question. That's really good. You don't need to respond to me. I'm done. Go ahead, Ben. Okay. so a few weeks ago I mentioned, Judea Pearl has this three tier notion of different levels of reasoning. And I think they were associational, intervention and causal. And he criticized machine learning as a field by saying other than reinforcement learning, it's pretty much stuck in the associational phase. And so reinforcement learning is interesting because it gets you into the intervention phase where you. Can choose what data you get next and all that.

what problems can you solve that you can't solve with just machine learning? one obvious direction would be trying to learn causal relationships. is there work on that in reinforcement learning? I know this is a discussion that's out there, but I don't know what's happening in reinforcement learning that hits on this issue. And then would we, wouldn't the causal relationships be basically model based?

isn't that the same thing in some sense, like to know, have a model of the world and say, what leads to what, what causes what aren't they almost the same thing? Model based or am I wrong? Yeah. Yeah. if, you have a model, you have the transition probabilities and then, what causes what, and it's not just, correlations between experiences and actions.

I think the difficult question would be how to get this model if it's not already given. And we know from children that they perform very directed, scientific experiments, young children already trying to figure out what causes what, how do things work, and This kind of behavior is not, as far as I know, not really seen in reinforcement learning agents yet. Even the ones with curiosity or similar things just seek out novelty but don't perform really directed experiments. And I, know there are some attempts to get this more ingrained in these reinforcement learning agents. For example, there's one environment that was proposed a few months ago called alchemy, I think. which is the agent can mix different rocks or chemical compounds and kind of experiment with what mixes with what, and is supposed to figure out these regularities and causal effects. But I'm not sure if they are like really good solutions yet.

Can I add something, Vivian? that caters to Ben and Jeff's question. I think model based reinforcement learning have been proposed for a while. I think the first one is like 89. 90 in Dyna, but it never really worked well because at the beginning of training, it all depends on the quality of the model you have. If, you're not relying on your experience, but you're relying on a model instead, that introduces a huge variance in your system when you do this rollout into the future. And at the beginning of training, your model is really, bad. So your rollouts are going to be catastrophic. So you get stuck and you can't like move away from this initial phase. And I think 20 years, model based RL never worked well because of that. Still doesn't work that well. But I think the difference here that, I think in Jeff's point of view is that for humans or in our model, we are introducing this very heavy, inductive biases, your model. So we already assume that, the model works like that, and that's it. So if you have that good model from the start, and it might be able to do that, but. Otherwise, if you're just learning from experience, that becomes really hard to do, at least in this framework. And that would be for the causal reasoning aspect. Yeah. I think the other thing is if you're doing like Atari games, you're not like learning, you already have, you've learned throughout your lifetime about how balls bounce, how things hit each other and how they move and all that stuff. And so you're able to immediately transfer that in experience into this new environment, which is totally different. so there's this, you've learned a lot of stuff that's relevant, even though it's not exactly this particular set of experiences, then you can transfer this knowledge very quickly. And this transfer, I think is, this ability to transfer knowledge so fast from one setup to another, I think Is a huge aspect of a lot of the stuff we're talking about, and I'm taking it one step further saying that the brain is designed as under the assumption that there are physical objects in the world and they have presence and they move in certain ways and it's three dimensional or whatever. And, and, so it's just assumes the world has a whole bunch of constraints on it already. even though you have to learn it, you have to learn the details, but, all these mechanisms. How much of this is predetermined? I'm saying the, what's predetermined is, we're talking about all the things we've been talking about, right? Constraints, transforms, and all this stuff is predetermined. So there's an assumption in the brain that you have this body, it's articulated, it's got sensors on it, it's got to learn the structure, physical structure of objects and how they behave. Those objects are going to be contiguous in space. They're going to have certain ability to move in certain ways, not other ways. All that's predetermined. You just don't know all the details of what objects are going to occur.

that's my point is that I guess we, that's, I guess in the language of machine learning, that's inductive bias. I guess that's the term. so we have a lot of those. you can't, this is why we have trouble to play something like Go. you go, you look at the board, you have no, you can't really see the structure of a game. It's really hard to, you have to spend years trying to figure out how to interpret this in some sort of way. way that you can look at a board and say, Oh, I know, otherwise there's a bunch of dots on the square.

And I was going to mention something to talk about games, but that plays a role here as well, because games are designed for our indictive bias, right? They're designed for the humans to be able to play. And that's why we're good at them, but we could easily design a game. That, doesn't attend to our inductive bias that a machine could solve it a lot easier than humans. I was arguing that Go is like an example of that. I was arguing that Go is an example of a game that actually our inductive biases don't work well. You just, it's like looking at, Greek letters or, some other language. It's I look at a go board, I can't see the structure of a go board, because someone's been playing it for years, has learned a new way of thinking, it takes a long time, where a game like Breakout is oh yeah, there's a ball, there's a paddle, I know that, and bingo.

and that, that's why I think deep learning networks have, the places where they've excelled, generally are places where humans have difficulty, even though we see the protein folding stuff, it's, we can't look at that and understand it. but with enough data, people can, I think we've beaten this up. And the AlphaZero that is super successful with Go and Chess and all, uses model based learning. They use Monte Carlo Tree Search. So it's just the difficulty is how to do it when the model is not given to you, when you have to learn the model.

And how do they do it in AlphaZero? I remember the first AlphaGo that they learned via human policy. it's not exactly reinforcement learning, but. They were relying on replayed games. How did they do it? Now, how did they start with a good model to work?

they have all the transition probabilities because of the rules of the game, of course, other player, the other player that you. don't know what it will do.

I just know the general Monte Carlo tree search. I don't know the details of the paper implementation, but you basically simulate possible future trajectories, their probabilities and their, possible future rewards and then pick the best, with some probability.

Okay, we should keep going and we have about 15 minutes left, Yeah, I think the other points are also not that long and more fun because they are pictures.

getting stuck in a local optimum will also bring us. reward function. So for instance, here's, the half cheetah it's called. It's supposed to learn how to run as fast as possible. and this is the half cheetah that found a strange local optimum flipping itself on the backside.

Yeah, as you can imagine, it will never learn how to run because it will, once it's on the backside, it's not going to figure out, it's actually better to not flip on the backside and it will just figure out how to go faster.

I think that encapsulates everything that's wrong with machine learning. There are some really funny YouTube videos with just reinforcement learning gone wrong.

Yeah, another popular example. Just in general specifying the reward function can be really difficult. It's like this Greek myth of King Midas. He had a wish with the god of wine, Dionysus, and he said, I wish that everything I touch turns to gold. And I don't know if you know the story. But, of course the god takes him very literal and now everything he touches turns to gold, even his wife and his food and everything he can't eat. His family is gold statues and he's very unhappy. and reinforcement learning agents are the same. They take very literally what you specify as the reward function, That can lead to also very unintended behaviors. For example, this boat is supposed to run on the racetrack, but someone thought it's also good for this boat to pick up these turbo packs. So it should get a little reward for that, but this boat actually figured out it will just run in loops forever, waiting for the turbo packs to respawn and pick them up again and get the most reward By doing that, So the reinforcement earning agent will exploit any flaw in the environment or the reward function if it can. And, we actually encountered this also with the robot arm, where I thought it may be good to reward the agent for touching objects because. to encourage it to interact more with the objects on the table, which just led to the arm going immediately down on the table and pressing down as hard as possible on the touch sensors.

So So, yeah, you might have very good intentions, but yeah, almost always leads to unintended consequences. So the reward function strongly influences what is learned, and it's difficult to state what you actually want.

Yeah, then also. If you only specify the final goal, it may actually never be learned. If the final goal is something difficult, that is not happening randomly ever, because then it will just never happen and the agent has nothing to learn from. And for that, the method that's also used for classical conditioning, for example, in animals called reward shaping can be used where you first reward approximations of the behavior, and then, more and more the actual behavior. And then other, also other, interesting directions, in the field of reward function is multitask learning and open ended learning and curiosity.

Okay, then the credit assignment, we also already roughly talked about. So how do you know which actions cost a delayed reward? also many tasks have a sparse reward structure, so it's difficult to learn. long action sequences, may take a really long time. And current solutions, as far as I know, are discounted rewards. So you give more credit to the actions that were performed shortly before a reward arrived than the ones that were performed a long time ago. And eligibility traces, which is, yeah, detail didn't go into, of, for example, TD lambda learning, also weighing states that are close to the current state more than the ones that are further in the past. Can I add hierarchical RL to that list? Oh, yeah. That's also a good point. Yeah.

Yeah. And then of course, also, if you have causal models of the world, that also helps a lot because we know that it's not only temporal co occurrence for the credit assignment. I can do something right now and know that it will be good for tomorrow because I have a causal model of the world. So that's what jumped out at me. It's we solve problems not by trying different things and seeing what works. We generally solve them by having a model and say, the model says I can do it this way. and we're actually not very good at seeing solutions that don't. Don't fit our model. someone can come along and show you another way of doing a thing, another way of using a tool that you didn't know, and all of a sudden, oh, that's better, but you almost never figure that out on your own, but, having a model tells you, you can know how to solve the problem right away. You can say, give me my model, and someone can solve that problem. Yeah. Yeah. You'd expect him to know that abbreviation. Yeah. And effectively a counter example is music, for example. So sometimes you can't, no matter how good of a model you have, you can't learn how to play piano just by thinking it through and then go and playing. You got to practice a lot. And that, that again goes a little bit back to what I was saying earlier about, learning to play breakout well, right? you can learn to read the notes and play them out of piano really slowly. but playing it at tempo with the right nuances is what takes years, right? It's not hard to learn how to, play piano slowly and poorly. Believe me, I'm trying, it is. I can play slowly and poorly and I'm not, I don't consider myself. I'm almost done. So just real quick, the exploration was exploitation. I just thought this quote was pretty funny. As I said, the problem is a classic one. It was formulated during the war and efforts to solve it. So accept the energies and minds of allied analysts that the suggestion was made that the problem be dropped over Germany as the ultimate instrument of intellectual sabotage.

So basically the question of whether you should explore more unknown options and figure out if there are higher rewards out there somewhere or just exploit what you already know. And it's a still an open problem. current approaches are using a stochastic policy. in addition, you can also lower the stochasticity over time. So having a lot of stochasticity in the beginning to explore a lot, like for example, in childhood, in humans, and then over time, decrease the stochasticity and go over more to exploitative behavior.

and then also there are other things like curiosity objectives that encourage a bit more directed exploration, in areas that are still unknown as opposed to just random exploration.

generalization and sim to real transfer. So basically reinforcement learning agents. often over fit to strange features of the environment. So one example is that some, people showed that, on Atari games, sometimes the agent basically just learns to memorize an app optimal sequence and kind of ties this memorized sequence to the game score. So just. focuses on the high score and then knows, okay, if it's at this number, I will do this. And if it's at that number, I will do that. Which means when you cover the high score, it just doesn't do well at all anymore. That's hilarious. Yeah. So just like a choreography and for a human, it doesn't matter if it sees a high score or not. Yeah, and one solution is, sorry, in real world robotics, there are other complications like wear and tear of the effectors. Then there's latency, and also less precision than in the simulations. And as solutions you can train on a wider variety of inputs, add more randomness so you can't just overfit, and also in real world robotics you can add latency models or, things like recurrency or memory.

And then safety, pretty obvious. If you have a random policy, it can be pretty dangerous. It can break the robot if it just does random jerky movements. So you can restrict the action space, regularize behaviors, or learn to recognize unsafe actions.

And just as a final slide, Are there too many challenges? Is reinforcement learning broken? I don't know, I trained this obstacle tower network with 2, 500 neurons, and it learned a pretty complex navigation and vision task. with just such a really small network, while, yeah, even cats and dogs will have a lot more neurons still, as we probably all know, sometimes do really weird behaviors, and act non optimally.

I, guess we, we can't always expect optimal behavior anyways, but, Yeah, and, I wanted to do also some overview of, what is actually out there and what can be done at the moment, but I thought that would make it a bit too long for one talk. I guess the one thing I would, I, this was great, by the way, Vivian, but the thing that I still need and want to know is what, and from a practical point of view, maybe a commercial point of view, What are problems that are important and unsolved? these are, you went through the whole machine learning theoretical perspective, which is very useful. but, that other question is also important. what, these are, most of the situations are things that aren't really practically valuable. For robotics more than for robotics. Yes, you know what things that people would like to do in the commercial world of robotics or practical applications that are just not doable today. Yeah, for that we'd have to get someone who's more like in the industry or something. Yes. So what I wrote as the ambitious goal proposals, I think, like a home care robot because there it would be really useful to have a model of the world because you need to pick up new skills many different new skills very quickly and it's let's break that down you know what are the primitives of a home care robot has to do I think we shouldn't, from our point of view, my point of view, I don't think we should focus on, oh, understanding the disease of the person or what their needs are, more what does physically that robot have to do? Does it have trouble navigating? Does it have to pick things up? Does it have to dispense medicines? Does it have to unscrew caps? what are the things that, Retrieve stuff from the refrigerator, practically those, low level things that you hardly see robots do much very well at all. or the, where I'm coming from, like what are the Maybe even things like, Oh, the person I'm caring for is acting weirdly or it's acting. Yeah, there's someone else I know who acted similarly ended up with the fall. So maybe I should prevent this, but that's not really robotics. Yeah, I'm thinking like, that's a good task too. But right now I'm thinking like, what are the robotics challenges they have? what are the, but those would be a lot of, stuff, a lot of the things you mentioned would be like subcortical things, right? Maybe. I think unscrew things, maybe not. when you think about your cortex, has to learn what a screw top is and how bottles open, different types of bottles open and so on. Maybe that's assisted by subcortical networks to make smooth actions and prevent you from using too much force. things like that. But those are cognitive functions, you have to learn how to, open a refrigerator door and how to take lids off of things and how to pour liquids out and so on and so forth. So those all require a model of the world. and to be able to manipulate the world. where I'm coming from, our latest work here suggests to me that we'll have the, we might have the ability to build very articulate robotic systems that physically manipulate things, because we're building models, very articulate, we're building very detailed models. Showing how we can learn very detailed models of objects in the world, and their articulations of those objects, and so how would we, what would be a robotic task related to the, just like building a better robotic arm, picking things up and manipulating and grabbing and turning things, and I don't know what those are. Oh, we've already solved those problems, or, they use a vacuum, or whatever, I don't know. taking your example, the simple, the straightforward, I'll say that. The specific task of transferring a invalid from a bed into a wheelchair is, requires just that. requires it, be able to perceive the position the person's in, where they will get the position, transfer away, how do you do that? That's a good example. That's a good example. That's, also a very, a pretty, a complex version of the kind of example I'm looking for.

you're starting, we're starting to talk about dealing with a human that's frail. You don't want to start there. and also one that requires multiple arms, multiple people to do, that's a tough, let's pick something like that. It's more, what, can one hand, what robotic arm do that's not going to kill somebody. I mentioned the Japanese are investing a huge amount. to try to have robotic eldler assistance. Yeah, I know. and Vivian suggested that too here, but, We'd be cracking an egg. These are all good. I guess I'm trying to get it more, that's a good example too. I'm trying to get it more primitives that, we don't want to make, create an egg cracking system, nor do we want to create a human lifting system. We want to understand what would be the basic primitives, functional primitives that require to solve a set of problems. Thanks. cracking an egg is, it could be an example of many different manipulations, right? taking the lid off a jar, to me, is no different than cracking an egg in some sense. It's, picking up an object with the right metaphors, getting the right orientation, manipulating in some way very precisely, Observing the results of that, to know whether you've achieved it or not, that kind of stuff. I think you did say, that it's a sufficient source to sufficiently hold on to it, but not so much that it would actually damage it. what I don't know, is that really a valuable problem in the world today, commercially? We can say here, imagine it's a hard problem, but is it a viable, I assume that's right. You have to talk to someone who's in the field of robotics to talk about, trying to sell these systems or build practical systems. So we can identify things like that, but I've learned that you just, until you talk to someone who's actually been in the field and you're dealing with these problems, you don't really know what they want. so at some point, we'll have to get that kind of information, because we could go down a long path solving a problem in in some sense that's already been solved. That's not the real problem. Most of the robots, are like things like bolts and stuff like that. It doesn't matter if you put a slight scratch on it, but anything else that's more fragile, you'd be able to generally pick up an object without damaging it by scratching or anything else, I think. But then you have to do what with it? You have to get in some position, you have to screw it in, you have to manipulate it in a certain way. just the act of grasping it. So these are all good examples. I think we need to get some people who know the field to do that. I mean like the thing about grasping it, that was famously solved in the Amazon challenge where Once they identify the object they wanted, they just used the vacuum and sucked it and picked it up. And it didn't damage the object, it worked, and that was the solution, right? so we have to be careful.

Alright, can I give my opinion on this? I think the issue is not solving a problem. It's actually very easy for the systems, given enough, simulations to solve something. if you do a good reward function, and if you just give it enough time, it can solve anything at all. Because it's just going to exhaustively search the space of solutions. but again, the problem is you have to deal with variances in the world, and, transfer learning, right? yeah, I'm going to get to that point. it's not hard to solve one thing. It's hard, but because the thing is, you find a very specific solution to a very specific problem if you do RL based. So it's not hard to solve one thing. It's hard to make a robot that solves many things. And I think that's probably the main challenge we're having in applying reinforcement learning to robotics. And I think what would be a big deal is solving that, solving a way of either. We can call it continual learning, a robot learning things in a sequence, or I'd even call it just transfer learning, maybe learning a good enough, like model of the world that could apply to different problems. And you could just like plug and play, you plug that part of the world and it can easily extend to some other activity. I think those Let's take an example. Let's say you, want to learn how to pick up and manipulate certain set of objects. And so you build a model based system, and what you really want to do then is say, here's a new object you have, I'm going to add it to your list, here's some good views of it, go for it. so that's appealing, that's the kind of thing that we've been talking about that we could do, I think. The, but then the question that really gets down to, my next question is, I really want to know what's commercially valuable. I'm trying to get to that point where it's like, what would someone say, oh my god, I need that in my business, all right, that's going to solve this problem in this commercial field. and we're a bunch of researchers here, so we're not going to know this. Like Google, for example, they just added a new feature to avoid Christmas trees.

That's, something that you just added, you just upgraded the model, that's a very, Google is not a very smart system. it's fun, but it's really quick to do.

No, I, again, it's just really interesting. I, it's interesting to think about. we'll have to get, this is going to have to be, this conversation's going to have to continue.