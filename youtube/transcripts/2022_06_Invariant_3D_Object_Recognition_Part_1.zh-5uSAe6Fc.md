Great, yeah, so thanks everyone. I'm going to be talking today about ideas around 3D object recognition, and its relation to grid cells and temporal memory, as well as, work with a specific focus on rotation and variance.

and in general, this is going to be high level ideas. There's no model yet implemented, but hopefully a direction that, we'll be able to solve a couple, outstanding problems. So yeah, general motivation, address, rotation invariance, and, particularly with ideas from temporal memory. and that's what we want to do, but it's unclear what the kind of appropriate space that grid cells should be. or even biologically can be modeling on. So the approach that I'm going to discuss is one of several that could be taken on. It's complimentary to other views or other approaches that could be used. But I think it has some nice computational properties. It would be a relatively simple kind of starting point to get something actually working. and also make some connections between, what I'm discussing in kind of biology, both at the neural and also the kind of psychological level, which will hopefully motivate this approach. and yeah, in general, the idea is that this should, whatever approach is being taken should, maintain desirable computational properties like path integration, and the kind of view I'm taking with, rotation invariance is that it should work as well as humans because, as I'll discuss later, humans aren't actually, perfectly rotation invariant, and that may represent, a trade off, which is actually a good idea. Because, yeah, in general, invariance can also come at a loss of sensitivity and so forth, but I'll get into that later. So the kind of concepts to discuss are, what can be called view spheres, and how this relates to 3D objects, recognition, and representation. And then I'll, yeah, talk about what they are, why they're interesting, possible connections to biology. And then I'll relate this to other complementary views to how to represent 3D objects. Space and three object recognition with grid cells. And then I'll look more specifically at how this approach can be used for rotation and variance. And some of the ideas that I'll discuss under rotation and variance, can be applicable, I think to the other approaches in the Monty project, and so they may be more generally useful, even if this isn't the ultimate approach that we take. and similarly, then I'll finish talking about, getting robust and, Patchwise representations that can generally generalize well, and again, this could be useful more generally for the Monty project and obviously feel free to interrupt me at any point. Something isn't clear. I'm hoping I'll get a chance to go through everything. There is quite a lot. But, yeah, we'll just see how it goes. yeah, the general idea with these fears, if anyone for anyone who saw my journal club presentation the other week, this will be familiar. But the general idea is you represent objects. Thanks. With a general sphere that essentially describes if you imagine your eye and then the object rotating, the eye is the surface of this sphere, and you're encoding the relations that are hitting your eye as rotation takes place. So something like a teapot, you'd encode with this lid at the top here, the spout, the handle, and then a flat surface at the bottom. And here are just some other examples. For example, a toaster. The lever and the dial or a car and then the kind of basic ideas. Then, you rotate the object as you're studying it. you learn where the features are relative to each other on this fear. And then, as with kind of standard, could sell path integration and temporal memory. At inference, you would then, look at the features that you're currently viewing, predict where the other features would be, and then based on the rotation that's being experienced, predict a feature and hopefully achieve, inference. And Niels, one, one quick question. Yeah. Is everything on the surface of the sphere or it can be anywhere inside the sphere? So it's, so it's represented on the, a 2D surface of the sphere. Okay, but I'll get into how depth and how the features can be obviously closer to the center and so forth how that can be represented. But in terms of the grid cell representation, that's two dimensional because it's on the surface.

Okay.

and yeah, so that was, an approach used in this Baratsky paper I talked about the other week. similar ideas have been used elsewhere in machine learning, spherical 3D CNNs, from a paper in 2018.

and, yeah. As I'll get into later. I think this has some nice, possible connections to biology. it's important to emphasize that. Yeah, it doesn't imply that the local features have to be 2D. So we're, mapping and remembering them on a 2D surface. but the features themselves may have some sort of depth representation, which I'll get a bit more into. And one kind of, nice approach to this is it may, have some, benefits for invariants, which I'll give a couple examples of. again, as well, it's worth emphasizing that this representation, doesn't need to be, exclusive to other ones. So although, yeah, 3D lattice mesh is one, we've thought about another kind of, as well as these kind of wireframe part based graphs. Another one that's been discussed is more kind of 2D planar views. And I think actually that this is also important and would be, complimentary and exist alongside these view spheres. but I think, having both of them brings, respective benefits. And in particular, view spheres are useful for this question of rotation and variance.

And just to give a bit of a flavor of this sort of, motivating, kind of idea for this, if you imagine an infant growing up, vision is quite limited, I think, to about, 8 to 12 inches for the first several months of life. In terms of, focused vision, and of course, babies tend to interact with objects at, arm's length. So you can imagine, any given objects being rotated many times, and fixated, within that, kind of distance. And might be at least one way that you build up, three dimensional representations of objects.

What's an NB? I'm sorry. Oh, so NB itself, what does that stand for? Yes. so I think it's Latin note, a penny or something. It just means bear in mind. Okay. It's not commonly used here. Okay. I will envy that. Avoid.

It's just me. Okay. Okay. Yeah, I think it might be a British thing. Yeah. So just at a high level, it's hard to know exactly, how well this will hold until it's, I think, implemented in practice. But, And my thinking this would help potentially with kind of distortions to the geometry of the object. So one thing we've discussed is what do you do once mesh of an object starts distorting slightly, but because a lot of the features are essentially just being projected to the sides of the sphere. You can make some fairly strong changes to the kind of shape of the features as long as the local feature detection is still operating. So as long as the kind of local feature detector for a handle is invariant to these changes, then the actual representations, in particular, the relations of these features relative to one another can be relatively robust on this view here. And this wouldn't necessarily hold if you're doing a representation, say, in profile.

I guess if you think of it, sorry, I'm keeping you feel I wasn't obviously what you just said, but go ahead. Okay. Yeah, saying, I guess in a hierarchy then the lid itself might be recognized in a similar way but lower levels, the hierarchy so you could have a lot of distortions of lids. and still recognize the lid and then at this level of the hierarchy, maybe you're recognizing the full object. Yeah. Yeah. I think that's definitely important. And, yeah, I guess what I'm just trying to say is, within the, sphere, you can have these, changes, to the object, but because everything's being projected outwards. I think a lot of kind of general structural relations like this feature is in front of this one will hold and this I guess gets to the next thing which is scale invariance and I'm not suggesting that this is going to be perfectly scale invariance, scale invariant, but I think in a similar way, because you're projecting features onto the sphere, the general relations will hold whereas Of course, if you had a 3D mesh, for example, the distance between, the handle and the spout is But to be very different, but in the case of the, view sphere, you'll still have them, you'll still have this 180 degree relation basically between the spout and the handle. Yeah. And that's essentially what I'm, sorry. I've heard it before that the scale invariance from a neuroscience point of view seems pretty easy to do. So I don't think, I'm not sure I need a new sphere for that, but the I'm not convinced. I don't, understand your comments about the, distortions, how the view sphere helps distortions.

I, think the way I've always viewed this is that as long as the relative features are in, the relative position to one another, you'll be able to manage it. It's you can have, it's like, you can have local, scale distortion and things like that. I'm not sure if that, I just don't understand why the view sphere helps with the, with that specifically. It seems like any kind of.

any representation where you're storing the relative position of features locally would handle distortions as opposed to A global thing. I don't want to stop you about my point. It wasn't clear how we actually help specifically in the distortion case. It seems anytime you have a, like a grid cell like representation where you're representing the relative position of features, it would work. So I guess so my issue with or concern, with relative arrangement is, yeah, as you distort and change things, if you're used to, let's say you learned with this object. And you're used to let's say you're representing or recognizing the spell by seeing this. Tip whether you know you recognize that fluid can leave or whatever. and the kind of distance relative to the handle, that this is, going to be different than when you're coming across this one. But, but, yeah, I'm not saying that I guess this is the only way to solve scale invariance. I appreciate. Yeah. If you're able to appropriately scale the input, and relative to your learned arrangement, of features, then that's fine. But. it feels like if you are seeing a novel object for the first time, it could be challenging to know actually how much you need to scale the movement. but maybe that's just something I'm naive about, but whereas with a view sphere, the idea is because you, the relative coordinates are based on, degrees on the sphere, then, basically all, that matters is that you have to rotate it 180 degrees. it doesn't matter how far apart, the spout and the, handle are. Yeah, maybe one way to think about this is if you have this spout here and it, you can distort it any way along this axis and the relative positions are changing, but the position on the view sphere is not going to change. Yeah, or at least not as much. Yeah, not as much. Yeah. Hey, all right, I'll just, I'm somewhat skeptical about. The benefits of each year for this distortions, but I don't care. I still like the idea. okay. Yeah, yeah, the thing I like about it and you and I communicated with emails that, I think we have to figure out how to map three dimensional objects using two dimensional metrics like grid cells. And, and this is getting you in that direction. the object is a three dimensional object with the viewsphere. You don't see the entire thing at once, right? You can only see part of it. So you rotate the part in front and then the part that's orthogonal to it, which I'm sure you're going to talk about, is somewhat a two dimensional at that point. so I like the idea. I'm just not ascribing to the benefits. You're necessarily agreeing to the benefits you're seeing right now, but I don't think it matters. Yeah. No, fair enough. And, I'm also until I see this actually working. I'm not sure how much this will actually hold. But yeah, but that's just one of the kind of motivations for taking this approach. And then, yeah, there was just this kind of brief mention of, okay, how do we represent the thing? The fact that objects are 3D, other than this kind of rotation element. And so if you imagine a boat performing some sort of seismic or other kind of sonographic detection of the. Topography of an ocean floor, you can eventually build up a three dimensional map of Earth. And of course, Earth is huge. So that's going to be approximately smooth. But if you imagine something with a lot more distortion, the general idea is that, at each location on the sphere. As well as, learning a feature associated with that location, such as color or texture or whatever. You could also learn, a depth, representation, and, this could be relative to either the center of the object. Or, it could and I can or the estimated center of the objects or it could be relative to, to you to the eye. I'm not sure which of these would actually be the best. but then the hope then is that this would give, at least a course representation of, for example, the mug. Okay. You know that's really far away when you look down into the mug. But when you look at the bottom, that it's quite close. And so there's this kind of depth offset and potentially that's enough that you can build a course mesh and understand that. Okay, a small object like a dice is going to fit into that. But if you look at something like an orange, which has this uniform depth all around. Then you could understand. Okay, there's no potential cavity. You can't put anything into it. It seems we've talked about here in the past that it seems like the brain has to represent depth into it. At one point, it would have represented depth as well as the eye. And then you represent depth relative to the object itself, and you have to have both of those representations, and if you do have both of those representations, then, then you can calculate, you can go back and forth, and it'll help you with scale.

I just want to point out, it's not either or. Yeah, that's an interesting point, I hadn't thought about that, yeah. yeah, okay, so yeah, maybe, just apply both.

and, yeah, so, far I've talked about a couple more intuitive examples like a teapot or a car, but the kind of view sphere can, get a bit kind of confusing, depending on how, what the object you're trying to represent is. I mentioned with the mug, you'd have this kind of maybe distance feature that you understand. Okay. It's distant flat at that side. And then you have this representation for a rim, but then as you kind of transition from a bowl and then to a plate, and essentially become more two dimensional, the kind of view sphere representation, it might still work, but it just gets a bit, a bit stranger, because you're most of what you're representing is just, that this thing is flat. And then at certain angles, you suddenly have this, change as you're viewing the rim.

and yeah, and then this kind of open question about how to represent the, the depth. But, but yeah, it might still work, and related to this, the even more extreme example is almost almost one dimensional objects like a pencil. and it may be that for representing, these kinds of objects, you don't really need a view sphere because they don't have much of a 3D structure. They are largely described by one dimensional or two dimensional, structure. And then the, planar views, which I'll discuss later, might be more, relevant. People more emphasize. I'm not disagreeing with the general concept, but even a pencil, pencils have writing on them, right? If you want to see the writing, you have to rotate the pencil. it's a little bit like the logo on a coffee cup. It's, not really a, it's still a 3D object, so if you need to make this distinction, it's just some objects would have long and skinny and some are round and fat and they have all different shapes, but at any point in time, every object, pretty much every object, and I can think of some exceptions. any physical thing would have, would have a view sphere that you would see different parts of it when you rotate it. so it seems that the principle would apply to almost anything. Am I getting that wrong? No, that's not, yeah, sorry, I'm not trying to say that it can't be applied. It's more just, how I picture it in my head and how the kind of features change. It's just, Yeah, I feel like it works nicely with more, three D kind of slightly rounder objects. It's just when it's something long and it just has these certain views where, when you're trying to look because if you're looking at a pencil, normally your I might follow the pencil, but if you're look if you're always focusing on the center, and this is an assumption and a limitation of this that I'll get to if you're always focusing on the center, then you're essentially just going to see the same feature with any orientation, except when you look at it straight on.

and that's where it just gets a bit, a bit odd. Where's it was following the object with your eyes.

Yeah, you get a better sense of the structure of the pencil. Yeah, okay. one question, I think I was maybe didn't get this in the beginning. Is this, what coordinate system is this view sphere in? Is it relative, is it in the sensory frame or is it an object centric representation? So I'd say some of the problems you're talking about wouldn't occur if it was an object centric. view sphere, right? That, that regardless of how you rotate the object relative to you, the features are still in an object centric. vSphere is still in the same place.

so say that again. So, with the pencil, so you're rotating it? Yeah. If you have a pencil, if you rotate it and object centric, if you were to project the features into a sphere that's in object centric coordinates, nothing would change as you rotate it. It's just relative to the object. And so you'd still have, what you're talking about is really when you have some very specific, your sensor is at a specific point relative to the object's reference frame. this is, I think about, and I said this in our correspondence, the view sphere is not an object representational. It's a way of thinking about vision. Isn't that right? Yeah, I guess that's my question. I don't see anything in here why it couldn't be an object centric. I guess I'm just It's not anchored to the object, right? The spheres are anchored to the object. that's a different problem. it's Yeah, it is anchored to the object, but it's Yeah, it's You're basically saying, okay, where is my sensor, around this object? In particular So my sensor is looking down at the approximate center of the object. And yeah. So that's not a problem with this fear. It's just it happens to be that your particular viewpoint is not giving you much information. About where? yeah, exactly.

Yeah, that's what I'm trying to say. Yeah. Okay, so in your proposal, this sphere is anchored to the object. And so this, the representation of the sphere itself, and the features on the sphere does not change as you rotate the object. It's just what you can perceive at any point in time. Yeah, and so basically your estimated location on the sphere. On the sphere, okay, yeah, I should let you finish, but the way I understood what you were proposing here is the sphere is, the sphere, not the fear, the sphere is a way of thinking about path integration in 2D in vision. That's how I was thinking about it. Yeah. It's a way of saying, okay, I've got a 2D path integration. I can, as I rotate the object, I'm moving in 2D dimensions, always locally in 2D. and, and I would want to, ideally, that's just the thing about path integration. The actual thing that you're seeing may, it's not a sphere at all. It can be anything. and you're going to get to in a moment here, what you really want to do is bring up the most orthogonal flat views of that object that you can, so that the 2D representation is least distorted. so it's, I don't understand it completely, but that's the way I was thinking about it. It's really a way to think about path integration and vision. Yeah. So at any point in time, you have some estimated. Point on the sphere that you're looking that your direction of gaze is or whatever, right? Yeah. Yeah, exactly. As you move around, you can pass in a great, as long as you're on the sphere. Yeah, exactly. Yeah. Sorry. I think, yeah, I think I explained maybe some of this in the journal club and I've forgotten what or I know several you weren't there, but I think I I just forgot that I needed to explain some of these ideas again a bit more clearly, but yeah, exactly, that's the idea that you are estimating where you are on this view sphere, which is a description of your sensor relative to the center of the object. And the idea is that you can then pass integrate over the sphere and basically predict features. and the, yeah, what, features are projecting onto the sphere is intrinsic to the object. So clarifying question, what if I move my gaze, so I've got a view here because I'm looking at one object. What if I just move my gaze so it's now partially off the sphere? Yeah, and in general eye movements, is an issue and I will, yeah, thank you. I will, touch on that because at the moment I'm making the kind of strong assumption that, you are, I don't know, an infant holding an object, and, just rotating it, and you're not cicading around the object, but obviously eye movements are super important, and yeah, I will touch on that in a bit. Okay, I'll let you touch on it. So yeah, so this is then, so that's the general concept now is where then I'll make some connections to, biology and psychology and, a bit more about why these kind of spheres might be interesting. So yeah, so why not just use a 3D mesh or lattice? so as mentioned, yeah, 2D surfaces have some appealing properties, which I'll talk a bit more about, and it may align better actually with how we, represent and understand the world, at least envision. so a 3D Landis is very costly, especially if you were to Yeah, if you were to actually, represent as a cellular level in all three dimensions. but, as this kind of clucus and, paper that Marcus Lewis was also on, explores, there are other ways of doing higher dimensions, more efficiently.

but, this kind of view sphere approach is a way of kind of just getting something potentially working. and this image here is not showing the actual. Experimentally measured good cells. This is a theoretical paper from 2013 discussing the possibility of kind of spherical grid cell representations in the brain. interestingly, there's a research group that around the same time did, study on rats raised in spherical cages. They then only published an abstract at a conference, but I contacted the author and they're potentially going to publish the actual study or a follow up study very soon, but I wasn't able to get a preprint in time for today. But did they claim that they found something like that when they, in the abstract. Yeah, in the abstract. gosh, so this is now. more than a week ago, so I'm struggling to remember for sure. what was it they said again? Yeah, it definitely wasn't a strong claim. I don't, want to misrepresent what they said. but it was basically ambiguous. It definitely wasn't like, oh yeah, they look like this. It was basically like, we found some interesting grid cell properties when we raised them and. In 3D environments or something like that. Niels, I thought it was only us old people who couldn't remember what happened last week. It's good that you also can't. So the sphere where the rats were running around, was it like a complete sphere or just a half sphere? I'm, yeah, unfortunately I don't know. I'm guessing it was like a spherical cage that either they could, I don't know if it was like a ball that, that moves under them or if it's a spherical cage with kind of a, mess that they can climb on any surface. Yeah. I don't know. They cannot walk like if's upset. They, very strongly my, hamster could. It's just if it Yeah, they're very good at Columbia.

Yeah. My hamster was very good. That kind of stuff. Anyways, Yeah, and and then, of course, grid cells, can have different, frequencies, and so this can then map naturally to, different hierarchies of feature detail, And then, not to go into this too much, but as has been discussed several times recently, the evidence of kind of this elegant 3D lattice of, good cell representations and animals navigating in 3D, at least from this kind of work, is not, it's just not there. seems to be these kind of clusters that have some structure, but it's definitely not a lattice like structure. And then there was this, study that, Heiko mentioned as well. Where they had this kind of slope surface and basically it was hypothesis two that was most consistent with their, measurements. and this would, if you extrapolate this, then you could imagine easily warping, this plane around. So the grid cells mapped regularly on the slope plane. is that right? Yeah. Yeah. And then now you're showing these grid cell fields below the plane. So the animal is able to go below that? What's going on below there? no. That's what they're trying to show here is just like hypothetically, how is the what does the grid cell representation look like? Is it a lattice? In which case, the plane should be intersecting Different ways you can achieve that, but he basically says it's, a two dimensional sheet. It just happens to be on an angle. Yeah. And that was what they found, Yeah. So as mentioned waiting this, preprint and then yeah, possibly some experiments on animals and pyramids, going on as we speak. but then just to mention, even if, grid cells aren't representing this view sphere. I wonder if there could be a role for 3d head direction cells, which are established, for example, in, in bats. Because they could potentially do something, similar. Is that, true that, the head direction cells in bats have been shown, they're clearly shown to be 3D and they do 3D, path integration and orientation? Is that true? As far as I understand, yeah, I don't know if there's been any, follow up paper to this one, debunking it, but, but yeah, but this is what I, came across, and, basically they found it was this toroidal coordinate, not the, because this has some smoother properties than the spherical one, because there's Yeah, I think there's been a bunch of work, since then, the details of which I can't remember now, but it was, I think it wasn't I know I co signed a couple years ago, there was, some presentations on this. I can, look it up. I'm trying to see the difference between these two.

I can go into it, but it takes a bit of, description. It basically has to do with how the animal, as it kind of pitches and becomes upside down. And then that, on the spherical coordinate, it causes this rapid, sudden change, in the azimuth, whereas in the tutorial, the one that's always smooth and, that's consistent with what they see with the, neurons, how the population geometry changes as the bats, orientation changes, but I don't think it's really important for, yeah, how, and my main point here is just yeah. I think we could justify this sort of modeling, potentially on, in terms of connections to the brain, just, based on, head direction, cells as well. Yeah, okay. I, actually, to me, it is actually pretty interesting, I won't be able to remember this reference. Could you just send it to me later? Yeah, definitely.

and then, yeah, I was just interested. So I remember when, a couple of years ago, I was at a presentation from one of the researchers behind this study, and I seem to remember them saying that The bats actually tended to fly, that bats in general tend to fly in 2D with the 2D bias and that they had to put food in lots of different, parts of the room at different elevations to really get the sort of like 3D behavior that they wanted. I couldn't find that in writing. I couldn't find a study really talking about any convincing bias in rats. but, what I could find, for example, was a study in dolphins, looking at kind of their behavior. And similarly, they spend a lot of time at a 2D plane, and then they shift to another plane. Of course, this could represent just a bias in the, world in particular, where concentrations of prayer, this is just observations like watching them swim, right? Yeah, exactly. Yeah, this is, sorry, this is the depth of the dolphin over time, hours, and, and they just have a tracking device on them. And this particular study, it wasn't making any claims about what 2d. Depth. They're spending the day. We're studying how their behavior changes over different times of day. But I was just looking at this. And, yeah, and there was also again. There was some popular science articles that claimed that their behavior tends to be Judy. But I struggled to find convincing paper looking at this. But, but it generally does seem that, yeah, they spend a lot of time not weaving up and down over tens of meters. and, yeah, that may be because of concentration of prey, or it may be because, yeah, it's just easier to do. Yeah. Yeah, also, it's really hard to extrapolate from this because you don't know when they're at those depths. Do they actually know where they are? you know what I'm saying? It's imagine I was flying an airplane and, and I got above the clouds and at that point I don't have any visual clues as to where I am and my path integration gets really screwy and then I have to come back down again to see where I am. The, dolphins may be get some depth. They have no idea where they are. They're not really doing path immigration. They aren't really, you don't really know exactly where they are. unless they have some landmarks they can navigate to, they, might have to surface or, it may, its be harder to go up and down compared to maybe I'm just saying just because they're spending time, there's some depth you don't know that they're actually No, they're actually, that's true. Yeah. I, dunno about dolphins, but as a. As a human, I know that, yeah, you have a general sense of how your deaf changes just from, having to equalize. as in like pressure, for example, in your ear canal, so dolphins may have a similar mechanism. But at some depth, they may not know, they may not know where they are at that depth. You know what I'm saying? In the plane. I see what you're saying. Yeah. Yeah. If you're thinking, it just doesn't mean they're mapping that space. Yeah. That was the point I was trying to make. Yeah. I see. No, if I was imagining flying in this office at different heights in the office, and I say, where am I? I think I would always try to map myself to the 2D map of the floor plan, right? that's, how I would locate myself first. I would say, oh, I'm in this room here, and then there's some height. It goes back to the idea of the view sphere where you have a 2D surface and then depth. I, don't view this, location under the lamp up here and this location where I'm sitting, as being fundamentally different in some ways. They're both at the same location in the room, they're just heights as opposed to oh no, that's a different part of the room. No, it's not. It's the same part of the room, it's just a different height. You know what I'm saying? It just, it's, anyway, I'm setting it off.

but yeah, but anyway, so this is looking at animals, but then getting away from grid cells and navigation and just thinking about, okay, how perception and humans, what relationship does this have to spheres and two dimensional representations? and basically it seems yeah, there is some reasonable evidence that we do have this kind of preference for 2d like representations. and I will get into later. If we learn certain views, how then are we going to generalize to arbitrary views.

yeah, one interesting finding is that in both children and adults, there seems to be this, bias of people to study the. side on, orthogonal, whatever you want to describe, however you want to describe it, views of an object, rather than these kind of angled views where you're getting multiple faces of an object at once. the study on children from 2014, the effect size was very small, but it looked more convincing, particularly in adults in this Harmon et al. 1999 study, people spent a lot more time, On these kinds of views, and it may make sense that at some kind of level that, if you let's say only if it's expensive to save views, then it may make sense computationally to save four or five views as a sort of summary statistic of an object and then perform some sort of interpolation between them to then recognize arbitrary viewpoints rather than trying to memorize every possible viewpoint.

Is it just based on, is it just based on time spent looking at those figures? Those figures? Yeah, so there was a couple different measures. So the Harmon et al one, yeah, the one I read in more depth was this first one. The Harmon et al one, the main measure was, time. Yeah, the reason I'm asking is I always find it a little bit harder to work with those. planar views like that, so I spend, it would need, I would need to take more time to understand it, whereas the 3D one on the right, I just can't understand. When you're learning a thing, I don't see why. Yeah, like I always have a hard, maybe it's just me, but those sort of projections, just to understand the object. The whole 3D. The whole object, whereas on the right, it's oh, there's the 3D object, so I would spend much less time on the right hand one. Because the left one is more difficult. One of these papers also showed that it's, I believe that it's harder to recognize the one on the right. It takes longer time. Yeah, so I'll get to that paper in a moment. So this is just looking at kind of preference in terms of how people study an object. Okay. Interesting. Sorry, it's just a short comment. What's interesting about the picture on the right, if you look at it, you'll find nothing wrong with it. But perspectively, it's totally wrong. If it were like this, it would be expanded, right? It wouldn't be I don't know. It is not as bad as your firetruck. Are you sure it's? It looks like the firetruck and drew, but something, maybe, but, it's, if you look at the rear picture, it's, it is, I can see that it's distorted. back to, I, the big evidence to me has always been, it's several things. One is I've been point out over and over again, if you wanna. If you want to describe your understanding of this object, whether you're writing it, or verbally describing it, or any other way, drawing a picture of it any way, you tend to do the one on the left. You don't tend to do the one on the right. It's just, you just, you see the one on the right all the time. You can recognize the one on the right very rapidly, but it doesn't, it seems like it's harder to record. It's, you have a story that where I have all these images, what cars look like from the side, the top and bottom. I can draw those very readily. So that was one piece of evidence. That always worked for me. And, the other thing is that, we're also dealing with the fact that grid cells seem to be 2D. So we seem to, we have a, if we don't have three representations, then we're gonna have 2D representations, and then you're gonna have 2D representations, which ones you're gonna pick. And we seem to pick these. but it's odd that you can recognize one of the right, very rapidly. even so if your internal model was the one on the left, then, but you can immediately see the one on the right.

yeah. Yeah. Yeah, I know. I'll talk a bit more about the kind of evidence around that. Just also mentioned, an answer to scuba test question, but James and I'll study. They did also look at they correlated, how well in general Children, How good in general Children were at recognizing and naming objects and found some evidence that the Children who studied planar views more were also better at recognizing objects. But yeah, possibly tenuous. I think a more interesting study. I'll mention in a second just to show. Yeah, Children. Interestingly, as I think most of us are familiar with adults many times as well will tend to draw objects in these kind of planar views. But I thought what was particularly remarkable was the way these coffee cups were represented because yeah. Yeah, it just, it doesn't even, almost look like one. So it's a coffee cup viewed from above, I think.

this one's in profile, but this is the saucer. I think viewed from above. Oh, the saucer, Got it. Yeah. Yeah. Okay. But, but yeah, a more interesting study. I think this one from Langlois et al. So basically they were trying to look at memory biases in humans for particular views. And what they would do is they would show a 3d objects, like this teapot for a while, and then they'd show, at a random orientation. Then they would show another random orientation, and the task was to recover, the participant had to use a keyboard to recover the, this first orientation.

and, they were told to do that as accurately as possible. And then whatever they, arrived at, this was then given to a different Amazon Turk, so a remote online participant. They then did the same thing, and basically they went through about 20 iterations of this. And what they found was that over about 20 iterations. And what this would converge to these particular planar views. So there was this kind of bias, even though they were trying to recover it to the one they've seen before. It's slowly drifted towards these particular views. And this is what they're showing here. So these are the clusters in the points that they drifted towards and different objects will have different clusters and, this kind of depends on the three dimensional complexity of an object. So this alarm clock only had two clusters, because it really just has a kind of a front and a back. Whereas, yeah, more complex objects had more clusters. And interestingly, these clusters also had an orientation associated with them. So people tended to shift the teapot to be upright.

and then importantly, it wasn't just that, there was this kind of bias. they also were faster and better at recognizing objects given these views. just for example, this is the, this is a good or a biased view of a, stiletto shoe. This is a, the. The kind of least biased view. And even though I know that's a shoe, I can't even really see that looking at it there.

so yeah, I thought this was more convincing. But it was also just interesting to see that. It's not just about the representations people have, the earlier studies that I'm mentioning were more looking at, okay, how do people study objects, and learn them, but they converge on a, similar argument.

it's worth just briefly mentioning though, this question of, so there's this term in psychology of canonical or kind of three quarter views, which is referring to this offset view, like the car before. or a horse viewed for like slightly from the side as opposed to in profile I thought the word canonical meant most typical or the what's used that's essentially what they're arguing because there's also evidence that people prefer canonical views and not these planer ones. So there's some evidence that people are faster and better recognizing. So there's literally, they're saying three, they're saying the three quarter view is the canonical view Exactly the word. It's weird. Okay. But, and basically the opposite, or not, maybe not the opposite, but a different result from this study, which is, this, study is comparing the kind of the planer view with the most non-player view. Where some other studies have looked at this kind of canonical view and, yeah, basically recognition can be faster. there's also this very classic result from 1981 where basically if you go around asking adults to draw coffee cups. It's a little bit more complicated, but it's a little bit easier for adults, unlike the Children. they tend to draw them like this with this semi three D structure. But, but but what does that really mean, as Jeff, you've pointed out multiple times to draw a horse from a slightly like from a quote unquote, canonical view is extremely difficult and something most people cannot do. So it may just be that with something simple like a coffee cup because it's relatively easy to add these kind of, features that signify three D structure than an adult's. It's going to do that. and it could just be the result of this canonical view. It can just be the result of there being more, unique descriptors of an object. Exactly. Exactly. Any, single. So that's the thing. So yeah, the planner views are potentially more useful for learning because they represent a, parsimonious description of the object. But yeah, exactly. Like you say, the canonical view is probably better because it gets more features at once. and so this might account for, yeah, both the drawing tendency and also the, recognition tendencies. there's also some people who believe that these views are more typical in the sense that, if you're moving around an environment, seeing an object dead on from a particular angle, is, less likely. But of course, that's on the basis that you're combining all the other views as being somehow equivalent. But, but yeah, I just thought it was worth because there's a large literature around this, mentioning this kind of debate. Yeah, I'll use the example I, I talked about before, you think a box like a die from the dice, right?

And, you learn what's on, it could have the pips of numbers, it could have pictures on it, a child's block with pictures on it. You'll learn each face, but people don't learn the relative position of the faces. They just don't learn it, unless there's some obvious connection between them. And, and yet, and so it's, really interesting. So you clearly are learning this planar view of each face. But you, as you said, you almost never see it that way. If I then show you what the three quarters view, if I just showed you the, face view, just the face of the cube, all I'll see is a square. And I actually don't know which are three dimensional objects at all, because I'm just seeing a square. But when you start at some slightly askew or three quarter view, then you see there's three planes, you know immediately it's three dimensional, and each plane can be almost recognized independently and say, oh yeah, those are all three object features on this object. it, but again, you don't really learn the relative position of those features. most people can't reproduce the pips on a die, they're all the same. I have a good, it's the same, but most people can't reproduce it. I have no idea. But, it's I'm just stating sort of the observations. It's like you've got these two dimensional views, you, it helps to see multiple ones at once because you get more features, and you also see the depth of the objects, like the coffee cup here. But it doesn't seem to me that's how your memory of some of these objects is, right?

it's, almost like saying, oh, okay, I'll see, two or three views at once. Each one I recognize is some slanted view of the two dimensional surface, or whatever, and therefore I have more data to work with, and I see this depth, and then I know what it is. It seems, I'm just arguing, it's still consistent with the idea that we're learning these planar views, and yet we'd still prefer to see Three quarter views or what kind of three quarter views.

Yeah. By the way, I'm wondering how you go with your story because it feels like that we have this preferred view directions. Yeah. Doesn't speak against the sphere and it's more like they would be separate. Yeah, You would think, you just had these separate, so I'll get into why I think they're complementary. If you think of the sphere as just a way of doing path integration and not the object, then I don't think they're contrary. If you think, and this is where I start to get a little weird about the projection onto the sphere. I just, I like the idea of the sphere as this way of I'm knowing how to pass on breakthrough movement, but I'm not, but I'm with you on this, Heiko, if you try, if you imagine projecting onto the sphere, then it's weird, you know what I'm saying? It's like, you want to project a two dimensional surface at any point in time, then did the sphere tell you that? I don't know. Yeah, yeah, so I, I think the sphere is definitely going to be important for how you, Relate these views to one another. so at the minimum, I think, yeah, you need some sort of spherical representation for because, yeah, because these aren't, these are inconsistent angles with respect to the object for different objects. So you need some way of knowing, okay, where's the planner view, relative to the other planner views. and A sphere is obviously a useful way of kind of mapping and saving that.

and but I think it's also just generally useful for, studying an object from different angles and developing a bit of a kind of curved representation, along that area, in addition to, a more Sakaid based two dimensional representation. and I, think you could have these I think you could have both of these and they could essentially interact with each other. And so when you rotate and path integrate, to a new view, you'd both be, having the representations on the view sphere that you'd learned, but that could also be recalling then the representations associated with a more kind of planar view where you're then circading around with your eyes, to study the object.

yeah, yeah, so just getting into that kind of, I guess there seems to be more evidence that, good cells can learn to represent, a different space as long as it's useful, whether it's a family tree or, a kind of Cartesian coordinates. it may be that both view spheres and planar views, along with other things like wireframe, part based representations are all going to be examples of useful ones.

I'm just going to try and go back. Can you go back to that slide? Yeah. Are you saying, I'm not sure the point of the slide here. This doesn't represent any space that is useful. Newspaper plan readers will be clogged with wire from repetitions.

I guess I'm just saying, if, you, ascribe to this view that, grid cells are, emerged through, learning structure in the world, two examples of structure that are going to come up are both, rotating an object and how those features relate to each other. As, you rotate it. So the view sphere and also holding an object at a particular view and looking around it, so the kind of more 2d planner view, and so good cells would potentially emerge to, do both of those things is all I'm trying to say.

I was looking at these sort of, these sort of conceptual maps onto 2d, onto grid cells. Yeah. and which are multiple examples now. And, as far as I know, they're always two dimensional, right? You think about the birds with the necks and the legs. But the experiment itself was only two dimensional. Yeah. Yeah.

Yeah. in these sort of abstract spaces, there isn't a, I don't know if there's, I'm trying to ask, is there an equivalent to rotating it? or is it just not, you don't even have to, don't even bother. It's Is rotation in 3D this weird thing that vision has to do with, but generally good cells don't?

I don't know what the equivalent of trying to imagine rotating over a family tree. I don't know what the hell, I can imagine a plane.

I'm not sure if there's any change the relationship between the action direction. So if you maybe if you rotate a family tree, you say going down a generation, actually break up generation. Yeah.

but it wouldn't be three dimensional or is it just, rotating around two dimensions. He's there.

it does. I'll throw a little weird thing. Something to remember, in vision, the primary visual cortex in mammals, or in some mammals, certainly in humans and, with the most advanced vision is unique, right? It's the striate cortex, because it has these extra layers, that process the information from the two eyes in very, specific ways. So we, it looks different, it has extra neural machinery that only appears in V1 in animals with very sophisticated vision. And so there might be some things that are going on. that only occur in vision, that don't occur in touch for sound or other representations. So I just want to throw that out that some of these mechanisms we're talking about here may be vision specific and that there may not be no equivalent to other parts of the brain, other parts of the cortex.

Just throw that out. Because eyes are, if I think about touch is not at a distance. touch is you, your fingers are actually on the object or close to the object. and, I don't know about hearing. But then these abstract things, I don't even know what the, what would equivalent to a 3D representation be. maybe, grid cells are really 2D as much as possible. And then, the brain comes up with some taps to make it work for vision. For v1, could we think of that as some sort of just fancy pre processing because, v2 and v4 doesn't have that. So it would be just, that. It's the very beginning, you're trying to, you're trying to, whatever it is, view spheres. Rotating and playing and dealing with two, dealing with dis distortions in three dimensions. Like I keep thinking, I can imagine a two dimensional object, a picture of a two dimensional object. It could even be just a printed word and you can rotate it in plain and you don't get confused about it. Even though it's all crunched up. You still see it as the word. It's like some other brain is able to compensate for that. And I don't know if there's an equivalent in touch or any other sensory modality. I don't, I can't think of it. It's, only, it only exists, it seems, because eyes are not on the object and things move away from you when you're, rotated. I don't see the equivalent in touch. Anyway, so yeah, some sort of pre processing that just maybe only this happens in visions. When you think about a picture like this, you have to be careful.

maybe that's Again, I'm just thinking out loud. Yeah, no, thanks.

yeah. So, just getting to these kind of player views. So then, yeah, obviously, if you're viewing an object from one of these favorite use, then. You can have more maybe of a two D. Cicada based code where you essentially just map features to locations on a grid. and then this could be important for supporting path integration along, these kinds of axes or these planes. that you understand that. The license or the kind of headlights are in front of the tire, which is in front of the car door, et cetera. but the fact that they're stored in these two views could explain you know why we're bad or competitively much worse. It trying from these kind of arbitrary angles.

and would be consistent, with studies on good cells in primates, studying scenes.

but, but yeah, but I don't think that these are contradictory or, Yeah, I think these would work well together because, yeah, essentially, the view sphere is going to help you understand how features, as you rotate an object relate to one another. Whereas the planar view is better for understanding the kind of structure along these sort of major axes of variation. And you could imagine yeah, as I was hinting at earlier when you move along the view sphere. so because you're not storing a 2D planar view at every orientation, then, when you move along the view sphere, okay, I'm going to the cells are going to reactivate. cells associated with a particular, planner, representation.

but I, yeah, I think having the kind of view spheres, the underlying structure is important because these aren't going to be these kind of necessarily perfectly orthogonal planes. and so you need some way of understanding how they, relate to one another, if you're going to then interpolate, and predict views from, novel orientations and, have rotation invariance.

Yeah, so this is just a high level summary. I think I've just said a lot of this stuff.

yeah, but there are some kind of major questions outstanding. So how do you account for eye movements when you look at the view sphere? I'll talk about this in a bit. I already discussed these other two aspects of representing narrower or flatter objects. And actually how useful the kind of death representation is compared to if we went for a full kind of mesh like representation. and then also just how do we establish the preferred optimal plan of use in some sort of unsupervised manner, maybe that kind of some fairly natural policies will, result in these emerging. Or maybe that you need some sort of summary statistic where you're trying to find the most orthogonal representations. Heiko, do you have an 1130? We do 1145. 1145. 10 minutes. yeah, I won't be able to get through everything, unfortunately, but we can maybe find a time. I'm happy to jump on in the next couple days and just finish the presentation if that works. we do have the research meeting tomorrow, but you're off tomorrow, right? Yeah, but I'm happy to join for just to give the presentation if that works. If you like, you can continue there. We don't have a plan yet for the research meeting. If you're willing to do that, I'd be happy to do that. I'd like to see the rest of it. Cool, yeah, because it would be a shame for, because I'm hoping that, yeah, that some of these ideas will be useful for Abby, whatever he's doing when he gets back and stuff like that. So it would be a shame to, I think, wait until I come back. So yeah, so just this question of eye movement. Yeah, so far you holding the object and looking towards the approximate center. But what happens when you start moving your eye and so one thing I just been thinking is that there may be some sort of transformation so that you can approximate when you perform an eye movement, where feature would be projecting onto the kind of view sphere. So here you know you could. Relatively, you could estimate, okay, go ahead. What, if I was actually holding this in my hand, let's say, and I used the card to the right there, if I was interested in something on the right, I would naturally rotate the sphere to that point being in front of me.

it's, what I would do. I wouldn't look at this view and if I had the ability to rotate it, I would. It's natural. it's if I'm looking at the coffee cup and I'm trying to see the logo in the cup, they'll always disappear and I'll just rotate it till the logo is facing me, right? yeah, we have to be able to see the cod over there, but it seems like we don't have a bias to, to, if we're interested in that location, to rotate it into view. Yeah. Yeah, yeah, I don't know. It may not be. That much of an issue, but I think it's just worth mentioning that this isn't something that the kind of use fear handles elegantly, but there may be ways of dealing with this. estimating where a feature is projected onto the sphere, and you might use geometric principles to do this, or you might use a trained neural network as a function approximator.

it seems I miss what briefly what you're saying earlier, but it seems like predicting where you're going to be on this year is pretty straightforward. When you make an eye movement, it's just more of the feature itself will be Could be somewhat distorted as if you had actually looked down that direction. yeah, no, that's true. Yeah. So the feature distortion is, another problem. but, yeah, I think, I feel like the feature distortion would be less of a problem, but I don't know. I guess it depends on the, object.

but yeah, but anyways, but both of these would need addressing if we're going to enable the system to actually, do both rotations and I movements.

and then I just thought it'd be worth throwing in a slide on, if we want to implement something like this. whether it's going to be by grid cells or particle filters. So in the journal cook presentation, I talked more about particle filters. Otherwise, there's a really nice video. I will. Yeah, this presentation links to, on YouTube that just describes intuition. but basically, they both have kind of pros and cons. but I think for me, the kind of main and most important ones are grid cells, because we reuse grid cells across objects, then there's this kind of more graceful scaling with. The number of learned objects that it eventually degrades. But, whereas with particle filters, at least as they're normally implemented for something like this, you would need a new particle kind of cloud for each object that you learn. And as that grows significantly, then, you may run into computational issues. But, but there may be ways of overloading particles.

yeah, I haven't thought about that too much yet. but the nice thing about particle filters is. at least in my mind, they have a more natural kind of probabilistic interpretation, which will be relevant to, the next thing that I'm going to talk about, which is the rotation invariance.

so I'm not doing okay. five minutes. Five minutes. Is this a good stopping point or maybe this is a good stopping point. Yeah.