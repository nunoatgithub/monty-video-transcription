All right, let me, share my screen.

and yeah, just I'm hoping that this will be a bit of an interactive thing, so just interrupt me whenever, if something is unclear or you have a question related to what I'm talking about. so some of this might already be familiar to you, so also tell me if I'm going into too much detail and it's not useful. But yeah, I thought I would start with just a brief overview of the code base. and what's all in there, and then we can go into the different topics. So basically I think this is the current version of master. At least I updated it earlier today. so there are a couple of folders that start with dot. you can ignore them for now.

the, then we have the docs folder, which is currently also not really used. HTML coverage basically is like a nice way of looking at like the test coverage of our code base. So I look there sometimes to see what part of the code is not covered by unit tests yet.

and then we have the README docs. A folder for RFCs, the unit tests are in the test folder and then a tools folder. and then the, I don't think I have, I'm sorry. I don't think I have HTMLConf, which is fine. It seems like it's grayed out, which means it's maybe ignored by Git. I'm not sure. Oh yeah. You might have to actually run a command to generate these. Oh yeah. I think, it's okay. You have to add the coverage flag to when you run PyTest. So it's PyTest dash cov or something like that. Okay. Okay. If you look in the circle CI, it'll show you the circle CI job. It uses PyTest with the coverage tools. Gotcha. So this is the command, pytest dash report HTML, and then run that and it will generate it for you. and it's, yeah, let me actually open the coverage report real quick. Cause it's a nice tool to have.

yeah, you can look at it like this and then you'll see each file and it has a coverage message, a percentage here. And then, I don't know, you can look at, let's take an act, a file with some actual content in it. I don't know, let's say object model. py and then it will show you like in green what's covered and then in red, the parts of the code that are not covered in unit tests. So if you add new code, this is useful because sometimes you forget that there's like an else statement and that's not covered in the, in your test and that, that will point that out to you.

okay. But so the main, the two important folders right now are projects and source and projects is basically a folder that tomorrow we're going to move over to the Monty lab repository.

And then in the projects folder, this is like a folder that has all the like random files that we use for different projects and different ideas that we've been trying and the files in here are not covered by unit tests. They're not really actively maintained. So a lot of these scripts might actually not work anymore. when someone makes a change, they are really only obligated to make sure everything in the source folder keeps working, but here they're like a bunch of Jupyter notebooks and stuff. So those are not always updated.

and the, yeah, I'm pretty sure none of the notebooks will work because I've not been updating them there. And remember I did the big action refactor. So there's. bunch of broken code here. And then after that, I just stopped updating projects folder except for Monty runs. Yeah. Yeah. And this is really more of an archive at the moment code that we used to run at some point and where we looked at some specific features, I don't know, we looked into speeding up the code and then there are a couple of notebooks for that. And we looked into object behaviors or yeah, different things. And the important folder for now that is maintained is the monty runs folder. In there we have the run script, run parallel script, make follow up experiment script. And then we have the experiments folder, which contains all the different experiment configs.

and in there the important ones are the benchmark experiments and the pre training experiments. So the pre training experiments are the ones that we run to generate the object models. We usually run that only when something changes in the way that graphs are being learned. So the last time we ran this was like a year ago, and the models are just on our infrastructure and you can just download them. And then the benchmark experiments just do evaluation on these models. and then. Okay. so when we, we can go into this more tomorrow, but just, so the projects will move, but we'll probably keep the Monty runs folder. Within projects. Yeah. So we'll probably not have it nested in a projects folder anymore, but we would want to keep the benchmark experiments and pre training experiments here. Gotcha. Got it. So yeah, people can either, or we can easily rerun the benchmarks when something in the code changes. And yeah, the idea of the benchmark experiments is that anytime we make a functional change to the code, like we actually change the way the learning module works or the sensor module works, we rerun the benchmark experiments to make sure,. To make sure we don't degrade performance or there's an unexpected effect of our change, we might also run them in general. When we change something, we might run one or two and just do a sanity check that they are still exactly the same. and then here in our documentation, we have these tables with the current results of the benchmark experiments. And if we do decide to make a functional change that does change these numbers, we have to update this table.

And yeah, the main two are here, the 10 object experiments, which are faster to run, but only run on 10 objects. And then we have A couple of, experiments on all 77 YCB experiments or objects, and they take a little longer to run.

we also have some more benchmarks on real world data sets and the kind of unsupervised learning experiments down here.

okay. So that's the, As the documentation is now in GitHub, you could update the scripts to automatically update the tables and then they become part of your pull request. Yeah, we always had this table in the readme actually on GitHub and we have this to do item for over a year now that says, Write a script to automate this process. So yeah, definitely something that we could do.

yeah. One thing we also have to figure out here is, so we are currently running these experiments on our Numenta compute cluster, but if we have external contributors, they don't have access to that and they might have very different hardware, so I'm not sure yet how we will do this, that the runtimes are still comparable.

we might want to have some kind of regression tests that we just run at regular intervals and that double check this, but yeah, that's still something to figure out.

okay. Let me go briefly over an experiment config. So yeah, Scott wrote some really nice tutorials on this. So if you want to just read through those. I would definitely recommend it, but I'll just give a brief overview here as well.

so in the beginning, we just define a couple of variables up here that use some utils like just sampling all possible rotations in 3D in a certain way. Interval, like this one, samples like all the different combinations of 90 degree interval rotations, so 0, 0, 0, 0, 0, 0, 90, 0, 0, 180, and so on, we define where the models are stored, and which pre trained models we want to use. So we're currently using version eight. The comment here outlines what changed between the different versions. So this is what I was talking about when we change something in the way the graphs are being built and learned that we need to rerun pre training. And that's not a very common change. So we keep track of this and with these version numbers.

on that note, I see, input channel referred to, and in one of the comments, and it's also in the buffer. Can you just tell me what input channel means here? Oh, yeah. So this was a change when, once we implemented all the, machinery to have hierarchy, so that we can basically have a learning module give input to a high level learning module. That means now a learning module can have multiple inputs, so it can receive input from a sensor module, and from a learning module, or from multiple learning modules. So the input channel basically just tells you where are these features coming from. Are they coming from another learning module? Are they coming from a sensor module? so that they can appropriately be matched in the graph. Oh, okay. Oh, when I actually, when I was like debugging stuff for 1LM, 1SM, for example, like an input channel was patch or the next input channel was viewfinder, because those were the two things from sensor module.

Yeah. And usually the viewfinder is not connected to the learning module. So it wouldn't actually send any information to the learning module. So in the graphs here, there's no viewfinder information stored.

Great question about future state. So I just remember recent Jeff conversation, when he was saying learning modules don't really know where their information is coming from, learning module or sensor module, et cetera. So he just made a distinction that it is useful to know where it's coming from, but in the future we're going correct that right now we're going to aim for a future where learning module doesn't know what's coming. It does not distinguish in sensor module versus learning module input. yeah, this is more, basically a cortical column gets input from different sources. It doesn't like technically know oh, this is coming from here or this coming from there, but it has these connections. And if something comes from a sensor or comes from a different column, it comes through different connections. So it can connect like it can be interpreted in different ways, because it forms. Physically different synapses. And here in the code, we're just replacing this with, dictionaries, these physical different synapses we were replacing with dictionaries. Oh man, I, I think I should turn this off. I keep getting balloons. but yeah, we were just replacing that with dictionaries and mapping it with these keywords, but it's not like it's inter It's like dealing with information from a sensor module in a different way than from a learning module. Okay, cool. Thank you. Represents it in the same way, but yeah, that's a good question.

okay. And then, yeah, we defined a couple of more things like minimum eval steps. We basically say we want to take at least 20 steps before we make a classification because yeah, we just want to. Get enough information and don't be over confident at the start. This is just to avoid early false positives.

yeah, as the comment says, I think it's not really necessary anymore. and also if we have some better policies, this might be even less necessary.

Then we define some tolerances. So tolerance basically means, okay, if, the sensed feature The difference between the sensed feature and the feature stored in the model, if that difference is within that tolerance, we're adding evidence for this hypothesis. If it's outside of that tolerance, we're not adding evidence or with this, with like location, we're actually subtracting evidence.

and then, we can also weigh how, much we want to weigh these features. So for example, with HSV, we weigh the saturation value only by 0. 5. just because, those can be influenced by lighting conditions and stuff like that, and we want to be a bit robust to that. it probably doesn't really matter right now, because we're not actually testing under different lighting conditions. but that's just another feature we have. We can weigh how much we want to rely on color versus curvature versus, just the morphology of objects.

how are the tolerance values, what's the general range for HSV and principal curvature logs? Like, how far off are they? is 0. 1 in hue, is there like a concrete, like red to orange, Yeah, so HSV, they are all between 0 and 1, actually, let's see, oh, okay, I guess that's because I'm in Germany, I get the German soccer clubs.

yeah, so they are between zero and one and hue is this color is the actual color. So if we say like a 10th of the circle is the tolerance basically. Gotcha. Makes sense. Yes. Yes.

And also for the principal curvatures in log scale, that's different in.

Yeah, so that's a bit hard. I have some figures on this, but it would be taking a long time for me to dig them up right now. Okay, Yeah, we can talk about this. Yeah, basically principal curvatures can get really large. But then they can be encoding quite a bit of noise, when they get like super large. So that's why we taking the logarithm of them, because really the lower values are the more interesting ones in most cases. And then negative tolerance of one is, it's, we ran a bunch of experiments back in the day and that was a reasonable tolerance to have. Okay. yeah, I can look up some plots I have on this and send them after this meeting. I guess I'm just wondering, it's it Different by one radians, if there's a unit or, yeah, but I can look at figures later and continue. Yeah, let me share, oops, hang on, pull this up here. Okay, so I have a lot of figures. I have random plots here. Let me just see if I, okay, okay, here's, for example, a histogram of how the first one and the second principal curvature, how the distribution is, for this can, and then if we log it, it gets more into the Negative seven, positive seven range. So that's, and it's a little bit more reasonable. and the scan is a bit, looks a bit odd, but, it's because it has these riffles in it. like you can ask these riffles. Okay. And then the one makes sense now in that scale. Yeah.

Fine. I'm good. Yeah. Good questions.

okay. So then this is the config for the learning module and by default right now we're using the evidence graph learning module here. If you go to the class, we list out all the parameters that are there with some text around it. I'm not going to go through all of them right now because that would take really long probably to explain them all, but if you want to understand it in more depth, maybe just read through all these parameters here.

and yeah, we basically define the tolerances again, like also the tolerance for distance, which is like about one centimeter, some threshold, how many neighbors we want to look at. And then some more parameters for the policy as well that we use, the hypothesis driven policy, where we jump to locations based on the models that we have in memory and the hypotheses that we currently have.

For this one, I had a question about what X percent threshold meant.

yeah, yeah, that basically means, so if we go evidence update threshold, actually it's below, so evidence update threshold takes, oh, I actually had a question about that too, but I'll give, I'll ask that. Yeah, so basically, an object whose highest evidence is greater than the most likely object's evidence minus X percent of the most likely object's evidence are considered possible matches. Okay, let me unpack that. basically, Every object and every location and orientation on that object has an evidence count on it. And we look at the highest evidence count across all objects. That's the most likely hypothesis. But it might still be that there are like multiple possible hypotheses. So we basically take the highest evidence count. And then we take, let's say 10 percent of that, subtract that and everything that's within that. Highest evidence minus 10 percent range is considered possible. Okay. And then, for example, the terminal condition is that there's only one possible object and one possible pose for that object, and that's what that range is used for. Gotcha. hypothetically, if I set 100, then all the objects will always be possible, right? Yeah, unless they have negative evidence. Ah, okay, because negative evidence. Okay, alright. and then the related question, evidence update threshold, which is a little bit above. So this takes, how to decide which hypothesis should be updated. And it says it can take either integer float or these strings. Yeah. for example, if we set X percent threshold, that means we only update the hypotheses that are still possible. So this is basically to save us time. Yeah. So if we do all, we update the evidence for every possible hypothesis at every step. That is, that gives the most accurate results, but it also takes the longest. cause we have to do all these matrix multipliers at every step, large matrices. So if we do X percent threshold, we only take the hypothesis that are still considered under this.

threshold. okay. And if we do mean, we basically update all the hypothesis where the evidence is higher than the mean evidence. Okay. And then here, all the ones where it's higher than the median e evidence, the what, would you ask in for intent float? what would that mean? that would be an absolute evidence value. for example, okay, if you have your evidence bound between minus one and one. You could pass in 0. 8 or you could pass in zero and it would just only update the hypotheses that have positive evidence still. Okay.

that's probably a dumb question, but what's the, is there a bound for evidence? So it can go negative. I thought it was like zero to a hundred, which was not correct, but, yeah, it's your upper bound, lower bound, or it can go infinitely higher. Yeah, good question. So at any one step, the evidence that, that can be added or subtracted from, for a current, for a specific hypothesis is between minus one and two. Minus one and two because we have evidence for morphology and evidence for features, and morphology can add and subtract evidence, and that is between minus one and one, and then features like color, for example, can only add evidence.

for example, if I've already learned a model of a red cup, I can still recognize a blue cup based on morphology. And the blue is not going to subtract evidence all the time, but, if I already know about a red cup and I see red again, that kind of biases me towards that, the red cup model. And so that's why it's minus one and two, because the features can add another up to two, one evidence to that. And then we have these past and present weight parameters and by default, they're set to one. So that means we just take, the evidence that has accumulated so far, and we add the one from the step, which can be between minus one and two. And that means the evidence can grow infinitely, essentially. If we, if the past weight plus the present weight add up to one. So if we set this, for example, to 0. 8 and this to 0. 2, then the evidence is bound to minus one and two.

we can do that to bind the evidence into a range and that also works. it's not as good as having them both set to one, because if we set both to one, we basically have an infinite memory.

If we set them to add up to one, we forget about things that we've seen a long time ago. Yeah, I think it in the future, it would be nice to have them add up to one and have the evidence bound. But I think for that, we need some more efficient policies so that it's not like we need to take 500 steps to get to the other side of the mug. But instead, yeah, that only makes sense with more efficient policies. That's great. Perfect. This is like clarifying a lot of things. So thank you so much. Cool. Yeah, Keep the questions coming. This is great. I noticed I didn't update Eh, in those two places, so I just made a pull request to fix it.. Thank you. Good. Just in those, it's just in the doc string. It's fine everywhere else. Okay.

What's your color scheme? I really like that the bad. The, names are popping up in red. 'cause mine is all yellow and that. Oh. I don't know. Where do I see that? I said that a long time ago and I thought it looked cool. let's see. Color theme. monocai. All right. Thank you okay, so we define a couple more things in the configs. some things we do a bit, specific to the surface or a distant agent. and then this is for the five learning module config, where we essentially just copy the config for one learning module five times.

and then put it all into one dictionary, learning module zero through four. then we define some things for the sensor module. So we define which features we want to extract from the raw observations. So if you remember, let me go back here, observations. We basically, we have the view, we don't ever see this picture. Each sensor only gets a small patch like this at any time. From this patch, we extract a couple of points in three dimensional space, like a point cloud, since we also have a depth image. and then from this patch, we extract a pose. So that's defined by the location of the center point, plus the point normal. the two principal, curvature directions. plus some features that are independent of rotation and location, and that could be like color or the amount of curvature that we have here.

and yeah, the features that we're extracting in these experiments here are pose vectors, whether the pose is fully defined, whether we are actually on the object or not on an a object, we don't know what object. HSV, and then the principal curvatures, logged, and then for the surface agent, we just need to extract a few more for the policy to work. So we extract also the minimum depth and the mean depth across the patch and the object coverage.

and then, yeah, for some of the experiments, we also add some noise, does this just for testing how robust we are to noise? It's not required for the algorithm to work or anything, but here we basically define how much noise we add to the different parts. So for example, how much noise we add to the locations and how much we add to the color and those vectors and stuff like that. Okay, so I, think I, The location, like 0. 002, is in centimeters, right? The unit? Yeah. Yeah, so I remember, max graph size could be 0. 3, which, like I saw in the comment, that's 30 centimeters. Yeah, 002 would be, that's about two millimeters, yeah, okay, I think in the, if we go to the benchmark experiments, we have, yeah, I think I saw like a little noisy, yeah, so this is how the noise looks like, gotcha, I had a question about that, Yeah, the, what is the distance, cause I've come across this doing actions and I couldn't find the units for the distance anywhere really. So now that I see that they're here, what is that coupled to the one meter? Where does that come from? Cause it seems doesn't need, cause it doesn't need actual units to work. So what is it coupled with? I don't know. I think that's coupled to habitat, and yeah, I think it's just to have a reference, like to know how much that would roughly be, So yeah, it doesn't really matter, The reason I'm like, when I was doing actions, all the distances, et cetera, they were unitless. And so, it, so this is only if we're like relating it to the ground truth and habitat or something for a real object. But we don't need the units, distance units for this to work. Oh, no, we don't need that. And the reason leading question there is because. If we have abstract 3d spaces, we still have distance, but not necessarily units of what it means to be there. Yeah. Yeah. So it doesn't really matter if for example, like a hundred, if one would be a centimeter instead of a meter, it would work exactly the same. We would probably want to update the tolerances that we use for matching. if we then use a tolerance for location of 0. 01, then that would mean like we, we can't have much noise in the locations that we're sensing. It has to be like exact to the millimeter. Those tolerances can just be adjusted, like scaled by how big the models are basically that we have in memory.

yeah, all of this is are coupled to a scale in the environment, but the distance is not.

the, yeah, the tolerances right now, we define them ourselves in here.

we, could determine them automatically and, yeah, the distances are very much related to the tolerances. The tolerances basically tell you how, much distance you can have between an observed point and a point in the model.

Yeah, I think we're noticing, a couple of, coupled, arguments like the one about distance and the one about, for example, the evidence threshold, if we set the weights. To sum up to one, that doesn't make sense to use evidence threshold with X percent, because it means that, anything within 20, but if the range is, minus one to one, then everything would be considered, right? there's a couple of arguments in evidence, so I'm going And, evidence graph LM that if you change, for example, the present weight of the past weight, for example, to solve the one, then you should probably change how the evidence is, like what, is considered match that kind of, yeah, So the X percent threshold would still work if it's between minus one and one bounded percent. Yeah. percentage. But if we, would, for example, set the threshold to two, then that would not make any sense because we wouldn't have to do anything. or another thing, if, for example, if we set like the feature weights. Here they are all below one, but if we would want to weigh Q by five, for whatever reason, then the evidence again is it's not, it wouldn't be bound between minus one and one anymore, between minus one and five. Okay, Okay.

yeah, it takes a bit, of, time to get the feel for all these different parameters and how they relate to each other.

maybe at some point we can automate more of the setting of the parameters. yeah, I think it could be a good idea to set the tolerance based on how large the graphs are that we have in memory and how high resolution they are, for example, or also based on how much noise we're sensing with our sensor. Yeah, I think it's okay for now. Of course, before the release, the less hyperparameters we need to fiddle around, it also reduces the experiment space. Yeah. Or prevents me from running dumb experiments because I'm the kind of person who will probably run like, Negative one to one and then but a feature of five on HSV and it's I don't know why stuff is not working.

yeah. Yeah. Most of the values, if you just use the settings in the benchmark experiments, that's pretty much the best we can do right now. And then if you do play around with the parameters, it makes sense to just read through the doc string ones and have an idea of how they, what they do basically and how they might interact. And is it true that I can accidentally misconfigure these if they don't correspond to the actual sensor module capabilities?

you mean for example, I specify that I want to extract a specific feature that my sensor can't detect? Or just even with the tolerances. so yes to what you just said, but also with the tolerances. if I say, Hey, I want to tolerate this noise, but sensor module actually generates. actually can't triple, 10 X that's tolerance of a sensor module, and it does not have resolution, to resolve what, we're setting here.

I'm not, I don't remember this detail, but I think if you specify an additional tolerance that we're not sensing, it doesn't matter because it just won't ever look for it. No, What I mean is if you go to one of the numbers, which numbers? This, the, we had the. The distance, these, yeah, these ones, nice parameters. There we go. Yeah. So like the location noise, right? So we're saying, Oh, this just says, we're just not add noise to the location.

Okay. Got it. Yeah. Oh, I'm cut up. I misunderstood it. I'm cut up now. Thank you. Okay.

okay. And then, yeah, we basically defined this noisy sensor module. and there we pass in these noise parameters as a dictionary. why do I have to change? Okay. And then we, also, we defined a feature change as M, which basically means we only pass It's a sensation from the sensor module to the learning module if the sensor module detected a significant change in features or locations. So essentially, if I'm like, if I'm on a flat surface, nothing is changing. I only moved like half an inch or whatever, a small distance. I don't need to send that observation to a learning module again, because it's not going to get a whole lot of information out of it. I'm going to wait until I'm actually going to get to an edge. And then the curvature changes suddenly, and then I'm going to send that observation to the learning module, and then the learning module can actually do something useful with that observation. so that, that's, that just makes it a bit more efficient. and for that we have these delta thresholds, so basically, Here, we say we, we send a new observation once we move the distance of one centimeter.

yeah.

yeah. Yeah. And then, yeah, we have the same for the surface agent, which is pretty much the same config, same noise parameters, just that surface agent SM is set to true.

should we also define like some kind of what means a significant change in color? So we have some kind of. Threshold by distance, we need to move at least one centimeter, but if, let's say, hue, color change from red to blue or something.

Yeah, we could, we could basically just go here and say HSV. okay. Yeah, just, yeah. And then we would probably want to do I don't know, let's say we, we really want to change once we detect a change in color. So we do the small value here, but then we don't care about saturation and value. We could do this for example. Gotcha.

I have a, let me know if this is, Too much outside the code. I have a question about the correspondence of feature change sensor module to anatomy. so considering the phenomenon that like, if you have saccade, but they've done an experiment where it's like the image remains stationary, you become You stop losing perception of the image because even your eye is saccading, it's getting the same input or you're hearing things and then it's just, we just become desensitized to the same input. Yeah. But does that happen inside a sensor module or in a cortical column? That's my question because it seems like the sensor module is implementing this desensitization, but is that what happens? And the anatomy or, yeah. So yeah, I think you, what you're talking about is habituation and, yes. I don't think you need, this also happens in like very primitive animals, not, that don't have a cor neocortex or a cortical columns. So I think it, it can be implemented pretty early on and doesn't need to happen in, the cortical columns. I'm not, I would be saying something that I'm not sure about. If I would say now, it always happens at the sensor level, but I think it's pretty easy to do this at an early stage.

I'm not sure if habituation can work on such a short timescale, if I'm just moving on the surface of a cup like this. I don't think habituation happens that fast that, but I think there are other mechanisms that just trigger us to attend only to this once actually something changes.

yeah. That's helpful context. Appreciate it. A quick question. Sorry about this one.

Again, this is change, when I send something when we change by one centimeter, does that mean, let's say, we only move a millimeter, but suddenly the color change, we were at an edge, will that still send it, because it's a significant change in color, but, or does it always need to be, it needs to be at least one centimeter away, whatever everything else is.

Yeah, I guess I was trying to parse through the if then else loop, I was just wondering, whatever, basically, whatever color change does that get pushed through, regardless of the distance? or does it, also have a change in distance of one centimeter? That makes sense. Yeah, I think in this case it wouldn't care about the color change. huh.

But that would actually be a useful thing to, to care about too. yeah, I'm not, sure anymore why we only care about those right now.

it would be something good to test again, But yeah, this is the function that checks it basically check feature change. And then we go through the features and the, in the dictionary only. So if, HSV is not in that dictionary. It's, not gonna check that change, along that dimension. Okay. Okay.

It looks like an or. so if any one of those things has changed, you get true? Yeah.

Yeah. Yeah. So it's not like both of these need to change for Yeah.

so because it's not on the, so it's not gonna go through the for loop. so it's gonna return false. in like feature change, because at the end, which means that even though there's a lot of change in color, it will still think that the feature didn't change. Yeah. Okay. Okay. Got it. Yeah.

Yeah. So we could try and play around with that and test if it's, better, if we add HSV in here as well. I guess it just reminded me, because I know Scott, you had some problem of like spoon and banana being mistaken or confused and because the morphologies are similar at some end but banana is clearly like yellow it might be related it might fix that problem potentially maybe it's just not detecting that Oh, it's not changing the color though. Yeah, it will still send the color to the learning module once it saves the centimeter. So yeah, that information should be changed. Yeah, fair enough. Okay, so then, now we get finally to an actual config. So here we put all these together into one. So we define configs currently as dictionaries and we have these entries. So we often have a class and then the arguments for that class. So we have the experiment class, which is a Monty object recognition experiment, and then experiment argument, where we have a, data class that kind of defines the defaults for this. So eval experiment arguments have do train set to false. And then do one eval epoch, and then we can overwrite some of these defaults.

like for example, number of eval epochs here is however many rotations we want to test. so one epoch is one rotation that we test for all of the objects. And we also specify the model path. which is where it loads the pre trained models for this experiment. we specify the logging config. This basically, for logging, we can specify like an output directory, and then we can specify a bunch of handlers. We have Monty handlers and Weights and Biases handlers. So Monty handlers. the most commonly used ones are the CSV stats handler, which basically saves a CSV file with all the statistics. and then we have a reproduce episode handler, which basically saves the sequence of actions that were taken so that we can reproduce an episode, which is useful for debugging since we're mostly using. Like random actions, which is dependent on a random seed. And then if there's like the 50th episode where something is odd and you want to look at that episode, if you just run that episode, it will take different random actions because of the random seed. so this way we can just reproduce that one episode. And then, we could also save a JSON file with more detailed stats as a JSON handler. and yeah, if you want to know all of these, I'll show later, they are all saved in one, in the loggers folder. And then weights and biases, we also have a couple of options.

and we can also specify, where it should log the weights and biases. Run, and then Monty log level is, basically how much information is being saved. And then Python log level is how much information is being printed to the terminal down here.

yeah, and then this one needs to be set to true if we're running a parallel experiment because we need to do some extra workarounds to get that working with weights and biases.

and then we have the Monty config. This is where we specify like all the interesting stuff with learning modules and sensor module. This is where we pass in our learning module config that we specified earlier. And we also specify the Monty arguments, which are some things like minimum eval steps, minimum training steps, maximum total steps, number of exploratory steps.

and so again, we have a default here and then we just overwrite a couple of values from the default and the default is really where all the Interesting stuff is, so for example, here, actually, I guess this is the difference between the noise and non noise experiments. Here, we actually, we're using the feature change SM and we actually specifying delta thresholds for all of these things. So here we actually like also looking at the change in color and curvature and pose vectors, and also distance. And then also in steps, which means if we've taken 20 steps. Even if nothing changed, we still send another observation to the learning module.

so yeah, let me just go to a basic Monty config to go through the fields here, because that's the most complex one. We have the Monty class, which is Monty for graph matching. we have learning module configs. Which is a list of configs, one for each learning module that we're using. And that one again has a learning module class and learning module arguments. You have sensor module configs, which is same as learning module configs. It's a list of sensor module configs, one for each sensor module. And again, we have sensor module class and then sensor module arguments.

and then we have the motor system config, which basically tells us the class of the motor system, and if we go here now, that will give us more information about what's specified for the motor system, and we have a couple of, mapping dictionaries, so this one maps sensor modules to an agent, which sensors map to which agent, and right now all sensors map to the same agent, but in the future we might have, things that move independently, like two fingers would be two agents, and each agent might have several sensors attached to it. And then we have sensor module to learning module matrix. This defines. basically that the zeroth sensor module connects to zeroth learning module. And if you saw this here while I was scrolling over it, we specified two sensor modules. One is the patch and one is the viewfinder. Viewfinder would be number one, but since it's not in this list, the viewfinder is not sending information to any learning module. We just use this for visualizations.

And then we have the learning module to learning module matrix that only gets used in the hierarchical experiments. So when we have one learning module stacked on top of the other one and receiving the output of a different learning module. So here this is, not specified, then LM to LM voting matrix. Since we're currently just have one learning module, there's no voting going on, so also nothing specified, but if we do have multiple learning modules, then we would specify, like a matrix of how they connect to each other. So for example, here we have two LM Monty config, and there we would have this voting matrix where they connect and. If we have the two stacked LM Monty config, that's the hierarchical version, there we would have something specified for the LM to LM matrix. We have five learning module configs, we have, for the learning module configs, we have five, for the sensor module configs, we have five patches. Plus the viewfinder, and then all of the sensors are attached to the same agent. Sensors to learning modules is a one to one mapping. Viewfinder doesn't map anywhere.

we have no hierarchy in this. experiment, and then the voting matrix is basically an all to all connection, connectivity.

yeah, I hope I'm not going over this too fast, but it's quite a lot, so yeah. Yeah, it's so far so good for me. Okay, cool. all right, so that's the most complex config in here. Then we also have configs for the environment and data loaders. So we have the dataset class, which, yeah, basically tells us. This is the first dataset we want to use and here the terminology we use is a bit confusing because we're actually working with environments, so not static datasets. Monty is not really made for static datasets. It's made for environments that can interact with, but this, we originally started this with a torch dataset, naming convention, and also I think inherited from their classes. So that's why this name stuck, but, we have a ticket for changing that terminology eventually. and then we have arguments for that dataset args. I'm just going to jump over here real quick to, for the dataset args, we have an initialization function with, which is in this case, habitat environment. We're using habitat and then arguments.

Here, I can also jump to that real quick. Since this basically specifies our agent and its sensors, so it says, okay, the name of my agent is agent ID zero. We saw that earlier with what agent the sensor is connected to. We have sensor IDs, we have the height of the agent, in this case, we just have it at zero for simplicity. We have the position of the agent, which starts And we have, the resolution for the sensors. So in this case, we have 64 by 64 pixel patches, and then positions of the sensors and rotation of each of the sensors. and then are these, sorry, are these, like agents for Habitat or is this agent like Monty where Monty is initialized in the environment? this is for Habitat. So this is basically used to initialize where their camera is and sensors and yeah, exactly. So this is, we use this to tell Habitat, okay, put a camera here at this location, and then here the zoom factor 10 and one. So the patch is zoomed in by 10. And the viewfinder is just a normal camera. Gotcha.

okay. And then finally we have the data loaders for training and for evaluation. and there we tell it that, for training doesn't matter right now because we're only evaluating, but we still have to specify something here, but that doesn't matter. And then for evaluation, we specify object names. In this case, we're just using a helper function, but we could also just pass an array of strings with the object names that we want to have.

we say 10 objects, and then, we use this initializer to initialize with the test rotations that we want. So the initializer basically, yeah, takes these and puts it in the right format that we need for passing it to Habitat and then distinct objects. These are the 10 distinct objects that we're testing on.

okay. That's one config. it took me an hour to explain it, but. for the, data set, how, I guess I'm just curious, how dependent are we on habitats or as long as it whatever third party that we use, as long as it can return whatever is defined in environment dataset, it will be fine. Yeah. So we have a bunch of different datasets here already. If you go into the frameworks code to datasets, we have. Okay, we have the habitat stuff, but we also have other data sets. So for example, 2D data, we defined one that's I don't know if you know the Omniglot data set, it's basically like these handwritten digits and you only have a few examples of it. and there we basically have the sensor moving over the strokes for the digits. or we have, yeah, okay. ModelNet is more data set, but for example, when we did the, demo of Monty in a real world environment, we would have I think it's actually in the 2D data one, Cicada on image environment, where we would have an iPad, we would take a picture and then it would take that full image and just have a patch move on that 2D image. Okay. So that's, for example, also without habitat. So it doesn't have to be habitat. It just needs to be an instance of the embodied environment where you can basically move and send an action, get an observation back, send an action, get an observation. Gotcha. So it just needs to follow this schema. Yeah. Exactly. Whatever dataset it is. Okay. Yeah.

Okay. And then every following config basically deep copies from this first config and then updates whatever changes we want to make to it. So for example, this is the base config for the distant agent and now we're making the base config for the surface agent and for the surface agent, we need to adapt the Monty configs that we're using. The surface and view state of the art Monty config, pass again, the same learning module config from above, and we pass a different motor system, for the surface policy and some minimum eval steps, and then for the dataset arcs, we just, yeah, initialize the dataset for surface viewfinder and so on. For the noisy one, we need to use the different noisy sensor module that we defined above with these noise values and so on. For each experiment, we are customizing a certain part of this config. And then in the end, we put all the configs into the configs dictionary. And then we can run the experiment basically by running Python, and then we have a run. py script in this folder here. and then we do dash E and the name of the experiment, and then that will run that experiment. Or you can call run parallel, same dash E and same experiment name, and it will run that.

Okay, that was, that took longer than I expected it to take, but I hope it's useful. I think it will be a video I go back to. Okay. okay. So what was next on my plan?

all right. Terminal. Logs. So right now in the terminal logs, there isn't a whole lot of output, but let me just show an example of if we would want to see some more.

so which experiment are we running? This one here. So we would want to update the logger. Let's see where we actually specify the logging. Okay, so we could just change Python log level to debug if we want to get some more information. Run this again and now it's gonna tell us all this information hopefully. Yeah, okay, this is maybe a bit too detailed. in some cases, but if you're debugging something, it can be useful to set the level to debug. where it really tells you for each step, what exactly is happening, all the evidence values. we can also set it to info if we don't want to be overwhelmed by it.

Does this also go to the log. txt file?

yes. A lot of times mine are empty, but I think that might be just because of the way I have the log level set up during pre training. Yeah. Cause yeah, I think they are set to error, so it won't print anything, but if you set it to info, it's actually useful. And I used to have this as default, so my brain still gets these huge dopamine boosts when I see a monty detected match. And the Python log level, it literally just follows the logging levels, right? The debug is for both info, warning, error, if I remember correctly. Yeah, exactly.

Yeah, for example, yeah, if we have here in the code, you'll see all over the place, logger dot, maybe not in this file, but see, evidence matching, logging dot debug will only be printed if, it's set to debug, logging dot info will print if it's set to info, logging dot warning will be, when it's set to So that's also when you're writing new code, we're not using print statements. So don't do print X, do logging dot and then the log level at which it should be printed. Yeah. Okay. I was just wondering about the log. Okay. So above error, there's actually critical, just FYI. So there's a code that you must run or critical is the way to go. But. and there's also below something below debug which is not set, but basically You know, debug would contain everything above debug, debug info, warn error, and critical. Yeah, exactly. Gotcha.

so yeah, now that the experiment is running, let me just show you what actually it's outputting. We can get to the logs.

We have this results folder, Monty, and then projects, here I think we're logging to. do you remember what the experiment was called? We're just running, I think this one today. Okay. Yeah, that should be it. Okay. So here we have, the logs. it always takes, if I run a new experiment and there's already an evalstats file in this folder, it will take it and rename it to underscore old and start this new one. That's just so we don't like accidentally lose something. Rarely ever do we need it, but we just have that. and then the log. txt, like you mentioned. Scott will save all the terminal outputs. So now I can actually go here and have a closer look. We see, okay, the target was the mug with this random rotation here.

and we're starting the simulation. We're skipping a step on the learning module because yeah, we're not getting input. Then we have the first. Matching step, we're sending input from patch to learning module zero, current most likely hypothesis is Boll with evidence 0. 96. The bowl is also red, same as the cup, so they usually both start with high evidence. We haven't even moved yet, we just saw red, and possible matches are still all of the objects essentially.

then step one, we're skipping step one on learning module zero, probably because features didn't change. Step two, sending input from patch to learning module zero, testing this and this many hypotheses out of so many for spoon, which are all the hypotheses where evidence is above 0. 19, that's that X percent threshold, and we do that for all these objects. And then, okay, current most likely hypothesis bowl with evidence 2. 08. And that goes on and on until, okay, Monty detected, okay, let's go to the last one, matching step 34, all possible locations are in a radius of 10 centimeters. added symmetry evidence, symmetry detected for four hypotheses, setting terminal state, detected mug at this location, this orientation and scale, and then we move on to the next episode.

Oh, how are the number of hypotheses, generated? So it's, it looked like for spoon, there was like 3000 or so number of hypotheses. I guess that might be more related to, we'll say generators and whatnot, but. Yeah, so the hypothesis, basically, at the first sensation, any, or before the first sensation, we could be on any location, on any object.

That's the initialization. But for the pose, There are usually only two poses for each of the locations, since we have the curvature directions. So if I'm sensing this coffee mug, I got a curvature direc I got a point normal and the curvature directions here. Let me just finish this cup.

if I'm sensing this curvature, the cup can only be rotated like this. Or upside down, because otherwise, if it's like this, the curvature directions wouldn't work. Yeah. so for pretty much all the locations, there are two possible rotations, except if we're on a flat surface, then the principal curvatures directions don't work. Could be flipped, they're ambiguous, could be rotated in any way along that surface. We still have the point normal, so then we sample I think we sample eight different rotations along that dimension. Gotcha, gotcha. And that's how we initialize it. So that, means that objects with a lot of like flat surfaces or like a round ball would be the same thing where the curvature is the same in all directions. those usually have more hypotheses initialized in the beginning and then, but then they narrow down pretty quickly and you're not testing all of them at every step anymore. Yeah, Do you think those, symmetric objects, flat objects, shirtless objects, basically, do they generally take more number of steps to get to the conc I know that the hypothesis space goes down very rapidly from 3000 to, I guess it narrows it down, but do you think those can be considered more difficult objects for Monty, or it doesn't matter?

yeah, so not because they have more hypotheses in the beginning. But more because of symmetry, so it takes longer to, resolve symmetry. we have a symmetry detection mechanism in there. so we can detect symmetry and then have an early timeout, but it still might take a few more steps to actually detect symmetry and say, okay, I can't resolve the pose along all three dimensions for this object. Okay. So is the symmetry, computation or estimations, is that a pretty local thing or is there some, yeah, it's local, so it's not object wide. It's pretty simplistic. So it basically says if two, if, what is it? If we have the same set of hypotheses that keep being valid for X number of steps, then these must be symmetric because the whole. All of the observations are consistent with all of these hypotheses, so it must be symmetric. That's basically the rationale behind it, but we're not like taking the graph and doing some computation on it or something like that. It's basically just how the hypotheses develop over time. Got it.

And it's not perfect. Yeah. Like you saw just now with this log for the mug, it detected symmetry. When actually with a MUG, you could resolve it by going to the handle.

And I think you've seen that too with the unsupervised experiments that with the MUG, it often just merges graphs with the handle in different places around that. Like it detects correctly how to put the MUG up right, but it just, yeah, gets that rotation. it doesn't, go to the handle and figures it out.

Yeah, that is interesting to me that it would, I don't quite understand the process yet about how, if I'm on a mug, I don't have a mug on me, close enough. If I'm on a mug, and I'm moving along this surface, and let's say this is the handle, I could be at any one of these points, right? Anywhere but the handle, essentially. But during the continual learning phase, say on the second or third epoch, it must eventually settle that we are somewhere, even if it's not correct. Like all these hypotheses are about equally good. Yeah, so it could say And then it picks a spot, basically. And that's why when it runs into the handle in an unexpected location, it just adds it. Yeah, so it basically goes into the symmetry condition where it says All of these hypotheses have been valid for the past a hundred steps or whatever. So I think there's symmetric and then that goes into okay, the episode is over. It starts a symmetric object, and then it just takes the first one for how to add the object into the, how to add the new observations into the graph. Okay.

Yeah. That's actually something we could maybe do for the unsupervised learning condition is to set that threshold for symmetry detection much higher. So it goes longer trying to resolve it.

okay. We only got 10 minutes left. Maybe I can quickly explain the CSV stats.

okay, actually maybe I show them for a five learning module experiment, because, that, that might be a bit more interesting to explain. Okay. So we have the primary performance, which is like the most interesting thing that the, that Monty recognized the object correctly or not. and there we have different options. We have correct. We have correct most likely hypothesis, which means. It didn't converge, it didn't say I'm done, but the most likely hypothesis was correct at that time. and here in this example, we have five learning modules, so LM0 through 4, and that basically means, okay, at the end of the episode, three of the learning modules had detected the object. They said, we're done. Our condition was we need three out of five learning modules. To be done. So it just said, okay, three, three are done. We're done. And these were not done yet, but they still had the most likely hypothesis.

other options are okay. Patch of object that just happens if one of the patches is not on the object. and then confused, most likely hypothesis that happens when like here we have, again, we had three that were done. One at the correct, most likely hypothesis and this one. And here you can see the hypotheses that they still had at the point when the episode finished. Both of them have Spoon in there, but this one just had a different one for most likely. And you can also see this one only got two observations versus like 36, 37 on the other one. So it probably just didn't get a lot of information. And that's why it didn't say I'm done. And it didn't have the right most likely hypothesis. I have a question about that. Yeah. So how is it that these, learning modules They're in the same episode, they're both receiving information from the same type of sensor module, but one has more observations than the other. Yeah. So that's because they are different patches in different locations. they are really nearby and move together, but they're still in different locations. And they are also using the future change as an SM. So they might not all be getting observations at the same points in time. And also it can, if you imagine these, and it's a distant object. the distant agent. So it's saccading like this. And if you have four of them here on the left side and we're on the side of the cup, then like the ones on the side here are not actually on the cup. So they won't get any observations while these are actually on the cup and getting observations. Got it. Okay.

And that might've happened here. That learning module two was just like not even on the cup. And so it didn't get anything.

Okay. And then we have number of steps. And if you go through here, you'll see there's a number of steps, there's Monty steps, there's Monty matching steps. So what's that all about?

where did I put that?

experiment, I think. Yeah, if you go to how Monty works in an experiment, there's an explanation of all the different types of steps.

number of steps, the first column is number of matching steps that a specific learning module performed. So here, these can be different for each of the learning module. Each learning module might have gotten a different amount of observations and performed a different amount of matching steps. Monty step are global steps. So this is number of observations sent to the Monty model. This includes observations that were not interesting enough to be sent to a learning module such as off object observations. It also includes both matching and exploratory steps. So here, if you look at Monty steps, it's the same for all of them. It's basically How many cycles do we do? How many, even if all of the sensors don't detect a feature change, it will still be a Monty step, because we went once through the outer for loop of taking a step in the environment, essentially, like the sensors actually made a movement, even if nothing happened in the learning modules.

And then Monty matching step. is at least one learning module performed matching step.

and yeah, there are also exploratory steps, which do not update possible matches and only store an observation in the buffer and they are not counted here. So matching steps is again, the same for all of them, but this are only the steps where at least one of the learning modules actually performed an update and got an observation.

So this was when the feature change was true for at least one of the module. Yeah, exactly. Gotcha.

and then, yeah, we have the rotation error, which is in radians, this is actually pretty bad, I'm not sure why, but it's between zero and pi, this is like the maximum, almost. maybe. Okay. Yeah. It might be like a mirror symmetry, like it recognized exactly the opposite.

and then, yeah, result, it's all marked, but these two marks are in a list because These two timed out, so this is just correct most likely hypothesis, and why would this not be done? It only has one object in the possible objects. Why wouldn't it say it's a mug? It's because for a learning module to say it's done, it also needs to resolve the pose. So these already said, okay, mug is the only possible object, but I'm not sure yet where I'm on the mug or how the mug is rotated.

most likely objects, mug for all of them, primary target object. so we have primary target object and stepwise target object. This is, basically a distinction that's useful for. Environments where we have multiple objects in the environment. So primary target would be the object that we start on. And then we might be moving on to other objects. So that's the stepwise target object. So that's what was the object that we were on when the episode ended.

Okay. And then highest evidence. That's the evidence, the highest evidence for each of these. Time is, How long? I think this is how long each learning module took overall, added up, and they are different because some of them took steps when others didn't. I would have to go back into the code to check why these are different actually.

and then symmetry evidence is like, how many steps in a row did we have? Where symmetry was detected and basically if one step happens where the hypotheses are not consistent anymore, this gets reset to zero again.

Monty step, matching steps I explained and then individual, terminal state performance. Here you can see again, those three, finished and We had a correct classification, but then these two had a timeout because they were finished early because we reached the terminal condition with three, here individual terminal state reached at step. it's at what step was it done, and we can see this one was done first, and then this one, and then this one, and these never. And we have the target position, rotation, the most likely rotation, number of possible matches, detect location, detect rotation, detect scale, those are all pretty obvious, individual rotation error.

mean objects per graph and mean graphs per object, those are relevant for the unsupervised learning experiments. So when we don't give labels during training, it might happen that two objects get merged into one graph, or one object is represented with two graphs, and that's being tracked here. And then, true positives, false positives, and so on are tracked in this column, which is also useful if we have unsupervised training, where it might be, oh, I have a model learned from the small, from the red mug and the green mug. They're both merged into one model.

so I think, yeah, that, Basically, yeah, it's a bit complicated to explain, we're over time, but, yeah, in this case, we can only have like target impossible matches or target not matched.

and yeah, maybe I'll go over this another time. This is for the policy, how many jumps were attempted, how many were successful, and learning module ID.