just, I'll start with just a really basic review.

what's really complex, topic, but essentially, if you look at like the retina on the back of your eye, there's a million fibers coming off the retina and they project to the thalmus and then the thalami projects to the cortex. lemme see if the area in the cortex is be one. And, then this basic arrangement holds true also for auditory and somato input. So everything from your sensors goes to the thalmus before it gets to the cortex, and it also holds true from region of the cortex projecting the region and the cortex. They also go through different regions of the, so the has a bunch of little regions, each one dedicated to certain things, but has this very common and simple structure, which no one understands what it does at all. but we have a theory about it.

there is a green thing here. Relevance to today's discussion. There's two, if you look in the cortex and you look at a column in the cortex, this is now layers one through six in the sheet of cells. And if this is in B one, there's two productions back.

there's six B and six A, and they both project back to the thout. And in theory, these are large projections theory. The cortex is con, is telling the theus to do something about the information that's being sent to the cortex.

and for most, almost all the literature on the thout, that is just, it's like they have no idea what it's, they'll say, oh, it's changing the receptor fields a little bit, timing a little bit, doesn't look like doing anything. And they, refer to the cells in here as relay cells. Because for, a long time when they observed them, once, it would be like a one-to-one ratio coming in. You have like about a million fibers coming in. You have about a million fibers coming out. Eat, you get a spike on one of these cells and you get one single spike out here. No, that's not true, but that's true in some situations. And they observe that. And so they call 'em relay cells because they didn't seem to be doing anything. A spike comes in, a spike comes out. But this almost is critical to everything in the brain. If you don't have it, you're dead and, or you're vegetable. And clearly that didn't look right. but that's how people talk about it and that's why I tell, okay. So well, leaving out a lot of details, we have hypothesized that one of the major functions of the humus is it does, orientation transforms. So you have, an input coming from the retina, for example, and then you rotate the eye relative to the world. That whole, all the inputs are rotated and you have to re back to the canonical form. And the same thing is true with, motion. That is if your eyes are moving, if you're reading text and your eyes and you're reading from line to line, you, my, my eyes horizontally to read the text this way, but if the page is slightly tilted this way, I can still read, but my eyes are no longer moving, horizontally moving like this. And so you have to compensate for that because the models in the cortex are not, they're, not, they're models that have a preferred orientation. The orientation they were learned in. And so this is the kind of stuff you do in computer, models, computer well, graphics. And also if you do this, you have a model of a house in the computer and rotate, it's just, it's, everything. Anything in has in one model, in a and a orientation sense.

So, just complete this picture right now. one of the observations about these cells, these relay cells is a cell body. They have a bunch of dendrites, they have a lot of synapses on these cells. maybe 6,000 is a typical number from number synapses on these cells. And, and of course the cell has an output. Axon, I should have drawn this differently.

and so well, if it's a relay cell, why do I need six cells in synapses? It doesn't make much sense. And worse than that, if you look at, if you look at these, if you look at these axons coming from the eye, for example, you, for a relay cell, you'd expect this axon to gone into one cell and that cell to project to the cortex. But there's a lot more synapses here than there are axons coming in. And so these cells, these axons, the numbers are kind hard to come by, but they clearly connect to in far more than one real cell. So an individual, accident coming in is, going onto multiple cells, which doesn't make sense for a relay, right? Because it's supposed to relay something, not it's not supposed to be integrating anything. And so our original hypothesis was that, and these, synapsis by almost no synapse is going on the cell body, they're always, out on the end rights. our hypothesis was that, the cell has different, if it gets, it forms synapses with different axons from the, retina, then it would select which one of these it is gonna relay. It's, not gonna integrate them, it's gonna pick one and therefore it can say, oh, it's a, multiplexer. You say, okay, my output now is from here and my output's from there. And so you could, if all this, all the relay cells are doing this, you could basically do it any kind of transform you want in this situation. You can route an input to a different, input to the same output. and now there's a lot of, we speculated this partly based on, observed physiological evidence from all the researchers, but also largely theoretical. Like we, first of all, I knew that the relay cells wouldn't be doing something as simple as relay, but they looked like relay. So the most obvious thing they're doing is rerouting. And then there was some evidence for that. And now there's a lot of evidence for that. Okay, so now we read it last week we read a couple papers that were talking about how. on a relay cell, they can activate individual dendrites separately. So you can think of this as like a, one routing mechanism, so on. And the lot we don't know about that, but this is almost certainly happening. and so that's all good and cool. So now, if, and now if you're ready, I can talk 'em all my questions. I have, you have questions about this, you can ask them.

would it be possible in the model we make to have the column just learn every rotation of the object instead of having a thous? that would be, very inefficient. Yeah. And then you'd have to figure out which of the models is the right one. And then, so you'd have to have many models and if you don't actually change anything, you'd have to have models for every orientation you've ever seen it. And you'd still have the problem of knowing which one's the right one at the moment in time. And if you're learning one model, then it wouldn't learn the other models. And so it wouldn't be very good. Yeah. Need, because you also maybe wouldn't have a great, understanding of the commonality between the models. So you think of them almost as like separate things. So I highly unlikely that's happening. I'm pretty sure it's not. Okay. So, this now brings up the question I wanted to ask is, there's two questions I want to ask is, how is the transform learned? Like, how does it know, my assumption this transformation is learned? I can give you the arguments for it. How would these cells learn what. What to do this, assuming that they're not genetically, de predisposed do this. they're genetically set up, but they, my assumption, almost everything Brian has learned and, I assume that this has to be learned. So that's my first question and I'll talk about that. And then the second question is, I talked about there's these two different projections back to the theus. These are two separate cell populations, often referred to six A and six B. And our model six A is a sort of a location signal, and six B is the orientation signal. So the orientation signal makes a lot of sense. This guy is telling this one what orientation, the current, the sensor is right now to the model. And so it says, go right now, you're at one orientation, so make this change. But we don't, and yet there's all these projections from six A, which is location, which we actually no longer have an idea what it's doing here, and I'll talk about that. We don't know why the Thomas needs another location on the object. so there's some question about that. So let's, let me just first talk about how the transformative learned. And I'll just run out what I was thinking and see if you guys have any thoughts about it. So imagine you have on the retina, again, this applies to any, this could apply to your skin too. It doesn't make a difference. the retina, this is the one that's most studies. You might think of a patch of retina as a bunch of these.

They're called ganglion cells. And the retina neurons and the retina that project out or ganglion cells. These, the retina does a lot of processing, but you can ignore that for the moment and just say, okay, the output of the retina is a sort of topographic map of ganglion cells. And I'm looking at a little patch here, right? there's maybe a million ganglion cells in your retina on a retina. So there's a little patch of ven on. These are all said, the outputs are coming in and projecting. Now they're gonna go, they're gonna go to the, to the thalmus. And, and after the thalmus processes it, they're gonna go to the cortex. And the general idea is that there's a, topographic representation, in the cortex that's similar to the topographic representation of red. It's not exactly the same. You don't see cells here that fire exactly like this. By the time you get here, these cells have a more sophisticated response than these. but there's still a topography that's maintained. it's not like it's all random. so this is your, cortex, maybe V one. And so this guy is, I won't go into it, but it learns a model of whatever you're looking at, century motor model. Like you're moving all around and it learns a model of it. And what we need to do is, you can think of it this way, you can think of if there was no, if there's no orientation transform, you might just say, this cell goes to, to this part, and this cell goes over here and this cell goes to here. Something like that. But, and in the th is, you have, these relay cells. Just, I'm just gonna draw it like this. It's not really like this, but you can imagine like this and you can say, oh, there's one to one and 1, 1 1. But if I, now wanna rotate it, I do some orientation transform. this cell now has to go some, this cell has to activate a different one here. And it's maybe worth mentioning that these, connections from layer six, to the thalamus are modulatory connections while the inputs from the retina to the thalamus are the driving. So these, that's point, these don't make the cell fire. Yeah, but the evidence is that they do select which dendrite is active. That's one of the things they do.

so they don't, yeah, these inputs don't make the sound. The sound is relaying information that is clear. but the goal, these things are not creating new information. They're just saying, okay, do something to them. And we think it's an orientation transplant.

So, you might think of imagine this whole thing was rotated then, this, cell here. maybe, you can imagine all, these, cells have to now respond to a different set instead of, here, this cell here might, instead of getting input from this, it might get from here or here as I rotate it. Something like that. I don't think this is pre, I think this has to be learned. cell doesn't really know much. Really stupid. There's probably not enough juta code to determine all this. we do know that people learn new orientations, they learn new transformation. So it has to be learned. And so the question I was asking like, how, would these cells, learn to rema to know which place to map over here? How do they know that? and that's pretty hard to do. So here's my thinking on it. so far, imagine you start with a child or appreciate, and you learn, an object first in a canonical form. You just assume that I'm, just like the, I'm living a word or a ladder. It's vertical or whatever it is. It's, and and that's the easy way to learn it. You don't show child letters all rotated. You show a child letters. Okay, here it is. A here's B. So on. So you've learned, let's say you've learned a model, and let's say for example, there is a mapping from here to here. Here. Just say that it exists, but I haven't learned any transformations yet. now, I now rotate this. How does it, and, when you rotate it, we can also assume that the cortex knows that it's rotated. And so it changes the signal back, to here, but it doesn't know what to do with that signal. This, these cells just, this is an input. And then now the input's changed. how would these cells learn what to remap to is the question. And, It's a challenging question. I, the only way I can think of doing this, and maybe someone can think of something better, is, where did I write this down?

here, imagine you start with, you're looking at, imagine you're, gonna use conc, continuity and time or histories or something. Imagine you're looking at a simple object, you've learned, and now the, now you rotate your head a little bit. and so now the inputs are changed coming in here. if you can make the following assumptions, if you could say, oh, one I know, perhaps are being the cortex, not you personally, you wouldn't know this. Just speaking, I know that, orientation has changed.

I'm gonna assume if this happens gradually, I'm gonna assume that the object observ I, I'm observing has not changed.

So imagine you're a child, you're looking at something and then you go like this. And then just the default assumption was that changing rotation might not change the object. And so what you, what these cells are gonna stay active and you're basically saying, imagine I'm getting an input from this guy's getting input from here.

and if you gradually change this. the input and the retina is no longer driving this thing. But you might think I, this cell still has to be active because that's the one that's corresponding to my model. So this guy could say, I'm gonna pick, it some, I'm now gonna get input something nearby, adjacent to the one that was getting, so the ganglion cell that was active, but that one, now I rotated. Now this one seems to be active and this one's not active. I could just say, that's the new input I should have based on a gradual change in orientation. I should, bring, I was there now under this orientation, I should bring that one over. And then under this orientation I can bring that one over. So this is, the only way I can think of doing this is that you would, if you don't know anything else, and neurons don't know anything else about the world, if you just, if you take a canonical representation and you slowly change it, and then you make the assumption that the model's the same and these guys still should be active, they can say it's like a hysteresis who's saying I should still be active 'cause it's the same object, I haven't changed anything else. then you can say, this person active, here's another active, I wanna put it, and I wanna map myself to that based on this orientation.

that's where I'm at on this problem. And yeah, I'm just trying to think 'cause I guess we want it in a way to be specific or sensitive to the orientation that's being sent down, and predicted, But you can also just like you or I guess we just. We don't want the cell, and the thalamus to activate to that set of neurons regardless of how they're on the retina. Because then it would, I feel like then it would, So what, you're doing to the orientation, what you would do if you looked at one of these dendrites, you would say, here's, let's say here's the, big synapse coming from the retina.

And, then you have a bunch of synapse that recognize the orientation. And so you would say, okay, I'm now gonna form a synapse to the, to retinal ganglion cell, and I'm going to recognize the orientation pattern at the same time. therefore I associate that orientation with that retinal cell. Retinal cell. Retinal cell.

and these retinal cells, they form these really, very large, unusual synapses that, that literally one synapse would make the cell fire. So that's unusual. That's not like a typical neuron. So you really, you don't need to, you don't need to recognize a pattern on the retina. You just need to form a big monster of synapse with that particular gang cell. this is a bit sketchy because, it assumes that, assumes a lot of things to make this work. but I couldn't think of any other way of doing it. At least I have a way of doing it, but I couldn't think of any other way of it. all So we want it to be a little bit flexible and we, but what's the argument against. Some of it being already hardwired, ah, beginning, because it seems like just for orientation transforms, it's not that much too hardwired. And it's like to actually learn it, like for a baby to learn this is super difficult. 'cause they can't even control their motor movements enough yet to actually willingly they don't have, they just look at something and their head babbling around. that's Yeah. But things are also just moving well, That wouldn't work then if, things are changing all the time, then they'll be confused. this basically assumes that you fixated on an object that's new and you're, again, you don't have to do this when you're young. Yeah. Once you've learned these orientation transformers, they're gonna be good for a long time.

what are the arguments here? There's a lot of arguments for why it has to be learned. We have the inverted glasses argument. Yeah. we have the fact that, very few things in the cortical system, the individual synapses are genetically determined. they almost, they're determined to go in a certain place, but the evidence that there are deterministic synapses, I, it doesn't exist as far as I know other simple organisms. That's true. If you look at a snail, the snail synapsis can completely, theia is completely specified by genome. It's got 400 synapses and there's specified every has the same. But the, inverted glasses argument. Could just be explained by the cortex. Learning to send different orientation transforms to the thalamus.

that's a little hard because, it does feel like, yeah, if we could, do it just naturally, like why can't we do it for such an extreme transform? what's special about, and I think in general, like a lot of these studies we found, which were like less extreme than that, it was like something like 60 degrees as you turn your head, then it becomes very noisy, whereas like less than 60 degrees, it's quite good. So it seems like it could be like experience dependent, although it could also be evolutions, don't really need to deal with being like upside down that often. Yeah. but I think there's also, there's all kinds of, of just, throughout the cortex and, there's all these issues of plasticity. where, you can reroute to that column you can bring in was, this is the old, ferret experiments where they take the optic nerve and that's going to cortex and they route in, they take the animal at birth and they route the, somato cortex to that V one. And it seems to work. this is some, and then also, think about all the variations of things you might do with somato century touch. There's been orientations and people have limited movements or some people, it's just, there's so much variability that you could be right. But over the years I've come to accept that there's so much variability in the cortical system that, you just gotta assume things are learned. That's a basic assumption because there's so many examples of plasticity and I bet you if you did some experiments with teaching people to read vertically, they get better at it. Yeah. Yeah. I'm just trying to think of alternative explanations. And I feel like orientation transforms are very, general. So you could just have this general set of orientation transforms that you apply to any sensory modality. And then the part that's actually learned and has all the plasticity in the cortex, which, why would the cortex have to learn anything? If you hardwired, why would you have to learn anything? It would just learn which orientation transforms to send to the thalamus. Alright. But that could be hard coded too, right? Classes, it would just learn to send the opposite. But you could, you could, You could just assume everything's hard code. I don't see why you have to learn anything. maybe that's the way to do it for, for auntie and a thousand brain projects. that may be the best way of doing it for you guys. but I, so I'm not saying we have to do this at all unless I start off by saying, but I think I, I need some really strong evidence to suggest there's a lot of evidence suggesting this is learned. also, there's a limit to how much, think about it, there's a limit to how much each cell can do in, this case, an individual, relay cell, may have, there's a limited number of dendrite segments that you could do this with. And there's a limited number of, there may be 10 inputs from the re from the retina that it can tra that it can change. and so the amount of the amount that you, the amount of remapping you can do is limited in a neuron. And so the interesting question about that too is, it says, okay, I should learn the ones I observe most commonly. And the ones I don't observe very commonly. I'm, just not gonna be able to do it. I just, it's just too beyond my ability. I can't, remap, I can't take a thousand axons and remap all thousand axons. Individual cells can't do that. it's just not enough den drugs to do that. Yeah. I feel like, yeah, I guess one. thing about this is it, feels like then the, each cortex needs to learn. A lot of each cortical column needs to learn a lot of transformations. I'm just wondering, so one thing we've talked about before is, each, each relay cell has to learn, right? The cortex is right, but I, guess how each relay, yeah. But, yeah, I guess we've sometimes talked about, phase codes, and I'm just thinking like a phase code feels like something where that could be a bit more general in that you don't necessarily have need to learn every single orientation, but if you can just shift it as necessary and that's doing the transformation, but the mean, and then I'm just saying relay. So like you're saying, the output of a relay cell doesn't have to remap, it just has to put a different phase on these different inputs. Yeah. and if it's one spike in one spike out, maybe that's just like changing the timing almost. I don't, the evidence, I don't think they show that. I'm trying to think.

yeah, I guess it, it depends also, yeah, what it would, what it's relevant to. okay. There is, when people try to figure out what relay cells are doing, and they typically do this in a slice preparation because the thalamus is in the middle of the brain and they can't really get at it. So they do a dead animal that, they slice the brain tissue. So you really dealing with the, a non-realistic thing, but get, they get a little slice of cortex and a little slice of thalamus and they see what happens, right? And they activate these cells and see what happens down here. Those studies tend to show some, small changes in the output of the thalmus. One of those changes is a temple shift, a little bit of a temple shift, like under certain conditions they can get the spikes to move, a little bit or may those, there's a little bit broader response and narrow response. So it could be something like that. Yeah. that's an interesting idea that if phased somehow a phased code, but it does still seem, you still seem, there's a lot of this evidence. You still get one spike in, one spike out. It's not oh, you get one spike in a lot of cells sub the same spike out, and there are different phases. It doesn't look like that. again, these studies are so limited. you can't really, it's like, it's hard to know what's really going on because it, the preparations are so right because yeah, for what's worth, I feel like both, one spike and one spec out and also just how powerful the thalamus is when it arrives at the cortex, the, those spikes. It's with a, more temporal code. if there is a temporal code in the brain, then it is much easier to do if you have like individual spikes have a lot of weight rather than, and, I think of question and, and there is some, there's some really good, there's some other evidence would suggest that too.

let's think about it a little bit. the cortex has to do with a bunch of transformations, right? We've talked about orientation, but we've also talked about scale has to do with scale. And, and the way we think scale is likely implemented is a change in the beta frequency, I can't remember beta frequency.

and maybe something like that. I, I don't wanna spend too much on, on this because I think in the end, Monte doesn't have to do it this way. You can do it way you want, but if you're gonna do with neurons, if you're gonna do with SDR or something like that, you have to think a little bit about this, right? it's easy to do with numbers. You just put a bunch of, fusion coordinates and rotate 'em. yeah, I guess the one relevant area for this would be if we learn to represent different spaces than 2D and 3D. Then you would have to have to learn right. Transforms in those new spaces. Yeah. The question is how relevant, that's another big topic we hope we'll probably avoid today is the 2D 3D org space thing, because it's really complicated. Yeah. anyway, I just threw this out because, we have a paper that some type will get finished where, the head paper, where we're talking about this concept and we ought to be, at least they'd be able to logically explain to someone how it might work. and, could, it could be a question, a sophisticated reviewer, we would ask, how's this learned? And, and so I was asking the question myself like, how would it be learned? How would I know this stuff? but I'd rather leave that first slide for now and go on to the next question, which is even more relevant to the thousand brain project, which is, you got these two projections going back to the six A and six B. Then I would do, now I would draw a whole thalmus, like a blob of the thalamus and say, we'll just call this the, part of the thalmus, which, l gm, which is a, from the retina. So it's a whole bunch of these cells. It's this one here and then there, as for those who are around here for a while, we know there's a sink called, thalamic reticular nucleus. and then if you have the cortex here and you've got six A and six B. By the way, there's a lot more cells in six than the six A and six B. So this is just a subset of the cells in six layer six. We know that, six B, which we think is orientation, just goes in and projects directly to the relay cell dendrites. So that's just what I was talking about.

six A, which we think is location information, is, projects more complicated. It projects both to the TRN and it projects to the individual cells. And the TRN is does some mis MR stuff and I can't remember, do we know that it's the same cells or it's like it's one on the same nucleus or, that was the recent paper we read or I read, I dunno if we talked about it here. I mentioned it where classically if you read about it. Oh yeah, the layer six I cell projects to here and the layer six B cell projects right to there. Oh. But then there was this recent paper that said, oh, that's not true. they're saying, they were tracing this and they said, that's true, but what it projects to is, a different relay cell. So the, in the same column, it wouldn't be projecting at the same cell, the layer six A cells we're projecting to, here and to here, and. They were, and they were suggesting, I forget the details of this, but there's a, sort of inhibitory surround about this. So what's happening in this scenario, if this column is active, that this cell is active, but this one is being inhibited, the surrounding ones are being inhibited some distance, not just not the niche adjacent cell. Yeah. But some distance away and that, and so that was like, oh, that's interesting because if you, look at individual cells from the cortex down to here, they look just like I drew it before, but you have to make sure you're tracing 'em from the same spot and seeing what the effect they have. And oh, it's not the same cells, it's different ones. And so the idea was that recent paper, and there was only one paper, was, that maybe providing some sort of, inhibitory surround where, and then I said, oh, that would be a way of dealing with capacity issues. 'cause a single cortical column, may not have enough capacity to do what we wanted to do. But if I could, say on any moment in time, I would, I could say, if I had, think about cortical columns and the cortex, and here's one column and here's a bunch of columns around it, there's a long discussion that perhaps what a active column will, will, inhibit neighboring columns, in which case then you'd, have a lot higher capacity. You could say, oh, at any point in time on a particular object, I'm gonna be using one set of columns and then another time another set of columns. And so there's some suggestions about that. Again, in the thousand range project, we may not have the limits. Of capacity that you have in the cortex. cortex cells have limited ability. It's quite a large, but it's still limited.

so anyway, so this, that was a complication there that these layer six A cells may not be the same ones that project right to the same cell as in layer six B cells. But the question I'm having is, why am I sending down a location signal at all to the thau? What theoretical possibility reason could there be to telling the thau the location on the object? You're, recognizing and bear mind in one, in our current thinking, this is a unique location, so it's unique location on a unique object. So there's a gazillion of these enc coatings, SDRs here, and if you're expecting the thous to line every location on every object, that's asking a lot of it.

it's possible, but it seems whoa, that's really hard.

so can I ask a Yeah. Question. What's the relatives number of neurons in a thalamus to the cortex? what's the kind?

the number of, the number of relay cells is quite small compared to a column. It's very small. Okay. In fact, the whole thalamus is, but yeah, I don't know about the size, density, size, small on a human, the size of a small bird egg, But one thing to think about it is, these, Relay cells project, they project up to layer four in a column, and there's a bunch of, there's a lot of cells in layer four, but they only form less than 10 of the synapses on those cells. And general rule is there's 10 times more star coming back than going forward. So these projections, especially layer six A, is there's 10 times more fibers going back to this guide than there are going forward. Again, consistent with the idea that you're not really encoding a sparse coating here. it's not really a distributed code going up. it's more like this non distributed code. It is a code, it's a relay code, it's like a one-to-one code. and and you're just rerouting it. That would make sense. That explains these low numbers. So there's relatively few of these. So if there's a million fibers coming in from the retina, there might be a million, relay cells at this point in time. but that goes all of V one and V one has, many billions of cells in it. It's just, very large. so it's a small number. but this number coming back, we build the orientation thing, by the way, is the way we think about orientation in brains. It doesn't take a lot of cells to represent orientation. It's not a sparse code. It's a, it's more like a compass direction. you don't take a lot of cells that represent the compass direction. But location takes a, it takes a lot of cells 'cause we're using sparse distributed representations and it takes a lot of cells. They're very sparse. So this would be less sparse. This would be very sparse in our theories. so you don't, see many layer six cells projecting back and that's consistent with orientation, but you do see a lot of layer six A cells. In fact, for a long time people ignored the distinction here because they saw all these cells from layer six A. They said, that's the main input to the thalamus. And these were much, much fewer of 'em. They didn't pay much attention to 'em. but that makes sense from an orientation and that makes sense for a location. and also the orientation signals would, in theory have more direct influence 'cause it's not many active cells, but here we're looking at a sparse code and therefore the individual cells coming back, which seem to do nothing and very little, but the code counts.

so yeah, I guess one thing we talked about at one point was like, if you had a second column with a hierarch, there we go with a hierarchical relation to this column. that like this one, this six B would, I guess be projecting to wherever its input is. To make sure that it's, it's direct retinal input is, oriented correctly. And then there's some kind of relative orientate orientation learned when you have a compositional object between this column and this column, like the output of this column, right? And, that, yeah, maybe that's stored in an L six B connection, right? But I'm just thinking if this, projection goes to a different neuron, maybe that fits with the possibility that it's going to this neuron somehow. Or like influencing what goes into, you're saying they're just segregated or something like that. Maybe I showed it segregated here. So yeah, I'm losing this. remember six B goes, sorry, should go through six A only goes back six A only goes back to the retinal ganglion cell. the th thalamic relay cells that project this to that column. So this projection only goes to the relay cells and project back up here, six B projects both to this thalamic nuclei and the next thalamic nuclei black. remember like it did be a higher order thalamic nuclei here, which is six A is projecting actually six A wasn't project down here, six A here only projects to this guy, but six B projects to both remember that was, and so that's true here too. This big D project here. So it's complicated. so we, Vivian and I worked out a hypothesis about how it's really calculating the relative orientation, which is what you need to learn because the object moves and the component objects rotate even further.

just being complete here. Yeah. Orientation projects to, to the, input Islamic region and to the next higher or Islamic region, where the layer six A only projects back to the input region. It's, that's very clear distinction. So this location, so we, it's complicated, but the orientation, if we want to calculate relative orientation and so on, that all kind of makes sense. But the location's only going back to here. So it's like somehow with this guy's telling this thalamic input, here's where I am now complicating this, is, we know that six A is a location. We don't know that the exact same cells that represent location are the exact same cells that project down here. there's lots of cells up in the layer six that are intermixed with each other. so although it's very clear that there's a whole bunch of layer six A cells projected back to the th it's not clear that the cells we're relying on for the ths are the ones, the same ones, although that's what's been reported. So here the way it looks here is, they say if Sherman and Gil will say, this cell layer six A, that projects back to the thous also projects up. To layer four. And, that is our, that is our model, right? And so it's saying, okay, location associated with sensory input, that's, our basic model, in which case that is location. And if they're right, that this is the same cell projectile layer four, that projects the layer al is what that, what they claim that would suggest that this really is the location signal and it's being projected back to the, thumb. Or at least it's associated with whatever's in th in, in the same way that like, but obviously this isn't, it's receiving location information, but it isn't itself location information up here. No. Like this represen, so this might be, it doesn't mean there's location on there, but the, but it needs to be it somehow. Somehow, right? It's like somehow I'm gonna learn something about the, location that's useful for me here. and, it could be different.

I've speculated that we've talked recently a little bit of speculation that maybe these cells are, very phase dependent. And so remember we talked about the set of cells in that all the same, movement orientation and they have different phases and therefore the phase determines what the location, but maybe you ignore the phase when you come down here. So then this, then if I have those cells, this is really deep. So I guess, if I have those cells that are basically all representing movement in the same direction, but there are different phases.

the, cells that are in phase with the, theta cycle would be the location. But if I ignore the phase, I just, then all of 'em are active at the same time. And then I'm, if I ignore the phase, then basically I'm just representing a movement direction.

and, why would I even know that here? I don't know. I just, do we know what the TRN is doing as opposed to the LGN? No, it's inhibitory, meaning it, the TRN cells themselves project back to the relay cells and inhibit them.

we also know that there is a, there's a complex sort of network up here. I believe some of these, neurons have what they call, roro connections, meaning that they, tied together through their dendrite, so they don't act like typical neurons. They more act like a mesh. and it is believed that they're involved in rhythmic generation, like generating different rhythmic signals so no one knows what they do, what the function is. but there's attributes like, oh yeah, they're generally inhibitory to these cells.

And they spread and they arrhythmic and they don't act like regular neurons put up. They don't project back to the, to the cortical. no. these UHT neurons just, they project to each other and they project back to this neuron. Does that show? Yeah. This is a, that I find this one interesting or useful to the things for what, how the, what you mentioned earlier, how it might be connected to the LGN neurons. So they put two different ways. It could be, and I'm not sure if, so this is the one that's showing the inhibitory? Yeah. So like the classical view was like the a where the layer six neuron goes and this is excitatory and it goes to the TRN neuron and to the thalamus relay neuron. And then the TN send a inhibitory connection to the same one in the thalamus. Whereas the alternative would be the layer six neuron sends excitatory connections to the thalmus and to trn neurons. But they are different like populations. So basically these that get input from the cortex then sent inhibitory connections to the surrounding relay cells. That's the idea that you're doing this column inhibition like you're selecting not only, with you're selecting the orientation transform for that one cell be inhibit disinhibiting other cells. Yeah. which I guess you were talking about capacity earlier, and maybe this is what he meant is just And also we were talking about before, like thalamus is pretty small. there's only so much information that can come through. And so like attending to, what is important by saying, okay, we're only gonna, it was process certain, when I was talking about capacity, I was talking more about the capacity of a cortical column. Okay. To learn, how many models can it learn? Like how many locations, sensory pairs, can it, can a rep learn like layer six, eight to layer four type of things and how many classifications can it learn of that? And we did some early modeling years ago and under a whole bunch of assumptions. So it's, it's what it's worth. And we found, oh look, an individual column with a reasonable number of neurons could learn, let's say 500 objects of 40 century pairs. Something like that, but that's not enough really. And if we think, so the V one can, it can learn a lot of stuff. You can't learn everything. That's one of the re that's one of the rules here. Even though it can learn complete objects, it can't learn all complete objects, but it can learn some set of complete objects. But it was still a question is, do we need more capacity? So I was thinking more of like the capacity of the court checks itself. If, we assume that every column is learning the same thing, then you, that's, it's not as sufficient, not as high capacity. If you learn that columns in general, there's a set of columns are active on, and then a different set of columns are active. So then you're distributing models on different models are put in different sets of columns. Yeah. So maybe it's almost more that's kind of competition in the long term to make sure that like columns, learn, yeah. Different objects. But then I guess the other is like competition in a moment in time, which is that like only certain populations of resell relay cells are active so that it's not just like cacophony. I, yeah. I guess I didn't think of it that way. I didn't think that the relay cells would have to compete with each other.

if they're doing what we think they're doing, just this, they're not, we're not asking too much of 'em.

we're just, we're all we're doing is asking 'em to remap the relay cell, the, relay cell, the remap, which input it's driving it. That's all we're really asking it. in general, I think of the th is not, it doesn't know anything about models. It doesn't, it's just it's just doing this transformation of input either sends me input on motor input, it's just rotating and it knows nothing about the models and the cortex. And six A makes sense. Six A six B makes sense. Six A doesn't make sense 'cause six A is model specific. If we're right about it, it's model specific. Therefore you're telling somebody specific about a specific model. And I wouldn't expect the thalami to know anything about a specific model. I think the thalmus is just a routing mechanism. you don't, the model should be up the cortex not in this little thing called the thal. So that's my problem we're sending down. If, the way we think about this location signal that it's unique to objects, we're sending it down to the thal, but the thal is gonna associate something, it's gonna be associating something unique to a particular object. And that doesn't make sense in my mind. I guess maybe since you said there are other neurons in layers, A like could be a different subpopulation in that area.

Yeah. Like one other figure that never made a like I'm not sure yet how to make sense of it, is this one which divides it into the power cornea and magnus cell pathway way. And maybe that's an interesting to look at, want to look at now in that context as well. basically saying, okay, upper layer six is the power, the pathway in. And that basically has the separation in the al GN as well, how like forward and in the backward mapping from layer six. I'm looking at this figure, see if it makes sense to my understanding. Yeah.

It doesn't quite make sense. Yeah. the problem we have here is that this is looking at primate V one, which has the strate layer four, which is unique. It's got four AB Alpha C beta. Most of the cortex doesn't have that. Most cor says layer four. And then many mammals don't have a strate like a dog and a cat. And dogs I don't think have a dryad one. Some mammals don't. And so this is like an additional capacity for vision only for certain highor mammals that, and no one has any idea why those different layers are there. So that's, this is not your canonical form, right? this is the, a canonical, this is the u unique system.

in my world, the Magna cellular cells, the ones are representing, they make sense to represent movement and the par cells are representing, features. And we can go through why that is. Again, so I'm looking at this and saying, oh, in my sense anywhere the Magno cells, mag of the Cells project. Should be representing movement. And they're not showing that here, they're showing only going to four C alpha, which I don't know what that means. Maybe that's the movement vector. I don't, I can't make sense of that. I'd rather see one where it's more the canonical column. Usually we talk about it going to layer five, four layer, the border of five and six, and the border of three and two. Those are the three classic inputs. And this doesn't show that. Yeah, I meant just the Mark Noella part. Wouldn't, we don't really think of it going to layer four as much as, I, we don't, I hope that this one shows it here, but layer four, the alpha, some people argued by the way, against neuroscience. It's just bizarre. Some people argue that this is a mislabel, that these aren't really different layer fours. That these are actually, different layer fives and different layer threes or, it's like this is the wrong labeling. You put four in front and you think they're all doing something similar. No, that's wrong. They say that was this historical accident that they did that. so I, I just throw that out because, we, say, oh, he's gonna layer four. Maybe it's not. Maybe that's really part of layer five and, and who knows what the hell. So I've tried, and personally I've tried to avoid thinking about these Stri eight, meaning striping layered V one, because then you have to deal with this crazy nomenclature that no one understands. it is, it's worth pointing out though, that what makes sense here is, the, relay cells. That are Magno and coio and parvo, they literally, they're physically aligned to project to the exact same part in the courthouse.

the relay cells here, all project to the exact same. they main maintained it topology. And so in my mind, if you think of this as motion and you think this is like a movement factor, and this is a sense feature, and we can explain why that is, then you would want it, you would want to implement the same orientation, transform on all three of those. All three of these guys, right? They're all being transformed the same, way. Like I'm rotate, my movement has to change and my, in my sense features have to change. So that, makes sense. and you have to keep them separate because one's movement and one's sense features, right? there's some evidence that coio is also movement, but it's, the, often they don't talk about the coio input coming from the retina. They talk about it coming from the, for calculus. I don't know why they show coming magnet. I don't know. What's the conal one exactly? It's just another layer.

they, like little cones I think is, but so there are, three basic, there are two big ones, par Magno, and then the Conal one is a little, smaller one. and also by the way, the, parvo Magno, are, subdivided by left eye and right eye. So if you look in the classic view of the cellular, the layers in the thumb, as you bring up Google, slaves, thalamic layers, you look at image, you'll see them. you want me to Yeah, sure. People help people. This is, I feel terrible for our new folks here. This is awful introduction to, it's so complicated.

just do Islamic layers and Google. You'll see it, the thalamic layers.

that's the real one. This is this is a real stand over the of it, right? LJN, that's the V, right? V one. And so you see, there's these layers of cell, these all congruent, receptive fields. So all these cells have the exact same point on the retina and the same point in the cortex. And then you can see this is left eye, a contra eye. Ipsy is the same eye, so these are the parvocellular. There's, one eye, the other eye, the other one eye, the other eye and mag cellular, same thing. One eye, the other eye. And, so they're coming in these, there's two retinal, there's two pathways from retinas, two retinas, and they're being processed all in parallel like that. and then they don't even show the cornea one on here. there is a cornea one, and they're not showing it. It's up here a little bit. It's a small one. So again, this is a classic neuroscience. These guys even mentioned the cornal layer, right? But the other figure there was, there, it was prominent right here. Here it is. It's like in between the, oh, it's in between. I thought it was mostly at the top and now they're saying knife was wrong about that. Here it's also oh, there you go. I that between thought it was, I thought it was one place at the top. So maybe that this is different than I recalled. that's more suggestive that it's doing something different. This this layering here suggested maybe it's modulating or somehow acting on all these at once or something. I don't know. Nobody even understands the cornal layer, so I don't, know what to say about it. Yeah. the main ones are the Magnus silo, which would be movement and the paril, which would be features and why it's divided in two here. No one knows. I've never heard an explanation for that. but it is left eye and right eye. Okay. Anyway, there you go. that's new for me. I didn't realize that the corneal laser interspersed between these.

so there's so many things we don't know. this is a, question that important for the thousand range project. It's bothersome to me that there would be a location segment coming down here because that means this guy would have to learn unique locations and it almost can't do that. It's almost there's no way it can do that.

That makes it bothers me. So and then if you believe Sherman Guillory that these are the exact same cells, projector layer four I, you should go back and look at that. Are they certain it's the exact same cells? Did they stain in a cell and see one up here and went down here? Maybe if it's the exact same cells, if is puzzling to me. And then that's where I came up with this hypothesis that, that maybe it is sending these, signals down here. but we're, but if there was phase encoding then it could be ignoring the phase and then it would just look like a directional sensitive cell. And the directional sensitive cell would be something this guy could use perhaps. 'cause it might, I'm not asking it to learn specific locations. I have a question. Yeah. So to make associations between two, relatively like independently moving sensor patches, let's say, two fingers or something like that, two fingers that has to happen in the follows, right? Why would I have an association between 'em, let's say every anchor the grid cells on one of the columns and I wanna send that signal to the other and that's a, that's an assumption that we haven't made yet. Right? you start off by thinking like how a single column works and a single column has to be anchor on its own right. And and if I understand that, I don't have to assume that it's telling another column how to re anchor this. So I don't, I want, I don't wanna start with that assumption, right? You could have two different columns. Imagine two different fingers. And this finger doesn't really have much to tell this finger, right? It is the orientation. This one's rotated independently of this one, and somehow we hope they all work out the same. so I wouldn't make that assumption. I would assume that a single column as many times it needs to be able to do this. and, and we need to work out the mechanisms for a single column. Yeah, I understand. Yeah. So yeah, just throwing out another, thought on the layer six, a connection, So could we maybe think that part of layer six A using a shared location space and then like we talked about last week, in, a subpopulation that shared location space gets specified to form a unique location space and it could, it only projects the shared location space down to it could. Oh, that's a lot of assumptions. And that, so it goes in the face. then you're saying I'm also learning a six, eight of four based on a shared location space. Partially, yeah. Partially, yeah. that would be like, okay, we learn the cylinder shape and it's rough features Like what Curvatures everywhere. Yeah. In a shared location space. But then the specific cylinder pull objects, like the coffee cup would be like a s specified version of that location space. it's also a little bit inconsistent with, the number of fibers coming back from layer six A to the th. So in V one, you might have a thousand relay million relay cells coming in, but you have 10 million life six A cells projecting back. That's a lot of cells for a shared space, right? Yeah. I mean it could be, I don't know.

depends on what, would I, what would the thalami wanna know something about? Why would, the thing I wanna ask is why would I wanna tell the thalami something else? What, other information could it use? I'm, it's, it is, we need to do two things. We, it needs to know, it needs to figure out the orientation transform. And I, we have a sort of a complete solution for that. So we're done. So what other information, it also has to figure out scale, and we haven't talked about scale a lot. Scale by the way, is when you know you have a composite object, the people, and you say, oh, the, object could be further away and close. That's different scales. And it could also be relative scale, like the size of this logo to the cup could be a smaller logo or a bigger logo. It has to learn that. and we, it also have to do scale. So it could be related to scale and maybe scale could be related to specific locations on a specific, object. that's something that you could say, oh yeah, on the coffee cup there's this scale, it has to be here. yeah, I guess yeah, I don't know if the anatomy's really consistent with this, but I guess that's why I was trying to bring up the. The relative orientation between a parent object and a child object again, because Yeah, I agree. This is a really nice solution for and, this doesn't change that the, for, calculating this relative orientation, but just in terms of okay, say we, we have a strong hypothesis we're now, or conviction that we're now on this object, this parent object, we move to a location and we want to predict the orientation of the child object. And bias that orientation, that's gonna be location specific. And we need to store that and communicate that some way. And thats what I was trying to think. Yeah, we do that again. Vivian say was it was a direct six feet to six feet connection. you guys hypothesized, which it could be. yeah. But it has found specific locations on the object, right? Yeah. You have to say for this child object on this location. Like I gotta have a coffee cup with a little logo and a big logo. Yeah. And a rotated logo. And a rotated logo. So I have to be able to learn both of those. and then, I appreciate the location of a lot of information, but if they really do have such like a rich dendritic structure and their main concern is orientation. Yeah. Then maybe that's what they're storing is oh, okay. Maybe it could be right? And it helps maybe that it's projecting to a different cell, but I don't know if it actually is this another poss possibility, another possibility. Getting back to our, this, idea of, shared location space and non shared location space. Now look at all across layer six a. I might have, there's a lot of neurons there and they could represent a highly sparse, unique location in space. But when I'm projecting down to this relay cell, there's these 10 milli million fibers coming back. But I'm only, the assumption would be, I, the assumption would be only a small set of, sub small area of this would project it the small area here. It's like there's a topological arrangement.

it, it might be that, that the relying on topology here, that the letter six days aren are converging in the same spot. Maybe it's more continuously spread out somehow. I, you show this again. That's how we did, how did we do this? Yeah. This is just the proposal for the learning, the relative rotation of the child. Object to the parent object. We didn't talk at all about layer six A here, right? but where, how do we associate the relative orientation with the specific location on the, parent object. That's, that was a trick here, right? We wanna say on a particular location of the parent object.

so we would send the relative orientation of the child to the parent up here, to the like lower layer, three. Yeah. Four to activate these columns and then Whatever feature goes in there would be associated to the location. Oh, That's it's done up there, right? That's right. It worked. That worked nicely. I can't remember. I'm so glad you took this picture. You made this picture.

I don't wanna, again, as with the first question, I don't wanna get hung up on this. I don't want this to get impeded, Monty, and it's progression and all this stuff. it is bothersome to me to not understand this. Excuse me.

It is bothersome to not understand this. and, lemme just send him a message.

I feel like maybe in terms of one potential takeaway for, Monty, 'cause like a lot of the transformation stuff that we need to do, we're already doing. Yeah. But this idea of like maybe thalamus is creating a more competitive environment between the columns is something that at some point we could consider, right? if we're feel, if we're finding that all the columns are just learning the same objects. they would, unless we did something like this. Yeah. Then, yeah. And it doesn't need to be, we don't need a th this, but just having something that's If we have the capacity issues. 'cause some people, some astute observers when they hear me talk about this, I've had this customer maybe twice, some says, what about the capacity of these columns? how do how many things can they learn? and it's gonna be limited. They can't learn everything so that, we talk about V one can learn complete objects. they can't learn all complete optics and, it's only gonna learn ones that are small areas of the retina. And, but if it's still named enough, then we might have, we have a way of expanding capacity. But maybe again, in, in Monte we may not have that issue. 'cause we can build, unlimited resources into anything we want. but that's a good takeaway. I'm still, I'm ultimately, we'll have to do with the scale issue. We'll have to figure out how scale is represented and how it determines scale per location. it might be related to that. one of the reasons to suggest that is that, remember if this is location and this, the TRN is associated with various, oscillations. I don't remember the details, but that you could be saying, I'm selecting a particular oscillatory, scale here. I'm, telling you what scale to do. I. based on the location. So maybe that's, this is time orientation and then we're gonna use this for determining scale somehow. I don't know. Yeah. So I think, one thing that's currently different in the multi implementation too, how we talk about it in the brain is that in Monte we do the, the transformation of taking the current location, adding the movement, and then getting the next location. We do that outside of the cortical. Yeah. And then give the column the next location, whereas here we're saying the column gets the movement vector and does the location transfer? Yeah. But is there a way to think of it that actually happening in the thalamus? So basically this layer a six a cell tells the thalamus, okay, I'm at this location right now. The thalamus gets the movement vector and does the path integration. I mean it, yeah, I know there's a bit of a crazy idea, but it doesn't sound there, it doesn't seem like there are enough cells in the thalamus to do path integration. Yeah. it's just a simple structure. Yeah, there are. Okay. so let's be clear. If you look in the, if you look in the, thalamus, there are basically two types of cells. There are relay cells, which we've been talking about exclusively. And there's the TRM that's separate, but there's also something called matrix cells and, matrix cells.

I have always been working on, for many, years, I've been working on the assumption that these are the cells that determine timing, for sequences and movements and so on. and there's a lot of evidence of accumulated over the years that's, that would be true that these cells, I think they project a layer one and they essentially could be priding a timing signal. if you're trying to learn the, durations of elements in a sequence, like the duration of notes and a melody that these guys would be, like a little, count countdown clock that says, okay, beginning now you can count about a second and then you have to reset another second. And so on. I mentioned that because there, there's more, right? There is a other cell type in the found. these are very different than relay cells. They don't get, as far as I recall, they don't get any input from, the sensors. they're more like, they get input from the cortex. It's like the way I think of me as they're told like when you start, if you listen to normal melody notes always have an attack the first beginning of the note.

and there was some evidence that these cells, become active at that attack and then they play out a timing signal, which then gets sent up the cortex so the court could learn the duration of individual elements and sequences.

I mentioned that 'cause beyond those two, I don't, I'm not aware of any of the cells in the ths. There might be some, but I'm not aware of. So what were the cells that would do path integration? Oh yeah. They don't look like they'd be there, by the way, in one of these other papers.

did I print that one out? it's on my computer. I think it's on my computer.

yeah, it's on my computer. I could show it to you.

where was I gonna go with that? why'd I bring that up?

yeah, it could be some interaction with the T rrn and that one inhibiting some of the cells and LGN, but yeah, Totally different mechanism than the grid cells, so, I would thought, I always sort, the general rule of the thumb was in my book was always, it's generic, meaning it doesn't know about specific objects, and it can do orientation transforms, it can do scaling, and it can provide a timing signal and it can modify that timing signal. Because the trick here was sometimes you want things to be sped up and slowed down, just like you wanna, you wanna be able to scale time. You could scale space, but you scaling, this is a scaling of time. So you could change that rapidly. This, signal, to, when you wanna, play back a melody faster than you learned it, these cells would speed up their timing signal. So those are, these are things that apply to all models. It's like. Timing mechanism and a transfer mechanism and a movement, transformation of movement and, scale. So this would be scale for timing. It might work for scale for other things too. but so it's it's a generic, it's nothing specific about any particular modality, any particular, models. That's how I do it. So I don't think there's other cells in here that could do like path integration. It doesn't seem to be enough cells. Yeah, it would have to be something in the T-R-N-T-R-N is pretty's pretty weird. Yeah, it doesn't look like it would be doing that. The TRN looks like it's some sort of, they often refer to as an intentional mechanism. if something occurred, that you didn't anticipate, it would essentially open the gates to other cells. So for example, in this example where you have it, you don't have it up there anymore, but the one that showed the thalamus where the inhibition. I can open that.

You had it there. I had it on the other laptop. oh.

not that one there.

one of the things I did is just that on B there, you're inhibiting these other relay cells nearby, but when you do, they're in this, they're in this, pre thirsting mode thing. So if an input comes in that's unexpected. Those, outside green cells would burst. They may fire rapidly, and then they reset everything. And it's like a, it's like an, if I'm, attending something here and then there's a little flashlight off to the side, I lose my tension. I move over there and attend to that so that it's associated with that. And so the relay cells, the trn cells, the trn are associated with this, resetting of attention signal, like you, if something unexpected occurred, and then you have to attend to something new as opposed to what you're attending before. So it's a lot of literature about that. That's interesting. So could that be a clue as to what the L six A signal is like that the location signal somehow incur, if there's a sudden prediction error in the location space?

In fact, that's what most the, a lot of people have speculated in, in, in the predictive coding world, that's what's happening is that the cortex is telling the thalamus what to expect. And if the th doesn't get what it expects it, the cells burst and it attends with something else. So what I have a problem with that is I don't, I could imagine the thalamus being told what feature to expect, here's a feature you should be expecting, this line or something like that. but I have trouble explaining how it could say I. Why it would be specific to an object, right? It's like it, the specific to the object part is it doesn't make sense to me.

I don't wanna spend too much time on this. we've probably spent a lot of time on it. I don't know what we have time for more stuff.

this was an interesting paper because going back a little bit, layer two three in the cortex are often just lumped together, layer two slash three. And, people didn't really know why, and some people talk about, okay. Up there. Yeah. Okay. Want to go to a figure? no, maybe in a second. yeah, go to the figure one. This is a very misleading figure, as I'll point out. Very misleading.

So, in our, models, we've looked at it like this. We've said, we have essentially layer six A and layer four are bi directly associated connect to each other. So this is a location, and this is a feature, and you're essentially associating features of locations on optic as a basic model. Then you need to form a representation. Sometimes you don't actually have to do this, but sometimes you want to put a label on this object, like what is it? Then you'd have, another layer, which we've always said is like layer three. And this is a, what we would call, we call a temporal pool. Basically it says, I take all these associative pairs, like every, these are lo, these representations represent a location in a feature. And I say, and since it's unique to the object and every one of these representations is part of this common object. So this would be the object, and this would be the, location feature pairs.

And so an object is a bunch of location feature pairs. That's it. And so this would be like a stable representation while these are changing. So it's oh yeah, my finger's moving over the coffee cup, but it's still a coffee cup. I know it's a coffee cup, but it's coming up, but is not, my perception is it's stable. And this layer three cells project to the next broadly. but these cells don't, so basically you're aware of the object, you're touching the object, you're sensing you're not really aware of. Unless you attend to it, unless you make a point of truly trying to notice it, you're not really thinking like, where are my fingers on this object and what are they feeling right now? you just pick it up and say, yeah, okay, I'm just holding my coffee cap. I, I don't say oh, my finger is on the little lipo there. You can do that, but generally you don't do that. So basically you're, I've got this object, and that's on a basic model, and these operations have to occur. You have to be able to do this. This is like a, this is like what inference is. It's figuring out a common representation for a series of changing century motor inputs.

so that's that. And this becomes the input to layer four in the next higher region. and there we would, we, in the paper we were writing that saying, okay, this ob this object now is a child a feature of the next object up in the hierarchy. All that really looks nice. so it makes me feel good about that.

but what about layer two and three? Because they're not one 10. So this paper that we see on the picture up here, what they did, they were trying to measure the physiology of layer two and three. That is what do the cells respond to? They weren't looking at the anatomy, so that is, they weren't looking at what layers two and three are connected to, but they're looking only how they respond differently to inputs. And what's misleading about this figure. This is an anatomy figure. And they don't, show layer three projection to layer two.

And, in the paper they make it, they, point out, they say, these inputs to layer two are not well known and we didn't show them. They may, come from layers three, four, B, and five, So basically if you look at this figure, you would think that layer three is not connecting to layer two. But here they say, yeah, might, we don't know. We're not really paying attention to that. And other people have clearly stated that layer three projects to layer two. So if you just start off with this paper, you get misleading representation of what's going on. This is like classic neuroscientists, ignoring a whole bunch of other data makes this picture, which would confuse you. So we don't really wanna look at this paper, this figure. This is really misleading. and it's the first figure in the paper. so you go down here, keep going. Let's see. yes. This, that one's important.

let me just talk about what's going on here. So the upper line is a layer two response property, and the bottom line is a layer three cells. Okay? And what they have is a, monkey is sitting in a chair, its head is fixed. They're displaying something on the screen. The monkey is trained to stare at a dot on the screen. He's thirsty and he doesn't get water if his eyes move from the dot on the screen. So the poor monkey, it's terrible. And and then off to the side, not where he is staring, but off to the side. Are cells. There's a part of the visual field where they're measuring these cells. These by cells is responding to that part of the visual field off to the side, and they put a sinus sort grading in there. They basically put lines moving in a particular direction and in another direction. So here's the big difference here. There's, a couple things gone here. These cells in layer two and three, the ones that they're showing results for, are direction, are movement sensitive. So the cells respond when there's an orientated oriented line moving. So you can see above it, there's like a long line and a short line, and they show little arrows on it, right? So there's, imagine there's a, it's not just a line, but it's a sinus salt grading that's moving through space, and they're showing how wide the line is. The cell prefers a width of a line. So how wide should that be? So you can see layer two prefers longer, much broader things. And layer three pre prefers much shorter lines. But the real big difference here is that in layer two, the cells respond if the line's moving either direction. So on that top line, there's a big, the every half a second, the cell, or maybe every second. I think it's every half a second, the cell, no, every, second, every, half a second, it changes direction. So it goes half direction, one direction, half a second, the other direction, half a second, another. And you see the layer two cells respond in both directions. So if it's not the, once when the cell starts moving, it goes, oh yeah, blah, blah, blah. These, those little vertical lines of the cell spikes. Okay? so layer three cells respond in both directions. A little thing on the right is just indicating it's a, plot showing how they respond in two directions, and then the layer three cell responds to movement, but only in one direction. So when the lines are moving, one way it responds and lines are moving the other way. It doesn't respond. So this is one of the primary results of this paper. also that the layer two cells are much less, they're much less picky. the line can be really big, it can be fuzzy, it's not very precise, where the layer three cells seem to be very much more precise. I need to have this skinny little line, this exact orientation, and it has to moving in this direction, and then I'll respond. Okay, so couple things come out, this, which are very prob problematic for us. One is, if these, if layer three cells are the inputs, the next region, if he is actually measuring for whoever they're scientist is, if they're actually measuring the same layer of three cells or projecting to layer four, those cells are only motion sensitive only in one direction. What kind of, that's not what I just said. I said these would be stable over, over, multiple movements of the, object. and I have some thoughts about this moment. then, layer two also. And sometimes there's evidence layer two also, and they showed it that layer two also projects here sometimes. So I'd have layer three and layer two and these different things going on. What the hell's going on here? Why is these motion sensitive at all? We don't want 'em be motion sensitive.

so before we get all bent outta shape about that, there's another paper I wrote read, which discusses an issue we've talked about a lot, which is the difficulty in figuring out what cells you're looking at and why you're looking at certain cells. So they take the monkey and they show moving sign soil, grades, it. They don't actually see what happens if there's an image. They only show movement. So they're only gonna find cells that respond to movement. These, they have to look for these spell cells. They're not easy to find, so they find one, they go, oh, this one's responding. But the vast majority of cells aren't responding to these inputs.

So this, remember that? So it's, it is they don't say that. they do, but it's hidden in the data. so this could be very misleading, right? There are, there could be lots of different cells up here. we've talked about the possibility that you're doing path integration up here. That is object is moving. Not in space, but it's, it has its own behaviors, like a, a staple opening, closing, that, that might be represented up here. And these might be related, the path integration in object, motion object behaviors. So it's very misleading. but I still thought it was interesting that they were able to find cells that the distinction between layer two and three is one cell responded in one direction, one spread in the other direction. And this gets, really obtuse because in the path integration, I'm sorry for other people, you're just not gonna understand any of this. The path integration, technology we've talked about, you really want, you really like to have cells that we, that are phase encoding, but moving in both directions, not just one direction. So this was suggested like, oh, maybe there's a path integration. Maybe layer two is doing path integration and it's really representing the location or, this path integration of, movement of components of, an object. They would en code like the state of the object we call the, you'd have to, you'd have to, it is encoding the state. But yeah. it's like it would be the predecessor to the state of the object. It'd be like saying, okay, the stapler is opening, the staple's not moving relative to me, but part of the staple is moving relative to the other part of the stapler that is a different than me moving relative, it's not the same as the staple moving through the world or me moving through the world. And so these cells who might be doing the path integration of that component, it just. Gate of its cage. What's that? What The gate of its cage that is learned. Oh, the cage. Yeah. The monkeys generally lives in a cage. Oh. Oh, maybe. he doesn't get, the monkey doesn't get to see that monkey just looks at a screen and looks at a dot all day long until it's, they say, we did this for about two hours until the monkeys satiated. So they get the monkey really hung thirsty life. and then it gets a little drop one, little drop, and it lasts for two hours until he, once he's no longer thirsty, stops doing at the, dock.

I just thought it was interesting. We talked about layer two and layer three and we didn't really have, what's the difference? We talk about classification. there are clearly sets of cells up there that are most insensitive and there's a differentiation between layer two and layer three. And I thought that was interesting. I'm filing it away in my head.

as oh, this is a clue to the idea that there might be path integration occurring up there relative to object behaviors. That's, that was, which I've speculated for a long time. That's where it should be. That's the lower input to the lower layer three, upper layer four that would be perhaps motion related. and so there's some evidence. Okay. Yeah, it's, processing motion and it's probably gonna do something, although it doesn't make sense to send those, I don't wanna send a. That lower line signal to layer four. If this is supposed to represent the object, I'm pretty sure it does. Then I'm, saying that there's probably different cells in layer three are projecting to this layer four than these cells. Sorry. But when sending that signal, I guess movement itself, is that kind of the object state that we might wanna send to like at the object level rather than, they might be seeing it when they do some sinusoidal gradients or whatever, but if this is more kind of encoding the movement of the whole object, this would be movement. me, this would be a movement of part of the object. I, okay. I was just wondering 'cause if it's movement of the whole object, that might be more useful to the next layer. moving the whole object meaning like through space or meaning it's changing or like changing, oh, okay. It could be, yeah, like behavioral state. then I would give, this is basically any kind of change to the object itself. Yeah, it's closing or whatever. Yeah, whatever. Morphing it's word. Flopping its wings. yeah. the point being is that you see that the, little, the line down there, the, on the B one, you see how it's restricted to a small dimension. Basically, if you, showed the whole field of vision moving, these cells wouldn't respond. They don't wanna respond to the, you moving to the world. They wanna respond to some part of the world that's moving and the rest of the world is not moving. That would, that's telling you, it's focusing on the object itself. That's a suggestion of that and not the whole world. And if you look at the cells and layers, the lower layers, they respond to very broad areas, of course, their vision. So that's more like motion to the world. It might not even be doing the path integration itself and layer two three, but rather the path integration could be happening in the lower layers, but then layer two three would just encode the state of maybe, but, just changing. But wouldn't you need to, I don't see how you could do in the lower layers because it's a, different, it's not the same mecca. one on path integration. My, I'm my location and space, like my body's moving. And that has to be maintained separately than the objects moving. So I'd have, I can't use the same mechanism for those two, right? Yeah. But you could have, so first I'm not sure how we would do path integration in layer two, three themselves, but like a way to, you could do it, in the lower layers would be that like each state of the object, is it like a different model, but then you, learn an association between how to move through the different states of the object? Yeah. I don't, if you think about the stapler you want, you could argue that the higher region would be looking at an object composed of two components in the lower region and the relative position. And those two components are moving relative to each other. It might, take a hierarchy to do that one. Yeah. But still, I feel like the relative locations in which those two parts of the stapler can be in, would be encoded in the layer six A to four, connection. maybe or it's really, it's more it's the, it'd be more a we, that's what we were talking about earlier, that we have to learn the relative orientation of two components. Two objects. It'd be more like that relative orientation is changing. I have to learn a sequence of relative orientations. Yeah. I'm just saying it like, we don't really wanna have to need grid cells in layer two three as well. Yeah. But why, what would I do with these cells? Why are they up there? What are they doing then? it would make sense if they just encode the state of the object and that state changes as the object moves. So if it encodes the, this state of the stapler, which is neither completely open door, completely closed, then it fires every time the stapler goes into that. Yeah. But it wouldn't, but once it stops, they would stop firing and therefore they wouldn't encode the position. They would only, they only, encode movement, relative change of that position. These cells would stop firing if the, stapler top stopped opening. Yeah. Okay. Yeah, that's a good point.

It might be part of the state of the object, whether it is currently in Washington, but yeah, I don't know. I don't know either. I we're for the newcomers here, we're bringing up all our dirty laundry, all the things we don't understand, but we understand a lot, trust me. but the research meetings, nothing to go over what we understand. Oh, the things we don't understand. Yeah. I'm just trying to figure out a way to explain this without needing like grit cells and location representations in layer two, three. I guess since these are temporary, these particular cells wouldn't fire the, if the object stopped moving, they then they seem like they couldn't represent, they couldn't represent the state of the object. It can only represent the movement of the component of the object. there is no, the, monkey never sees the, rod, stopping. It only sees it it stops, it does stop at the transition points. It reverses direction and sells it. Yeah. but the event will be, or the state of the model will be Yeah, in motion always. Yeah. I'm pretty sure, I'm pretty sure, I'm not a hundred percent sure Ram, but I'm pretty sure like 98 certain that these cells would not respond to a static image.

I'm pretty sure, it would respond to motion only, but I can't be a hundred percent certain about that, but I'm pretty certain about that.

so again, there's lots of cells in these, places up here, and they picked the ones that they're recording from based on. based on how they set up their experiment, oh, fuck, your husband, your baby might be important. Yeah.

anyway, I just, it was more like interesting data. It was like, oh, I never saw this. This is interesting. And it's what am I, make of that? who knows? Can't figure that out yet.

All right.

I just, these are things that I'm file away for some future.

under future discussion, for the newcomers. There is this concept that we opt opting out. All we've done is money is recogniz static objects, the movement sensation, but it doesn't really dealt with the issue of objects that are actually moving themselves and have behavior, I call it behaviors. And they, therefore you have to be able to say, a bird's wings out is still a bird with his wings in, and if it's moving him back and forth, that's a typical bird, but you wouldn't see a bird moving one wing out back. That's somehow, that you've learned that, right? So the model of a bird has to include behaviors, or the model of a staple has to include behaviors, that typically observed, opening, closing staples come out, various things like that. And it's a pretty important part of century model learning because so many objects have behaviors and we push and touch and manipulate them. and we don't really have that worked out yet. we do have oh, how does it recognize objects and multisensory touching and same seeing and things like that. Okay. I don't think the problem, there's answers to these questions and we're getting close to 'em, but at the moment it doesn't, may not seem like that.

and then I have one other, unless there's one someone who wants to discuss this further. I just thought this was interesting, this paper.

let me see. There was this paper called about concept cells. Is that the one I have here? Yeah.

Yeah.

this, okay, so just, what's this paper about? this paper is about in humans, in their hippocampal complex at the hippocampus and an cortex, which is called the medial temporal lobe in humans. So they talk about MTL, but that's the same as hippocampus and in Toronto, cortex and rat. So you have to know that, so there, and humans that are awake, 'cause these humans, are in the, are, have electrode planted in their heads because they have, intractable epilepsy and they're gonna remove part of the brain to stop their epileptic seizures. And so for five days they go around with this electrodes in their head so that they can, the doctors can try to figure out where the epileptic seizures are starting. And then they know where to remove tissue. So during those five days, the humans agree to be subjects and experiments.

so they have human data. Maybe I should project from this. can I project easily? Oh yeah, sure. the human data for this, and they can measure from individual cells or a group of cells at once. That's what we're talking about. And concept cells is a name for what some people refer to as a number of years ago.

people found during these experiments, like the subject is the, got the electrodes in its head, and those electrodes are in the temporal, which is deep in the brain. in the, in Toronto cortex and Thalmus, the hippocampus. And, they find cells that respond to concepts. Like the famous one was Jennifer Aniston, I guess is an actress. I watch tv, but she's what, friends or something like that? Yeah. Yes. Okay. So friends, and they say, oh, there's a cell that responds to Jennifer Aniston. and not only did it is not just a particular, it is like anything to do with Jennifer Aniston. you spell her name out and that cell response or different positions or, So the name written and stuff, right? So it's oh, Russell, these, they call these concept cells, right? And, so this was about, what I read about this here, it's down here. So this was about that. It wasn't particularly interesting. They're showing in the human, this is where the hippocampus is and Tex.

and but there was some, what was interesting about it is like the whole idea of a concept tells like a grandmother neuron. This Lauren just responds to Jennifer Anderson. That doesn't really make sense. You, have to have a lot of spell respond to Jennifer Anderson because you can't assume that one cell represents Jennifer Anderson. That's nonsense. No one believes that. And then how many different things can you encode? And this, gets to the nature of sparse distributed representations. 'cause we've argued that most of the things that when you represent object, you're doing this very sparse code. The individual cells don't really, they'll always respond in the same for that object, but they'll also respond to lots of other objects too. so if it's like a 2 sparsity, that cell should become active every 50 things.

and so the que and this paper got into this question, you, I thought there was some interesting piece of the data here. Data.

so it says here in a, population of a billion neurons, less than a million. Are involved in the representation of given concept such as Jennifer Anderson or Halle Bailey.

and each of these MT learners may code up to a few dozen of the 10,000 to 30,000 things a person can recognize. So they're saying, okay, so maybe these things respond to maybe a dozen different objects. That's not enough, right? That's just not enough. it, doesn't, the math doesn't work out. But then it says, however, both of these estimates should be taken as upper limits. The true values may be a couple order managers lower. Okay? That's, it's a big, that's a big caveat. A couple order managers lower. and because it's difficult to select very selective neurons. So let's go down to this box here.

they're just talking about, if I'm looking at a particular neuron and they're trying to get to see what it responds to, the vast majority of neurons seems silent. They don't respond to anything. and so they just figured they're not doing anything. But in our role, those sparse distributor representation, they would be, you just didn't get the right stimulus, right? So here they say, they're talking about individual neurons here, and they said, there's three neurons. They're looking at here, these blue, one on red, one green one, and they say the third neuron fired only. These two fired a lot of spikes. 6700, 1800 over some period of time. The third one fired only 200 8218 spikes during the 30 minute recording. So they're recording for 30 minutes showing all these images, and they only got 218 spikes outta 30 minutes. That is like noise. That's so little, right? It's very little. And, yet they were able to, and they would, and then they could show what this, what that cell seemed to respond to. responder these two different towers and some rabbits and, so I thought, and these cells responded to 40 times more. So what they were basically arguing in this paper that the data that you typically get from these experiments, they try to figure out, pick, come up with things that someone would know. oh, they watch tv, let's show 'em friends characters, or let's show 'em pictures of their family, or show 'em pictures of their researchers. These are people they know intimately and they see if they get cells to respond to it. But it turns out maybe most of the neurons are not. That's, really biased data because most of the neurons are still representing things in the world, but are extremely sparse and you just wouldn't know it and you don't see that. So I just, Neils and I have had this debate a while about, our neuron, we as a group, we've talked about this, but they made some good arguments in this paper why it's very difficult, to really get a proper representation.

let's see here. Another one's here. I'll just go back up to the top here.

Just in images where concepts that are very familiar to the patients. For example, pictures of patients themselves. Alright, so they're showing you your own picture, family members, experimenters and celebrities to increase the probability of getting a response. Indeed, personally relevant items are shown to elicit the largest number of responses. and most of the things tend to 30,000 things the person can recognize may not be represented in the ETT at all, as these may not be signed enough to trigger a memory process that may not be true. Basically. They also, later they argue, they might be representing the MTL me temporal lobe, but, but you're just not gonna find 'em because they only spot, spike every, 200 every 30 minutes, spikes. Anyway, there was some interesting, if you're interested in this topic, there was some interesting data in here. That's what I thought it was like, oh, this is interesting data. They talk about the problems of detecting and the bias that they have. They, so the, when we talk about, oh, they find these cells, they seem to respond to concepts of very listen, that's the exception. The vast majority of concepts that things you could do, they wouldn't find, they're not very familiar. And they gave an example one cell that seems to expire, 218 times over 30 minutes and it represented was fine for shipping things. And they basically said, we were lucky to randomly find something that made that cell fire. Most cells. They said you would find something that would make it, yeah. yeah. So just to clarify, yeah, they're arguing basically that, they're not claiming that it's a Jennifer Anon like grandmother cell. It's just It's still a distributed code, but the only thing they found that this cell responded to was, happened to be Jennifer Ton. and obviously there's also arguing that, and there'll be many other things, But they also, argued, and there was a nice figure here.

is it in this paper? I think it's, yeah.

This figure here.

so this is interesting. So the, this is, these are cortical regions. The pink one here is V four, and this is upper auditory cortex, right? So these are places where you already expect objects to be recognized in these two cortical regions. And they say that, they say the cortex already has representations of Jennifer Aniston or anything else like that. And, then the blue ones are the media temple of lobe. So this is the, an Toronto cortex, the hippocampus, and the, these two things parallel. And these are like three steps in the in, in processing. so you're already feeding in, you're not feeding in raw sensory data. You're feeding in representations of Jennifer Anderson already into this thing. So our argument we've talked to in the past is like the comms themselves, they rep the mini comms they represent, you might have Jennifer Anderson Minicom type of thing. And it was like, 'cause it's so highly represented anyway. It is, it's just they ignored that. They said you're starting, you only really get Jennifer Anderson up in the hippocampus something. But anyway, it was interesting data, but I thought this was fascinating because our basic theory is that this lead, these blue things are the predecessors to a cortical column. These existed perhaps in older animals, and they were just the ENT around cortex and hippocampus. And a cortical column is basically a collection of, is like similar to this. and, onic cortex is location, hippocampus is like location cells, like object feature cells. And, so they basically, the cortex has its own model of Jennifer Aniston. And then it feeds that into this blue thing, which is basically learning, temporary memories about Jennifer Aniston temporary associations. oh, I saw the, I saw horrendous movie yesterday. anyway, it was really interesting how they talked about why you might not see why the data's biased and why they would not see typical representations, because the data is so biased. and so it's just a warning, that we've talked about. And I just, it was nicely written about in here.

I'm not gonna, I'm not gonna try to read all this behavior.

as mentioned before, our concept cells do not act in isolation, but as far as cell assemblies, they're, this is, Neil's, this is what Neil's wants here. So here they're saying, this is speculation, this is, they're speculating and this is what you are speculating, Neil, is that I have representations for Luke Skywalker, Yoda and Darth Vader. But some of the cells are common in those representations so that when you see Luke Skywalker, you get some of the same representation as yoga. And, so these are all Star Wars, things. And the be, it acts like it, but they didn't walk through the numbers to see if it actually works, which is, which the problem I have with this. But they're arguing this might be what's going on, which is what you've argued Neil, that there's these overlapping bits. But they didn't, in my opinion, they didn't walk through it in detail. They just threw it out there.

And I, yeah, to be fair, my understanding, that's also something you meant to use to Yeah. I argued for this. I get talks on this. Yeah, I know. And I think even still, does he still do it? Yeah. 'cause he is SDRs should have this kind of Yeah, but I dunno how it works. Maybe I, one time on it. I used to believe it. I don't believe it anymore. Maybe I'm wrong. I don't know. it's, it is difficult to. I can walk again through the difficulties, but here, the making this is a picture of what you've argue might be interesting. Yeah. there's a nice drawing of that. Yeah. Sort of.

so this paper talks a lot about this stuff, so I thought that was interesting. Yeah. Thanks. Yeah, that's it.

and again, lots of these are papers that I thought were interesting numbers. just throw 'em out there because course the stuff that's worth keeping track of. Okay. Just like an interesting curiosity. I don't think it's necessarily that significant, but, there was a paper a couple years ago that basically showed similar things in, these like transformer networks from like open AI and stuff where they found like a Spider-Man neuron or a Trump neuron that would activate to like Spider-Man written down the Spider-Man, logo, Spider-Man images. It was it, were those sparse representations or were they, so I think they, I think they argued something similar that it's it's not like this Neon is only active for this, but, but yeah, they were relatively sparse, I think. because the problem I have with this cons, this coating, this is when you get very sparse. That's the problem. There aren't enough neurons to do the overlap. Yeah. I guess those are real value neurons, so maybe they have higher representation capacity. Yeah, they're, all kind, the real value neurons and they have very specific real value synapses. And I don't you, they're not like spars, you're not really doing a sparse code. You're or are, I don't know. Seems like real value neurons. You're not relying on the sparse code.

yeah, this was a regular clip.

they, they, show a, model, an image, and then they show it the text of that, image. Like captions of that image. Yeah. And then they map those two representations together in dense space. And then, so they're real valued and they then, they're not really sparse coded, but then they learn these, this mapping, and then they look at the individual neurons, what they act, what output they activate at the end. Or actually they go backwards. They go from, from an image. They look at the representation, then they go backwards, and see, what, neurons are responsible for activating that image. And then they find, oh, this neuron is responsible for all of the same concepts. yeah. it's, we've talked about, write this down. Yeah.

there's different ways can represent stuff. And, and, this may or may not be useful, write. Is it? Okay.

That's something I think it would, be worth it sometime to have an explicit session just talking through this idea of SDRs encoding meaning and Right. Come to a conclusion if this is something we should keep pursuing or, yeah, good idea. There's a lot of that in, HTM school as well, so we don't need that. I know it sounds is so great. I just, so I thought, we're trying to, one of the big questions you have is how do neurons represent information? And the plastic way of going back a long way ago is you start off with firing rate. meaning the, the neurons spiking and the faster spikes, you it's a graded signal, right? And there are lots of examples of that, like a neuron that's integrating a muscle fiber. It's basically a, rate coded neuron. Basically, if it fires faster, the muscle pulls tighter. And as far as less and there's lots of parts and there's lots of neurons in the old part of the brain releasing chemicals and so on that, that are firing rate. But firing rate, it is hard. that's one thing, right? But we don't believe that's happening everywhere. the, another thing you could do is you could say, a an alternate is you have a sparse distributor representation where the, so you might have a million cells a. 2 are active. So you know, you have 10,000 cells active.

that's a lot, but it could it could be 10,000 and, and a hundred. But the point here is that the actual rate of the firing dal difference, it's which cells are active at the same time. So this has a very high capacity, meaning you could represent a ton of stuff, capacity, unlimited number of representations. And, it has all these other desirable attributes, which Sumati wrote about in a neuron paper. the noise written tolerance, robustness across, it's amazing. It's an incredible, and we know that in the brain or in the cortex specifically, there are lots of sparse representations. So we're pretty sure that's happening. And some, it's happening in the cortex for certain, but it doesn't mean everything is in the cortex. It's working that way. we've also, we also have a, another idea which is there are the mini columns in the cortex, which is, this would be like, you might have 10 mini columns representing different orientations. And, what you have is three active at once. And you're just representing like a 360 degree orientation and you're moving it, it's like a three of eight type or three of 10 type of representation. not high capacity. it's not unique, but it's simple and you can do path integration on it, and it suffices for many things. And so we've, our basic neuron theory combines this, plus this to form sparse representations. You start with a non sparse minicom representation and you sparse fire by picking individual cells and it has all these other properties. another one that we've learned recently, is, representation that was before.

this is the phase using phase. So this is the idea that you see in place cells and grid cells where you have a, you have a, a representation, which is, it's in, in between sparsity. it's not super sparse, like the grid cells and place cells are not particularly, they're sparse, but not like super sparse. so you might say semi spars, representation, but they, the same neurons, different cells fire different phases. So you, so while you're, if you're looking at grid cells in in a, single theta phase, meaning like a, be a hundred milliseconds, something like that. you, go through a series of representations in order. So you get, you're packing in a bunch of 'em, right? In order. So you might have, representation one and then representation two and representation three. And these occur very rapidly, because it's the same neurons, but which ones are in, which ones are in phase, where the beta, the theta cycle changes at any moment in time. So you have a series of reputations in time, and this is clearly happening in parts of the brain. and so this is oh my gosh, could this be happening everywhere in the brain? Could it be in addition to SDRs? Could you also be doing phase versions of SDRs? That's essentially what this is doing here. So the decision of which cells are attached to which phase is that hardwired enc coded in the, genetics?

I don't think any, I don't know. I don't know if anyone knows that question. it, this has, this comes about from sodding grid cells in place cells, right? And they see this phase procession, this for the new people here, an animal moving in a, in an environment. And there's a, there's, a representation of some location in that environment called the play cell. And when the animals there, that cell fires, right? there's other cells that fire here in here. So as the animal approaches, it's not as it approaches. This one first comes in phase with the base frequency. So at first, before you even got to this one, it is saying, oh, you, this one's active. And then as you, then, as you actually get there, this one becomes in phase. And this one's active, meaning it's in, it's a coincident with this. And then after you pass it once you're past it or this one becomes active, so it's like it's coding a series of locations in order as you're moving and they're going boom, really fast. it's, it, would be a little saying if you were decoding these and you're looking, trying to code these, you'd say, oh, I might be here. I might be here, it might be here, I might be here, I might be here. I've always thought this is likely to, be way of, counting, noise in, in, in, path integration. I, I don't know if I came up with that idea, if someone else did, but the idea is you don't always know exactly where you are. So if all of a sudden you, this matches your sensory input, then you say, oh, I'm actually further along than I thought I was. And, I have adjust, right? it's like I, they walk in the dark in your house and I'm expecting this and I do this all the time, and I in the middle and I go to the bathroom, make sure I don't even close my eyes, and then I walk along, oh, I should be filling the wall and it's right here. And if it's a little further than I thought, I then gotta re-anchor myself. You can just feel it. You can almost start going, oh, I wasn't there. I'm here. you can just feel that happening so that, that might be what's going on there. I dunno if I answered your question. the question is, are these hard coded? I think the mechanism for doing phase progression is hard coded, but yeah, there's there is about how you can convert, almost like from a firing rate code to a phase code is where like you can imagine if you have, lemme remind myself.

Yeah. if this is like membrane, voltage, here, and this is time and imagine you have two neurons that get a lot of inhibition, which brings them down to a fairly common, membrane potential. 'cause I think the way conductance works in synapses, I'm forgetting all this now, but generally, like there's a, there's only so negative that they can go, below the zero before essentially the sign of how, a voltage gate is acting flips. And so they'll basically they can almost like rest at this very negative value. And then if you imagine, okay, these are two different neurons and they're receiving different amount, different amounts of firing rate input, one of them is receiving more. So its membrane potential is gonna rise quickly. This one is receiving less, its membrane potential is gonna rise less, slower. So these are not spiking yet. These are just not, these are not spiking yet. Okay. But at this point, they reached the spiking threshold. Then the one that was getting the higher firing rate input is gonna spike earlier. And this will actually be proportional to how Oh, I see. How different the fires are. I see. Alright. That makes sense. Yeah. that makes sense. But, yeah, so that relates more to rate ENC coating or if you, it would be using rig coating as a basis it to switch to a phase in coating. Yeah. Because once you spike doesn't matter. There's no more rate. The rate spikes are spikes. There's no more rate.

there are some people who believe that information encoded in the intervals between spikes. I think the evidence says really weak. They like it because you can encode a lot of information that way. But I think with, rate encoding the problem with rate, so rating coding is a simpler version of that. It's just saying what's the rate of which this is happening. the problem with rating coing, it's very slow, like between one spike and the next takes some time and you don't have any rate until you've got the second spike. it's really slow. Anyway. This is interesting. this is like a, an integration. It's an integration of input. That then converts to a spike at a particular phase. Yeah. Or time. It's interesting. And, yeah. And then for what it's worth, I guess there's another one, which is like poly crinney, which is, that one, I know it's this one that this guy Evi in 2006, he wrote a lot about. But it's because with this phase code, this is a synchronous code. Like it's saying that these neurons are synchronous at this point, which is why they're, or like they're synchronous, at the point, like relative to the phase at which they fire. they work together well, they're all, at the same frequency, right? Yeah. But as in isn't it like we, we have a, let's say these neurons all fire with respect to this like background oscillation or whatever. But we're gonna say these like neurons are firing together. These are encoding something together because they spike at the same time relative to that. It's a, it's, so in that sense, it's like synchronous. Okay, synchronous only in the phase peak. Yeah. Okay. But phase peak, actually relative to the basic could see, because yeah, all the, all those cells in those mini columns will be falling at the same rate, and they'll have the same, they'll all be fine. The same rate. It's just like relative, the base frequency, which ones are maximized are actually there. Yeah. But then, who's, can I guess if I join the zoom link, I think, sorry, you wanna unplug?

There might be, you wanna just plug in? Oh yeah, if I can just plug that in. That's fine.

So poly connects code is where the, there's some offset in when the neurons are firing. yeah, this is a, okay. Imagine you have these three input neurons, B, C, and D, and the output neurons. A and E. Yeah. And so if they all fire at the same time, what, All, B, C and D or BBC and D oh yeah, sorry. Time is along the, X axis. And then are these millisecond, these are axonal delays. Yeah. This is the axonal delay thing, which I appreciate, has issues, but yeah. Yeah. Okay. But anyways, I just thought while we're talking about neural codes, I thought it'd be interesting to mention. yeah. And then these are the different neuron IDs here. So these fire at the same time in this instance, and then these arrows are indicating the conduction delays associated with their spikes. 'cause the spikes is like an all or nothing instance. And there are, like, depending on the axons you look at, there are relatively large conduction delays associated with 'em. Some, right? Some are very fast. Yeah. And then, but anyways, if they, if these neurons all spike at the exact same time. Then, you can see that these signals arrive at different times, right? And again, some neurons, not all, but some neurons are very sensitive to the exact time at which inputs arrive.

but if they fire with this particular pattern where the D neuron fires first, then they'll c then the B, and with this precise delays, then they all arrive at a simultaneously, if they fire with a different pattern, they all arrive at D simultaneously. So what is the arguing though? Is this, so they, this is like an extremely, rich code in that so you could imagine it would start looking something more like this where it is rich, but could, is there evidence of this actually happening? I think similar strength to like phase code outside of the, interal, or sorry, the OCaml complex. Yeah. Yeah. So I think it's definitely speculative, but in theory neurons can do it. and like there, there's also what's called myelin plasticity, where myelin is like the fatty sheath around neuron axons. And that can change over time, which can change the conduction delay of neurons. yeah, but I could see that, I could see that, worth getting into it. But yeah, it could create some really cool, interesting, this is a little bit like, people who, who argued for, spike timing. TDP the delays, right? oh yeah. And it's like very high capacity. We like it because mathematically it's nice, but I don't know if there's got anything to do with brains. Yeah. Yeah. I think it's, I think it's, potentially not very robust a noise, depending on, and then how would you encode, I guess you're saying maybe the myelination would change and you afraid form memories by changing the myelination? Yeah. So it's possible. I, wouldn't say it's not impossible, but it that's, a pretty slow learning process. Yeah. Yeah. So it, it was actually something I explored in my thesis. 'cause this was like a way of encoding that my professor was a big fan of, and like the two main No, that's alright. I am equally skeptical. Like my, a part of one of my chapters was literally like looking at, okay, could you realistically encode something? and I was actually trying to avoid myelin plasticity. And so just say, okay. Can s tt p given input synapses select the correct conduction delays given so we've known, or like we know from certain experimental studies like what the distribution of conduction delays looks like. Yeah. And it's this kind of log normal distribution where it's like tailed. So it's like most, as you say, most connections are quite short and fast, but there are some even in nearby areas, cortex that can be up to I think it's 20 milliseconds. Yeah. if they're like very fine. Yeah. but anyways, and then yeah, you get a neuron, sees a bunch of input synopses, and then STDP, which is very time sensitive. this is like a learning rule. might select the right ones. But Yeah, what I found was like there was this narrow space where there's all these different kind of parameters that affect, like there's the time constant of s ttp, there's the distribution of axonal, conduction delays Yeah. And all this stuff. And there's there's this narrow region where it might just work. Oh, and there's varsity of connection. What am I gaining from this? Because if I'm not learning, so you are learning, but all I'm ha what am I doing? So, you're selecting, so you're selecting some patterns that you become But highly specific to, but, the fine, but the, actual delays don't really matter. I learn a different set of patterns, but the, it matters in the sense it's learning in the same way that like an SDR R is But I don't, but the difference is that you can basically reuse like the same neurons. Could it is primarily a capacity thing. So like the exact same neurons could be active, but if they're active in different points in time. They will encode a different pattern. So it's it's an sdr r where you add an additional dimension of like right on the receiving end, I don't know any of this stuff. I'm the receiving, I don't know what your actual plays. I just see if it matches and matches. but you know which one you, in the same way that like a neuron that's sensitive to a particular SDR doesn't like care what neurons. It just knows. Okay. I've received, like my dendritic sigma is active, it's co I get coincident inputs. So yeah. So this is coincident inputs, but if there were delays, those incident inputs, I don't know that I'm just, it's still coincidence. So that's the pattern I'm gonna learn. but so you only respond when they arrive in that narrow window. So you're sensitive to just more specific pattern. I'm always, but I'm always sensitive to that. The SDR, the learning rule and the dead rights that we know Yeah. Is very, it's very a small window when things have to arrive. Yeah. And so imagine I'm, I imagine I'm going between layer four and layer six A and there's millions of fibers going back and forth. I'm trying to learn these associations with between them.

I, guess no, that's not a good example. I'm just, saying I can't take advantage. I'm not really, yeah, the same neuro could represent. It would be different time to time, but I can't take advantage of it. It feels like it is what it is. It's like I'm trying to recognize, you can, I guess just in the sense that like any neuron that's receiving, let's say an SDR like you, we no longer, if we're concerned about the capacity of the SDRs, we don't have as much of an issue in that.

okay, this, This set of neurons, they encode this sdr r they've already been used and it's yeah, but with what? Temporal delay relative to one another. 'cause that's like a totally different code if they have a different temporal. But if SDR have no, I have no capacity problems. I guess I just, and then I guess the second potential benefit, like if I'm the sending act neurons Yeah. And I can encode different delays. Yeah. Then I can encode different things. But if I'm just relying on the delays that exist in the neural pill and they're just what they are, I can't control it. And the receiver can't control it. So they're invisible. I can't really use 'em. It's just, it feels like a little weird unless I have some control over it. But, and then I guess the other potential advantage is like if your input signal is somehow temporal, then that can influence, how, you were receiving that information. And again, that was what I was trying to explore was like whether you could do something like capsule networks where as you move a feature in a receptive field, that changes like a time. Yeah. Yeah. And then whether you get some coincidence of, anyways, it's let's bump this out again. It's interesting, Vivian's back. So she suggested maybe some future time we talk about this. 'cause in coding is really like key to everything. by the way, for the newcomers here, we don't do any of the spiking stuff right now. We're just using real numbers and there's been a debate whether we need to.

so maybe it's all useless. but it's really nice to know what's going on in the brain because I find it's generally not useless.

even if you don't do it this way, to know what, oh, what are the things that is representing, how's the do the different types of information you can represent. it's very helpful. So you, there's a spectrum here, right? You have a spectrum, like one cell firing rate, very limited. You might have the mini column hypothesis, which is essentially a limited number of things or mini columns, but you, because it's only a few of 'em and, and so it's, it's not a rate in coding as much as it's a, it's like a place code if you will. which three of these is active at one point in time. That's more capacity than this, but not a lot. And, but it's, it's, you're not relying on one cell, which nothing really works on sale. And then we talked about really sparse stuff, but maybe there's some things in between, maybe there's some things in between that is, between non sparse and super sparse. And, and the idea that maybe they change over time, I don't know. There is a question about that. And and then the whole phase thing is another, play on top of that.

maybe there are InBetween representation. we do see a lot of variety of density in the cortex, right? You can see very sparse representations where they, they see a cell fire 200 times in 30 minutes. Maybe. that's not much. That's like you better keep your eyes open, you miss it, to, it's super sparse to things that are much less sparse than that. So there could be, and we've talked about ways you could have multiple ones, less sparse and a variety of sparsity and, those idea just brought up could be another one, one of those lines. anyway, that is an interesting question to collect data about this specifically and, maybe come up with some thoughts about it, further thoughts about it.

one thing about the, nice thing about our neuron paper is we introduced a mechanism where you go between two of these modes. You go between those not so sparse mini column representation and a super sparse SDR. So the SDR is always, the same. This representation, which is not so sparse as many specific sparse ones that are, all equivalent to one of these. So in some sense, you are going between you, you can go between these. it's like you have two represent, you have two representations simultaneous. You have the mini column representation and the MDR version of that, the, reason the same cells in some sense. So in that sense, it is like a sparse and less sparse version. When we say it's 2 sparse, is that in the mini columns or in the sdr r version? that would be SDR version. This is three outta eight 10. That's not s spars. No, this is not sparse at all. Two. This would be like 30, right? 2 is, I have a million cells and 10,000 are active. but the whole population is going to be sparser, like the whole, all the mini columns. If you count all the cells that are active, if you count the cells, then there's sparse, right? That was the whole point. The mini column itself is not sparse. So in the neuron paper, we, you might say you might have 20 cells in a mini column, in a layer, in a particular layer, right? That's not unreasonable. And, good number. So you got 20. if I'm, essentially I'm right. I, if I have, I might only have 10 different representations here, but for each of those 10, I can pick, 20 to the, the third. Is that right? It's 8,000 version. 8,000. there's 8,000 ways of representing each of these, orientations. Alright. Alright. Yeah.

So you can get a much higher representation space by sparse volume, right? In this case, you would have, you'd have, since if there's 10 columns, you have, 200 many, 200 cells here. These are pretty low numbers. It would be bigger than this 200 neurons, of which only 10 neurons are active. So it's 200 choose, it's two, oh, shes 10. But, not quite.

so anyway, that was that was, I think one of the greatest things we came up with here. 'cause it really InCorp, it brings branched between these two modes of an input, which is not unique to an input, which is the same input, but it is unique. and an evidence verse is very high. that's almost certainly correct in some form.

today was a day of just banging our heads against the walls and speculation and going up problems. We don't know.

yeah. Did we have any new ideas today?

yeah, at least from the, I think we talked about yeah, one takeaway was maybe the com competition thing. Yeah. yeah, I think we definitely had lots of interesting biological ideas, but yeah, I put these in a file in your head of things you. Can come back to later when a problem comes up and you say, how am I gonna solve that problem? And Oh yeah, that was, that could do it. Yeah. it was opposed to oh, we solved the problem. I don't think that was the case. and we didn't do that today. Some days we do that, but not today. So good to go over this, all of this again and clarify the problems and questions. Yeah, it's, yeah. It's, it's just like a settling process. We throw all these crazy ideas around and eventually sorts out. Makes sense.