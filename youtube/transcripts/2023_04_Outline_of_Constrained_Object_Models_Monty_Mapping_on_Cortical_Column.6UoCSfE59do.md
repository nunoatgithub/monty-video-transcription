so I have some slides on the dynamic and constraint, models, and model building and all that, but then also today I reflected a bit about the meeting we had yesterday and generally like the cortical column structure and how it maps on the algorithm we implement right now. but I'm a bit scared to talk about it because I feel like I might have gotten something totally wrong and you all think I'm really stupid and don't understand anything. So don't you realize I've gotten over that fear long ago? Yeah, I feel like I preface everything I say like I could be totally wrong. you just say that and then you're good. Yeah. Okay, yeah, if you, want to humor me and, hear my thoughts on that in the, slide. it's up to, it's up to you, but I wouldn't not talk about something for fear of embarrassment. Yeah, no, yeah, I try to take that being afraid of doing something as a signal that I should do it because that's when I learn the most. Okay. Yeah. yeah, I can show really briefly, what I was thinking, so this will also map onto kind of the drawing I'm, going to show next with the object models and having these location representations. But basically the idea being, we have a movement signal coming in down here. we have the movement, gets applied to different possible locations in the objects reference frame that we might be in. And then these, locations in the object reference frame, the, neurons that represent those kind of project upwards.

They project to neurons up here, which for one, get this kind of location input, which is a very strong driver for the activity of this neuron. And I know this is a super strong abstraction saying that, this is basically like a cup outline and each point on the cup is a, neuron representing it. But this is It didn't catch that. It looked like a random bit film. Yeah. Sorry. It's supposed to be like a cup outline. Now I see it. It can't. Now you can't see it. Now you can't. This is the secret to the cortical computation. If you do slices through the cortex, you'll just see 2D models of the objects.

Yeah, I know it's a strong abstraction, but basically saying, okay, so this location on the cup, it gets strongly activated if that location in the cups reference frame gets activated, but it also has like dendrites and these dendrites get the feature input and the incoming features can prime this, point on the mark to be, to become active. so if the features match, this neuron will respond faster. And then, once we have a strong, one of these response very strongly, we, it projects upward and represents the object ID and location on the object.

and just This is my interpretation. I don't know if you'd agree, Jeff, of, like the hierarchy aspect then. So basically this column would be a column on the second level in the hierarchy. So that would be LM3 that we had up here. It didn't have to be right. This could also have been 0, 1, 2, right? It wouldn't have to be. This is just an example now. Yeah. Yeah. So in this example, that basically the raw sensory input that goes in here as features. So everything that goes onto the standard is coming in here from the site as the features, but basically LM0, which is at the center of this receptive field.

Larger receptor field itself, attached to the dendrite, closer to the soma, while LM one and LM two, which are more here on the outside of the receptor field, would attach further out on the dendritic branch. Yeah. And right. If we get the input from LM zero, this is already enough to prime the cell to fire, but so we don't really need to. Input from LM1 or 2 doesn't really matter that much anymore, but if we don't get anything from LM0, these might still be able to have an effect on the 0. 5 or so. Yeah, that's what I, that's what I talked about the other day. That's a good, picture of it. Although, just remind yourselves, I made that up.

although there are many places in the brain where this sort of segregation along the dendrite, how far away it is from the soma, does exist. We know that exists, we know that it exists in the thalamus and other places, so it's not a crazy idea, but I had no evidence specific, actually there's data on this, we could find out for layer 4 cells, but that, that does capture what I speculated. Although that wasn't like, oh yeah, this is the way it is. It was more like, it could be like this. yes, that's, that, that reflects my speculation. Okay, cool. That's really great to hear. Because then, that's kind, that kind of also solves the problem that, LM0, 1, and 2 don't need to represent the object ID with the same pattern. the SDR that represents the logo pattern doesn't have to be the same. for, all of these, it just needs to be that this dendrite detects the pattern for a logo here, and here, but it doesn't need to be necessary in the exact same pattern. If you think about it, if you think about one and two and zero, these are all separate neurons in these, columns. And it's, actually meaningless to say they have the same pattern. It's like they're separate inputs. there's no one ever looking to see that there's the same pattern, unless you somehow combine those axons that they join together somehow. I just want to point out that it's almost, it's meaningless to say they have the same SDR. Yeah. So they don't, which is good.

From a machine learning point of view, you can say, oh yeah, this is the same representation, but from a neuron point of view, these are separate populations of neurons and they don't know what the other ones are doing. And anyway, that's good. There's a big addition to this diagram, which at some point I should point out, but I'm not sure if you're done with it yet. yeah, I have another few additions to it. do you mean to the left or the right diagram? I'm thinking about the left diagram. Okay. Yeah. So that one is definitely so like I would add the top down input and voting, but, yeah, there's another big, yeah, there's another big one. Yeah. And, remember, we've, separated out morphology from features. this is a kind of a, deduce this has to happen. Yeah. And, so remember, if you think about V1, in V1, in V, in layer four, where you show the dots for the coffee cup, the predominant cell property that people report about are, edges at orientations, right? and so the minicolumns in that layer four, there's a whole bunch of cells in the minicolumn that all respond to the same edge orientation, and the next minicolumn over is a different edge orientation, and so on. And so the speculation, it's speculation, but it also fits really well, is that There's two things going on in layer four. One is, a minicolumn is detecting, in some sense, your point normal, the edge of the object, the edge of the thing. And that, that, in some sense, if you just, imagine there's a series of cells all vertically aligned that all represent one point normal and another point normal. And, and what happens when you get these features coming in or, on a specific object, on a specific location, on a specific object, it picks, One of those cells in the minicolumns. in some sense, you have a representation of the edge of the object, the point normal of the object, which is defining its morphology. And then, we want to represent that uniquely, on a particular object. if I just have the edge coming in, I don't know what object I'm on, so I activate all those cells in the minicolumn. But if I have more data, and if I know where I am, I have some additional information, I can narrow it down. the theory here is that there's sort of two things driving layer four.

one is, this sort of predominant edge detection system, and then, and then within the cells in the minicolumn, they get this input from, from the movement grid you showed on the reference frames. They also get features, and that way, those help us determine, okay, we've got this edge, we've got this point normal, or this edge of the object, and Which cells should we activate in the minicolumn to represent, a set of minicolumns? Which cells in one in each set, each of the active minicolumns, should we activate to represent the specific objects edge? And that would be determined by the features and the location. so that way we have to have, we want to be able to recognize the object purely by morphology, and we also want to be able to narrow it down, by, specific features or knowing specific locations. If I know where I am on what object, then I always, activate a specific cell on the minicolumn. Does that, did that, does that sound familiar? Because we've talked about it in the past. I know it's complicated. Yeah, I think so. I'm not sure if I got everything, but, basically. So here in this diagram, we can recognize the object without ever getting feature input just from, like, how the locations, get activated in the object's reference frame. But then also, we get morphological features as input here, like the point normal and the curvature. But the morphological features are, I assume that the morphological features are always, that's the base thing, you get that. That's, that's what's detected primarily by the retina in some sense. And if there's additional information, whether it's a feature from another region, another column, a module, or if it's, a color or something like that, it helps us narrow down. It's just a temporal memory algorithm. We have a set of meaning columns which represent the, current, at some location, this is the current, point normal, in some sense, right? At some point, some locations, okay, I can think of more the edge of the object is pointing up or sideways or horizontal or something. And, but then we want to, that's generic, that's not specific to a particular object, and we want to get to, eventually get to a particular object, and then we want to pick, A unique cell in each minicolumn that's active, and then I would say, oh, this is the, horizontal edge on a coffee cup versus the horizontal edge on the, the stapler or something like that.

I could draw a diagram, but I can't do it here.

if that's, I don't know if that's helpful.

Yeah, I think it, makes sense. Yeah. I wouldn't know concretely how to modify this diagram to incorporate that, so if you can draw something sometime and send it or so, that would be useful. I can try to do that. Maybe send me a reminder of that so I don't forget to do it. I got so many things going on. yeah, just to give you a little bit more flavor of this, it looks like in the brain there are two sets of cells that are getting these inputs from the retina. There's the cells you've drawn here, which are just these layer four cells that everyone reports about. But then there's these other cells, which I've talked about in the past, which seem to drive a minicolumn. They're very They're these bi, they're very, they activate all the cells in the minicom together, or they, and that's it. They're very, they, drive all the cells in the minicom together. These are, these bipolar cells. They're not as, there's not many of 'em. So one in one sense, those cells detect, these bipolar cells are actually detecting the point normal and they inhibit one another so that they form, not a very sparse distribution, but a. like a, still a, it's a, it's not super sparse. It's, there's maybe 12, 10 different angles that your edge could be at. And then, and so those, drive all the, cells that you've shown here as a group. Let's say, oh, here's a bunch of cells that represent this edge, and then here's another bunch of cells that represent different angles to the edge, and so on. And then we want to pick a unique cell in that minicolumn, in each of the active minicolumns, and that forms a unique representation of that point normal for a specific object. So I can try drawing that, just, if you could just send me a reminder to do that, I can try to do that. But we did conclude, or I concluded that it's so surprisingly that we need to have these two representations. We have to have one that's representing the morphology of the object. And then one represent additional features that may help us, and those additional features could be the location, or they could be the color, or they could be projections from zero or one or two, that kind of thing. Yeah. But then the morphology would also be represented in this, layer, and we also move in the same location reference frame through that. I have to think about the details of this. I haven't thought through the details of it yet. But just imagine there's two cell populations, there's one of these bipolar cells that represent the point normal. And there's another, and then each of those, associate each of those as a set of other cells which are used to represent the point normal uniquely. Yeah. And, how all that works, I don't know. I'm trying to remember, I think in the By the Bay, we, last one we had, we had some diagrams we drew where We had the kind of feature and morphology layers separate, and I can't remember exactly what the sort of anatomical correspondence was, but I think we had a separate feature. I tried to separate them. I think in the top layer, the ID, we had two different output IDs, one for the morphology and one for the features. that could, make sense. again, imagine there's these two populations of cells, remember, remember yesterday, and I've talked about all this before, but I know it's a lot, it can easily get forgotten. yesterday I pointed out there were actually three different locations in a column that get input from the sensor. I said that the one that everyone talks about is the layer four cells, those are your dots here, and the other two were the border of the bottom of layer three and at the bottom of layer five. And, what I'm thinking here is it's possible that the one that comes in at the bottom of layer three, this, isn't critical, but I'm thinking along the lines of what if it, what if that drove the, minicolumns, those connected to the bipolar, cells. And so that's driving the morphology. And then, the other ones connect to the, directly to the layer four cells. And, and that would, explain all this, right? I thought that the ones at the bottom of layer three would detect, object movement and then I did say that. Shift the object location. That is a, that was a speculation I proposed, but it may not be inconsistent with what I just said either, so I'll have to think about it a lot more. But there clearly is, these bipolar cells, let me give you a, tell you a little bit about them, I don't know if you remember what they look like, they're like, they call them, sometimes they used to call them in the old days, they called them like horsetails, because they're like the, axons and dendrites of these cells are just very fine bundles that are very narrowly constrained, 50 microns wide or something, and they really define the minicolumn. And they span up and down.

those cells, those bipolar cells, actually, they get direct input from the retina, we know that. We also know that they have the same basic, Edge response properties as people have reported in the layer 4 cells. So we, and we also know that they fire earlier than the layer 4 cells. That they're quicker. That could be because they're getting input from the magnocellular pathway, or it could be because they are just faster responding. So that, but the, we don't know the details of this, but it's totally consistent that The input comes in, the first thing that happens is, okay, I've detected my edge of this object, I know what orientation it is. Now I'm going to see if I can pick a specific representation for that. It is possible that the bipolar cells are actually temporal pooled as well. and therefore, they would they might be more of a, a class object, right? They would say, yeah, forget the details here. this is a cylinder or a coffee cup. I'm not really paying attention to the, the bipolar cells wouldn't know that there's a logo or that it's green or something like that. It's just that I could just, basically, the bipolar cells themselves could be pooled in some sense to create a morphology model. There's, problems with this, so I'm just, throwing these ideas out, but it's not worked through. I found the notes from what we discussed, by the bay, yeah, what I've got written here was, so there was two possibilities discussed, or yeah, one was dendritic branches on L4 cells, could receive parvocellular input, Which might be processing non morphological features, like color. And then the somatic inputs via intermediary bipolar cells could be providing morphological features. so that's where it's two different parts of the cell. The other one that was discussed was, the non morphological bias might, and this could maybe be an addition, be received and represented by stellate rather than pyramidal cells in L4. So then L4 would be a combined code, changing as a function of space for morphological features, which was, is in the pyramidal cells and non morphological features in the stellate cells. Both bound to locations via layer 6. I forgot about that part. Yeah. And then we might then pool over this combined L4 representation to develop a combined, stable representation in L2 3. Yeah.

and Yeah, okay, yeah, those are good ideas.

I think, we, want to get off the topic so you can get back to what you wanted to present, Viviane, but, there's a lot of, traditional neuroscientists just, they detect these edges in these layer 4 cells, In anesthetized animals, or animals that are not looking at an object, they are really just, they're just giving sinusoidal waves and so on. And in that case, the pyramidal cells and the stellate cells, they all look like they're edge detectors. But when those same cells, and this has been shown, those same cells, when the animal's looking at a real object that the animal knows, they become much sparser, and the cells become unique to the object. Those are the, object, what they call them, the object, edge, there's like a, there's a name for them, I forget what they're called, anyway, Oh, you mean a border ownership cell, almost? That was it, border ownership cells, a border ownership cell is exactly what we were just talking about. It's a, cell that, when you're looking at a, not looking at an object, it's just you got some raw input coming in, oh, the animal's anesthetized, it says, oh, this is just an edge detector. And then, but in the context of a real, object, in an alert animal, they become much sparser, and that cell only, that cell is, becomes selective to particular objects, at particular locations on those objects. and, it's exactly what we were just talking about, so very strong evidence that this sort of sparsification of the edge is, occurring. It's been observed in the border ownership cells. I'll try to, I'll try to work through a diagram of this if I can. I've done in the past, but I haven't done in a while. Yeah, sounds, great. Yeah, I, think it's what's going on, the details are complicated. Yeah. Okay, that was the additional, that was a pretty important addition to your diagram, but I'm not sure where you're going next, so it may not be important for the discussion today. Yeah, just let me add a few more small things in here, and then we can move on. So basically, One thing now related to what we do in the code as well is that we have, certain rotation hypotheses for the object, and we need to use these rotation hypotheses to transform the movement that we get in body centric coordinates into the object's reference frame. So that might be happening in the thalamus, for example, that we get the displacement coming in of how the sensor moves. relative to the body and then rotate this displacement depending on how we think the object might be rotated and then gets into this object's reference frame and then we can use it to, activate the next location in that reference frame and activate the neurons up here.

that's one, one addition in here. Oh, and then the second thing is, to keep track of what the possible locations are. We would need to look, another connections going from layer four down here because yeah, at the beginning we could be anywhere on the object. But then, through movement and through getting features and successive movement, we want to eliminate a lot of the locations in this object reference frame, so we only want to keep the ones active that are still consistent with the movement and features that we saw. By the way, I don't think it's layer 5, I think it's layer 6 we're talking about here. Oh, okay.

yeah, let me change that. I think all the data would suggest that layer six is, tracking orientation and, and, location. Okay.

So we have speculated that this is done through unions that get narrowed down, I think that's part of the solution, maybe it's not the complete solution, but it's part of the solution.

Yeah.

and then I guess an open question for me would be how this system would keep track of the possible rotations. So like the rotation hypotheses that go down here, and then how they get narrowed down and associated with the different paths in this reference frame. I mean in the code it's easy to do, but I'm just not sure yet how it would be done in this neural framework. That like multiple, maybe there could be multiple neurons representing each of the locations in this reference frame, and then they could each be associated with a different rotation. And then whichever ones are still active get sent down here as a signal for the thalamus to rotate the displacement.

yeah, there I'm not sure.

I don't know if you're going to show this, but the blue arrow that's talking about features. That also has to be rotated, right? Yeah, yeah, here I actually updated this diagram. I figured you were going to put it in there. You just stopped short. I'm like, why did you stop short? Yeah, I noticed that too late. I had already exported the figure, but then I Okay. So yeah, this also goes through the rotation, like the morphological features, like the point normals. They also need to be rotated, like you say. Yeah. And then, yeah, the top down object ID and pose from a higher level learning module would For one, go onto the dendritic branches of the neurons up here that kind of recognize the object or represent the object ID. And then these neurons would, send out the lateral voting connections. And then somehow this could also, yeah, influence the location neurons here.

if, the top higher level learning module recognizes an object, location and pose. It can reinforce some of these location neurons as well. Yeah. Yeah. because if we're on the coffee cup, a specific location, and there's a logo at that point, there was a coincident location in the upper module and a location of the lower module. And so in theory, the top down could re invoke the specific location that you were on the logo at that point in time. And that's why that lower arrow became a cross there. Yeah. One thing, one thing I talked about multiple times that you didn't show here, but, and I don't know if this is right or not, but it seems like it should be right, is I've been arguing that there's these two layers, six cells, A and B, that have these very parallel, parallel weird organizations, I showed that figure from the Doran, Thompson paper yesterday, and, they seem to be, like, yin and yang, six A and six B, and I have been Working out somehow the hypothesis that one's representing location, like a grid cell, and one's representing orientation, like head direction cells. And, I say that because both need to go through path integration. Both some movements will just change your orientation, and some movements will change your location. and so we need to update both of those based on our movement. And so there might be two sort of gritty things down below. You're just showing location, but there might be You know, I'm working on the hypothesis that there's, a, set of cells that represent location, a set of cells that represent orientation. They're very similar, they're both sort of a movement updated, path integrated, population of cells. and the one that represents orientation is the one that's sent back to the thalamus, because that's the one that helps us rotate the things. the other one would be sending up, would be sent up to the thalamus. Layer four cells, and this is inconsistent with the data, so I'm still trying to make this work, but this general idea that there's these two populations down there, location and orientation seems to be like, worthy of pursuit. Yeah, definitely let me know if you make some conceptual progress on that, because that's like my big question mark down here still, like, how is this represented? here the rotation hypothesis, I just started this error in the middle of nowhere, I don't really have an idea of how this is represented. plays into the, like, how this would be represented, so yeah, if you have some. imagine I had a system that's only, it's only job was to figure out orientation, it's only job was to figure out your head direction. I would have a set of cells that, are updated just like grid cells, the same basic mechanism would work, I basically get a movement vector, in this case the movement vector would be like, you're turning your eyes left, you're turning your eyes you're, you're lifting your eyes up, you're lifting your eyes down, it's like you're not moving through space, you're just standing left and right, up and down, or turning, rotating in plane, but it's not a feet, it's not a movement forward or backwards, it's, just changing the position of the eye in some, location space. I would then, I have to, I could break a mechanism that will look just like this. I would say, oh, I have a movement vector coming in, I have a representation of my current orientation, and when the movement vector comes in, I want to do path integration to predict the next orientation. I'll use, sensory feedback to, to lock in the correct orientation because my path integration is noisy. it's, totally analogous to what we think is going on with grid cells in, in location. I think having two representations that are very, similar, one for orientation, one for direction, for location, makes sense. And, I will have to set some time to think about that. I can try to do it today, and see if I make some progress on it.

Yeah.

yeah, sorry if you mentioned this already, but just with the top down feedback, yeah, if that's influencing the apical, Branches, then that impacts both L2, L3, and L4. L4, generally, Layer 4 does not have, Layer 4 does not send apical dendrites up to Layer 1. In fact, the stellate cells have no apical dendrite at all. Oh yeah, sorry, L5. L5, it impacts L5 and Layer 2. If we want to consider that. but we don't understand that. I think that's the motor cells, right? And I don't know why it's doing that, and so it makes me wonder, it's, what are L5 cells really doing? They are motor related, but we don't really know exactly what they're doing, and maybe somehow the representation of L5 is also incorporating You know, movement relative to the state of the object. Is the stapler open or closed? there's some mysteries here. yeah, that's a mystery. It's pretty easy to understand layer two and three. They get this feedback and say, oh, here's the object you're on. Great. Invoke that object, and it's easy. It's actually not too hard to understand the lower projection, which you show here going into layer six, because that could say, oh, at this location, Not only do you know this object, but you're at this location on the object, and at this orientation, the object has this orientation, so it just helps this lower module lock in exactly what it's supposed to be predicting. What the layer 5 cells are getting feedback from, I, this has to do with our hierarchical, action policy, and I have no idea.

that's a mystery to me. Yeah, for me this was one of the eye opening moments, because when I thought about how this projects to what we want to solve and all, these are exactly the two locations where we would need the top down signal, so it's really nice that matches up. yeah, and I feel bad, for a while I pushed back on this, telling it, I said, oh, it's not going to tell you anything about the location, but then I forgot about that lower projection. So yes, it can.

Yeah, I'll try to, I'll try to take this diagram and make a more elaborate version of it. Oh, yeah, that's great. Yeah, I hope, this was, maybe nothing in here was really new. I just had to frame it for myself. No, it's great to have the diagram. The diagram helps us all, me too, think about it and say, oh, yeah.

Okay, cool. One quick thing on your previous slide, if that's okay, just, Jeff, when we, started the slide before.

This one. Whoops. This one. This one, sorry, yeah. yeah, Jeff, when we were talking about this earlier today, Viviane, we were talking about the backpropagating action potentials, and I was just trying to remember what those do and how that could be relevant to all this, because isn't that, is that something where they actually suppress the sort of more distal inputs? No, not suppress it, no. Okay. So you want me to review that? I can just talk about what we know about that. I guess the practical question was if all LMs 0, 1 and 2, they all produce an output, does, the cell still consider anything from LM1 and 2 if it has already gotten good input from LM0 or does, the LM0 input suppress the inputs from those two? let's talk about learning and backtracking potentials, right? The basic theory about backtracking potentials is the cell spikes. And then that spike travels back up the dendrite as well as down the axon. The back axon potential is not as strong, and it doesn't travel to the end of the dendrite. As a general rule, it doesn't do that, where an axon potential will always travel to the end of the axon. But, if, imagine you show four locations on that dendrite that received input, the evidence suggests that if an NMDA spike was generated on the dendrite, and so the NMDA spike had traveled to the soma, then spikes the back section potential will travel back further to the point where the, NMDA spike was generated. So there's a lot of evidence that says if the dendrite was depolarized. Like an NMDA spike that primes it for sending the back action potential back to that site. So what that says is we wanna train the synapses, we wanna train the synapses that were predictive, of the cell spiking. And, so we could, without knowing what exactly, we could just say in this case, if. Let's say LM zero was the only one that actually generated a spike, an NMDA spike, then those synapses would be, reinforced. if, LM one also generated an MNDA spike, the back section of potential would go back there and reinforce those synapses. So anybody who, any input that was predictive, would, meaning they generated an MNDA spike right before the gen, the action potential, they would get reinforced. And inputs along the dendrite that didn't generate an NMDA spike, there'll be lots of those. There's lots of synapses active along the dendrite that don't generate an NMDA spike. They do not get reinforced. So that's part of the question you had there. the cell would reinforce anything that led to its prediction, that correctly predicted its activity.

Okay. Yeah, that makes sense. Thanks. Yeah, that is, different from what I had thought, but, but no, that makes sense.

Yeah. In terms of what happens when, if they all, what happens if they're, they, these different points, become generate NMDA spike to different points. I don't really know what would happen. I guess it depends on.

the order they come in, and I don't know, I don't know what will happen there. My, the default assumption is that, oh, we can just, any, NMDA, spike is as good as any other, so if any one of these things, contributed, then, that's good, but it might be more complicated than that.

okay, should I move on to the next topic, or is there some more on this?

No, it sounds good. Okay, cool. So It only took us a week to get to this point. Yeah, I originally made three slides about the connectivity and thought I'll just quickly go over it and double check with everyone and it turned into three meetings or something like that. My god, sorry. Yeah, I guess now, I definitely have a lot better understanding. So it was, Not, wasted time.

okay, so regarding the models, I started with just sitting down and thinking about what we want from the models because my plan was to Rewrite the code for the object models and, redesign it. So it has more properties that we want. And also, so it's a bit more useful for hierarchy. And so what we want is the model should be fast to update and fast to search. we have a limited storage capacity. So large detailed model is decomposed into smaller detailed part models. We can't just store. a huge model at infinite amount of detail, but we need hierarchy for that and composition. Can I just, just a slight word thing. When I hear the word limited, it sounds like it's small. Oh, that's not what we're saying. We're saying it's fixed, right? I used to often use the word fixed. I just wanted to clear that we all agree to the same, what we mean by limited. It doesn't mean that, oh, it doesn't have much storage. It has a lot of storage. There's, hundreds of millions of synapses here. there's a lot of storage, but it's limited in the sense that it can't keep expanding. There's a, it's limited in the sense that there's a limit, not that it's small. So I just wanted to clarify about that. Yes, and a module can't learn everything, obviously. But it can learn a fair amount. It's just, it has a limit. Yeah, and not just limited in the amount, but also like in the size, like if I understood it The lower level modules learn more models of small objects, like a small letter A, and then higher up you can learn, lower resolution models of large objects. I wouldn't say that's a limitation. I would, you could argue that's a strength of the smaller module, the lower module. it's just a difference. We could assume for starters that the columns in V1 and V2 and V4 all have the same capacity. that may not be true, but we could start with that. yeah, there's some evidence that suggests that as you go up the hierarchy, that the, column size of the learning models actually get larger, meaning they have more neural tissue in them, but that is not clear, it's not, the evidence is that it's mixed. So I throw that into oh, it is possible that a higher module could have more capacity than a lower module. yeah. Okay. Yeah. I can find, I seem to remember finding a paper a while ago that it was V1 and V2 that could have like on average 150 cells per, yeah, basically 1. 5 times as many cells as higher level ones, but As a, column? Yeah. Really? In a mini column, in a mini column. I think. Oh, when it made it out. Let me double check that. All right, look, there's different capacities here, right? There's the total number of cells in the column, there's the number of minicolumns, there's the number of cells in the minicolumn, right? yeah, there's, I think we could, here's the way I view it. The system ought to work no matter what capacity we give any one of these modules. It should work. it might work better if some learning modules in different parts of the hierarchy have more of one type of capacity and less of another or something like that. it's totally, It really, to me, it's data driven. It depends on what the nature of the world is, and but, we, that's just a parameter, that's a hyperparameter that optimizes things, but it doesn't change fundamentally how it works.

Yeah, and speaking of it being data driven, we want the models to learn statistical regularities in the world and not just, store every random, variation that we once encountered, but more learn what's consistent.

and then, yeah, we want the models to efficiently cover the whole object, maybe we want to represent a concept of surface, we want to be able to Is that the same as, that's our morphology? yeah, it's Yeah, basically saying that, yeah, you move along a surface, but you don't move inside the surface or off the surface. Oh, you're talking about, then that's an action policy. yeah, it's an action policy where that kind of information is stored in the model or associated with the model.

Oh, okay.

okay.

We want to be able to associate features from different input channels. So from learning modules or sensor modules.

we want to be able to Again, the solution was seeking. Seeking says the learning model doesn't know. Yeah. It doesn't know where these things are. Yeah, but I want, it should be able to receive input from both.

and then different, feature and morphology maps, it'd be nice if they can be associated with each other so that we can have the morphology of a mug, but the mug can have different textures on it or different logos or whatever.

and then that's a bit odd in the future that maybe the feature and morphology maps can be state condition. we can represent object states and behaviors.

That is a total mystery to me still. every time I think about it, my head explodes. It's I just share some of my confusion about this. It's if I imagine, I think of a model as, a set of locations that are unique to the object, right? So we, form a sparse representation of the locations. And so it, this, it's unique representations of those, of the object and question. Now, if I have a, if I have an object with, behaviors, it's changing, its morphology like a stapler do. I, there's multiple ways that can happen. I don't understand them. One way you could say, oh, it's all in the same spatial map, I've just moved to a different point of my location space where I'm representing something. it could be like, I remapped the, it's like I formed a new object. It's this is no longer the stapler, this is the stapler in the open position. Or it could be, I don't know what it is, but it's really hard for me to imagine how this works. If anyone has any thoughts on that, I'd appreciate it. Yeah, I was just thinking, I'm not sure if that makes any sense, but if the, incoming displacement that we get in here, if we are able to Add another transform on top of it, we could kind of state condition the object by saying transform the displacements in this way and that way, no, I guess that makes sense. That would be moving in the same location space like Yeah, now it doesn't make sense now that I think about it, because it would have to be different depending on where on the object we are. The other possibility, maybe the most likely possibility, something like a stapler, is as soon as it starts moving, we stop thinking about it as one thing, and we start thinking, oh, there's two parts of the stapler, and they're separate objects, and now we're just, we're learning the, relative displacement of those two objects, and but I, don't know. It's still confusing to me. Yeah. Okay. that's interesting thoughts. Yeah, so I'm not going to tackle that last one. I'm, mostly looking at those three here. So which three? the, Oh, right there. I got it. Yeah. Yeah. so basically, Adding some, fixed, not limited, fixed, parameters to these models, to, yeah, enforce learning statistical subparts of objects.

And how I'm doing this, is to have three constraints. So two are shown here. One is the maximum model size. So here are just some examples. So for example, it could say this learning module can learn, you're thinking physical size here. Yeah. Physical size in the world. So saying this learning module can learn objects of up to 10 cubic centimeters. Yeah. and then the second one is a number of cells per dimension. So for example, in this drawing, it would be four. So it'd be four by four cells. Each cell covers a space of two, cubic centimeters.

and then, so that would be, so the maximum model size would constrain the spatial coverage, how large can the objects be that are modeled, and the number of cells per dimension would constrain the spatial resolution, so how high of a resolution can we represent of this object, and then the third constraint is the Maximum nodes per graph. So basically the most statistically regular cells are going to be like, they cross like a node permanence threshold, and then they're going to be used for matching. And however many top K winners we pick here defines the model complexity.

And then here are just some pictures that I showed last week already. but just to recap, so this is the graphs we have right now. They're totally unconstrained. We can learn as large objects as we want, as detailed as we want. and then these are the new constrained objects. So the top row shows a larger, a cell that can learn larger objects. So here it covers the entire mug while the bottom row, can only learn smaller objects. So here it only learns, part of the mug. And then, for example, the difference between this one and this one is that this one has more cells per dimension, so it can represent a, more detailed spatial resolution. And then this one over here has a smaller K, so we pick less, so that's, the number of winner cells that we pick, basically looking at spatial resolution. which points were visited the most often, and then the ones that were seen most often in the world, looking at the statistical regularities, those will become permanent and will be used for matching, for recognizing the Mug. And if we said, okay, smaller, we just, Really look at the ones that were very frequently visited. The one in the middle and the top row, that one has both more cells and more k than the one on the left, right?

Because, so the cells is the granularity, whereas the k is like how many nodes there are, or like how many points in the graph, right? Yeah, Seems like there's more points. Yeah, it might. Yeah, I think. You're right, yeah, it might be also a larger case. I don't know, I feel, so yeah, I feel like whenever I look at this or when I was reading the code, I was often getting confused. I think partly the term cell, just because it's so it's such an overloaded term. Yeah. I don't know if there's a, I can't necessarily think of a better term, like voxel or something like that. Maybe. Yeah. Voxel is a good idea. Oh, I see. It's not a biological type of cell or a neural network cell. Yeah. It's just supposed to be like one of those subcubes. It's a volume of space. Yeah. Yeah. But yeah, but even if it's not biological, yeah, it can still feel like it's referring to the nodes itself and not the space in which they lie or something. Voxel is a good idea, I think. I should change that.

yeah, so Moving on, I, will just visualize it in 2D because it's easier to draw, it's a bit difficult to draw points in a 3D cube, but obviously, in implementation, this is in 3D because we deal with 3D objects, but basically the statistical regularity idea is we have a bunch of observations in space that these are the gray points here and each of them falls into one of these, voxels. And then we count up how many of these points fall into the voxels and the ones that have the most, the k voxels with the most observations in them will become permanent nodes in the graph that is used for recognizing the object. and then, yeah, I tried to draw this general idea in a bit of a biological analogy, I don't know if it makes any sense, but so I was thinking of it just, let's say we would have a set of grid cell modules and together they can uniquely encode, this, these different locations. So they can distinguish between locations within each of these voxels, but they can distinguish between all of these voxels. once we go out of this, grid, the representations repeat themselves.

everything in here are the locations that can uniquely be encoded.

And then, when we move in the world, we learn associations between these uniquely encoded locations and observed features. And depending on how often we saw these features at these locations, We have different strength of associations and after applying k winners we basically make the strongest associations permanent and those will then be used for when I'm trying to recognize a new object. And then over time if I now see the handle being more bent very often, this, we can form new connections to wherever we see these new statistical regularities. Does that kind of make sense?

you mentioned the, repeating of, once you go outside of this grid, so did you, end up implementing this kind of wraparound thing? no. So I think, and it doesn't make, I was, yeah, we talked about, implementing that it actually wraps around, but I think it doesn't make sense. And I think, how I'm explaining this away is that we would have one, grid cell module or whatever that just recognizes whether we're in that field or not in that field. So it's like a huge resolution. And if when, only when we are in that field, it allows, passes up the location. So just, okay. I, A bunch of thoughts about, I'll keep waiting, unless you want me to jump in now?

lemme, Yeah, no, go ahead. This, particular thing about the wraparound grid cell mod, any grid cell module wraps around, but a set of 'em, it takes a long time. If you get the same representation again, it could be like practically infinite. So it's in the neural representation of location. my assumption's always been that.

You can go really far in distance, and not have a repeating your location. Even if we just have, three of those, for example? I think you need more than three. You need a bunch, because you need to form an SDR of them. I think the thing is, yeah, they do repeat, but not so much within a single object, but more if you've learned multiple objects, then, Yeah. Yeah. that's when you get the repetition, right? let's say I have, I'm working on the assumption it's not actually grid cells themselves, but it's these one dimensional grid cells, these movement vector cells. And let's say there are, each one of those dimensions has, ten or twenty positions, let's say. And if I have twenty of these vectors and twenty positions, then, the representational space is, what would it be?

I have to think about that. I'm, the outside is confusing me. It's quite large. we're probably talking billions or something like that. so I actually think from a practical point of view, we don't have to think about space being limited. the problem is that, when you, imagine I have some, space I've defined, and now, and it's, let's, lay it down in physical words, let's say, the, point, the distance between points in this space that I can represent uniquely is a centimeter. I can have a point cloud of, say, a billion, locations, and, that's a really big space. and so the problem isn't that we're limited. The problem is if I start moving, the, resolution of my space becomes it becomes noise on larger objects. So if I have a, if I have a very large object, let's say a building, and I'm trying to represent locations on a centimeter point by point, then there's just too much noise in the system for the locations to be very meaningful. Like I can't path integrate with any reliability. moving, 20 yards or 20 meters if my resolution is a centimeter. it just gets, it becomes meaningless at that point. I, view it more of a, sort of a statistical noise problem, that you can't get too big because path integration stops working and your models are not that precise. And so it's not the representational problem of the neurons, it's really just the noise of the way the system works and the statistics of it that, trying to represent a house at a centimeter resolution is technically doable on a computer but in a brain it ain't gonna happen. It's it's just too much noise in that system. Yeah. That's how I view it. I'm not sure you shouldn't do it the way you're doing it here. I'm not suggesting it's dangerous. Yeah, I'm not actually implementing grid cell modules or something, I just wanted to try and make an analogy. I guess the main idea is just that we have a set of uniquely encoded locations and they are spatially limited and also limited in their resolution. Yeah. But how that is implemented is I'm not saying it's like that. I wouldn't do anything that says, relies on the fact that there is like some hard edge to your representational space. And if you take one step further, you're going to get, a repeated location. that's not going to be true. You can say, yeah, I'm going to limit the size of my models here, but it's not limited by the fact that I start repeating locations. It's, there's all the limits. so I just don't, make that. Yeah, I guess it would be, probably random where it does repeat, It'd be so far out in space, you just don't even have to think about it. It's just yeah, yeah. If you have enough of these modules, even 20, and each one has 20 locations, it's a very, 20, possible, it's, it's, it's, it would be, let's say it would be 400 choose 20. What's that number? You can type that into Google. You can answer. 400 choose 20. Something like that. It's not quite that. Anyway, it's a big number. I think it's fine the way you're doing it. Just don't. Don't rely on this hard edge to your space to say that's the limit. Yeah. So maybe I guess like looking at these models, we wouldn't really want to have a hard cutoff of the mark here, but rather be like, yeah, I've already added like 500 points in my model now, so I can't really grow further in that direction unless I, would sparsify the model over here, or something like that. Yeah, something like that. It just, it'll stop working well if you do this.

Okay. Another point I want to make, and I don't know how, again, I'm not telling you to change any of this, it's just useful to think about it. The way, you think about a column in V1, and it's getting input from some part of the retina, some, and it's, detecting a bunch of these, Ganglion cells that are coming in, and those ganglion cells represent some area of the retina.

the very strong evidence throughout the brain is how many of those ganglion cells converge on, on a layer 4 cell in V1 is dependent on statistics of the environment.

Imagine that the world only consisted of vertical and horizontal lines. And they were all perfect, and there was nothing else. It just, and so then the cell could say, look, I could, get input from a whole bunch of ganglion cells at once, because the number of things I can, represent n number of patterns on the retina, and there are very few patterns I'm encountering at my resolution. Therefore, I can look at a large area of the retina and still work. If there's a lot of detail going on in the retina, that is, a horizontal line or vertical line didn't extend very far. Then, if I start looking at a larger patch of the retina, all of a sudden I have a very complex space to try to interpret, and, there's a lot, and then what it would do, it would narrow down the number of ganglion cells it gets. It would basically, it would learn to expand its receptor field or contract its receptor field to match what it can represent. This is almost certainly happening in the brain everywhere, so we don't have to do it that way. But the limits are, it automatically, dynamically updates what it's looking at based on, okay, I can't represent a bigger area, therefore I'm going to narrow down my focus. and that means that a V1 column could learn, if the inputs on the retina are very detailed, it means that the V1 column is going to learn smaller models. And, and if they were not detailed at all, it could learn much larger models, larger in space.

Okay, yeah, that's, yeah, no, I'll definitely think about if I can incorporate that property. Yeah, because you're putting these hard limits, oh, this is the size of the space, and this is the size of the pixel, I think the size of the space and how much of it we look at is really dependent on the statistics of the data. Okay. We're doing a k winner. We're doing a k winner on the, in some sense, we're getting these axons coming in from the retina. And we're trying to put them through our spatial pooler, which is like a K winner type of pattern recognizer. and if the, if there's very complex patterns there, then it'll narrow down its viewpoint. And if there's simpler patterns, it'll expand it. We actually modeled this early on at Numenta, so we know that works.

But you don't have to do it that way. I'm just Yeah, no, I'll definitely think about if I can incorporate that, should be possible. It's just a different way of, it's, being statistically limiting versus some physical constraints of space. Yeah.

yeah. So just now how it is implemented right now, basically here's an example where we have two learning modules. One has a, larger low resolution model, one has a small high resolution model. if we get input to both of these, let's say we moved along the cup, then learning module 1 would be cut off at the moment when we go off the handle. Learning module 0 could learn all of it. We would get a bunch of observations, which are the breakpoints here. Each observation falls into one of these voxels. we count up how many observations fall into each voxel, that's supposed to be the colors in here, and then we take the k winners, and this will become the models for matching, and the step by step kind of thing is, the very first observation that we get. we start at the center of the grid, the reference frame is arbitrarily anchored to the first observation, and then everything else is When you say in center here, so yeah, because there's an even number of Yeah. There's no center. And then it's just this kind of slightly offset one. Yeah.

and then, yeah, each model basically has a scale and an offset parameter that tells it how to map new locations into that grid. So every object model has its own reference frame.

and then like I just mentioned, we count up all the observations in the cell and we average the, locations. I don't understand this diagram. These, this grid represents what, exactly?

locations in space, but, can't a point on the retina look at all locations in space? I can move my eyes around and see anywhere in space. Yeah, so this is why, this is all, like this, those observations were collected by moving over the handle of the cup. But I don't, I'm not restricted, this, column is not restricted to looking just at the handle. It can follow the handle to the cup and the rim of the cup, it can. Yeah, no, it did, move also over the whole cup, but the other parts of the cup would be, learned in different models in this case, because this learning module only learns small detailed models of objects. So that's this hard cutoff here, but it did see more. It just is in different models.

This is an area that's very confusing to me. Not what you're presenting, but just in general, it's confusing. And, so imagine, the way I think about it, if you go back to your previous picture, the one that shows the two different large scale modules, small scale modules. Yeah, that's it, there you are. Okay, go, to the next one again. Oh, this one? Yeah, either one, it doesn't matter. the way I view it is, zero, learning module zero, and learning module one are both seeing some subset of the copy, right? Okay. Because learning module zero is going to see a bigger subset of the cup because of its resolution.

no, I just put it that way. No, I'll take it back. They're both moving over the cup. They both are pointing at some point in the cup at any point in time. And, the, pattern that can be recognized at any point, is fuzzier and larger in zero, and it's smaller and more detailed in one. and both learning modules can move over the entire cup. And they both can learn models of the entire cup, potentially. But why wouldn't one learn a model of the entire cup? It goes back to the problem I said earlier. It's like the building, you're trying to represent a building with one centimeter scale. It's just too detailed.

it's not clear to me how Learning Module 1 It's totally unclear to me how Learning Module 1 decides when to break a model and when to not break a model, when to say it's something else or not.

It just doesn't, I don't know how that happens.

and, it, we, talk about it as if it's a, as if it's a binary break. It's oh, it's now moved from the handle and it's now onto the rim or something like that. I don't know if that's true. It, it, seems odd that it would have some means for saying, it would be statistical. It would be something like saying, I'm learning this thing and these parts I can predict better than these parts or something like that. Anyway, I'm just pointing out that. I don't understand this, and I'm confused by it, I don't think it's a binary, this is the, this is the scope of what learning module one can learn, it's, it's gonna be looking at the whole cup and trying to learn the whole cup all the time, just a, model that's looking at the house is trying to learn the whole house with a centimeter scale resolution, but it won't be able to, And so locally it'll be able to do okay.

I just don't know how that breaking down into submodules or subcomponents would be implemented. It's not going to be super hard like this. Yeah, yeah, right now it's just this hard cutoff whenever we go, more than like 10 centimeters from the first observation to the left in this case, for example. But, I was What I was thinking is that basically, so learning module one, after moving over the entire cup, it would have five or six models that learned of different parts of the cup, basically, because it cut off and started a new model over here. And then over time, it would see many, cups, and then it would learn that, it would there would also be a constraint on how many models a learning module can know about. So it would only store the repeating elements. So for example, the handle would be seen on many different cups. So that would be like a model that is persistent, but if we only, if we learn like a sub part of the, like the corner where it attached to the handle, that might be forgotten because it's never seen again like that. Not often enough. Yeah. Yeah. it would be helpful for me, I think a better picture here, that It'd be less, I find it less jarring or objectionable, would be to show learning module 1 and learning module 2 having the outer rectangles being the same size, projecting the entire cup in both learning module 0 and learning module 1, but then showing the resolution of learning module 1 is much finer, just like you've shown that the grid is smaller. But there's something about learning module 1 that says it can't look at the whole cup. And so it is observing the entire cup. It's just at this higher resolution. So in some sense, zero and one have the same size outer rectangle. They're both looking at the entire cup as your eyes move. And, and then we, say okay, within those two larger rectangles, one at high resolution, one at low resolution, there's a limit to perhaps, what a model could learn. And so a subset of that finer grid in LM1 would be what it could statistically learn. But it, to me, it's misleading just showing it like this. It's showing it like this, if the learning model one actually observes the entire copy. and it's trying to learn the entire cup. It's trying. It's not like it's limited in space. It's just it statistically can't do it. like the building with one centimeter resolution. So that would be a better starting diagram for me. It would be the two large rectangles, different resolutions. And then we say okay, each of these has a limit to statistically what it can learn. And perhaps in the, learning module one, it's going to hear some of the features it might learn that are statistically, memorable. You follow what I'm saying? Yeah, so then I guess, with the learning module one, it would be like this big grid, but at a high resolution, but then, basically starting from this first observation, we can only add so many points in the vicinity of that until it reaches its capacity. it, to me, it's until it reaches its statistical capacity, not some physical capacity.

if for example, there was only one cup in the world and it was always identical to And never varied, and something like that. It's possible that learning module one could learn the whole thing. It's it's always exactly the same. It's it's possible it could learn the whole thing. But, because cups are different, and we, I don't know, there's a lot of variation in the world, it can't do that. Yeah, if points are laid down not only as a function of space, but also as a function of feature change, then that would, yeah, naturally capture that. I think, yeah, But out of points. The way the figure is drawn, it's not wrong, it's just, for me, it was misleading, and I interpreted it, so I, again, I would start off with two, think of it this way, they have two grids, one's fine resolution, one's not, and the sensor moves around from point to point in this grid, And, in the fine resolution grid, there's just a lot of points to finding a cup, and they're not going to be statistically, after you go a certain distance, they're going to start, becoming, statistically difficult to combine them together. And, so it'll, basically learn a model of a subset of that space. but it sees the whole space. It sees all of the, it sees the entire cup as the eyes move. Yeah, so I guess, yeah, one thing that's confusing here is this is not learning module one. This is one model in learning module one. Learning module one isn't just this handle. It knows, it can know a lot more, it can know about different parts of this mark. but I think the figure is misleading for me, maybe not for other people. Because I didn't interpret that the way, because you're showing the, all right, I interpreted it differently. I interpreted it at here's the cup and these, learning modules can learn, this is what one can learn, this is what the other can learn, and this is only seeing the handle. But no, learning module one sees everything. Yeah, no, I think it's, yeah, maybe it's helpful to clarify like, we did discuss, I think, a while ago, having, a more organic, building out of the model, where you, yeah, start at one point, and you just yeah, add points based on spatial distance and variance, whatever, but, but yeah, but you've implemented this for a good reason, Viviane, which is that, it needs to be computationally efficient, and, doing this, strict, structured, grid size, Is effectively very similar, but just can run more like quickly, but yeah, I guess you could introduce the graphs you have here on the right, which is like conceptually what it's doing anyways. And then it just happens to be that's like in a, this 3D muscle structure. Oh, sorry. Yes. Yeah, no, that's, right. Yeah. I'm not objecting to the fact that you're showing LM1 having a smaller model. Or even the way you're doing it. What I found objectionable to this figure is how I interpreted it. and I think it would be more useful if you started off by showing LM1 Oops, my battery's low. I just knew laptops still sucked.

I'm going to be out of power here soon.

Maybe it's not a bad stopping point either. If you could do this, just humor me. Make the same diagram, show LM1 looking at the entire cup, just like the same size, looking at the same time, the outer rectangle is the same size, just higher resolution. And then you could highlight a subset of LM1 saying LM1 can't learn this entire model. Of this cup, it's too many points or it's too statistically difficult to correlate the different thought. I have to look within some area, which it can learn a model. Yeah. And then, you could highlight what you've now shown lower. You can say, oh may, it may learn a model of just a handle, or it may learn a model of just the rim or something like that as a subset of that larger picture. Yeah. That would be a helpful diagram for me. That would make it more consistent with the way I was thinking it. 'cause it threw me an off when you showed me this. Yeah, that's good feedback. Yeah. I'll try and do that. It's not changing anything you did. It's just a different way of presenting it. Yeah. Okay. Yeah. Just, yeah, definitely, just before wrapping up, I guess I would have one more, figure. that I think after that would be a good stopping point if that's okay. If you guys have another minute. Yeah, keep going. I don't know how long my laptop's going. Yeah, no, that sounds good. Good. Just. So this again is showing really, it's really drawn from the perspective of what's happening in the code, basically just the kind of things that we keep track of. So we have observations coming in that are these gray points, and then we have kind of three representations of that. We have a count of how many observations fall into which voxel, and that count is used to do the k winners. then we have a grid that tells us the average location in each of these voxels. So that way we don't just have a very gritty looking model at the end, but we still preserve some of this continuous location representation. And then we have the average features in each of these, voxels. So for example, average color or average curvature or whatever features we have. and. We basically, to get the model that we use for matching, we basically take the average locations and average features, and then we threshold them by the K winners of the count. So we take the, oh, did we lose Jeff? I think we lost him. Oh, I think that was the battery. Yeah. Yeah. anyways, that's basically it. So we take the K voxels with the most observations in them and then store their average locations and features in the graph that we then use for matching.

and that's like a real life example of the, 2LM heterarchy, where we have the larger, lower resolution model, at the top and then the smaller, higher resolution model at the bottom. and that's the models that they have learned. Yeah, cool.

And it sucks. It's just terrible. I can't stand it. The battery doesn't last. I can't see the keyboard. And, it's got other problems. oh my God. Anyway. all right. I'm back. So you can continue. Yeah, I just finished up. but yeah, the only thing you missed was basically that we take the, the K voxels with the highest observation count. And use the average location feature stored in those voxels to build the model, the graph that we then use for matching, so for recognizing the object and This is just a kind of real example of running this, code on the 2LM heterarchy example where we have the lower level learning module with, which learns like a smaller, higher resolution model. And then the higher level learning module, which learns a larger, lower resolution model. And here the features can also store object ID that's passed on from this lower model, for example.

that's all I said.

So this, this approach may work great, I don't know. this is one of those situations where I say, you don't necessarily have to do it the way the brain does it, but I don't know. Yeah, so the thing is, it's, definitely not an improvement to what we had before because, nothing is as good as just storing as much information as you want. We're basically limiting our models. but I think it will make it more interesting now introducing heterarchy and actually learning compositional models. because before you didn't need a hierarchy, right? Exactly. Yeah, before we could do everything with one learning module, basically.

Yeah, that's an interesting question.

Yeah.

I've mentioned this before, I'll just mention it again. When we first did, the columns paper, which was the first time we tested this basic idea of oh, locations and features and temporal pooling, because you got to do all those.

We just did, I was worried about the capacity of it, and how much, and we had fixed resources, we were doing it with a neural model, so we had a certain number of neurons, and and we didn't increase the number of neurons, or something like that. And I was surprised at how large a capacity it was. It wasn't infinite, of course, but we got up to something like, five or six hundred models of some modest complexity of, two hundred locations each or something like that. it wasn't teeny. And, so I was like, oh, okay, it's, I don't know, let's see, six hundred models times two hundred points, that's, a fair number of location feature pairs. anyway, I just throw that out because it was, once So it's a semi empirical observation about the capacity of the systems. It's not really accurate, but it was a test we did. Yeah. yeah, basically, I mostly looked into making that change because without it, implementing hierarchy isn't very meaningful because The detailed learning modules, the lowest level learning modules, can and will learn very detailed models of large objects. So there's nothing forcing it to learn subcomponents of objects at the moment.

I think though if I were to, if I imagine I had a very detailed model and now I move my eyes looking at the handle and now I move to the other side of the cup, we want to make a prediction of what we're going to see there. And if that prediction doesn't occur, then I assume my model's wrong. but again, if it's very detailed and I move a long distance, in the brain at least, the accuracy of my location after that movement is not very fine. It's going to be off. and it's going to be often enough that I'll say, oh, that was a misprediction, I'll have to start inferring again. Where if I move a little bit, it'll be good enough, and I'll say, oh yeah, that was the correct prediction. So it seems to me there's a practical, in any real system, there's a practical limit to how big you can be able to do models and still have them being predictive with movement. It's just not going to work, The world isn't, our movements aren't that precise, so I think you're going to, we would run into problems anyway. Even if we didn't force it. If I gave unlimited capacity to these models, you can still run into problems, it seems to me. But we have to face this sooner or later. Yeah. So yeah, that's the general approach that I took for now, and I'm still working at, working on the details and trying to make some improvements and, yeah, I have some more ideas of also, yeah, how to make it more efficient and all that.