I'm gonna start with a series of statements. the first one here, is not too controversial. I think a lot of people agree with this, that machine intelligence is gonna transform the 21st century. Similar how computing transformed the 20th century. And you think about how computing in the beginning of the 20th century and the end of 20th century impact computers system, it just changed everything. Communications, entertainment, transportation, science, literature, arts, you name it, it just in ways that no one really anticipated, and literally didn't, they couldn't know that in the 1940s, 1950s. And the same thing is gonna happen in this century with machine intelligence. it is just gonna transform every aspect of society across the globe. Maybe I think even bigger than the.

Now, a lot of people think this, but I'm very confident in that. The second statement is, a little surprising perhaps. we don't believe that deep learning, which is the current AI technology, is gonna be the technology underlying this transition. it's what's hot today. but, there's lots of reasons to think that this is not gonna be the technology, that's gonna be driving this forward. I'll get more into that in a second. and, we're not the only people feel this way, by the way. I'll show you some quotes. so what is gonna be the technology we think we know, we think we figured out? And, the technology that's gonna align the future of, ai, the sensory, is sensory model learning, and that's technology. Our learning is. Where you're combining inputs from some sensors or sensory system with the movement, to learn models of the world. we learn, humans learn, by moving through the world, moving our arms, fingers, and our bodies moving our eyes. This is integral to how we learn. We not static, entities. and we learn that way. Of course, the human brain is essentially learning system, so that's, there's no question about that. And, the bed here is that is gonna be the method that we're gonna build ai, in the sensorimotor, and then, the meta is building the first developer platform based on sensorimotor is, we are, leading in this, and this is the opportunity, that. That we have. Okay, here's some quotes. some people, Francois is a very famous machine intelligence person at Google, and he says, you cannot achieve general intelligence simply by scaling up today's deep learning techniques. He's exactly what I just said. Jeff Hinton, who's really one of the inventors of deep learning, says, my view is throw it all away and start again. Referring to what? his own creation. Jan, all very famous, AI people that.

he get closer to what we're working on. What's missing is the principle. Allow machines to learn the world, how the world works by observation, interaction, moving through the world. This is the biggest obstacle, this sign progress in ai. So he's on the right idea there too, although I'm not sure he understands how to do that. De Hasis, who is the founder and CEO of DeepMind, which is part of Google. there was like 800 pound gorilla in ai. he, they also said, yeah, we gotta study the human brain. 'cause it's the only, proof we have for general intelligence. And so they have a bunch of neuroscience that's working. I've spoken to de they have a different approach to this. But you never wanna count them out. They're, working on this too. Jeff Bezos, of course, is not an AI scientist, but I just wanna point out there are other business leaders who have similar ideas. and he says, humans are just doing something fundamentally different than the current way we do machine learning and machine intelligence. The implication is we probably have to do something different. this is.

so what is, sensory model learning? These are two sort of cartoon diagrams of deep learning and sensory model learning. And, and it's, I'm showing it to, to contrast the two. So on the left you have deep learning, which consists of a series of layers of these artificial neurons. Sometimes there might be a hundred layers of 'em or so. Here are just so few. They're highly interconnected. And the way you train the system, way you learn is you present some pattern of input. At one end of this network, you present a desired output. You use this called backdrop, which sort of adjusts all those weight, so blue lines, and you do this over and over again, and eventually the system learns the matching between the input and.

on the right you have deep essential motor learning. You have an input, again, just typically from a sensorimotor. and, but there's a second input, which we call motor, which is really just how the sensorimotor is moving. so imagine you can imagine your finger sensing something and moving, or you can imagine your eyes sensing something at some point in the world and moving, and internal to the century learning system, you have, a concept of location in a reference frame. So there's an idea that this observation is occurring at some point in space and, and then you pair that point in space with the actual observation. And in this case, most of the learning occurs in this pairing between the observation layer and the, reference frame location. So essential motor systems learn models of the world, and they learn where things are relative to other things. The idea of space and is critical to how this whole thing works where this, that is not an inherent capability on the left hand side. It's not something you might try to train it on, but it's not there. let's just talk about how you might go about training one of these systems. So if you take a, classic image classifier, for example, you presented a picture of something like this and you say, A, that's a car. And then you present another picture and you say, that's also a car. And then you present another picture and you do it. That's also a car. You do this maybe a hundred thousand times on different categories. each time you're slowly adjusting these weights very minimally. So you're trying to get the system and it'll look pretty good at, now showing another picture and saying, that's a car. on the right hand side, you wouldn't show a picture of a car. You would be looking at a car and your eyes would attend to different parts of that car sequentially. So you might first look at one thing on the back here, you'd say, oh, that's a tire or a wheel, and that's gonna be, there's gonna be some location in the reference frame. This case, the reference room in the referencing of the car, he said, oh, there's some location on this object where that thing was observed. Then you might move your eyes and look at another thing. This is the door handle. And when you move your eyes, the motor signal tells the internal reference frame to move its location sensorimotor. You just move, you're now tending to a different point in this reference frame and you're pairing it with what was learned there. And you do it again. You send down and move my eyes to the front of the car. You update the ref the location of the reference frame, and you pair it with the observation or the headlamp, and that's on. In this way. You build up a model of the car, of its components that relative to each other. It's very different way of learning than with deep learning. there's some real strengths and, downsides to deep learning. On the plus side, deep learning in theory can learn any input output mapping. In theory, in practice there's problems. but in theory it can be really, good and you can get incrementally better just by using more and more training data. on the downsides, it's brittle, because you can't train on the infinite amount of data. There's always just points where it fails miserably. you might change a couple pixels on this blue car and you don't even see it. And then the system says it's a blue bear. that is the kind of stuff we see all the time's, very well documented. There's very little ability to generalize because the system doesn't really know what it's looking at. It has no structure about what it's looking at. it's, there's no, it's very difficult and it can't say, oh, here's another thing with two wheels, or, and, or four wheels, but it's on a bunch of, tracks. Maybe it moves and transports people, things like that. It just can't do that. to get these things to work acceptably well requires unsustainable, large training sets, training times, and energy. this is a long problem in the AI world right now. Everyone's trying to solve this. There's just huge models that very expensive run, require huge amount of energy and lots of carbon dioxide emissions. they also don't, aren't good at continuous learning. You can't say, oh, you know what? I now need to throw in another category of my things. I wanna recognize if you wanna do that, you generally have to start all over from scratch and turn the system from the beginning of that. But in the real world, things are changing all the time and you're always learning and updating your, the real world as always, big changing. And these systems can't really do all that at all. on the essentially motor side, the pros are some of the opposites. They're very robust. They're much more robust in deep learning systems. It's just because of the nature of the way the data is stored. because you have the structured model, there's a, there's an inherent ability to compare models, which leads to generalization. So you can see two, two different objects have similar components and different, similar relationships. You might assume that they're doing something similar. It requires much less training data and much more energy efficient. training is fast and you can learn continuously about getting other things, you just can constantly add new things and change things, and it's problem at all. It's interesting. The models are also actionable in the sense that since you have a structured model, it's possible to do things like planning and thinking, where you can say, how would I achieve a certain result? And you can say, to achieve that result, I have to move here and do this kind of action. Things like that. the downside is it's not always the best solution, especially for point problems. So if I wanna create a system that does, plays the best game of go, I'm gonna use a deep learning system. It's always gonna be the best. If they've already beaten humans, the best humans, and, you can't beat it. And I wouldn't wanna use sensory mar learning to do a point problem, or play the game, or, even, recognize images. Because, you can, in theory you can do better on deep learning. So in summary, the deep learning systems are best for point problems. That means narrow domains, no general knowledge required. century learning systems are best for general problems that require knowledge and understanding of the world, and that is the big thing that's missing in today. AI eye, that's what everyone is critical about it for. just one slide on neuroscience here. we didn't just come up with this. We actually been studying the brain for many years, and we figured out this is how the brain works. And so I, a little very brief background here. that's the picture of the neocortex. Which is the organ of intelligence. It's a sheet of materials. If you could, look at it under microscope, you'd see that it's actually composed of these columns. There's over a hundred thousand columns in the human brain. these are like the size of a grain of rice. They don't look like this, but that's, you can visualize 'em this way and, all the different parts in your cortex look like this. It's surprising and even what to do vision and, and, and you, we can say that they all have a sort of complex architecture. They all have the same prototypical layers and cell types and connections. And so the big mystery is what do these columns do? And how did this lead to intelligence? We added our first big discovery here, or one of our big discoveries is that each column in the New York Collect is a complete sensory learning system. It was.

Okay. If you have questions, just stop. Okay.

from system, so going back to that picture I showed you before and I said there's a reference frame and then there's an input. We see that in every cortical column and we propose that there's a layer of cells in every quarter column that's a reference frame, grid cells, and, at the time made prediction It.

the second discovery we had has to do with how these columns, collaborate with each other. And so there are these long range connections in, the near cortex, that I show here with these horizontal green arrows. And this is where the models collaborate with it. This is how they talk to each other. We can think of very simply as they're trying to reach a consensus on, what it is they're seeing and where it is in the world. and so each column has got its own modeling system. It's getting its own input, and yet together they can come to this general consensus. this is this basic architecture of the neocortex, and this is the architecture we're gonna use, in our monthly system. this has been documented in a series of neuroscience papers done extremely well. Richard Dawkins, heard about the theory. He said, brilliant. He famous biologist. he wrote a nice forward to my book, bill Gates. Took, took the book, my book from last year and is one of the top five who recommends the only non-fiction book, he recommended. pretty thrilled by it too. Okay, so let's just talk about Monty. Monty is a codename. It's based on the, on the name of, scientist called, Vernon. That's where we got, and I think of it as a develop platform for Sensorimotor ai. How would you build AI systems? The basic idea here is the, these systems are built using two types of modules in varying combinations. the first module we've already seen, we, we call this a learning module. it's basically, it's more complicated than this, but basically it's the idea that you take an input, you have a motor, thing, which is how the sensorimotor is moving. And you build these, structured models through movement of a sensorimotor. these can be very flexible. They can model different things. They can model objects like, a soda can or a computer or a mouse and a chair. They can model environments, like rooms and, outdoor spaces. They, in your brain, you also, they becomes also these learning modules also model your body. So you have, some of these are dedicated the modeling space around your body. Pretty much anything with structure, and this is how we can, how the brain can learn things like math, history and language, even physical structure in the world structure. If you, use the reference frame right, you can learn any structured of ledge base. this is all, that's not a speculative idea. We know that this is, we know brains do this, and if we're right about learning this.

the second type of module we'll call a sensorimotor module, it's a bit different than you think of both sensors. I'll explain as we go into this. First of all, notice there's two outputs from the sensorimotor. There's what it's sensing, we'll call that the output. And then there's also how it's moving. It's like it is gonna tell the world how moving this way, and I'm moving that way. and of course that matches up really nicely with the learning modules. There's the idea that you have both the thing that you're sensing and how, the sensorimotor. Learning modules can work with any type of modality. So we have biological modalities such as vision, touch, hearing bats, do echolocation, but it would also work with yr radar ultrasound. It really doesn't matter, as long as it has the right sort of output, it'll work. now here's the key idea. Sensorimotor modules are not like a camera or something like that. They did, they detect features at locations in 3D space. One at a time. When you look at the world, you don't actually see a picture. You actually, even if you're not aware of it, your eyes are attending to different points in space and you know where they are. I'm, you're looking at me, I'm at some distance from you in some orientation. It's not That's, that starts right at the beginning of the system. The locations in space where things are being sensed. so they sense what is sensed and, how the sensorimotor is moving. And then the learning module has to figure out how to map that onto a reference fit. And because all sensorimotor modules share an understanding of space, it can be largely interchangeable. That's why the cortex can process touch and vision in the same way. My, my finger has a location in space when it touching something and, my brain, it tells my brain how it's moving when I move it over an object. But the same thing happens with your eyes and my eyes report where they're sensing something in space and how they're moving. And that's why we can intermix, all kinds of modalities in our brain. They're treated the same in some ways. Okay, now there's another thing going on, which I hadn't mentioned earlier that this motor signal is bidirectional. so the learning modules can output a motor signal saying, here's how I want you to move, and the essential modules can input this motor signal. Now, think about it this way. I could just, my, I could be exploring an object with my finger and learning how it, feels, but I can also. Now that I have a model of the thing, I might say, okay, I wanna hit the power button on my cell phone. I, the learning modules, I know where that is. I'm gonna tell somebody in the sensorimotor world, move your finger there and move it in this direction. And so this is the essentially, this, the entire system is, robotic from an, from the beginning. You don't have to use that component. But in the future, AI and robotics are really gonna be one field. And essentially model learning is gonna achieve both of those, goals. So that's the basic idea. Now, many applications can stale in multiple dimensions. And so we see what I'm about to show are things we've seen. We, know that these are the kind of things we see in, the simplest thing we just talked about, you could have one, one learning module and one sensorimotor module. And actually this could be quite useful. it's not like this is a stupid idea, but typically we, you might have more, the first way you could change in different scales. You go to multimodal, you can say, okay, now I'm gonna have two different types of senses, maybe touch and vision, or, lidar and vision or something like that. Each of those is gonna be going to, its own little learning module. and yet we, because of that voting layer I talked about before, they can collaborate with each other. I have, I can, they, I can, one modality can inform the other and back and forth. And so you can mix a maximizes we do all the time. this makes the system more robust. for example, if I was using a vision system and now I'm in the dark and I really can't see very well, I can supplement it with, another modality. and it makes a faster inference. 'cause now we have two things trying to resolve what's going on at once. You can expand this in a horizontal or wider realm, so you can add more, modalities, more, sensorimotor modules, the same modality and more models. think about it this way now. Now I have I have four fingers on the left there, four touch sensors. and so if I'm using my index and thumb on two hands, I can do a lot more, a lot quicker. I can learn quicker, I can defer quicker, I can manipulate things better. on the vision side, I get more acuity. So we see this kind of like expansion in the horizontal direction. You can also go deeper. So you can put models on top of models, like in a hierarchical form. And so you essentially represent hierarchical knowledge and you get deeper insights. Just to give you a sense in general, it's believe that humans have a four level visual system like this. and mice have a one level visual system. Mice can do a lot of sophisticated vision. they can, go out and see different things. They recognize what's food and what's not food. They can know how to grab that food in just the right way so that when they bring it up to their mouth, it's in the right position to eat. And then they can look at it and eat different pieces just like you would like an apple. And, they can do that with one level visual system so you don't have to go deeper to do some very sophisticated and in. You get more of this as you get into the higher animals and humans have very big, large in your cortex. And these give you deeper insights in multiple domains. So this is the general, very high level plan for how this works. and the good news is we can start small. We don't have to do all of this. We can, even a small systems can be powerful and, but we can design the architecture to support this kind of expansion. even if we start small today, to really build human level brains, we might need to do all this stuff in silicon and there might not be these modules built into silicon and so on. We may not be able to do this in software. certainly not. So we.

I have one last slide here, and this is a strategy for how we go, step go forward. First of all, I think there's an opportunity here to establish a leadership position. and this is one of the key things Ev every startup wants to do, every new technologist wants to do well, any startup wants to do, as establish a leadership position in a.

Most people don't understand what to do next. And has, we have a multiyear head start on this. I know how many years, but it's a multiyear head start. people will catch up to us eventually, but right now we're ahead of this. so we have the opportunity to define the field, to find the terminology to define. what this technology is and, and what it's good for. And that is always a, tremendous thing to have that opportunity. We're not trying to fight away into an existing field. We just don't wanna go where the other people are. Create new territory here. I've, I always tell it's important to focus on developer platforms from the start, and you can have the best technology. but if someone else comes along later and makes it easier to u to use, you lose. There's so many examples of this in technology. so it's not good enough to have the best technology, also have to make it so that nobody can just outrun your mind, Hey, I made this easier. so we have to focus on that from start. In addition to the technology, I think we, from an application space, you need to pick one, maybe two application areas to begin with. We can start very small. These don't have to be flashy applications. They don't have to like rock, the world. they have to show some conversional Value means somebody says, I want to use this. I'm willing to pay for it. and it has to demonstrate the principles of Sensorimotor AI and the Mon platform. whenever you create a new platform, I've observed this both in my own world and other people. There's always a whole bunch of people who wanna rush to that and see what it. And so you just have to get it going. and they will do that. They'll come, and, but it's gotta be easy and it's gotta work.

whether that's self-driving cars, we could do stuff like help division systems, self-harming cars, whether we do robotics, whether we do multiple sensory security, I don't really know yet. We haven't really chosen, what these application areas are, focus on. there's lots of partnering opportunities, so over time, which is great for a platform. So people who make sensors might wanna modify those sensors to be compatible with, Monty and to really be location aware. and so there's, an opportunity in the part with 'em on that there's a partner. Opportunity partner people in building new types of learning models that work on slightly different variations, the technologies or pre learned or have certain biases. There's opportunities to work with silicon vendors to implement this stuff. We already have conversation on other, parts of our work. There's opportunities to work with systems integrators, who can take this and then help, less capable end developers and then of course end developers themselves. So in a summary, we have the opportunity here to basically create and shepherd. So one of the most important technologies of the 21st century? I think it's exciting. I think it's also, it's, something we have to take very carefully thoughtful about because it's, really is, it's gonna have a big impact. We have to think carefully how we handle that, but it's a tremendous opportunity. We have a lot of tech, a lot of issues we still have to resolve, but I think everything I've just told you here is pretty clear. and that's how we're going forward. That's it.