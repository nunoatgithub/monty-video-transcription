So I'll try a little bit of storytelling of what happened in the Monty project so far and where we're at right now. And how we got there and yeah, for brevity I'll focus on the parts that I was involved in, which is the graph learning parts, but it obviously doesn't cover just things that I did on my own, but it was a huge team effort to get there.

yeah, let's get started. Sounds like this is the, follow on to the, Shears of Blood. Yeah.

That's pretty cool. I'm going to take that. I'll stick with you. You're very induced.

Once upon a time, the Monty team washed up on the shores of Nupic Island. Oh, yeah They figured the first thing they had to do was to get oriented and figure out how to use their compass. This meant to draw up the building blocks of the intelligent system they were imagining. So we drew up basic principles inspired by the brain and the thousand brains theory. one of the principles is to use sensory motor learning inference. that, sensing an object and sensing the world is an active process of moving around and yeah, sensing the world. A second principle was to use reference frames and not just a bag of features. Another principle is to use a module structure. We don't have just one input image and no sensor perceives everything at once. we have multiple sensors that each perceive small patches of the environment and they can move around and they can communicate with each other through voting, which brings me to the next principle as a communication protocol, which makes the whole modular structure possible so that every module can communicate with every other module they all speak a common language, no matter what's going on inside of the modules.

here's another high level overview. I won't get into too much detail of how this looks like. I think we just fast forwarded about 400 years ago. We lost our, ancient motifs. Yeah, I'm sorry. I kept the font.

yeah. So the basic building blocks we are, we envisioned were to have sensor modules and learning modules and sensor modules can be modality specific. So that can be for touch or vision sensors, for example, but then what the output is unspecific to the sensory modality and can be the input to any of the learning modules. And it includes features. and information about movement and location and space. And these learning modules can then communicate with each other through lateral voting or hierarchical connections to other learning modules. And another essence from the principles before is that these learning modules use reference frames for the models that they build.

They use their compass to reach the nearest town. Then they took a break to set up an environment in which they could test their ideas.

yeah, I won't get into too much detail here either. But Ben nicely set up this great code framework where each of these boxes here is a Class that can be customized individually. So we can write custom sensor modules, custom learning modules, and so on. all building on these principles that I just named. And this code framework is basically the basis of everything I had to come. and yeah, thanks a lot to Ben for setting up this really, nice framework and then Lewis set up the habitat environment where we have, 3d object. in space, and we can have different sensors. For instance, here we have a camera, which can move in space. And, since we have this principle of, small sensor patches, this is in the middle, we see what the sensor patch is actually perceiving, and it can then move in the world across the object to, build up a coherent picture of the image and build a 3D model of it. And what we use for most of our experiments is the YCB data set, which contains 77 household objects. that, yeah, shown here with the framework and environment set up. We have sound effects. Yeah. I can bring it onwards to work on the first prototype.

They set up the first learning module which could recognize objects independent of location and translation by using displacements starting graphs. So the displacements that we use here to recognize objects are the gray lines in here. So basically how we move from one location to another location. And since these displacements can be represented rotation and translation invariant, we achieved, yeah, this invariant object recognition where we can move along the object with our sensor patch. And through these displacements that we're sensing, we can recognize which object we're on and where on the object we are.

However, their system relied on sampling the same displacements stored in the model. So if we sample different displacements, movements that are not corresponding to any of the gray lines that are stored here, This approach did not work. So they continued their journey.

After a long walk through the forest of smaller and larger issues, they came up with new learning which avoided this problem. They spent some time in the next town tweaking and improving this module. So this new learning module is what we used for the past months, until very recently. It relies on storing locations and then taking displacements as input and rotating these displacements to try and fit the 3D model. And with this mechanism, we can still recognize objects rotation and translation invariant, but now we can sample arbitrary displacements. So let's take a bit of a closer look at it. how this works. basically, we learn an object in an arbitrary orientation by moving the sensor around it. mentioned before, each learning module receives features from a small patch of input and learns its own model of the object. And then features can be pose specific, like point normal and curvature directions, which are shown here as the green and red and orange lines, which means pose specific means if the object rotates, These features will also rotate. We can also have pose unspecific features such as color or the magnitude of curvature, which do not change if the object is rotating. We use the pose features, so these three vectors I just showed, to initialize the hypothesis space, possible object locations and rotations. and basically the rotations are tied to the different locations on the object. So if I would be on the top of the can here, the rotation of the can is exactly 180 degrees rotated from if I would be on the bottom of the can and 90 degrees rotated from if I would be on the side of the can. Then we use this hypothesis space to, to test the using observations that are made like successive observations to eliminate hypotheses. So first use the pose independent features, such as color and curvature to determine the possible locations on the object. So for instance, here we're sensing like a very curved. on a yellow object. we think we could be on any of these green locations in the model. And then we use the pose dependent features to determine the possible rotations at each of these locations. So saying, if I'm sensing these light colored displacements, pose features, And I would be at this particular location where I have stored these vectors, then I would have to rotate my sense vectors a certain amount to align them with the ones that I've stored in the model and that would be the rotation of the object. And we do this.

successive observations until we eliminated all hypotheses except for one, and that would be the object and pose that we detect.

Okay, now we can detect objects independent of the displacement that are stored in the model. They continue that journey and decide that it is time to add voting into their system. Huh?

They add voting on object ID and pose, which helps achieve faster inference and more robustness. So if we do not vote, the model needs more steps, than if we actually vote between multiple learning modules that each receive sensory information from a slightly different patch on the object.

Next, I decide to tackle the same. Oh, sorry.

Yes.

Okay, five learning models, no vote, five learning modules, opposed to any vote, plus TC, three, five, what's that? that's about the terminal condition, so basically saying, if one of the learning modules has recognized the object, then all of the other ones just Okay, so we're still voting, the rest of that voting stuff got truncated, and we're still, okay, got it. Yeah. Yeah, exactly. So next they decide to tackle the sampling problem. so far we could recognize the object independent of the displacements that are stored, but if we sample new locations on the object, it still caused some problems most of the time.

so they have seen this on the horizon for a while now, but it is finally time to tackle it.

they develop an evidence based learning module. This learning module does not discard hypotheses based on inconsistent observations, but instead updates the evidence, count for these hypotheses. So before we had a list of hypotheses, and whenever we get an observation that's inconsistent with one of these poses or objects, we remove it permanently from this list of hypotheses and it cannot be recovered. And now in this new learning module, we keep all of the possible hypotheses and update the evidence for each of them at every step.

so for each hypothesis, we calculate the evidence based on stored points in the model in a certain radius. so for instance, like this red radius here would be what we use for most of experiments. And yeah, using the observed features to update the evidence. And for this, we use, for one, the morphology error, which is the distance weighted angle between the pose features, and this can add and subtract evidence depending on how high this error is. Optionally, we can add evidence if pose independent features such as color match with what is stored in the model. This is also distance weighted. And then we have an evidence decay factor which is applied to all of the hypotheses. at every step to push the evidence towards zero, and not have it grow too large. and then additionally, we could, if we have multiple learning modules and sensors, we can also receive evidence, through votes, which is, also distance weighted. All of this can be done with efficient matrix multiplications, which makes this, yeah, actually faster than the previous learning module. the new model outperforms the old one in detecting the 78 YCB objects. So the old one reached, on the full data set, it reached performance of about 84 percent, and, the other episode ended either in detecting the wrong object or having no object that matches. And now we have a performance of about, 93. And most of the other episodes end in a timeout because we have a pretty inefficient action policy which is just a random walk and the YCB data set actually is Yeah, includes a lot of symmetrical objects and, ambiguous views of objects. so some, of these, timeouts, if you look at them, they are very reasonable why the module couldn't find a, correct solution there.

also in this learning module, voting cuts down on the number of steps needed to recognize an object. So again, here, no vote, and here with voting. and actually it cuts down a lot faster than with the previous learning module while not affecting the performance.

okay. So they take a short trip to a small seaside village to perform an extensive robustness evaluation on the new learning module. That's where you live, right?

Yes.

The new evidence learning module works surprisingly well. It tested on a range of scenarios, including new sampling. So here we have in green, the new module and in blue, the old one. And on the left side, we have, the performance when there's, when we sample, approximately the same points, which is a hundred percent performance, and then on the right, we have two different scenarios where we sample new points, completely new points. and the old learning module degraded in performance. If we sample new points, the new one pretty much retains the performance.

We also tested rotation and translation invariance again, which seems to hold up pretty well. So rotating by one or ten degrees from how the object was learned still works quite well, and also, yeah, moving the object in space.

change in sensory modality. So here we use the touch sensor that Philip implemented.

and basically we learn a model using, the camera. So the vision sensor, and then we try to recognize the object with the touch sensor, the old learning module couldn't do this at all, and a new learning module does quite well at this, and then also sensor noise. So adding some Gaussian noise to the detected features and locations. this. Pretty much ruined the performance of the old learning module, but the new one, using evidence, still reaches 100 percent performance. which again makes sense because if we get one inconsistent observation, we don't just discard the hypothesis, it just receives less evidence and we can still recover it, with more steps.

On the way back, the test crew makes a short stop at the cave of generalization.

Here they find that the amount of evidence received for each object model can tell them about object similarities. So here we look at the end of each episode, what is the evidence for each of the objects in the YSV data set, and then do a clustering of these, of this evidence matrix, and we can see that we form some meaningful clusters. So for instance, if we zoom in on this orange one on the left, we see it clusters round objects, and then also within this smaller cluster. Here we have round and yellow orange objects. If we look at another cluster here, we have a cluster of cups, which are actually also sorted by color. So first here, the ones here on the left are yellowish, orange, cylindrical shaped objects. And then we go further in the cylindrical shaped objects, through the red colors and then to the blue colors. And then also the other clusters seem meaningful. So we, here we have a ball shaped objects like golf ball, apple ball, baseball, and so on. Marbles. We have elongated objects like the fork, spoon, and spatula. We have airplanes and we have boxes and bricks.

Also, if we, present the object with a, present the module with an object that it has not learned about, it retrieves the object that seems to be most similar to it in the, in its database. So in this case, the picture on the left shows an object that the module has never seen before. So for example, a mug, and then we look at. which is the object that receives the highest evidence. And in this case, it would be, this cup that's also red. here it's two different cups that have slightly different color. if the objects are, if there is no object that is that similar, it still seems to be reasonably close. So here, the picture base is a cylindrical blue object and it. And the object that receives the highest evidence count that would be the master chef can if it doesn't know about this object here. This looks like it's being heavily biased by color in this case.

it looks like that, but it's color and shape. It's picking up the red cup if you give it the red mug, but it's not picking up the red apple. Yeah. So it is, it's using both. Yeah. And it's just that this is the closest, it's not saying that like other cylindrical objects don't have any evidence. It may be that all of these cylindrical objects have higher evidence, but the blue one has the highest one because it also matches in color. But for example, you have the marbles if we don't have a small round red object, the highest one would be this golf ball, which is actually white, but it still has a similar size and curvature and shape.

Okay, back in the evidence LM town, we look back at what we have accomplished so far. We have implemented the Monty framework, set up a test environment, have achieved rotation, translation, invariant recognition, sampling arbitrary displacements, voting and speed improvements, sampling invariance, robustness, and some basic generalization.

This was a short summary of the path the Monty team has taken to end up here. Of course, many details were left out. Over time, we have sent out several search parties to explore other routes. We have implemented alternative approaches such as temporal memory, ICP, and HDCD. The travelers have dealt with many struggles along the way and learned new lessons daily. Our path was windy and long but we kept following our compass and moved one step in the direction every day until we ended up here. The evidence LM town is a beautiful place and we may see test all of your capabilities and regroup. But what's up ahead?

So Niels has already started forging ahead into the fog, getting started on implementing some more intelligent action policies. Like I mentioned earlier, right now we're using a pretty random policy to move along the object, which is not very efficient. So we can really make this a sensory motor system by making the policy, actively test hypotheses and seek out features that will. help them recognize the object faster. We've also had several brainstorming meetings lately talking a lot about hierarchy and how this could be implemented.

another thing for them in the future would be attention, especially if we have many learning modules that communicate with each other. we may not want to pay attention to everything at once and have everyone talk to else. also so far we've dealt with one object, an empty void. But we would want to be able to tackle multiple objects in a scene and occlusions. Additionally, the objects right now are rigid objects that do not transform and do not move, but we would want to be able to cover object behaviors and transformations. Further out, we would also like to have continual self supervised learning, so not, currently we're doing some weakly supervised pre training before we test these object recognition capabilities. but learning from scratch with a very weak supervision, continually would be nice. And then further out in the fog, this, representing non 3D concepts or dealing with two dimensional images or other more abstract, conceptual structures.

Of chapter one.

Great version, really great was, it was fun, nicely done. Also, really had a lot of great content. Surprisingly to me, I think the fun wrapper you put it in made it easier to understand.

Very nice. Thanks. You created a reference frame for the Monty story. Yeah, it's interesting. Yeah, it's a big island. Yeah, as opposed to like just a temporal sequence of slides, if you put them on an island, it's locations, it's easier to know. All right. And I, of course, you need to add avatars that look like us.. Start with you. Start with yourself. Army. Just, you can start with, in our medieval costumes, Yeah. How about, a soundtrack too? I made some, I made some pictures with, the Dali, like, this picture here. This picture here. Really? But, yeah, it was just a bit, it didn't really work that well, so I gave up after the first. on Halloween you'll have some more opportunities. That was great. And the work is great. And I'll say one more thing, I really appreciate you starting off with the principles in the beginning again. so we don't forget those. Yeah, yeah, I feel like that's really been our compass. It's so easy to forget that stuff.

that's one thing I appreciate. I had one question. Could you talk about your algorithm you're using for evidence decay?

just for the decay factor or for the whole learning module? is, it just, you advance it by, the number of steps or what's, the axis on, the decay? what's making it decay? Just the fact that you've got, haven't gotten evidence recently or what? right now it's pretty simple. It just takes the current evidence factor and then, it's, it, like if the evidence is negative, it adds a little bit of evidence and if it's positive, it subtracts some. And the further away from zero it is, the more it adds or subtracts, so it pushes it towards zero over time. But it's one of the things that still need to be tweaked a little bit, so it's still, the evidence can still grow quite high. yeah, it's not a working idea. My thinking was, that if you're exploring an object and you develop some evidence and then you start exploring a different section of it so that particular body of evidence doesn't get refreshed while you're concentrating some other aspect of it. Is, does it really want to decay or, if you're getting negative evidence against it, that makes sense. But I was just worried that you're decaying it because you haven't seen anything for a while.

yeah, so I guess right now that doesn't really happen because we always see something. but yeah, I guess at the moment, the decay factor is mostly just to keep the evidence count in a certain range because we're not normalizing it. But yeah, I'm not, sure yet what the best way to do this. I wonder if, this addresses kind of what Kevin said, because I guess, yeah, the evidence doesn't stay with that part on the object, it moves with. It moves with the new displacement. so yeah, I'm not gonna as long as you're still on the object, it's not going to decay. Yeah. Okay. Maybe we can take, in a future research meetings and go into some of this in more detail again. Yeah,