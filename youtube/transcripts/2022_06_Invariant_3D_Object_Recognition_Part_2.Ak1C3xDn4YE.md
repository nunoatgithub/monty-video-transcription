Okay, great. yeah, thanks everyone again for tuning in. And, this is a follow up from yesterday's presentation. so apologies. I think to, I guess everyone except for maybe Christian was there. So sorry if not everything is super clear, but if you were at the journal club. that I did two weeks or a week and a half ago, then hopefully most of this will make sense. but anyways, going to be looking at, yeah, carrying on the discussion about view spheres and these preferred views and tying this in with this kind of motivating question of rotation invariance. And then the that will be the first half and then the second half will look at what do we do when we're sampling points of the input space that are not familiar that we don't have a learned representation for. And yeah, so for rotation variance, the kind of general approach that I'm describing isn't fully rotation invariant in the sense that you have to test multiple possibilities in order to arrive at a solution, it doesn't automatically handle rotation variance. Thanks.

perfectly, and I'll explain maybe why, to do that. and then how to do this, but in an efficient way with basically some kind of heuristics, experience and then also how view spheres can help with this. And then I'm not really going to talk much about this, but just the kind of particle filters that, come up a few times. This could be a potentially useful way to handle the sort of more brute force nature of this problem, just because particles can be, parallelized, quite easily. Can I just ask about the particle filter things? I made a comment in Slack. I watched that video and it struck me that in that video where they described particle filters, it was very, much like the union property we do in sparse representations. Had we talked about that before?

I think we have a little bit, not, explicitly, it's, yeah, there's definitely analogies. I just based on what that video showed. I don't know, I think I was part of it. But in that video, I'm saying that's exactly what we're doing with unions. Yet the multiple hypotheses are being basically occupied simultaneously. It's not a, it's not a probability distribution, but it's multiple hypotheses. And, and, they're being narrowed down through this movement, sensory movement experience. Yeah, I think the one difference is you do need to be able to read out each particle separately to be able to do things. maybe that's the way they implement it in particles, but with But the, but in the union you do simultaneously because of the union of the, because of sparse properties. But other than that, it's basically the same hypothesis. You have multiple hypotheses, multiple possibilities are being narrowed down through, continued inputs. I just thought, wow, that is exactly what we do in the temporal memory. it just struck me, it struck me. But they also resample, around the most likely ones? Yeah, they do, but, those are Details, right? yeah, some of those would be that's what I meant. Some of that kind of thing is hard to do with unions because you can't read out each pattern independently, but there may be ways of doing something. I assume that in the park filters are like, these are. These are probably, I don't know, some floating point numbers and they're very precise and if they have to re sample or something, I don't know, but but so there are those differences. I'm not, but the idea that you've got multiple hypotheses, accurate ones, and you can start out with a lot of them and then you just get narrowing down and you're doing them in parallel. maybe in the actual particle filter code, they do them one at a time, but theoretically they're being done in parallel, and that's what unions are doing. And the fact that the union is not a distribution in the sense of a normal probability distribution, it's multiple hypotheses. anyway, the language in that video, it was a very clear video, it was simple, but the language was like, wow, that's very similar to Tumblr. Anyway, I thought that was good, so just wanted to point that out before we go on here.

Yeah, No, I, yeah. And I think Marcus at least has discussed some of the similarities, at least when I've spoken to him. And, and I think in, the kind of columns plus paper, he has a paragraph about the similarities, but that's funny. I don't remember that I've posted on slack. I think in the thread. But, yeah. but I think it could be helpful because there may be some, it's still unclear, but there may be some very efficient libraries for particle filters that we could use. And, that might be a, just an alternative. Or, either, temporal membrane could be an efficient method for particle filters,, it's like that's an efficient mechanism for doing it. if you could combine those two. Alright. I just wanna point out brute four sounds like it's not desirable. But, when I thought about, hey, temporal memory does this, and it's, it can be very, efficient, I think. So anyway, I just want to mention that. so yeah, so why even debate whether we want to, maybe on some level you'd think, surely we want to find some elegant, perfectly, rotation invariant representation of objects. But, in general, invariance comes at some sort of cost and sensitivity. So a classic example of this is max pooling in CNNs. Reduces the kind of spatial sensitivity of the feature representation, which helps with translation invariance, but it also coercions the spatial code of features and potentially makes them have these kind of undesirable properties, like being approximately bag of feature detectors and so forth. and so I thought, if humans seem to use kind of a search based solution, or at least semi kind of search based solution. Rather than solving this problem. Test perfectly, then that might represent the fact that biology has found it to be a good trade off. and, yeah, and then that way we can focus on the representations being able to handle other, equally challenging advances like translation scale and just novel examples of objects.

the kind of this isn't a new idea. The kind of one obvious thing to do is to just test the most likely orientations when you come across an object. and but just to get into this a bit more. so most likely can be a combination of experience heuristics based on local features that I'll get into. and in order to talk about this. I'm going to loosely define it is principal access of an object. And so every object will have this. And it basically is just with reference to how the objects is normally placed in the world. due to the constraints of, for example, gravity. So a coffee cup.

and it doesn't specify rotation around the principal access. So Kafka can obviously be oriented around that in various ways, but at least the principal axis tells us if it's right way up, upside down, or on its side, or in some kind of more implausible orientation like this. And, as you'd imagine with experience, you'd come to learn that some of these are more likely than others, and should be tested, it should form a kind of, denser part of the distribution over possible rotations.

and interestingly, humans do seem to do something like this. this is a study where they looked at, how quickly people are able to recognize. So this was just line drawing. So to the images. things like airplanes and, yeah, boats that were oriented, in the plane, at various angles. And this is how quickly people recognize it. lower down is faster, is better. And at kind of an upright, typical orientation, people are very fast. They're able to tolerate slight deviations from this. And of course, if you turn it completely upside down, then people are much slower. What's interesting, though, is that the kind of worst ones are these kind of deviations from the, so Upside down is seems to be better than upside down and slightly skewed and similarly being at 90 degrees seems to be better than these slight deviations from that. So that does suggest that, yeah, we might have these sort of non uniform priors over what the kind of typical orientations of objects in the world are when we're trying to recognize something we're Parsing through this feature space and, testing hypotheses, with greater likelihood that correspond to these kind of more common orientations.

but of course, sometimes the objects, won't be, in its typical orientation and it'd be nice to have some other way to cons constrain the hypothesis space and make it faster.

one thing I was thinking was that the kind of local features in particular principle curvature can give us some information. some, sense potentially of the kind of orientation.

but essentially a flat surface will generally be, or often be, in objects either parallel or perpendicular to, this kind of principal axis, often because it represents the base or because it represents the side of some object. Obviously this does not always hold. if you had a polyhedral dice or something, there's, many examples where, this doesn't hold, but, but this could, as I'll get into a bit more concretely, be one way to suggest, okay, the axis might be in one of these. directions.

and even if the local feature and the principal axes aren't flat, they give us, sorry, the principal curvature isn't flat. It gives us, the maximal and minimal curvature and one of these is also again typically oriented, so either parallel or perpendicular to this, principal axis. Are you defining this principal axis with respect to gravity, purely? Essentially, yeah. Yeah, because this cucumber or whatever it is, you could argue the principal axis is actually, almost horizontal. yeah, and yeah, I haven't decided for sure if that's the best way to do it, but I think Yeah, but, in general, it doesn't matter too much in the sense that normally, normally, yeah, if the principal access isn't, like that way, it's, gonna be that way. It's if you see what I mean, it's not gonna be any of the kind of. Angles in between, right? but yeah, but, but when I was thinking about it, I thought just a natural way of defining it because you can have very kind of asymmetrical objects and things like that. So actually, maybe just a natural way of defining it would be when you encounter this object in the environment. What is its most typical orientation? Yeah, in some ways it's simpler than thinking of it as shape based. It's more gravity based.

Yeah, exactly. And because the problem is, if it's shape based, you at some point you have to decide, you then have to first learn, okay, what is the shape of the object? And then decide, and, and how you parse that. And, so then bringing all this back to, view spheres, The idea is then, so yeah, for a view sphere like this, teapot, you have, the features, are on this view sphere relative to the learned principal axis. And when you're inferring an object, of course, it can be at any orientation in the natural world. So you need to not only infer where you are on the object, as in where you are on the view sphere, but also, what the orientation of the object is, that you're actually studying. and so this can also be, conceptualized as a sphere where you, the point on the sphere is basically if you take the principal axis out of the object. It's pointing to wherever it is oriented on the sphere surface. And so this is just a single example of a, orientation hypothesis that you might have.

And, so what this means then is, for example, if you were to see a feature, and the idea is you would have multiple hypotheses that you're running in parallel about this orientation. So this is what I mean that it's search based.

You're just testing in one, along one axis, so you don't, represent rotation, around? Yeah, so I'll, exactly, so I'll get to orientation, because, if you have an estimate of the principal axis, of course that estimate may be wrong, but it, let's say this is one of your hypotheses, is, the first one, and this is another one, if you see a spout, That tells you, essentially what your, what the rotation is about the principal axis, because, and this is one of the, benefits of the view sphere is because you can only see a feature, yeah, from one particular side, then as long as you have an estimate of the, axis, which again could be wrong, then, Seeing a feature is going to give you some information about, the rotation about the, the axis. But isn't that feature, isn't that the same problem as the whole object? I'm looking at the spout. I have to figure out the orientation of the spout. It's, it's, So I'm, assuming here that the, spout representation So if the spout representation tells you some information about orientation As well, as in, whether the spout itself is upside down or right side up, that just gives more information. It makes it easier. I'm assuming that the spout representation is invariant to rotation, but why would it be? it seems established as another object and it seems just the same problem as a whole topic. The whole teapot. It's I haven't figured out how to do this. Yeah, that's, a fair point. I guess, I was imagining, yeah, a local, feature detector that could. Recognize, the things from, yeah, because it's, a smaller feature, generally simpler features you've encountered more often in the environment. And so you'll have encountered that more orientation. So this would be more dependent on training base. I'm not sure I buy that. So what I was saying yesterday, I think you have to assume this is in some sort of a hierarchy or multiple cortical columns and every cortical column is doing this. So you might have a cortical column that. Knows about spouts and it's doing the exact same thing, but at the level of the spout. And then you have a higher level, maybe a V4 IT or something like that's operating at the level of the full Maybe, I don't know why It's the same operations, it's the same thing. It may be, I think, but the problem is that it has to occur at each column, or yeah. Whatever it is, we don't want to rely on hierarchy, but the point is, that spout is not really easily recognized. in fact, if I isolated it from the teapot, in this view, I'm not sure I'd figure out what it is.

it's certainly not oh, there's an oriented spout, they already know the orientation of the whole thing, it's, I don't think so. Yeah, I think if you isolated the spout to rotate it 180 degrees, you wouldn't be able to recognize it. in this picture, I don't think I'd be able to recognize it there at all, it's, if I just isolated the spout, anyway, yeah, no, it's you can make a hypothesis based On the symmetry of that image, that would work, or there's an implied principal axis of the symmetry of that image, but I don't think I could get it from this table.

I'm not sure this is a problem because, again, you have lots of cortical columns at different scales that are operating together and voting on things, and the only self consistent view is this thing. all right, so you're saying orientation would be voted on, and Yeah, I'm assuming that, yeah. But Because you have multiple hypotheses for orientations. Yeah, but we still have to ask how each column's hypothesis is generated.

Everybody's rotated here, so all inputs are rotated, so everyone would be confused. Everyone has to solve.

I don't know. Yeah, I think, yeah, I think at some level I'm approaching it from, sorry. Yeah, but that gets back, it's not like it happens instantly, it happens over time because these unusual orientations do take time, as, Neil showed earlier. So there is a some sort of a search process or settling process. there's two different things. One is, one is the brain could literally be trying different orientations. You could still be voting on it, but you might be going through them surreptitiously as opposed to, Oh, there's this distribution of orientations and the right one comes out. That doesn't seem right to me. It's more like it seems like we, because it takes longer. It's not just a voting thing. It seemed like there might be, like you might be doing multiple orientations. But yeah, it can still take longer, even if, yeah, even if you're doing multiple, trying multiple orientations. It could be both. This could be some combination. Yeah, it could be a combination. Yeah. I would agree that there is some voting and settling going on, but it, still seems like it wouldn't be like, oh, some of these guys have got it right from the beginning. Everyone's going to be confused by this. It's a pretty confusing image, Yeah, I think, I guess that would be part of this, if we go down this road of testing some of this stuff out. how would this happen in parallel, across cortical columns? It seems like it's plausible. What is plausible? Multiple quarter book columns could figure out these unusual. Yeah, but are you saying that some of them guessed right to begin with and they went out? Or are you saying Or eventually it will guess right. It could be some combination of parallel. I think, it seems to me that it has to be some combination. Yeah. If I look at this teapot, Ken Interspecton's one I've read so much, and I said, what, really clues me in on what the orientation is of the teapot? I think it's the little knob on the top. That's a very, that piece itself is, it's it's got more shape to it than the other stuff. So I focused in on that one. I like the spout itself is very confusing. It's yeah, I agree. Oh, it's like very touches the table. Is it touching the table? Yeah. I don't see a table. The table is on the top right. I don't see a table. Yeah, I see it. I don't see that at all. I just saw the teapot floating in space. Interesting. It's difficult when, obviously, because also, yeah, here we're looking at the whole object. Yeah, at once. And how? Yeah. How well we would do this? because, yeah, all these kind of, task looking at people recognizing objects that weird orientations that is showing the object in its entirety at once. It's not requiring that they cicade over a tiny. Are they able to cicade or do they just get a flash in it? I'm assuming they're able to cicade because, How much time is involved? Yeah, actually, no, not in this, case, yeah. Yeah, that's funny. Very quickly.

but yeah, but they're basically processing it in parallel. And so that's, of course, going to give additional signals about orientation of an object. Also, my subjective experience based on introspection is, if I look at this thing, I feel like I like to rotate it. It feels more comfortable. That's why, that's the question, right? Why does it feel like you want to rotate? you've got, there is an implied principal axis in that image, symmetry, and, and that would imply that it's not right.

principal access boundaries of the image itself. So yeah, the way it's that's true. I'm not trying to ignore that. There's something that wasn't part of the test here. you may better cut the images around, just give away the orientation. Yeah.

but in 100 milliseconds, you don't really get a chance to do much at all. Yeah, the maximum two seconds, I don't think. you typically get three to five and a second. And so you, I don't think you in a hundred milliseconds, you would not be able to be just one presentation.

interesting. I don't know. Okay.

but yeah, but anyways, but so I, guess the whole desire or kind of motivation for this is that the hope is that potentially the, or one of the benefits of the view sphere, maybe that it reduces the degrees of freedom, of testing orientations of the object, because I said, yeah, in this case where you're You're getting a sensation, which in the reference frame of the view sphere is along the side. Then you, you just have to test all the hypotheses about the orientation of the principal axis. You know where you are, with respect to around the, the principal axis. If you, on your first sensation, you, experience the lid, then you need to test, all the hypotheses about the orientation about the, about the principal axis, but you know the orientation of the principal axis because it's pointing at you. if that makes sense, and then if you test somewhere in between kind of along these surfaces, then you generally get more information about both because you'll get some information about the principle, assuming that features, are, are not symmetric on every surface. If obviously if features are symmetric on every surface, then it's the typical issue of just you'll have to test for all the different features that coincide with that. But at least. This constrains the or should constrain the number of orientations that you have to test.

and, yeah, the kind of this example where you first feel it spout and you have, for example, these two hypotheses and then you and rotate the object like that with one hypothesis, you then see flat, which is inconsistent with the other hypothesis. You see the lid. And then so you know that principle access was the correct one. and it's Yeah, it's just the standard idea. But, so what I think this would then basically help limit the total degrees of freedom of, pose. And, so both location on the view sphere and orientation of the object. So the kind of general pose to, four degrees of freedom, to basically the azimuth and the elevation, on the view sphere, and then the azimuth and elevation of The either the principal axis or the orthogonal axis, whichever it is that you're trying to work out, depending on what you first started feeling.

I'm lost now. Okay, we're trying to figure out the orientation of the object and you're, seeing you're limiting it now to four degrees of freedom, but that seems like a lot. I typically think of orientation as three degrees of freedom. so four degrees of freedom also includes on where you are on the object, so it would be six degrees of freedom if you were both the orientation of the object and where you are in x, y, z space. If I just think of the object as a whole, not like what part I'm focused on, it's orientation. So orientation is only two degrees of freedom.

it, that's, yeah, I guess it, yeah, it bundles them together. you, you get a course orientation with two degrees of freedom. You get full description as well as of the orientation, as well as where you are on the object. in general, just to be clear, I'm making sure I'm thinking straight here. In general, orientations in 3D is three degrees of freedom, right? Yeah. And, your location is three degrees of freedom. Yeah.

So this is trying to do both with in total fewer, but yeah, you're having to describe orientation with if that's all you cared about, then yeah, it's using more degrees. It assumes you're centered on the object somehow, right? Yeah. Yeah. That you're like holding it at a, a fairly frequent, or common distance.

if I am actually the distance doesn't hopefully doesn't necessarily matter so much. It's more that you're Yeah. Rotating it consistently with respect to your eye, you're not moving it all over the place. if I'm presenting this to the eye all at once in a flash, and we have to assume that there's, voting going on, there's multiple models trying to model this at once. otherwise I have to think about Oh, I can only see a small part of the object through a straw in which I'd have to move my eyes. So we're looking at a situation where you have multiple models, multiple columns. Each sampling object in different places, they have to vote to reach a consensus. they're not going to, in this 100 millisecond example, there's no, saccading going on. I'm just stating things I think are facts in this situation. I'm not drawing any conclusions from it.

And, Yeah, I think, yeah, this, so this won't be able to tell. This doesn't say anything about rotations about the principal axis, right? That's the degree of freedom that's, is that true? That's what's missing here. So that's what, but that's what comes with the view sphere. With the position, it comes with the position of the view sphere. Yeah, the position on the view sphere. Yeah, It's because we've taken this complex object and flattened it out to a sphere. And you can only move along the surface.

yeah, doing this without movement, isn't something I've, I'm assuming at this point that This is being solved by, yeah, rotating the object. I, I have, I, have a approach this with the idea of, how to do this with, flash inference. Okay. but that earlier, experimental data was with flash inference, right? yeah. I'd have to double check because that could have, for example, been a corrected, estimate of how long it takes them to respond as opposed to how long the stimulus was presented for.

I assume that, was how long it took them to respond. That was my assumption. Okay. So after stimulus onset, yeah, they flash an image and how quickly can you push the button, or identify or something? no, it has to be an offset because you can't have zero. Yeah. Oh, so the shortest time that a person can pursue something is 60 milliseconds. Yeah. could you show it again? yeah, you asked me today is going to do a press or whatever it is. Okay, it is stimulus duration. It is mean stimulus duration. Oh, it's stimulus duration. So what, does it mean? It means How do you mention it? This means if I gave you, if I, if the stimulus was any shorter period of time than this, you wouldn't be able to recognize it? Is that what that means? This is the, time it takes to which you can then recognize the object? Up to some accuracy level, probably. Yeah, exactly. Probably something like that. I need to look again, to be sure. Okay. But yeah, it would be something like that. So just to be clear, is the response identification the object or identification the orientation of the object? the object. Okay, so you could, presumably, assuming that there's some verbal omission involved, rather than it's purely phobia omission, you might get a general impression. Exactly, yeah. Yeah, you're gonna get the whole image, the whole thing's gonna be presented to you, but but it's, I'm going to assume that a soda straw is just not enough to, solve the problem. Yeah, It has to be, involving voting of some sort. Otherwise, you just, because you can't do it with a straw, therefore, and we're not moving the eyes, because this is a stimulus presentation, you don't have the option to move the eyes in this purge of time.

okay.

but yeah, basically, what I'm trying to get across is that it may be that, so, they the, first stuff I, discussed about testing most likely orientations and heuristics, that It's more general that isn't specific to the view sphere. But, yeah, what I was just trying to get across was that it may be that if we adopt this approach, we can reduce the degrees of freedom, that basically need to be considered, which can make the system faster. no matter how it's, yeah, implemented.

and this is just hopefully making it a bit clearer, Yeah, how you can test, with this kind of probability distribution, for example, with, particles where, initially you might have this prior that the Principle axis is either at the kind of upright or inverted or on its side, which is anywhere along the equator. But then, for example, you get a, a sensed kind of curvature feature, which has the principal curvatures oriented like this. And so that gives you some low evidence that principal axis might be actually oriented like that, or, I forgot to include it there, or around that equator. And then you just update your sampling. based on that, Neils, I just one thing about, because when I, we went to the four dimensional thing. Yeah. Are you assuming it is possible for the principal axis to pointing head on at you? Yeah, that's what I'm, that's what I was saying. That if, that's what happens, then, what you need to, then you know exactly what the principal axi. Because the feature you're seeing is associated with the exact direction of the principal axis. But then you need to test the orientation about the principal axis. if the principal axis is head on to you, you don't have any features in line with it to validate it. What do you mean? So in this case it would be the lid. if you're looking directly at the lid, you may or may not have enough information. I'm assuming there's more information if the principal axis is somehow, parallel to the plane of vision, rather than perpendicular to it.

if that was the case, then, is that true? What do you mean about more information, sorry?

when the if you're used to seeing the object with the principal axis vertical. Okay, yeah, it's not perpendicular to you. Yeah, it's parallel to the plane of view. that's your, if you wish, your most common viewing position with gravity obtaining and you're coming in from the side.

it takes a little bit of mental leisure to me to say if all of a sudden I'm, I've not seen the thing from the top and all of a sudden I'm being shown the picture with the lid being the foremost thing, it might take a little bit more effort to actually recognize that because the principal axis is now head on and all the features that we've seen so far have been oriented with respect. Isn't that the same as saying if I'm seeing the object from a viewpoint I've never seen it before, I may not be able to recognize it at all, right? It's because I just haven't seen it from that direction, or Potentially, but the, I guess what I'm trying to say is that it's not the most common. Yeah, but the most common means I haven't seen it very often. It might, it may or may not have a lot of features. For example, if I look at the bottom of a car. there's a lot of stuff there that could identify the bottom of a card, but unless you spend some time underneath the cards, you may not recognize it at all. Where, I'm trying to head with this is that if, the, capability of recognizing the, principal access is only one dimensional. In other words, it's rotation in the plane. It still gives you a lot of affordance in trying to figure out what the object is. it takes in my mind. If you're allowing the principal access to come out of the plane of the viewing direction, you're asking a lot more in the system.

you would expect it to be slower axis.

So, yeah. With, this approach, you would expect it to be slower. If the principal axis is coming out of the image at the f the I by definition, if that's the case, then you're looking at an uncommon view by definition.

so I'm saying that you might get. A way in a lot of situations with just having one dimension of the principal axis, identifying that it's going to be somewhat easier task than the case where it's foreshortened in your right. But, yeah, it'll definitely be easier and that would be preferable. But of course, there's going to be the possibility that the object will be oriented that way. There's a possibility but it might be you might. Bring other mechanisms to bear to resolve that. In other words, you might not be able to do it in a flash recognition, but you might be able to do secondary mechanism. Okay. Yeah, I see what you're saying. Yeah. So just basically consider. Yeah, whether it needs some other kind of, yeah, step involved when that happens. Yeah, there are definitely times, I can, easily imagine situations where a flash inference is not going to work and then you have to, start attending the different features and figuring out what those features are and, but there's other times where you can do it in flash, inference, at different orientations. So it seems as we were saying earlier, it's a combination of both, right? There are times when you have to break apart the system and attend to different components. Other times you don't have to, Yeah, I'm actually, in some ways I'm surprised by how well this works, because there's classic examples that, for example, distinguishing T's and L's and recognizing those in periphery, with very short presentation times, if the T's and L's are oriented right side up, the familiar orientation, then you can do that pre attentively, and very quickly, but if you, if they're skewed at 45 degrees angles, 45 degree angles, then you can't so you're fixing at a point your fixation at a point they showed in the periphery and you have to. Yeah. Okay. Yeah. And basically, yeah, so there is an effect of kind of the familiarity of an object. And being able to in parallel, process it. Interesting. What puzzles me about this experiment you've shown is, it's, if it's, if there's some mental process going on, it takes longer if it's at an angle. Why does it matter how long it's presented on screen? Because what you show here on the y axis is the presentation time. Because what they typically do is, I think, that's probably what happened here is once the stimulus is gone, they mask it with some. Random dos or some mask thing, and that wipes out your, processing completely. So if you, it's not like the image is gone and you can still think about it in your V one and V two. It's like they is being disrupted. Yeah. So I imagine that's, I'm not sure that's typically what they do. Yeah. Otherwise, you're right, you can just keep imagine playing it. If you didn't do that, which is interesting, like, where is that? Where's that image reside? I guess the cells keep firing or something. Yeah. Yeah. But they keep firing away. that if I didn't mask it, they keep firing away that I can mentally manipulate still. So, it's like, you've got this impression, but somehow I'm still able to manipulate that, even though I'm not getting it, it's you'd be trying to give it the neural mechanisms for that. It's weird.

to, visualize things too, when there's no signals at all. So there is circuitry for doing both. They did their column.

that experiment is pretty consistent with that. idea we presented a couple of months ago, where you have each cortical column maybe having a few different orientations that it's exploring in parallel and different cortical columns at different orientations that they together figure out which is the best one without any, additional sensory equipment. Yeah, and if you just have more cortical columns representing the more likely orientations, then it's more likely that those will converge quicker. Yeah.

yeah. just the kind of last thing to say on the rotation invariance is just that, one thing I haven't addressed is how do you actually learn the principal axis? I mentioned that it was relative to how the object normally sits in the environment, but, yeah, I was talking to super tire about this the other day, and yeah, he was just maybe there's something where we can initialize it randomly. And then, yeah, iteratively update this and that's something to do with the kind of frequency of encounter orientations or, something along those lines, I think, but the way you phrase it as being dependent on gravity makes it actually easier. It's not. Making it shape based is a little bit trickier, I think.

yeah, I think the gravity would definitely be a good starting point. Just assume that the thing is, your first impression is assume the thing is oriented vertically relative to gravity, is that what you're saying? That would happen most of the time. Yeah, some of the time you might see this first time on its angle, but very likely the first time you see it, it's oriented in a natural way if it's gravity based. And, if you, yeah, I'm assuming that over multiple sessions you would encounter an object, you wouldn't necessarily. I suppose your, principal axis on the, if the first time you encountered it, if that's some weird angle, then maybe your principal axis is initialized at that angle, but, that it could be updated as, a more common orientation. Yeah.

And we typically, learn to the world we, experienced things multiple times, even a single time I'll hold something around or, it seems. It's hard to imagine, oh, I only saw the copy cup once and it was at this weird angle, and I never saw it again, or I didn't manipulate it when it was like that. But in the columns paper and columns, we just randomly did it in the very beginning, and then we never updated it, even if We only did one at the beginning. The anchor, the orientation, the object's reference frame. The object's reference frame, yeah. Including orientation, randomly initialized. Yeah, That's a subtlety of But we were, we didn't even do orientation in the cons plate, but we just did anchoring the grid cells. We assumed the orientation, we always assumed orientation was the same, right? And just, yeah, we didn't really solve a bunch of these problems.

Yeah, it's an interesting exercise to think of this without movement, just as a flash of, that forces you to think about the loading. Yeah, And I guess on the flip side, it also has to work with just a single cortical column. With movement. It has to work in both scenarios.

It's the same, it's just voting in parallel or voting over time. It's the same thing.

It's still not clear to me, and maybe this is just a misunderstanding, Mike, but how voting really deals with the spatial arrangement of the columns, because Generally, I feel like from my understanding, that's the key difference is that generally with the, obviously with inference, you're doing path integration. And so you're taking account of the spatial arrangement of features, whereas. Voting, it's more about, okay, what is, the hypo hypothesis at this location. But when the voting is taking place, the columns don't have any sense of where they are relative to one another. Yeah. Yeah. And they need to, no. they would They need to. We didn't. We didn't show that in any of our papers, right? no. But we talked about it in Monty, every column. the position of the sensor relative to the body and, this sort of thing that is still being accounted for. That's been, yeah. Yeah. Yeah. Okay. Okay, cool. That makes it more simple. I even have it, even in the Thousand Brains book I wrote, right? I was saying it's these little agents in the town, but now the agents know where they are relative. Oh, yeah. Okay. I remember that bit, actually. Yeah. Yeah. We never implemented this, I don't think. No, but it is a core piece of Munty. Yeah. Every module. Every cortical column knows its position relative to the body. Yeah, because otherwise it's just going to become another bag of features.

yeah. And this allows it to work, whether it's touch plus vision or, one cortical, one visual field versus another visual area, it all works out. It has, you're estimating. At the end of the day, you're estimating the orientation, the pose of the object relative to the body, and you know the pose of the sensor relative to the body, and now you just have to figure out the pose, and, you have the pose of the sensor relative to the object. Those are the three quantities that you're working with. In terms of, Just random again, I'm just thinking out loud, so I just hope you don't mind. I keep coming back to this idea that I, there's a really good chance in my mind that, this orientation, a lot of, a good portion of the orientation is being handled in the thalamus. And one of the things about the thalamus is it takes, it is a way for many, large areas of the cortex to be processed at once. because it's all close together there. it's not as distributed, this large area would be one, but the, equivalent, processing center in the thalamus is close enough that they can all be connected directly. And so that would imply somehow that this that the orientation resolution is being done, it's being applied equal to every, all the columns. It's not like each column is independently solving the orientation. It's like they're solving it together.

you can have multiple, you can't, you can have multiple hypotheses, but there's only one to be tested at a time, something like that. I'm just, again, thinking out loud. Yeah. And we thought about the processing speed of the it seems like It should be all at the same time, right? it's just specific orientations. I wasn't thinking about speed. I was just thinking in terms of, the thalamus is a way of, it's one of the questions I've always asked is why, why is the thalamus, why, if it's, many people think of it as an extension of the cortex, but why isn't it distributed throughout the cortex? One of the reasons is if you bring it all together, then you can Do a simultaneous process that affects everybody and so all the columns, it's a way of applying something quickly to all the columns. So again, it just suggests that from, it would suggest that the individual columns aren't independently trying out different hypotheses about orientation. There's going to be a single attempt, that applies to everyone. If that doesn't work, there's going to be another one. It goes away from, the classic voting on orientation, more towards it's going to be, everyone can, contribute a guess, but the Thalmus is going to try one. It's not going to, it's not going to multiply the hypothesis going out at the same time. The paper I read, might have been less for anatomy and more of a hypothesis, but the notion was that for the LGN, that a certain amount of edge detection was going on. Does that correspond to your thinking? No, That's not related to what I was just saying. I'm not going to make a relation, but I'm just asking, is that the case in your knowledge? no, actually it's not the case in my knowledge. I wouldn't be surprised if someone made that claim. It seems like someone makes a claim about everything. but the general view is still that, oh, it depends on the animal. I'll think about it. I think they've discovered that in some animals. But in primate B1 or, even, I believe they have not seen that. So I think that was true, for rodents or something like that. But in primate B1, like a monkey, or I think even cats, they don't see that. They see only center surround respond properly. Okay, because There's a, the paper kind of went on to it was more image processing, but the notion would be that you could come up with the notion of orientation by voting on the frequent, the number of edges in a particular orientation. Yeah, I think in general in primary vision, you don't have that, you can't, it's just centered around. And I just don't appear until you get the couple and see it gets the fortune. I think it's a pretty solid result. It will be a nice voting result.

Yeah, I don't see it as voting necessarily, but it seems I don't think Nathalos could vote. Yeah, it's more like it's just doing a transform on the input. And no, I agree. I was not saying the voting was occurring there. I'm saying they're producing inputs that could be voted. you can produce the edge in the thalamus. You can produce the edge in the retina, you can produce the edge in the cortex. but if you're trying to center the notion of orientations on the thalamus, if they're basically, I think that's what I'm saying is the thalamus could test, could, is at least as capable, it seems capable of. Our hypothesis is that it would be able to, to reorient the input and in a sense give you a chance to try it out, say, hey, what if I rotate this thing 30 degrees? and, then the cortex looks at it and says, oh, that looks normal, right? it's more of an implement, a deorientation mechanism than it would be, But we don't have a necessary mechanism for hypotheses to produce. the cortex would have to provide those because it's through the pediatric form. Okay. Okay. Got it. Yeah, I'm always struck, and I've been this many times before, is that your head is constantly tilting left and right. When you're reading, when you're doing anything, and you just don't even know it at all. There's no, it's just totally compensated, even though the input's on the retinas. It's getting wonky and twisted all around as you do this. So there's got to be a very ready mechanism for compensating for that kind of orientation change. And you think that this would be something that was discovered a long time ago, and animals started moving around, because they move around. So it would be a sort of pretty fundamental mechanism, to overcome these orientation differences. Now we're trying to extend it to complete objects from upside down and things like that. Talking about animals, how old is evolutionarily speaking, the thumbs? I don't know that.

It's very tied in, at least in mammals, to the neocortex. Yeah. I don't know.

It was birds. They seem to have on a physical level, a compensation for the body orientation. They're very good at keeping their heads really straight. I would argue the opposite. birds, you look at a pigeon, their heads are going back and forth. And I've heard people say, like, how do they see? Aren't they going crazy? And I said to myself, no, the perception of the bird is the world is stable. It's the same like our eyes are moving all around, but we don't notice. It doesn't bother us. And the chickens or the, the pigeon's heads going back and forth. They don't even see that. They just, the world seems stable to them. It's just like a cicada for them.

There seems to be pretty strong homologues between bird structures and hortical structures. So there's these blobs that seem to be like a section of cortex in some ways. I think there's going to be very similar neural mechanisms going on, even though they don't have a cortex at all. But multiple people have made this analogy now. that, hey, these blobs, but they literally call them blobs in some of these programs, that, hey, they look like they might be in some other coral structure. But they're not all that many sheep. They're just blobs.

I was going to say, yeah, so the, last bit of the kind of presentation is just to go through then, So the patchwise representations, and in particular, this is an issue not just to use views, but in general, if we've learned nodes or kind of viewpoints, whatever you want to call them, and we have features associated with that. What happens when we sample at inference nodes that we're not familiar with and how to handle that. so this is just an exploration. Are you saying we're seeing the objects and we're seeing parts of the objects we've never seen before? Or we're just seeing From an angle that we've not seen before. An angle we haven't seen before. for example, you've, you've primarily studied these kind of principal viewpoints. And then, but you may not have seen it from a very particular angle, or this could or, or this could be a new features. It's just a different orientation. Exactly. Yeah. Okay. So that's. Yeah. Okay. I thought that's what we're talking about all along here.

the, but the thing is, when you learned the objects. You'll have learned features associated with certain viewpoints, but ultimately that's a, it's going to be a semi discrete representation of space, whether it's a grid mesh or it's a view sphere, or even a 2D plane. and but you need some way of kind of handling the continuous space between these sort of points where you actually subscribe to memory. a particular representation. Okay. So there's like a mechanism for doing this. Yeah. So what I'm going to focus on is, the feature representation and just how to maybe make that kind of robust to or able to handle this. Okay. and yeah, and cause basically the desire is to, it's not plausible or, desirable to learn every single possible orientation of an object. Sure. You might then be able to recognize it from any orientation, but it's just, yeah, memory wise and Efficiency of learning. It doesn't make sense to do that. So how can we sub sample the space and still generalize this idea. And and, we seem to be able to do some sort of interpolation of features, even for completely novel objects. I'm assuming you've never seen these particular objects before, but you can make a reasonable guess in your head of what you're going to see if I move these. It's not this. that wasn't what you probably predicted in your head. It was probably something more like this. the ring and the fur continue. You may have also predicted these kind of little bumps, that there would have been more of those. but yeah, the basic idea is, we can, even for novel objects, we can do a reasonable job just given a set of local features of predicting, a nearby feature. And, there's some work from DeepMind in 2018 where They were doing something that I think is a related problem where they had this, room full of 3D objects. So a simulated room. And basically the agent received, multiple viewpoints. So this view, and this view, say. And it would then learn to, given, and, with those viewpoints, it was also given their orientation. So it would get, a view and an orientation, view and orientation. And then its task was to predict, given an arbitrary viewpoint in the room. what it would see from that angle. And it could, it could learn to do this, using a deep neural network. and so the idea was that it developed a representation that was both invariant to particular orientations and able to process, the particular orientation to give a particular viewpoint. And so the kind of thought is just that to learn, a long term function that approximates given a few local viewpoints what you would see at an unknown point on that. This can then be used in conjunction with Kind of temporal memory principles. on an experiment like this, there's two ways, I don't know how the deep learning network does this. There's two ways, one way is you give it a very few set of images, and it builds an internal model that allows it to rotate this stuff. Or, you've given it a gazillion images, and now your novel one is just a little bit different than the one it saw before. which is it? So yeah, it's probably a combination of both. So they did show some examples, for example, that this worked with objects had not seen before. and, and also it could generalize to some degree for it's a different numbers of objects. For example, but it did eventually break.

but yes, more like how much training does it get right? I think a lot. Yeah, yeah, I think it's in the millions of images. Yeah. So then, we're trying to figure out a more mechanistic way of doing this, where there's these internal representations for orientation and location and so on. which I'm pretty sure the brain is doing, but I generally think of deep learning networks as not having anything like that, although some practitioners will claim, they're going to learn that stuff. but, so it makes a real big difference. the example we use all the time here, so I've used many times, is you walk into a room and you see the arrangement of furniture, and then you leave the room, and then you approach the room from a different direction, and You can imagine what the room would look like, and you can recognize it right away, and it's it's just a couple of presentations, and, I can do that, or I can do it from a novel, and I can imagine what a room would look like from the ceiling, and I've never seen it from the ceiling, ever. No, I don't think so. my point is, I can do this for very novel situations. I'm saying that I don't think an infant has that capability. of course not. You have to learn, right? No, that's what I'm saying. Yeah, but, I don't learn it in by, Presenting me with millions of different my point was I can learn this on a novel situation with one from one direction I don't want to be presented lots of different directions. I just do it from one direction Okay, so I don't have millions of training. I just know I'm saying no training on All right, let's. Representing space. So fine, let's just put it aside. There's a mechanism in the brain that's either learned or evolved, it doesn't really matter. Now I present this new thing it's never seen before. I give one presentation, I very quickly scan on it, then I come from a completely different direction and I recognize that and I can make predictions about it. So, I didn't do that by receiving knowledge of those rooms, right? No, but he's saying, I think he's saying that this thing can do that, but only to a limited extent, right? Exactly. Yeah, it's much more limited and it's much more low level. It's what I'm trying to show here. this is a completely novel object, but just given You know, continuity of color, continuity of texture, continuity of edges. You're able to do a basic job of inferring what you're going to see. It has nothing to do with the fact that you're familiar with this object, or how it changes. It just has to do with low level, essentially statistics of.

I would make an objection to this particular example, just to be clear on the one on the left. I clearly envisioned it being a ring and I know what rings are. And and so if it was even slightly different from that ring, I would know it was wrong. On the one on the right, I can't make any predictions about the individual bumps and stuff in there. All I can do is basically some sort of general, if you show me a different arrangement of bumps than that square, I wouldn't know what's wrong. So there's a difference. That's fine. so what I'm going to describe, it's fine if it was a different arrangement of bumps. What we don't want is for the predicted representation to be as meaningless as the smiley face. we want it to be something, right? I'm, I just wonder the one on the left is not based on statistics. The one on the left is based on the fact that I recognize a ring. If it was based on statistics, I might have just as well assumed that the brown fuzzy stuff is where the, block is. yeah. I don't argue against that. I think, yeah, I agree. I guess the problem is, at some point we need to deal with the, fact that we're sampling space discreetly and we need to kinda interpolate between it. And, yeah, and so I don't know. I just think when I think about things, it might be careful. Yeah. Yeah. So it may help if I motivate a bit more what this is trying to do. of course, this is a course example, but just let's say when we studied it, we'd seen the teapot from the view of the handle and the view from the lid and we'd. Memorize those and wrote this memory. but now we're getting a view from this angle, and we're trying to so we have the kind of pose, and we're trying to estimate what we're going to see there because, they're not going to be anything written to memory. And if we just try and use the nearest node. That we do have in memory that may not work that well. So here, obviously, this is a great course that we just have a feature represented here and a feature represented here. Of course, we have some in between. But the point is that, they're gonna be in many situations where we move off of the learned points on. We want to estimate what's there. And for example, in this case, this is where we currently believe we are based on our, Kind of history of inputs. And we also in memory have this representation of the lid at this pose. and so the question is, how can we use this information together? To, make a kind of guess about what the view is going to be. And, what I'm suggesting is to use a neural network, which is, at the end of the day, a function approximator to, essentially approximate given these data points. So these, the views and the poses. Both from memory and from, our input history, as well as the pose of where we're going to be to then, estimate this, possible viewpoint. And then, when we move there, we can compare the internal estimate to what we actually experience. And that can give us some, sense of, okay, we are actually at this query point, or actually, no, this is completely wrong. We're potentially on another object.

and, But then the, kind of key point then is how do you train it to actually, approximate this function, and I think, what would be useful is to combine ideas from both, so contrastive semi supervised learning, which is essentially what these top two, terms are showing, and then a, term that's, a bit more like what the, DeepMind paper was doing, So just to break this down, so this is the loss function, so this is what the neural network is being trained through, say, back propagation to minimize, and it has three terms. These gamma parameters are just scalars that determine how much influence a particular term has on the whole equation. the first one, and yeah, simim is just a similarity measure where the higher it is, the more similar two representations are.

So for example, so for this first one, this is about getting the features to be invariant to noise. So this isn't actually about the interpolation question. This is just, the first two are about just having useful features generally, because interpolation isn't the only problem that they need to handle. So yeah, so basically what this is saying is that you get given a viewpoint and you perturb it in some way, you add Gaussian noise. You maybe flip it around, a mirror access or you, change the color slightly. And basically you just want the, representation still be, relatively similar and invariant.

the second one is, basically saying that you want representations of different features to be different. So if you take a viewpoint at one point and a completely random viewpoint, ideally from a different object, then those, representations should be different feature space. Again, and sorry, just to clarify, so F is a neural network as well. This is after it's been transformed by the neural network, that the representation should be different. And then the last one is the key one for interpolation, which is. You have this, other neural network, which is sitting on top of, the first one, and it's taking these transformed features as well as their poses, and potentially taking an arbitrary number, depending on how many you have to work with, so up to K, representations, and then the, query point, and then it, you want it to then give you, okay, what is the viewpoint at that, yeah, what are you going to see at that viewpoint, and based on, yeah, The history of, papers in machine learning or kind of recent developments in the last few years, something like this should develop representations that, can do a decent job of giving a few viewpoints from memory, as well as from the, what we're actually seeing, predicting, what we'd see in an unknown space, and it could be interesting whether introducing sparsity into these representations could also help with Making the kind of compositionality a bit more direct. of course, a handle and a lid are very different. ideally the representation that combines them, and from that novel viewpoint would have both of those aspects. And having sparsity may help with this. but I wonder whether we want to move away to some degree from STRs and have real valued neurons. at least for this particular point, because yeah, having, SDRs causes problems with, gradients. You can't do gradient passes through, binary representations, and, but so it could be a mix of, SDRs and real valued neurons.

and so just connecting this back to some of the early ideas. So the idea is that, the fact that humans have these preferred views could just be a parsimonious way of kind of capturing useful views, without having to store all possible views and memory. and then, these kind of interpolation, this interpolation function could then handle. More novel orientations.

it's important to say that, yeah, although this is all based on long term learning and deep learning, this is still all dependent on also dependent on path integration and. Rapid learning with heavy style feature representation. So it's a mixture of the two.

and then, yeah, it's as I said before.

yeah. And then, at the neural level, if we're implementing this as kind of columns, it may be that the kind of if you move somewhere and there are no known features based on the movement, then. I don't know, maybe a lack of dendritic input initiates this kind of interpolation, which could be a more top down, feedback. and it's, yeah, one thing that I haven't included in the kind of interpolation equation, but which might be interesting is a global hypothesis about the particular object, which might also constrain, Or inform what you're going to see at a particular pose, and and this might actually benefit from something like the, continual learning dendrites paper approach where you have context applied to the, neural network.

and yeah, I'm, almost done. but just to say, yeah, the one difficulty of this is what happens if inference starts at an unfamiliar point. Because, I've described works if we've visited nodes that we are familiar with, and we're visiting one that we don't know. But, but it's not clear what we do if we start and then a junior point.

this is going to be most important for the first instance. First instance. First sensation. but what you could potentially do is interpolate location. So basically, you're given your century input, and you have the storage features in memory, and you could find the K nearest. Features in feature space and then basically interpret the pose way in the, contribution from each feature based on how similar they are. and if, yeah, for example, we're doing something like particle filters, then, your estimated pose would be probabilistic. it wouldn't have to be just a single point. And this would, Yeah, basically mean that you could, start doing path integration from a point on the sphere or point on a graph without having necessarily ever visited it before. I imagine this might be quite computationally slow to compare to all these. Possible features, but, you would hopefully only have to do this for the very first sensation, and you could maybe have some sort of threshold. So it's only if the first sensation is very unfamiliar that you begin this process. If it's very familiar, then you just associate your location with the node that is associated with that familiar feature. I'm missing something here. I'm feeling something that's unfamiliar. Why wouldn't I just move and try to find something that's familiar? You can also do that. You can also do that. Yeah. That seems like the simplest thing to do, Yeah. you can also do that. Yeah.

seems so what I would do, but yeah, I guess sometimes it, yeah, I dunno. A lot of the, yeah, I think it, it'll just depend on how sensitive, the features are. I think in practice to, matching, whether, yeah, whether that's, I don't know, in, in how close you actually have to be to, for it to count as familiar. that's true. There's still, there's always that question, how do you deal with variances? but as a general principle, it seems like if I see something that I don't understand, I keep looking around to find some part of it I do understand. Yeah, that's true. and yeah, and that would avoid this, slow, Mostly for this.

but yeah, but so that's, that's everything I want to talk about. yeah, the next question is then, whether we want to, at least for the HCM work, yeah, do something like view spheres, and or planar views. but, yeah, to my mind, still, the planar views would, it would be useful for translation and variance, but I'm not. I think it'll require some form of kind of sphere like coordinates in order to deal with rotation and variance.

and yeah, that's everything. Thanks very much.