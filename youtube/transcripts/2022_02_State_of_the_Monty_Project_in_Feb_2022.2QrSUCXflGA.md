Okay. So I was, I've been working. a bunch of problems and I was, I, would love to be able to present the solutions to those problems, but I'm not ready to do that. But I figured instead of just saying, Hey, I'm not ready again, I think at least walk you through what I'm thinking and what I'm working on. And maybe that would be helpful for people and we might book, questions and so on.

this is just a list of topics I'm trying, that I've made and, actually they're mostly in my head, but I wrote them down this morning. So I'll show you these lists in a second, but they may not be complete. first of all, over the years and previous months, but actually over the years, we've made a list of all these different things that the models in the brain have to do, and it's quite extensive and difficult. we.

And try to figure out how to do this, like melodies or this or that and so on. But right now I'm trying to resolve all of 'em. So I, have a long list. I'll show you what they, what that list looks like in a second. But I'm trying to be more like, Hey, maybe we can tackle all these at once. That's pretty ambitious, but maybe, I've separated out, hierarchical models because. up to now we've pretty much focused on what a single column can do. We've pushed off on things that require multiple levels of the hierarchy. so this, that means the models themselves are arranged in a hierarchical sets of models. And I'm trying to deal with that now as well, which we've avoided in the past. But some of the things we're trying to solve require that, I'm trying to do this in two, two modalities at once. I've recommended we all think that way, so I'm thinking about it in touch and vision. And it turns out when you think about touch and vision, there are differences that are important. And, so I become aware of those and said, oh, let's change my thinking a little bit here. another dimension that I'm trying to, we have always tried to do is understand a single column versus multiple columns, like a single column versus columns voting. And I put a question right next to single column, 'cause I'm now beginning to think that even a single column is insufficient. That you might need several local columns next to each other, at the minimum. So you can think of a spectrum of how many columns are involved or how many models are involved. We used to think it's one to many. now I think it might be very few to many. And, so that's a new thinking on my part. by the way, I should just mention Lewis. I think there's a problem with those sensory presented yesterday. I didn't think about it last.

When we touch things, we actually almost always rely on multiple different pads of sensors on your skin that are not the same. And so I can talk about that, but like you slide your finger on something, the edge of your finger bumps up the things, and you might need multiple columns to detect curvature and things like that. So there's a, I had a question for you about that. another thing I've done is try to look at, I say the toolbox of neural mechanisms. We have defined two major neural mechanisms for doing inference and model building. one is the temple memory algorithm, and the other is the temple pooling algorithm. These are both cellular layers. They both are useful for narrowing down and inferring. and I've been trying to understand 'em at a very generic level, thinking about and you've seen me do this, like the temporal memory amalgam can be applied to different types of data. And so just trying to. Another way of thinking about the solution to the modeling problem is like, what are the mechanisms that could allow that models to exist? And so I've been thinking about those sort of very generically trying to understand 'em a little bit better. And then the thing I'm not doing that we really still need to do eventually is so the action policies and goal and behavior as part of the models. I haven't been working on that. I've been, I've just been thinking like, okay, that's just too much.

Maybe, I dunno, it's too much. So I've been, I've made a list of all these things and I have all these ideas and I'm trying to resolve 'em all at once. Trying to figure out what is the, structure of models in the cortex that solve all these, that meets all these different constraints. And and we have pieces of the answer and we pieces that are mysterious to us. I'll not just walk through some of the, some of these items a little bit more detail and then I'll then the third page I'll describe what my current thinking of solutions are. Does that make sense? No. Any questions about this so far? Again, I just wrote these lists down this morning, so it's not like I've, the list themselves of printed words here. Could be, I could easily forgotten things and not thought about 'em, but they're mostly in my head. I figure well put 'em down papers in my head. So just let's talk, let's just talk about, list of model capabilities and starting off with a single level, meaning a single column or a single level. Again, maybe a couple columns next to each other. So these are just some of the things we, we talked about. We know we have to do inference and novel poses. That's a problem we have to solve. I also point out that, there are 2D objects and 3D objects. I'm able look for a circle and a sphere. the, mental logo is 2D, and yet the coffee cup is 3D. So this is an interesting dilemma that like, oh, how does this system deal with both of those? then lately we've, I've come to think about it as objects defined where my by morphology, so these are, objects where there's continuous surfaces, but there's no salient points. So there's no point. You could say, oh, there's a feature here and a feature. There's clip book example is a circle and a cylinder. I've talked about a bunch of these recently. How we can, recognize and how we can learn them. it really is, it forces you to think or made me think oh my God, you can't really just say there's these features that were figure out that displacing you. Then there are clearly objects that are defined by features at some relative displacement. For example, A word is, seven letters in a certain arrangement. That's it. There's nothing else to it. There's no, there's necessarily morphology to the word. It's that arrangement in a certain order. If you change the order, you get it in word. Another example would be a bicycle, which is basically composed of, wheels and frame and pedals and things like that. and then I've been asking myself, are these really, can they be the same? I'd like 'em to be the same. That is, I'd like them to be two points on a spectrum and that is what I'm working on right now. can a same mechanism do both? and I'll talk when I talk about the solutions, that's my current approach. I'd rather not say, oh, there's these two types of objects and I've talked about here on the whiteboard saying that, but I don't think that's, I'd rather not that be the case. I'd rather it be somehow we just have this continuous, mechanism and in some sense, I'll get to it when we get to the solutions though. another thing that we have to deal with is changes in scale. And, there's multiple ways this can happen. There's changes in absolute scale. So imagine I have this coffee cup and now I have a little miniature version of it. I, recommend the exact same thing, but if it was scales and I would be able to manipulate the smaller one, just like I manipulate the bigger one, I with no problems with that at all. I'd say yes, I know it's smaller, but I would notice it's the same thing. But then there's other changes in scale, like envision you can, a change in scale with perspective. We saw that the picture of the truck that Heco put the other day, you like on the retina, the rear of the truck is smaller, the front of the truck is big. that's a different type of change in scale. that doesn't occur in touch, but it does occur in vision. so that's an interesting question. Then there's changes in scale that are not like absolute. you can take an object, you can stretch it. So I can, say, oh, imagine if this is a fatter version of this same height, but fatter. I can easily imagine that. And. And I can learn that. And and then I would, I was somehow able to take my monolo cup and stretch it in one dimension. So that's a weird thing.

we'll come back to that again. So scale is not a singular thing, right? It is, Various weirdness is about scale going up.

then there's a distinction between static objects and changing objects. Changing objects, sometimes I refer to as behaviors of objects. And, so objects. It's easier to think, it's easier to think about. Static object never changes. but we do have objects that change in morphology. That is the, that is. the actual object themselves can change its shape. The stapler is a perfect example, of that. There are other ones, there's plenty of examples of things changing their shape and their arrangement of their features. oh, and then some are changes in features. Like you can say, okay, the morphology object change, but the features say the same. Or you can have the morphology object say the same and the features change, like the icons on the display of the, of your phone. So that's weird. and then there's there's d there's a spectrum of these changes. The very predictable ones, like the stapler, it only have a certain set of behaviors. It always the same. It's, fixed. We can learn those. and then there's unpredictable ones the one we've been talking a lot about, like t-shirt, a t-shirt has a shape. You can imagine what a t-shirt looks like, but half the time it's in a pile on your floor or something like that, plunged up net. we're still able to manipulate that and work with it. it's thing, it is the same t-shirt. We don't get confused and say, oh, that's so different as a pile on the floor. so there's a spectrum of morphological changes and changes that can occur on objects. and then of course, we are able to infer and represent the object state. So it is, I see the stapler open or close with different icon on display. I, know that and I somehow there's a state to that object and they say, yes, the state was open. And how do I represent it if I have a, if I have a model of an object in one form, what's, how do I impose that state on that? and is the state saying no, the state was normally closed, but now there's a state where it's open. it's a weird stuff going on that which to figure what's all about. then I walked one time I talked about class versus specific instances. So that's another problem we have to deal with. what's a cup versus my cup? What's a cat versus a tabby versus my cat, right? These are all scales of on a spectrum of classification and inference. and we seem to be able to do that pretty well. we can run specific things, class of things, and the class of things can be broader, narrow. We can have different labels on different levels of the class. So that's another weird one. Yeah. in that case you're considering, one specific object would belong to remaining classes, right? So your cat would be a cat and your cat. So it's just two different classes and object belongs to, I, all the words made sense, but I didn't understand the original question. Yeah. My, my question, are you thinking of different classes? there is my cat and there's, I wasn't thinking different classes. I dunno what the right word to put on it is. I think that people got like an ontology. You have animals, cats, and then my cat, that kind of, and even my cat, there seems to be the spectrum. you can break off pieces along the way and say, Hey, it's got the same set of features, but the arrangement is slightly stretched. That's one thing. Or you could say it's the same exact pathology with different set of features, or some combination of those. and you can put labels on, and so you can say, yes, that's a tavi versus a, Siamese cat or whatever. and but then you can also say, that's my TAVI cat. I, it's faces. So I guess all I'm saying is there's a continuum. Class of labeling abilities.

and I don't know if my cat is a specific, oh, there's a special version of that one unique thing. I think it's just seems to be a spectrum of, similarities and we can label those so I can, it's not like my, somehow my brain is able to, put different labels on different parts of the spectrum of shapes and features. It's a surprising thing that you can do that. it might be easier to say, oh yeah, we're gonna just do one classification two, but it seems to be multiple ones. I know. I dunno. I just, these are open, they're not, some of these are not really clear how you specify this, but it seems you are able to do this But WordNet does that for every English word, basically, or all common English words, like this hierarchy. It doesn't have, your cat or my, or whatever. It doesn't have that. But everything about that is, but that's based on semantic versions of words. This is based on physical structure of some object in the world. So it's a, maybe they're a cheating, similar kind of result, but they're on very, yeah, that one is semantics. This is very different. I now you could just look at statistically and use it of words in the world to make the, make that connection.

no, I think it's it's semantics. Yeah. But the semantic come, it's not shed based. No, But the semantics come from word usage. Where does it appear? They don't sit there and codes down whatever. No, This is a hand, this is heavy.

fine. I don't, that's not what I'm talking about here. Yeah. Yeah. Here we're learning a model through sensorimotor in inference, right? We're sensorimotor modeling, we're touching something, we're seeing something, we're building a model that I need to classify that model. I need to do inference. I need to do inference at novel poses. And I'm, somehow able to do inference at different levels of, I I can put a label on different, changes in the, in that concept. There's again, the changes in morphology, changes in size and changes in feature sets. And those seem to be the three things you could do. And and we're able to pick off pieces and label 'em. Yeah. The only reason I brought this up is that the graph matching algorithm or development should be able to handle those. Like it's never gonna match one thing. Each one object has to be several things. So we can't return a single, each one option has to be several things. Like why is that? the cat's gonna be an animal, the cat's gonna be a cat and the cat's gonna be garbage. But, couldn't I just say this is my, this is a coffee cup and be done with it. Does it have to be something else? there is, like I said, there is your coffee cup. There's, maybe not. Maybe it's both. Maybe, I don't know. I, okay, fine. I, guess I wasn't enforcing that it had to be multiple things. I'm just saying it's capable of doing multiple things. So what I, the problem I get is when I think about how neurons do this, we, our, basically our classification is the temporal pool of layer, right? Where we, that's at least where we form a stable representation for an object. And, so you can classify that pretty easily, but it'd be very difficult to classify that temporal pooling cellular pattern in you. You could associate different classes, but it, I, don't, I, it's just complicated in the brain. Maybe it's easy in the method you're using, So again, I'm thinking about this in writing mechanism, so maybe you've got a solution to that. That's fine. but anyway, it's something I think about it gets back to this whole issue that objects are, stretchable and morphologically changeable and the feature sets can mix and match and yet you still have a model that works for all of them, right? It's like you can say, yes, that is a feline. Animal, but it could be a tiger or a cat, or, something else. Who knows? Lucas was your original point. That it's not just, one thing belongs to multiple levels of the same hierarchy, but that you can have different group eye operations. So like a cat is, yes, it's my cat, it's a feel, it's a ma but it's also okay, what are things that are fuzzy? Okay. Jess jacket, my cat, a stuffed animal. And so switchers, yeah, I guess that's, what I'm asking. Just HI think even horizontal, that gets back to the point we discussed a month ago that, there is no one class for anything and it's just not like a car. I agree with that. It there's a spectrum of patterns that represent this thing you're observing and you can assign. Some subset of those patterns to some label. Yeah, exactly. Yeah. Associations. Yeah. But it's interesting because if you think about the representational structure that you're dealing with in the brain, that representational structure, you have to be able to do that somehow. it may not be hard. I, it's just, a thing that has to be done.

I, I didn't say these are all unsolved. I just said these are things we need to do. So that's a good point. I I, haven't been thinking about this particular point too much, but I've been in your category, look, it's like, I'll just be able to associate this with different labels, but I have to have flexibility. what qualifies as a tiger is also quite flexible. So there's a range that's a tiger and there's a range that's cat and a range. That's, wild animal. I, there's a whole bunch stuff. Not a hard one. I just put.

we haven't really spent a lot of time talking about hierarchical models. at Menta we've tried to avoid that. We've tried to say, let's stick to a single mo, a single learning column, a time. but I'm trying to think about that more right now. And, and even then it's it is really interest weird actually. So I'm thinking like, okay, what is a hierarchical model? The hierarchical model is, you've got, some mo basically some object models that are now part of a bigger object model, right? And, and you can break 'em down to different types. So here's here's an interesting one. We have a morphological model on top of a morphological model. So this is like a coffee cup with just a circle on it, right? The circle is a morphological model and the, cylinders a morphological model. So there's no features in any of these things yet. I'm place a circle on. Ah, okay. That's interesting. so I can't rely, I have to be able to accommodate that. Then you can have features on morphology. That's a classic example of the logo on the cup. the logo consists of individual features range to well to each other, and now I can place that arrangement on the cup. So I don't know if you understand why this is, these are bothersome to me because if I'm thinking about morphology and features as sort of separate things, then it gets complicated how you do all these things. and then you can have features on features for example, feature models, so like letters, you think letters as an object. It could be in a word, and the word can be in the logo. So you have, these are just relative arrangements of pieces and relative arrangements. These are relative range of pieces. And other than the letter itself, it's not morphological, it's just it's like displacement of features. And, then there's another interesting question. It seems like we can go very deep hierarchically in these compositional models. and we talked about this, even in I think framework paper, like they could be recursive. and, but you have to be able to do this in a finite number of levels in the hierarchy. So I can imagine okay, a bicycle's got a wheel and the wheel's got, a tire and the tire has a stem, and the stem has a screw hole, and the screw hole has a, valve, and the valve has a little piston, I push on it. And so there's a lot of levels to that hierarchical composition, but I can't rely end levels in a hierarchy in the brain to do that darn enough. And, that's not how it's gonna work. So somehow the mechanism accommodate, recursively Down within the same set of neurons, somehow not get lost is the best I can describe it. Regarding the features and features letter. Example, consider letter to be a morph. Yes, I did say that. I said letters are morphological features, but now it's an object. So now I have this object called the letter and the arrangement of the letter is features relative to each other. And then the arrangement of the word, memento relative to the i, the triangle of logo thing is also features to features. So it was more like, it was like letters, letters. it, it, was like, I'm just trying to show that the letters arranged in a certain way and then the word is arranged in which of the components of the logo. Certainly like discussing that two weeks ago, we thinking along the line that. Actually more like a pixel color of a specific point.

we have to be careful getting features, yeah, I, that may be a different use of the word I'm using here. Yeah. in a moment. I, wasn't trying to be careful about here. what I meant here was, you'll see in a moment, what I think a feature is just another object. So that would be a way of it, of putting this, you could say objects on objects, so letters, objects, and then a word as an object. So you have letters on objects and then another object. So on I, I think I'm gonna get away from the word features at all in the future. Could I just do a on that, you saying, you keep saying your circle has no features. It has no, what the word I use up there? Continuous service is no salient points. That's the word I use there. But they have boundaries. Those are salient. they're not, specific. A salient in my means it's different than the rest. You can salient means this thing is more interesting. This is how it's typically used. This thing is more interesting than the other things around it. In this case, there's nothing more interesting about one part of the circle. Then the other part of the circle, there's no, there's nothing to do to directories to one point to say, this thing is the thing I wanna pay attention to. But that's a description of the west. I'm just saying there's no salient points. Okay. So nothing you can fast it on, but. I didn't say there's no features. I said there's no sounding points. Okay. Obviously we've talked about the solution to this as being a continuous feature of curvature and, we'll get to that in a second. But the point was when I first started thinking, we started, thinking about building models using, displacement. It was always this thing here, this object is at this location and this object is that location. We can calculate the displacement between them and, these, there's a lot of things involved that just don't have that. I understand your distinction. Yeah. If that's how you're trying to represent Yeah. There's no points in which you could say, this is the thing I want, calculate my displacement from. That's, what that means. No salient points, randomly points, but that's not gonna work. So we.

It has to come down to local curvature at, any point. and then learning that continuously so that you have a continuous representation of the feature space in some sense as opposed to, a punctuated or a representation of feature space. Okay. That just forced you to think about that times, touch and vision. I, mentioned earlier there are differences. I've always pretend there aren't. I said let's do, its works in both cases, but it's not really true. Vision has, when you actually see things, it has scale that changes with perspective and that doesn't happen with touch.

that's a difference. And there's gonna be no mechanisms that deal with that difference. when you touch something, you know exactly where your finger is. Let's say pretend you do, and, but the thing you're touching doesn't change and scale based on how far away you look, right? So you know the depth, but you don't have to infer that from scale and scale changes from the way it is. I, the interplay between envision, touch and the scale, look at the cost on there. Can imagine picking them up fingertips. Yeah. Car, it's a can imagine touching the whole side, you aren't actually fool, you don't think that's a change. Little car, it's a big car and it's, there's an optical illusion. when you look at things on the horizon, you're very bad at judging the size of them. you think the moon is really big. You, if I look at a ship across the highway, it looks bigger than it actually is, and it's just, you just can't shake this.

It's sort maybe like the what columns in your brain or saying, I know what this model is and I'm not really, I'm doing it independent of scale right now, the model, and so the, it sort separates the perception of what, actually it's related to that problem. But I don't think it was a small shift. I don't think it, the moon is being teeny. anyway, it's interesting problem, in the neural mechanisms. What about the voting layer? That's, the temple pool. I included that.

okay. In fact, that's the main, that is the main method by which it does inference in my mind. it, you can narrow down by sitting, passing in a series of sparse representations, but its inference power really comes from, from voting with other columns. Yeah. I, just included that there. and the spatial pooler is that, the spatial pooler is included with temporal memory.

I just viewed, there's two two cellular layers. Okay. so there's a cellular layer we called Temporal memory, which includes a spatial pooler and then a temp, temple Pooler, which includes voting. what about attention? Were doing attention.

good point. Good point. oh yeah, I'm sorry.

I just, yeah, I just thought that those were, Attention. We don't have a neural mechanism, at least in our papers. Yeah, we don't have mechanism for it. but it is, I would put that under, it needs to go somewhere in here.

how about we can just say, how about that? How about knowledge transfer? I, you, you could argue I don't think of that as a separate thing. I, that to me comes up under, Okay, we could do that. yeah, that's another thing I didn't list here. I don't, so would that be a neural mechanism or would that be a property somewhere else? Yeah, I think I, think that is, I just haven't, it's in, it is in my head when I think about learning. I didn't have that, a question about that. I What about I short term memory and learning and that's not anywhere here? I think the short term memory is, that issue I'm not particularly concerned about. but learning. Learning, yeah. yeah, sure. I, guess there's an assumption under all of this is for learning to movement, right? no, but specifically being able to learn things quickly, I just, I. I didn't break it out because I'm assuming that's basically, that's like the hard thing. for us, learning quickly is not hard. We form synapses quickly, right? Humans. Yeah. Yeah. And, all of our neural mechanisms could learn quickly, right? We've never introduced any normal thought of any neural mechanisms that couldn't learn quickly. so that prob that's not a hard problem for us, I don't think at all. it's the Shouldn't it be in here? I just, I assumed it's okay. Fine.

I guess it wasn't, a problem I'm trying to solve because I think we don't have that problem. That's quickly, you learn very few observations we learn Yeah. Past, this is few ob this is few observations. and I guess I didn't see that as a problem for me to think about because again, because nothing we've done actually is, I didn't even done fast. Observations, the whole, property and the way Spar memory works is it just happens. There's nothing we have to do that. prop has problems with that, but none of the mechanism we've considered has a problem with that. As long as you can form synapses quickly, you're done. There's no settling, there's no multiple passes or nothing. All these mechanisms could do one shot in it.

I would just say, easy with and other stuff.

again, I'm not worried about that. what about senses other than touching vision? No, I'm not worried about that either. First of all, the, other senses are not nearly as interesting. And, we, I think there's, very diminished benefit from adding more. I, think a little bit about audition at times, because audition has some of the capabilities that we have with touch vision. You can locate a sound, but you really can't. there's some limitations to that. And it's really, relying on temporal patterns, which we do automatic. I didn't put that in here. I could have put in, learning sequences because I think that's part of the whole thing I just didn't put in because I, the way I look at, it's a temporal memory algorithm can do both. It can learn sensorimotor inference and if things repeat in time, it will learn sequences. It just comes for free. So I didn't have to put that in you. what about an, airborne molecule analyzer to, that appeals to me like with the coffee example, because. So, we're trying to get core fundamental learning things right? And so anytime I think about another sense or smell or touch or whatever it's sound or tasty, you have to ask them, is it gonna anything fundamental to the algorithms here? This is not a practical exercise trying to figure out, oh, how would I implement, an aroma tester, right? this is a, this is like more like getting a fundamental base mathematical principle by which, models are built. And then I could throw in, color and, tonal qualities and, chemical sensors and a whole bunch of other stuff. Is this sort of, oh yeah, I add, I can add these other sort of attributes into the system, but I don't need to worry about any particular one. Okay.

and, I think in some sense, I think some of those are not. Getting at the core essence, again, think about vision. You can see pretty well in black and white, right? color ads, but it's not really essential for a vast majority of what we do. So it's not oh, I have to have color. I can't understand vision. That's not true. not true at all. so you have to ask, what are you adding for that? So I put in a little category for the side, yeah, we could add these sort of, some sense, important but not essential components to inference. I could add a color to something and say, I can sing between your cup in my cup, like color. That would be, yes, I should be able to do that. but I just put that in the category of there are other sense things that are not so spatial and.

Anyway, I'm, that's how I do that. I get, again, you have to, this is already a huge list of things to deal with simultaneously. so I'm still making, some choices, right? One way to look at it is what is the two set of census that someone could have to build a workable model world? So Kelly Keller with a little bit of pre-learning, because she lost, her vision at sight, wasn't birth, was a little bit after that. But she built a whole model of world just on the pure touch. She, was very young when she lost it, wasn't She, she was young, but she had enough. the key thing in there was that she had learned the word WA or water, and then she finally made the connection. Yeah, okay. That's, she was pretty permanent. She wasn't like she had real language. no, I understand that. So it wasn't like, but she had several months of vision. Yeah, I I think she, she did, she had an audition too. her big, thing was actually that her epiphany moment was that there are words that represent things in the world and that, the person was trying to, trying to do that, it took a long time before she realized that there's actually a, pattern that represents this thing and another pattern represents that thing. So her language ability was very small. I think you can just say pretty much she was close to born without any of these things, but borderline. But you can say, someone who's born blind, or one eye doesn't need that type of death. Percept people born blind completely get along. they have deficits, but their model of the world is pretty good. Yeah. it's a workable model. it's really workable. They just can't, they just can't always infer the right thing. because they can't infer at a distance other than through here.

I think Kevin's point is a good one though. It's I'm not trying to build a, minimal set for workable thing. I'm trying to get at the core essence of model building. So it doesn't mean that vision and touch are the things I care the most about. Those are just two that are really pretty important and self-sufficient and, and gives us sort of a ability to look at the two things, already I can start seeing there's differences between 'em and it's important to understand like the, core components of the model building can't rely on what something vision has versus something touch has, the core components have to be the same in both cases.

That make sense?

Touch and vision Again, I honestly, briefly about the single column thing. I realize when you touch something, you might, I always talk about, oh, there's a single column getting this input, but that's not actually what happens when you touch things, because if you run your finger along a surface, what happens is. You bump up against something this lip here. And I'm not sensing that with the same part of my finger. I'm sensing of a different part of my finger. It's it's a different part of my sensory organ saying, Hey, there's something over here. And so that's one of the first clues, it's not like I have this one little patch that's moving over everything. It's like I, this patch that's moving and then all of a sudden the other patches kick in and hit things and that seemed to really, I try to do inference with imagining just your finger and never, one little thing. It's really hard to do that. and even when you grab something like this, I don't grab it with the, pads of my fingers, this was touching this and this is touching this and so it's not like I have this perfect, thing. Like the kind of sense of who was just talking yesterday. I can always tip orthogonally. I just, I sense a bunch of things at different points. At different parts and, each one's reporting like, oh, I'm sensing a, border here, but it's not the same patch. Another thing is this whole idea of detecting curvature, which is really problematic. but it's not problematic if you assume that you have multiple patches sensing at the same time. and so when I'm touching this edge of my cup here, or if I touch the edge of this whiteboard, there are multiple patches on my skin getting input. And so I don't have to have a single sensorimotor patch saying I've detected an edge. I can, I could rely on the fact that this several in a row and those together are voting on an edge. So that is, that's a big change in my thinking in the past, and it's not it, and sometimes I like it. It basically just says that, inference is on a scale. We used to think the scale was one to many. I now think it may be a few to. And in some ways that's nicer. certainly it's just a different way of thinking about it. so I'm willing to accept that as a premise Now. I talked about the different mechanisms and I talked, I don't really, haven't thought much about action, policy and goal and behavior. I mentioned that already. these are, so this is like the list of things I got in my head plus more that I just couldn't think of right now. And, trying to resolve all these things at once. It's, it's hard.

the, so some, of the solution ideas I'm working about you, you've heard of all these already, so I just thought I'd go through them. Make explicitly, I'm working on the idea that one mechanism could do both morphology and feature displacement. That is, I've talked at times, maybe there's two mechanisms, hey, there's a morphological model and here's a feature displacement model. But the thinking about the About the, the circle and the sphere and the oval and things like that, then we really question that idea. and this idea, you could say morphology is just defined by curvature, which is just a local displacement. It's an instantaneous displacement at some point. and, and then we would store, just, if I move my finger out a morphological object, I'm storing, continuously storing displacement, which is continuously storing the, curvature at that point. And I think that can all need to work with li with reasonable amounts of memory. and, I haven't worked through the details of yet, but I think that can work. And then you could vote and all those things would work. Then, the, second problem comes about, which is, I'm trying to figure out the displacement, for example, between, on this table, the team house and the coffee cup. This actually might be even harder because where's the location of my coffee cup? It's not one thing, it's not one place. do we pick the oid or, I don't like that idea. the original solution we have for displacement in the framework paper didn't rely on that kind of thing. It was just, it just didn't require that sort of point. The point, yeah. That's what it's my doesn't the frameworks and columns plus stuff do that? Yeah, but didn't work. It didn't work. The only reason it didn't work was because we didn't have the reference phase information. Yeah. Yeah. And, but now we have a handle on that. it didn't, it wasn't just that we never could, we could never get it to work. Even thinking reference phase, I could never get it to work. We were using, multiple grid cell modules for that. And derivatives of multiple grid. Some module, now we're not looking at multiple grid, some modules anymore. So that mechanism, I'm not right. The mechanism might not be, but that it still did that and it didn't and it didn't do orientation. That's the reference frame. okay. I, don't know if we could have gotten it to work with grid cells with reference frames. It should be, I'm not sure. We, when we worked on it, I brought that issue up and we just could not get it to work. Maybe you can tell me how to do that. And then it was also an issue of the curvature of the cup. It didn't do that. It didn't do continuous shapes. Yeah. So there was a, we, did it. It was beautiful actually. But it didn't solve all the problems. And we knew that in the paper. We brought it up in the paper and I, and now we've moved away from these multiple grid cell modules, therefore we moved away from multiple displacement cell modules. So we have some parts of it there. So I think that's a really interesting, it was really, it's, that's a good lesson. It's oh, there's some really clever things about it. That's why I brought it up. I brought it up because it's tempting to say what's displacement between these objects. You can just point, here's a point and here's a point and calculate displacement. It doesn't seem to be that's what we're doing. It's more like we did in the frameworks paper, but somehow different. The frameworks paper didn't work. completely. yeah, it's gotta be more like that. So I'm just pointing out, I now feel more comfortable doing the continuous displacement than I do during the point to point displacement, because I dunno how to do that. So I'm working on that.

anyway, in terms of morphology, the temporal memory mechanism, I, we've talked about this. It can do this, it can. It can learn, continuous, displacement. And it can, as with in sequence, like in a temp, a sequence memory, like a melody, it can do point to point, in some sense. it's close. I just haven't gotten all the pieces to work, yet. really the hard thing I'm struggling with is this, issue of, where's the location of this object? Where's the location of this object? And and it feels like it needs to move more like we did in the frameworks paper, but I dunno how to do that.

do that, especially since we're not using multiple grids, some object.

'cause it was, all this evidence has suggested that was wrong.

okay, that's a good one. It required, what it required is required quite a few grids, cell modules, right? Because you, you're gonna, you're trying to create a, an sdr r across multiple grid cell modules. And therefore you had to have this, if you needed, if you, let's say we need a minimum of 10 bits or 15 bits to form a, dendrite to recognize a pattern, they'd have to have at least 15 of these grid cell modules. And they don't exist. And the grid cell modules are in the ENT cords are stretched out. They don't, and they don't seem to communicate with each other and they're very different. There was a pine thing. What's that pine? There was a pine oil, pine tank paper. Oh. not the tank paper. There was some kind of sensorimotor in the, that was in the, in the V one model. Oh, Anyway, it doesn't appear that, at the moment it doesn't appear that we're doing that. There are multiple good cell modules in a single column or even in multiple columns near time. and there doesn't appear to be enough good cell modules. Even if I allow myself to look at neighbors and other things. It just doesn't. I could be wrong. Alright. But the moment there, and as I pointed out, there's other ways achieving the same result. You could feed in, if you think, about the tank paper, which showed what a grid cell module looked like in a plain view of enter cortex. And it was, composed of six, phase groups. And about total, there were about the size of a quarter of a column. So you can you say, okay, maybe the quarter of column goodell module looks like that it's got these multiple phase clusters each representing the bump. And now in each one of those would actually be a mini column. And if I use the temple memory on that, I could call a, perform a very unique representation of space, not by using multiple goodell modules, but by using a mini column, of cell active in each of those. So imagine that the minicolumns of this, of the temple memory are not features. they are the location, the grid cell locations in each of those, in all those phase clusters, that actually fits really well. It looks beautiful and there's evidence to support that idea. So that's a different alternate view of how you get to a unique location. Using grid cells as without using multiple grid cell modules. And, that to me is now clearer and cleaner and probably more likely if it's doing that, if we're, if we need to get unique locations, that seems to be the better way to, but that's sounds like the same mechanism. No, it's not. I dunno if people online can see this tangent. You need to get the we papers in here, wipe this off.

the way we did it in the, framework paper is that we assumed that there were multiple grid cell modules, lots of them. And we had a of activity in each one.

And these modules had to be different. They either had to have a slightly different scale, or, they had to be a slightly different, orientation to each other. So the bumps don't move in unison. They move around and if you sample across 'em. So if each one gets anchored independently, so I'm on an object, a coffee cup, and each one gets anchored independently on that coffee cup, then the combination of these guys would be unique. And as you move around, it continues to be unique. It's this very, large space. The, propo, the alternate proposal is that a grid cell module is actually, like a temple memory, and it's actually a layer of cells.

it's a layer of cells. And these, actual, these cells are really represent minicolumns.

And so you have a bump of activity going along in the mini home space. and yet you're picking individual cells in year. Oh, I see. And this is much closer to the, this is the temporal memory algorithm where you're applying grid cell input versus, feature input. And in this case, you'd end up with very unique, representations of each location. And, and, then you could just, like in the temple memory, you could, you essentially, you could define a space of this object where all the points are unique, right? But you still get, you would still get the, the path integration at the mini call level. And in the tank paper and other papers. What we found, what they pointed out was, individual grid cells. Often would not fire when you expected 'em to fire. And they did so reliably in a particular context. Now, I would've preferred to see it very sparse. They didn't report very sparse, but they did say, individual cells would say, I expect that cell would be fired because we're already, the bunk is here, but not fire cell is not firing. And it does sell always in this context reliably. So it's not some random thing or error. So that's evidence that at least moving in this, but this might be possible. and then I thought the tank, I think about, about when he did his, he does this, they do, they did this with this photo technique, where they can look down in the active cells and that technique actually looks at a very, narrow layer. It doesn't look at the depth at all. And so they wouldn't see the activity of these cells down here. so anyway, that, that's, that would be my current hypothesis, better than this one, because there's a lot of things wrong with this one, and we don't, there's no evidence for this. Okay. So like the duck analogy, it's, going along, but underneath it's padding furiously with all those things that they can't observe. I guess I hadn't heard of the duck analogy, but yes, that would be like the duck analogy. It looks like it's three. It's more like the ducks feeder, just little pieces of them. Know it's not, if the duck wasn't sdr r then yeah. Okay. Jellyfish. So this is how I get, this is I went back to sort that the basic normal mechanisms like the temporal memory mechanism soon as I well includes the spatial pooler. In this case, I wouldn't have a spatial pooler here. I have something that's equivalent to the spatial pooler. The spacial pool takes like a, some sensory input and it turns into a, sparse, or not two sparse, but some sparse representation. Minicolumns here I could, I already have, the green cell is already a somewhat sparse representation of space. And if I, just assumed, that the Minicom was actually representing, that, and I don't need a spatial pooler, it's to substitute the bump for the spatial pooler with the bump for this, the right. And then yet the same mechanism says you gimme something that's not unique, that's measurable in some sense it's measurable because it's path integration. You gimme something that's not unique, I'll form a unique representation for it. And now I can associate this non-unique bunk with a unique location on the coffee cup all the time, and nothing else is gonna be in the same space. It gets around. It also gets around the whole issue of the grid cells repeated, right? 'cause it know, essentially when you, the minicom space, you can only represent a small number of things. And they have to repeat. and, but in the, cellular space underneath it, it's a very large space. and they're not gonna repeat. So I talked to, I think I talked to, Ben and B about this recently, maybe someone else. but I'm work, I think that's a reasonable part of my toolbox right now. That's the same sum that, and so what, I'm actually thinking about, okay, there might be multiple cellular layers in a column that are working on these principles. we've talked about, one could be like taking displacement as, as your, as your sort of, input meaning like equivalent to the output of the spatial pooler equivalent to grid cells. And displacement says, okay, here's a displacement. But I can also represent that displacement uniquely in the context of this particular object at this location. And therefore, in, in the context of the coffee cup. I know that if I have this displacement in the context of coffee cup, I go this way, I'll have something else. In a different context. I, it would be a different thing, right? So you go from I'm measuring displacement, but I need represent in the object space.

okay, so solution ideas.

I already mentioned this one. that, I'm trying to get it to that. There really no distinction between morphology in this, in feature displacement. It's one way to think about this. Imagine you're a, primary column and you're getting input from a sensorimotor.

and I've always said that, this thing you're getting in, you can't really subdivide it. It's not it's imagine all the ones get mixed up on the way up here, so you really, it's. You don't really can't, it's not like you can't detect power in here, but it's, they're not, they're just spatial. they're just powers. And so imagine this, is your input coming into this column. This is the thing. We detect what we might call a feature, and then ultimately you're gonna do temple pooling, because we're gonna have a stable representation of the object. That's the whole goal of the inference here. You're moving, touching, touch, and touching, touching, and then you have a, you have this changing input coming in here, and then you have a stable, representation of, the label of this object, right? This is why the world seems stable and your eyes are moving. This is why you move my fingers. This, I don't think the laptop is moving. so that's like the core essence of inference is multiple inputs get knocked through single or a more stable output. in, in this case, the input I'm getting could only be what's coming from the stemer. Which is impoverished. So let's say, I'll just say, I'm gonna call that, I'll just say maybe I can detect, a curvature, and that's the only thing I can get to work with here, or curvature plus color and other things. I'm not gonna talk about that. But now I have another column, which is somewhere else. And it's getting the output of this guy's input, right? now this is a limited number of things I can detect in the sensorimotor. The sensorimotor can't detect thousands of things. It's just gonna say, you to this way, but the output of the scene can be quite big. There could be a lot of objects that this, a lot of morphological objects that this guy can recognize. Hundreds of them, maybe thousands is there's a lot. So the input to this column is limited, but the input to this column is not very limited. That is now I'm, I can now build, a, compositional model where the input is not coming from the sensorimotor, but it's coming from these other columns. And so I have a, if I call these objects. these are objects that, the lower level has already recognized. Now I have a lot of these objects that I can build decompositional structure. we're here. So now I've started using the language where I'm just gonna say all these things. You have, one object coming in at a time, and then you have a pooled version of a compositional structure. And then you have object coming in, a compositional structure. in this case, the input may be very limited. In this case, it's not limited, but they're doing the same thing. so my point is, By right nature of the input to a low level column, you're gonna learn morphological structure. You can't, I this low level column can't learn, oh, I'm gonna learn, a bicycle because, on the bicycle it also needs to know wheels and then wheels and needs, it, it doesn't, can't detect wheels. So we can learn morphological structure, but this guy could run a bicycle because it's got all these, these, this guy could run all these morphological structures and this guy could then assign them into, so anyway, I'm trying to explain how it is. You might unite these, that the same mechanism, depending what you give it as an input, will run morphological objects or are feature to feature what we might call feature to feature, displacement. So that's the general idea. I'm trying to see if I get that to work.

Does that make any sense? Yeah. Okay.

The other idea I was just talking to, we were talking about this yesterday. Oops. That wasn't me. Did I do, that's not me, someone else? No, that was Ben. Ben, I think. Oh, he was trying to show this to the board.

when we think about displacement, what's really important is the relative arrangement of things. But it seems clearly that the actual distance between those things can change at times. Like very rigid. I looked like my coffee cup is not, but it clearly, I can stretch in my, i if I stretch this object and, so I'll still see the coffee cup. 'cause all the local feature arrangements are, they're not exactly the same distance, but they're in the same arrangement. Imagine you have this Tesla related sheet of features. Imagine just a, and you can stretch it and as long as the features stay in the same relative order position, you view it as the same thing. and, it's like the, t-shirt folding the logo on the coffee cup wrapping around. it's a, it's, that's, that doesn't change the distance, but it's not, basically, it's it's, the actual distance between the features is not as important as the relative arrangement of 'em.

and if you change the relevant arrangement, you get a different object, right? So I can take a word and I can stretch out the letters a bit and, bunch 'em together a bit, and I won't get confused. But if I change the order of the words, I'm gonna get confused. It's not gonna be the same word anymore. So I, all I'm pointing out is that we, when we learn displacement, somehow it's gonna be the displacement are, when we're doing inference, we're somehow able to say.

here's these two features next to each other, A and B. they're in the right arrangement, but they're the wrong distance, but counts or counts up to a certain point. You know what I'm saying? It's that's okay, I'll still get it. I won't reject this displacing just because a is not the right distance for me. But as I said, there's a certain point you cannot stretch everything. Yeah. Right up to point. if I separate the letters of the word out too much, I just see 'em a bunch of letters.

But there is quite a bit of, We were, talking to a high school this morning about the t-shirt problem and how you recognize a t-shirt, right? in the dark or pick up a t-shirt and just pile of cloth and how do I manipulate that? you move hand along it, you find an edge and you move along the edge and decide whether you're on the bottom of the top. And what you really wanna do is get to the collar that has a different feel and then you move your fingers one the collar. Once you get to these, what I do is I get to these two points, these two seams. They're always a little bit towards the back. And now I know that I'm holding the collar and I know, because the seams are not symmetrically opposite. I know which is fun, which is back. So I found these two features that I'm stretching out and I said, okay, now I know where I am on the object and on church, different church it's different. but it's the same method I use. So somehow that works, And the rest of the stuff is the, rest of the pocket, if there's a t-shirt on the pocket, a t-shirt, t-shirt has a pocket. The features of the pocket are all relative to each other. Even if the pocket's bent or curl, you still see the pocket 'cause all the relative features in the right position anyway.

now here's the big one I've been working on. This is like the, maybe the big opening, and we've talked about this briefly.

the original idea behind the model is okay, you have a reference frame from the model and that model has an orientation and relative to the body. So you can take a three dimensional model and you put it any location in orientation to the body. That would be the clear engineering solution here. And we've found that it's.

a lot of evidence for that, and I didn't write down all the evidence here, but maybe a few of them.

so this is the third point. Models have only one orientation, dimension, which is vertical. So there's a lot, there's a lot of evidence that we learn models in the vertical and, a particular one dimensional, one dimension of orientation is so is, has to be correct. And, this, when I gave the example of writing the e and the three and the m and the W, it's not like I see the m and the W as the same thing rotated. I just see one as an M and one as a w and that's it. And it's so the models are. Depended on which way's up. It's not like there's this independently floating model out in space and I have to figure out a orientation. so there's a lot of evidence for this. So there's an evolutionary argument that when you're walking around the world, trees don't, go upside down. everything seems to be in the same thing, but you're, in different positions. there's head direction cells that should be direction cells argument, e versus three argument. And one idea we ssed around in the past, right? I think there's a lot of validity to it, is the following. So we, have two things to do this. I'm gonna argue that the idea I'm working on right now is that we have to have the correct north orientation. It has to be one, but the other rotational orientations don't really matter. And there's, this idea that the thalamus actually is able to compensate for changing in north. the clearest example of this is that as you walk around the world. your head is always tilting one way or the other, and when you're reading a book, it, you don't even notice it, but it actually completely changes everything coming into your brain when you tilt the book 10 20 degrees. It's just like everything's, I, would, I'd have to rotate the reference, James, and you don't seem to do that. and remember the example I gave where I lie laid in bed, some, it might have been here, but I showed the pictures out my window. And when you're lying in bed, you see there's this horizontal scene and my eyes are scanning the scene, but my eyes are actually going up and down because I'm lying in bed. and yet it's, there's no effort in this. And so one of the ideas is that, that the thal is actually, is designed, one of the things it does is.

This rotation in the vertical orientation and the vestibular system, the inner ear, canals actually feeds into that. And so the brain would, you could think from an evolutionary point of view and animal needs to see the world in its upright position and, as it's rounding around. And at any point in time it might be tilted this way, in this way. And the vestibular system says, I know you had to tilt it. I'm gonna correct it for you. it's not like I'm rotating the model in my head. I'm just rotating the inputs and I'm rotating the motor commands. So, when, if I need to move horizontally in the scene, I don't, I it basically let will say, I know that given the current orientation, you have to move your eyes this way, so give the same command and I'll change them for you. so this is, I know it's a little weird, but it seems like this is reasonably, this is happening. so now I can, compensate for the vertical in some sense. And if it gets too much, we know that. If I rotate something around 300, 180 degrees, we know that, then your brain has to so slowly rotate it back. it's a mental effort to do this. If I should recognize something that's upside down, if it's not clear right away, it takes linear amount of time to get it to position where you can actually imagine it. so now the second question is, what about orientation? Like this? So I say, okay, I'm assume that, I got this call vertical, but I different views, wouldn't I have rotate the model? and the example, can I borrow, is that, can I borrow that coffee cup?

Not much. So imagine we said there's a, reference frame for this cup, and the reference frame includes the, 3D orientations of the reference frame. so that would mean if I learned this model in this case with a handle on the right, that I'd have to, as I see the handle on the left, I'd have to rotate the model and figure out what the orientation is. I'd have, to infer the new orientation. And the idea I'm working on is that you don't do that at all. what happens is, if I, think about displacement as always being, imagine just for the moment, imagine they're always in a two dimensional arrangement, right? They're always a 2D. and that is, I'm looking at something, I'm looking at some surface of something and I'm gonna calculate distance to the next thing. But I'm only calculating it's. It's a plainer angle, not it's three dimensional angle. Then, then if I look at this coffee cup and I learned the displacement here, I'm looking at the displacement here, and I look at placements here and I can look at placements here, and I look at the placements here and so on. I end up with, instead of a three dimensional model displacement, I end up with this sheet of displacement. It's not physically sheet, but that's how you think about it. it's like I'm unraveling the, coffee cup in a, linear thing.

the displacement would be more like a, imagine just on the cylinder, you have the cylinder. I'm unrolling it that, here's what the displacement, when the handle's on the left, and here's the displacement when the handle's on the right and, here's the displacement when the handle's not visible. and so when I infer the object, I just have to match the correct displacement. And now, but what basically I'll be doing is I'm matching some subset of this. At any point in time, I don't have to rotate the model. I. This is the model and, and I'm just gonna recognize a piece of it and say, yeah, okay. That's, the model. And that, that, and, so I'm, inferring the model, the object to coffee up at the same time. I'm saying this is what it looks like when the handles on the left and this is what it looks like, a handles on the right. It, I don't think this is gonna take a lot of memory. In fact, it might take less memory, to do this. So it's a little bit weird, but remember, I, I deduced a long time ago that if we're gonna wrap an object, like the logo around the coffee cup, that the logo has to be a two dimensional displacement field. And then the coffee cup has its own morphology and sometimes you can wrap it around that. You put it on a flag, you put it on a t-shirt. and so you forced yourself into thinking about displacement are in two dimensions. That's the, and then there was an underlying, morphology to the thing. anyway, so this idea is that in this case. There is no orientation to the model. You infer the orientation by just recognizing, inferring the objects. And once I infer the object, I'm in some part of this model. And, I can't see these pieces over here right now, so I, don't know that, but basically I'm looking, I'm, making a series of observation about displacement and I said, oh, those match the model in this point right here. and if I was looking at a different angle of the object, I'd say, oh, they matched this part over here. That kind of thing.

it's a little bit strange, but it gets you around the whole idea that you have to figure out the orient. You do have to get the north, but after that you think, now there, there are issues with this. I know there are issues with this, but it's very appealing at the moment, because it solves a bunch of problems. and then it leads to the suggestion like, imagine, I know basically I now have a model that represents different. you, could almost say this is a model of the coffee cup, and this is one state of the cup, and this is another state of the cup. It's oh, this is the state of the cup when the features arranged this way. And this is another state of the cup when the features arranged this way. And again, I don't think this takes lot of memory. I haven't proven that yet, but I think it wasn't, so then I said to myself, could all states of the model be represented somehow in this same basic scheme? could I think about the stapler being closed and the stapler being open as, part of a larger sheet here of some sense of a, set of displacement where there's different regions that represent, the different states of the object. And, I haven't worked this through yet, so it's very fuzzy idea. but it'd be nice. I thing about it's nice is you can end up imagining a model of an object where all of its states are representing the same sort of, Set of displacement. You can't view all those displacement at once. but when you infer the object, you're inferring the state. And, so I don't have to have a separate thing that says, oh, this is a staple. What state is it in? I just says, oh, because I've inferred it in this section of, its of its displacement space, then I know it's state. That's just by definition. And then movement of the object would be some sense moving between these regions somehow, like when the state were open to close. I'm, going from a bump of activity here, representing the state, gonna close state and, over here a bunch of activity open in open state. I don't know if this is gonna work, but that's what I'm thinking about. so this is displacement. Are 2D arranged like a sheet and all displacement we thought of lying on a 2D sheet. Different views of an object are represented. Different regions of the sheet. Inferring a cup with a handle on the left versus handle on the right does not require to determine the orientation of the cup. Both fuse are stored in the model. So what about the known special features? what? Like color? Oh, I told you I didn't wanna talk about that yet. Okay.

the way, I think I'd go back to the way we were thinking of the grass earlier. you can associate with, imagine there's some, you're, at some point in the object and, you can link from that point to other objects.

And maybe feature, like some even maybe something like color, I don't know yet. But the point is you could say, oh, somehow I have to get okay, the, somehow the mental logo is only visible on one of these things here. it's also, it's continuous until it disappears, continue. and, so it's only located, and yet somehow the logo has color associated with it, right? So at some point you'd say, oh, there's, there's this, at this point, this is the, letter N and or this is the word menta. And I have a link to that over here. So I, can't put it in this model. And so I somehow.

the underlying morphology and shape of this object is not completely dependent on the color. So if I know the color, I can might isolate where I am, but if I don't know the color, I still can infer it. if you, as soon as a set of attributes out there and that let's, say, or are even safe that you, have this thing that you're perceiving in front of you model, and maybe it matches in some of those attributes, but not all the others. So you've got something. So recognition would be, it's similar. Yeah. At some point you would say it matches in all the things I know about, so now I have identity. Yeah. Yeah. so that gives you a soft way of, Yeah. Associating things from the most important aspects of whatever it is, being stored in, in the two dimensional map. Yeah. Yeah. No, I think that's right. I think that's right. I agree with that. So this, that attribute is gonna be part of that state now? It would be, I think the attribute is not critical. it's like the, I have a lights on, the lights off because different states of the light that's light, some of them are broken. I, some of them are broken. Okay. Okay. And like once the same light, they have different state. One is on and one is off. Yeah. There's no color there. It's just or something. Yeah. It's this attribute that t that light is on by this off. it, I think many models of the world, I'm hearing you talk about this list, and I say my model of this room doesn't include what the lights look like or where they are or which ones are on. If you ask me to draw a map of where the lights are, I'm assume, but if I tell you how do you represent, that camera is on the camera off. There's a LED there at the off. Fine. Okay, fine. That's an example. Yeah. My point is, you have this sort of, and this is just like we talked about, the graphs, the graph represents in some sense the shape of the object, right? and it's, and then you, can link to the specific things that might be associated with that point. Like you can link to.

the mug, the, actual mug would link to the logo, wouldn't, the logo wouldn't be part of the logo, because we've already learned that separately. so I think what Kevin saying, you can still infer this shape here, independent of this, but if you do know this, it can help you if you've learned that. My point with the lights in the room, I haven't learned what the lights look like or which ones are on and which ones. Also looking at the lights doesn't help me identify the room. But if I study lights every day and I thought, okay, what kinda lights in here? I'm a lighting guy, okay, obviously, blah, blah, blah. I've gotta memorized. The next time I'm looking in the room, I go, oh, I know this room. But most of, I wouldn't do that wrong, right? I just, it's too much memory for me. I think about it. So my point is that these links are, you can temporarily look at what's associated with some place here, but you may not remember. I'm not sure if I'm addressing your comment. I'm just pointing out I'm, just like, I agree with Kevin that this actual attributes to the location will help. Some things are really helpful. I'm saying the lights for me wouldn't be helpful in my case. But the arrangement of those two screens and these windows, yeah. That, that would be, I, that's what I would use. I also think that if the main thing is to recognize where you are and recognize that you're in the same room or, not, then all you just need to know sufficient number of attributes. The room for your perception to say, I know that I'm in the same room or not. I don't have to memorize any detail in the world. Okay. But if I have spent my life as a lighting engine, yeah. I might take in other attributes just because of the fact that I am geared. To add those. I used, I've used an example of, my sailboat right. On a sailboat. And, I know every little detail of that sailboat. You'd walk up and you wouldn't know those things. You'd just kind sailboat. I wouldn't have names for this. Yeah. So to me I'm like, I would see oh, someone with this little clip on the wrong orientation, you, I'd see that and you wouldn't know that. So it just depends how much time you spend looking at it and, playing with it. And, I've spent time calling all other thing. I know every little thing. And therefore my model, the boat is very rich and yours would be very model modest and just 'cause I spend a lot of time at it, anyone can do that. But I think you say a certain level of which you say it's sufficient for me to operate on. I'm being bombarded by GLS all the time. Yeah, And, but if something comes on up that I say I need to observe something more, like I'd be keyed. if someone says, I'm looking for this book, I might, hone in on, on something like that. Yeah. So I think it's a flexible Yeah. I Agree with that. so I, think I've covered all these points. I'm trying to understand object behaviors in this idea of, of a single sort of model. And that you're, what you're doing is instead of having a model of what object looks like and then a, separate, I mean it not like our old views were displaced or behaviors were wrong. I just, let's just have a different mental model for it now.

That, in some sense I have, I have these memories of all these different displacement, all these different, I can memorize the views an object in different states as, as actually what I observed looking cleaner at this thing in terms of its, displacement and as I manipulate the object, I'm looking at different parts. So somehow there's this continuation between, again, I'll just say it one more time. The, couple, the handle on the right and the cup of the handle left are really the same object, two states and similar, then I might be able to do multiple states of the objects, deformation, morphology, or. That, that I would say, oh, okay, now I know it, you know it. That's all I can, that's all I'm trying to do. Yeah. And I'm trying to make, prove myself that a two dimensional displacement and that is displacement, that, really are, primarily two dimensional, would work. That's not so obvious either. So Yeah. Anyway, I ask a couple questions. Yeah. Again, just remind I'm not done with this. I don't really know if there's anything this gonna work. Yeah. I just think it's such a fascinating idea that I wanna dig into it a little bit. Okay. Okay. so one question would be, where would a model like this live? what are, what neurons would be responsible for this 2D sheet? it's pretty simple. Yeah, I talked about earlier how you could feed displacement into the temple memory. So instead of putting in a spatial, imagine what you're representing in the minicolumns is, let's say local curvature at, that point. And so what you, it's then it, I, haven't, worked through the details, so I, but the basic idea, all this could be represented in this sheet of displacement could all be represented in a single layer of memory. It doesn't have to be spread out, it's just, at any point in time you say, I know where I am. And at this point in the model, these are the displacement I should see. And then, because it's a highly unique representation, and then you go, okay, and but if you, fed in displacement into the temple, into the minicom structure, so the system is representing displacement, and I'm feeding in, as context, the unique location on the object, then I can say in a unique location on the object, this displacement is represented, uniquely. and therefore I can link displacement. I can say, okay, at point A on the coffee cup, there's a, there is a curvature to the right here. Let's say that's, go this way and something curves this mount on another object on point A. It would go a different way. So if I, had unique object and I have the displacement, then I can learn the transitions of displacement like I learned through s of melody, notes in the melody. And it could be unique for the object. If I didn't have a unique representation of the object, I had said, this could be two different objects, then I might say, at this location, if I move the right, I'll get one displacement. If I move the right, I get a different displacement on object A and different displacement. Then I, as soon as I move, I would know which one it's the same way the temple memory windows down from notes. It would window down a series of displacement associated with locations. So the 2D sheet, the same 2D sheet here, can actually represent multiple objects and in fact multiple objects simultaneously? Yeah, it's very much like we did in the columns plus paper. there we were feeding in, a unique representation location as context, but we were, we were using features, observe features as the, as the inputs of the temple mount. Right here, I'm gonna say switch out features and put in displacement, observe, displacement, and then you basically get the same thing. Now, it's environ to the, now it doesn't, now it's inver to the, it's just different and it'll narrow down on things. And this representation still specific to specific scale of it to be very specific. alright, so I already mentioned the scales a problem, right? So we don't want it be scale in this case. What does scale in this?

Let's imagine we're just talking about curvature, okay? And we're going around the cylinder. And the cylinders can be bigger or smaller. The curve, the scale would say, okay, when I move in this direction, I should feel it rotating away from me this way, not like this way, but this way, right? If I go this way, it's straight. If I go this way, it's curve. So that's my prediction. Now, my prediction says maybe if I consider the, distance fixed, then I expect a very specific curvature. But what if I get a sharper curvature or less? I need to say, oh, that's not the right curvature, but it's in the right direction. Something like that, right? And 'cause then I say, oh, it's a tighter curve than I expect, but it continues to be a tighter curve. And then somehow I have to say, oh, this is a different scale.

Infer the scale on the fly. I think you, based on the amount of motion, and based on my model, you, say oh, moving this much only moved me this far on the object. Oh, it's the same thing, but it just means the object. Yeah. I think it is. it's complicated because a lot of, I had unique objects that were different scales in that size. I wouldn't wanna confuse 'em. yeah. But I also wanna just different in scale. Okay. I think the hope would be that as you locally, you always have the sort of same displacement. And, yet the the scale and deformation is that okay, the distance traveled is somehow changing. It's like it's sharper or less sharp, but as soon as it starts going the wrong way completely, then it's not a scale issue. I think about it along this line that sort roughly the same, then it's this way. So since I don't know. Where I'm on the object, at least initially. So I represent as a, as an input. I have some coverage here and have a displacement in a certain direction just going over there. Yeah. You, me, sheet just going in certain direction and I observe another dis local coverage at this. So have a sequence, special sequence of those lower coaches. Yeah. And those recognize the object without, but, somehow the actual specific differences matter too. Because I, I would know if this is the one I had before. I know the difference between a little, the Met Coffee Cup and the regular met, so I don't have to say, oh, I adjusted, it's a coffee cup. No, I said, no, this's a small one. So that tells you it's a, an Uber parameter. It's it's something that the model, inferring the model says yes, I recognize the shape, but I. By the way, it is not the right dimension. So it's not the dimensions I learned in. So the model has to include those dimensions because I learned a specific coffee cup, right? So that's gotta be part of that. But then I'm able to infer it even though it's not the right curvature, which it is not as steep, or it's, one way to think about this is it is to say, okay, if I can recognize everything, just like I observed it, it'd be perfect match. But if, I, if things aren't, I observed, I'm able to still keep going. Even the fact that the, distances are wrong, right? And, and, and at some point it would say, okay, they're all wrong by the same amount. That's why I could, just a smaller version of the shop.

Or, maybe, I don't know yet, right? But I think it can work. maybe some concept just go by. Direction, right? Without taking location into account. And then you would recognize, oh, this is a coffee cup or species of the family of coffee cups. And then there be others which have taken into account really, once I've figured out, oh, I'm looking at the coffee cup take into, maybe I would rather, I'd rather have one pound everything.

maybe that's, the evidence we would have would suggest that columns are columns in that different, in that regard. but different regard. Input. yeah, they have different input. But, going back to, imagine just roughly imagine this is a grid cell module here. And so this is a bump representing location in space. And this, these actual cells are unique to any particular object, right? That location on the particular object. And then imagine you have another set of cells, here. That is taking in displacement.

And same idea, there's minicolumns here, but individual cells say, okay, here's a displacement in the context of a particular location on a particular object. So imagine the disc feeds into this, and this feeds back to this.

your movement, you're saying, I know how I'm moving. And and I know what I'm detecting for my displacement. So if I start narrowing down here, I'll start narrowing down here. And if I, and then, when I'm narrow down here, make different predictions about this. And, but this is the, basic idea I'm, working on is that you can be modeling this temple memory layer basically says. Gimme something that is not, gimme an input, which is not unique to this object, but I can detect and determine and, I'll, and then in the right context, I'll make it unique. And so then they can feed into each other. at one point I'm getting the displacement and next year I'm getting the path integration from good cells.

and, so these could interact back and forth and you n basically narrow it down to, where you are and what, you know what, then you could predict what the displacement in the next, because these patterns would link to each other. they could link to each other, but they're linked to these things basically for certain, anyway, it's a rough, it's a very rough idea here, but the way I think about it's the temporal memory layer gives something you can measure that's not unique and, and conform, unique representation of it. And then you have, and if you have multiple ones that are working, Different sort of inputs.

one could be detecting this and the other one's detecting this, and together they narrow down the answer. and you can even think of the temple pool layer as an extension of that. You can think of the temple pool layer if it, we don't, we haven't hypothesized it works on the minicom idea, but the temple pooler, like I says, I have a union of ideas here. These are the, this is all the different objects I could be, I could be looking at. But then you have these inputs from other columns that it been another context that says, oh, given the context of these other inputs, I can narrow this down. Then you could basically, since you narrow this down, then you could have, feed this back down here and say, okay, my neighbors told me it can't be those things, therefore, I'm gonna tell you can't be those things and you can't really do this. So some of, there's this balance between these things that are so narrowing down as you move and, narrowing down as you get input from other, sensors.

Seems idea there's, one perception also that I think is interesting. I think it's slightly at a higher level than we're discussing, and that is if you come into the room and someone's broken off the two pieces of the state and they're sitting on the table, yeah. You instantly recognize broken. Yeah. If there's an object that you come into a room that you've never seen before and you see something that's all kind of part of the object and then a divot in it and then a thing line on the floor, you instantly come up with broken. So I'm saying it was a object connected. The fact that I see two objects. Matching color, whatnot. I can match the, okay, that thing came from here. You very quickly can recognize you. You could recognize, something's broken. I'm not sure I would agree with your second point. The first point I agree with, okay. If it's truly a novel thing. Truly novel, right? it's not like anything else I've seen before. I don't think I would know that it was broken. if I see something, it's kind oh, that's kinda like a cup and some two parts. So what I think is going on that Kevin would be like, imagine the big sheet by race, right? And you can infer any, you might infer like the bottom of the stapler would be part of that sheet. And, then, then as you move, you would expect to see the top of the staple, but it's not there, right? And, then you'd see another thing down here. You'd see the top of the staple, and that's part of the sheet. But then you realize the bottom missing. So then you say at some point you say, this is in, there's something wrong here. It's missing these components. It's supposed to be connected, but they're not. they're supposed to. I should predict this. It's there and it's not there. And so I looked for where it, I don't know. I don't know, but I just, if you walked in the room, you saw the top of the stapler, that's all I saw. I'd say, that's the top of the stapler. And I would say it's missing the other half. I would, that model could explain that. yep. I recognize that's part of the stapler, but I'm missing the other part. I'm expecting that, that it should be there and either a closed or open state, it should be there. I can't, I don't see it.

that's one problem. I think we can, I can explain how that happens now, piecing together the fact I see a topless staple here and the bottom of the stapler down there. I don't, that seems like it might maybe a more of an Uber, higher level cognitive. Yeah. That's what I'm, it might be a higher level. Yeah. Like putting two and two together that bo that and it got broken. I, but I'm saying you're chaining things together. Has the potential for maybe handling this more abstractly? I think it definitely has the ability to handle, oh, I'm missing a part of this thing, or This thing is broken, or this thing is bent outta shape and it doesn't, things like that. Yeah, it would do that. It would, it, and, I suppose if I break something, imagine I take, the coffee cup and I break it two pieces.

in some sense, the broken edge is a feature of that object now, and we can see it doesn't belong there. And I might even recognize it as, oh, I know what broken ceramic looks like, so I, that's getting pretty, that's getting beyond my thinking right now. I'm just trying to figure how do we.

could I think be extended into that? I think you're right. Yeah. that's the hope at least, right? The hope is you get to the, you get to the core idea about how models are built, even if you have a lot of the details wrong, the core idea about how models are built and how they're inferred. Then if you get that then you can tweak the ideas of it and figure out all these other things, right? But, these ideas I'm working on here are very odd. I went and predicted this a while ago. this idea of this two dimensional displacement sheet and, wrapping around morphology, I, all this stuff is, or even the idea that there's only one orientation that matters vertical, and the others are not represented, ex not represented, ex externally, explicitly. They're represented in the model itself.

but, so I, my point is if I get this all to work. Then it seems oh, that's a whole new base in which to start tweaking it and figuring out other things. 'cause if this is wrong, then I have to start like scratch again. yeah, but it also makes it simpler, the fact that, however you wanna look at the retina. It is primarily getting a two dimensional perception that Yeah. And that, makes it a lot easier. Yeah. Have to defer 3D This is, this has been one of the big bug booths from the all through the beginning. The qu is two dimensional structure, the, the sensory organs are two dimensional, and yet we're building these 3D models. We have to represent 'em somehow. there's an entire question even about grid cells, whether they can represent three dimensions or not. And and Marcus and I run over some papers of that recently and it's questionable. maybe yes, maybe no. And so it'd be cool to come up with a way of modeling things using two dimensional structures that capture three dimensional shape. Would seem to be a, really nice result. there is, there's concept heard us talk about self-organized perhaps, stuff like that. And you look at, where the, binocular vision where this, the, deeply intertwining two halves of the vision. That word. Huh? Is that a word? Twin.

Okay. But, that was, taking something that was higher dimensional and forcing into a planer configuration. That's what you, the self-organizing maps do that. Yeah. Yeah. And so the same thing. Oh yeah. You're, yeah, I get it. When you get the pinwheels of the orientation in there, that's again, a multidimensional signal that's being impressed into a planer representation. Yeah. So the power of self-organizing maps, which I would maybe argue is happening in a, in cortex, is that you get these multidimensional inputs. They have a signature of some sort, but it looks really weird being imprinted on there. Yeah. I read that a long time ago. That was Dr. Who did that. Yeah. Yeah. So do I need to read that again?

Either that or I could, no. the question, is it really important that, I have the general idea. You're, collapsing these structures onto two circumstances, right? but do I need to know more details than that? Yeah. But if you look at the way that you build it, you basically, there's a gradual process where you move these things closer to each other using some kind of metric that these things are similar to each other. Yeah. So that's a very, it's not clear to me that we're collapsing 3D structure on the 2D server, the whole idea, the dec thing. Some higher dimensional thing, some other attribute, there different ways you could do that.

To some other object or some other thing. Yeah. it's, not clear to me that concept of collapsing on these onto a surface is the right metaphor? I'm not sure. Okay. It's different because they would, they do dimension reduction, Which we don't wanna do here. It's not to mention reduction. We still represent all the different states of the object, like quotation or opening, closing up the stapler. We still represent all the different possibilities. But in, in terms of, if you look at dimensionality as being the attributes and that you have, and that the ability to, how I'm trying to pick the right word, localize the detection of. Whether it's orientation, whether it's, which kind of lines up an pinwheel thing, whether it's the, this thing that where the two halves and visual fields are put close to each other. those are, there's, a morphology there of the cortex itself that is suggestive of the fact that you're putting multiple dimensions. I, don't believe that the only thing that those columns, are doing is just doing orientation. It's just that when you look for it, you see that particular pattern come on up. They're probably doing a lot more. Yeah. I think orientation is.

Alright. I think, it's interesting observation that self organiz, I'm not sure yet if it's, the right metaphor for me to work on right now or about, but I think what is saying is really not trying to do dimensionality reduction. We're just, I'm trying to sometimes finess the 3D structure by learning 2D cur 2D points of curvature. Oh yeah. The way, like Marcus and Merko, that's a completely different way of, representing high dimensional Yes. Lots of different grid cells. Yeah. Each one has a preferred direction. Yeah. And you project everything according to that. And you have a bunch of these and together they represent the full. I don't know if that could, that principle could apply here. it's an interesting, question. It's pretty generic, but the idea that I have to be able to make these projections onto different planes, happened here. And that's just the. Proximal synapsis that, it's a, the, basic principle that laid out there is, an extension of the principle we had in the columns paper, which is in the framework paper, which is you have multiple modules representing the same thing and together they represent rich representation. Yeah, we did in two dimensional sheet, two dimensional plane. We're doing it now. These representing different slices of three dimensional space. Yeah. Same basic idea there. yeah, there'd be like a preferred each. You have a group of cells that has a preferred direction. Yeah. And then another group of cells is another preferred direction. it doesn't have to be good memory.

Yeah, no, that's another way. Yeah. So I'm more comfortable with that one. I feel like that's pretty certain that's going on to some extent. That's how we did the nick stuff. Okay. You always, I always can't remember what that was. Yeah. But basically I could do these multidimensional mappings with the temple. Can I read that again? It was just a demo, A hackathon thing. Yeah.

I mean, it's just a, it can map anything to anything the, so you can just learn all these different maps, the location. Yeah. In some sense, this is where the granularity of locality, where this is represented. it could be way down deep as you could organize at a larger level, But actually is. The data that, I've read to what, is there? But it is an interesting notion is, how distributed, is it, how, how localized it could be? the theory is that everything has to happen within a single cortical. So all of these, I'm actually in a couple ways. One is if we start think of compositional objects, you might have to go that Yeah, that doesn't mean the entire sheet is involved. The map are pretty much at that level.

like I said, there could, be a granularity boundary here that, I can't, you talking about this another interesting idea. Can I change the topic? Sure.

the weird thing about V one just gets input from. Any prime that one gets written. Retina, v2, okay.

Rotation.

but now V two gets input from retina and from, it's like in other, and that's the general rule you go before, which is next after V two is, it gets input from, V one and V two and from the retina. It's oh, how do you make sense of this? so one thing I ask myself like, what if I was trying to learn the coffee cup or say this one device specific, I have a cylinder and I'm trying to put the logo on. Where would those ations be? One could argue that might be saying I'm object.

It's a morphological object, but I'm being told there's features that are being detected in V one, which is the logo, and therefore it flips it around like the unspecific morphological object might be represented in V two in this case. And the specific, this is the logo or this is the word or whatever it isn't V one. And so if V one is saying you're determining the morphology, but this is the feature that you should associate with that point on this morphology, it flipped it around a little bit. I used to think oh, you have this morphological stuff lower down and first region. And then I'm just pointing out that's an interesting twist to ask yourself as I figure out how to do these.

What if it's involving multiple regions in a hierarchy? Who's representing what and in what are the links between those two? I know in the past I've always already, we shouldn't do this, but now we shouldn't be think about. No, I think I've got, we also have a little feedbacks that going downwards. Oh yeah. But that's the point, right? There's a huge of linkage both ways.

Yeah. Something.

or it's, it basically, it's not reconciling as much as it might be just forming links between the features. The morphological object in V two would say, or the second region, whether it's S two or whatever, is saying, okay, I'm on this point in the, on the surface of the cylinder, and I'm just gonna associate that point with, what you're representing down there in region one. And so you're saying there's a logo here. Fine. Okay. So I'm just gonna, when I, when not this location, I'm predicting you should seeing a logo, so it's telling me one, and then D one would be saying, I'm seeing a logo should important what object, what overall? Yeah. I think that's the key where one layer.