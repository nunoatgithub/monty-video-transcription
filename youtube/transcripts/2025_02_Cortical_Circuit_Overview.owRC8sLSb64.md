I thought just a little preface here, the brain has all this complicated anatomy. It's been documented for decades and decades for over 100 years. it's really messy. there are different terms that are used for the same things. There's different reports, reported empirical results that conflict with one another. There are empirical results that are on the same topic, but because of the different techniques and animals and so on, they just They're talking about the same thing, but there's no overlap between them. and I've said this many times, if you want to know anything about the brain, you have to read at least seven papers before you start getting a gist of, okay, what's really happening here? because any individual paper can be very misleading, not wrong, not incorrect, but just not. Using the same language, the same things, whatever, you just get different answers. it's also true that the anatomy itself is not like a circuit diagram you can look at and deduce what is going on. You just can't do that. It's just too many unknowns, too much complication. It's not possible, so give up on that idea. That you could look at and go, Oh, I could do this. What's going on. There's a few simple circuits and biological neural circuits and that people have been able to do that with but not anything close to in your cortex. It's just crazy. And there's so many, this was still discovering how the neocortex represents information and so on. So it's You can't do that. So what good is this studying anatomy? The good of studying anatomy is that it's a classic science problem. you have top down theoretical constraints, which you, say, okay, the theory and deduction says, this has to happen. And then you can look at the anatomy and say, can that thing that has to happen match up here at all? And how would it match up? And maybe there's one, only one way to do it. Maybe there's multiple ways to do it, but at least. It's this interaction back and forth, and then you say, Oh, I wish I knew more about the anatomy because that may prove this theory. Then you dig and you find out more about the anatomy and physiology, and maybe it just proves it So my point is, on its own, the cortical anatomy and physiology, and with brain anatomy and physiology, isn't, it can't do anything with it on its own. You have to match it with, with, with, deductive and top down theoretical constraints. so that's the process we've gone through. any questions about that before I dive into it any further? All right. There's one axiom. That, that, we follow, or I follow, and we all follow here, and actually this is something you just take as a truth, right? It's a ground base, you just have to accept this, and this is Mountcastle's proposal, and, this is the cortical column hypothesis.

which is at the cortex, which is this big sheet of neural tissue is divided up into these columns, and they're all basically doing the same thing, and it's not purely made up. There's a lot of evidence for it, but it's the one thing I don't question as we're going through our work. I never go back and say, I wonder if my customer was wrong about this, because we could come up with some, the consciousness circuit over here or something like that. I just don't go there. this is the, there's the one sort of axiom that we stick with. And so it's underlying everything, and hopefully it's right, because if it's not, then we're all screwed. But so far, there's a lot of evidence for it, and I think it's fairly safe. One thing maybe on this is, in terms of how you define the extent of a column. Because I think sometimes there's more like a histological point of view, and then numenta has a little shift.

so part of Mountcastle's proposal, it talks about what is the extent of a column. So this is your, neocortex. It's about three millimeters in thickness, and it's big sheet, right? And different parts do different things. And, and when they first observed it, under a microscope back for Cajal, they saw that the predominant architectural figures feature with layers. So they and you've seen the picture, hopefully of the picture that, that, the hollow draw June, but you could see this different cell densities and different cell types. And it looked like these layers of cells and these extend across the whole. The cortex continues like this, and they came up with 6 layers that is incorrect. We aren't 6 layers. There are different cell types and different densities, and they just made up the number 6. and, and for example, today, generally, layers, some layers are grouped together. Layers 2 and layers 3 are often just referred to as layer 2 3. Some people don't make a distinction. There are papers that talk about layer 2 A and 2 B and, if I write correctly, and layer 3 A and 3 B. Well, who's right? I don't know. Depends on how you judge it, right? You can look at cells and say, oh, in between two and three, their cell types are a little bit smaller. They make slightly different projections. You say, okay, I think those are really separate, even though sometimes people don't talk about it separately. layer five, it's often there's two types of cells in layer five. Now there's actually a third type of pimple margain valve that connects to the striatum or something. And then layer six, it's got a half a dozen different types of cells in it, at least. So these layers, visually, you could see them, maybe you could count six, but they're just placeholders that help you locate cell types relative to each other. But we often fall into the habit of saying, oh, layer 6a connects to here. even in layer 6a, there's more than one cell type, so we have to be really conscious about in the back of your mind.

the next thing is that these cells almost all, the next thing is that, the general rule is that if you look at information that comes into the cortex, Information flows up and down vertically. You see a lot more vertical connections than horizontal connections. So there's a lot of, there's clearly a lot of information going back and forth, up and down. And then there are connections in some layers that go small distances or long distances. Sometimes they exit and go out and back in again. So there's a very heavy connectivity vertically oriented, and then a sparse of connectivity, but still a lot, going elsewhere. this is the first suggestion that there might be columns because it seems like the information is first and foremost processed in a sort of vertical fashion. And then Mountcastle made the argument that, that if you look at the cortex and you, look at, what might, you might call a column, That these what distinguishes things return. There's no visual market demarcation between column and column, but you can't see a line between them. It just looks like layers, but he made the argument that a column is defined by where it's getting input from. So if it was like on your hand. He's a famous picture where this is on the hand of a monkey, the skin of the monkey, and these columns. Are getting input from a patch of the skin, but it's not continuous. That is all the cells within a column are getting input from this patch here. But if you skip over to the next column, just move a little bit further. They're all getting input from here. And of course, sometimes in the cortex, you go from one region to another or one modality this might be a somatosensory column, this might be a visual column, and they're getting it from a completely different sensory order. It's completely some part of the Are these sensory patches not overlapping, or do they overlap? The sensory patches do overlap a bit. If they didn't over yeah, but The but not a lot, not completely. And this is his argument. He gave evidence for this. But the most important thing is, it's not about the overlap. It's the fact that all these cells in from here to here all driven by the same area and as soon as you jump a little bit, they're all driven by a different set of things. It's a it's not a continuous thing. It's a sudden jump. so he said that instead of his proof that these are separate processing units, this thing here processes all the information from this patch. This comp, it's only getting information from this patch and so on. And the same thing, it happens in vision and touch and some, in auditory and he gave evidence for that and all those. so this is his proposal about columns. And then he said, since they all look identical, pretty much, then they're all doing the same thing. They're just processing a different part of the world. And again, remember, I could have a vision column right here next to a, somato centric column, or I can have a V one column boarding a, they're not intermixed, but they're points in the cortex where they are boundaries between these regions. And if you just slip, jump over a little bit over the hoop here, all of a sudden you're in a different modality. In fact, it was, what's his name, who proposed that, synesthesia is a cause, is caused by, cells incorrectly making connections across modalities like that. what's his name? The famous, anyway. that was, again, evidence, yeah, there's these, there's these hard boundaries sometimes, but then even here within a single modality there's these boundaries. So then you just think, okay, a column is just, it's just looking at a patch, it's getting input from that patch, and that's all it does. And then it processes it. there is a, there is another confusing element called minicolumns. if you're not familiar with that, a minicolumn is again, columns you can't see. You can't see them in the cortex, right? the only example that's different is the rat barrel cortex. Rats have a column for each whisker and that you can actually see those, But generally you can't see columns. And, but there is something you can see called the minicolumn.

and now I'll just draw it like this.

And the minicolumn is much, much smaller. Where a regular column can be anywhere between a third of a millimeter and a millimeter in dimension. Minicoms are much more smaller, that may be like 30 to 60 microns or something like that. and they typically, it's typically reported, each minicom is like a little stack of neurons. that go across all the layers. So the general idea is that all the different types of cells that exist in a column, exist in a mini column. And, and so a typical number, you'll see different numbers, but a hundred, 120 cells is a typical number you'll see for a mini column. and they come about when the, when you're in utero and you're developing your brain, the way the brain develops, the cortex develops, there's a sheet of neural tissue, just a single layer. And then the cells start replicating, and they move vertically, they move in this direction. and so the, essentially a single progenitor cell ends up creating 120 cells. And they, grow up vertically like that. So that's how they come about. So the minicom is a real thing, because that's how the cortex develops. And, you can often see them under a microscope. There's these great pictures, you see these little skinny little things, cells, and then a little gap and other skinny little cells. They're not always visible. So, in some animals, like in rats and mice, it's hard to see them, or they don't look like they exist. And people say, they don't really exist. They say, they were there when you brain grew, but they're not there now. And other people say, even if they are there, they're not functional. what do they do? maybe it doesn't matter, it's just the way they grow. Malkin has argued that this actually was the most important replicable unit of computation, but he had no hypothesis of what it was. He said that is the unit of replication of the brain, the minicom. And he said if you take a bunch of minicoms, several hundred minicoms, you put them together, they form a column. And they work together. So this is the two levels of organization that you see in the cortex. I have a theory about minicoms. I've never really shared it completely with everybody. We can get into that later. but I have, I'm developing a, partial understanding of how would I think about what does a minicom do? as an independent unit, obviously you can't do too much, right? Because. It's just 120 cells, and obviously you can't do, they have to work together as a group, but can you understand what they do? I think you can.

so that tells you, if, you said, oh, there are six layers of cells and you might say, there's 20 layers per cell, 20 cells per layer or something like that. But, we've already seen that, that's, that's not really true that there's six layers. In terms of the lateral extent, so I think sometimes when I've read about a column, it's suggested it's like around 10, 000 neurons. my understanding is, Oh, they're much bigger than that. Yeah. It's more like a hundred thousand at least, but I thought that was arrived at and you mentioned stuff through some other logic. If you look at the cats, the hyper columns that Hubel and Wiesel talked about in cats, those are about a millimeter square in area. And basically just do the math. In a hundred and a square millimeter, you'll have about 100, 000 neurons. Okay. now that's arguably the hyper column in a cat. No one, I haven't really gotten definitive answers on it. Is that a column or is that multiple columns? It's debatable. but at least at the high end, you might have a column, which is a square millimeter, a millimeter in area. 100, 000 cells. If you, if your column was, smaller than that, it was, let's say it was 300 microns by 300 microns, that would be like a tenth of that. There'd be like, maybe, 10, 000 cells like that. the numbers generally work out well, that is you cross correlate them, but they're not always, they don't always fit properly. there's also evidence, people argue that some of the cells in the cortex do not start minicombs. Some of the inhibitory cells generate through a separate process. So there's more cells than in minicombs, that, it's not, it's complicated. It doesn't have to be a question. The minicombs make up all the neurons in a column, and you just said it. Yes, but maybe no.

no one really knows. If people argue different things, but the general answer is yes. the general answer, there are, other cells that are not. So if you look at the cortex, about 75 percent of the cells are excitatory cells. These are cells that you input. They have a positive, they affect other cells in a positive way. They make other cells fire. And then roughly 20 to 25 percent are inhibitory cells, which make other cells less likely to follow. The definitely all the excitatory cells, I believe, are in mini columns. Some of the inhibitory cells are weird. there's these ones in layer one, which hardly anyone ever talks about, that are really weird and schlong things and, they don't really fit one per mini column, that kind of thing. So who knows? one of the rules I have about neuroscience is that for every rule, There are exceptions, every single one. And so I could say, oh, the cells are there. And then someone said, no, they're not. And I could say, oh, these are all excitatory cells. And they said, oh, no, here's one, never ending about that. Okay, so that's the general layout of this common algorithm, divided into columns, each one getting a patch, an input from someplace. And the next column processing input from someplace else, and then, that being divided into many columns, which our theories generally don't talk about, but if you look at our, if you look at the, the temporal memory algorithm, it's really relying on many columns, the whole, so, we do take advantage of it, but, in, in our, Montage, I don't think we do anything with any columns, we just take the concept of it. So this is, any questions about this now, Aaron? So the proximity of the sensory patches, is that mapped into the proximity between cosmos? They're generally, except, when you get to borders. If you switch from one modality or one region to another, there's a discontinuity.

yeah.

cortical computational unit, minicolumns computational unit, how, in the standard architecture, different areas, different Brodmann areas, will have different looking layers? No, the same layers, the differences would be that you mentioned Brodmann. Brodmann is a guy who tried to define regions by looking at the visual differences between parts of the cortex. So the typical variation is the cells, the number of cells in each layer can vary. You might, one region might have more layer three cells, one region have more layers, five cells. they, cell types may vary in size. So like the, what people classically call the motor cortex, Which have neurons of which project to your spine, those cells are really big because they have to project a long way, but the same cells exist in other regions, but they're smaller, even though the same cells, they're just smaller cells because they don't project very far. Maybe they only project to another part of the brain. There's all motor output cells. They're all LaFe5A cells, but someone will look at the, they'll look at the motor, quote, motor cartridge, it's really somatosensory cartridge, will say, oh, those cells are different, they're bigger, so we can define the motor region because it's got these big cells in it, but functionally it's not really any different than regions of smaller cells in the same place. That was going to be my question, so that distribution, that doesn't mean anything that maybe for motor system, they have to optimize for different No, it doesn't, it could be, and it almost certainly is. Okay. So there are the classic big exceptions to this common algorithm. One is motor cortex, these big cells. which people at the time didn't realize are the same as the little cells in other regions. And the other classic difference is the primary visual cortex, V1, which as we see is the first region that gets input from the retina, has some extra layers. In fact, a classic V1 in a primate has about twice as many cells per minicom.

and, but it's only in the, they call it extra layer four cells. So they have an extra few extra layers there. Now, here's the thing about that. So obviously that is an optimization on vision that works better, but many mammals don't have that. Dogs and cats do not have what they call a striate layer 4. They don't have those extra layers. So presumably this is something that primates have evolved to have something a better vision somehow different than the vision of dogs and cats. but dogs and cats still see. so how do we handle that? The way I handle that from a theorist point of view is I say, okay, clearly these are important, these extra layers, but they're not essential for the basic algorithm of the cortex. They don't, exist in touch, they don't exist in hearing, they don't exist anywhere else in the cortex, only in V1. And not all animals with eyes that see have a striated cortex, so it's important, but it's not, it doesn't overthrow mountain R axiom. It's and so the general idea I've always said is we can just focus on the core, commonality across all modalities, because there seems to be the same basic arrangement of cell types in all modalities. And then the variations we see, whether it's an extra few layers or extra bigger cells or more of these cells and less of these cells, they're all tweaks that, and so we don't want to focus on those. We want to focus on the core algorithms first, what's going on common to all of these things, and then later we might say, even in our work, we're a learning module, we might say, hey, you know what, we're going to tweak the learning module for LiDAR. Here's how we could do it. We can add a little extra twist here and put some more of these things here. I don't know. we can do that, but let's not focus on that. So don't, get hung up on that. A good question. Any other questions before I go on?

Alright. I know some of this, for some of you, this is all review. Some of it may not be. I'm now an erase if I can or should I just turn it around? Go other side? Sure. I doesn't turn. Maybe you can flip it. Oh, second footboard over there. We flip it. Oh, I see. Oh, this clever, the wheel locked there. Okay. Great. All right. This is a classic example. If you were to go to a neuroscience one on one class in any university, and you're talking about the brain, and they would say, here's the cortex, and here's a column, and, and they would say, okay, what's going on here? they would say the following. They say, okay, there's layer four is the input layer. So it comes from some, I'm going to draw it as if it was coming from the outside. And you're like, oh. this, okay? Layer four is the input layer, because this is where the eye projects to, or the skin projects to, or the hearing, it gets pre processed, vision doesn't really get pre processed. Vision goes on right from, I haven't talked about thalamus yet, but it goes right from the eye, right from the eye to the visual cortex, but, when you, your touch sensors in your hearing get pre processed a bit, but it ends up coming up here. Then layer three is considered the output layer and layer four projects to layer three. And then layer three is the output layer, and then the next column over, or hierarchical. So now we're going, say V one to V two. Then this input goes into layer four of, of, of, the next guy. So you see this and this goes up to layer three again, and then it just repeats as you go up the hierarchy. So this is your classic view that information enters layer four, gets processed, somehow goes to layer three, becomes input to layer four, gets processed to layer three, gets input to layer four, and so on. Your classic hierarchy. Of course, this ignores a lot of stuff, like what's everything else doing? And, but this is your classic view. we under, we think we understand this pretty well right now. I do want to point out something that all inference is a matter of mapping many inputs to fewer outputs. It's if I'm going to label an image, there's millions of images that I might call cat, and millions of images that I might call dog, or this is true of, millions of different patterns, I'm like, so that's this song, that's that song. Inference, if this isn't obvious to you, I'll expand on it, but inference is always a many to one mapping, and if that's, oh, I should mention here too, Mountcastle said that every column in the neocortex is doing the same thing, the correlative to that. is that every column does what the neocortex does as a whole. You can't really escape that. If something's going to happen in the neocortex, it's going to happen in every column. Because, and I, it doesn't mean every column processes vision. It means whatever processing elements that are going on are happening everywhere. I just, that's the background to this. So I'm assuming that anything that happens in the cortex is going to happen in every column. Inference is going to happen in every column, and mapping from many inputs to one output is going to happen in every column. So this tells me, right off the bat, there is a many to one mapping between layer 4 and layer 3, if I follow that group, right? And many to one mapping, the term we use for that is temporal pooling. It means that different patterns over time, they get mapped into a single output. And I can state that even before I understand anything about sensorimotor inference. It was just like, that's like a fact, it has to be. if you're gonna, if you're gonna put a bunch of patterns and say it's one thing, then it has to occur, and there's, if this is the way I go up the hierarchy, then it has to occur from here to here. So this is a, and our current theory is that temporal pooling is that, is a term for that. so as your eyes move or your fingers move, this thing is going to be changing, but this is going to be more stable. And it just has to be. There's nowhere else for it to occur. Okay. so that was that. Then we, said, okay, so we could state that. then we came up with a theory for how the brain could learn sequences and make predictions of higher order sequences like melodies and so on. It's a really challenging problem. And we realized we could do that even in a single layer, like layer four, and we use many columns. I won't go through that algorithm now unless someone wants me to. But this is the temporal memory algorithm. We said, okay, so I could have a series, if I was listening to a melody, I could have a series of patterns here, using these mini columns, and then, then, I map all those patterns to layer 3, and that would be the name of the melody. So that's the temporal memory algorithm. And it's very important. There's a lot of, there's a lot of really important ideas and how that works. And it's challenging people to get it in person. So that was great, but he didn't do anything about sensory motor inference. And, I avoided talking about, I knew that most of the changes occurred to the branch because we move our bodies. We move our eyes, we move our fingers, we tone. That's why most of these changes are occurring. It's not like melody. Melody happens on its own, but most of the changes are coming into our brains because I'm moving my head right now. Everything's moving around rapidly, and these are leading to different changes constantly. Think about the flood of information that's coming in right now as I touch things and look at you. so I didn't understand how that could happen from a sensory motor point of view for a long time, so I just ignored it. and I figured, we'll figure it out later. we did figure it out. and the answer, it turns out to be, that there's another layer of cells, which is representing a location and that layer cell is, there's, and we're going to call that layer 6A, and there is a bi directional connection between layer 6A and layer 4, this is a very major connection, and what's happening here is that, this is keeping track of where the sensor is on the objects in the world. And, therefore it says the location I've sensed this thing, and if I know the location, I can predict what I'm going to sense. And if I know what I sense, it can help me narrow down the location. And so this pairing of sensations And, or features and location is the foundation of basic model. It's like it's that's the foundation of the model. and so this was the idea that neurons in a column could keep track of a location of the sensor seemed crazy. How could that be, but we couldn't deduce it had to happen. That's the only way your finger can predict what it's going to sense. If the finger is, if this thing's getting input from the tip of your finger, and you can predict what you're going to sense, it needs to know what it's sensing, like what object, and it needs to know where it is on the object. There's no other way around it. And and then we realized, oh, this is, it seemed crazy at first, but then we said, oh, no, we found these things. We didn't find them. We know about these other cells in part of the rainbow grid cells, which acts like a location. So we speculated the, sonic grid cells here. grid cells exist in the antelope cortex, but they, were like a reference frame. And, and they tell you, they tell you where you are on the object. And of course, then you need something else. how does this thing work? we know a lot about grid cells because it's a field of study. And so the basic idea here is that there's a second input coming into the, into each column, a movement input, and as opposed to a sort of. feature input or sense thing.

it can be a feature, it can be more than, it doesn't have to come from a sensory organ, it comes from the cells. So now we have two inputs to our column. One is updating where the location is, I know I'm moving in this direction, therefore I can calculate where I will be over time, and grid cells do that, we know that, it's well proven, it's incredible data on grid cells. We speculated it happens in the cortex, but it's known to exist in the entorhinal cortex. So you have a movement information, which updates the location and therefore now you send something at a new location and it doesn't matter what kind of I'm going to learn how something feels. It doesn't matter what order I go around it. It doesn't really matter as long as I cover different locations. In any pattern, in any order, it'll keep track of where I am, and then, and then I can build this model. And you need to develop this, if you don't already, this image in your head of a model of a three dimensional structure where each location you have some knowledge about, that's what Monte does, about what is there, what was observed there. And so that is the basic of how models are built, sensory motor models are built in the cortex. And so we only, we guessing what these layers mean, but you can see how how they, how you forced to say, layer three must be doing something like that. And layer six, eight, because of these bidirectional connections must be doing location. And, so now you can deduce these things. We might still get it wrong, but. I, think the actual layer, layers is not as important as the functional role that must be done, but it's a pretty good guess what the layers are, at this point in time. Is there any, physiological evidence of layer 3 being more stable as we look at the subject? no, but I, don't, I'm not aware of it. But they haven't looked for it because they don't want to look for it. this is a great question. So let's just talk about how, a layer of cells represents something. If you look at layer 3, maybe it has 5, 000 cells in it, maybe 10, 000. I don't know, something like that, right? and, and it may be, it's very sparse. We know that. There are very few neurons that are active at any point in time. So you might say, let's say 2 cells are active. So maybe you have 100 cells that are active, right? What is stable is that pattern of 100 cells. Any particular cell, by the way, it's only stable if the animal is observing an object that it knows. so an animal has to be awake, it has to be observing an object that it knows, it has to be continually to observe that same object and alert, and then you would expect to see a stability of this hundred cells. Any particular cell will become active one, two percent of the time, so on one out of fifty presentations, right? So if you look at the experimental paradigms that people typically do, most of those conditions are not met. The animal's anesthetized, they're showing sinusoidal gratings, They're doing all kinds of stuff where the animal is not awake and alert and looking at something, and they're not, and they'll say, the cell, it fired on this image, but it didn't fire on that image, and, and get back to the work that you and Niels are doing, are these SDRs with these sparse distributed representations. Do they have semantic overlap? If they have no semantic overlap, meaning one SDR is randomly chosen for another one, then you won't look, you won't be able to see a cell assign any meaning to it. One moment it might be a cat, and next it might be, a bicycle. it's just meaningless. it's the step that matters. it's very difficult to know what to look for. Now they have techniques that they might be able to do this. But again, it's very difficult experiments. Animal has learned an object, recognizes the object, is fixated, moving on that object, is alert, and they're licking a whole bunch of neurons at the same time. Hard to do.

But there's evidence that It's more sparse, in that layer, Yes, And has electrical connectivity, which, as I said before, fits with it. You've got these 10, 000 cells, and what you really want to do is you want the SDR, the sparse distributor, the 100 active cells to self reinforce. and then another pattern of 100 self reinforced, and so on. The whole, the whole, the sort of mathematics of sparse distributed representations is really interesting. We're not using that at all in Monte today. It's not clear if we have to, but brains do it. All right, other, questions. I'm going to fill in a few more pieces here, and then, is this helpful? Yes. I'm talking a lot, so I don't know. One thing we didn't understand at first is that there needs to be a concept of orientation. it's insufficient just to know where like, where the patch of the eye is on the object or the fingertip is on the object. You have to know the orientation of the patch. Like I tilt my head left or right, the whole image is on my, the patch is all rotated. Or if I take my finger and I rotate it like this. My perception isn't changing. I perceive, it doesn't matter if I run my finger this way or this way, I perceive the same object, but it's very different patterns coming in my finger. And so the orientation has to be compensated for in many ways. So there has to be another signal which represents orientation. This is equivalent to head direction cells in the hippocampal complex, if you, or elsewhere. So we have an orientation signal, a location and an orientation, and both of those will be updated by movement, like when I have a, if the cortex is being told my finger or hand is moving and my fingers moving some way, it could change the orientation, it could change its location. So we have this input coming in here. Both of these can be updated by movement. So now we know the orientation, the center to the object, we know the location on the object. Now, this is something we figured out fairly recently, I don't know, I don't know how many years ago, two years ago, I don't know, how is orientation taken advantage of here? what we want to do is we want to change, we want to basically We want to remove the input. Imagine if I'm touching an object with my finger. I don't want the orientation to matter. It should matter what the orientation might be. I want to get the same result regardless. Or if the object is rotated, I'm looking at it this way or this way, or I tilt my head, it should be the same thing. I don't want to have, it can't change it. So here's something we figured out fairly recently. It's in the, something we, figured out, I dunno how, okay, so we look at the cortex, we've got columns. There's another part of the brain, which is absolutely essential how the cortex works. it's intimately tied to everything in the cortex. That is the th the thal is like a, it looks like a little bird egg. There's two of on each side of your head in the center of the brain, and every part of the cortex connects to Thalamus and every part, and, and Thalamus connects back to every part of the. it's intimately connected, and in fact, if you have an eye or finger or whatever, any input to the cortex first goes through the thalamus and then goes up to layer four, right? And it stops at these things called relay cells. And then the cortex projects back to the thalamus, and in a major way, lots of details known about these series of connections, but all inputs to the cortex go through relay cells in the thalamus. And if you look at a lot of the literature, they don't always talk about this. They'll say, Projects to, to the thalamus, a thing called LGN, which is lateral nucleus. Then it goes to V one, then V one projects to V two and V two projects to V four in the cortex. this is just how you get into the cortex and the information flows from region to region, which is true. But then, what is also known that every region, the cortex gets input from the th every region and every column. So this is where the heterarchy paper comes about, right? There's clearly a hierarchy of, projections in B4, but it's also in parallel.

so sometimes these, regions are working in parallel, and they're also working hierarchically. And what we now believe is in the whole heterarchy paper is that Each region is learning objects. Each region is, or each column in each region is modeling objects. Each one has to have a complete centri motor interface to the world in some sense. but then these connections are for hierarchical composition, objects composed of objects. So you don't have to, the classic view is that V1 just detects little features and you have to bing and fence to get objects up here. But what we think is going on is that everything is recognizing objects, they just, people didn't know it, they couldn't, they didn't know how to observe it. and these connections are like saying that the object in V2 has a, feature which is an object in V1, with the classic coffee mug with the logo. The logo would be here, the coffee mug would be here, therefore the logo is part of the coffee mug. but this is recognizing the mug and this is representing the logo. it's not, and, I'm virtually certain that's what's happening. Higher level input, does it also go to layer four? higher layer, what do you mean? what's going to be two and three? this is, going back to the layer three projects to layer four, like layer three projects to layer four, layer three projects to layer four. Oh, okay. there's some more details there that I didn't tell you. People talk about the main input to a quark. com going to layer 3. excuse me, layer 4. Layer 4. That's not really completely true. there's another input that goes to lower layers, 3. And there's another input that goes to, the layer 5, layer 6 border. this could arguably be layer 3, layer 4 border. It depends on how you look at it. So there's actually three different inputs to the, to a column. Both people talk about layer four as the main input. what I think is going on here, and I think this is the motor input, the layer five plus layer three, layer six one. It's basically saying, here's, I'm going to drive these guys to, to path integrate and update the location. And, I think what's going on up here is this is actually related to object behaviors and movement, it's movement related to the object as opposed to this is movement related to the sensor. The object is moving something on the object. So we can come back to that maybe when we talk about object behaviors. now your, did I answer your question? Yeah, so it's the same for v1, v2, and v4. They all get the same composition. This is basically all look like this as far as we know. As Murray Sherman has told me several times, he says, everywhere we've looked, we've seen this. But we haven't looked at it yet. And these inputs from the THALMOS II lead to nuclear, and so the THALMOS is, separate, divided into many different nuclei, and they get different inputs. Each region has its own nuclei, right? And by the way, the, not only do the sensory input go to it, but the, oops, the motor input goes to the THALMOS II, or the, there's a separate, there's, two pathways to the thalamus. They don't call it motor, but we know it's motor. there are these parallel pathways to the thalamus. And, from the eye, they're related to the magnus side and parvus side of the pathways from the eye. So there's two pathways from the eye. And one represents movement. We're pretty certain about this. And the other represents features. so you have the movement goes to the thalamus and the sensory input goes to the thalamus. What is one of the things, one of the major things that Thalmus is doing, is part of our theories, they call these relay cells because they look like one spike comes in and one spike comes out. great, what's that for? Not very useful, is it? So they're like, oh, it might be a delay, but these cells are really complicated. They have 6, 000 synapses and very complex architecture. It's a huge amount of complication down here we can get into if you want to know it. But what's the point of this? If it's a relay cell, what the hell is it doing? that doesn't get useful. No one really knows. We now know. We're pretty certain what it is. They're remapping. So what spike output turns out that can vary. That is, cells, they get more than one input. They have more snaps in some parts of different parts of the, the retina. And so they're like multiplexers. They can basically take an input, remap it and send it to the cortex. That's the basic idea. There's a lot of salty and confused about how it does that, but there's a lot of suggestions that's what they're doing. And theoretically, it makes sense, because what we think is going on is, one of the, one of the major projections you have going down there is layer 5A, which we think is orientation, and that projects down here, and it says, basically, given my current orientation of the center to the object, remember this is the orientation of the sensor, like a sensor patch, like your eye, if I tilt my head, the orientation changes. Oh, if I rotate my finger, the orientation changes. That orientation comes down here and tells this guy how to compensate for it. It says, rotate the input and it has to do that for motor too, because if I'm reading text and I rotate my page like this, which you do all the time, your eyes have to move diagonally to read the text. So your motor behavior changes based on the orientation and the, what you sense changes based on the orientation, and this is compensating for it. So now we have the complete system. You've got a system with movement coming in, features coming in, there's a, there's a location, objects are modeled by pairing locations with features. the orientation is used to make sure that movements in the, the, features are rotated properly. We do temporal pooling, and now this is the object coming out. And the reason it goes into Lay 4 in this region is saying this feature on this, column is the entire object of this column. It's this is the logo, this is the column.

I'm going to stop soon.

And you have one column to one column, but just for completeness, like the V2 column, will we see L4 input from a whole bunch of other columns? not clear, actually. I didn't talk about columns to columns. The way I've always viewed this is we need to understand a single column first. Because that was Mountcastle's axiom, and then we have to understand how single columns work together hierarchically, like one column here, one column there, and don't get distracted by all those other columns, but now the question is, is there convergence onto V2 or something like that?

I don't actually know the answer to that question. There's evidence that there is, but is, but that convergence, it's, I'm hesitant because I think it can be interpreted wrong.

Isn't it's often physiological, as in if you measure the response in B2, it's oh, this is getting, this can see a lot, but it's, not necessarily the case that's because of convergence of input. It could be more. imagine you have a visual column and you're looking just like through a straw, like one column, and it sees a cat, right? And you can't only recognize the cat by moving the straw around a little bit. Okay, I see there's a cat. If this is the thing, the output will be cat, and this guy will be getting input for cat. And that looks like a broader area of the cortex, right? Because it is, it's everything that this guy could see as moves around. so physiologically, as Neil said, this would be broader area would respond to anywhere in the cat. And if you look at this layer, it's only going to respond to what's part of the cat. But if they knew how to look at this layer, they would see it was responding to cat. But as we said earlier, they don't really know how to look at this layer, because it's sparse and there's all these things. And so these are testable hypotheses, but it has to be like this in some sense. It's deduction. It just has to be. People get annoyed when I say that, but that's the way I do it. The surface area is smaller, though, as you go up. either the columns are getting smaller, v2 and so on, or you're getting a conversion input. okay. Not necessarily either one of those will have to be true.

first of all, interesting going from v1 to v2, v2 is actually a little bit larger than v1. So you have the, you typically wouldn't see the same with the S1 and S2. The first two regions are really large. Yeah. And then they start getting smaller quickly. I interpret that as. Most vision actually occurs in V1 and V2, and you only need two layers to learn a compositional structure. And so there's a huge amount of memory in V1 and V2. It's got to be learning a lot of the world. And then these things get narrower, very quickly and they become more multimodal and other things. so that's the number one thing. it's not a strict convergence going up. and there are multiple ways that a region can get smaller with or without, I guess I want to avoid the word convergence because that's essentially saying, Oh, this column can only understand something if it's looking at a larger part of the world. It can be physiological. It doesn't, I haven't worked at all, I don't like to, if you really want that we can spend some time on this, but it, I don't think that's the right way to look at it. It's not, the right way to look at it isn't like I have to have convergence of columns, because I think the whole system can be built with a single module, single column on top of a single column on top of a single column. It won't be the most efficient system, but it would work. So you don't have to have convergence of columns for this to work. We can build our first oppositional object with two learning modules if we want to do that. Yeah, I guess maybe one way of saying it is like, it's not an issue if there is convergence. It can be accommodated. It's not necessary. That's a good way of putting it. Don't fool yourself and think we have to have it. We don't. It can exist, but you don't have to have it.

In fact, the theory came up with compositionality, is the following, you have, imagine this is v2. You have a bunch of columns here and here's V one roughly the same size.

the columns can be a little different, but roughly the same size in the area. Maybe V two is a little bit larger, like I said.

what we, what the, theory that we worked out, and this is the last time I, it was the first, we kind of piece this together when Vivian was here about whenever it was, we're working on this problem, is to understand that the way positionality works. It's on a location by location basis. So imagine this is getting input from some part of the retina.

let's see here's eyeball.

the retina is back here, right? Imagine we're getting input from this little patch right here. Okay. This column is getting input from that patch. And this column is also getting input from a patch that is co located with it. They're essentially pointing at the same point in space.

V1 is looking at the retina, it gets input from the retina, V2 gets input from the retina, and this column, these columns are both centered on the same part of the retina. When, we learn computational structure, and this is all in the heterarchy paper, we're essentially saying this, co alignment is essential. You learn what the system is doing. If I have a coffee cup with the logo on it, it's basically saying, at this point on the logo is also a point on the cup. And at that point in physical space, I can assign this point on the logo to this point on the cup. It's on a location by location basis. So these two guys, as you move around, it says, oh, I'm going to, I'm going to learn to connect, basically this is a sort of a bidirectional connection here. I'm going to learn at this point of the cup, I'm at this point of the logo, and I'm at this point of the cup, I'm at this point of the logo, and so on. And this allows the logo to be morphed and changed and rotated and all kinds of weird stuff and still learns it.

so when we build compositional structures, We can really do this with just two columns. that is efficient to do it. It's sufficient, not efficient. then less than theory tells us. And yeah, and that location by location also can store the orientation, which is why, yeah.

But yeah, maybe it's worth checking that. It makes sense because that will be important for the location by location for the behavior. It has to know the idea of the object. this is the logo on the cup. It has to know the orientation of the object. like it's rotated here, and it needs to know the scale of the object because it could be bigger and smaller. We learn this stuff really quickly and instantly, so we need, we learn these things for everything. And the surprising thing was it has to be done on a location by basis, but if you do that, then the logo can have distortions and be twisted and can do various things. So these are the three things we have to learn as part of the model. We have to say the feature coming into this guy. Is rotated in a certain scale at this location, and we're going through the circuitry for how this actually works when it's in the paper, but you can see how this actually works. And then maybe it's with that. But just that like that, informs inference. So If you've learned like a particular mug with a funny logo or whatever that information coming in about like what the pose at a particular location of the logo is can tell you, oh, that was that mug that had the weird thing, but also you can, it can assist prediction and that let's say, because of the other side, you saw the mug. This is that weird mug with the weird logo. You can predict at a location a particular orientation scale of the logo.

so yeah, I guess it's just that the location by location base is important both for the feedforward and for the feedback. Which comes into, the anatomy. and I'm just emphasizing it because when we come to talking about the behaviors And that's the location by location. Yeah, it's helpful to understand that. And, I think when we talk about that, we should project the images from the paper because they're really not right in these papers. So are we saying that input to v2 is more receptive field or is it the same receptive field as the input to v1? It could be a Typically it's a larger receptive field, like from the retina itself, more ganglion cells in the retina converge onto V2, but that's not important. It gets it lowered out, right? it may be very efficient and useful, but it's not theoretically essential.

and yeah, and it would fit with this learning, like a coarser model and this learning a more fine grained model. So there's only so much information that any cortical column can store. So if it's getting small inputs, Or like kind of fine detail inputs. Realistically, it's going to learn a fairly small but like detailed model. This one, it's getting over a larger surface area, so it cannot necessarily distinguish details as much. But it can build like a coarser I've always thought this part was I believe it's just in hierarchy and not I've made a good type of pool in the hierarchy to take care of that part. Because I can't Like how big can this receptive field go, right? Because we get bigger, be one. They're really small. Yeah, before does it cover the whole object? it's not clear. It's V two definitely gets input from the retina. I don't know about v4. so it cannot be that big that can cover like. Larger objects, it would be bigger than V one. I think there's a lot of evidence that the, convergence from the retina onto the cell and V one and versus cell and V two V, there's more convergence onto V two, makes the broader area.

that's a, that's also property and somato cortex. so it's, that's an empirical observation in some sense. We accept it. I think Neil's, there's a couple advantages that Neil's mentioned. One, it's the V1 is the highest acuity you can get. There's nothing more refined than that. So if I wanted to, if I'm reading the smallest print that I could possibly read and recognize, it's almost certainly happening in V1, if I'm saying that's the smallest. And V2 would be too blurry, right? But if I have a very big letter, V1 is too small because you have to integrate over a huge amount of area and it's just not very useful, but V2 could do it in a more reasonable way. So that makes sense, from a just scale perspective. It also makes sense that when you have composite objects, it's really hard to come up with an example where a child is bigger than the parent, right? It's almost always the child object is smaller than the parent object, so that also makes sense. In fact, if it was bigger, then you would probably flip around and say one of those, so So this will, but from a theory point of view, it's not essential, it's just that they have to be two different objects, and they have to have the ability to have the same, at times, the, same, point on them. So they're looking at the same point, but one is focused on the Right, one is attending to the logo, and one, so when you see a new object, I picked that one up. It's got water in it. I don't know. that's not a great one, but it doesn't have much of a logo. Oh, it's just Kirkland on it.

when you see a new object, oh, that's a bottle and then you tend to focus on the details and you start saying, Oh, it's Kirkland. It says this and that. Oh, it's got this little funny bottom to the cap. While you're doing that. I believe what's happening is there's a higher region. That's still saying this is the bottle and you're attending to all these components and in series building up the model of the bottle. Yeah. So now I say, oh, yeah, I see this, the Kirkland logo, and there's this thing, and then there's that thing, and there's this thing, and I can look on the back, oh, there's this thing, and now I have a model of this object, because I knew it was a bottle, and I'm looking at the intended features one at a time. So that part of our, mechanism for doing compositional structure is to keep, we were talking about this earlier, about your unsupervised learning. One region or one column has to be maintaining, I'm looking at this bottle, and the other one is saying, I'm, no, I'm looking at the features on it individually. and recognizing them individually. How does the higher level know it's a bottle without the lower level? Because the higher level also has, it basically has its own feature detections that come straight from the retina in some sense, right? it's, the boundaries of the bottle can be learned. these are grosser features. and, and then only when you, then you basically attend to a subset does the bottle stay the same. I don't know how else it could work. it's like you deduce it has to do this. I, I can't, maybe I just failing in my brain, but it seems like this absolutely has to happen. Once you understand it, you're like, there's no other doing this. You have to be able to sign these individual components to a larger object. And you do that by moving around them. look at them one at a time.