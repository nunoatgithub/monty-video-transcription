I will, share the document, Google Doc, where a lot of people have put a lot of comments in, which was pretty exciting. and then I will, we'll go from there.

Okay, make the zoom smaller, okay. yeah, after the meeting last week, I had. Or I wanted to walk through an example of my own, and so I have done that with lots of people commenting, and then for today, I just went through everybody's, I've been on top of the comments, but I've just gone through and, put together some questions that we were, still some open questions, yeah, yeah, that's about it, actually.

just a little refresher, the example that I've been working on or, that we started to think about last week together as a group was this hunch only, Keyboard, where we can sense what the key is. I tried to draw it a little. Nice drawing. Thank you. So this is something that presumably we can sense that it's an N without having the vision. sensor and, we were trying to achieve the goal of typing phone number or, the word Numenta, in two hierarchical ls. So that's, where we were.

Yeah. And, then I think, one of the point that I, we had the most kind of conversation or uncertainty about was. where is the keyboard model? And I think it's different. The settings are, I think, different. I think what we're learning, I think we can all agree that there is a keyboard model that's being learned. I, in either of the, learning modules, either in the lower level or the upper level. I call them R1 and R2. but in the case where we have content so proficient that we're not really thinking about typing each letter, but we're typing the word itself, do we still have a keyboard model? Yeah. No, it was even, I went even further than that. I was thinking about how we learn to type. And, and I'm not saying we don't have a keyboard model, we do, but it's not a very accurate one. and, how we learned to type, it seemed like we would often use, as human, we would use vision to locate the object we want to touch, and then we would touch it. and so there was a, so the model came into play. That it helped you visually find the key quicker. this is just, this is not our, this is not a task we're doing necessarily, but this is how a human would solve this task. the keyboard model wasn't even essential for a human to do this. the hunt and peck idea is you hunt. You just look around until you find the key you want, and then press it, and the keyboard model helps you find that key quicker, at least when you're first learning to type. So it was interesting. yeah, I feel like that's key, I guess that, that it makes it quicker, because that's what we're trying to do is, use models to make learning more efficient, otherwise it's essentially like a random walk until you find the key, which is what deep learning, reinforcement learning does, but I just want to make a point that when we think about what the task is that we're doing here.

I, just, it's not clear that we have two learning modules, one is going to use the keyboard model directly, it's I feel, oh, I need a, learning module that's going to learn the actual movements to do this typing, and that's, that learning module, that's like a motor, and one type of learning model, doesn't actually have a model of the keyboard in it. it doesn't need one. it just learns these, behaviors relative to the body position. that was the insight I had from this. It was like, oh, wow. I'm not saying we get rid of the learning module, but I'm not sure it fits neatly into two, get rid of the keyboard module. I'm not sure it fits neatly into two learning modules. that's, that was my conclusion from that little study there. Like you might need, you need a word model. you might, if you want a keyboard model, but then you also have to have the actuator model, which is a, which doesn't have the keyboard. I don't know if that was clear in my little write up, but I felt wow, that's a surprising realization in my case. So do you think that we might not need a keyboard, so like in the situation where you sit down to a keyboard that has a different spatial scale than others, like it's a little miniature or a little larger, what, do you think a keyboard model would help you rapidly remap the scale? no, I don't, think that's right. I think, cause you don't relearn the type. What you just try to do is scale your movements, right? Assume you still touch typing with two hands. Yeah, that, the more logical, it's not oh, this is a new keyboard. I'm going to relearn it. Let's go back to my keyboard model. It's more oh, these are the physical movements I usually do to make these things. They're not working quite right. And you can scale the whole thing. We've talked about this in the past, there's ways of scaling, object models and, behaviors, and so that, I think the more logical thing is you have to just apply a slightly scale factor to the whole thing, and then you're, the, your movements will be slightly less, something along those lines, but I don't think you have to go back to the keyboard model because it's I don't do that. if I'm just not typing right, if something's wrong, you just, not by looking at, studying the keyboard and, or thinking about the keyboard, you just just move less, that's how you do it. It almost seems like the typing part is. a model free policy at this point, like we're not really using the model anymore to do the typing, but then if we do use the, hunt and pack strategy, then we switch back to slightly model based policy because we use a rough model of the keyboard of like where are the letter keys to constrain where we look for them.

and also I think that's right, although I wouldn't say it's model free, the motor behaviors, I mean it's a learning module, it's playing back a sequence. It's more like playing back a melody, it's playing back sequences of physical movements that it learned, that it was exposed to over and over again. So I could say that when you're typing a word. there's a model of how to type the word. It just isn't a keyboard model, and it isn't a word model. It's just the motor system is given okay, it's time to execute this, sequence, and go ahead and do it. So I wouldn't say there's no model there, it's just more like a melody. But is that, yeah, is that really a model? Or I appreciate it's a representation, like depending on the linguistics, you could argue it's a model, but it feels like the kind of, like the whole point of a model is how you can reuse it in different ways, where if it's like a sequence, that's like a very fixed thing. And that's it feels like a model free, like current model free systems could learn that. do that. okay. I guess what I would say is that I think it's happening in the cortex. It's happening in a learning module. It's just like playing back a melody and one could, you can argue, is that a model? I said, it's not the sort of, the model, it's not the kind of dimensional model we, we typical think about. in Monty, but it's, a type of, it's certainly happening in one of the layers in the cortex, and it's just basically saying, I'm, I've learned a sequence of SDRs, and I'm just playing it back. I, just don't want anyone to think this is outside of the realm of learning modules. I think it's inside of it. I think learning modules do this. So we're going back to it. so just, in fact, we started by studying melodies, and we never really got, we made a lot of progress on it, but we never really got it figured out completely, with this whole issue about, Pitch invariants and other things, which, but, but anyway, I think, okay, it's a sequence. Anyway, I'm just saying, if I were to build the system now, I wouldn't say there's just two learning modules. I'd say, if there's a keyboard model, that's one, that's in one learning module, a keyboard model. If there's a word model, that's going to be in another learning module, and then a third learning module is going to actually execute the sequence, whether we think that's, It, if it's just doing a replay of a melody type of thing, fine, but it's still a third learning module. It'll do other things in other situations. I think so. Anyway, that's, couldn't the sequence be learned in the word? One, I was thinking the word is like a particular word would be like a state in a learning module or the id in the learning module that models words. And so it's going to carry out the actions until it's, state satisfies that word.

I didn't follow that. Your first question I got, but the rest of it I didn't get. I think, I would say, that the, in the model that's actually moving the finger, making the motions, that is, it has no concept of word. it just says, given this, you've asked me to execute a sequence, I'll execute it, but any other definition of meaning of the word isn't there. It doesn't know the letters. it just says, to type this word, I will, Move my finger in the following ways, but it doesn't know that those movements constitute letters in a word. It's just a playback of, yeah, the concept of a word disappears as soon as it starts sending a letter at a time to someone else.

Yeah, so one way I was thinking is if you had higher up You had, yeah, some learning module that models words. And so that's like in an abstract space. So it's like a sequence of letters on a 1D thing. And then it say passes a goal state of like to the keyboard learning module, be in the state of a letter being pressed, a particular letter. So it sends like an abstract letter. And then the keyboard model knows approximately, not perfectly, and this is before we have learned the model free version. It knows approximately where that key is, and then it sends a motor command. either directly subcortically or to, a learning module for a finger. That's close. I think, the keyboard model wouldn't send a, it would send, it basically would send a location. Yeah, exactly. Like a goal, yeah. Move your finger to this location relative to the, this allocentric, this egocentric location. I feel not even necessarily move your finger, just. Someone be applying pressure at this location. Okay, if I send it to, if we only have one motor It could be achieved by different But in this case, So you're right, let's go down hierarchically. We start with this, somebody knows what a word is in terms of its compositional letters. And then it says, okay, it starts out sending a letter at a time to someone that's you have to actually do this. Okay. but, it could be like that letter was spoken, it could be the letter was typed, it could be the letter was written, so as you point out, there's a lot of different ways that letter can get instantiated. And the word model doesn't know, doesn't know any of the details about how those different instantiations are going to occur. somehow it gets directed to, the system we're talking about, which says, okay, this is going to be typing on the keyboard and, and now I have to figure out, where in space, this thing is that the finger has to press. And we've talked about two ways of doing that. One, you can have a model of the keyboard and says, oh, using my model of the keyboard, I know where to put my finger. And the other, I think what we actually do in humans is we often just, we'll use a visual system to locate something in space. It's more find the letter A and then tell the motor model to move to that location and press it. But it could have told someone else to move to that location and press it. it could have done that too.

If I, if you're following all this, I'd like to keep going on for a second. We talked now about just like doing one letter at a time, right? You're decomposing the word one letter at a time, but clearly Learning moves down the hierarchy, and we don't want to do one letter at a time. So we just talk to, okay, we're sending a letter at a time to someone else, but ultimately, we want to just be able to send them the whole word and say, take care of the whole word. so I think that's easy, but it means that there's two messages going down, down the hierarchy. One is here's the letter, but you also know the context of the word. So when you learn the sequence, I don't have to tell you the letters anymore. You can, I just, as soon as I give you the word, you'll, know what sequence to play out. okay. You know what I'm talking about? It's like this. It's like a trick. You have to have two signals going down the hierarchy. Yeah. I guess with the, I was just thinking when you, yeah, with the way it was described, like with the hunt and peck, I feel like that could still, that's still probably supported by the keyboard model, but the goal state it's sending rather than apply pressure here. It's look, or rather move sensors here, to confirm my hypothesis that this is approximately where the letter is. And then you would saccade there, see what there is there, and then that would that keyboard model would then be basically updated based on, Maybe, Again, I was impressed by how poor our keyboard models are. I used to think, yeah, I have a perfect model of the keyboard, but it's really not at all. I think, the whole system could work without a keyboard model, but I'm not saying there isn't one. I'm just saying, because really what was, what, we could try to decompose this further. Someone says we need to type this word. Here's what, let's do it one letter at a time.

and, then we decide we're going to do it this means, meaning with a finger. then, so someone, basically what the system does is, okay, I have to find that thing. you could use a model, but this seems like we, personally, we don't do this very well. But we have to find where that thing is, locate it in the space of the hand, and, and tell the hand to move to that location and press. so in the end, the hands part and the finger part doesn't actually know that it's typing an A, it's just being told to go someplace in the context of some word. It's given a context of a word which it doesn't understand, it's just some SDR, and it says go to this location, now go to this location, now go to this location, now go to this location. And in this context, you could, if you do this enough, you'll learn how to do these things in sequence. So if I just give you the word, I don't have to give you the letters anymore. That's how it feels to me.

Yeah. Yeah, so it's odd that the motor model has actually no idea, other than it's getting a word SDR, it has no idea what it's actually doing. it's just playing back a sequence. It was pretty, pretty bizarre. So I guess. Oh, go ahead. No, you go ahead, Viviane. Just zooming out a little bit, So given that it seems like a lot of typing is not using the keyboard model anymore, it's definitely a cool example to talk through and I feel like we talked through a lot of issues and different aspects of it, but And if we actually start to implement something, do you still think this is a good example to go with? I don't know.

maybe, not. So we could implement, so all I wrote up was like, oh, this is an interesting observation that humans don't use the keyboard model very much, right? It doesn't mean that we couldn't design the system to use the keyboard model because that was our first, all of our first explanations. So we could design one that says, okay, we're going to learn the keyboard model and we're going to use it to figure out how to move the finger to the different locations. That would be a good problem to analyze from a, like a test problem. Yeah, I liked our dinner table example that we came up with. A while ago, because there we had like explicit compositional structure. We had the different objects on the table, the cup, the plate, the, cutlery, and then the arrangement of relative to each other as the set dinner table. I think it's a great problem. But the problem is that we move the objects. But the problem I was trying to do like an end to end analysis. Okay, like, how are we actually going to build something to move the plates and pick it up and, there's a lot of stuff that's got to go on for us to do that. We're, that's not really described in that problem or it's Yeah, I think in the first iteration we said we just want to look at decomposing goal states and basically the lowest learning module would just output its goal state of moving that object to this location and then the actual moving part of it. would be like just setting the environment to that state instead of Yeah, like we basically said it would be like a subcortical or like a robotic. That's just okay, so plate needs to be in X state. I guess I feel I, I don't disagree with any of that, but I feel like we're just shoveling off some of the hardest stuff to subcortical. not subcortical. It would be more like, like the lowest level learning module would need to have some kind of robotics approach to do this path planning of how to move the object. But it could still be happening in the learning module. It was more like as a first step to focus on decomposing your goal into subgoals instead of focusing on how to actually execute it. I'm not going to object to that. I just, I found from, and it's just maybe my personal needs and biases, that, often starting with what seems like a trivial, very simple problem, Exposes all these difficult issues that if you start with a more high level problem, you're going to get bitten by it later because you didn't really think through the whole stack of things that have to be done. Oh, I guess to me, the dinner table example is the simpler problem. Like the keyboard example seems to have all these extra things in it. like model free policies and learning sequences and associating. It's funny. I think it's just the opposite because. There, I can actually imagine building something that moves in types, right? And I could actually make the whole thing work, where we talk about the table example, we can talk about the goals and the hierarchical compositional goals, which is great. But I'm, I feel like there's a whole bunch of stuff that we haven't really solved some of the hardest, they seem simple, but some of the hardest problems of like, how does that somebody actually pick up the plate and hold it and move it and, all that kind of stuff. I feel. Yeah. 'cause in the keyboard example, you wouldn't actually be manipulating the word, the world. You, would just be moving the sensor. but you do, 'cause you need to press the key, I feel like they both, like both of them have this, I feel like the hardest problem is yeah, how do you coordinate the, finger or whatever. I think we have a pretty good idea of like how we generate goal states to satisfy like some learned model or whatever, or learned sequence. But it's like the motor coordination part. That's the, hard part. And it's maybe a bit easier with the keyboard, but I feel like even there, it's, yeah, maybe actually that's a good time to, because, I know Hojae, you'd also written a bit about learning modules focused on, motor systems. So I thought I could just like recap of it. About what we've discussed in the past on that, and then we could maybe discuss that more. Who's drawing right now? I can't tell who's drawing. I'm drawing.

okay.

I have a sort of a quick comment on, I think, both of these approaches, which is, I think about, that step where, let's say you, you don't see a word on the screen. And you want there to be a word on the screen. And it's this mismatch that drives something, right? I wouldn't, go there. What mismatch drives something? Where's there a mismatch? Between the current state of the world and the goal state, which is to have had that word typed. Okay, I would use the word mismatch for something else. To me, a mismatch means I actually expect something to happen and be there, and it's not there, versus that's not the same as saying, oh, I want something to be there. That's not a mismatch to me. That's a desired goal state. It's not like something's wrong. I don't type a word because it's wrong. I think it's a mismatch because the action is what brings those two things into alignment, even if, my goal is to have my hand up here and it's here, it's the distance. I'm not, I don't see it because I'm not looking at it. hand here versus hand here, we're at, even something that simple, I can compute the difference between what I want the goal state to be and the current state and use that to guide the actual motion. I'm just minimizing. I guess I just, I would just. Personally, I wouldn't use the word mismatch. Okay, I can reframe the vocabulary. It's it's that's a, that's like a, I have a current state and a desired state, and how do I get from the current state to the desired state? A mismatch, we've always used the word like a mismatch to mean there's an error. Your model is incorrect. You've got an expectation that's not correct. that's, this is not the case. This is a case where I have a goal state, a desired state, And I have to calculate how do I get from the current state to the desired state. Maybe discrepancy? Or just difference? It's just a difference in representations. There's a desired representation and then there's the current actual representation. I just think it's more, it's not a discrepancy, it's just if I'm in my kitchen and I want to go to my living room, it's not a discrepancy, it's just I have a goal and then I say where am I now and then how do I execute to get to that goal? This is, we can take a difference between these representations. But that's, the calculation that's done to achieve the goal state. Yeah. It's not something I measure. It's not yes, I have to determine what that delta is. But to describe it as some sort of error or difference, it's more like, how do I get from one location to another location, from my current location to a desired location? I just don't think we should call that just a discrepancy. it is a difference calculation, the reverse of path integration. It's the inverse of path integration. how do I get from point A to point B? I calculate that. Yeah, it's a difference, but, I just whatever the, term is, like the next step is really critical, which is that we've assumed at this point that we just know that the cause of letters going on the screen is key presses. At some point, you have to learn, the causes of the things that change in the world. So in this case pressing keys or moving things on a table. So I wonder if when we actually train a system, it would be, we would actually need to have these like exploratory periods where hands are just moving objects around on the table. even if there's no goal or whatever, in order to create that association between, hand on objects moving around, now we have a cause. We know what type of motor action in the world creates the desired, creates changes in states.

yeah, that part seems it's interesting, Niels talked about this a lot, if I just want to move something, I want to move the fork from one location to the other, that's like a, you could say the cause, it's a direct movement type of thing. When it comes to typing, it's a little weird because it's not clear up front that if you push on this key that A letter will appear someplace else.

that's not something you can just learn by exploration too easily, right? We don't just put children in front of a keyboard and say, just play with it and see what happens. What children do, pick up objects and move them around and learn, that's like a direct cause and event. The key typing one is weird, it's like you have to do one thing here to get something else to happen over there. And the thing, if I press this key, the thing that happens is not tactical, it's visual.

I was thinking, we don't, we don't just play around when we, learn to type. We have an instructor or somebody sits there and says, this is a keyboard, when you press the key here, you have, the letter appears here, someone has to tell you that.

Yeah. So I guess, firstly, yeah, I think that's a really interesting point to bring up, Scott. And yeah, I agree. That's definitely important. I think, yeah, I guess this one's in terms of someone instructing you, I agree. that's how it's normally happens, but I guess in principle, a human who's put in a room with a, say a 15th century person who's never seen a typewriter or a keyboard, like they may go up to and start interacting with it and then discover that kind of effect of that action. It definitely doesn't take long for a kid to figure this out from right, I think there's just a lot of advancements and that's the same principle. I'm just bringing up that a lot of these cause and effect things are multimodal. Yeah, definitely. It's a weird thing sometimes. Yeah, and in terms of, which also makes it interesting, like how you verify the effect and how that gets integrated is another complication, yeah. I, think this idea, this multimodal stuff is actually gonna be really, important, in our goal and behavior. 'cause it came up already in the typing example. you, visually locate the key, then you then the visual system says where the move in body space and then, and their finger decides to move there.

the same thing, it just seems like all these things, it, there's a lot of things in the world where you take some action and you do some physical action and the result is completely elsewhere, different, and so it just brings up the fact that, a multimodal example is, might be important here, to, do these things. Like when we're doing the plate, Rearranging the table, do we have a body that's moving things and a, physical thing that's touching and moving things and we have a visual system that's seeing things? probably, yeah, that's also like typing, right? if I'm setting a table, I suppose I could learn to do it blind.

and blind people do, but, it's a hell of a lot easier to do it visually. You're visually deciding if the plate's in the right position and the vision system is guiding the hand to move things a little bit this way, a little bit that way. I'm, just speaking out loud, I'm thinking out loud here. When you're learning to type, you start with the visual hunting and pecking for keys, and over time you distill that down into, a concrete model where you don't have to look at it, Yeah. Did you just join the conversation, Michael? I've been here, but I didn't read your thing. Okay. All right. Yeah. That's what we were talking about. That was your exact example. Yeah. Yeah. Exactly. The hunt and peck is like you hunt and find the key. The visual system tells, locates, it's worth walking through these a little bit, even if we don't do this example. the visual system says, This is the thing that needs to be actuated. And at that point, a lot of different body parts could do it, right? You could use any one of your fingers to press that key or you could use your nose or a pencil or And you do when you're learning, right? Like you use your dominant fingers initially and someone forces you to not use them. That's what I wrote. Like it's the training. The training is to say, don't use your index finger. you have to use some other fingers, which are not usually very good at things like this.

exactly. So, there too, this compositional breakdown is you've got a concept of typing, or not typing, a concept of expressing a word that's turned into expressing letters. if you're in the learning phase here, that gets then translated to what is the physical means we're doing this? Am I speaking? Am I typing? Am I writing? Am I signing in sign language? then you have to then say, now that I'm doing typing, which is the actual physical thing that's implementing the, interacting with the world, is it my index finger or some other finger or, your nose? all these, and this, of course, this is, when we solve problems, we can solve them in lots of different ways. So I think somehow capturing that is important in this task, Just on something that was said earlier too, I think it's not that, in my mind, it's not that there isn't a model. It's just that the models. Not singular that it's spread out across the, let's say the hierarchy key. The model When you say a model of what? The model of the keyboard. Like it, oh, I don't know. It's not in one place necessarily. It's, across that thing of why that translating word into. Letters typed on the screen. So I think we, I think most of us on the call would disagree with that. We'd say that there is a model of the keyboard. The model of the keyboard is just a physical model of various features at locations, those features happen to be keys, but we're just calling them features at some location. That model of the keyboard doesn't know words. It doesn't even know letters. It just knows features on, at some location. It's like a physical model of the keyboard. Yeah, and I'm not disagreeing with that, I'm just saying that model may not exist in one learning module. I'm saying it could, so I'm not, I'm pushing back a bit and saying, why do it could, it's, this is the, I made a little comment at the end of my notes, it's I had this sort of epiphany about abstraction, that I, thinking about this problem made me realize that all these different learning modules have completely different knowledge of the world, and the same thing means different things to different people, we're already achieving abstract concepts, like a concept of a letter is an abstract concept that can be implemented in lots of different ways.

and the people implementing it don't know what they're doing. A letter eos or a layers, they're just doing something. They're being told to do something. I'm spewing course randomly here. I would say, look, I think you don't need a hierarchical model of a keyboard. You have a keyboard model. You have a model of words. You have a visual model that can say locate also can locate a particular object in space and say where is it. And then you have some, and you have a bunch of actuators, like a finger that could then go to that location. And my point is that if you have all of that, and you just zoom out to like the level of viewing all of that, you have a model of a keyboard. I think these semantics, these words are really important, I think, Michael, so we should explore them. To me, the model of the keyboard is not the model of the entire process of typing. It's just the physical keyboard. okay. And if you want to say we have a model of typing And so you're saying we can have a system that lets us do the physical processes of typing without having that physical model of the keyboard. Instead we have models of how to move fingers. Okay, and I'm agreeing. I'm just saying at some level you can view the system as having An understanding of a keyboard, even though it has no model of a keyboard, which I think is the same thing. One of the things about the thousand brains theory, or this whole cortical column theory, is that every learning module builds its own models. And they're self contained. the models that a cortical column have to be self contained. They don't have to be anything else. they're not, a model, by definition, we're going to define a model as something that's inside a learning module. And then these models can be connected together in hierarchical ways, but even if I have a higher, even if I have a compositional structure, like the coffee cup with the logo, The model of the coffee cup doesn't know the logo. It doesn't know anything about logos. It's just a feature. It's some bits. It doesn't have any idea what that feature is or what it means. It just says at this location on the cup, there is something that someone's told me. It's this vector or this, SDR. and I would just say, yeah, it's maybe worth revisiting on that point. Like one of the key points of the cortical messaging protocol is that like we don't pass bold models as information. at any point, that we pass objects, at poses, and and that's just key. So like a one learning module can't be like, Oh, by the way, I have this model of a keyboard or whatever it happens to be like, do you want to take a look kind of thing? It's you only send these, objects at poses, but, composed correctly that's very flexible. and it actually helps constrain it. It's a somewhat custom vocabulary. I'm not saying that you should change it, I'm just saying it's slightly counterintuitive to what like the deep learning world would view as a model where the whole system represents the model. I think you're right. It's exactly, you hit a nail on the head there, Michael. we could call it custom vocabulary, but it's in the language of brains and thousand brains theory and Monty, Models are, things that learning modules do, and they, and then as Niels just said, they communicate with each other, but we should avoid saying, models span, multiple levels and multiple columns, it's very misleading to, I think it would be really confusing to people, so it may be counterintuitive, but that's I think we, I'm arguing we should stick with that language, we should be very precise about it, I guess my thought was just to, yeah, to, This is where every time I reread the thousand brains thing, I've come to a new perspective.

But that seems like key stuff to say in a glossary. Maybe, There are a lot of counterintuitive ideas here.

we all struggle with it. I struggled with this probably more than any of you guys did. went through all these same things.

Anyway, just that seems important to me. We have Think about these learning models as isolated units, they contain their own models but they're self contained and that how do they work together is the question.

anyway, so nobody else knows about a letter outside of the, the, model of words. After that, everybody else is, because those letters could be different things and different instantiation. All right. anyway, we lost track. We were starting to talk about the document that Hojae wrote.

Yeah. Hojae, do you want to walk through the figure that you were drawing? Yeah, sure. I was just, sorry, I lost my track of thought.

I was just trying to going back to the hierarchy things again, but, I mostly had a question about how. we have some, this kind of sensor modules, learning module that we like represent as like long rectangles, but how the kind of motor system comes in, that was the first one. I had a different or better thought just now, but I forgot. So let me try to walk through this one first and then maybe I'll go for that one. So I'll try to walk through what I wrote on the document as well, because that will also help me refresh my memory. So basically I was thinking about the learning phase, so when we have, when we're learning to type for the first time, and the goal is to, type, let's say Numenta, put it this way, and then this R2 will send R1, okay, type N, Or is, let's back up for that. Actually, I think, I don't know if it would say type, it doesn't know about typing. Yeah. It says we want the letter N Yeah, we want the, but I, this learning module can't know about typing and sign language and speech production. It. We can't expect it to know that. It just says. My next state I want is an N.

Yeah. So not, instead of type, like B, have state of Produce N. make an N someplace. Yeah. And then the sub tool that will get past your R1 will be Change, be in the present, have the state of N, of Numenta.

okay, but, going back, just, even going back further than this, when we're learning, there's, just no goal, just pure exploration. That R1 will, move around, sense, and the idea that I wrote on the document was that it will have, A model of a single key, like N, a model of a single key N, and then a single key U somewhere out there, single key N somewhere out there, and then, but So this is a keyboard model? This is a, R1 is a model of the keyboard? of a single key. A single key? Yeah. I don't know that. Is this for a three level hierarchy? Is this below the model of the keyboard, or? I'm still thinking about, just two where we're just exploring, no, there's no gold. It's just I am just moving around in space and I'm sensing N, I'm sensing U and I'm sensing M and putting it into my, memory.

And then what would make, are you storing these keys at locations or are you just storing features of the keys?

I haven't really, I know that you've made a comment about the locations, at first I thought that I was storing them in their locations as well, but, it can't do that, right?

but I think it comes together as a keyboard when we have the goal of now I want the state of Numenta and when R2 tells N, okay, create N or like sense N, then somehow the motor system, I'm just assuming that somehow the motor system will know Like, where to go to press this N, for now. So that needs a model of the keyboard to store where the key N is, not just knowing how the key N feels because, the keyboard might be anywhere in the world, you, just You have to know where the key is relative to the keyboard to be able to move there, right? I guess you could, do without the keyboard, but yeah, it just feels less efficient. But I guess just to recap, like the location of the Like for the N learning module, or like the key specific learning module, its output is going to be like the letter at a pose in like body centric coordinates. So that's not fixed, like that'll depend, as you say, Viviane, on like where the keyboard is in the world. I guess in theory, like you could, then that keyboard could have a goal state, like press somewhere specifically on my, on this key, which is also in body centric coordinates. Okay. But I think my issue with that, I don't think there's anything wrong with that. I just think this would be a lower level in the hierarchy and you would still have the model of the keyboard just to more efficiently, find the keys.

What I type, I depend heavily on those little ribs that are on the, That's like an anchor coordinate, right? we had a lot of discussion about it. On that? Okay. We can bring it up again. But it anchors in body centered coordinates where the keyboard is. And where your finger needs, where your relative finger moves are. this is the clue that, that this bit, when you learn to do rote things like typing, that you do it all in body centric coordinates.

that's where the one. model of typing, moving your fingers in body centric coordinates, and it's not using an allocentric model, because that's what those little divots are for, so that you can, you get your hands in the correct position, and then that's it, you're done. To be devil's advocate, could it be a local coordinate system just for the hands? Yeah, of course. When I, yes, you're right. When I say that, it's, I don't mean it's, I use the term body centric. Yeah, but body's not always the torso. It could even be, like, the eye, or, yeah, or a hand, and, yeah, there's different neurons. We haven't figured this all out yet, but the point was there's no model that, at that point, When you've learned to type, you're not using a keyboard model. A keyboard model would be an allocentric model. Like the keyboards can be different places in the world and different orientations. Once your fingers are registered on the keys, there's no allocentric calculations going on. It's all relative to hands and it's just playing back motor sequences. So, actually, so about the location, like where to go, let's say like when we're learning, we start the hand at, let's say zero, zero, whatever that is. And then, we go around, for the most part we don't sense anything, and then it gets to N, can it keep track of, the overall displacement so that it has the location of N, relative to where it started? okay, we started at 0, 0, and then da, gone, turned around, and then finally at N, it's oh, this is, I know I started at 0, 0, this is Based on, all the vector calculations of movement, maybe the location of N is stored, in the motor system, maybe? we are introducing complexities, Hojae, which we, I was hoping we'd, your hand is moving around in space, randomly sampling stuff, and then, I'm not, sure that kind of question is going to be fruitful at this point in time, but I don't know. I'm just, again, I was trying to like really isolate some components of this overall problem, and, that's why having this sort of grounded hand on the little nibs on the keys was helpful, and I, that's why, I, don't think we just randomly explore looking for keys. it's, Yeah, maybe just to, a child looking at, a young person, anybody starting to type, the first time you see a keyboard, you actually make a visual model of the keyboard, you're not making a tactile model, you're making a visual model, you're looking at the different features of it, it's very difficult to learn all the arrangement of the keys, there's quite a lot of them, and they're very close together, so it's not a very good, easy thing to learn. But you make a visual model of the keyboard, and then, I don't, I actually don't think you actually have a, it's not clear you have a tactile model of the keyboard, only when you start, the visual model of the keyboard is instructing the fingers where to move. Go to this location, go to this location, go to this location, which, by the way, is a very common task in setting tables, doing anything, right? You use your visual system to locate where you need to go and implement things. so I just, I'm just pointing out that we have a visual model of the keyboard, the visual model of the keyboard is locating, places that, we want to go based on what letter we're looking for, and then it instructs them how some part, different parts of the body to move, to that location.

So fingers aren't just roaming around finding letters, we, look at a keyboard and we study it, and then we try to remember where different, letters are.

Yeah, I guess the question, so let's say we have the keyboard model, so that we know the locations of individual keys. So this would be a visual model?

sure. Yeah, I think, it is. then I guess the question of first, how do, how does the hand know to go to, and I guess I'm confused about like locations probably and how the, I, yeah, it seems to me once, if the visual system has to be able to locate something in, in body centric coordinates, whatever that means, it's, it's, somehow relative to, and then it has to say, how do I move a part of my body to that thing?

It's basically, it's, the, finger that's going to touch the key does not know that it's touching an end key at that time. In the visual system it says, you just go here and press that. And so the finger goes there and presses that.

there is, that's, we were earlier, Viviane was saying, that's trivial or something like that, but, I don't know, this is the way it works. I had some thoughts about this, translation step. Yeah. It's, it would, yeah, start with, there's this key in space that needs to be pressed. And so the desired state is to have that key down, basically. Now, the motor system, as you pointed out in the doc, Jeff, you could do this with chopsticks, you could do this with your left hand, you could do this with your right hand, but if you think about computing the distance between a current state and a goal state, In space, the right hand, let's say I'm flipping a light switch over here. Yeah. The right of my body, the actual distance between current state and goal state in space is smaller than with my left hand. So many of these things in the motor system could be saying, I can do that. and here's like the length of the trajectory it would take to do it. And what actually ends up doing it is the system that says, That's the smallest possible definition. That would seem to be often the case. But then again you're going up, to the thing and now you're holding a coffee cup in your right hand. And so then you don't want to put it down so you can't reach across with your left hand, right? yeah. it feels to me that, it's pretty obvious to me actually, that if you have a, that somehow the body can say, there's a, I've located a point in space I'm not sure what it's relative to yet. It's, some body centric coordinate. And then I want to say, how do I, what is the trajectory from a part of my body to that point? I could, it could be from my right hand, my left hand, it could be my foot, my toe. we're able to do this very quickly just by thinking about, one part of your body at some point in space, and then you know what trajectory, then your body knows how to get there. it calculates this, automatically, right?

and so you can reach with one hand, the other hand, you can use your knee, you can use your elbow on a doorknob as a lever. all these things, it's Yeah, I don't know, Let's say you do consciously make a choice to use your left hand, even though it's further away, maybe you have a coffee cup in your right hand. I don't know how to think about that yet, but if I just think about like the typing case, where like the letter U is closer to my right index finger, that's, what most people do when they hunt, hunt and peck, right? They use just the two, index fingers, whatever's closest. Yeah. I'm just thinking about it. So yeah, I'm just thinking about if we are actually trying to implement some system, this is like a potential mechanism that you could use. You're just computing like deltas, like I can, get you in that state, but I'm X distance away from being able to do it. And this column says I can get you in that state and I'm Z distance. Certainly. And the question is, what is the policy for deciding this? I could tell you, I could say, Scott, your job is to type everything with your left pinky. Okay. I can do that. I would say, you'd be slow, right? You would, you'd go around and go, look at each key as you're doing, just like hunt and peck, but use your left pinky. And then the question you're bringing up is what do I naturally do? What's the first thing I, oh, I, maybe I picked the thing that's closest or maybe I pick always my right hand, even if it's not closest, because it's more dexterous. I don't know the answer to that question. But clearly we can do it with any body part, right? There's no question we can do it with any body part.

Yeah, it feels like it's natural that yeah, the goal state that's okay, someone is pressing here or someone is picking up here or whatever, you project that widely and then, yeah, I think that's, it's intuitive that, if there's like a low kind of delta or whatever, then, a learning module might answer that. But in general. It seems natural that like a learning module, when it receives a goal state, it, because it knows the models it knows, it has some sense of whether it can basically achieve that goal state. And so if it's in the state of I'm already holding something or whatever, like it's just not going to answer that. That feels like maybe the, more, slightly more unconscious, like why you would use one hand rather than another or whatever. And then there's probably some like prefrontal cortex that recruits the basal ganglia or something. If you want to. Specifically say avoid using this thing just arbitrarily, like that's more like inhibition rather than it's oh, I can't answer that call. It's like some other part of the brain that's don't use these things. this brings up a really interesting mechanism, right? So imagine we're, we've said over and over again, we're saying, okay, we're going to try to, we'll try to express a letter. And there's all these modalities, right? You can speak it, you can sign it, you can type it, You can type it on different types of keyboards. There's all these different ways you can do it. I can, I could even say, ask, I said, my hands are busy, Niels. Would you touch the end key for me? That's another way of doing it. and so you can imagine that there's this sort of big fan out that are not a fan out, but this, message must be. Must be transmitted to a lot of learning modules in different modalities, and somehow the brain is able to either direct it, somehow it's able to direct, pick one, like which one am I, which one am I supposed to use right now? Am I supposed to speak the letter? Am I supposed to type the letter? it's a mystery. It's we, somehow, we have these abilities to do all these things, but at any moment in time, we're only doing one of them. And even then we brought the situation, even if I'm trying to do one of them, there might be an obstacle which forces me to use another one.

I think that's got to be part of the architecture here. The overall architecture has to include this idea that there's many different ways you can implement this particular task.

and then there has to be some sort of policies about deciding, which one we're using right now. So I'm just adding this to our list of things we might want to solve, in a particular, as we get into this problem, is we might want to have at least two ways of implementing the same goal of letters, or whatever it is, Yeah, I guess if we were gonna try and implement the keyboard thing, it might be useful to say okay, we've got a left finger and a right finger. Okay. And then, so that's like the simplest version of and then, we only learn with the left finger, or the right finger, but the left finger, so that eventually develops the model free one. I think, finger is also able to respond, and then so we show that, okay, if you inhibit the right finger, or the right finger Cannot respond, then the left finger can be recruited. So something like that, let's get back to Some sort of action policies to decide which one to do. But I think having the ability to switch between these would be a key component of any goal oriented system. And whether we call 'em two fingers or just two actuators, it doesn't really matter. They're independent means of doing this. yeah, I think that's pretty important to have that in there. as part of the solution here to this. It strikes me from an earlier comment that, these sort of seemingly basic things of moving limbs and sensors around in space, solving or searching the space of possible things for the one you're actually going to execute, dealing with constraints of space, and these are all fairly big problems in robotics. and we're going to solve it. But I'm just saying, so getting these basics right, and even as a human, before I, you never, or maybe you can, you probably don't learn to type at less than five years old, because you need a whole lot of just learning about how things in the world are connected. Learning how to move your body, etc. Solving some of these lower level problems of just locating limbs in space. one of the, we have to be a little bit careful, there's a little tangent here, Michael, but a large part of why it takes humans so long To develop, is that our bodies are changing during that whole time? We're growing? And so all these, anything we learn changes the next month when you're bigger. You have to relearn these things over and over again because your body is changing. So just bear in mind, it's not like we have some fixed body. It just takes years and years for us to learn how to manipulate things. It's like everything is changing. The brain is growing, the body is growing. So I think we can assume. That, we, I think we can shortchange that attempt somehow. We can come up with problems where we have, we've endowed the system with some, abilities that aren't going to change, that are baked in, model free, subcortical stuff, whatever, I don't know. Yeah, that, I would counter that with the example of the deer that learns to walk in an hour or two, and its body is still growing, so it's solved, that problem very early. Yeah. But again, that's true. Although that is a non cortical thing. Walking doesn't really take the cortex. It doesn't. Yeah. I guess I'm just saying that I think some of the reason as humans takes a long to learn is because we've, we come with so little innate knowledge or so. Yeah. Partly that's like a deliberate, kind of tabula rasa on the part of evolution, but I think part of it is just We've basically, as developed as we can be to squeeze through the birth canal of a human being. And so if we could, we would probably be like two years old or something like that before we were born. and that's what happens to animals, but our brain can't, like our skull wouldn't fit.

Two year olds run around really well, as I know. Yeah, but, so, yeah, I think a lot of that development happens in utero that has to happen afterwards for humans. And my bigger point is maybe work rather than worrying about keyboards. And it's a good end goal and a good way to think about this, maybe it's the simpler, the more narrowly focused problem of just like moving a finger to a point in space. Okay, I would, my preference would be to say that is a subcortical, no, the actual coordination of muscles to move anything is really complicated, and a lot of that's subcortical, and I think it's okay for us to, assume that we don't have to do that. Learn that, we, it would be great just to assume that we have a system that can do certain basic functions. I'm not sure whether that, I thought that wasn't okay. what? We talked about that earlier. no, like things like, like, to move a finger it requires. lots of different muscles that are contracting. Yeah, the coordination of the limb. Sort of the inverse kinematics part, we can say. But the constraint solving, I feel like, you've got to move your finger to pick up the glass, but there's a wine glass in the way, so you've got to go around it. Oh, that's definitely cortical, right? totally, right? We have to solve that problem. Yeah, that would be like a scene level understanding. it's also, But yeah, that doesn't easily solve it because you have to have an understanding of like where your arm is in space and what space it takes up and stuff like that. and that's almost all done visually, right? you don't if I'm, so much of what we do is we locate things visually and then we move our limbs to those locations, and the visual system is, verifying that we're doing the tasks correctly. So we have to, that has to be sort of part of this, or almost certainly has to be part of this kind of problem, solution we're looking for. Is like a first person shooter a good starting point, simple model? You've, you've got a target on the screen, you've got to adjust your cannon to move the target on the screen and fire at the thing you're trying to know, I think we're looking for tasks that can be solved in different ways and have, compositional structure to them. Niels example of making coffee is a, there's a multi step, multi, task. Decompositional problem, which it seems to me much more easy, much better than to say, hey, can I, you can train at any neural network to be a really good first shooter, first person shooter. It doesn't take a model at all. You can just run it through a deep learning network. But, to achieve it within this model system and then, yeah, it's, I think the coffee's a good one. Yeah, the coffee's a good one.

Yeah, go ahead, Jeff. Oh, yeah, just another twist on the keyboard example that I've been thinking about that might simplify a few of the problems is, or at least with the keyboard, it feels like we're very, trained on typing the keyboard already. So just as another thought example, could be to think through playing the piano where you also have a keyboard. whenever you press a key, you hear a different note, so you can immediately tell which key you pressed. It's pretty clear that we have a model of the keyboard because it has a very repetitive structure. but still, if you, need to play a new melody, you can, it's pretty much a slow process. Like even if you have sheet music, it's pretty slow in the beginning. But then if you've practiced it many, times, you can play the melody very fast, similar to how we can type words very fast. If we know the words, if it's like just random sequence of letters, like Scott posted that example, then we are much slower. So maybe that's another. I think the keyboard example is, almost identical to, the piano example is almost identical to typing on the keyboard. It, to me, it has all the same. Yeah, it's just a bit less. model free, like we're less practiced at playing it than we are.

unless you're a piano player, then it's not the case. it always struck me as odd about, playing piano, which I've done on and off of it is, when you get good at it, You, it's like, in language, a word, we know, oh, I know a word, I know all these words, a word's a word, and in, on the piano, the structure that you learn that's equivalent to a word is, is, it's a little bit more flexible, the better you get, you can see Some sort of an entire bar of music that has maybe some sort of there's various riffs that you learn and and you see the whole thing at once. So a really expert piano player can just look at several bars of music and recognize it as something and play it back. they don't have to think about any of it. They don't think about all these thousands of notes on the page. They just look and see this whole structure of like, a very complicated word. but it's not, but there's many of them and you just. I'm just saying it's very similar, but it's harder, because the word equivalents you're learning are not well demarcated, Yeah, I just thought it was interesting to think through because it makes it easier for us to imagine like typing a new word, like how you would play a new melody or learn to play the piano. It's a tedious process where you use the model of the piano to figure out where you need to press next. And that's, I feel like, where we want to start with the learning module, to use the model to, go to the correct key and press it. I think the piano is identical to the typing on the keyboard, it's just more difficult. that's how I view it. I'm thinking about it right now, walking it through, and it's it's got all the same attributes. Yeah. That, yeah, that's why I'm bringing it up because yeah.

yeah. Even even if I was, if I gave you some new word you hadn't learned before, or even, there's a lot of long words, so you may not, have 'em all memorized. It's a single sequence, and what you do is you, just go through them in a piecemeal, like a, syllable at a time, and, music piano's just like that too. Not a piano player myself, but like the keyboard as a touch typer, I locate that little nib, and I never need to look at the keyboard. I'm assuming as a piano player. Because of the size of the keyboard and the fact that you're moving your hands freely you have to visually locate. If you're, a beginner or intermediate player, the amount you have to look keeps going down, the better you are, a real expert piano player does not look at the keyboard at all. Even when they make a large hand movement? Even when they make a large hand movement. You can just look at them. They don't look at it. If they had to look, it'd take too long. They also have to be able to read the sheet music. But they don't read every note. That's my point. They don't even read everything. They don't have to look at the keyboard while they're reading.

Piano players, the good ones, they just, unless there's something odd, something unusual, different movement that's typical, they would never look at the keyboard. That's quite, that's quite impressive from a, Robotic manipulation precision, because you have to make large moves with precision.

But I imagine for, even an expert piano player, it would be hard to, have their fingers at central, center C or whatever, and just say, okay, play, highest D or whatever. And just go all the way over and press the exact right key. It's more that you tend to evolve. You walk up and down. And so you're re anchored as you go along. No, it's more it's like saying, the equivalent there is saying, if I ask you to type random letters, you've got to think about them, right? Or I find the numbers at the top of the keyboard hard. But I know the arrangement, I generally, I can't type those as proficiently as I can the normal letters. Because they're bigger movements.

I would argue it's bigger distance, and it's also, I would argue, how often you have to type them.

for example, some of my passwords have numbers in them, and when I type those numbers, I do it, I can often do it without looking. Because you tape it every day. Yeah, I tape it every day. So I, think, Michael, you bring up a good point, there's a, it's, a physically impressive feat that a, an expert piano player can move their hand two feet in, leaning across the keyboard and land on the right key with the right finger, right? Because you're having to use all those. I forget. All the inner body senses to get you. It's it there. It's just your body's been tuned to do this very precisely. It's like an athlete, right? An athlete has to do very precise movements to achieve. performance, whether you're playing, table tennis or something like that, very, precise movements very quickly. Yeah, I think in this case.

I like this idea of the long stretch out to hit a key that's far off. I think most pros are going to look to some extent, a quick glance if they're going a long ways. And it does seem like this could be part of just the physical restrictions of if we're talking grid cells. Grid cells have a certain, different modules have different degrees of granularity, and when I'm here, I'm just dedicating a lot of the most granular grid cell modules to the smaller area, but the stuff out here, it's like, it's just coarser in terms of the spatial representation. Yeah, and also, the path integration gets noisy and you, your proper reception is just not that exact that you could tell, if I'm moving my hand through space without feeling anything in between except for how my muscles are, contracting, it's really hard to do that, precise enough. So let's, go back, from Monty's point of view, This is the problem we don't have to solve because, we're not building these things out of, muscles and bones and tendons and stuff. we can assume, I think we can assume that if we can locate something in space, a location in space, that we can accurately and quickly move an appendage.

to that location, I don't think we have to worry about like how piano players move their hands so far.

Which is like you're moving a robot with high quality sensors and reproducibility. Cause I, that, that, is almost certainly subcortical in in a brain. It's probably involving the cerebellum more than anything. but it's not cortical. The cortex doesn't do anything precise, right? so I think we can avoid that problem. We can marvel that human ability, but we can avoid that problem here. we can just assume that if I can give you the location, we can accurately move something to that location somehow.

What were you saying, Hojae? yeah, I just said, I just wanted to ask you, Niels, if you wanted to say something about the motor system, 20 minutes ago, you mentioned something. yeah, that's right. Yeah, just because you brought it up in the document. I just thought I'd mention, yeah, that, what kind of the open questions were and, what we briefly discussed, which was, yeah, just how, yeah, I guess the kind of key anatomy or whatever to recap is that, every. Cortical column, and therefore every learning module has direct motor outputs, but those are in some sense going to be like potentially primitive, or that's of course like going to recruit whatever kind of, that's connected to, and then I guess the, other way to do some policy is the kind of goal state, which is probably like L6 top down feedback, going to, to other learning modules. and that's potentially recruiting motor cortex learning modules, for example. And then those, learning modules, or sorry, cortical columns and motor cortex would be modeling rather than external objects in the world, like objects of the body, like the structure of the hand, the structure of the arm, and things like that.

that's of course a very like high level, Description. But yeah, anyways, it's yeah, that's something we've talked about, but then how exactly you coordinate it is gets to the difficulties we've been talking about today.

yeah, okay, you've projected that goal state of I want pressure here. personally, there's the question of okay, how do we know which learning module answers it? But then if it's some really complex movement where it's okay, you need to wrap your arms around something and you need to press somewhere or, something like that. Like how, did all of them get coordinated and, but I was thinking maybe one example that I think could be useful is just like grasping objects, which, like we were talking about last time, how like for infants to learn to do just like the pincer grasp takes a while or whatever, but let's say you were given some like really weird object. Oh, actually, I've got the perfect.

a piece of coral, so now you want to grasp this or whatever. okay. I have a lot of subcortical policies so that I can leverage to a certain degree, but let's say I was younger, I'm still developing that or whatever. Like it feels like to a certain degree, like as long as we're doing it sequentially simplifies the problem a little bit that it could be like, okay, you put one finger, another finger. We still need to learn the kind of cause effect thing that Scott was bringing up. Like, how do we understand that? Pressure is needed to grasp, or like this will now enable us to manipulate it and things like that. but yeah, so that could be model based directed that okay, there's a location in space, but some finger needs to be there. And then maybe, the model for the fingers, the direct motor output, what that could help with in this case is as I touch it, I might feel that oh, okay, this finger doesn't really have very good contact. And that finger might then decide, okay, I'm going to readjust to get a, better, like better pressure or something like that. and so it might be like a mixture of getting your fingers basically into the course correct position is what the, like sequential planning does. But then, then there's a degree of like direct motor output that helps you readjust. And then I guess then the final thing is if you do that a lot, if I had, for whatever reason, I wanted to pick this up every day, And that's where it becomes more model free, whether that's cortical or subcortical, and you just your hands naturally form to that thing in parallel without having to think about it finger by finger. that's a perfect example, that coral is a perfect example of something that's so novel that you don't even know how to begin interacting with it, you have to think about it very carefully, like, where am I going to put my fingers, to pick this up? That's exactly how it is when I try and play piano because I never was good at playing piano. So I have to do like finger by finger. Okay. Yeah I think this is everything. This is way typing is too, right? so it's, really, I think this we're getting at some really core ideas here that You've got, we have novel ways of expressing behaviors that require a lot of attention. We have to decompose it thoroughly into, one step at a time, and then we're able to combine them quickly. I want to go back to this idea. I'm just stating it so I don't forget it, okay? So just bear with me. This idea that what we have to pass down the hierarchy is both this specific thing that we want to achieve, and then And the overall goal, because, you need the overall goals, the subcortical, the next region down, not subcortical, the next region down can actually associate, learn a sequence of individual elements as, one thing. it wouldn't be, it's not sufficient to pass letter to letter. I also have to pass down. We're trying to implement this word meaning just some SDR. It doesn't mean anything. It's just an SDR. This is the overall representation of the thing. It's just like the object ID, the goal ID, and then here's the individual elements in sequence, so that, so that the lower region can now say, okay, I can handle the whole sequence my own. I don't, I'll just take care of the whole thing, so you don't have to think about it. That's what we do when we type, that's what we do when we key. Isn't that passing, I felt the sequence learning happens at, within one thing, because otherwise we're passing, two levels of object. We're passing, a feature and an object. I, we are, because I'm, that's my observation, because otherwise, how would I, if I learn a sequence, let's say I, the sequence is A, B, C, D, okay, or Numenta, whatever you are, the word Numenta.

I want, in the beginning, the word model has to say an N, then a U, then an M, and an E, and a T, and so on. But I, ultimately, I want the, lower model to do the whole thing on its own. But I can't know what that is, just, if I say type an N, it doesn't know that's the word Numenta. It doesn't know anything. And so I have to also tell it, hey, by the way, in the context of this, is this SDR? don't, you don't worry about what it is, but the context is this SDR, we'll say it's the word Numenta, but I'm going to pass you down the individual elements for it.

Already doing with the feedback connections, right? we were thinking of them before in terms of object recognition that the higher level module passes down if the object it is sensing, like, all right, the word Numenta and then a lower level module can use that as context for the letters it's sensing. But it would fit perfectly with this since we're passing the high level object, but also the goal states for the individual components of it, the sequence of letters. Yeah, you mean, it's like the information that goes to L1 versus the location L6 on the site.

exactly. Oh, there it is, L6 and L1. Wow. Yeah, so we did. Okay. Nice. it's, yes, it just, it has to do that.

and, sometimes if you don't do something over and over again, it's not going to be able to learn it, right? But if you do something over and over again, it's okay. I've seen this pattern before.

I'll learn it.

to avoid all the complicated motor stuff, could you just make a simplified keyboard and keyboard controller where, you have like the notion of a finger and a finger can reach a certain set of keys? And so you still have a system with a bunch of constraints, but the higher level part has to figure out, okay, to get a U, I have to activate the left index finger to position three. I think that's the way I've been thinking about it, Michael. In my mind, we talked about, we used this example of the keyboard. I'm not Thinking we physically do a keyboard like this. I'm thinking it's just a placeholder to think about concepts.

yeah, so I, in my mind, I was already thinking this is some abstract object, which has got features at locations. We can call that a keyboard that helps to think about it. And then we have some actuator. Which can move to different locations, that it's been told to move to, and we can call that a finger. Okay. Yeah, but it's not like we're really building a keyboard and a finger. These are placeholders that describe concepts in my mind.