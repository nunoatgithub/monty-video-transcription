this presentation is about the common cortical communication protocol. it doesn't sound like much, but it's the glue within the Monty system.

so what actually is it? I don't know who is there, but if you were, do you remember November 2021? We had this meeting and Jeff wrote on this whiteboard. and we were talking about the, yeah, it's yours. I went back into the meeting recordings.

but yeah, we were talking about the cortical communication protocol or also the AI bus. Also, Everyone hated it, so we didn't use that. Yeah, yeah, talking about, how we might, build out an AI bus, which is a common protocol of how the components in Monty communicate with each other. So how learning modules communicate, how sensor modules communicate and so on. And it was talked about as like AI bus as a platform when vendors sell sensor modules, robotic modules, and so on.

and. Yeah. In the same meeting, there was also put up like a product roadmap with, potentially looking into reference frames and TBT next, and then also options for what to do next, at AI bus research listed down here. And I just drew two circles around this, which turned, as I would say, into the Monty project now.

and yeah, this is, there's also the mega project came, It was listed here as well. But anyways, the idea of this communication protocol was pretty much at the beginning of Monty, and it's still there even though we don't talk about it that often.

So what, where do we actually need this protocol? So Monty, just as a refresher, has three major components. We have sensor modules, we have learning modules, and we have motor systems. And then we have communication within Monty, which includes sensor module to learning module to learning module hierarchically, and learning module to learning module, through lateral voting. one note on the hierarchical communication, it can also go top down. from the top to the lower level learning module.

and then finally, learning module to, motor system. And then we have communication between Monty and the world, which includes world to sensor module and motor system to world. And basically, we want that all of the communication within Monty uses a common communication protocol, a common format, what is always communicated. And basically, these two are the interface between Monty and the real world. the main job of the sensor module is to convert raw data into the communication protocol. And then the motor system converts. The CCP into motor commands like muscle movements or moving actuators in the real world.

okay. Why do we need the CCP?

this is a nice figure that Jeff made, I think. and it nicely illustrates, I think of, How we want Monty to be a really flexible system, where we can just plug and play these individual components as we want. We want to be able to have as many learning modules from potentially different modalities interacting with each other. We want to be able to stack them on top of each other, to have a deeper hierarchy. You might want to have the possibility for parallelism, and that's really only possible if they all have a very similar, or all have the same interface and we can, yeah, easily combine them into arbitrary architectures, depending on the problem.

Okay, how do we define the CCP? So this is how it is implemented in the code. we'll go into a bit more abstract as well. So basically, all Monty components expect instances of the state class as input and are expected to output them. That kind of enforces, All the messages passed within Monty to contain certain attributes or information.

State always has the same attributes and is just interpreted differently in different contexts. So what does that mean? For example, a state output from the sensor module is interpreted as an observed state. The learning module output is a hypothesized state. which object do I think I'm on right now, and what do I think is the pose of it? learning module votes, so lateral outputs are all possible states, so list of state instances. And then, finally, the motor output is a target state, and just encode, so you can see it, for example, here. Hypothesized state would be the output of the learning module. Your observed state, this would be the output of the sensor module. This would be your vote. And they all have the same attributes.

so what are these attributes? So each state communicates a location and orientation, which is the pose in a common reference frame, for example, relative to the body and features at this pose and features. are optional and it can have as many features as wants and, features can also be modality specific.

And then on the side, some additional information that we use for information routing and weighing is a confidence value. So how confident am I in this location and orientation and the features that are detected, a sender ID, sender type, which is learning module or sensor module, because they are treated differently in some cases. And whether to use this state, which is just a Boolean.

almost last slide. What are the attributes of state and detail in the code? So at the moment, location is explicitly expressed as XYZ coordinates relative to the body. Then we have morphological features, which is a dictionary. It must contain pose vectors, which are three orthonormal unit vectors. They encode the orientation. So together those two represent the pose. And we have pose fully defined, which just tells us. how much, does the, is the orientation fully defined? Do we have some symmetries, for example?

And then non morphological features, again, it's a dictionary, it's optional, variable length and type can be modality specific, these features don't change when the object pose changes. So for example, color, curvature, or temperature, or something like that. And then confidence is between 0 and 1, your state is just a Boolean flag. Sender ID is a unique string for each of the building blocks of Monty, and then sender type is either sensor module or learning module.

So here, in the, there's no information as to whether it's visual or auditory or tactile. It's just completely independent of that, right? Yes. So the non morphological features. Might be like features that can only be detected by a vision, like color, but, yeah, you, it's always the same format and always the same attribute types, independent of modality.

But even the features, it seems to the receiver, you don't, they don't really care. It just might be color, but they don't, at least in the brain, it's just patterns. You don't know. Exactly. Yeah. It doesn't matter what it actually represents. We were talking about that this morning and it's one of the most amazing things about brains is that the neurons don't know anything, they don't know what they're looking at, they don't know what data represents, they're just cells and the system has to work completely. With nothing knowing anything about the nature of the data, but it's a fact for brains, each neuron just has dendrites and connections and learning.

It sounds simple, but the depth of that, making a system work this way and understanding it's actually very challenging, I still struggle with it myself. Yeah, and we've been So for the past, yeah, since November 2021, we've been implementing all the Monty components to figure out what actually do we need to communicate between them and to fine tune this information. it was pretty clear from the beginning that we want to communicate features. and poses, but just whether this all works and we can, and that we can actually use this format for all, the different types of communication within Monty, really came together nicely in the past months, I think. This is really nice. I didn't know you were doing all this stuff.

Yeah, I can go to the last slide just with what we can do now. and then, but yeah, go ahead if you want to do some, say something first. Okay.

So yeah, given this, nice protocol that we have, we can now flexibly combine Monty components and generate arbitrarily large and complex architectures.

we can use the inputs to learning modules to model sensorimotor input using reference frames, which is not directly Like that much related to the CCP, but we needed to make sure that we have all the information in the CCP to be able to do this. And yeah, basically we can, given the features and poses, we can perform fast learning of structured models and fast and robust inference on object ID and pose based on arbitrary movement over the object.

We can vote between models of different modalities. We can combine multi sensory models of object components into compositional objects and scenes. That's the hierarchy work we've been doing lately. And use learned models to guide movement and interact with the world efficiently. That's work that Niels has been doing. And, yeah, that's it. Wow.

Any questions? No, I didn't know all this was going on. Maybe you worked on this in context of the patents or something, like they just got today, They implemented this a while ago. they have to show all the If anything, we see it like this, and it's simply really quite amazing to me. I think, that's exposed in something. One way to think about Monty and with the brain is that we always think about AI systems or vision systems or whatever it is, processing visual data or processing auditory data or processing tactile data. But I think what's exposed here and one of the strange lessons of thousand brains theory is that The brain is really a processor of space. That's what it is. It processes space, the data type of space and pose and orientation, and the vast majority of what's going on in your brain is processing reference frames and spaces and distances and, it's just a process. And then the, what we typically think of, our sensory input is it's not, obviously it's important, but it's it's, a feeder into the space processor, And it's oh yeah, we're going to use this stuff to figure out what's going on in all these positions of things in the world, and how you know where everything is in relationship with how things move. it's just a, it's really a very different way from traditionally thinking about both computing and AI, and, I think it's, really cool.

It's interesting to think about transformers because they, with language, it's just one need. But they had to put in a notion of position overlaid on the feature to get it to work. And then the more recent stuff is, I think it's a cyclical location. It's not an absolute location, basically. Going back to what Viviane presented, the nice thing about what you presented is that We're taking, laying a foundation for, the whole idea is you're laying a foundation for which you can build a sensorimotor, it is a backbone for how communications works and, and starting on that premise as opposed to slowly discovering it through various, AI projects and so on. to take a much more fundamental approach to the whole problem, which I think is something unique to them out there, and it's really cool that, I think that 3D position orientation is something that could be elaborated over time. it serves exploring sensorimotor space, but I have a feeling that it could generalize on that. But it could be in any, it doesn't have to be, it could be in some abstract space. No, absolutely. In fact, that's the only thing, if our communication protocol defines three dimensional space. That would be a limit, I think, that brains don't exist. we define that it has to contain a location and orientation. It doesn't say it has to be three dimensional. Wow, there you go, smart. I thought you said three dimensional. I think brains discover space. Neurons don't know about space. other parts of the brain do, perhaps, but the neocortex, the neurons don't really, they just have to discover what space, what actually the world looks like. But it really looks like that's what they're doing.

So we can say that you allow to have, the real fundamental thing of those things is that they encode, independent dimensions.

Yeah. Yeah. And you need to be able to path integrate through that space, So like it works the same if, for example, in 2D Euclidean space, it would all work the same as long as you can do like location transforms and rotation transforms.

Anything you feel is missing in the CCP right now?

we've got some kind of exciting, I think, exciting stuff in the pipeline. yeah, Viviane briefly touched on like the, like motor, goal state. But, yeah, we're hoping that will really unlock the hierarchical action policies where different learning modules can pass goal states to each other and unlock this more like sub goal. I think that's a pretty basic system where, at the moment, yeah, the policy is still quite simple, but a lot of that is still, yeah, in the conceptual stage. Yeah, it won't change the CCP. The CCP will stay the same. We're thinking about additional connections in the network, like top down connections, and like Niels says, the motor command connections are not really used at the moment. and then another thing that we just talked about in the research meeting is that, as part of the features, for example, that get communicated, we might also communicate like object states and object behaviors, but that can also still be phrased in the framework of this CCP. Yeah. In some sense, the goal state, what's, that's an interesting one to think about. What is the difference between a goal state and an object? a goal is still something you can reason about and, manipulate and decide what aspect of the goal you want. It's still, it's not that different from an abstract concept. Exactly, yeah, so I guess the kind of whole point we wanted to do was unify that so that the goal states are in the same form. And basically, as you say, equivalent to the states that a learning module can be in. So as in, a learning module can express what it sees, or it can receive a goal state of what it should be in, what state it should be in. And based on that goal state, it can try and output other goal states, other learning modules, or, like basic motor actions to achieve that. And that way we can also have hierarchical action policies. So basically. It can decompose a larger goal state into sub goal states and pass them down the hierarchy. And a goal state could, for example, be I want the sensor to be in this location and orientation, for example, so I get some information to recognize the object. It could also, in the future, we don't, haven't implemented that yet, be something like, I want this object to be in that state. to manipulate the world actually, in some way, not just sensing. Here's an analogy I think it's apt, you don't want me to go over grandiose, but I think this is apt. the internet is based on communication protocol, it's very simple. IP Addressing protocol and packets and so on. And it was designed sufficiently robustly that it enabled everything we do on the internet today. it's gone through a few revisions, but pretty much held up intact. I think the stuff you're working on is as fundamental as that. And, in the future, these, if it's not this protocol, something very much like it, will play a similar role, it'll be as important. It sounds crazy to say that right now, but it seems inevitable in my mind.