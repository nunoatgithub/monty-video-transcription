Yeah, I guess this is, a yearend review. Hopefully, yeah, there's, not too many slides. there's around 30 slides. Hopefully I can, maybe that is quite a bit of slides. Maybe I can go through, the parts that I've already gone through in detail relatively quickly. I really want to, take time to talk about what I took away from some of these projects and what are the key next steps that I think could be taken. so yeah, questions are welcome.

All right, cool. Yeah, so I guess my time here started last August, and it's August now. So it's been a year, four key projects that I was working on. One was the dendrites project. The next was like a deep dive into spatial pooler and temporal memory. Then I wanted to integrate that into Monty, so there that again, the temporal memory for Monty Project, and I guess for the last two or what, three months I've been working on grid cells for Monty.

Okay, so the Dens Project, just a very brief recap. I guess this project started before I joined, but I came and contributed a bit of things, a few things. one of the key intuitions for this project was that we, they started looking at, neurons and deep learning, and we started comparing them to real parametal neurons in the ne in the neocortex. and the basic idea was that artificial neurons. A really vast oversimplifications of what's really happening. On the left, you see an artificial neuron diagram, where it's a soma is basically replicated as a linear weighted sum, and the axon is like the output of a non-linearity. but in, in reality, this is not at all how dead rights work or this is not how all, how neurons work. Neurons have basically thousands and thousands of synapses. and these dendri segments, segments act as independent pattern recognizers, by checking to see if a set of synapses are, are connected or not. And this is what depolarizes the neuron, the cell body to fire. and this is like some, this, is a deep complexity that's completely missing in deep learning. And this was the impetus for the dendrites project. so the first part was augmenting the standard point neuron, the artificial neuron model. on the bottom you see the feed forward input, and this is like the standard part of a, of an artificial neuron. But the key contribution here was adding context vectors, which kind of mimic basal dent rights. and these context vectors, you have, one weighted, one weight per segment per se. and you would feed a context into this, into this, into this area. You would compute which segment response has the maximal response to that context vector, and that would modulate whether then you're on fires or not. so yeah, I'm gonna try to go step by step here. This is like the full architecture of this was the key contribution of that paper. the first contribution was adding dendrites to the standard, point you're on. The second one was adding sparse weights, and that's the thing at the bottom. Instead of a fully connected dense layer where all neurons of one layer are connected to all neurons of the second layer, we drew some inspiration for the neuroscience and we decided to say that, these connections should really be sparse. this would basically reduce the sort of gradient interference that you see with most deep networks. and that would probably help alleviate, catastrophic forgetting, in like dynamic environments and dynamic training scenarios. and the third contribution was adding sparse activations instead of the classic, relu activation, which I think in its best case, only gates, maybe 50 of the neurons to be active. But here we introduced a variable sparse activation where only the top k neurons become active. like I said, this project was well underway before I got here. I think most of the continual learning experi experiments were already run. I started working on the multitask reinforcement learning experiments. and this was basically looking at this environment set up called the Metal World V two benchmark, where, you have this robotic arm, it needs to solve a bunch of these tasks, and you would ideally learn all 10 of these tasks together using some reinforcement learning algorithm. But a standard deep network would really struggle to do this because, when it's learning one task, and you're also trying to learn an orthogonal task, two tasks that don't have any semantic overlap, you would see a lot of gradient interference during training. but this is something that we really tried mitigating, by introducing this active dendrites network.

So some of the results here, I think the, the den, this is just for, the, reinforcement learning experiments with dendrites, which used about 7.2 million parameters. We had, a, model that had about an 88 average accuracy across 10 of these tasks. A regular deep network with maybe about 500,000 more parameters was far less accurate, about 11 less accurate. and an even larger MLP just showed that, adding more parameters doesn't solve anything, even though the large MLP had about 2 million more parameters than our Denverites model. all it showed was that, no matter how large you go, you'll still have problems like catastrophic interference and gradient interference when training orthogonal tasks. so yeah, this was cool to see.

Yeah, and this was like one of the key figures of the paper where we see that a neuron, in this model for both multitask reinforcement learning and the continual learning per experiments that Karin was working on. you see that before training and after training, how selective a neuron responds to a particular task. What you don't wanna see is that a neuron responds to all tasks, but you see here that it responds selectively, which means that, we see these sub networks automatically forming in our deep network, that correspond to different tasks. and these sub networks are automatically non overlapping, when they don't need to be and overlapping when they share knowledge. So that was cool.

yeah, we've got some good, PR out of this. this paper we published something in the Frontiers in Neuro Robotics Journal. That was cool. all, all of us gave like a, small talk at Yanik Kils YouTube video. that was cool too. just recently, Karin and me presented something at the Sparsity in Neural Networks workshop, about this work. and just last Sunday, me and Karin recorded a podcast for the Auto ml, yeah, auto ML podcast. They wanted to hear more about how dendrites might help, stuff like neural architecture search. So that was cool.

just a question for the, yeah. For this slide, or the one before with the, context, did you guys have time to play around with Task composition, like where you could have maybe a similar task that you, try and generalize to and you recall a similar context or, like composing two contexts that have been learned for a task that requires both of them or something like that. That's actually an interesting idea. I don't think we tried that. we used like fixed context vectors, and we let the dite segments learn whether a particular task was semantically similar to another task, but the context vectors themselves were entirely orthogonal to each other. They were just like one, not encoded vector IDs describing the task. Yeah. Okay. Also a criticism of Permuted is that it's like really difficult to study, transfer knowledge across tasks because the tasks are basically, they're the same task, but they're basically independent because they've been permuted. Yeah. Like a really simple example might be to like split into the first and the last five classes and train separately on both of those with different contexts. And then see if you can superimpose the context vectors to handle both situations, like classification across all 10.

Yeah, that is a good idea. but yeah.

Cool. No. but yeah, really interesting. I'm, sad there's not a screenshot of you, of the sunglasses. Oh, I don't need the, I don't need the reminder. There's a reason I put this picture up instead of the one where I didn't have sunglasses. Oh. It's just, yeah.

Okay. so I guess what's next? we briefly talked about how sparse we could use den right segments with sparse weights instead of dense ones. we didn't really explore that, but it was talked about. but going back to, I guess to what Neil's you were talking about, I think definitely more compelling benchmarks would be interesting. not many papers explored this notion of continual reinforcement learning, but I think that would've been really cool to try instead of multitask reinforcement learning. and instead of permuted where instead of learning all these 10 tasks together, for like a complicated robot set of the, I showed, you learn them in sequence and you don't see all tasks after you finish learning on them. Another interesting thing I think that could be tried is more realistic dendrites. Right now we treat dendrites as like standalone context processors. They're like floating weights, that don't really get touched, except when they're updated by back prop. But what we could do is make more realistic ones where we connect neurons laterally by a dendrite segments. So within a particular layer, just food for thought. that's not really a more realistic dendrite. So it's more of a more realistic, architecture. yeah. I should say more realistic, deep learning dendrites. it's not, it's really you're using the dendrites, but the change here would be, connections between cells and the layer, Is that what you're saying? Yeah. All right. So the, it's not really making the dendrites more realistic. It's more realistic neural architecture, just to be clear. Yeah, that's right.

Of course we did do that in other places. Of course, the whole temple memory built on that idea. Yeah. I guess this like purely for. Okay. onto the fun stuff. I did a nice deep dive into the spatial pooler and temporal memory stuff, 'cause that was something I really wanted to work on when I first started. but we were working on the, the dendrites project. So we, wrapped that up, got some really good, nice results from that. and then starting January I worked on like this sort of deep dive in trying to understand, all the nuances behind the spatial pooler and temporal algorithms. when I was exploring this, I found that all the existing code, or the fastest versions of the code had been written in c plus and linked to Python via some PI bind library. so the, one of the key things here is that it worked quickly, but it was hard to decipher and learn. It was really hard to, optimize the algorithm. it was also built on this really abstract sparse matrix library called Sparse Matrix Connections, which is something I think Marcus wrote. and it's very, detailed. It works very well, but it's also very hard to improve and optimize. so yeah, that was like my first thought when I started doing this deep, dive. what I ended up doing was, for better or for worse, reproducing all of the spatial pooler and all the temporal memory code purely in Python. And I think anyone can use that now. if anyone goes to the frameworks slash HTM folder inside research, they can find like the spatial folder that has been written completely in Python or the temporal memory algorithms, which have been written completely in PyTorch. and yeah, this is like a snippet of, all those three, would those implementations be slower than the c plus? Yeah, I'm getting there. Okay. yeah, but I would like to say this is like the most readable code I've ever seen. This is it's, you almost can read the code and see what's in the paper, direct way. So I, it may be slower, but it's incredibly instructive. Thank you, Ben. I'm just trying to understand the, point of sometimes we spend huge amounts of efforts trying to make things fast, like moving away from Python, and now you're going the opposite direction. You're moving to Python. It just, I'm not questioning it. I'm just saying it's like funny because we spent all this time optimizing things to see and now you, found out, you wanna go back to Python. yeah. So I guess, you're right, but one of the things I, I did when I started this was my first intention wasn't to make it faster. I already thought it was pretty fast. My, my intention was to, make a readable version of the code where anyone could understand the algorithm just by reading the code. and I guess that would improve, if you really wanted to build upon the algorithm, you would do so here in this prototyped version and then move it to SQL Plus to become faster. that was my idea.

But in terms of speed, so this is like a questionable outcome of if anything here was useful or not. the Python spatial Pooler is readable and it's fast and it's also easy to build upon, so that's good. it's almost as fast as the c plus version, but it's negligible. You could use either and be perfectly fine. The temporal memory one was a pain though, the code is reasonable, readable, but the actual, the runtime is, really, slow compared to the c plus version. and I moved it to Pito specifically so that I could run it both on, CPU and GPU, but I realized that even if I didn't move it to GPU, the, operations that were happening for the sparse matrix multiplications are, still inefficient. Moving into GPU will really change that. so I guess like the biggest outcome of this was that it's a good learning tool to grasp all the nuances of the algorithm for anyone who's starting an htm. how much slower was it? just roughly, 26 times more from the last experience that I, yeah, it's really slow. That's significant. One thing you should know, defaultly, the way that sometimes they do these operations, that they take the spars representation, blow it on up to a dense one before feeding it down the normal path and then reconverting the results back. So I was exploring using some of those sparse libraries. Yeah, that was something I was gonna do, but I never got to do. So what I'm doing here is actually a dense map wall. Like I intended to do a dense map wall and then it was just stupid. they, could have changed it when I, looked at it, down at the deepest level of the A 10 A level. they're just doing dense multipli. the only savings is, storage representation.

So you, should have, the fact that it's even slower than that is, is a testament to, the cost I guess, of moving back and forth between the representations. Yeah.

There are some good takeaways from this though. One thing that we were talking about a lot, or I was talking about a lot, but I never got to do, was, using the spatial pooler to create SDRs of image patches. We were always, like in all our Monte experiments, we were facing this problem head on of how do we create SDRs for real value data. and I presented some stuff in the temporal memory section where I used this coordinate encoder strategy where, I was taking real value numbers and trying to create them into SDRs. But one of the interesting things I wanted to try was using the spatial pooler to automatically do this for image patches. I never got to do that, but interesting idea. Maybe someone can do that in the future. The temporal memory stuff. So even though the code that I wrote was, not really fast, much faster compared to the c plus version, what we realized was when I tried temporal memory in the Monty project, the c plus version actually was still very slow after ingesting a lot of data. And the reason for this was twofold. One was that you're not deleting unused DTIC segments. You're just gonna create segments as they go. and so the sparse matrix just becomes exponentially bigger. And the other thing is that sparse matrix multiplication in the sleep c plus version still uses four loops to, to basically compute a map hole. and while it is relatively efficient because you're only gonna loop over the non-zero values. we're still using a for loop here. and I feel like that could be done better. So maybe the next step would be to write a c plus version without any sparse matrix multiplication at all. And I remember, and I were talking about this very briefly, but, we were brainstorming this notion where you could use pointers to basically calculate the synaptic overlap between a given SDR and a set of D segments instead of doing a sparse or dense matrix multiplication. and I feel like this is, this is pretty mission critical because if we used temporal memory, the current c plus version in any of the Monty experiments, you're really limited by how much data you can ingest. if you wanted to really use HTM to its, fullest benefits, then we would have to make you rewrite parts of this c plus algorithm. Yeah, it's, I'm trying to remember. It's a little surprising because when we first did the temple memory, I used to, Always refer to something I called, fixed resources. in brains you just can't add, segments willy-nilly. And, neurons just don't get bigger and bigger. This, this growth and things can happen, but, the number of neurons doesn't change quickly and, in most parts of the brain and, the size of the ginger trees don't see, a realistic neural model would have constraints. It says this is, we got a fixed amount of resources and you have to reallocate them as you go. I think we did a lot of modeling doing it that way, for this very reason because if you just keep adding dendritic segments, it will slow down. you can't do it forever. So I'm surp I don't remember the history here, but I'm surprised that that's, in there like that as opposed to more of a, a resource constrained network. You know what I'm saying? Yeah. So Lewis and told me the same, but those versions are probably no longer runable because they're, either written in old person, old version of c plus or the Python bindings are also very old. And right before Marcus left, he translated some version of temporal memory to c plus, which is runable. And that's this version. okay. so fine. But if we think about the future of these things, that's what we should probably be heading towards going back there. Whether it's, just 'cause it's not runable today, the problem. Is, the problem is that you can't just keep adding, resources, indefinitely. that's probably the solution to those problems. at least say very interesting ways about how does the system degrade as you, as you fill up the resources and you can't add more, and you have to trade off. and I think one of the things we found, and it made sense logically, was that the system tends to generalize more. It tends to say, things that could see as different in the beginning, and now groups 'em together and, which is a, nice failure mode in some sense. it's, not like an incorrect failure, it's just you just or unless you, over time, you lose the differentiation between specific events and things like that. anyway, the, just the, everyone should know that if we wanna work with this in the future, we ought to be, thinking about fixed resource. Also I'm a question about what's the problem with, for loops? It's, I, what's, that's maybe a stupid question, but what, why are for loops inherently bad? I think they're done sequentially. So if you wanted to compute a map, Mo you probably wouldn't wanna do it sequentially. You wanna do Oh, It's, there's nothing wrong about 'em algorithmically. It's just you, it doesn't allow for prioritization. Is that what you're saying? Yeah. Okay. Go ahead. The way it's written, it doesn't, okay, thanks. One, one of the problems is if you're going across a thing, deciding whether to multiply or not, each mispredicted if is extremely expensive. It's 30 cycles worth.

the more you can run the code sequentially, deterministically, the better off you are with, the CPU architectures.

where is that, Kevin, that, that comes up? as in, if you have a, logic statement within the for loop.

if you have the for loop itself obviously has a condition on it, but the once it goes through it first time, it's able to predict where the next instruction is at the bottom of the loop. So it's not so much the fact that there's an, if, it's the fact that if it's a random value that you can't reliably predict what the, which way the code flows. What happens is it's, if it misre predicts, it's gotta abandon a deep stack of partial results and reload the instruction cash from that. And that's what's expensive. And the time that you could, that you'd take a mispredicted, branch, you could probably do about 128, multiplies. So that's, why it's really expensive to, have mispredicted branches. In this code, there's no conditions inside the for loop. It's already somewhat optimized, but we're only gonna iterate over the non-zero indices. We have that knowledge beforehand. how do you determine which one's the non-zero, the you, add them like that. Like you always keep track of which is, which indices are non-zero. Okay. so then the other problem is then the, you're doing a gather operation. It is. yeah. So then, you have the coherency of the access to the memory being not cash coherent and that's the other side of the thing. yeah. Okay. But good, you didn't do it brain damage, but Yeah. when you mentioned the for loop, that's why I was thinking it might have been that simple. Okay. Anyway.

Okay. I presented this project before, maybe two months ago. This was temporal memory for Monty. and the whole idea here was can we use temporal memory to achieve sample in variance? basically, can I disambiguate between many objects while observing features at locations on that object surface?

This is like a, kind of like a roadmap diagram that I presented at one of the lunch meetings, maybe also two months ago. and this is like a summary of all the projects that were happening two months ago. The first was Vivian's graph learning, which was, I guess one of the more important things about that was that it's in very to scale and orientation. Karin reproduced the vector neurons from the paper, and this was, this was supposed to be in variant orientation and the temporal memory part, which you can see here in the bottom. the whole goal here was in very disassembly. if I revisit part of an object I've never seen before, can I identify what object it is, no matter where I sampled from was the main question I wanted to answer.

and this is Going back to that, that, that same old discussion, how do I turn locations and curvatures or, any real valued items into SDRs? and I use this old technique that was used before, it's called the coordinate encoder strategy, where if I wanna ha, if I want to basically turn this purple point here into an SDR, I find a neighborhood of points. I, find some pink points around that purple point. I randomly pick some of those pick points, hash their values to an SDR. I set those indices as on and I set the rest off. This is like one of the strategies for creating an SDR from real valued points. you can do the same exact strategy for, curvatures. And that's what I ended up doing.

And this is the data that I was working with. I was working with two different kinds of objects. basically one where, I uniformly, sampled this object, for training and testing using this clustering technique. where I tried to find, what are the optimal training points that I could sample from. and I did the exact same thing in another case, but instead this time the evaluation points are basically, a small parts of the object. This is what we called occluded, occluded objects. So a huge majority of the object would be occluded, and we can only see part of the points during testing. And that's like these green points that you guys see here.

so yeah, there was, I, went over this briefly in the past. I don't want to go over the specifics of the algorithm all over again. But, during inference what we really wanna do is predict the object with the highest total overlap between two sets of cells that we collect. One is the winter cells that we collected during training for these, these curvatures and coordinate pairs. And the other set is basically these predicted active cells that we collected during evaluation. and basically the sets that have the highest overlap, you sum them all together and you try to figure out what object was this testing point most likely from. and that I, presented these charts in the past. Hopefully it looks, familiar. this was one of the simple experiments that I was running for, occluded evaluating on occluded objects. Basically the question I asked was, how many testing points do I need? If I wanna get a high testing accuracy, if I only see part of the object? and in this like very simple experiment, with about, I think 13 objects, and 50 training points, if I had more than 500 points in my, non occluded part of the object, then I would get about a hundred percent testing accuracy. but there's a lot of caveats to this experiment and, that's basically what this slide describes. the locations that I was working with are globally fixed, the object moves in the environment that this method completely fails. so something that I wanted to work on next was achieving translation in variance by working with grid cells. To encode the location of the sensorimotor on the object surface at any time. and also just hit the nail on the head again about speed issues. because I was using that c plus temporal memory version, there was only so many objects I could basically ingest data from without speed completely hampering my experiments. Is it scaling up an instance, solution to speak? I know you did some parallelization code, so we just use a library instance like that. Make it viable. I don't think I've explored, parallelizing TM like that over in, over instances. I just, one instance, but a lot of, lot, of course I have no idea was all bring local, either way there's like that core problem where I'm just creating way too many segments and like Jeff was saying that I shouldn't probably be doing that. So on the last slide, before this, do you have the same chart for, this is for with occlusion. Sorry. Yeah. I this the same chart for without ocion. I do. Is it fewer samples than you? It's significantly fewer samples. It's 50 or a hundred testing points if it's not occluded. just because you're also picking like uniformly random testing points. It's like the perfect case. Okay. Even more fun stuff. so I started working on grin salad for the last two or so months. and just to recap for everyone, a good cell module is a, term I'm gonna be using a lot, and these are basically a set of cells that share the same scale and orientation, activity across multiple modules can encode unique locations. That's the gist of it. and there's two existing, implementations of grid cells. One uses an anatomically consistent version of grid cells. Basically you have a rish shaped lattice of these cells, and there's another simpler version that doesn't use these Gaussian estimation processes to determine, how the grid cell algorithm works. Instead, you have this very simple rectangular lattice, but it's actually functionally equivalent to the anatomically consistent one. So that's the one I ended up using. one of the key, so these were two existing code bases we had Yeah. When, I don't remember the rectangular one. When was that created? Who, what was that for? I believe that was the one that Marcus originally implemented. for the columns plus paper. And then in order for the figures to look more biologically relevant, he adapted it to the, Rhombus one. Oh, interesting. Because he, was a pretty stickler about these things, and I don't remember him ever talking about rectal lattice, but maybe he did. Okay.

The key thing here is that both of these work, for displacement and only two dimensions. and I wanted to do this for three dimensions. so what I ended up doing was I re recreated both of these implementations. They existed in an old version of Python two, I had to bring them over to Python three. and I was working on making this grid cell layer, we just call it L six A melon, to work in three dimensions X, Y, and Z. So what I ended up doing was actually not complicated at all. I was just creating groups of grid cell modules where each group would handle path integration along a specific dimension. as you can imagine, I would've three groups, one dimension or two dimensions along one dimension. Okay. So that's even simpler than the biological ones. So just, oh, sorry. when I say one dimension, sorry, that I should never written it like that. It's two dimensions. It's long, it's along one plane. It's along one plane, like the oh one plane. Oh, okay. Okay. So this is, this goes back to the paper that, Merkel and Marcus and others did, where you had multiple grid cell modules, 2D grid cell modules that created 3D spaces. Is that it? Yeah. Okay. More or less. More or less, yeah. you would create like these groups where each group would handle, along one specific plane. and just for argument's sake, I just provided some numbers here. if I had 10 modules for each of these groups, each with maybe about 25 cells, I would have about 750 total cells that I was, looking at. One of the key things is that only one cell per module could become active. so if you're looking at an SDR, that's an output of the squid cell groups. You would look at an SDR that has 30 active bits out of a total of seven 50 and the 10 modules they were at different scales and orientations, or were they different scales and orientations. Yeah. So see you combine the concept that we had from the frameworks and columns plus paper with, the 2D, to 3D Merkel Marcus paper. Yeah. Okay. I think these, I didn't, so just full disclaimer here. I didn't have enough time to run, like in depth detailed experiments, to try to figure out if I change the scale of this module, how would that affect my training results? Nothing like that. I really just wanted to get this code up and running as a proof of concept that something like this could work for 3D objects the way we are dealing with them. Point clouds, I also borrowed some, I guess like maybe some anatomic details from the tank paper where there was a small snippet where scales between successive module or Yeah. Scales between successive modules had this multiplier effect of 1.5, something like that. I brought that in here too. I have no idea if that, actually contributes something or not, but all this stuff is detailed pretty much in the code. Just be clear. That wasn't his idea. Yeah. Okay. that's, an old thing. That tank didn't come up with. Be clear empirical observation that was made many years ago.

Years ago. My bad. That's alright. Was he probably mentioned the tank paper, but it wasn't his idea.

Just one question, since you're operating with planes, I don't know, did you get a chance to look at, 'cause I guess if you just use two planes, that would be sufficient to specify a. A point in 3D space.

yeah, so you're saying like, I could do like X and Y and maybe like X and Z or Y and z basically. Yeah. Yeah. and, maybe you have more accuracy or something with three planes intersecting, but, but it might not be necessary. I was considering that, but I just wanted to go like full on, just have every combination of every plane possible. I think when, fair enough, if you do that, if I recall you, you end up with, there's nothing wrong with it, but you end up with, elongated field. So it's very easy to get up to elongated field as opposed to a spherical, it's like a cell. Where would a cell respond?

you'll end up with, classically, if you look at a good cell in 2D, it's respond to some circular area. but when you, limit the number of dimensions like that, you end up with these elongated fields, it still works. just point out. But they're not, they don't seem to be observed.

Okay. Yeah. Yeah. if the two planes are very closely aligned in auto agonal, then you end up with this, this sort of elongated field if I think, if I understand what you're saying. Okay. I'll try to go this, go through this, in some amount of detail. so this is like the training procedure that I ended up using. This is a picture that I borrowed from Neil's paper. where if you look at the top layer, that's the sensorimotor layer. I'm just gonna denote that as L four. That's the temporal memory layer. And bottom the location layer is denoted as L six A and that's the grid cell layer. So what the algorithm here is trying to do is not strict object classification. It's not, given this object that I have no idea what it is, even though I've trained on a bunch of those similar objects, it's not like I have to identify what that object is. It's not like that. This is instead object recall. If I put my finger or like a, a sensorimotor along one of these objects that I've seen in the past, can I identify that I'm on that object?

So the first step here, I'm confused and be, but you could classify it, right? that wasn't your goal, but once you were able to do that, you can classify it. No, it's not like a, it's not a strict classification where I can go back and, I don't wanna use the term cloud. I know what you're saying. I know what you're saying. what I'm saying is this is, there's this odd thing about all these very sparse representations, and we talked about like unique locations, which you're trying to get to unique locations in your grid cell modules. once you, every unique location is classifiable because it's unique to a particular point on a particular object, but it's unique to that object. and the same with the sensory layer. The sensory inputs are represented uniquely in the context of a location on a particular object. So all these things are potentially classifiable. You just maybe didn't do it. You didn't, you have to, learn that with temple pooling. That's, yeah, that's a much better way of saying it. Yes. Thanks. Okay. that's also how we use the graph match for classification. Graph does the same thing. You can only tell whether you're on an object you've seen before. So we can only, we basically actuate for all the object that have seen before and see whether we're on any, so you could essentially do the same thing here. I'll go next step. Yeah.

but yeah, the algorithm here was borrowed a lot from the columns plus paper where, the first thing here labeled one, I have a motor input to the grid cell layer. And this grid cell layer is doing this for three planes. Remember that? So it's doing displacement para in parallel for all three planes. I provide this motor input to the grid cell layer, and I do path integration and I get a set of active cells for this layer. at the same time, after that I basically get some location representation from the grid cell layer, and that is serving as the basal context for L four. and while that's happening, I feed in some sensory input to L four, in the name of the curvatures. So it's the same curvatures I used in the previous project. and after that is done, you basically get some kind of active representation coming from L four. and the grid cell layer is basically updated back with this new sensory, this joint sensory location representation. and one additional thing I added here was that step five is you store away this sensory associated location representation to compare against during inference. you're on mute, Jeff. so I'm sorry. So you're saying you're gonna use that representation for classification, is that right? yeah, almost. You said, you stored it for later? For inference? For inference.

I don't, is identification, come closer to what you want. Yeah. It's, more, there's a nuance here that I can't explain between like strict classification. Now classification would say it is a, an example of a cat as opposed to something specific that I've seen before. Classification would allow you to generalize, right? Yeah, I got it. I got it. But in this case, it's a distinction. I don't understand how it's, how you're making that distinction. maybe it's not important.

these, representations are for particular objects, are they not? Yes. So this would be inferring a particular object. I don't know how you'd classify it. classification says, Hey, it seems to me like, oh, this is a location on a particular object. Therefore it would be, it would be inference, not classification in that regard, but maybe inference is the wrong word here. I guess it's just during the, yeah. Alright. You're storing away. I'm just saying what you're storing is not, is a, unique location on a unique object. That is correct. Okay, fine. You, can you recognize, so if you do the same thing with the rev matching, let's say I see 10 cats and 10 dogs, and then at infant's time I run this and say, and this kind of looks like six of my dogs, but just four of my cats. So it's more likely a dog that is Could you introduce that here? That is absolutely possible. I just didn't, but I don't understand. at some sense at this point, you're, there's no effort to say these ca two cats are, the same. You're just, learning objects and you're gonna try to the algorithm will try to d see them as different things, won't it? it's classification doesn't come for free. You have to put something in there to do that. Yeah, so what you were describing Lucas, is, yeah, basically what, I was doing with the grid cell net stuff. and yeah, this is, yeah, I think more accurately described as recall because as everyone's saying, it's a specific object that you're identifying. I'm gonna go onto the next step. Now, again, I don't know if I should call this inference or not, but in the next stage when I'm trying to recall a specific object, the same training steps apply. I do the same thing. I pass a motor input, I bias my temporal memory layer. I pass in the sensory and put along with that to get some set of active cells. I use that new associated sensory location representation back to, to tune the cells in my grid cell layer. But the inference rule here is that location sensory representation is only correctly inferred if the inference location representation is a strict subset of the representations you collected during training. In other words, the representations need to have the strict overlap with the ones that I collected before.

It's pretty, yeah, it might be naive, but it's, pretty straightforward. okay. Gonna go on. This is like the data that I was working with, I was, I didn't have much time to do more than this, but I created these two different kinds of randomly generated points. The first is on the left where you can see like somewhat of a sequence of continuous paths. if you ignore the red ones, which are used for testing and only look at the green ones, I can maybe zoom in here a little bit.

If you look at the green ones, they're they're bunched together in this sort of path. So I wanted to, I wanted to mimic what a sensorimotor would be doing, if it's going on some kind of continuous path. And I have a series of these paths, all of which I see during te training and testing. and similarly, the red points are basically the same. You do the same for when you're doing an evaluation object recall phase.

this is the alternative example where I just have a sequence of uniformly distributed random points, for both training and testing. Okay. so I just ran, I ran a bunch of experiments, but I only wanted to show these two, because I guess these are like the ones that I guess you can maybe learn the most from. the first here in the top where I used. 50 pots for both training and testing, whether kind of these 10 sensations or points per path for a total of 500 sensations on an object. And I use these four random objects in my, set. and as you can see from the results, I don't have any like nice visualizations to show. All I can show you is what my, terminal put out. But, for the first object, which is object 10, it was unable to see from those random paths or random sensations or from the testing sensations is it was unable to draw a correlation with what had collected during training, but it was able to do that for the other objects within a certain number of steps or sensations. The second experiment did the exact same thing, but I used, instead of these paths, I used random sensations. And you can see how quickly you can converge upon a correct representation, because you're just covering more of the object. So the chances that you see a testing representation that has the strict overlap or strict subset with the training ones is pretty high. So you're able to converge upon a correct representation for all four objects very, quickly compared to the, the amount of time it takes for, some of those objects in training, which took like maybe 200 steps for one. And that same object took, takes one step to do here when you're looking at random sensations that, that's the first one actually worked with the paths because yeah, in the picture you showed before, the paths didn't even look similar. do you know why it worked? there's not really a lot of overlap between the green and the red. There. So there, there might not be strict overlap in the points itself, but if it's in a, like it's in a vicinity, like some neighborhood of points, then those SDRs will have overlap. So that's, why I'm, that's why I'm, that's why I think happened for the, for the uniformly sampled ones, when it shows one step. So is there like a zero step, does one step mean there's still a, like prediction step? You have two points or is that literally just given one point? It's given one point. It's given exactly one point. huh. Going back to maybe, can I do that? It you, your green and your red were both done with paths, is that right? You tr you trained on little paths and you inferred on little paths. Is that right? Correct. Correct. so then you can, it's, Yeah. So then if you have, if those paths overall don't, aren't near each other very much, then it's not gonna work. you could, they don't have to be exact same points, but the, there's gotta be points that are nearby. And so it'd be easy with a small set of these little paths that there's paths aren't near each other. Like I see some paths on the tail of the airplane. There's no, they're red, honestly, in a green one. Therefore, there's no way in the world that's gonna recognize anything back there. so you could have had randomly sampled during training and then inferred using paths, and I would think that would work much better. I'm just trying to understand this. I think that's my observation about that. That's, also valid. I could have done that. Yeah. but is the fact that you're, sometimes, the fact you're doing these short paths, it's like you're just highly under sampling the thing because you've got a whole bunch of points in one space and then a whole bunch of points in another space. And they're not evenly distributed, and therefore the intersection between the inference and the learning path, could be very low. So in my head, maybe I was wrong, but thinking, thinking it like this, but I was thinking of creating like this realistic training scenario where if I'm looking at an novel object, I'm not, running my finger to random points on that object, trying to understand it. I'm going along this continuous path, for maybe part of the object, moving to a different part, touching that in a continuous path. That's what I wanted to replicate. that's a kind of impoverished, learning strategy. reality is we would, we, we typ typically focus on areas of, interest, things that are unique and different. you typically, during learning, you would cover the entire object. I can't learn what that airplane is unless I've somehow sampled all parts of it, even briefly.

it'd be imagining I was looking at this object and I had to look through a series of little holes. I have a, my vision is restricted to a series of little holes, and the holes are pretty small, and that's all I can see. And now I then, when I'm in referring, I'm looking to a different set of little holes and, they may not be overlapping much. If, I could have learned the entire object, then my, then observing only part of it during infants would've worked better. Anyway, I, it's okay. It just, I think that was, is probably not the wisest choice for a learning strategy.

but, I at least can see why it wouldn't work very well. Those points, there are just the scope of that local feature, right? If you imagine a fat finger and you're taking swats across it, so you're seeing more course representation in the thing, there's a greater chance that you would have some kind of intersection that would if you're if you're blind and you're taking this object in your hand and rolling around in it, it's like Jeff said, you'll basically. All the interesting extensions of the object and then smooth here and stuff like that. So it's gonna be a lot denser than just I'm finding a point feature here. Yeah. So just Making more And that's brings of a good point because something we've never really tried, but we've suspected and we've written about is, we do have, in central modalities and vision and touches, you have, regions of the cortex that have different scales at which they work with. And one way to look at that is they're hierarchical. The other way is we proposed, you can look at 'em as just modeling the object in different resolutions. and so you could say, oh, I have multiple models of this airplane. One is a very fuzzy resolution, which gives me the overall. It's hard to, I can't really get details on it, but I get the overall shape because, 'cause my points are big. And then you could say, oh, and then the finer resolution region will focus on things that require finer resolution. I can learn that maybe the detail of the propeller curvature, where, but I wouldn't learn the overall, the, details of the fuselage or something like that. So there's other ways we could address these issues. We just don't wanna be too much into the fact that it didn't work well in this case.

when you sense the point, do you get any features there or Do you only get location? So if you recognize the object from just one sensation, is that just because it's a pretty unique feature or is it because you're using an absolute, coordinate system and it's only that object exists in this location? So there's no, absolute coordinate system here. The only feature you can get is the curvature. and I'm assuming that if it, in those cases where it does only work in one sensation, it's because it's something that it pretty much saw during, in doing training. but, Viviane asked a separate question, which is, are the locations absolute, you just assume the plane's in the same reference frames in both cases. Oh, it's, wait, is it a question about reference frame is, or is it a question of if it's fixed, if there's like a global fixed location, like how I did in my past temporal memory project?

Yeah, I guess both. I'm just trying to figure out how you would recognize an object from just one sensation. Okay. So it's using a fixed reference frame for sure, because if I rotated this object, it would not work. the grid cells are, yeah, there's a fixed reference frame, but at the same time it's not using a fixed object coordinate location like I was doing in the past. So if I wanted to look at one sensation, I'm pretty sure it worked in those cases because that particular sensation was something I saw pretty much similarly during training. it was like very nearby. It was a unique curvature. and yeah, there was a lot, but nearby assumes that you have this absolute reference from. It's it's saying like that you're not get, you don't have to read, you don't have to infer which grid cells, In each module. Yeah. He said it was a fixed reference frame, so Yeah. But to, to the, to Vivian's question, the feature he's looking at is curvature, not, a point coordinate. Got it. but you do have the location of the coordinate of that feature. Yeah. Presumably. But it's a, if you, miss by a little bit and you're still part of that curvature shape, then you might have a, still have a close match. So I guess it depends upon what the scale of the, curvature is that you're taking it over as to how, restricted. It is how close you have to land to it and say, oh, this is the curvature I recognize in roughly this location. Correct. That goes back to my coordinate encoder strategy that I was talking about before, to find a neighborhood that points anything within that neighborhood or that real value neighborhood, that's something you consider over overlap four. So you don't have to have an exact match. it's, yeah. I got you Still have some scope. I don't wanna beat this too much, but imagine I had only one object actually occupied the location. 0, 0 0. In these, graphs here, although the axis are not the same here, but imagine there's only one object that occupied 0, 0, 0. All the other objects didn't come close to it. Therefore, the question is, if I detected a feature at 0, 0 0 and that, even if I say that feature has to be the same feature. But that's how you could uniquely identify. You say, oh, I have this curvature here, but this is the only object that has a curvature in that location. That's how I interpreted Vivian's question.

and I think you were, I think you were doing that but that, that shouldn't be happening here if, grid cells are being used, that should be an internal reference frame. Ev, even if you may have kept the objects fixed in the environment at training inference, it should still be an internal reference frame. but then I don't, but then it, yeah, it, i, if you just have one location and you have coverage, yeah. temperature should be the same everywhere on the outside of the cup, basically. yeah, so that's the thing. I think it'd be worth, it'd be worth checking Abhi, like how big the location representation is. At inference time, because in my mind on the first sensation, you're not gonna be predicting anything in the feature layer. So all of the cells in any given mini column are gonna light up. So you should like, pretty much no matter what, be getting lots of location cells, becoming active. And so I'd be surprised if you can do it in one sensation, but maybe with efficiently small training, few objects rather. How many objects do you use? Are we, there's only four. Four?

Not many.

and did you test for translation variance?

I didn't, have a test for transmission variance, but this is transmission and variant the way I'm using grid cells.

Jeff, were you asking something? You were, I was, just making a funny comment. He said There's only four objects. Object 75 and object 45. it, I get it is funny.

So on the first sensation or first data point, how does the grid code activations get established or, so the, in the very beginning, the grid cell I anchors at a completely random point. so what do you mean by point, not point here, point in the grid cell space. You anchor somewhere randomly in that point then you hang on. Are you talking about inference or training inference, yeah. In inference, in both inference. That's not what happens at inference. Okay. How does that inference do not, do you not activate a random point in the grid space, compute displacement to that point?

No, at the start of inference, the first sensation will activate, so let's say like columns two, seven and 20 of the, or like minicolumns, those ones receive inputs. All of the sensory cells are going to become active in those, minicolumns. 'cause you don't have any prediction because you don't have a location representation. And then all of those cells will have some learned association with a grid cell. And so they're gonna, that input is gonna drive a bunch of grid cells to become active. And then on the next step, all those grid cells are gonna be updated with path integration. They predict next sensation. And then hopefully at this point is when you get the, paring down of representations. Ah, okay. That is a nuance that I might not have implemented then. yeah. Other worries. Abhi, this, you were, you had to do a lot of stuff and only a couple days on. Yeah. Don't worry. Yeah. So that would be translation and rotation and variant, right? Not rotation, just translation. it, I think it's a general question is you just don't know your location. You have to infer what the right anchoring of the grid cells, regardless of what they're representing. And you weren't Yeah, I, guess what, I'm sorry. You just, you weren't narrowing down the possible set of active grid cells, which is I think what the general idea is. Yeah. And just I guess regarding what you were saying, Phil, yeah, my mind at least, rotation in variance doesn't fall out automatically because you don't necessarily know how you're moving relative to the features and your learned representation.

so you either need to kind of test lots of possible. That you could have relative to the object or, try and infer that somehow. I see. But, translation in variance should be Yeah. automatic. Okay. yeah, so this is, I guess like the last slide. this is just based off of the experience that I was running, what I think could be interesting to do next. the first is probably some kind of intelligent action selection. We've been talking about this a lot, but it seems useful here. instead of having those naive paths that I collected during training, I could possibly do some kind of action selection to maximize the most information gain, and also probably more realistic grid self. architectures instead of the groups that I created here, it feels almost inefficient to use this many grid cell modules to reconstruct 3D space. I was talking with Viviane about this yesterday, about an idea. I think Jeff, you and Viviane were talking about this briefly, maybe unwrapping 3D meshes of objects into 2D sheets and using grid cells in standard two dimensions. and I was thinking about this maybe a bit further too, where, if that unwrapping is really challenging, you can just project it onto view spheres, which I think Niels was describing maybe 3, 3, 4 months ago. And just deterministically, unwrap the sphere, which is, that's a known, there's a known solution to that problem. and you could just, I think both, have challenges. So yeah, there's, you can unwrap the sphere, but then you have to pick your distortions and you lose angle or you use area, mapping is, it's just a known problem. It's a sphere is not a, what they call it. Composable, but there's a, there's a problem with, projection. Yeah. That's an intrinsic curvature. You cannot, yeah. I think, Viviane and I were talking about an idea that, I wouldn't say it was brief, it was pretty complicated and long, but, how you can have locally 2D you, you assume that locally in 2D everything is plainer, but, the over longer distances, it's not at all. And so how could you represent, complex, shapes using grid cell type mechanisms, but you just have to assume, there's a way, it seemed like there was a way of doing it where, you just, you can't make long distance movements. You reliably have to local movements you assume are linear or close to linear. It's a long topic. but it wasn't a simple idea I was trying to pick up was a developable surface. that's one that can be mapped to a plane without distortion. What was it, what was the words you used there, Kevin? I'm trying to make an adjective out develop. oh, okay. Developable lovable. Yeah. So that's, there, there are some surfaces that you can, unfold onto a, onto a plane without distortion. This is, yeah, this is, I think probably even more complex than that because we're trying to take any kind of surface anyway. What we were trying to do is understand how, if we just accept the fact that we don't really have, apparently don't have 3D grid cell modules. There's no, evidence that they work like grid cell modules when you're in 3D.

Then, how could this actually, how could you actually learn three dimensional objects? And, so that was the challenge we were looking at. Okay. that's it. That's it. Yeah.

Thanks. I wanna seeky pig. Where's for pig? He's supposed to come.