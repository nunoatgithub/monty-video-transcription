I just thought it would be nice given that this was, a huge effort to get this out to just kinda revisit because, yeah, it, it's definitely been a big, milestone and, it's ended up being a, big project that we worked on for a long time. But I think also, everyone's really proud of, what we've put together and rightly because yeah, our ambitions grew as, this unfolded. and the original kind of motivation was to. Essentially establish Monte these capabilities, show that to the community and kind of communi communicate that to others and what the current state of the art of a thousand brain system is. and of course there's a variety of reasons, would want to do that. But amongst other things, one of the main kind of aims of the TBP is to get people excited about and using, a thousand brain systems in this approach. And of course, showing what it's actually capable of is a big part of that, especially the academic community. but along the way, I think it was just surprising how many kind of new things that came out. So new metrics, new understandings of what Monty's kind of actually capable of, that we hadn't act, really conceptualized when we kinda first decided to write this. and also just lots of kind of fixes and general improvements to, Monty's implementation. And, many of those were definitely not, trivial.

and so it's, been a huge team effort, everyone at The Thousand Brains Project. And also, like I said, on Wednesday, a lot of the groundwork for this even started at Menta, as been saying, although we've been working on the DMC paper for maybe eight months, really this started more than three years ago, with the actual kind of, initial work on the thousand, on, kinda Monty. but, definitely special shout out to Scott and Hojae for the, huge amount of effort they've put into this. I think when you guys joined, the idea was this was gonna be like a two, maybe three month project that would be a nice way of getting to familiar with Monty's, experiments and how to run those and things like that. And, thank you for, all the effort you've put in. despite it being a actually a much bigger, project we ended up doing.

and so yeah, To rewind back to when we kinda started on this, what we had at the time was, of course, Monty, certainly a similar version to what we have now, but, but with some, as I said, bugs and things like that, that we'd later discover. And we had some benchmarks gauging approximately things like accuracy, in the context of certain amounts of noise and things like that. And we had some, visualizations, but nothing particularly, glamorous. and so when we were doing initial scoping work, I said it started, fairly simple, but then eventually grew, quite a bit ambition, and I'm not gonna spend a long time on it, but just to put into a bit of context, of the amount of work that went into, some of these figures, we had these alii draw boards where this is just a version three where you can see there's, a huge amount of, variations and comments and all this kind of stuff, going on. and this was version four. And, I think it went all the way up to, version five. but we, what we ended up with was these six, I think, really beautiful, results figures that do a really nice job of just showing how diverse Monty's capabilities are. and that's, I think what's so exciting about this is that any one of these would be an impressive result. but the fact that all of these capabilities fall out almost automatically from developing sensorimotor ai, informed by the cortex is, pretty exciting. And I, a pretty, it feels like a pretty strong, kinda signal that, what we're doing is on the right track.

and, along the way, this resulted in two, new repositories, which, are providing comprehensive kinda support for anyone who wants to replicate these experiments. that's really important from the point of view of open science. and it should also really, build trust in, in what we've done and, increase the likelihood that people build off of this. And, but yeah, these, this was a huge amount of effort when 30,000 lines of code and experiment configurations for the kind of main paper repository and then this epic repository that, Hojae put together TPP floppy, which for the first time enabled us to quantify the amount of flops, that, the amount of compute essentially, that Monty, consumes and, enable us to benchmark that n bts. but, this doesn't capture all this other stuff as, was highlighting before. even just things like replicating the experiments, not to mention, running them in the first place and running them many times, I should say. Every time we'd realize some kind of change we should make to the figure or. some bug had been identified or whatever it happened to be that kind of led to a cascade of running all this again. So a huge amount of effort.

and just a quick kind of summary of some things in terms of new evaluations and metrics and things like that. We added, better visualizations of movement sequences for the first time we properly quantified symmetry. and found a way of verifying that we looked at adversarial color robustness. we looked at the kind of stepwise effects of the model-based policy. looked at scaling of voting for the first time, and figured out tie breaking, quantified learning efficiency. And also versus VIT, we had no comparisons before at all to deep learning architectures. And that was a huge, effort, particularly on ho j's effort to, get that. Also comparing flops and quantifying that versus the VIT also looking at qu continual learning and comparing that against the VIT. Plus, outside of that stuff, we got updated professional diagrams. we did a literature review on related approaches. had a whole bunch of improvements to the configs. Just simple things like specifying a voting, system in Monty where you have multiple learning modules in a grid, was not trivial at all before. Scott made lots of nice improvements to that. we have now a comprehensive mathematical description of how Monty works, which we didn't have at all before. and notation for that and then fixed a whole bunch of bugs, which yeah, I said none of these were, trivial things. Like when we started this voting only worked with, learning modules and sensorimotor modules basically aligned in a one dimensional grid, or, aligned. We, couldn't have kind of other arrangements. something with the X percent threshold. We fixed patch off object. that was getting a huge effort on Scott's part and then semantic sensorimotor. This was a thing that was leaking into our. experiments that we didn't want to. So I'm sure I've missed some stuff. so it was, a lot of, things. and of course, I think all these metrics and evaluations that we've now developed and the pipeline for visualizing them, all this stuff is, work that we can reuse, when Monty's capabilities grow and we want to communicate that again. so it's definitely not a one-off, use case. And of course now we have this 32 page paper that we can share with everyone, and talk about when they ask what can Monty actually do. so yeah. Thanks everyone for their hard work. Yeah.