Niels Leadholm: morning, everyone. My name's Niels, and I lead the research team at the Thousand Brains Project. And today I'm going to be talking about, what Monty's capabilities are. today, why we're excited about those, and why that gives us kind of confidence that we're on the right path to developing more general, more capable AI systems. And in particular, this is a question we think a lot about, is, how do we actually know that, even if we're in an early stage of this path towards more general AI, how do we actually know that we are following the right direction, alluding to that compass that Vivian was showing before? And to understand the importance of knowing whether you're going in the right direction. I think it's worth taking a page from the story of evolution, as it were, and in particular the development of eyes. imagine that you are a simple organism in the ancient oceans of the world. And a predator like this is swimming over you. If you've developed this kind of sheet of light-sensitive cells, that's already a really useful innovation. You can notice that this predator is approaching you. And swim away. But there are two ways that you can make this even more useful. One is to form this sheet of cells into a mound. And the other one is to form it into a dimple. And these seem like subtle distinctions. And at this point, they give, actually, the same effect, which is to give you some sense of what direction the light is coming from. And so when that predator is approaching, you have a better sense of which way should run away to. But what's interesting is this kind of subtle decision at the start has profound implications later down the line for what eyes look like over millions of years of evolution. And in particular, the mound leads to a compound eye, what you will be familiar with from insects. Whereas the dimple leads to camera-like eyes, as you find in humans and other complex animals with large brains. And neither one of these kind of solutions is the right solution. They both have their own advantages, compound eyes are good for a wide field of view, and if you have a small body size, whereas the camera eye is good for high acuity vision, and particularly when powered by these large brains. But what's important is that once you go down one of these tracks, it's very difficult, almost impossible, to go back and redesign things from the ground up. Really, you're committed to this initial template in a way that constrains all future solutions you might come across. And we believe that a similar story is playing out in terms of the development of intelligent AI systems. In particular, in the 20th century, it was already realized that neurons were the core computational unit of the brain, and that there are various ways we could mathematically model them. But then in the 80s, it was already apparent that it's very difficult to get these networks of neurons to learn and do anything useful, and so this algorithm, backprop, backpropagation, was invented, as a sort of workaround to get the system to learn. And we believe that, at this point, there was a fundamental forking between biological intelligence and the AI systems that rely on this back prop, given that it's not a kind of biologically plausible form of learning. This ultimately led to what we are familiar with today, deep learning. But what we are pursuing, and what we think is important, is this alternative path, relying on, first and foremost, local learning, like Vivian mentioned, and also these other key principles, like reference frames, distributed modules, things you're going to hear more about today. And we really believe that these are the key principles that enable the sensorimotor intelligence we find in nature. And that they are going to be essential to develop a sensorimotor intelligence through artificial means. And once again. There's this issue that if you go down this path, once you are relying at, the core level of your design on these early assumptions, it's very difficult to remove the issues that come with these underlying assumptions without going all the way back to the start. And we're seeing this play out in the way that some of the key players are attempting to develop improved AI systems today, where kind of the only recourse left is just to scale systems up larger and larger. But as you've likely heard, these approaches are not sustainable from an energy or data perspective. And I'd like to ask you to imagine, by analogy of why this is a problem. Imagine what would happen if evolution tried to develop an eye that has the same acuity, the same kind of high resolution as a human eye, but based on that template of the compound eye. you don't have to imagine it, because Kirschfeld in the 70s actually calculated this and provided us with this drawing. But really, this just kind of hammers home the importance of, early on, really finding what are the key principles that are necessary to reach the end goal that you want. Another way of maybe looking at this is that, every technology has its appropriate ideal problem set. calculators are great for numerical operations. And deep learning is clearly a powerful technology with a wide variety of use cases, what we would summarize as function approximation and generative sampling. So something like AlphaFold is a great example of where deep learning can do really well. But we believe that if you want a system that can interact in open-ended, real-world, embodied settings, and all the complexity that comes with that, then you really need to embrace a different set of principles, a different set of learning principles in order to develop such systems. And that's what we're really focused on, here at the Thousand Brains Project. So that's why we think a lot about that problem. Now then the question is, what gives us some confidence that we are, in fact, on that right path? And to answer that, I'm going to show some of the capabilities that Monty has today. But really, the kind of high-level summary is that we've designed Monty really just looking at neuroscience, evidence, and thinking through, first principles of how this system should work. And what's really exciting is a whole list of capabilities have just emerged through very little effort in this system. Any one of these capabilities, I should note, would be a kind of significant finding in machine learning. for example, symmetry recognition. So the ability to realize that as you rotate an object around certain axes and certain objects, then some of these rotations are comparable. That's a very difficult thing to have emerge in a learning system without really baking in that already. But this is something that Monty, we found, just developed naturally. Continual learning, you may have heard about, is a major challenge in deep learning, and again, this is something that Monty Essentially has, naturally, just as a result of how it's designed. But importantly, we didn't set out to solve symmetry or solve continual learning. Rather, we just set out to design a system based on these first principles, and then these are the kinds of capabilities that we're just finding that it has.

There's a lot on that list, and unfortunately I don't have time to talk about all of them today. If you're interested to learn more, we have this paper that Vivian mentioned we posted online last summer, and as she said, it's just been accepted in neural computation, so I definitely encourage you to go and take a look if you're interested. But to give you a bit of context before I show some of the results, essentially Monty is looking at a series of objects. There's around 80 objects we evaluated with in a simulated setting, and these are just everyday household objects. And Monty's task is to look at these in turn, learn about them, and then it's going to be presented with an object, and it has to both recognize it, and also try to estimate, basically, the pose of the object, its orientation in space.

And so what you see here on the right is, again, just giving you a bit more of a context of, what does that look like? But on the left, this wide view is just for our benefit. This is not what Monty sees. This is just to orient you in space and realize, okay, Monty is near a red mug, and it's moving over it. But actually, what Monty sees, this is something Vivian mentioned briefly, and you'll hear more about in Scott's talk, Monty actually just gets this very narrow perceptual input. A bit like looking through a straw. But by combining this with movement, it's able to do all the amazing things that I'm going to talk about soon.

And what is the kind of first one of those? the first one is rapid learning. And humans naturally learn very quickly. You'll all be you'll know that if you look at a new object and kind of study it for a few moments, you can already develop a representation about it. You don't need to study all these images of that object, to the count of millions in order to learn a useful representation. And this is something that we've observed in Monty. So the way we can look at this, for example, is to take those 80... around 80 objects I was describing earlier, show just one view of each object, so it just sees it from a single side, and it just gets to spend a single episode studying it. And then we're going to show a novel view, an entirely, unseen rotation of the object, and we're basically going to ask Monty to try and recognize it, and again, predict the pose. And just given this one view of each of the objects, Monty can already achieve a classification accuracy of around 50. And it's worth highlighting that in this dataset of around 80 objects and many rotations, chance is just over 1. So this is a pretty, pretty significant finding. The way it does this, I won't go into it at this moment, but essentially it comes down to how Monty represents these objects as learning based on their actual shape, and also how it's transforming the inputs it's receiving based on its hypotheses.

But we can then compare to a deep learning system. That gets a comparable amount of data, and in that case, it only gets around 30 accuracy.

This is really an extremely challenging problem, as you can imagine, if you've worked with deep learning systems before, to train it, given so few views of an object. This finding holds as we extrapolate the data more, so if we give 8 views of the object, you can imagine if you were learning about this one, you would hold it in your hand, rotate it around, look at it from multiple angles. And again, we'll present novel views to Monty. And this time, we get around 90 accuracy, so approaching ceiling performance, which can... is, consistent with the idea that you are presenting Monty with the different sides of the object, such that it can build a more, full representation of it. The other thing we're showing here is, if we look at the rotation error, so how well Monty does at predicting the orientation of the object in space, it does really quite well. Only about 40 degrees of error, so lower here is better. If we compare, again, to a deep learning system that gets a comparable amount of data, the accuracy is more around 70, whereas the kind of rotation error is significantly higher, so around 110 degrees.

But it's not just that Monty learns very quickly, like humans, it also develops robust representations, and what I mean by that is that we can throw things at the task to make it harder, and Monty can still perform well. If you have some familiarity with machine learning and deep learning, you may think that object recognition is a solved problem. But the reality is that even the best systems today struggle at recognizing objects, given unusual and adversarial conditions. for example, this pink elephant is very obviously still an elephant to us. But because deep learning systems seem to have an over-reliance on things like color and texture, they can easily be thrown off by this kind of stimulus. in this case, one of the networks thought that this was a flamingo, presumably because of the color.

And in the case of Monty, we took the objects that it was seen in simulation. And we perturbed them in such a way to make it more challenging, to make it harder for Monty to recognize them. for... one way we did this was to add noise to the location where Monty thinks it is on the object, which it's constantly trying to estimate through movement. We also would rotate the objects in unusual ways that it had never seen before, and then, with the kind of last condition, we actually just changed the color outright of the object to something totally different, and importantly, something it had never seen before in the case of that object.

And so what you're seeing here is those different conditions along the bottom. Where they're becoming increasingly difficult, as you go towards the right. And then on the left-hand side, we have, in the kind of blue bars, we have the accuracy of Monty, so how well it's doing at classification. And on the right-hand side, the purple distributions, we have the rotation error. So here, we want lower is better. And what's important to highlight is that all of these perturbations that we are doing here are out of distribution, so that means that this is not something that Monty's been trained on before, that it's seen this kind of noise in the past during learning. In the case, for example, of the new color, Monty has never seen this mug in any color other than its natural one. But what's really encouraging is you see this steady performance as we ramp up the difficulty of this task. And just to give a bit of, context, again, chance performance in this setting is around 1.

It's also remarkable, given that even for humans, for example, in the case of changing the color, the particular dataset in question, some of the objects are just inherently ambiguous if you entirely remove that color information.

going back to then, the deep learning I was talking about earlier, deep learning systems, this is exactly the kind of perturbation that they struggle on, where it's out of distribution, it's not something they've been trained on before, and it, This is the kind of perturbation that leads to the sort of weird results I was showing of the pink elephant.

But it's, so now I've talked about the, rapid learning and the robust inference that Monty can do. But we also want the system to be able to act and move intelligently in the world. And one kind of... one of the encouraging things we found is that with the architecture that we've developed, it's been relatively simple to add in these more intelligent policies. So policies are how a system decides to move in the world, given the learned representations that it develops. And so what I'm showing here is something we call the hypothesis testing policy. Where in this particular case, Monty is looking at something like a spoon, or it is looking at a spoon, in this case. And it starts here on the handle, and a bit like a finger, is basically moving along the surface until it reaches this point. While it's doing that kind of sensing, it's developing some hypotheses about what it may be feeling. And by the point that it reaches this kind of bottom of the handle, its top hypotheses, its top kind of beliefs for what it might be sensing are a spoon and a fork. What it then does is to use the learned models it has for these objects to understand that, based on how they would be in the world, based on what it sensed. Where should it move to most quickly distinguish between them? And this is where it then develops this kind of goal of moving to the tip of the spoon, because it understands that's the point that differs most from the fork that it's familiar with. And so this is exactly what it does. It generates this goal to move to the head of the spoon. This is passed to the motor system, and then the motor system executes that action, and Monty moves there. And what we find then is, as Monty moves to that location, it quickly realizes that the observations it's receiving are consistent with the spoon, the evidence, the belief in the fork representation falls off. And in kind of larger experiments this leads to Monty recognizing more quickly and more robustly. A problem I mentioned earlier that is fundamental in deep learning and has been open for many years is this one of continual learning. Of being able to add new representations without forgetting about previous ones. Now, naturally, humans are continual learners. We are lifelong learners without any kind of difficulty And so it can be a bit counterintuitive why continual learning is an issue in the first place. And to in the case of deep learning, and so to make this a bit more, intuitive. I want you to imagine, how you're or observe how your brain learns. if you're looking at an object like this. Your eyes move over it, you study it, and if you were the person holding it in person, then you could interact with it physically as well.

what you would find then is, very quickly you develop a representation such that 10 seconds, 10 minutes, or tomorrow, I can ask you about this object, and you would be able to recall various details of it without issue. But in the case of deep learning, things unfortunately don't work that way. in order to learn about this new object, it's not a case of just presenting this new data, and then having the system learn. Instead, the new information needs to be interleaved or mixed with these other representations that it has come across in the past, and then it needs to retrain on all of this data. If you don't do this, then what happens is these other representations begin fading, and this is something known as catastrophic forgetting. And what's exciting about Monty is it does not show evidence of this catastrophic forgetting.

And in order to demonstrate this, we're again going to use this dataset of household objects, but we're going to change the task setting slightly. And in particular, we present an object, like the LEGO block, and then we ask Monty to try and recognize the LEGO block. But then we show another object, like the fork, and this time, we ask Monty to be able to recognize both the LEGO and the fork. We present both of these objects and see how it does. And then we continue this pattern, showing additional objects, but always seeing how the system performs on all objects it's seen so far. And we did this for both Monty, as well as a deep learning system that we compare to. So to do well, the system needs to remember the most recent object it's learned, as well as all the previous ones. And what we see here is along the x-axis, we have the number of objects that have been learned so far. And then, on the y-axis, you have the accuracy across all of those objects that have been observed up until that point. And with this VIT network, this is the deep learning network, we see exactly this catastrophic forgetting I was talking about before. Whereas a new task is introduced, the network becomes seemingly hyper-focused on this, all the weights become dedicated to learning about this new task. And it forgets about any information it's learned previously. Whereas with Monty, we have this kind of gradual, drop in performance as some of these new objects that are learned are similar to one another, but overall, it maintains a really strong accuracy throughout, even all the way up to 77 objects.

And it's worth emphasizing again, we didn't build Monty to be good at continual learning, this was just something that emerged naturally. The, last, then, capability I want to talk about is computational efficiency. So you've probably heard how the brain uses a similar amount of energy to a light bulb. And yet you've also probably heard how deep learning systems, the state-of-the-art ones today, need to be trained on massive data centers powered by nuclear plants in order to learn. So the question is, how can we get to systems more like the brain that are much more efficient? And another exciting finding is that Monty is already showing signs of this. So if we look at the compute used for learning, we quantified this by looking at flops. This is essentially the number of computations, the number of mathematical operations that are performed during learning. And I should note this is a logarithmic scale. We then compare Monty to two different deep learning systems. One is a network that's trained on the same data set as Monty. And another one has also been trained beforehand on a very large dataset from the internet. And what we find is that even in the case of the network given the smaller, the much smaller amount of data that Monty sees, we get a significant boost in the amount of computations, or reduction in the amount of computations needed by Monty, about 34,000 times less. But if we compare to the pre-trained deep learning system, the difference is even more stark, around 530 million times less compute required. The real kicker, though, is that if we look at the accuracy of these different systems, that actually Monty, again, given this small amount of training data, is actually significantly more accurate Than the other two systems.

Now, the compute numbers I was showing are maybe a bit abstract given their magnitude, so to make this a bit more concrete, if you imagine the amount of compute that Monty needed for learning is the length of a grain of rice. Then, for the deep neural network trained just on the small task, it would be the length of a shipping container. And for the deep neural network that was also trained on the internet dataset. It would be the distance between Manhattan and Los Angeles. a pretty significant difference.

Unfortunately, I don't have time to cover all of the capabilities that Monty has today and that we've explored. Definitely do check out that paper I mentioned earlier if you're interested. But just to reiterate, we're really excited about this because we've seen all of these capabilities emerge really from not that much effort on our part, once we had the kind of right design at the beginning. I think another way of looking at this is that this is really fertile ground, and so there are many exciting capabilities that aren't yet on this list, and this is exactly the kind of area that you could contribute if you're interested in working with the Thousand Brains project.

With that, I'll hand things over to Scott, one of our researchers, who will be talking about these principles that I've been talking about, and how they relate to the neocortex.