So basically the idea is that we go through, three projects. Each project gives a little presentation of a couple of slides presenting what they did. And then we, our independent Judge Terry, who might not be so independent anymore now, picks, an object that each of us needs to evaluate on. And then, each of us shows how they're set up, does on this object, and we see who actually gets it, If we have a tie, we'll have to pick another object until, we have a clear winner can go all night.

and so that's half of the, points you can get. And then the other half of the points are rated by our in independent judge Terry. Oh gosh. lot of pressure here from me. Yeah. everything rests upon you now. Oh gosh. So there are a couple of different categories. And Terry, if you feel like, uncomfortable rating on any of these, don't feel like, you can skip it. but basically you just put an X in the column of which team you think did best on that. If you think there's a tie, you can put two Xs, or just nothing if none of us did good in it.

and so the categories are best in show slash most fun presentation. Easy on the eyes. So the most beautiful setup, impressiveness of the solution, the most ambitious project, the ma most creative solution when encountering issues, team players. so a team where everyone contributed really uniquely with their skillset and you collaborated very well, best demonstration of Monty's capabilities and made you most excited about Monty's future. And then the last item is rated by all of us. So basically each of us will put one X into one of these columns and we count them up. basically you say, this team helped us the most. You can't put it into your own column, obviously.

Does that all make sense?

So it's not gonna be blind then? No. no. Okay. You worried that people will get mad at you 'cause of your judgment? No. No, I'm okay with me. So I just, can you tell me, I know that it's not a tumor is the, international team. Everything is awesome. Is whose team? Oh, that's Rami. That's the Lego team. Okay. That's the Lego team and, okay. And the other ones. Okay. Yeah. So I think the only two, the main two that I bill uncomfortable with is because I don't know enough about Monty is the two that mentioned Monty. These two? Yeah. Yeah. So with this one, you could just think of which one you feel like shows the most promise, or you feel like this might be something really cool in the future, since those are all just demos. But, okay. If you don't feel comfortable after the presentations, you can just leave the road blank. Okay. And this, spreadsheet is where, I'll send you the, link again. Link. Okay. I'll send it to you on Slack. All right. So I have to take notes. I haven't taken notes in years.

it's not that high pressure.

all So is, are all of you guys ready? do we have a volunteer for who wants to pres present first?

Does Terry have to pick someone? Oh, gosh, so much pressure here. Okay, Okay. And yeah, so with the presentations, try to keep it under 10 minutes if you can. yeah. Iris is our team name that, Scott came up with. I think it's pretty, I think we have the best name, but I know that's just me. I'm biased. C Iris is a robotic unmanned scout, by me, Scott, Jeremy. And, this is the only slide that I really want to present, and I think that everybody has put the origin of their names.

So, Iris is this Greek mythology like character, from Creat. so there's a little bit of a tie with the, it's a team of team there. so the story is that him and his father, who is like the other, the old man in this picture died of Lis they built this labyrinth in Crete to, put Mino in there for King Min. but then they were put into that jail, of the Labyrinth Maze Jail, because miners got mad that, thesis was able to defeat Mau and I don't know, become a hero. So they opened in jail, but then Diana Lewis was like this, really smart scientist and engineer that we were trying to be during this week.

thought of a plan to like, put together, glue together, the some feathers with wax so they can fly away. and unfortunately Iris, like his son got too arrogant, flew too close to the sun, and then the wax started melting and he fell down to the sea and died. But he auto landed he auto okay.

between the two men here, unfortunately our drone was more like the iris, so just like randomly die time to time. so it was a little bit hard. That was one of the technical problems. Like we would turn on the drone connected, but then just, it would just decide on its own, okay, it's done. Even though the battery's not dead, like it will just shut itself down. I guess the name is somewhat fitting in that sense. So the project goal for us is, despite the, kind of, finicky drone, the goal was to put Monty inside there. So there are some unique challenges for the drone team. or the starting point for a drone team is the camera, that is attached to the drone. So we are implementing a distant agent. So in Monty that means the, we're, look, the agent or the drone itself is, not basically not on the surface of the object. and. With this. so we get this, like RGB, normal picture, from which we had to do, normal, novel things like depth estimation. I guess I wanted had to do it in some sense. But, depth estimation, object segmentation, and projecting that to 3D space. the ideal goal was to learn, all the real world YCB objects that, I think mostly meals ordered from Amazon. So this was like the, spam can that you see here, the, there's some hot sauces, new cup, mustard bottle, et cetera. and to perform inference on it. So our general plan, it says current status, but like our general plan on how we approach this problem, was split it up into two different kind of areas. So one is the input area, which is the data loader, and the other is the output area, which is the motor system. So in, in the data loader section. so they, again, the plan, was the general step is to, okay, take a picture with a drone. that picture, is like a really, large, I guess it has the whole view of the, of the object. So for Monty, we need to take little patches of it. So that's what patch five means. for the, each of those patches, we need to be able to estimate the depth. get the, semantic label. So that means okay, in the picture, if, wherever in the pixel there's an object, we put a one. And then wherever it's background, we'll put a zero. That's, that's what semantic label here means. And then using, the above depth information and semantic label to be able to project to 3D coordinate. there were other challenges that we had to do called CPU to actually, implement this. So one was keeping drone alive, I don't know what's a better name than this. It tells you what it is. Keep drone alive. we had to, we worked something calibrate, calibrating depth. We worked on some segmentation, something called the very good detection. So I'm not gonna go into each of the in detail because it can get quickly technical. do you wanna comment on the motor systems, Scott? sure. I guess I'll just say that, knowing the location of the drone and knowing the depth of the stuff it's looking at are obviously very crucial pieces of information for Monty. And these were real challenges that we kept trying to pair back. Like the rotating spam here is, at one point we just kinda realized we weren't gonna be able to control the drone. In real time and do what we wanted. And so this is just one of our like, simulated setups that you're looking at. So as fairly early on, we abandoned, realtime drone control. Yeah. And I, will, we'll show you the realtime drone control in our demo. 'cause I think that's the only thing we can show. and you guys can see like how the drone motor system works, but here anyways, like that, this animation is basically simulating Okay. if the drone is flying in a circle around the object, like what is the kind of image that it would potentially take? So this was, that image is, just because we like produced this ground truth data set and I just, wanted to make sure that the, coordinate systems were as expected based on, the geometry and making sure everything was aligned. But that's all that is. Okay. I have a request for Will, is to make this gif into a official TP thing. and then I wanna speed it up so that, like, whenever I'm thinking instead of my brain wheels turning, I'm gonna say my spam wheel is turning. Okay. So keep that in mind. I don't know if we can make this like a slack, sticker or look like, what is it one of those like Yeah. Emojis. Yeah. I'll remake it if we, wanna go official. Okay. All right. Yeah. But, anyways, that, yeah, I want my spam wheels be turning for brainstorming. That's, my thing. Okay.

depth estimation. So I think we have pretty similar challenges as like the Monty meets world from last year or two years ago. I don't know when it happened, sorry. but in this case, I think they had like a true deaf camera from iPad or some like other way to estimate depth, in our drone, like it's not a fancy drone, like the Microsoft Connect, where you can actually get the drone data by lidar. It's just RGB image. So we have to draw our own depth, depth estimation. thankfully we just, we were able to use like a deep learning model called deaf B two, from Nvidia to, get a DEF map. And here's some example, like outputs, RGB image and, the def map. So the lower volume means it's closer. in this like image, the camera is pointing slightly downward. That's another thing that we notice. If the drone is flat, the camera is a little bit tilted downwards, which is weird. but that's like fixing into the drill. So there's nothing much we can do about it. and then anything like in the yellow is like far in the background. we did that also for this kind of, makeshift box that we made to calibrate dust. here, like the depth value that you get is like random value. So the lower is closer, higher is farther, but doesn't really mean anything in meters. Anything like that. So the rg so we had, we did a calibration where we put a box in, like a known, known distance, and then we can pick a point on like how far that away that is. yeah, so that box is slanted away from you. So there's a bunch of known points there. So we just fit known, locations against the estimated locations so that we could try to, it convert, remap it into actual, like meters basically. Yeah. Because it comes out and, but it turned out that it worked really well in this particular example of a single box, but I, when we try to apply it to other, we took that fit and applied it to like a spam can that you, everything came back weird. So there isn't really one set of parameters that is going to invert that. So I have a feeling that the depth map might be setting back, like relative differences between things in the scene. And it's not really something that you can just. Undo, yeah, so try to get units. but anyways, yeah, there's multiple challenges, but yeah, many things. and then another thing was segmentation. this two I graciously taken from the Iman meets world. I trying to see like how they did their, like depth, like objects mentation. So in our current Monty we have this kind of idealized simulated world where the object just floats in space with nothing around it. Like it's literally object floats in, with nothing. Yeah. So it's, very like non real world. There's like gravity is turned off. we can see from the bottom of the object. In real life we can't do that 'cause it's sitting on a table, Or on the floor.

and because the def map that we're getting has like very, it's not one flat def map. If I try to send like dry shoulder, then some parts will be like, the floor would sometimes be included, while not all of the cups is included and whatnot. again, end, we just, implemented an alternative way to segment the object, using, again, another deep learning model called Segment anything model Sam. and then we, so we use this kind of information to cleanly take out the it's not perfect 'cause it's missing some of them up. But clean as, as cleanly as possible, take out the object, from the scene. So this way, if we segment out the object, then we can remove all the background, we can remove the table and put it into this, ideal world and, our Monty world where there's just a clean object floating in space. Do you have to choose which object to segment out of the, like you have to say?

yeah. So because our object, I assume that the object was at the center of the ob center of the picture has the object. and then in other cases I do founding boxes. You might have seen that in the, on the slack. to, yeah. this one I'll, go over quickly, but there, there's basically some tricks in computer vision to, again, extract out the, real world coordinates with respect to some kind of, a landmark. So there's this QR code like landmark. We can say that the middle is like 0, 0, 0, which means that then we can, 0 0 0 as in X equal zero, Y equal zero, Z, equal zero. and using that information we can theoretically get, like the actual position of the camera, actual positions of the, Of the ground truth object. and same for post and rot, the, post or rotation? Yeah. the main thing here was the, when we were flying the drone didn't know where it was in space. So the idea was to look at this code if we knew the position of the cube and the orientation of the cube. And then, the drone could know its position in orientation relative to that, QR code basically. Yeah. And if you look at the values, they're actually quite sensible. So it says, so this is all in meter, so this is like just eight mil. Let's see, yeah, 0.8 centimeters. yeah, you can see that it's pretty, like looking at the marker dead on. So the X and Y values are pretty close to zero while the Z is, 20 centimeters or 19.1 centimeters behind. And that's about the location that we put the drone in. So it's, it's pretty good at accurate, estimation of the, position and also the rotation. So you can see that there's a little bit of like tilting like, with role pitch and then the jaw is gonna flipped over negative 80. But, I think, if we need like real world things, sorry, then, this is a pretty good way.

and, I think will came up with this idea. So thank you. Will, I didn't know about this until this hackathon.

so anyways, with all the depth information and the like, the segmentation, so we are able to, put it into kind of 3D points. all, these images are I mean they look like just pictures, but that they're actually very densely sampled points. So individual there, there's hundred thousands of points here that make up this object and you can clearly, most clearly see it in, in this picture where you can see some stray like points coming out.

because we took the spam picture in like multiple different, in 12 different, locations around the circle of the object, we can try, we tried to, project it, project each of the images into a 3D, 3D point. So this is the result of our depth to 3D and everything.

and, so individual pictures, they turned out okay. You can see that it's quite like sensitive to like sweet parameters. It can look quite distorted even if the picture is okay. so this was like one of the more I guess tricky parts. and then I think trying to put all these three coordinates into the same space was the most tricky.

Actually, Scott, do you wanna explain like this? sure, yeah. So like on the left there is with the, so once, once we calibrated the depth, we had a mapping from the values that depth Anthony gave us to real world based on the calibration we're on the left is what the point clause looked like after calibration. It essentially kept thinking everything was super close on the side of the screen. and so after that I thought, maybe if we just adjust the distribution for each point cloud to move it towards, so its center is near 20 centimeters, which is what we knew we were actually from. So we'll just push everything back and see if that helps. there were just like so many things that we just threw at the wall to try to see if we could get these things into a reasonable, 3D space. I think the one on the right Yeah. Is from the non calibrated. Yeah. Just broad depth. no, it's from, real depth, but the i from the less precise estimation, which turned out to be better. Yeah, the, yeah. anyway, and you can see that, like you can see individual spam kind of pictures that you would've taken, like this would've taken, this image here would've been like looking straight at it. This is at a slight angle. This is slight angle. but we're somehow putting it in a way so that we're modeling inside out. Like you can see that, like ideally you'll form a circle, right? Instead of this kind of. Gon gun shape. but this is as far as we get in a week. Yeah. yeah.

Then, there, there's many lessons learned and just didn't put them all here. Yeah, I would, I guess my, I, think there's just a lot to break off on the, getting going side on the drone project. I think probably just like starting with something that's already got some better ability to control the drone, know where it's located in space, have good, reliable sensorimotor data is probably a better, starting point for, just in a week. we didn't, we were so close to being able to get Monty to push data through Monty, but to be honest, and we wrote the data loader and everything, it's like ready to go. Yeah. It was just this step of we've got this data that we can't figure out how to get into the same space reasonably as what center, pre-trained model, like the, at a minimum the units in the scale are gonna be completely different.

but definitely I'm not saying that, but I think this is a very. Doable and not, maybe there's just not in a week. but like mon could definitely live in a drone. if you got a little bit, better drone, then we can actually get some realtime feedback that, that would be also good. 'cause, one of the other issue was that we had to connect to the drone, we have to give up on the house wifi, and just, we take a picture, but then we realize, oh, the drone is actually tilted, or it's too far away, it's too close, or it's, so like we have to do a lot of physical manipulation to take good pictures like that. So I know that sub animation like here is it is cool because Scott and I like measure the exact distances and the angles, to get this, to get it as precise as possible. But thankfully we have some wet lab experience, so this was Okay. so this was like overall, like very, informative. I, I mean I, I definitely learned a lot about depth to 3D. I didn't get to play around with the motor side still, but, at least on the data loader and a little bit of the sensory module, I think this was very informative. very cool, very fun project. I just wish that we would have, been able to jumpstart getting the data into 3D space a little quicker so that we could start, Yeah. And Jeremy actually worked on a lot more of the motor side to keep a drone alive and move in circles. it's just that it never re. We don't have any outputs for those because like we never got, like the data to Monty and to emit a motor signal, oh, move up and down. if it were to say oh move up then like Jeremy class would, we'll be able to control the drone that way. so if we don't have to return the drones, maybe like Scott and I will take one each back and work on it a little bit. 'cause I think we are super, super close. So I think I start with the different Yeah, it's really cool and yeah, it kinda like you guys are saying, it sounds like there's a lot of pieces that are just like super close and maybe if Yeah. Had the hackathon had just been like a few days longer. 'cause yeah, I think you guys definitely had a challenging one with Yeah. All the different pieces and stuff. Yeah. And yeah, I think that was also the, at least what we imagined, that people spent most of the time on during the hackathon will be like the robotic side. assembling the robot, tracking the locations and everything is definitely like the hardest part. yeah and it's the kind of stuff that it's good. We're not trying to do like normally 365 days a year, but it's probably, it's useful to dip into it I think for a little bit and just yeah, appreciate the challenges, appreciate yeah, what assumptions we're relying on when we are working in with simulations and things like that. Yeah. for sure. Cool. Thank you. Yeah. were those all your slides? yeah. Yeah, So yield the floor to the next team. wait a minute. I have a couple questions. Could you explain to me, the, your explanation of what, how this demonstrates Monty's ca capabilities and also h how, does this PR show for Monty's future in your words? Okay. so in terms of cap, you wanna, I guess well look a bird. Yeah. Oh my God.

it, it's hard to say, we got a lot of experience with Monty in prepping to, to lead it, up to it. So I learned a lot about how we might be able to make the process easier for jump starting a new project, but we, of all the teams, we're not gonna be the ones that give you a really good answer on the future of Monty because we didn't, we just didn't really get that far along to, where Monty was really big part of the equation. so at 2:00 AM within a week, yeah, in a week. But I think, in terms of like real world application, probably among the three of us. Drone is probably the most accessible for the public. So that means that, if we, we can put Monty into the drone, we just haven't gotten there in a week, but that means that, if somebody had like a flying agent of some sort, then we can readily apply Monty so that it can, do object detection and pose estimation, in, in the wild. yeah. I guess that's about the Monty's like futures, I think demonstrating Monty capabilities. I think this, we're not, we haven't, extended any capabilities. We're just using the existing like graph, learning modules and, so I do think, we could have, in terms of the future of Monty, we could all get together afterwards and talk about what kinds of like architecture we might be able to add to make some thing like pre-processing things more plug, plug inable. yeah. on the, on that end. yeah, we've got a lot of user experience and like what if somebody wants to adopt Monty into their project, like what kind of challenges they might face, so to, to make that easier for other people as well. So I think we yeah, least gain a lot of insights of that for sure. All right.

But if, but what, with the technology that is already exists, how would Monty imp improve that? it seems to me like there's drones out there that can pretty much, are they specifically just done on location or can they actually, are they drones now? Can they, recognize what they're hitting? is there any kind of visual ization or is it all based on, lo location? yeah, they'll, so once, we have the images, then yes, Monty will be able to recognize like, different objects that exist in, in the scene. So right now, like it's just a spam can, theoretically a drone can fly around and put like boxes around, oh, like I see a spam, can I see, mustard, bottle, yada, yada. I guess in that sense, it's similar to like, some drones, like if you do like just pure deep learning way, like I've seen like some demos where it's like trying to do object tracking, for example. it is at that level, but it, we're doing it in the, I guess the sensorimotor way instead of deep learning way. Yeah, I think that the main point that it would show is that Monty is a sensorimotor system. So that was one of the ideas that for this drone, drone project, that the drone can intelligently move around the object. It's not just like a static image that it does image recognition on, but it like uses, its learned models to move in the world in an intelligent way. And so if it tries to find a specific object or tries to recognize an object, move specifically to where it needs to, collect, the information that needs to determine object ID and pose. and yeah, it's not like a static image recognition system.

So do, right now, do we only have, I mean that you know of, is that the only things that drones can do is static rec recognition and not anything else?

It's like a, I think in general it's a mixture, either. Yeah, it would be more kind of static, or, using single images that are coming in. Or there's some drones that might be specifically designed to build a particular 3D model, let's say a 3D model of a building that is being surveyed or something like that. But right now you wouldn't go out and use Monty instead of that. But the whole kind of idea of how Monty builds representations that become more complex and more abstract. Means that, with a full Monty system, it would be able to understand the environment in a much more intelligent way that no drone can, right now. Because there just isn't a, an AI system that can understand the environment like a human does. And it can like inter continuously learn on the fly. Like it can go into a disaster zone and see objects and configurations. It's, it has never seen it before in the training distribution. And learn those really quickly and then recognize them or feedback to the user what it's seeing or like where trees are blocking a road or something like that. Okay. I think we may have taken up too much time, so move on. Maybe that's okay. Yeah. All right. I will stop sharing and, okay.

I'll plug Hojae for creative solutions. 'cause that a ruco idea was really nice, really elegant. The idea of figuring out where the Jonas's position in space based on things like your codes. Really nice. I'll toss that a compliment to Will because it was his idea first, Okay. And it's, actually quickly ask about that. So could you calibrate the depth with that? Because if you knew that was 20 centimeters away and it was in the depth image, could you just say any pixel that's at that point or is the issue? Yes. Okay. Yes. Yeah, that would be the most ideal. but we did the slanted book thing so that we can get different depth. you, okay, yeah. You do multiple of those codes or, yes. Yes. I printed out four and put it on four sides of a cube. but this, it's, or like even in different planes, yeah, that's fine too. I, it technically I can put it in all six size and each of those, actually, the more is better because that they, each of them will give you some kind of estimate and then you can take the average of those estimates to get a more precise pose. one of the reasons why I like the, it's, we're not getting like perfect numbers is because, I'm just estimating from one marker scene. Actually you can, detect multiple markers like that. Actually, that would be more ideal. and then aggregate information from there. yeah, maybe we have a follow up meetings and time to go into the details and just Oh, yeah. do the high level Yeah. Conversations for now. Otherwise, we'll take a long time with the demos. okay. Yeah. Lego Team, do you wanna go next? All right. And yeah, try to keep it under 10 minutes if you can I share my screen? One second.

Do you have music in this presentation? no. We should have added the, oh, you should. The background track. Okay, everybody see this? All right, hi, we're team. Everything is awesome, Rami and myself.

there we go. Explain the team's origin please. so in the beginning, we got some Legos. we bought a bunch of stuff, started assembling it into a platform, and then this is the platform actually running. So this is our, robot assembly, so that's one of our motors and our part of our motor system. And then we were calibrating it very scientifically of seeing how fast the thing can go unfor you. So we calibrated it well. then we're also, so the other part of the robot is a sensorimotor module. This is the elevator that goes up and down. And in the background there, you can see the, this is the sensorimotor with the camera and the depth with the RGB camera on it and the depth camera on it. and that's what the robot looks like. Combined.

We did a bunch of debugging. This is an example of, debugging the depth sensorimotor. there's interest, like you can see different depth resolution versus RGB resolution. So every dark point is an RGB point, and the depth sensors were like more pixelated, worse. Some of the lessons learned, early on we were just trying to remind each other not to eat the data set. This is since some of it was edible and it was in the kitchen, so we just finally separated out of the rest of the food that we planned on eating. the other, another lesson learned was it was really great, I think everybody was mentioning this, building something with Monty end to end, so that we had to write and understand every single part of the Monty system. Like I know Ramy and I are way more familiar with how the whole system works now. of course, IANS we, I understood, TER for a little bit. so that was great. And I think I still remember them. There's a resource, that really helped, with the intuition. some of the Monty framework thing, things, there's some ideas for action refactor that I wanna do after we're done. noticing too many resets happening before an episode. if you're running a simulation, you never notice it, but if you have a robot that's like, why is it doing the resets so many times before it starts? So that was a neat finding. Yeah, we ran into the same problem, spent way too much time on it, and then decided that we'll just always have to press the button twice before we start the experiment. We had to do the same, like we had to take two pictures that, because the first picture would always be black for whatever reason. Nice. I removed the extra reset, so that was my, because our, coding approach was to, we just forked the entire Monty repository and we just hacked whatever we needed to. So that, that was convenient. That was also a convenient approach to the hackathon for us. Yeah. picking coordinates that make the problem easy. As you saw, our robot has a central platform that rotates and a thing that goes up and down. So we're like, our units are the robot radius and we are on the unit circle, and that simplified a lot of things immediately. so that was super helpful. Next on debugging and visualizations. Yeah. I think to me, I feel like having a, very good, this is what I figured out early on, that if you have very good visualizations and a bugging that will go, a long way in, in something like this. so yeah, we started working on some, nice visualizations. You'll see that in the next, few slides, but, basically we are able to see the sensors positions live as it's basically making movements and basically also what like the current MLH is or like what Monty thinks is happening and what the rotations are and all of that stuff. we had some, trouble with, the coordinate systems. So we, we started using, so for the get state of the agent so that we know where the agent is. We're using a different coordinate system that wasn't Monty and we figured, it will still work fine. but then the, it does not plug well with the depth to 3D. So we ended up changing all of my, our computations for the, get state of the agent. And that's where, learning the co also that's where we started looking into cos too. the, and then we tried to do model scaling. basically we're using different, like different, a different scale, than what is in the dataset that was provided to us before. So that did not work very well, and I'm not really, sure why, but, so we, have, we added some code into the grid object model that will basically, you can just say, scale this model by this scale factor and we'll just scale the mesh and recalculate the KT three and everything. but that still did not help very well. what we ended up doing is that we basically trained our models, but then the models that we trained are, very sparse. they may not work very well, but the idea is that if we train a dense model, it'll work, better. but you'll see that we still have some performance, A reasonable performance. and early on we, like in the first day, we had some, a lot of networking issues. And, it basically, the reason is they have multiple sources of, like they have multiple routers, and if you have the different clients are connected to different routers, they'll not be able to talk to each other. So what we ended up doing is that we needed a router, but then, we could not make that router connect to the internet except through a hacky solution where we plug it into a wifi extender and that wifi extender gives the internet to the router, and then we can connect to the router.

but the, the cool thing about what we built is that we could actually like decouple the simulator from, Monty itself. So we, what we tried is, running two different mons on the same hardware. So basically, Tristan, and I, we were running experiments at the same time and we were fighting for the hardware, which was very cool to see. Basically like the, sensors moving, like I, I give it a command to move somewhere and then Tristan just moves it somewhere else, and then we start arguing about, and need the resources and, all of that. yeah, and a lot more stuff that we, the death camera was like giving us a lot of trouble. yeah, too much troubleshooting and debugging that went there. And yeah, like I said, debugging setup goes a long way. yeah. Okay, we did some, so with troubles, with changing the scale of the existing dataset, we ended up training on the dataset ourselves. So if you don't recognize it, but that is a cup. All right. And so we, this is training sped up 40 times. So on the left you see the visualization being built and on the right you see the robot, using a, training policy was essentially orbit around the object from the bottom up and just scan it. And so that was the, that's the idea behind that.

this is what, the final scan for the cup look like. So it does end up looking like a cup. It looks like a cup, but the handle is, yeah, I think we, we have some trouble with the depth information and maybe the focal point of the, like the, field of view and all of that stuff. So it, looked, it's a perfect circle and everything, but the, the handle is a little, detached from, the cup. and this is why this, oh, I see. Our evaluation works fine with the trained models. but then maybe this is why scaling the meshes didn't work very well because, they're different, during inference we can see that, so this is, a different kind of visualization where it's basically, I'm plotting the current MLH. in, in real time. it's basically if you can figure out the rotation, the rotation, this little.is a sensorimotor, basically, this is where thinks the sensorimotor is compared to where the mug is. and it was able to figure out that the cup was, right side up, but it could not figure out the rotation, like basically the rotation on the y axis or like where the handle is.

but it could separate it from all the other objects in the dataset. that's also good. and we were using, and essentially we use based policy, just random actions. we had to rewrite it a little bit to, you use our actions, but it was, but as you can see in the video, those actions are just purely random, the up, down and ro and rotating around the object. But the sensor's always guaranteed to be on the object in these cases. No, How do you move back? It is random. It's built in. Yeah. Yeah. So if you're off object, how do, how does it know that it's off object, I guess is my question? there's a, the, transform, right? So based on the depth, it knows that the depth see, yeah. So the depth is like too far, then you're not an object. Okay. So we go through the building, the semantic, labels based on the depth. Okay. I have more questions. Okay. We can get into, is this the time for live demo or do they come after all the presentations?

maybe we wait until after the presentations and we pick one object that we all demo on. Okay. Sounds good. that's it. Nice. Great job. Looks really cool. Yeah. So can I ask those same two questions with regards to Yeah, could you ask them again? Yeah, I gotta bring 'em up. Okay. how did your project, demonstrate Monty's capabilities and, how does, what, how do you see Monty, what the application you're doing, for the future? Yeah, demonstrating Monty's capabilities, I think this would count as, the, probably, I don't know, I'm not sure, but this is probably the first system that has, actual movements in the world, basically instead of, like circadian over an image. So it is, not like we're taking an image and then we're moving a patch over the image. it's actually the patch itself is moving through the world, which is in my opinion, is very different from, what we had before. so it's demonstrating a different side of Monty's capabilities with an actual, motor basically. Is that, that this thing works. As a sensorimotor. Okay. Yeah. Can I throw in there, so just one point about the patch, like in the previous presentations you saw like a full image of every, being taken. Literally, Monty only sees those dots. there's nothing else, there's no image of the cup, there's nothing. Monty learns just those dots on the left. And based on that, it learns to recognize objects. There's no full picture given to Monty at any point. Go ahead. Yeah.

and then in terms of, future, capabilities, I think that, so this is a first step, in my opinion. There's, just like with the drone team as well, there would be, opportunities there to explore boating, which I think is very important. it's easy. Now, we're building this with Lego, so it's easy. Like we could, add a different sensorimotor and add that through a different angle, and then we can just move it, we can move it independently or we can move them together as, part of one agent, so they can move, like two sensorimotor patches on a finger together. Or they can move as like different hands and then they like look at different parts of the, of the mug. And so basically it's very, extendable, the way that we set it up. so yeah, if that answers your question, I'm not sure. Yeah, I think it's this, it's a great point. It's the very first time that Monty is working with a motor in the real world. It's a, huge thing. And then I guess just for us, it'll be a great setup to have to test Monty in the real world. Right now, we basically, everything that's a bit more complex, anything that involves movement, we have to test in simulation. So now we have a test bed in the real world, and we could give it any kind of object. We don't have to create like a simulated object data set or anything. We can pick something from the kitchen and put it on the platform. See what Mon Monty does. Yeah. Yeah. it, yeah, it also scans very nicely. it, it scans very fast, but then, the, it's still sparse, compared to what, I would expect. Like I looked at the measures that, you gave us, and they're very nice, but like very dense. So this is sparse, but the fact that it still works even with those sparse models is, mind blowing to me. So does it work, at any speed? Does, if the, if you slow down the motor or speed it up, does it still have the same results? slowing it down is always easier. speeding it up. We are, we're basically, we're running at a, at a high speed actually. And, but we're limited by how, like how far, how, how much time it takes to transfer the image from like wirelessly, from the sensors to the computer. And process them. I, this is actually the bottleneck with one of the challenges was I was sending a bigger, so that to be able to stitch the, depth camera with the RGBI was sending both like bigger images from the Raspberry Pi, from the, from Monty, from the, robot to the computer. and then stitching those. But then, we found out that if we just send the patches and do the competitions for which patch we want, which is basically sends smaller images across the network, it goes, much faster. so Monty doesn't have a problem. Actually, Monty is much faster than, the bottleneck of sending, okay. Images. So yeah, it works fine with high speeds, Short answer. And yet to Monty it doesn't matter how fast you move or in what pattern you move over the object, it's not like you need to have seen that pattern before during training. You can generalize to any speed of movement as long as what Rami says, the transfers fast enough. Yeah. And you can increase.

You're muted. Muted. And we can have, and you can speed up the inference, by having Monty have multiple sensors. So I have thousand sensors looking at something. You can recognize it instantaneously rather than having to move or you can move it once. so yeah, you can, once we put this in hardware, this could go very, fast. And another thing about the future demonstrating future capabilities is we have zero precise movement. So when the elevator goes up, we ask it to go a certain distance, but it goes whatever distance it manages. there's minimal distance when it rotates, we're like, rotate by seven degrees, it's gonna do whatever it wants. but we, but as a result, we, but we are able to, work on proprioception and make sure that Monty was still processing where the thing actually was. So there's no, so what we also demonstrating is you don't need any sort of precise movement as long as you figure out where you end up. And the policies still seem to work Okay. And still infer and train objects, even with imprecise motor control, which is a new demonstration outside of simulation. Yeah. Actually there were some times where the, the ro the, rotation robot, that one that rotates the platform, it was not even moving. Like it, it tries to move, but it looks like it doesn't have enough power or we're just doing small steps. but the, basically because we're using pro price entry, set, we're getting the, position of the agent, it doesn't matter. So basically it would just add. Points at that same location or depending on how the, object is built doesn't need to add these points. So the lesson learned is that, just being able to get an accurate, position of the agent is more important than moving the agents, moving the, agent somewhere or, or trying to move it to somewhere accurately. It doesn't matter as long you're, as you're able to get the position accurately.

that was helpful.

Yeah. Awesome. Looks great.

That's how they got their name. Yeah, everything's awesome. That was Tristan's idea.

all oh, then I guess it's our turn. All right.

How do I start the presentation? Sure.

Will I think this, yeah, I came up with the team name. this is where the team name came from, the movie Kindergarten cup.

What's the matter? I'll have a headache. It might be a tumor. It's not a tumor. Not a tumor at all. At all.

and so we are the ultrasound team. I'm Will Viviane, who you know, and Niels. and so we are scanning things inside a bag, that are not medically related. So that's why we call this not a tumor plan. Again, let's keep better this.

Okay. so this is the ultrasound that we've been using. we got this on loan from butterfly ultrasound. so this is it in my hand for scale. and on the right is an actual, ultrasound at the top. You can see the bit of the probe. and then down here you can see something that's definitely not a tumor. This is the cross section of a, French's mustard bottle. So that's, that's your primer for what ultrasounds look like. Good luck. You weren't able to tell. It's pretty obvious. Yeah, it's pretty obvious, really. Come on.

Yeah. And what's exciting about ultrasound is obviously it has a huge number of applications, medical being one of the main ones, and it's an inherently a very sensorimotor, way of clinicians understanding, what they're seeing. and it's very challenging to use AI for this because in the medical field you generally don't have large data sets. There's lots of reasons that Monty would be perfect for this kind of use case. but ultrasound is not a very intuitive, modality, at least if you're first dealing with it. this is for example, what, the Thousand Brains Project Mug. Looks like in, ultrasound or part of it, rather. and what you need to bear in mind is that ultrasound, the probe is producing a two dimensional beam like a fan. but that two dimensional fan is moving in three dimensional space, going through, a structure. And as the name implies, the probe is emitting, sound waves, ultrasonic sound waves, and basically the time it takes, for that to bounce back tells you how far away something is. And the intensity tells you how, white that is. So something like bone or metal, or ceramic like we were dealing with tends to show up is really intense, really white. whereas kind of soft tissue and things like that appears, more gray. and and then water would just generally appear like black. and so that can give you a huge amount of information, but it is challenging to work with. for example, the TBP mug, as I was saying, this is an image of it. And in particular it's an image, a 2D plane going through the side of, the mug and side of the handle. And so what you're seeing here, for example, this is the top of the cross section of the handle. If you can imagine you had a very sharp saw that went straight through here. This would be what the top of that would look like. And this would be the top of the. Side of the mug, but you don't necessarily see everything you would expect because, for example, you get what are called acoustic shadows, where the sound, cannot continue to propagate. And so it's black behind this, edge. and you also get lots of artifacts. So these can be caused by various reasons, if sound wave gets delayed, if it gets reflected multiple times, all this kind of stuff, you can basically, the image kind of hallucinates, as far as the probe is concerned, it thinks there's objects out there in the world. And so this is, these were some of the things we had to deal with when, when interpreting these images. Yeah, so it also turned out to be quite a lot of moving parts to setting this up. So I'm just gonna talk you through the architecture diagram really quickly. so over on the left we have the ultrasound device to it. We strapped an HTC Vive location controller, with just a piece of Velcro. when you click the button on the probe, it captures what the probe is currently looking at and sends it to Monty with some met metadata about the depth, the scale that we were looking at on the image. At the same time, the HT C Vive controller is streaming its location to, two, two sensors on the floor, which then send them to not pictured here 'cause it was even more complicated, which is then streamed to the HTV VI service where it stores the current orientation and three dimensional location of the probe. as Monty processes it, gets that image and then it asks the Vive service for the data. And then, it visualizes that over here so we can see what we're actually looking at. it will also suggest, where you should move to next. So Monty can't move us, but it could tell us where the next best view to get the next best inference would be from. and then also, and visualization of, how Monty is doing what it's learning as we go. And so you'll see all of this in the demo. We wanna just give you a quick overview.

Yeah. And so one thing just to give you a bit of a sense of, what the whole thing looks like. So we had what was called a phantom. So phantom is when you're testing ultrasound or someone's learning how to use ultrasound. And so you have an artificial structure, that can be scanned. might have an obstetric phantom. But since we were dealing with objects that Monty has learned in kind of simulation like the YCB objects, we created this setup with essentially a plastic bag filled with water. you can imagine it's something like an amniotic sack with objects suspended inside of it, and then all of this kind of set up with some tripods so that we can, kind move around it. And then, oh, sorry. We're basically putting the probe on the surface of this, bag with some, ultrasound gel. so that it gets a consistent, propagation of the signal. and of course, the task is to determine what object is inside the bag. and as Will mentioned, we were tracking the position of the probe. The way we did this was with these, What are called kind of base stations. These are actually designed for virtual reality gaming. And so people have already spent a lot of time engineering a good solution to this. And basically this tracker, it actually is, look, this tracker, looks fairly what do you call it? Static, like it's not doing much, but it's actually looking for, invisible lasers that are coming out of these two base stations on either side of it. And that tells where it is, and as well as its orientation in the world, and quite a good amount of, sensitivity. so that was really useful for, this task. And so after we basically get the ultrasound images plus the tracking data of where the probe is in the world, we now need to extract some more information from the image. And so this is the processing pipeline. First we have the environment class from Onet that we customize so that it, gets both of these types of information and synchronizes them. then in the data loader, it actually looks at the, we look at the full image and try to find a small patch in there, which should be on the surface of the object. So how we do that is we basically start in the middle, and move downwards until we detect a significant edge, and then we extract a patch around that. Then from that patch we detect, some point normal in curvature and we combine that information plus the distance from the top to the patch plus the tracker post to get a CMP message to send to the learning module.

so in the demo you're gonna see this iPhone, iPad app, just to orientate each to it. So we have, the image here and then we have the depth that we're shooting at and the gain is how white this image is. and then we have the number of, images we've captured so far. It starts at minus two 'cause obviously the first two don't count, as we all found out. and then like a manual capture button here we are set to the default, type of musculoskeletal, which was we found was like the best for identifying the kind of hard objects that we have to in, in the demo. so that's not the iPad app, just so you've seen it.

yeah, so a bit more detail on extracting the patch. So there were several, difficulties. so the acoustic shadows and artifacts were really giving us some trouble. So for example, sometimes the highest peak in the kind of edge detection is not actually the object. So in this case, we actually want the patch here and not here. 'cause this is a, an artifact. and so we had to do quite a bit of fiddling to figure out the right settings and right kind of algorithm to get the patch of the surface. Then next, we basically, figure out a bunch of points on that surface, and try to fit a circle through those points. And that circle tells us about the curvature at that point. and then we also try to estimate a point normal at that center location, which can tell us about the pose of the patch. And that also took a bit of fiddling because, again, there are all these little artifacts that, that sometimes get these blue dots to show up in the wrong locations. we had to sometimes limit the range in which we try to fit the, circle because, like surfaces, like on the brain are very, bent. but yeah, eventually I think now it, it works reasonably well. and then we have to combine the depth, the sensorimotor and the agent location. So if we just look at the agent location, so just what the HTC wife returns, we basically get the shape of the bag. And then after we add the depth that we extracted from the patch and the kind of, offset between the probe and the tracker, we get more something that's, shaped like a mug. if you have seen it many times you see the mug, but, this's, the first time you see this, you might not recognize it. it's not a tumor. Yeah. And then. Yeah, but it could, it, it could be a lot of other things besides a mug. yeah. So if you visualize it at 3D, so you can kinda rotate it. You can't see it on the slide, but if you rotate it, that this is the body of the mug and then down here is the handle. it's a bit hard to get a good screenshot of it.

but yeah, it does look like a mug. and then we added a bit more, live visualization that we can run during an experiment so we can see what's going on. So we have the input image and we have the extracted patch. You see how we fitted a circle through that patch and a point normal there that we extracted. And then that gets combined into locations or, and orientations in a common reference frame. So like relative to the world. And over time while we move the probe. So each of these blue dots here is basically, Neil's moving the probe to a new location and taking new another picture. So over time you have a bunch of points and they update our hypothesis space. And then eventually you, you have a most likely hypothesis and a couple of other possible objects, and eventually you hopefully recognize the object, which in this case we actually did even recognize some symmetry, in the pot meat. Can, yeah. Yeah. And another thing that we wanted was to be able to show to the user, the person who's operating this, where the probe is, relative to this bag. This that's suspending the objects. And the reason we wanna be able to show this is because, as you'll see in a moment, we also want to be able to direct the user, where to move the probe. And, going into the farther future, if Monty was ever used, you can imagine, someone who isn't, doesn't have a huge amount of training in sonography, if they can just figure out how to move the probe, and they can get some direction about where the probe should move, then that could enable them to acquire some quite high quality images with guidance from Monty. but the first step in, in this is actually showing it, okay, where the user, where's the probe right now? and so that's what you're seeing here, how we have this, visualization of the, kind of bag as this, kind of serial box like, white rectangle. And then the probe is this blue rectangle. And, you'll see this kinda live soon, but as the probe moves in space, the position of the probe in the, visualization gets updated. but in addition to this, we can, occasionally Monty reaches a state at which it, thinks there might be a goal state that if it moved to it could reduce uncertainty about what it's seen. and this is what we call a goal state. And so that's what's, the visualization is showing here, is that. at that point, this new blue arrow comes in and that's basically telling the user, you should move the probe, into this location. And basically that blue arrow will continue to be there until the user has, moved the probe there. It's still up to the user to decide whether they acquire an image there or not. Monty doesn't force them to do that, but it's at least giving some kind of guidance about where an interesting view, of the object might be to disambiguate. what's been observed?

yeah. Issues encountered as Neil's already showed, it's quite hard to work with ultrasound data when Will and I first got the first images. We were not very optimistic about this week, going well for us. but with Neil's expertise, we got something working. then second one, coordinate transforms are well as usual, huge pain, to figure out. yeah, Tristan Rami you mentioned, like having good visualizations help but still spend a good amount of frustration and time on that. And then the data streaming was a bit tedious. in the Airbnb. The IP addresses would randomly change sometimes and internet would drop out and things would go to sleep. the trackers kept falling asleep and then, or batteries dying. Yeah. Forgot to, so just all the communication, was a bit shaky between the four different components. we also tried to do a bit more AI assisted coding, which sometimes helped to get hacky solutions, but also sometimes really did not help. it tried to import a module called Monty Python and which, hallucinate some funny things. not, we could not resist that temptation to eat. Some of the dataset tried to make a good case for why it doesn't matter if the hot sauce is filled or not. So it was open. Doesn't affect the ultrasound image. Yeah.

We also had a magnitude 6.4, 6.2 earthquake at 6:00 AM in the morning that woke us up and, for a couple of hours we were debating whether we should go into the mountains 'cause there might be a tsunami. Luckily there was not.

and some fa family emergencies of, kids falling down the stairs. Down the stairs. Oh my, she's got a black eye now.

yeah. And, I guess there's a lot of kind of interesting things you could do in the future in terms of shorter term things that we'd love to look into. a big thing is the challenge of getting kind of reliable features extracted from those images. And as kinda Viviane was alluding to, if it picks up on an artifact and it thinks that's the edge of the object. Then in general, the representation's totally off. and there's a variety of ways we could handle that better. One thing we don't have at all is, we're just looking at the ultrasound as these kinds of images in 3D space, or sorry, edges in 3D space. And that is important for how humans understand ultrasound images. But it's also only part of it. We also look a lot at what is the kind of actual texture of what you're seeing.

whether it's a lung or gallbladder or whatever, all these things will look very different. and it's true of the objects we were scanning as well, but that's not accounted for. curvature was something we didn't get to being fully integrated. We measure it, but it, wasn't in a way that we can compare it to the simulated or the, models learned in simulation. and then, yeah, we had some ideas for how we could make sure we get this thing called the kind of maximum curvature dimension. which again was working in principle, but just needs to be made more robust. and then some of the things that we're actually working on the research side, like for example, Rami's working on with kind of resetting curring hypotheses, would really help with the kind of robustness of the system. So one of the things we're excited about is we've collected a lot of the images along with the positioning information, and saved that, which would be the start of a new data set where as Monty improves on the research side, we can actually rerun it on this kind of real world data set. I. and see how it, how it improves.

yeah, it's a tuna. Yeah. But, yeah, there was a tuna can in the dataset. that was, I guess one of the things we were practicing on, this is one, having, you can't see it on the screen, but it's most likely hypothesis is the tuna can, which is what's in the bag. And here you can see on the iPad screen, you see the depth, the ultrasound image, and Niels is holding the probe against the bag, and then the data gets streamed to the Windows laptop and to the Mac and Monty runs here. So that's the whole setup plus a bucket to collect, water potentially leaking out of this, what's actually a urinary catheter bag.

So is, Monty actually running on the Windows machine or on the Mac machine? On the Mac, yeah. The windows is only for, so open vr, which is what, is used to track the, the, marker, the tracker, doesn't really support, Mac or definitely doesn't support Mac and doesn't really support Linux either. especially like a headless raspberry pie without a GPU and all that stuff. Yeah.

Good. Okay. So how about answering the two questions?

can you please skin? How it shows Monty's, capabilities is that it's like a very kinda shape based approach to recognizing objects. It's not just like low level features that it picks up, which it couldn't do much with. in this ultrasound data. It's really about moving a sensorimotor in space and getting, locations relative to each other in space, which Montys uniquely made for. then also this kind of gold state generator that Neil showed with a point that tells the user where to move to the probe. Next is Montys Intelligent Action Policy, where it uses its internal models to tell you which view would, help most recognize the object. So it's not just like a random policy or like a methodical scan as, any current, ultrasound AI solutions are. but it's like very methodical about acquiring the information it needs. Yeah, and, then in terms of the kind of the long-term future applications. So of course it's not ready to be used now, with improvements and with improvements to Monty, this could be a huge, help for bringing ultrasound to parts of the world where any kind of me medical imaging is hard to do. Ultrasound's amazing because you can hold it in your hand, you can connect it to a smart device. something you cannot do at all. Of course, with something like a CT scanner, an MRI. and it's, generally safe. It, there's no radiation and all these things. So it's an amazing technology in that sense. The real hurdle is that you need someone who is had years of training to know how to use it and interpret it. and so there have been efforts to use kind of current deep learning systems to, help with that, but they require huge amounts of data. They don't have this kind of inherently sensorimotor spatial understanding that enables them to suggest, acquiring data the way we've, shown here. and so yeah, this would basically be a way to bring, the kinda benefits of ultrasound to, many parts of the world, that, currently, don't have it. Yeah, I would echo that, like the cost of the ultrasound was like a thousand dollars for an ultrasound. Cost of training a doctor is like 200,000 per year, and then hiring and da. That's a lot of money. And so the ultrasound device is gonna get cheaper and it's very easy to get them out to places where they need them. And I think a highlight a point that Neil's made about the lack of training data as well, is that. medical data isn't very available, but especially abnormal medical data where there is a problem, like if there's a tumor on the heart, like we don't have any scans of those 'cause they're so rare, but you need a device or a system to be able to say, Hey, this looks like a heart apart from that bit there. so while we didn't get to that in, in our, hackathon, like it was part of the stretch goals and we think it would be enormously beneficial for being able to scan a healthy thing and then be able to tell the difference between an unhealthy thing without having any additional training data would really Cool.

okay. Does that answer your questions? Yeah. That answer my questions all? Yeah. Then it's time for demos.

should we start and we go backwards in the water? Sure. Okay. Since we're already on mute, maybe we can show the, the Benny Hill music. Yeah. So I'll show around the setup while Will and Neil ski everything ready. So yeah, here's our Windows laptop that, communicates with the vibe trackers. The vibe trackers are set up down here. There's one I hope you can see, and then there's another one over there. And this is just wrapping the tracker to the ultrasound device.

This is indicating it's on, and then this is, connected to the iPad over here. It's a, it was sufficiently complex setup that we ended up creating like a surgical checklist that we would go through every time to make sure that everything is, is running. because yeah, we also have a few, we have the windows basically running the server providing the, location information. So yeah, so panel blah, blah, blah. iPad. While we're setting up, Terry, do you have an object that you would like us to test on? What about first, what are my choices? so we have the TBP mug, we have the tuna. Can we have a mustard bottle? The spam? Can we have a package of Rat Ramen noodles? We have hot sauce. we have a heart and the little menta brain stress ball.

yeah, I think those are the objects. Oh, okay. So why don't we, and these are all in your data set sets. You already have these Yes, these are the ones you can use. You would do us a big favor if you picked an object that doesn't float. Oh, okay. Okay. the heart probably floats. Yes. The brain probably floats. Yes. does the mustard float. A bit. Yeah, a little bit. But we can try it. what does, what, let me ask you this. What doesn't flow? The spam? Can the mug? The tuna?

oh yeah. We also have some Issa sauce.

Oh.

Oh, let's go. Tomato soup. Can, let's go with the tomato soup. Can. All right. We have never tested on that. So let's see. It's, no, I'm just, I was just gonna say the tomato soup can is it doesn't have a distinguishing feature. Oh, that's true. And it looks like the cup. So let's go with something. Yeah, exactly. the cup has the advantage that has the handle. That kind of, okay. How does the hot sauce float? no, the hot sauce might be a good one actually. Let's do the hot sauce 'cause it has a different shape.

And how empty is the sauce? Hot sauce, nails, no comment. Yeah, you may need to hold it, but I think that's a good one actually, let me just fill up water. Yeah. Neil said that he wanted to be like a normal person for once and travel without hot sauce and then he ended up eating our dataset.

Alright. Not much control there, Neil. But I was on holiday for two weeks in America as well, so it was a long time. America has hot sauce everywhere. When he told us that it doesn't matter for ultrasound, it was a bit like an addict trying to convince us that Yes, it, yeah, the visualization. so yeah, as things are shaping up here, you can see, so while you guys get set up, I just, it seems to me all three demo, all three projects are rely heavily on networking network. Yeah. I think that's like a robotics inherent kind of problem. Yeah. Okay. Because that kind of eliminates you being able to go to different places in the world with your, if you don't have good network 'cause of all the data coming in.

yeah, if you would Okay. Design like a full fledged product solution. I'm sure you could also work with just cables to connect stuff. Yeah. so yeah, this is more like the quick hacky version.

Okay. So we're gonna start, inference, are you sharing your screen? Yes, I am. Okay, cool. Okay. And then, so did you calibrate the track? Yeah. Okay. We've been through the text machine. So you are, right now, you'll be seeing the, you can see the probe. Sometimes it gets a bit laggy when we're on Zoom. Unfortunately, there's just, yeah. Too many devices running. The kind probe moving relative, and then maybe if you switch over will to the other view. What we have here is a view of try and make it full screen, what do you call it? The, yeah, on the top left you see the full ultrasound image, and then next to it you see the extracted patch and the features that we extracted. And then to the side of that, you saw, the relative locations in the world that it estimated.

and then on the bottom row you can see Monty's current hypothesis of what it thinks it is sensing. so there's still, almost all of the objects are still possible, but the most likely hypothesis right now is Monty's heart. what did we actually put in change that? The hot sauce. Hot sauce. Okay.

So we have, in our kind of experiments set up, we have 30 steps to try and, correctly maybe see you have a gold state? Gold state up here? Oh yeah, not yet. Okay. Sorry. the ultrasound just needs, it needs a lot of jelly and fluid to, work properly any air and it doesn't see anything. We're not a hundred percent sure yet that all of the coordinate transforms are correct. Yeah, there's definitely a, so there might be some. Room for improvement.

can you use Vaseline?

sorry. nevermind.

What top hypothesis do we have at the moment? Interesting.

That's surprising.

Yeah. It probably is the case that the according to transforms are totally right 'cause this should be doable. Ram. Yeah. we just have a few more, like a few more observations. Oh, yeah. Did we show the goal state? Sorry, if you go back, yeah, no. Okay. Am I, seems like the tracker probe is not calibrated, right? It's oh, yeah. Let's, for what it's worth that's just for the human. it doesn't affect the data collection. No. there is a, suggested pose up here. You wanna have a look at it? Okay. Yeah. If you rotate. yeah, I guess that was before calibrating, so maybe it's, yeah. Really off. But anyways, you can see the, you can see the principles operating as well as the coordinate transforms that are clearly still in need of debugging. how many steps are we on now? Okay. We're all try and be a bit quicker.

You can see on the ultrasound image, like at the top, these like zebra stripes. that's just one of the kind of artifacts that comes in. And we have to are those reflections or something. Okay. There we, and we finished. yeah, it's like bouncing back and forth and creating like a false image at a certain depth. So what did it categorize?

Monty heart. Monty heart. Oh. Oh, that's a shame. Oh, next we can try multiple times, right? Or not. we can try another object. but maybe you first demo yours. Yeah.

If you also get it wrong, then we can have a tiebreaker object. So can I head downstairs? Set?

Maybe the last parameter changes didn't help change because we did have some the, it looked the way it was plotted, but yeah, we did recognize the objects in the dataset we collected. So I guess this is the demo effect. Because yeah, the dimensions are quite different for the hot sauce bottle, so it should be quite doable. Yeah. even with noisy point normals and all that, maybe we, for the next one, we remove that last change, just to see if that maybe broke something. Sorry. That's okay. Back up 50.

this your robot's camera?

no, it's not a robot's camera. We just got, can you see it? His laptop is blurring things. Oh, it's because he wants to look for your Yeah. Blur the background. Yeah. I'm gonna do another thing here.

Unfortunately, it'll keep seeing that unless that's turned off. No, you're like, it's for sure blurring. I don't understand what looks, yeah. So I'm joining with my phone and I'll do the same thing. Recording in progress. Yeah. For what it's worth, I think that's setting, if it's on Zoom, it's under the video, the up arrow. on the bottom left corner. You, there's a setting called Blur. My background.

It's, yeah, it's not Blur. Oh. this would be blurred. This is Oh, it's just also, yeah. Or Apple is applying its own thing. Yeah. Then that would be the green camera in the top kind of menu bar if you click on that.

Okay. I also did my phone view down here. Okay. Oh, my phone died. Oh.

The demo guards are not nice.

Oh, there you go. That looks better. Good. Okay. That's good. Okay. Okay. Okay. Maybe it's focusing on me. Okay.

Could you put the phone on the tracker? So we get like first person experience, with it.

Yeah. Okay. We need an object.

Yeah. It's the same object. Hot sauce. Okay.

All right. Starting, what you're observing now is, it's end, it's resetting into that starting position.

Cool.

Oh, wow. Already hot sauce, most likely hypothesis things that might be Monty's heart. So I don't know. Oh, it can I, oh, it's neck and neck, yeah. And back, neck, and neck with what? I can't see the. No. Now hot Sauce is a clear lead, but there was Monty's heart as well there for a few steps. Monty's what? Oh, detective Hot Sauce. Oh wow. Good job. That's pretty awesome. Nice. Nice. Wow. let me share my entire screen actually. 'cause that will be, better.

so pick an, pick another one. Can you do the mug? Yeah, they already did the mug. Let's do something different. Okay. Spam. We can do something. Yeah, do the, you can do the spam. Ours are pretty quick. hold on. Let me just turn off the, I wanted to share because I wanted also e See, there we go. All right. What's on the, okay. Yeah. Yeah. It's still, all Here we go again. It's resetting.

oh. Terry, by the way, the spam can here is called potted meat. Can Oh, really? Full name in the dataset. Oh, okay. No product placement.

And by the way, you can see the models in the upper right corner of the visualization. that's, all that Monty knows. That's cool. interesting. That sparse.

Okay. it's between the two visual visualizer, it keeps saying global matching Step three three. Oh, we didn't debug those logs. Maybe it's, what's happening is the camera is above the object, so it wasn't getting any sensorimotor input, so it was just bypassing the learning module off object. Oh, nice. There you go. Whoa. Boom. Wow. What else you wanna identify? how about the Numenta brain? All right. This one is a little different setup. You have the platform. I don't think this one works very well. Do you have the platform?

Okay. Yeah. It's, it is the top platform, right? I don't remember on, yeah, just put it on top platform. Okay. So we, we have an extra platform for this one because our sensorimotor didn't go far enough to see it on the platform that we built.

and we also, presumably the platform is kind of part of the model. yeah. Although, we're not sure if this is the platform we use or if we use the different ones. So we'll find. Okay. Cool.

testing robustness. Yeah. So here you go. This subject is also very small, but the patch is sometimes just off the object a lot and that, some, way though it doesn't work for that reason. And hopefully it will not fling the brain off of the platform either. it's pretty light. We don't have many tolerances built in here. A flying brain. Yeah.

Oh, don't see Monty's brain in hypothesis space. Yeah. This one might not work. We'll see. Yeah. Right now the sensorimotor is above. So when you're seeing stuff like this, it's just not getting any data. 'cause the, again, our policy is just purely random movements.

Also, the time flight Sensorimotor works best with shiny objects and, Objects that are not like metallic or whatever, they don't really give good data.

Oh, yeah. Sorry. Visualization.

Yeah, I thinks it's a tuna soup. Can, this is why it's tuna soup. Can wait tomato soup or tuna can tomato soup. Sorry. Tomato soup probably. I guess that probably. Okay. you got an example of a failure mode That's not bad for three. Yeah's. Awesome. Do we get a second try on the path? Yeah. What if we get the brain? No, it's actually pretty good about the brain. We just put it on the bottom, right? Yeah. Just normally. Yeah. the heart. The heart, sorry. Yeah. So it's pretty good recognizing Monty's heart and you can see the image of that's its Model of the Heart. Yeah. Wow. That's pretty cool. Yeah. Do you wanna try the spam can again? Yeah. Should we try one more just to you, you change, hopefully have us, yeah. Okay. The checklist. Yeah. I would say the price for our highest, actually definitely it goes to you guys, but, wanna have, a little bit of success. Sounds good. Okay. I'll stop sharing as soon as this completes. So are you, like in monthly when you rotate, is it moving the agent around the circle? Yes. it's, we are faking it. Orbiting an object.

There we go. Monty's heart, Monty heart. So you're saying that this setup would be something that you guys actually will be using in the, a lot in the future to do some of your testing? Yeah. Yeah. that'll be like a real world test for, and that's not made outta Lego and way we can order one for each of us and each one will have one their home. You can probably point on the open space there. we're done with this demo we're doing now. We'll, we're gonna try and do the po We can all, if you have better left, anything timed out, get the track. You can see the meat can in the bag. Niels can actually, most of the time not see it 'cause it's not see through from the other side. Okay. Oh yeah. Calibration still looks and so yes. Will screen, I think it's also being shared when you can see how the tracker's moving in real time. And then the iPad screen. Seeing the corner of the can right now.

Maybe if we go to the hypothesis based screen, make sure we're getting images.

So yeah, you can see it does a decent job at extracting the patches and the curvature and point normal and all that.

just be a few things left to debug with the adding it all together. What's in the back? Spam? Can spam I did you tell from the images?

Oh yeah. There go.

Cool. Yeah. The edge of here, I think. What's the mlh Mented mug. Okay. You kidding me?

I think you like overfitted on the menta mug. Okay, so it's the patch right in the center. The patches being extracted? Yeah. Like this. It's not always in the center. It's when it first finds, oh, when it first finds the edge. So we lost the tracker. One second. we, track we need to re we're better off not having it 'cause it's, if we want to continue. Oh, okay. Because, otherwise it's gonna change the location. Where did you use for the 3D visualizer? my put.

Getting that quick. The live one is B Python. Okay.

the, Sensorimotor location. Yeah, I think that's a vibe thing. Yeah, part of the, okay. Yeah. Yeah. Cool. Oh yeah, here we go. Here's a good goal state. right now the probe is over here and the goal state is on the other side. So the user actually moves the Wow. Oh, okay. But the problem is we, yeah. Lost one of the trackers, unfortunately. But otherwise, basically if we move it to a similar position, then yeah, the tracker. If, if the pro moves on this side now it'll, we can try get, it can try that better. Maybe it's back. Okay. You go. Okay, that's pretty close. It's, maybe if you rotate it will, so I can see. Okay.

So once it goes into the actual point that the arrow disappears, so you see that once arrow, that's the arrow disappears, and then if you keep doing inference and get another goal state, then, what you call it, it will, it'll do the same thing and it'll create a new arrow. Yeah. That's really cool.

how many steps are.

we'll just collect a few more data points, see if we can, there's like a canned inside.

Yeah. I think, we learned that you over-interpret a lot of stuff if you know what it is, but a lot of it is just artifacts. What did start Oh man. What, okay. Work much better earlier today. Yeah. We, made the fatal error of trying some last minute, adjustments to things and that may have thrown the system off, but, that's how it is. If only we'd pick Monty's heart. Yeah. Clearly we've, gotten a hundred percent accuracy. Yeah. Steps higher.

Yeah. This is the, the number of points that we collected just then, and the point normal pointing out from the Nice. Yeah. And at least some of the learned, like we did, we're both the potted meat can and the Menta mug, we scanned it with 200 points. and we've plotted those learn models and they look decent. Yeah. but yeah, like right now curvature information isn't used at all. really. And yeah, there's just we're basically just matching the locations and a little bit of help, from the point normals, but, nice. Yeah. So now it get, now we go to the scoring phase, right?

stop sharing. is Terry sharing her screen now? we'll give a few minutes to think. and then, yeah, I don't know, Harry, if, since you saw everything too and you're pretty independent, if you'd like to also score, I can send you a link. Okay. let me find it again. can you see the link filled out of mine? Hang on.

not, yes I can. Perfect. Yeah, that's great.

okay. You're gonna put it in the chat to me? Yes. And then do you want me to explain mine, Harry, use, use zeros for or use zero, os for your, choices.

yeah, Terry, go ahead. Okay.

I think, Now, Harry probably might be a better judge than I am, but I'm basically going from my viewpoint, so it's pretty, subjective. I, I understood, more of what everything a is awesome team did. I thought they had their visuals match the I image and, the different, d perspective, the different angles. You could really see how it, it was going around. I understood their explanation a little bit better.

I, thought their, setup was, a lot more condensed and easier to understand. So I thought that was easier on the eye. I do believe that it's not a tumor was a very impressive, solution with regards to dealing with this, the sonogram and the difficulty of sonograms and the future capabilities of doing, medical diagnosis remotely, around the world. that was impressive. the most ambitious, I, all of three were very ambitious, but I do believe that the vicarious, had the most ambitious just because I don't think that they had the, Equipment that they needed to get further down with their project. I think if they had more of a sophisticated drone or whatever, their outcomes would have been a lot better. They spent most of their time just trying to get that, the setup, which kind of took away from the rest of their project. So I, think their project was a lot more ambitious just because they didn't start with, sophisticated enough equipment. I thought all of 'em, all of you guys ran into several different, problems and hurdles to, get over. And all three teams really did extremely well on all of those. I thought, trying to come up with, solutions and creative solutions. I thought you guys did all three teams, did a great job. also with the team players. I think that's one thing with regards to thousand brains projects. You guys all work so well together and, I think they, everybody did their part and actually pretty much were a part of a full part of each team and each team's project, which was nice to see.

I think that everything awesome, demonstrated that the. Monty's capability just because they were able to detect, figure out what the item was. pretty much on everything except for that brain, which was, exciting. And, I think all three, actually gets me excited about Monty's future because all three of them have different applications that they can apply. also I think the awesome one is something that you all three, the whole group can use in the future right now. So the benefits are short term, of this project. And so I think probably I would award everything. Awesome. The prize.

I think, everyone will understand this decision. Yeah. Yeah. yeah. Thanks a lot Terry, for, the great, ranking. I would agree with all of that. and yeah, I think since everything is awesome as, pretty far ahead in points, we don't even need to fill out this column 'cause we can't catch up to them.

yeah, altruistically fighting.

Yeah, but I'm, it's really impressive how much you guys, my brain hurts and I don't even understand 90, 90 of it. the 1 part that I do understand, my, my brain's exhausted. I think you guys did a wonderful job and, it's I just really enjoy watching your whole team work so well together and, develop, these projects just in a short period of time, I think it's wonderful.

Yeah. Thanks, Terry. Harry, do you have anything to say?

I'll make a few comments. I did not take notes, so I can't fill out things at the level of granularity that, that you did, Terry. but I concur with the general sense that everything is awesome on the whole, takes the cake and, walks away with, first place. I think to me, what's most interesting to me is, it's not a tuna. Not a tuna, because I think that really, shows how Monty could be used in very. Different real world, applications, as opposed to the other two demos, which are more the classic, take pictures and see if you can recognize an object. Everyone's working on that and, that's fine. But I think the, idea of dealing with ultrasound, its complexities, it's amazing that you got any results when you look at the a, a sample image and the artifacts, dealing with that just represents such a, fantastic, promise that, I certainly hope that's pursued. In addition to the other sort of classic ones, I do have a question, or maybe it's an observation of my understanding of how these, systems collect data and try to resolve it into a hypothesis. And that is, we do a very good job at understanding the world around us with one eye depth information from these devices is pretty crude, and I think that's a long-term feature compared to the resolution of the RGB camera. And I wonder if including, the, depth information actually makes the problem harder. Working with just a, a 2D snapshot, but moving it around from all different perspectives gives you all the information you need, I think. And I'd like to have your thoughts on the pros and cons of using, depth data given the technology that is deployed in, this, in these two examples.

Yeah, I think, definitely in humans we have a bunch of different heuristics how we es estimate death besides like binocular death estimates. so I think it's still quite important in human vision to have depth estimates and we do it pretty early on in the visual system. but it is definitely not like as accurate as you would get with lidar or, maybe even the ultrasound. so I think it's a bit of a mix of both that you need some kind of estimate to be able to tell how far things are away from you. 'cause otherwise you have no way of knowing like how big like a movement of your eye would translate to movement in the world that you see, like object as this far away is much smaller than the same object, when it's closer to you. But then also being able to deal with those just being pretty rough estimates. which right now we're, Decent at, but not great.

but yeah. So still necessary to a degree, but not to like a millimeter precision. Yeah, it's like in the, everything is awesome case. The robot is only moving up and down and the depth changes because the object is, has shape, but the average depth is, it's kept at a constant, for the ultrasound, it's kept, constant with respect to the bag that it is in. So again, there's not a huge depth value, like variation in the depth value, even if you get it. So I don't think depth in like those senses are like very useful. Yeah, I guess so. I think that's why, like maybe you, you thought about it that way in, in our case, like it's very, if it were to like work and Monty, so if it, if admitted up motor policy, actually if we're far away then it should been a policy like get closer. in that case, having accurate like depth information will be like crucial. so that one, it can, get closer to the object of interest is trying to learn. And two, so that doesn't also collide into the object. like here, they're like artificially set up so that there's not gonna be a collision between the agent, and the the object that we're trying to identify. So yeah. Yeah, I think that's also another good point is that you don't necessarily need an entire depth map. it's more for example, in our ultrasound, example, we are not using the whole kind of depth image we get. We're just using one depth value at the center of the patch, and then we are moving it. that's this approach of just saying, how far away am I from what I'm currently looking at? And then move around that to build up the model, but not actually requiring to get like an entire depth map, and integrating that information.

I don't know if that answered your question or, master thought. No, I, I think, no, you, it, does, but I still think that if you have the capability and the design to be able to move the sensorimotor and orient it in any direction and, space, then parallax information is every bit as good as Oh yeah. Depth information gathered from the image. Oh, yeah. I just think that given the resolution of those two different images. A depth map with today's technology versus the actual, 2D snapshot. I'm just suggesting you may be better off using the 2D snapshot and the physics of parallax, to figure out what's going on. That depth information is actually introducing noise, making it more difficult for you. But that's a hypothesis actually. But I did try, SFM, so because I took 12 pictures, with, overlapping, the, and tried to, do 3D reconstruction from a bunch of 2D photos, right? that, that's using more of the triangulation that, the technology that you're talking about. I think the problem was that the image, because there were a lot of backgrounds, the features didn't really match like there, features from the floor and try to match fe, that's right. Yeah. So one of the nice things that at one point we were reviewing different theories for how depth information first appears in the primate visual system. And, it is probably something similar to, or, it definitely relies on parallax, both with binocular and also with motion. but what's interesting is because again, Monty's a Sensorimotor system, the primates are sensorimotor and they focus in on one area. In that setting that actually solves a lot of the challenges that parallax faces in computer vision where you're trying to match like thousands of points in these massive images. it's computationally expensive, there's a lot of ambiguity. it's just interesting that, again, like just the kind of way our eyes are set up and everything, solves a lot of that. And yeah, so it's something we don't have, at the moment in Monty, but I think it's a great point that it would be, yeah, really interesting at some point to, to bring that in. Maybe the, next, robotics hackathon. Yeah, I think intuitively one, one always thinks, oh, more information is better. and I'm just given the specific technology. Wonder if that's true in this case. Obviously in the long run, if you had very high resolution depth data, surely that will be helpful.

I have a funny, I have a funny story to say to tell you. when I first moved out to California, I worked for a company called Biometric Imaging, and it was run by a guy that's very well known in the valley. He did spectro physics, he did a lot of LA lasers, but he does mostly medical diagnostic. just about six months ago he contacted me because he saw my connection with Numenta and he, he's from India. His name is Bala Manion, and he was, when he was a child, he was in a, an accident and he lost one of his eyes. So he is always been wearing one patch. And so his question was, he was always wondering how could he drive a car when he only had one eye? And so he said, he, told me that, the Thousand Brains Theory plus another book about how do I invent where his two books that he kept on his desk every, all the time. And he would period periodically just pick 'em up and read 'em again because he feels that Thousand Brains Theory is a strong thing to explain to him why he can drive with one eye and really feels that it's a great theory. to bring that up, to, to, when Harry said One eye, I said, I know somebody with one eye. And he, that's what I was imagining and it'll be very interesting. I would be, wouldn't be surprised if there aren't research papers about how people with one eye navigate and what, deficits they have or, don't have. yeah. So it's probably been, studied. I would imagine. Yeah, but he's a very interesting guy. If you ever wanna, when you're out around, he would love to talk to anybody about it. he's, very, excited about it. And, but like I said, he does a lot of, medical devices and he's very in tuned. He wants to see how possibly thousand brains, he's, really into the gut and brain connection and the whole thing of, taking into that, information. But, I thought that was interesting that the fact that he had the one eye and has that same question that Harry had. Yeah. Yeah. That's cool. Definitely be cool to meet sometime. Yeah. Yeah. he's a very smart guy. he would really, he's and he is quite a character. He is. He's, very entertaining. Sounds great. I see Tristan, you're awarding yourselves the trophies.

Everybody who hasn't seen these coffee's, that one as a result of the hackathon. So you go around. Yeah.

Yeah. Congrats. Thoughts on Congratulations. Awesome project. Yeah, it was awesome. Yeah. Isn't there a spare spam? Is that Yeah, there's a spare spam can, I was thinking the team open one and enjoy it. That's your treat. I think you can hand this to Hojae. There aren't too many trophies that you can actually consume.

I'm not sure if there's actually still spam in there off there. I hope not filled it with something. I know. I don't, think there is. There's holes in them. If it is I, and I don't think potted meat is much of a trophy.