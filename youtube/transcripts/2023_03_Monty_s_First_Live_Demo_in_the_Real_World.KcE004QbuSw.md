Welcome to the Thousand Brains Project YouTube channel. This video is one of my favorites. It's pretty short, but it's the first demo of Monty on some real world data. The background is, in March 2023, Numenta organized a little internal hackathon where everyone at the company had a week's time to accomplish a moonshot project. And at the end of the week, we would demo our results. And we would form little teams to tackle these projects. So the thousand brains project team, which back then consisted of four people, was thinking about what we could do. And we've been thinking for a while about doing a real world demo of Monty, and we decided that this week would be a great time to do that. So we tried to figure out a real world demo that we could accomplish within just one week Without having to buy some expensive robotics hardware or anything like that. And also, half of our team was remote, so Niels was in England and I was in Greece. And then the other half of our team was in the Redwood City office. So we would have to have something that we can share and we can all work on. So we ended up deciding to use the iPad or iPhone camera and write a small iOS app. That takes the camera picture, and then we would manually take a little patch and move it over that picture. Which is a little artificial, but it's a good starting point, and it was something that we could accomplish in a short period of time.

I won't give away too much, you'll see everything in the video that's about to come, but I'm hoping that this little project will be maybe some inspiration for you to think about some other ways Monty could be used and some other little projects you could try with it. Have fun

Yeah, so this was definitely an exciting week for Monty. it was the first time that Monty saw some real world data.

so yeah, this was really something we've been wanting to look at for a long time and, the hackathon was a great opportunity for it.

yeah, you can go to the next slide. So yeah, the goal for this week was to set up an environment for Monty to be tested in the real world. And we decided to use the iPad or iPhone TrueDepth camera as a sensor because we all had access to it. So we didn't need to. Buy any equipment and it gives reasonable depth estimates. I, didn't realize the iPhone had a depth camera in it. It has for the face id. It's for the face id. Oh, I didn't know that. It has LIDAR. LIDAR is the other one. Oh, LIDAR is the other one. Oh, okay. Yeah. LIDAR is on the other side. LIDAR is more for like long range depth detection, and then the one, the front facing one is more for. Accurate short range depth. Up to five meters. Is it LIDAR on the back? Yeah. Yeah, we tested both, but, yeah, then chose to go with this one. Yeah. but yeah, the main goal was to just see how well Monty can deal with real world data. And to have a testbed for that so we don't always do everything in simulation and forget about some problems that occur in the real world. And then the second nice to have that we weren't sure about, whether it will work out. It's just to have a first, start of a demo of Monty, showing that it can actually do something with real sensors, yeah.

Okay, so yeah, so in order to evaluate this in the real world, as Viviane said, we are going to perform inference using these depth images, taken from a phone or iPad. but one of the kind of, stepping stones in this whole process, current limitations of the whole, setup is we don't have information about how the iPad or iPhone is moving through space and where it is, and because Monty is a fully sensorimotor system, that's essential for how it learns. and so in order to enable learning still, we had to get real world objects into, essentially a simulation environment where Monty could then learn, and then based on learning in a simulation, it could then perform inference in the real world. and so in order to do that, we took a couple, nine real world objects that a lot of people have lying around, like the classic, Numenta mug, or the wonderful Numenta brain, or, a Halloween model of a human heart. just whatever you happen to have, lying around. And anyways, and use, a technique called photogrammetry, to essentially stick together, stitch together a bunch of 2D images, create 3D models, that we could then put in Habitat and learn on.

Yeah, this is how the data pipeline works. So we have an app that is iOS, native, it's, just concerned with image acquisition. So it takes RGB images and depth, images as well. then it's streams directly the data, so both the RGB and the depth to a local server that is hosted on my laptop. And this is the inference phase now. Yeah. So you've already trained on that. Whatever.

okay. Yeah, so the, then the server picks up all the streamed data and saves it into a, local folder, and Monty is running in parallel, so constantly looking at the folder, seeing if new data has come in, and loads it into the, the inference pipeline, in real time, and then, so the images that are taken are used to initialize the Monty environment, and then the action policy that Monty is, It's used to extract patches at different locations within that complete image.

And then, yeah, eventually Monty runs, the inference is run, and the app outputs the most likely object that you can pose that is recognized.

Yeah, this one is just to quickly, remind how, yeah, Monty just uses sensor patches like Jad mentioned. the big, camera image is never given to Monty, at once, but Monty has like a small patch that it can move over the image. And then it has the internal models that Niels talked about that were learned in simulation. And it basically updates its evidence for different hypotheses of where and on which object it thinks it is right now by moving through this image.

And yeah, and I should have maybe mentioned that although the, kind of simulated models is, sort of one way to get around this whole bottleneck of the motor information, not currently available, it's actually, it makes it more challenging because we are doing this transfer from simulation to real data, which is, something that, for example, deep learning systems classically, struggle with if you've just trained on simulated data.

Yeah, and, as I mentioned, the input are the sensors of the app, and the app right now is using a front facing camera on the phone and the iPad, which it gives you this, that gives a more precision 3D depth image than the Urizon, because it's over 5 meters. And once the app collects both the depth image and the RGB image, and that's where it can send to Monty, just like the simulator. Like when we're using the simulator, we just grab the same information that we got from the simulator, we pass it via the app. So we have two separate things we're capturing. Correct. It's a depth map and a color. And I call it RGB. It's an RGB and a depth map. And you probably will see that during the demo, what they look like. Okay. I think it's like a xbox style. No, it's what they do, they project an infrared grid on your face, and then you see the distortion of that infrared, that grid, and that's how they computed that. Oh yeah, we actually tested this also in the dark, so you can also get. also if you turn the lights off, maybe I can set that up. Yeah. She works in the dark. Yeah. You can see that this explain. See that, that the little flashing. Yeah. So that's the infrared projecting on your face. How about is it, this explains how sometimes my phone recognize my face in the dark. I was wondering what the hell's it doing? How's it is? The infrared projection of a grid of dots. Wow. How far does it go out? So you could actually use this without, the imaging in the dark, right? Yeah, so I take pictures of the ceiling, so I was able to get some silk from the bottom. That's pretty cool.

Yeah, maybe we, I don't know, can we dim the lights or is that complicated? Does that help? It does help actually, because it removes the reflection. Get a bit of the demo mood going. Yeah, Jad, I guess you, you should share the screen with the, yeah, nice. Thanks. Want to get a dark one?

All right, so this is the server. So this is the server. You can start the server. So here's the image that we've got. Oh, quick thing. Once Viviane and Niels talk, then we'll no longer see this demo thing when it's recording. So I guess Vivian and Niels should mute themselves. No, Did you get it? I don't know. It's usually recorded as a person that's talking. Oh, I see what you're saying. yeah. I'm a little confused. Who's taking this picture? Oh, it's this iPad here. Yeah, but how come I see the front of the iPad? Because there's many cameras involved.

That's just the room camera we're looking at. Yeah, this guy here.

But that's an interesting point, actually. This view is not on the shared view. so I'm not sure, because it usually records the screen that the person's talking. So I'm not sure if this is recording. So I guess Viviane, you should probably mute themselves, then for sure this will be recording, like, where the sound's coming from. Whose screen is that? It's mine. That's mine. That's yours. So I started running the iPad camera in the same window as the shared screen, so it may be being recorded. I don't know if it's recording all of them, but I definitely see both screens at the same time. No, you see it, but the recording won't. Yeah, that's what I'm saying. Yeah, I mean everyone is here, so let's, yeah, we can do this again at another time if anyone wants to try it again.

Okay. Yeah, so I just run Monty and now it's waiting for a new data to come in on the iPad. so I guess we So we have a button here that says save, and I still have to press this button to take a picture. Oh, wow, and it's scanning. Yeah, it's updating the hypothesis. I know we've got them up. It's got them up. How many objects are there?

Multiple, let's try this one. Yeah, nine objects, nine objects at the moment. But there's a fair amount of ambiguity. It's a pretty difficult data set. So he works in, not just on the phone, it works also on the. How's the accuracy? we even have to think about that. Probably the wrong question.

You know how in ImageNet you have like top five? and then, obviously that has a thousand categories, whatever. Our top one accuracy is around 40 percent at the moment, but top two is 80 to 90. That's pretty awesome. And that's a simple real transfer. If you do, if we do, training and inference in simulation, it's around 90 percent with noise. And hundred percent.

Yeah, exactly. Almost. Got it. It's, yeah. That's very cool. What is Kashmiri Chili? it's a pack of chili powder, what's Montys heart. So would it recognize it if we rotated and then, as in the brain on the other side? You can try. This is Monty's heart.

We tested a lot of different orientations and some work better than others, if you just see the cylinder, it often confuses it with, because there are two other objects that are cylindrical and wide. And so if we don't see like unique features like the handle, it sometimes. Yeah, gets it wrong, but do you need to flip the image because it's backwards, the writing is backwards, or, I don't know. Oh, that's interesting. But it still looks like it's. Between the two mugs question, I'm not sure. We checked about, oh, that's because it's cut.

Yeah, that's pretty good. They got it pretty early too. Wow.

that's very cool. That's awesome. Excellent. That's incredible. It looks like it was hard to get this all the work. Did that was it? Yeah, it was definitely quite a team effort. it was really, we had a bit of an advantage because we were in two time zones. So we would have, Niels and I work the same day, then we have a handoff meeting. Basically, we're passed the baton to Jad and Niels, Jad and Luis, and they work during, while we sleep, we wake up, we have a, it was like 24 hours around the clock.. And how did you use ChatGPT? What was that? Oh, right that closely. Oh, I see. Okay. It was really minor. The only thing I used it for was to save the RGB image, but yeah, most stuff, we don't see. Pretty cool. Wow. That's the first, real time. Yeah. Sensorimotor, reference frame. Has anyone ever else done this in the world? This way. I know people have been working on an object like this for years, but not like with a convolutional neural network with a sensorimotor system. I think there are active vision systems out there. Yeah, it'll be really exciting when, yeah, maybe next hackathon, who knows, we can have the kind of motor information coming from the iPad and actually move around it and things like that. Yeah, and another thing we wanted to do, maybe at a future hackathon is to actually send back the detected object and pose to the iPad and then AR render it onto the camera image. So we, you can move it around and have the detection rendered onto it in real time. That would be like the spy movies. Actually if you stop sharing for a second.

Yeah, now if we speak I think.

that one hopefully should be recording this scene right now. At least on the video. It's just that it only picks one camera image to record. And so now, at least a little bit of this video will definitely have this.

Viviane took like a baby steps video of the demo earlier. Very impressive. Yeah, that's really amazing.

Think of Subutai for lending us the iPad. stand, and the stand. Oh, my iPad was there first. You're on Monty? Yeah. And the stand, and everything else. You need to engrave it out. Yeah. Frame it out. Yeah, I don't know if, if other people have had this experience, but a few times in my career, there's been times when things first start doing anything you know, like we wanted it to. Yeah. Yeah. The board comes up with palm pilot for the first time and the months of debugging or something, and this felt like that, except this one was a week of debugging. Yeah. At the beginning of, at the beginning of the week, we really had no idea if it would work at all or if it would just. Colossally fail completely. so yeah, I think on Wednesday Niels, you sent me the first results and it got it correct. And it was like, too good to be true actually. This is definitely a very unrealistic goal to attempt. Congratulations.