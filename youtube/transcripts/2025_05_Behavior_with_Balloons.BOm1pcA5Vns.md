yeah, I guess there's a few things we mentioned in the group that we could chat about. but a lot of it revolved around this example of a balloon, things like that. I also had some kind of thoughts on that, that I'd be f for sharing at some point, but I don't know if you wanted to start Jeff. no, I, we were going some, I just wanna, I remember last week we talked about one of the things we had to just figure out once is how learning is, how we solve the problem of every column has to learn every model. Yeah. and then it became really obviously a difficulty problem and, when we were doing a behavior models. And so I have, I just, I have a, I think I had the general direction to solve that. So when we get a chance, I can talk about that, but I don't have to do that. I can do that after we talk about balloons if you want. Okay. Yeah. Yeah. Maybe, I could talk about, I was gonna say, 'cause I think it's pretty brief and. We might all disagree. And then I think it's a nice segue into maybe what you're gonna talk about. So what's your topic is, what about balloons? I guess it, it's about balloons, but it's also about this thing about model sharing. Okay. Alright. And basically it's just this suggestion that maybe we're making it more complicated than it needs to be. And what we have is in some sense sufficient, at least for this stuff. Because, when we're talking about like the stapler opening, there's this issue of okay, when you're observing the behavior, you might just be observing it for this location, and learn that. And then later you see it at this location and it's like, how do you generalize? And so that's where this kind of question comes in of sharing the models of all the different locations, that have been learned. And then I guess, the balloon was a similar issue of, how do you describe this, complex distortion of the balloon. Increasing in size, but it's not just increasing in size, it's doing lots of different things. As you say, Viviane, like if you had a logo printed on the balloon, that would be doing complicated things. I understood the balloon, I understood the balloon problem to be, that all the features of the balloon are moving at once. and how would that work in a primary region? Is that correct? that they're moving at once, but they're not moving together in a consistent way. Like with the logo, they're all moving in the same direction, but with the balloon, they all move in different directions, but for different amounts.

but yeah, so I guess what I'm just thinking is with the stapler top, or something like that, we've talked about how if you're modeling this as a child object, you could then vote on the state of this child object as a whole. so where that is in, in space, what its orientation is and things like that. And if you're, if that's the information you're passing up. That's the information that's forming the basis of the behavioral model in the higher level, learning module. Then later when you see a different, when, you're looking at a different part of the stapler, and it's showing the same behavior, as long as you can still infer the state of this child, that representation will generalize. It doesn't matter that when you first learned it, you were at this location, right? And now you're here because you're describing it at the child, level. I thought, that's how we ended up last week. What felt pretty good was that the lower learning module is basically inferring the child, and as long as it's inferred the child, it doesn't matter where it's looking. And, yeah. And then, but then you don't need to share models in that case, I don't think, because the only instance you would need to sh like ba basically the description of the child is generally sufficient. I think if it's not, if it's a, if it's such a complex behavior like someone dancing or something like that, there's multiple children and stuff like that. I think that's, then that's the case where you probably do have to attend to all the locations to learn the behavior. If there was, even initially you may not know that, I can think of two staple tops there, or several many now. Yeah. One is the whole staple top moves at once, and the other is it divides in two and it, goes this as it opens, another one, it rotates this way as it opens. So you can think of all these things. And so all those situations would require multiple observations.

and you just really hard to imagine if I had a single column, I'm looking through a straw. How I could possibly learn those. It just seems really, hard. yeah, it real, if he assume that the child is, there's a single child, and I know, by the way, I have to know that's the child, That wasn't, that I guess we don't know which, in the state, but we may not know there is one child, maybe there's 10 child, maybe the staple of bottoms rotating. So there's a, it's a little bit more complicated. We can't just assume that the staple one thing. Sure. Yeah. I, and I think, yeah, how we segment it out and, represent that is, is a separate and unsolved problem. But, I guess I'm, just saying that I think that if we have that kind of segmentation and we're representing the state of the child and we can vote it on it and things like that, then I think a lot of these concerns, around kind of sharing models between learning modules, disappear. I think that was more of the question. We started off by saying, how do I learn even a single module learn a behavior. and the question then was, how could observe all these things at once? and it can't. So I think if we knew that you were gonna parse the staple in the two parts, and one part was moving and, there are two columns and they knew which was the parion, then that would be easy. what would work? But then again, what if there's multiple pieces moving at once? then it gets harder. It's I, yeah. I guess one with, I'm thinking it's just that, like if you, if you imagine you're looking at a stapler. Wow. That's cool. So we're using our, we're using our logo pattern. Yeah. I as a template for multiple columns. That's cool. Exactly. for voting. so you imagine each one of these columns is seen a part of the stapler, right? if this part starts moving. Basically all of the columns that are seeing, that are getting some information about a particular state of the stapler, they don't necessarily know that it's just the top, but they might all be agreeing that, okay, yes, stapler at like the, object level seems to be moving in a, consistent direction or rotating in a consistent direction.

so that it feels like that could help them, like all of these learning modules basically be talking about the same thing. okay. And then if there's, yeah, I don't know. Let's say that this, while the top opens, this one shifts back and forth. wait, this is the, child level we're looking at here. yeah, I guess that's the child shown. So remember, the child level at this point is seeing all parts of the staple. So we've, somehow pulled out the parts that are moving versus the parts that are not moving. yeah, so I think I guess what I'm just saying is like. We talked about how the first level of the representation of the child objects is just the same, it's still just a stapler. But then we have to somehow like, attend to part of it or somehow kinda cut off part of it segment. But segmented. But just the first part I think is, straightforward that all these parts are talking about the same stapler because their stapler, everything they're seeing is moving in a unified way. Whereas all the, ones that are talking about a different part of the stapler, they're experiencing basically a different stapler. It's almost like it's two different objects. because, how do they, the other properties associated with it are different. It's this part, this stapler isn't moving, and things like that. So it, I'm, I think this begs the que, there's two issues here, right? One is we're just trying to figure out how do we segment this so that we know that the child's object is the moving part? Pay atten pay no attention to the other ones.

that's part of the problem. And, yeah, I guess what I'm just trying to get is this is how one learning module here could be confident. There is a stapler that is moving, even though it only sees part of it, but this, yeah. Just inference. Yeah. This assumes we have a model of the inference, but are now learn, but we're now learning the behavior. So we, we have a model of a static stapler that we can leverage would be learned in the higher level column in this case. Yes. Yeah. And then, so are you suggesting that we would use the votes to learn where to lay down points in the behavior reference frame?

no. just that the votes help a single learning module. Recognize a state of the entire child object. Oh, okay. Yeah. like, that. It's changing in some ways. And, that is whats passed to the higher level. And that's what's here's the problem, the stapler, before we've done anything, we don't know what parts of the stapler are gonna move and which parts don't. Just Sure. You gotta forget. we know that. You and I know that, but yeah, just the system doesn't know that. And so when something starts moving, the one column has no idea what, what is moving. It just says, the thing I'm looking at is moving. But that could be the back half of the stapler or the front half of the stapler. Or it could be the logo on the stapler. It could be, we don't know what's the top half of the stapler, because we don't know about top halfs yet. Something moving as, as far as it's concerned. The whole stapler is moving well, but Yeah. But doesn't know. I think what Neil is suggesting is that. correct me if I'm wrong, but, as far as I understand it, I think it's a good point. that if we assume that the lower level column has learned a model of the stapler, Then having many lower level columns with associated to all these patches means we can do flash inference of the stapler and its orientation. so we can very quickly know the, orientation and ID of the child object and pass that up to the parent co Learns the behavior. but I see a problem with that. we, the, we don't know, imagine many different parts of the stapler might be moving now, and that single column can't say different co it would come to one conclusion looking at one place and another conclusion looking at some place else. Exactly. But that's the thing. So it's basically gonna be saying maybe a, clear way of showing it is basically we're gonna have two. Models of the stapler as far as, low level columns are concerned. So this the learning module here, sorry. Gimme two seconds.

Oh, that's slow.

basic. Basically the learning module here, what the arrow's pointing to, it's voting with a bunch of other learning modules that are seeing the same thing. They're basically doing flash inference and saying, oh, the staplers at this orientation, a learning module here is voting with all of these and saying, no, I see a stapler at this orientation. So those learning modules are going to basically be thinking they're seeing two different objects. Like it, it would be as if there was two objects in the, I don't, these two objects, they don't agree. So they're talking about different objects. And so all they're gonna pass up, like, all this learning module is going to pass to the next level is a stapler at that orientation. But if we look down here, it's going to pass. Okay. There's a stapler at this orientation.

Yeah.

they would still have conflicting votes. I'm not sure if they would actually. Yeah, they would. They would dis Exactly. they would disagree. I don't think they may. They would, I guess they would agree on Id, but I think it, it seems to be the only way you get 'em to agree is you have to mask off the other parts.

that, that seems, but why is it any different?

I think the masking becomes important when we wanna make like predictions and things like that. no, I think even just, what if I'm a little, I'm a learning module and I see something locally moving, but everybody else sees the staple. My, my vote will be overshadowed. It'd be like everyone's voting and say, no, this is staple. You're, you just got some weird thing going on over there. we're all agree. I, feel like unless you attend to that and you pay particular attention to that's gonna be kinda like a blind spot. you probably wouldn't even notice that. But if you're attending to it, then you, that learning module will basically be saying, I'm definitely not seeing a stapler. So attending, I'm seeing like a small piece attending that's doing something else. Attending is including masking. Attending is saying, this is the part that's important. I pay no attention to the other parts. and so that's where we came up with this idea that, the parent has to say, or somebody has to say, attend to this section. Ignore everything else. Attend. And that's essentially saying mask out everybody else, these are the only common you care about. So I'm not sure. I'm, I don't think we can get rid of the idea of masking. I'm arguing for that. I, don't want to either. Yeah, I agree. Okay, so, then I, guess I'm just arguing. Yeah, I think so, so let me finish my argument. I don't think we need to share models. is I think what I'm arguing. I, okay, but I didn't, I think we have a problem. We can talk about how we're gonna address the problem. The problem, as I see it is, as an object behavior unfold, there's lots of things that might be moving and, they might be moving in different directions. I don't know how many child things are changing. and, and therefore I have to, I, I don't, I can't attend to them all at once. I can't attend to all these things at the same time. That's, my, the trust thing I was struggling with. imagine if the top of the staple, it didn't raise, but it just, it, maybe as it went up, it became like an arc. It bent, it, it went up and it turned like it, it bent and then when it flattered went up, and, so there's a lot of things going on there. The behavior that I can't say, oh, it's just part of the stapler moving. It's oh, there's a lot of changes going on. yeah, and I guess that gets to my other one, which was relates to the balloon, which, yeah, I think it's an interesting one to think about because before we start talking about the balloon, we should make sure we all know what we're talking about. yeah. I actually made a couple of animations for it, but, Oh, wow. I don't, if it's a quick point, Niels, you wanna make, you can maybe Neil, I'm not sure if I understood your point. I think the point, and I thought we come to some sub agreement about this last week is that especially during, during inference of a behavior, we could, that was the thing that I was focused on. We can make correct predictions if we know that the ob, the child object is moving, we only have to be observing one part. If we know that part, how that part is moving, then we can make predictions about the other parts of the child object. So if I'm only looking at the middle of the top, I can predict what the end of the top is. 'cause I just know the whole staple has moved. I thought that's the conclusion released last week, or at least that's how we left it. And yeah, I guess what I was, at least in my mind, it's not clear yet how that would work with an object is actually deforming. if the features are moving in different directions such as a balloon. Yeah, exactly. Like the balloon. Okay. So I'm not sure I wanted, I don't know if I missed something, Niels, if there was something addition on top of that, that you were really a point you were trying to make. no. May maybe, I misunderstood your concerns, but, I felt when you had argued for why we need different models, or, sorry to share models rather across columns, it was this issue of if, I've only learned when looking at this small part, how do I generalize when I see this other part? No, I think we came up with a reasonable solution for that. and I think you just reiterated it. yeah. maybe in a little bit more detail, but I still think there's an issue of, how a single column can learn, the behavioral models, when you don't know what the p what parts are gonna be moving and how they're gonna be moving. So the Yeah, I agree. Yeah. if it's learning like in with through the straw or whatever. Yeah. which I think we've also argued several times, is that even possible? can we really learn behaviors looking through a straw? No. I've always felt that when we can we learn a morphology model looking through a straw? Yes, you can. It's pretty slow. And I've argued that I don't think we do that. I don't think every column has to view every part of an object. So somehow we can train a whole bunch of columns that, that they don't all have to experience every location on an object. But I, it wasn't a hard requirement because the single column can do it by moving slowly. We've done that. That's what money does.

but now with the behavioral model, it gets really hard because we don't have the time and the luxury to go around and look at all the things and say, who's moving which way? Let's, it's all going by really quick. So then it it just forced the problem upon us. That's how I felt. and so I felt oh, there's always a problem, but it was nothing really big. We hit it, but now it's in our face. and so it was a learning problem. Yeah. And I think we did solve, as you just talked about, we did solve oh, once I've learned it, how do I make predictions? Okay, I'm with you. Yeah. We can now predict, even a single column can predict, but how did that column learn the correct? yeah. And I guess what I was arguing, and I'm not saying this is anything new, but just emphasizing that even during learning voting is still relevant. it, that's where the flash inference for the state of the whole object can help, but that inform learning. Assume that assumes, what parts of the. Object or moving and Right. Which I guess in my head it, I think it, it is fine that we would just get these islands of agreement. and alright, but now we take, it's just the same thing as looking at multiple objects, but now we take the balloon. Okay. And the and the bend and the bending staple. Yeah. Yeah. I can't, do that. I can't vote staple. so then the balloon. so the first thing was, it was two ways that I feel like maybe we're trying to make life harder than it needs to be. First one was, I think, flash inference helps a lot train learning. The second one, in terms of things like the balloon, like you, I think wrote Viviane. How, okay, it could just be like scale, but that's too simple because, distortion is more complex than scale. That, but I was just thinking do we actually model it at that level or would we just model it at a course level? Like scale as in we're talking about okay, how to make accurate predictions, but. when we look at a balloon, we're not expecting to see a particular contour at a particular angle or whatever. We're just expecting roughly as it inflates to see something that is consistent with our internal kind of predictions. So just whether, yeah, and, maybe whether this applies to a lot of the distortions that, like our behavioral predictions, unless we've spent a lot of time and become like an expert with a particular behavior, they aren't necessarily gonna be as good as I think sometimes in our thought experiments. We are projecting them to be. the balloon would be that case. The t-shirt would be that case, but, yeah, like the t-shirt. Take my staple top down. I purposefully didn't, talk about the T-shirt because the t-shirt is so complex, but the balloon is so simple that we all know how a balloon inflates and how it changes scale differently at different locations and how it would distort if something is printed on the balloon. Like we, but, do we, really, are we really able to predict that? Or is it more like if the logo, like when the logo is distorted and we look at it, we just see, oh, okay, it's a distorted logo. Like now we're more learning a new distortion. It's not that we are like, wait, this distortion was like inconsistent, if it could fall. I can imagine it in my head how it would distort. Like I don't have to be recognizing it.

okay, maybe your imagination's better than mine, but if I like imagine a logo distorting on a balloon. It's a pretty fuzzy mental picture. It's not like I have a crystal clear sense of deformed. If the checkerboard pattern printed on the balloon, how the, like which checkerboards would, get larger and which ones would not get much bigger?

the, now we're getting back to, I feel like I had to like mentally attend to the top and the bottom, and then I could roughly say yeah, okay, at the top the scale is increasing more. So I expect them to be bigger, but it's not again, I have a peer picture of acute angles are becoming two. Or you have the, so with compositional objects was a logo wrapping around the cup. 'cause that's a distortion just like the balloon. and so that was solved by not distorting the logo, but by assigning it a point by point on the morphology of the cup. So I, the, logo on the, or image on the balloon could be just like that. It's we're not really distorting the child object. We're just creating a new parent morphology and the point by point. yeah. So I get that. I think that's a, valid v Can we start with maybe Viviane, you wanna describe the balloon problem? I just make sure I Oh, yeah. just before I forget the point, the main reason why I didn't go with that solution is learning it on a point by point basis require, requires active learning and specific connectivity. So we can't reapply a behavior to a new object just out of the box. You have to learn the associations on each location. only if the balloon was really a, an a expansion of the reference frame.

and you really were a scale and you were just, maybe the balloon blowing up is really a scale change then you wouldn't have to require any new learning. Basically, the points on the balloon are now further apart and yet, and but they still correspond to the correct points on the logo or whatever's on the balloon.

yeah, a hundred percent comments, but I guess, before I pull up the slides, I'm not trying to convince do you wanna go over, do you wanna go over what you prepared on Xra first? Because I feel like if I start the slides now, it will probably end up being a longer discussion given. Sure. Yeah. that assuming might won't be a long discussion, but I only have a few images. it's in the, how do I do this? Do I have to share my screen? how do I do this Here? I share my screen. I think that would be best. Then you can point that stuff. And then I only wanna share the second screen. So I can still see all your lovely faces.

and it says I'm sharing a screen. Do you see that now? Yeah. Yeah. All right. Lemme just, I'm just trying to keep my, okay, so this is just, I'm assuming you're all seeing the same thing. You can see my cursor, or should I go to the lasso? Yeah, we can see the cursor. Okay. So here's a single column with one, or, with a learning module, this would be, it would be, let's assume it's a, a behavioral or morphology model or both. And so we start off by saying a single learning module must see every feature, location. it doesn't have to look at every location object. It has to look at ones where there are features. And and if it's a behavioral model, it has to look at every location, every active point in time, right? So that's a lot of burden to, to have a single column learn. I. I've also often felt oh, there has to be some sort of shared, learning or learning transfer or something like that. And in the past, I've talked about this second image here. I said, imagine you have a whole bunch of columns that are in the same region. Only some of 'em are beginning active input. Those are the ones with the green, let's say those are the ones with the green, squares. And others wouldn't be getting any active input. They're not observing part of the object. they're looking at some space in the object, but there's nothing there. And so one way you could do is you could say, oh, the ones that are learning could be sharing what they're learning with their neighbors. That would be speed things up. It w this column here is learning something and this column here is not getting any input for this guy says, why don't you learn what I'm learning? we'll share information and you can learn the same thing I'm learning. if you're not doing anything, why don't you be productive? that kind of thing. So I often felt like this, but I never really liked it. And, after thinking about it yesterday, I think it's not right. I think what's going on is something completely different, and that's gonna be shown in these images here. magic. Whoops. What the hell did this happen? Yeah. Sometimes it just switches to another template, another scene. How does it do that? I don't know. Scrolling around this switch. okay, so here we're now looking at typical hierarchy. This is drawn like a cortical hierarchy, but it'd be a hierarchy of learning modules. These are regions, so there's multiple learning modules in each of these layers here. This is your cluster. There gonna be many learning modules here. I'm not showing 'em, and we're just gonna make the following assumption that the things at the top of this hierarchy are very fast learning. The top is instantaneously learning. That would be like the hippocampus. And as you go down, they get slower and slower or just they're all slow. But the bottom here, things are learned very slowly. So you're not gonna learn quickly here. And there's a lot of evidence, and I've talked about this before, that when you first learn something, you learn it in a way that's modality independence. So if I, even if I learn what a cup looks like with the tip of my finger, I can visualize it immediately and I can make inference, visual inference, even though I've only touched it, which is, not consistent of learning in a single learning module, even a single region, low level region. And so it, and this sort of standard neuroscience dogma is that when when you learn something, let's assume it, it starts at the fastest. You, you have to information enters at the bottom here, but it, goes up to a series of higher series of levels. And the first learning module you learn is at the top. And this requires attention. So part of attention is routing where you want information to get to transfer. don't pass this information up. Don't pass this information up only. I wanna start here and get that information all to the top. So I'll have to go through some number learning levels here to get to the part. These guys all know something, but they're slow learning, so they're not gonna learn right away. But they already have previous knowledges. So by the time I get up here, I've already, I'm learning maybe com. I'm learning fast learning of existing compositional objects. So I walk into the dining room, I'm looking where the plates are on the table. I know what plates are with glasses and forks and the tables. So all I'm doing up here is learning the arrangement of those high objects. but it doesn't really matter what's gonna happen. You're gonna attend to something, information's gonna flow up, and you're gonna learn this model up here at the top. because this is slow learning. These modules are slow learning. They may start learning something, but they're too in the first initial thing here. They're just not gonna have enough time to do it. Now, once you've learned this model at the top, which is it in some sense, independent of where the information came up from, right? It doesn't matter if it came up from over here or here or here. it's independent of where it is on your skin or where, which modality you used. You've gotten to somehow to this mo, this higher level models. It's modality independent and location independent. Now, I can infer from a different direction. For example, I can, if I touch the object with my finger here, and now I'm looking at the object, I can infer it through vision because I follow the same channel up here and I get to the top. I'm using the same model I learned earlier, which is modality independent and location independent. so infant can use different modalities, but I still, I didn't show, I still have to attend. To say, okay, attend to the visual system here and I could now recognize the object I learned by touch. And then how do these guys learn more sophisticated models? If as you do this, these guys are all trying, as information flows up, they're all trying to learn models. They're just slow. So one shot's not gonna do it. But if you practice over and over again using a specific set of columns going up to the top, then these other modules, we'll learn models too. so if I end up spending all my time just not looking at coffee cups, but touching coffee cups, then I will get really good at touching coffee cups and, I will learn models lower down that are touch-based. so I don't, in the future, I will not need to go all the way to the top. I will not need to attend to the cup because I've learned what it touch I by touching it. So many times I've formed models down in the touch regions here that are sufficient. I don't need to attend to it. In fact, I can learn down to the lowest level here or just the lowest two levels. So that frees up my attention. I don't need to attend to the whole stack here. I can, be driving the car using these low level models while I'm thinking about some concept talking to you. So my hippocampus is involved in our conversation, but my driving is down here. So this involves no transfer learning. no models are transferred anywhere. But it does say that I have to learn multiple models at different levels. And, but I don't wanna learn multiple models at all levels. I don't even really wanna learn at the ones I've or I really experienced a lot, So I don't need to learn, what letters feel like I need to only learn what letters look like. and, and I, once I've done that, I don't have to attend to it at all. But if I wanted to like, feel what letters feel like if I wanted to learn braille for the first time, I'd have to think really hard about it and use the whole stack again. So the basic idea is there's no transfer learning, but we do need to have, we do need a way of many, learning models pick up, to learn slowly. But until they've learned slowly, I can rely on the higher level models. so I can immediately start doing what looks like transfer learning between touch and vision. But, it's not really transfer learning. I'm just, using a sort of modality independent model at the top. All right. So I've said that's the basic idea. I don't know if I've said it very well, but, this to me is a direction of how I think about learning now, and I don't have to worry about does every column have chance to see everything. So in the beginning when it just passes the information from the lowest level to the highest level, when it passes through the intermediate levels, what are they doing? are they just in outputting the same thing that they're getting as input? No, it's, it depends what they know. A let they say a child knows nothing, then, none of these things are learned at all. And, you just came out of the womb, so you'd be passing up the most basic, I don't even know what you'd be passing up. It'd be the most basic stuff.

But as after you've learned a whole bunch of stuff, as the information goes up, this guy will say, look, imagine, I am, I'm trying to learn the logo. I've never seen the logo. So I'm attending to the logo. I'm, and, and maybe I don't even know the word. So I'd attend to each letter at a time. So I don't know where letters have been learned. Maybe letters have been learned at this level, maybe letters have been learned down here. 'cause I do a lot of reading. So I would be passing up the letters and maybe then this would be like, a, a, syllable. But then I would be learning the word up top here, for the first time. so it's not like I'm passing up the raw data. The raw data can very quickly convert it to previously learned objects. But those, but in the end, you're trying to learn a new configuration of child objects. And so what child objects are passed to this top learning module? It depends on how, what you've learned before. I could have learned the whole word menta, and I could be passing up the child object, could be menta, and I'm learning the logo as a composite object of menta plus an image. Or I maybe have never learned the word menta as I might be passing up syllables. Or I may, I, maybe even poor reader, I'm passing up letters. or I, maybe I'm looking at a different language, like K and I don't even recognize the characters. And so I'm passing up individual strokes up here trying to learn what the strokes look like. so it, it just depends on what's previously been learned here. they will try to do compositional structure, whatever they've learned and pass up okay, the new arrangement of these features that are composite object and how sophisticated they are depends on what I've learned in the past. Yeah. does that make Yeah, The main thing I'm just unsure about is like that each of these layers will try to learn composition structure, and each of them will try to pool features. Into a condensed representation, like the layer for input into the layer three output.

it just seems difficult to imagine that they would pass forward basically the same thing if they haven't learned about it before. Like how, is just a stroke would be passed through multiple levels of hierarchy and, remain that, I I think there is a, you're maybe asking about a startup problem, like how does the system get started because it seems to me it would work very well once you've had some kind of learning in the system. are more the hierarchy, like where, I guess are the, to what degree there are direct connections from lower levels. Yeah. The hierarchy to the hippocampus. That's alright. I that's a great point. I didn't show that on here, but I did write about that thinking like, I've shown the green arrow has to go through all these things. it's possible that the voting layers are shared more globally. And if the voting laser shed more globally, you could skip over these layers. You basically, every, at the top here, you're saying, oh, what's, who's voting on what? But I don't know how that would work. yeah, I guess what I, definitely like this way of thinking about it. it seems to solve a lot of issues to solve, to use hierarchy and solve fast learning for it. I guess the one thing that I was thinking about differently, in this framework was that I was thinking in the beginning you would pass the lower level features. Very high up to the fast learning modules, but directly without them passing through several layers of hierarchy and trying to compose a model of objects. And then the way you learn them lower down would be top down predictions, basically, that the high, higher level, quick compositional model can top down predict, say, okay, I have stored this cup ID here, and whenever I send this cup, let, lemme challenge your first point. I wouldn't wanna send up low level features if I already know what those low level features represent. Again, I'm looking at the dinner table. I don't wanna pen up edges of plates if I know it's a plate. So why would I, should just pass up. It's a plate, it's yeah, This is like early, earlier in learning, so, maybe it is, as you say, Jeff startup issue example. Yeah. No, we. We can stick with the plate, like we know the plate already, but now we have to go through three more levels of hierarchy. what are you gonna compose in those x extra three levels? I don't, know. That's a good question. I don't know. And that's what brought up the question of is there some way of, is it, is the hierarchy strict like this? Or is there really more of a pooled understanding of what the current object is and that pooled understanding goes up? I don't know. That's a good question. we, it remains to be seen, but I didn't say this was all worked out. I did feel like this is the right direction, is what I feel.

yeah. I agree.

we could start out with, you don't need eight levels of hierarchy. You could just have three, region one, region two and hippocampus, and see what, see how it works. that was the way I'd probably start thinking about it. Yeah. In some ways we've started with the hippocampus anyways, given how fast our models currently live. Yeah. We, have, one column in the hippocampus. That's what we have right now. which is fine. like we can do all these variations that, that brain, that biology can't do. So I think it, I've never had, I've had a problem with that at all. And we can turn up and speed up and slow down learning all we want, right? We can have a single learning module say you're not learning now and other times say you're learning everything instantly. So those are all fine. yeah. At least we have hyper parameters we could use to slow down learning with the grid object models. Yeah. But I really, like this 'cause this explains a lot of stuff. it explains why you have to practice, practice, to really good at something because the memory at the lowest level of the hierarchy is very precious. And, you can't learn everything everywhere, again, like if I'm typing, the, my, the, these are very low level reputations. If I just try to type by using, shifting my hands on the keyboard one key over and say, okay, you're just starting a different home position, but you know where all the letters are. it's impossible. It's because I've learned very precisely which finger is moving in which direction for typing letters and that's it. and and also one more point, and I'll get off my, my, my horse here. I thought, I find it as I get older, it's really interesting, I'm, forgetting how to spell words, complex words. I'm always like, oh, is it this way? Is it that way? I can't remember two L's or one L. And but I can recognize the word and I can recognize if it's wrong. So it's like I have these low level memories of reading these words and I know exactly what they look like, but if you ask me to spell 'em out, which is a. it's like that requires attention. That requires like a model higher up. what is the spelling of this word that's not gonna be down in V one that's gonna be really high up in the heart cortex someplace. And and I might have forgotten those 'cause I haven't exercised that enough recently, or my memory's failing. But if you stop exercising the components and you just learn the whole thing, you can forget the components that you originally learned it with. it's just an interesting observation and I can come up with others like that too. So the point is, you can throw these really good memories, lower down, but forget the original ways you got there.

Yeah. And I like this point about that, there is still a utility to these lower level learning modules that it's hard to, get, but in terms of like the lack of attention stuff No, I think or like being, able to attack, act without as much attention. I think, as I said many times, I think V one is right where I recognize small letters. even worked or happening in V one, so there's a lot of utility there. but, anyway, I think I've said, and I guess I, I think you alluded to this, but maybe just to clarify in order for this to generalize, zero shot for a new modality, you require some amount of like co-learning of in, in the highest level, again, in this kind of startup phase of, different modalities. So that you basically understand that okay, when I feel like a contour, that this is what a contour looks like. So that basically in the highest level, it understands that those two are the same thing and it can predict somewhere. I don't know where you've given the other, I don't know where it predicts that. it doesn't have to be up the hippocampus, but, somewhere it is really clear this happens, right? you're touching an object with your blindfold on it. You can visualize it.

So, somewhere up there, there's a convergence is classic neuroscience dog. There's a convergence of modalities. we don't have to even think about it. We just, there's a convergence of learning modules. so I didn't even break these pictures up into different regions. just more there's a big collection of learning modules and you're picking out some subset of them, some subset could be part of the retina and another part of the retina, or it could be a part of the retina and part of your body skin or something like that. something low level, like how a surface or an edge feels and how it looks like. Couldn't you also just use voting connections between the lowest level to Yeah, but but it is the compositional recognition, right? So like the, so you're now looking at like a new, or like you felt like an entirely new object. So it's a new combination of contours and then now you look at it. You can recognize it and that, I'm assuming that's because basically like the input representation at the highest level or a higher level or whatever is, it is common between those. So it's it, yeah, it's okay, I'm getting contour and then I can predict either contour in, vision or in touch. And it, I guess it fits as well with I think there's some studies where they, took congenitally blind children in India, with, cataracts from birth and and or like early in childhood fixed the cataracts and then they showed like how much they struggled actually to do this versus how we would, so, like they would feel a pyramid, which they could recognize easily 'cause and then, but then they would see a pyramid and they really struggled to connect those two any more than a sphere versus, feeling a sphere versus seeing a pyramid. There's a whole book, there's, I don't remember, I read the book by a guy who went through this, who went through this. And it turns out that some people who have later in life, when they enable their vision, they have regretted it because it is, it becomes this impossible distraction for them. They can't, they can see things, but they can't internally process 'em. It's like it's, and and literally they, some people have gone crazy. I think some people have murdered, killed themselves. 'cause it was so bad. And so this guy wrote a whole book about it. He went through the process himself and tried to describe what it was like. And, but it was along those lines. it's yeah, wow. If you do this later in life, not only is it initially bad, but it may not ever get good. And it becomes a, it's this sensory input. You could see things are happening, but you can't understand them and they can't correlate them with other stuff. And it was interesting, let's not spend too much time on that one, but all I one point out was, I think this general idea is probably right, almost certainly, right? Is that there is no transfer of learning. It's just we have this scheme of which we have, we learn slowly at the bottom of, fast at the top. And then through practice we've learn relearn models lower down in the areas that we've practiced a lot. and so just to tie it back to what originally motivated it, how, would it solve, learning your behavior quickly without observing all the locations on the behavior? do you think there's some convergence of the input at the higher level? I don't know yet. I haven't even gotten that far. Now is, have I, all I'm gonna say is that there's a general tool about how to think about, learning in multiple modules and, and why I don't have to, this tool says why I can learn what a cup feels like using my right hand and then, later recognized it with my left hand and I didn't have to tree train all those columns. It just, it's just a general philosophy of how you about doing that. So I haven't applied to any specific problem yet. so I haven't thought about how it works with behavioral models. Okay, but that, I didn't miss that. no. But that would be the next thing to do. At least. At least now I have a tool to work on it. I didn't like the tool I had before. I didn't, I just didn't, I didn't think it was right. and, then, and I think we need a tool. I don't think we can solve this by just assuming that a single columnist is zipping all around the world all the time, learning everything. we can do that in Monty. That works fine. Iani, we can skip all this. in fact, I wanna make that clear. This is the brain's why we don't have to do this way in, there could be many other ways of doing this, perhaps. but at least I'm one other way of thinking about it. Okay. I'm done.

Time for balloons. What caused the balloons?

Can I, I just my thought, can I, try to like, go through how, like I'm trying to imagine myself just like learning, trying to learn a behavior by only looking through so a straw. like how that might go. I'm, not sure if it's I don't think you can. Oh, Hojae. I don't think, it's not if, the behaviors not in one go, I think even if the behavior is complicated. Let's say the behavior involves multiple things, like the staple opens, but multiple parts move in different directions. Not just one piece. I think it would be extremely difficult for you to be able to observe the behavior through a straw and then see it again looking someplace else and know it's the same behavior or might be not a different behavior because it's, that's the problem. one way it can be simplified is if it's a behavior you can control. So if you are the one opening and closing the stapler, you give yourself the supervisory signal of when you're changing the state and you can give yourself time and know where in the sequence you are. So for one, if it's like a really simple thing, like just pushing down a button that you can sense at one location basically, I think you could, it's no problem to learn it in one column, but then Yeah, if it's like a larger behavior, I think being able to control the behavior yourself might be Right. An age. If you knew, for example, that's a great observation. If you know that I know that this is the same behavior, 'cause I'm controlling it. and I can go fast and slow and back up and, I'm disabled. I'm going like this, do this or this, link you or whatever. And and then, whereas if the thing that's moving on its own and the thing's moving on its own and you don't really know if it's just a hand behavior or not a different behavior, you're like, imagine trying to, it might be how we, you learn most of the behaviors like Link can spend an hour just opening and closing and opening and closing and just going. Yeah. let's try and imagine, I'm trying to, what's the difference between someone walking, running, doing jumping jacks, five different behaviors, right? And, if those behaviors require looking at all the limbs at once, 'cause the individual limbs may be shared on different behaviors. trying to discern someone. Running versus walking versus, I don't know, moonwalking or, do a straw might be really, hard. Just you wouldn't know which one's going on. You'd see the leg is moving this way, the foot's moving that way, how do I know what's going on? Yeah, so maybe a learning module can learn like just one movement. if the stapler top is opening and like whatever is rotating, at least I can learn one, one thing. I think what Viviane said is right, if you knew every moment in time that this was the beginning of the sequence of behaviors, and I'm going through it in the same order as I've done for every other time, and you could somehow keep all the, neurons tuned to that, then yes, you could theory, you could learn a complex behavior through a straw, through one learning module. I just don't think that's not the always the case.

So I'm not saying I, disagree with this hierarchy and stuff. I think, yeah, this makes a lot of sense. But I guess in terms of, learning behaviors, I'm just trying to understand to what degree it can solve this issue of, everything's moving and essentially without some amount of voting still being necessary. I didn't, no, I I didn't say there's no voting. I think there's voting occurring at every level here. So it's more just about sharing the models. it's, two things. It's like attending to some subset of the stack of learning modules and by attending to them each, when I say attending, it's not like one learning module in each area. It's like you're attending to some area. So tho that area is gonna be voting. So you're gonna be voting on, plate or logo or whatever it is. I don't know. so there'll be voting going on, but on a restricted area of the input space. And, that, and, I guess that's it, that's all I'm gonna say about that. It's, still voting. Yeah, no, that sounds interesting though. I, like this idea, which I think is what you're saying of like with the top down biasing the voting to, to basically force that island of voting columns to be smaller in some way. that decision is happening at a higher level. I don't have the time at a higher level. I, it seems to me that, we know that, the, restriction is gonna be, to anything that's changing, right? So it's easy to detect when things' changing. that's what the inputs, the send us around inputs will tell you. They'll tell you when something is changing. And I'm not sure if it's the parent or the child level. I. But any region, a set of columns of saying, Hey, my input's changing right now.

let's go look at only the parts that are changing. So again, this is not a worked out theory for how, we learn behavioral models. I'm just saying this is the framework which I'm gonna think about it.

and so I think there's voting going on still. There's some sort of restricted attention. I've expanded attention to being not just restricted area, but doing it through a stack of regions. So you're basically saying, here's a, route this information from down here up to the top.

and voting is occurring at each level. Okay. Yeah. Thanks. I guess I'm having some trouble understanding how can we learn, higher level objects before we can learn the components that compose these higher level objects? we can't, you always have to learn. Components first, right? so may, maybe then I'm, I misunderstood, this hierarchy, it feels but, Rami, at any point in time, unless you just came out of the womb, you'll have something learned in these regions, right?

so, link or any child, has some limit to the number of component, the things they've learned. And so their composite objects are gonna be limited to what they've already learned. So if they haven't learned letters yet, they're not gonna learn words.

so, at first, so we still have to go through the slow learning of lower level objects, and then we send those lower level objects to the higher level? No, I think you can start off at the top. The very first things you imagine, I don't know. Let's imagine every column, all they, don't know anything. All they know is like edges or something. I don't know. I haven't thought through this from, but. At the very beginning, the only thing you'd learn at the top are really simple things. they just like basic shapes or something, that's all you could learn at the top. You can't learn anything else because there's no supporting structure underneath it.

so the whole system has, is slowly builds, right? This is why this is, one of the reasons it takes us so long to learn the world. it take, why did it take 20 years to become, or 18 years to become trained? It's a huge amount of time because we're slowly building up all these, these, component objects that are more and more sophisticated. I don't know if I've addressed, your question, Rami. yeah. it's just that I feel like, I could look at a car and I can see. I know that I've built a model of the car by first learning about the components of the car, and maybe after then I can forget about some of the lower level components. Once I've built a model of the car in my, mind, I don't think you would forget. You wouldn't necessarily forget them. You don't have to, if they're not really used in other, if, a model of the tire is not really used in other objects, then I may actually like cluster the whole But typically they are. Typically the world, you would've other objects, right? It almost everything would be used elsewhere. And the ones that are not used, they probably are forgetting or clustered with the model itself. Because I don't really need to learn. Just like the dispelling, example that you were, just saying, you, learn, you first learn the letters and then you, compose these, higher level of models of the whole world words. And then, after a while you can forget how they're spelled. in my mind, the spelling is this is how you built the higher level object and you've forgotten how you can spell it. And I think, I feel like, you think there's something missing? No, I, think this is how, I think this is you're basically, forgetting the lower level representations that you used to, build this. So there's, no reason you have to forget them. you might, if you never use 'em again. Yeah. but I don't, like a tire on a car, it's gonna be its own thing. 'cause it moves, it rotates. So you're never gonna, it's always gonna be its own object because it's part of a behavioral, thing.

Yeah, I agree. I suppose, like Matt, take the, let's say take the thousand brains logo, the little image. Okay. The little swoopy lines, whatever you call that.

the first time I looked at it and Viviane showed it to me, I was looking at all the lines, oh, what's the curvature? Do they intersect? I, really tended to it carefully, oh, look at the different colors. And so I, the day she showed that to me for the first time when we were debating like, which she was asking me which ones do I like? And I, oh, I'll give you my opinion, but, but now, and so that day I probably could tell you. Recall all those details, but today I can't recall any of them, but I recognize the logo. So there's an example where the details of the compositional object at one point were up in my hippocampus and I could talk about 'em, but now they're just down somewhere in the visual cortex. Yeah, that's, I, that's exactly like the spelling, problem you're just talking about. But I think most of the world's not like that. Most of the world is, I don't forget what letters are just 'cause I've learned the words. 'cause the letters are used in other words. So as long as something is either a, moving component or it's basically anything. If it's is a child of another object, whether it's static child or if it's a moving child, you're not gonna forget them because you're reusing 'em all, the time. I see a, new object with a tire and I, oh yeah, there's that tire thing again.

now there's three wheel vehicle or something like that. Yeah.

All right. I think that's enough of that we should get onto Vivian's thing.

sure. I have to stop sharing.

alright. so I didn't make a lot of slides, but I'm trying to get through the first ones rather quickly to get to the main point. but I have a few just building sites in the beginning again to recap the solution we talked about last time and show the general way I'm visualizing it. but just to preface it, I'm ignoring a lot of different cases here. Like right now I'm just visualizing the case where the sensorimotor patch is not moving and the object is moving and there's just one sensorimotor patch. And There are a lot of different cases that I'm not showing now, because they're not as relevant for this. Oh, my problem. I'm trying to point out. all right, so you make me feel inadequate. I should have done this.

I enjoy this stuff. So it was fun to make this, so I'll start with the example of the moving logo. So we have, the logo on a cup, which is a compositional object, and we have a behavior, of that logo, which could be applied to other objects. so just for modeling the, oh. For modeling the logo cup, we ba this is basically what we're sensing. We have this, logo with a cup in the world and there's a little sensorimotor patch. And that Sensorimotor patch sends, information, like it could contain movement information off the patch. In this case, there's no movement, features and changes, things that are changing. and it can send that to the columns at the lower or higher level. at the lower level, we would recognize the Thousand Brains project logo and locate where we are on that logo. and also are, we're assuming that there's some voting going on, so there's more than one patch, or, are you just assuming somehow it knows it's the logo? Yeah. This is a very simple example, just one patch. it has already learned the logo. Right now it's just doing inference. Okay. in reality that's not enough information to infer the logo from one patch, but we'll just assume it's. Yeah, so in this example, let's say we had moved this, for a couple of steps already. got it. and so like I said, I'm really just showing a small part of the whole theory just to get to the main point. Okay.

so yeah, let's say we've moved over this cup and logo a bit and we've inferred the logo, the location on the logo, and the orientation of the logo. And so we're passing up the logo as a feature to the higher level column, which models the mug and recognizes the mug. and it has stored the logo at this location and it recognizes that it is at this location on the TPP mug. it also recognized the orientation of the mug, and same way it can use that to rotate the incoming movement vectors. it can also calculate the relative orientation of the logo to the mug and. Also use that to recognize the mug. Just I wanna remind everyone, you, we've stored the, relative location of the logo at that point. 'cause the logo could be changing. So it's always on a point by point basis. You don't, store the whole logo you store at some point. Yeah. Yeah. It's basically at all of these different locations, it says, oh, here, the logo exists here. It exists. Exists. I just, the language bothers me if we don't remind yourself. Yeah. No, it's always good to clarify because, when you think of it first, you would just assign the logo to the cup. It's after we spend years trying to figure out how it works. I don't wanna use that language anymore.

sorry. okay. So simple enough, nothing moving yet. but we have a compositional object. Those are both morphology models. and then we can also have top-down projections. So knowing where we are on the mug, we can predict where we should be on the logo.

so now we have this actual moving, logo on the mug. So we have a behavior. And just for simplicity, I'm gonna remove all this rotation stuff out of the image that's still going on, but I just don't wanna have so many lines there. I also removed the pink line here because in this scenario we're not moving the sensorimotor patch anymore. we're just here and the object is moving. So basically what's happening is we have this little sensorimotor patch. It senses how the logo is moving. Sometimes it is not sensing anything. In that case, it's sensing a static feature and it goes into the morphology model. And then sometimes it is sensing some change, some movement, when the logo passes through this patch. And then that movement is passed as an input to the, behavior model. and using this sequence of sense movements, we can then, infer where we are on the, we can infer the behavior and where we are on the behavior. And then the main solution we talked about last time is that, we can then take the inverse of the movement stored here in the behavior model, send it back down here, and apply it the same way as we would apply a movement command from the sensorimotor to basically move where we expect to be on the logo. so basically, as. The logo is moving here and we have this, behavior that we recognized we can use what we know, like we can use the stored movement to move us through the space in the logo and to make predictions of when we should sense which feature on the logo. Does this make sense? This is a review, right? Yeah. This is a re review.

Makes sense to me and it's very beautiful. I think it's lovely. The only thing, I was looking at was like, really what I would do is I saw this logo like this. I would probably track the logo with my eyes, meaning, I wouldn't be fixated at one point. I would be, I'm not sure if it changes anything, but, I would be pursuing the logo and so the, what's actually being sent to be a little different, but it's the same idea. Yeah, so I, like I mentioned, I'm just showing one scenario here, right? So for example, if we were actually moving the sensorimotor patch to follow it, then we would be getting a movement input here as well. And that movement input would also, influence where we think we are on the logo. So in this case, if we're doing like smooth pursuit, that would actually cause the two to cancel out and we just keep thinking that we are on the logo mark.

okay, so that was all the review. now I would get into the issue I see with it, but if anyone ask questions here.

okay. So one, I'm gonna. Present them in the opposite order of how I wrote it in the document. just in case you read it that you don't get confused. But, so one problem is that I see here is, we still have this compositional model of the mug, but we're not moving anything in here. So what kind of predictions is this model going to make about the feature this that's coming in?

that, that model doesn't really get any information right now from the behavior. So it will keep predicting to see the logo at this location where it was stored.

so I guess one potential solution could be to also apply this movement here and move the same way. That way we would predict the logo at the correct time, but that only really works for predicting. When to expect the child object. It doesn't really work for predicting any of the other features that are not changing on the mug because we're not actually moving on the mug. We are still at the same location on the mug. We're not moving our sensorimotor, so that doesn't seem like a valid solution. another one, is that, we are actually getting input from the lower level column. oops. So that gives us some information, but we, still don't have a way to predict when we would get this input. there's no way of us being kinda surprised when we see the logo at a different location right now. second issue that I think is the same issue as, at least in my head, is like object deformation. So the example of the balloon is the simplest one I could think of where we have an object. All of like features on the object are changing their location, but not in a co coordinated way. So with the logo, all of the features are moving together, but here the features are changing in different directions. oh, hang on.

I messed up the slide. anyways, this, inflate, deflate, animation should be on this one as well. anyways, We could still have the balloon model down here and apply the behavior movement down here like that to move us through the reference frame of the balloon. But that only really works for the location we're currently sensing because if the balloon now stops in the inflated position, we can't really make accurate predictions about all the other locations anymore once we start moving our sensorimotor because the morphology has changed completely. All the features are in different locations now. we could have the balloon model in the higher level and solve it the same way as we do with the stapler, basically decomposing it into child objects or the same way as we solve it with the, with the mug. that kind of works. But then we have to decompose the balloon into a lot of different tiny child objects because all of the locations on the balloon change their, all of the features on the balloon move. in different ways. So we would have to decompose it into a bunch of different child objects, which maybe we would have to do that. But it, just seems to me, it seemed like something to point out. and yeah, I guess both of those led me to come to a similar conclusion that we need to have a way to update the morph, the locations at which a morphology model expects features within a column. So like some mechanism where we can, change the expected locations without, hierarchy. So without having to decompose it. Because there are examples where we don't have clear child objects, but it's the direct sensory input features that are changing.

I guess the potential mechanism I was thinking of was that we have, and we talked about this before, in a different context of having like a common space for all objects. When we talked about it before, it was in like, what space do we move through before we know which object we are sensing. we can't really move through like a unique location space yet.

So if we had, like a common space and then, and each location in the common space has an associative connection to each location that's unique to the object, then we could learn how to apply behaviors to that common space, distorting that space. And because of these associative connections, we could apply it to any object we've learned about before and essentially just condition at which location we should expect the feature we already associated. So we were basically moving this kind of orange bump somewhere with the behavior model. And because there are these associative connections, we would then have this unique location on the object fire at a different physical location in space or expect, this feature at a different physical location space.

yeah, I know it's a little bit out there, but, I guess that's the best thing I could come up with, over the past days. and maybe you don't, you still disagree with the problem in general?

I see the, I see there, maybe there might be really two different problems.

and the one with the cup with a logo in different positions, I. As I wrote, I think there's a, an obvious, seems obvious to me, solution to that problem that it's almost like you have to do it.

and I can describe that briefly. Yeah. I wrote it, but I'll describe it. imagine that the logo, I purposely said, imagine it moves quickly and then it stops at the top for three seconds and it moves quickly and stops at the bottom for three seconds or five seconds, whatever. Not like this slow movement, up and down, it's clear to me that you would learn two morphology objects for the, for that composition object. No one has a preference. There's no reason to assume that the logos should be in the top position or the bottom position. They're both equally valid positions. and they're just two states to the object and.

and I would learn them and I would infer them if I saw the logo with the cup, with the logo at the top, I would say, oh yeah, I know that logo. If I saw it at the bottom, say, yeah, I know that, you know that cup, I know that. And then I would expect it to move. Yeah. But the problem I see with that is, one, you can't really make predictions about any of the in-between states of how the, logos should move in between. But the bigger issue I see with it is that requires learning the specific, behavior on the specific object. So we can't just apply a behavior to, a new object. You have to learn the different key states of the object first.

let's, let me come back to that in a second because I didn't, I have to think about it in a moment, but it seems logically to me that I will do, I will definitely learn those two states and I'll be able to infer them. Yeah, I agree with that. Yeah, I guess I just, so yeah, dunno it just before I understand your objection to it, let me just describe how I think that would work. The knowledge to know, therefore the, within the column that the higher level column that represents the, mug. I have to have some sort of state which says, oh, it could be in this state or that state. It's, like those two positions of the logo can't coexist. They don't happen at the same time. I don't see H logo at the top and the bottom at the same time. I don't see the top have the left side of the logo on the bottom of the right side of the logo. They're very separate entrance. So I have to have some way of knowing I'm in A or B. And the obvious place for that to be is the state of the behavioral model. If the behavioral model reflects the movement up and then move it down and move it up and movement down, the behavioral model will tell you where you are at any point in time. And therefore the behavioral model could be a gating factor On the morphology model. It says, at this state, in the behavioral model, the morphology should look like this. And at this state in the behavioral mo ology MO would look like this. There's a con. So does that make sense so far?

So now there's a continuum, right? what if there were, five different positions of the, logo and it stopped in each one for, for so many seconds? That's one possibility that would still work. what if the logo, but then you say, why wouldn't be able to predict what it looks like in between If the logo moves really quickly, I wouldn't expect to be able to, make a prediction there in some sense, because it doesn't stop. It just moves through it. And, and so it's not an obvious thing, But what, I guess the point I wanna try to get to is that how many morphology models we learn really depends on how many morphology instances there actually are. And it's a continuum because things that are moving quickly and it doesn't stop, I wouldn't learn those. But things that do stop for a long period of time, I would learn those. and and so it's like I don't have to pick either or I basically, the system could be trying to learn morphologies whenever the behavior stops. but if it's moving through that, I wouldn't be able to learn that as a new morphology. At least not, easily. 'cause it doesn't stop at that position. so it seems to me it handles this issue very nicely of pairing within a single column, pairing the behavioral model with the morphology model. Now, the point of this is your question is that would work for the logo, but it wouldn't work for other objects. That's true. So it basically the morphology model, the only thing you could predict would be the locations and the orientations. You couldn't predict the actual feature. because, that, because if you did, if I was rely on the actual feature, then I wouldn't, be able to apply this to something else.

the behavioral model interacting with the mythology model, really the mythology model is locations and orientations, and that's what you can do in the upper learning module. the actual logo ID is helps you infer the, for the object, but it's not a requirement. So once I'm in on a lo, if I'm on a, if I'm on a mug and there's a different image and the image starts moving and I go, oh, I recognize that behavior, then the upper learning modular can predict what the position and orientation of the. Child object will be at any point in time, but it can't predict what the actual child object is. Yeah. But, with the logo, it's maybe, a bit easier to think of because, if you apply it to a new object, it's just changing the object id. But, originally I thought about this with the example of the stapler. It's just a bit harder to visualize or animate, but basically if you have the stapler opening and closing, the, lower column has like the top of the stapler that changes location and orientation. But then how does the high level column that has the model of the entire stapler, how does that column make correct predictions about where it should be sensing, the stapler top at any point in time? or if the hinge behaviors applied to a new object where we haven't learned these different states yet. I guess that would be the main question. wouldn't the high level learning module be, would be telling the low level learning module. the orientation it should be seeing at that point in time, right? You already had that as part of your Yeah, so we can do it at the lower level. We can make correct predictions about the stapler top at the lower level. But would we just say, this one doesn't make any predictions because it can't rely on the lower level. Lower level lower. which one can't make any predictions. the high level one, like the composition. even in our standard compositional objects, the higher level doesn't know what the lower level is. Yeah. But it would still, wanna predict which features are coming in from the sensorimotor. No. The only feature can it, which, so we haven't really worked through this completely, but I, we've stated it multiple times. A morphology object is really orientations at locations. That's it, that's what a morphology object is. We then can use a feature ID to help us identify the morphology object. So fact that there's a, the, parent object of the mug with the logo, just not even a moving logo, has no idea that there's a logo there.

it just says, I, what I know is that there are, I've learned that there are, there, there are points on this, on my morphology object that are, that exist at these points, at these orientations. And and I can, I will, somebody, the child object that's currently there, I will link to it, but I don't know what it is and it can send it to me and I don't know what it is, but it'll help me infer my morphology object, right? It's oh, yes, if I know there's a logo there, there was some ID that came in before. I don't know what it is. 'cause under the parent, I don't know what the ID represents. It says, this idea is usually associated with this, this point on my object here. and so that helps me clue in to what object I'm looking at. But it's not requirement. I can still recognize everything with just my morphology model. Yeah. I'm not even talking about predicting which object ID we expect. It's really more about predicting at what locations we should expect any kind of features, even just low level edges that come in from the sensorimotor directly. I thought even low level orientations and locations thought we, I thought didn't we already saw that? Didn't Niels already talk about that? That, once you've, and, maybe this is insufficient, I don't know, but I thought that once we inferred the lower level object, then the individual prediction would be made by path integration. yeah. At the lower level object, Yeah. Maybe if I can rephrase like. I think what you're yeah, I can see the problem you're raising Viviane, it's almost like in this instance when we're using the behavioral model with the lower level, it is almost like the morphological model at the higher level doesn't become relevant, or at least is less relevant. Why is that? It's almost like it, because, yeah, it's just going to not be making very good predictions, about the direct low level features coming in. It can't, it won't. Okay. okay. Maybe it just depends, like suppressed or something. it will, if the, child object stops moving and I then learn a morphology, it then did the standard, composition object thing. Yeah. So it's, almost like the, is a single column attending either to the morphology or the behavior? No, think of it this way. While the, child object is moving and it's not stopping, I can't make accurate predictions about low level features. Is that true? Even? Let me think about that.

the parent object, the parent object can't tell the child object what location it's supposed to be on, but I still don't get it. Explain what my logic here is. I have a son, imagine now I have these two columns, and let's say they have learned a set of morphologies where the behavior stops, the staple is open, or staple is closed, the logos at the top, the logos at the bottom. We have now learned two separate, morphology o objects, and they on the parent object. We know which one we're looking at based on where we are in the behavioral sequence. Okay? but I can infer either one. I confer with the staple or open the stapler down, John. Now, when the, child is actually moving, I haven't learned the, composition pairing. I haven't learned a location by location pairing between the parent and the child. that's not there now, but while it's moving, I can do path integration of the, because I'm sending in motion commands to the child. And, and the child can know what to expect to predict on its own, but the parent can't tell me that. What, what is missing in that? I'm just not. Yeah. So then the answer would basically be that, the compositional object wouldn't make any predictions unless it had learned the kind of key frame morphologies and was being invoked by the behavior model. It would, it couldn't make, right. I guess it wouldn't, I'd have to think about it a little bit more, but it wouldn't it, but it, yeah, it has to start with a, I don't like the word key frame, because I think that implies Yeah. Specific purpose. I guess we have a better, as long, we don't all think it's the, the motion picture definition of a key frame. yeah, but it's and so basically let's say we have the stapler. We learned the behavior on the stapler. We have learned several states of the stapler. Then the behavior model up here can invoke a state of the morphology model, in within the same column. And we can make correct predictions. But then if we observe something else with that behavior, like a whole puncher or something, we've never seen a whole puncher, with that behavior before. Then we have to use kind of the child object of the whole puncher. Just the top of it. we would during, so during the static moments in time, we have a morphology model and the link between the two is location of location. Yeah. During the motion portions of the time the child, the parent object can't tell the child doctor what to do, but the parent object can tell it to update itself through movement and says, so the child object knows where, what it should be sensing, not because the parent has told me, but the parent has told me how the sensorimotor is moving. Then if it stops, I start learning again. I could start once it, if it stops moving someplace, I'll say, oh, and I'll establish some links there between the two. Yeah.

Okay. Yeah.

I guess that, that is a sufficient solution for me for this problem. Just basically inhibiting the predictions, of this one. If we haven't seen that. there wouldn't be any predictions because during the movement parts, I've never established any, projections of the location in the cup to the location on the child object.

yeah. So yeah, it would basically know that once the staplers opened and in the open position and stopped that it shouldn't make predictions. It should just learn a new morphology model about it. the child object will know its position. And it can make predictions and the parent objects knows where it is. But I wouldn't, it'll, try to make predictions, but then fail and so start learning the open stapler position. and because it's stuck open or whatever for a period of time, it's open for long enough that it could actually learn it. I don't think it, at this point, imagine we have a novel object and it's moving and then it stops, maybe even stops halfway. At that point, the child object knows where it is and its orientation to the sensorimotor. The parent object knows what's going on, so everybody can make correct predictions. At this point in time, we just haven't established a connection between the two.

that is. I have, if, the parent object had gotten to that location on its own, it wouldn't be able to tell the child object what it is. But the child object at this point says, I know who I am, and the parent object says, I know where I am. And he goes, okay, let's, there's no problem here. let's start establishing connections so that in the future we could infer this position.

that this would be a way of inferring this object in this new state, by just looking at the, the child and the parent. I don't, see a hole in it yet. We're assuming they don't vote on a, behavioral state at this, point, who's not voting on a behavioral state, like the, a higher level and a lower level? no, I, there's no transfer of behavioral state to a high level. There's no voting there. the, remember the lower level module has no idea about the behavior. It has, zero idea, just like the lower level module. Never knows anything about the parent. The logo doesn't know it's on a cup. It doesn't know that.

It is a oblivious behavior state on the lower level is not sent to the higher level.

right now in this object, this thing here is we haven't defined a behavioral state for the lower level. there isn't one, the lower the logo does not, is not observing any changes to itself. So as far as it's concerned, it's a thing. It's, it knows that it's moving relative to the sensorimotor, but it's not behaving. it's not changing anything about its model. It's just, it's just at some point in the world and where it is in the world is changing, but that's not a behavior. It doesn't, in this scenario, there's only one behavioral model. It's in the parent, and it's, and it, and the parent communicates to the child via movement directions. it says you, that's how the parent communicates to the child, but the child does not know that it's moving or it's part of something else. It just says. My relationship to the sensorimotor is changing now for whatever reason. I don't know, maybe the sensor's moving or maybe I'm a child, doesn't, I don't know. Just someone's told me that the location of the sensorimotor is moving. Okay. Yeah. And so basically, this mug model up here still makes predictions about what the sensorimotor patch is sensing when there's like static input. So like when we have this, just the white patch, when the logo is not passing through the sensorimotor patch, it still makes the correct predictions. They still match the input that it gets, but then when the logo passes through the patch, it doesn't get that input anymore because now we're detecting change. So instead the input goes into the behavior model, I guess it would still predict to sense a white surface, but it doesn't get any input. So there's maybe not no prediction error in this case. I'm, confused. I don't understand. First of all, I never think we predict white space. 'cause I thought we're only predicting features that have our, that have some Yeah. or like the point normal there or whatever. That was just a bit easier to I don't, yeah, I don't even know. I don't know what we do if, yeah, it has to be something there can be just blank. yeah, the edge of the cup or something. Okay. I, and I didn't follow everything, but I, the way I'm thinking about it, think of it this way and maybe this helps, I don't know. The morphology model's always trying to learn morphology. It's always trying to learn like what point on the child is it, what point on the parent. But if it goes by quickly, you don't have time to learn anything. this wasn't enough time, so I'm not gonna learn it, but if it slowed down and stopped, then I can learn it. And and I imagine something similar is happening, the behavioral model. If things are moving, I can learn something, but if they slow down and stop, then I can't learn anything. So I don't think there's a binary thing going switch to happening necessarily happening here. The neurons are just trying to learn and if they have a chance or they will, and if they can't, they need some time to learn something. Yeah. I guess if we, yeah, just accept the constraint that if we apply a behavior to a new object, we can't instantly make predictions about a new state of it. if we have a different hinge that just opened, if a normal move to the open part, I haven't really learned what's there yet. or we could still use the child object. Child object. Yeah. I, don't stand even a novel object. It seems to me, even if a novel object, if I have a behavioral model, I'm, I'll be able to make predictions. So what am I missing there?

I.

If, you have a different hinge that you apply the opening behavior to, and the, you have the child object here for the top of the hinge. what do you mean a different hinge? I'm confused. Just be very, are we talking about a stapler still? Or I've thinking about a stapler. Let a stapler looks like a hippopotamus or something. I've been thinking about, oh, the stapler. But it has a different shape to it, yeah. Let's say the stapler. You've learned the stapler, but you've never seen it open and close, but you have a behavior model for something opening and closing. Oh, so can we say, I learned the stapler and now I'm applying it to something else? is that the equivalent? But you learned the stapler, but now you're applying the, behavior to the stapler. You've never seen the, stapler do that. isn't that equivalent to me saying, I've learned what a stapler does now I see a hippopotamus and I, and it starts to open and I. Sure. Yeah. Yeah. Okay. Yeah, it's the same thing. Okay. okay. So the hippopotamus head model is up here. and so we know how it looks like with the closed mouth and then it, it opens its mouth. We, we wouldn't really be able to make predictions yet about like, where the new locations will be. like where will the, why not? I wouldn't be able to where the predictions are, but I would be able to make sensory predictions. I would be able to predict what to see when I get there. Because, but you'd have to use the child object for that, Right. What's wrong with that? I basically turned the head of the hippopotamus, it's the child object, and I've now basically changed its position relative to the sensorimotor, and I've changed its orientation relative to the sensorimotor, so now I can make predictions about it. Yeah. But so then, we're saying that this compositional model of the hippopotamus head. Would not be making any predictions at this point? At that point, no. But as soon as it, as soon as it stops moving, I could start learning a compositional object. Yeah. You'd have to learn it first. You couldn't make predict, like once it stops moving, you couldn't make predictions about the morphology in this new state. Unless you have, you, you, learn that new state now, I'm still confused as the head of the hypothesis is moving, and it stops, the lower column will make correct sensory inputs and predictions about what it's seeing. There's prediction, there's nothing that's unex, there's nothing I don't know.

but then, but the higher level column, the compositional model can only make correct predictions after it has learned the morphology of the whole head in the stage. Think of it this way, if I didn't see the head move. On the hippopotamus, then the top level model would not be able to predict what the object's gonna look like. If it was open, it, I'd have to, I'd have to know there's a behavior and so once I start recognizing the behavior, then I could recognize it in the open position. put it this way, I recognize the hippopotamus with the mouth closed, but if I see the mouth open, I would say that's not the same thing. It said something different here. unless guess I know about mouth. So that's hard to, there's, I guess there's two ways that you can establish a relationship between a par parent and a child. There's the static way, which we learned the connections from layer six to layer six. That's the one way. And there's dynamic way where, was the movement. And I can, based on the movement, I can, now predict where the child would be relative to the parent. Those are my two options. they both work well. if I ever wanna just infer the object in some position I will have, without any movement, I will have had to establish that child-parent relationship through the morphology connections. But there, but generally there's two ways. If there's a behavior, then I can, make all the predictions I want. There's no, I guess I don't see a problem here. Okay. Yeah, no. Yeah. Maybe that sounds good. nice. And yeah, maybe that's a good, stopping point. is it, or can we talk about balloon for a second?

I don't mind talking for a bit longer if, and yeah, I guess people can always drop out if they need to. It's recorded, so I don't know if this is the exact same problem or not. I think it might be a different problem. No, like we couldn't apply the same solution to this. I would say. So in, in that sense, it's a different problem, right? Unless we say that we decompose the balloon into a bunch of little child objects, then we can apply the same solutions solution. this is, in one hand you're tempted to say, oh, isn't just the same shape, just different scales, And, yeah, but I tried to show that with this kind of model here, it's, not, I drew this like very exactly from this animation, that like the way it expands and contracts like the, arrows at the top are much larger. versus at the bottom they're really tiny. And that's just because you've anchored the reference frame to the bottom right, which I, not against, but that's why that's the case. If I just had a circle that was going small to large, the small to large, I wouldn't. I wouldn't anchor it at the bottom of the circle. I could, but I would typically think about the center of the circle as the stationary point. And then, or I could show it. those, those arrows just show what like a small sensorimotor patch would perceive as the my point is motion in that area because, you're not showing the bottom, the, bottom of the balloon is stationary. Yeah. It's not moving out. And that's why you see more motion at the top than at the bottom. So I'm, there's nothing wrong with that. I'm just pointing out if I had showed you a circle expense, this is not a counter argument. This is just filling out other issues here. If I showed you a circle that was getting larger and smaller, how would I perceive it if, the bottom of the circle was sitting on top of a desk? I would perceive it like the balloon. If the circle was sitting out in space and it seemed to expand and contract on the center point, I would think of it as different than I'd say, oh, the expansion's happening everywhere. That's purely a scale change. Here you've got this not exactly circle, it's like a balloon shape and it's fixed at one point. so it's more complicated. yeah, you can think, this is like I, I wrote in the write up, you can think of lots of other cases where you can just say, all right, maybe you just, roughly apply scale, like with the balloon. Once it's inflated, you can poke it in any place in it. I understand. I don't, I just, the only reason I bring up scale is it can play a role sometimes. Yeah. it, there are times I think we can look at two objects, say, oh, that's just a bigger one, that's a smaller one. And in fact, they could be mixed together. You could say, oh, that's a bigger version with some sort of deformation. And that's, like, literally the, A mug could be on different scales on your retina.

I don't know, it's just scale could play a role in some situations. It's not like everything has to be handled without scale. There, there clearly we, we use scale at times.

a related problem, which is a little bit simpler perhaps, and what I thought about a lot in the past is imagining a circle that turns to an oval that turns back to a circle. It just oscillates. Yeah. Just like you're doing here. It's a little bit, it's the same problem, but it's a little bit cleaner. because you don't have to think about, oh, there's a little tied end here, and it's, it's an odd shape. It's, it might get at the same basic issue. It's where's, how do I learn that morphology? yeah. How do I learn that? Yeah. I was thinking first about visualizing like a ball that bounces and, I, went through all these a while ago, years ago, I think. and, but I ended up settling on the circle to oval. because that was like the simplest one I could think of that, that captured almost all the problems. But you could use anyone anyway, that's a little bit more pure. 'cause I can't say it's not inflating, it doesn't even have to be getting larger or smaller.

it's got some shape to it. that, yeah. That's a, maybe a even cleaner example. I only bring it up 'cause it's cleaner and it's one I've thought about. So yeah. I have a preference for it. Just less easy to refer to. You can't just call it the balloon. it's just sort like it's getting at the abstract essence of you've got this, you've got this changing morphology that it, every point, it's like, what's changing? It's the orientation of each piece in the location, I don't know. It's, and it's continuous, right? That's the issue you're really getting at here. There's a continuous change in the morphology. There isn't like a piece moving relative to a piece. It's like there's this continuous flow of change on every point. 'cause you can imagine the circle becoming an oval one way, then switching it down and become an oval the other way. And then, changing accesses, yeah. So there's no point that's not sub changing it, but different than its neighbor. Yeah. So that's why I was thinking of like basically distorting grid cell space by applying like slight phase targeted phase shifts or something like that. 'cause then that would be like a continuous shift that can be, but can be targeted to basically expect one, place cell to fire in a slightly different location. but yeah, I don't know if. I don't like it, So take the oval in the circle. Okay? So imagine, now imagine that I have a circle. It turns into an oval, horizontal direction, and it goes back to a circle, and it turns an oval in the vertical direction, and it stops in those three points. So now I have three behavioral models, of the, of that object. there are two, three morphology models. I could associate 'em with different states in the behavioral model. but, I, what I, if I were to distort the, if I were to distort the grid, which, would be the base and which would be the distorted one, is the circle, the base, and, then the ovals distorted. Or what if I showed you the oval first? Then it turns into a circle. So when I start with the, grid, that's that, it's rect on the oval and then it becomes distorted for the circle. I guess it, it doesn't matter. You could learn it both ways, right? I guess I, I'm trying to make. I have a gut dislike of distorting reference frame, yeah. I also didn't like it, in general, but I guess the reason why I thought it might work is because like we could, the reason why I didn't like it, is because it would require a lot of learning, a lot of very targeted learning of how you distort the space, which is not feasible if you wanna apply a behavior to any object. but if we would have a common space where we can once do this very targeted learning and learn how that space can be distorted, with different kind of movements in the behavior, then we could just, through these associations to, to the unique location space of each object, basically distort any object and we can even anchor them at different locations on the object. And then, yeah, start different places on the object. It's not clear to me this is really better than learning multiple morphologies. I, in some sense, you're gonna look, it would work to apply it to any object at any point in time. you can think of a, but, think about it, the distort the balloon is pretty, I could come up with examples where the distortions balloon, like it distortions are very local and therefore there's a lot of, imagine I have one of those squeegee toys where when you squeeZ it, all the eyes pop out. You know what I'm talking about? Yeah. The little antenna come outta his head. Imagine. So I've got this, it looks like a coronavirus thing, right? And you squeezing all little, all these pop out, right? am I gonna learn that with, how am I gonna learn that with a distorted reference frame? that's crazy.

that seems hard. I would, I don't, it's, the distorted reference frames make sense when you look at something simple like this balloon or circle, but.

the little ski Yeah. Exploring head thing I don't think would work very well. Yeah. I'm also not sure yet, I would have to think a lot more through the mechanism to know if this would work in general for even larger distortions like you mentioned, but I just couldn't think of any other way, like just learning you, you can't just apply it to a new object. Why can't we apply the behavioral model to any behavioral model, to a new object and achieve the same result? I understand that. we can do it if we can have child and parent object relationship and use this solution we, talked about before. but if we don't really have child-parent relationships, like if we don't wanna decompose the balloon into a bunch of small pieces, then that's I don't see how code works. I think poetry you applied within a column, so I, let's go back even simpler. I asked myself once, how do I recognize a circle? What's the model of a circle, right? So it's got features at locations, and those features are continuously changing. the orientation of the features are continuously changing at these different locations. And, where do I break it up? When do I say, it, it's like an integration problem, right? We've defined a model where we have unique points and unique orientations, unique features, but now we have one that's continuously changing. There's nothing unique anywhere. There's no point you can say, this is where the feature changes, or this is where the, the orientation changes, it's continuous. So this is a pure, simple morphology model of a circle. I think if we solve that, we'll solve this problem. I. I can convince myself that our morphology model works for circles. I would say you would basically, if you, if your sensorimotor sensors a circle, it would still like, sense the curve, like an edge. Yeah. but how many features do I store? And how do I interpretate between them?

I guess you just, maybe distance, maybe a related kind of thing. Yeah. It feels like in this setting, a combination of, attending to regions when it's particularly relevant, I don't know, a logo that's being distorted on the balloon or something. And simpler things like scale and stuff could be enough to predict probably as well as we actually do.

are you talking about the, like the attending. I'm talking about the balloon, but, basically I think it's a similar to what you're saying, which is that, yeah, we need to figure out what is the granularity of representation, but it's not necessarily like the fundamentally different, way of representing it.

So here's a, we'll leave you, we'll have to answer some, but we'll leave you another problem to think about. This is the one that really bothered me. So I was thinking about circles and how I'd represent a circle that's simple. Morphology my lip circle then, and ma and it and that, and it's just basically there's no, there's nothing else. There's no features. It's no color, there's no line thickness. It's just ness, and then it can be made of anything. Then I ask myself, what's an egg shape? Not an egg, but an egg shape. it's kind of circle like everywhere, but it's not a circle. at some point my circle becomes a little distorted, and now it's like a little. It's like an, it's not even an oval. It's, like an egg shape. And, how do I recognize that? How do I know that because this at different locations, like when does the circle become an egg? that's, that sort of tells you something about the, granularity of the problem. You mean like on the continuum as in you could have a morphology model for an egg and a morphology model for a circle, but what are the morphology models for the in between? it like, and when does it become one versus the other? if I show you a circle and now I slightly distorted it and maybe wouldn't notice and I slightly distorted it again, you might know at some point you start saying, oh, I think it's not a circle anymore. That gives you something, some clues to the granularity of the representation. I would say you, you just have a model of a circle and a model of an egg, and then when you recognize it, you recognize whichever fits better with the features. let's say maybe I don't even have the egg anymore. I just have the circle. When do I say that circle's not regular. it's like when you start failing to predict what you're gonna see and, But it's it just gets you, I'm not saying this is an impossible problem. Maybe our models work for it, but it teases apart some of the issues. It's then I say, what is it? Is that the location of the feature that's, important or is it the change in the curvature that's important, or both. I don't know. but, anyway, I'm, I was just trying to like, how do we, it seems like recognizing a circle and an oval and an egg are such simple shapes to recognize, but I don't wanna have this super complex model to recognize 'em. I don't wanna have a model that has thousands of points on it. Somehow it seems like that doesn't seem right.

yeah, I think that's in general a weird thing that we can maybe discuss another time, but just like how we represent surfaces and everything right now is of course very dense in terms of, The, maybe that's, the lesson for today. Maybe that's the walkaway for today, and that gets to the balloon problem. How do we recognize surfaces without being dense? I feel like we already solved that problem, at least some, how do we do without being dense? Like when we store points, we don't store thousands of points, on a circle just because the features aren't changing. We, we just have some, criteria of when we put in a new point. So for example, after the point, normal has, changed a certain amount. We put down a new point, so basically. If the current, point in the model has a point normal pointing in this direction, and then we have another one at points like 20 degree rot rotated, then we would lay down a new point because it's, the features are sufficiently different. And then also if there's a sufficient distance between two points, we store two points. but what I, my interesting question would be if I, drew a circle and you wanted to learn the circle that way, and then, then you sort the points using your algorithm. So we don't do it densely. Yeah. Now, could I show you a hundred sided, polygon that it would say that's the same object and what you mean that the, it's like a circle, but all of it are straight edges. I'm trying to, I'm testing the model saying, okay, you've got this model where you've, used this formula. There's no I interpolation Really? Yeah. I think it would work because it's still, but it would have lower evidence for it. So it wouldn't say with the same con. No, but I think that's a good thing. It wouldn't say with the same confidence of an actual circle, that this is definitely a circle. It'll say, yeah, I'm getting circle vibes, but maybe I'm gonna learn this as a new object. Every point is correct. Isn't that right? Isn't every point correct. the location, if every point is curved features don't match like the curvature. Oh, you're is different. Oh, you're storing the curvature at that point? Not the, not just the tangent. Yeah, both. alright, you're right. Sorry about that. I was thinking you storing the tangent, the normal is correct. Yeah, the normal and the, the post basically. So the location and orientation would stay the same, but the feature would be different. The, actual amount of curvature. but if you sample points on the, so if you have the a hundred side, the a hundred sided ones, any sample points, because the way that we calculate the curvatures from points, so sample points on the circle and we still calculate the same curvature. It's going to be the same. I didn't follow that, Ron. you're saying if we, you're saying we're not how we calculate the curvature of, in Monte, we have points and we, get the curvature from there or, yeah, but we said we have a small patch. It would probably extract like a, that a sort of straight line. If the patch is sufficiently small, if the patch is like covering half of the, thing, then it would extract a curve because, like problem too many sides, then even a small patch would still, you'll still sample points that will give you the right curvature. So even Monty wouldn't know. So I, guess the, point that Jeff is raising is that there is a resolution problem where if you just make the circle, have. Too many sites, then even Monty will get confused where the No, to me that sounds like a circle on a computer screen, which is made of pixels. Yeah. that sounds like perceptually it would be a perfect circle, but, even though it's made of straight edges. Yeah, And also, the morphology would match. so yeah, we, would kinda get confused because we would also get confused if the edges are too many. and again, I feel like if it's somewhere in between, like there's lots of edges, but it's not quite a circle, like some, I don't know, send te Han or whatever it's called, then I think Monty would be like, yeah, I'm getting high evidence for circle, but not the same as like a prototypical circle. And you could start learning as a new model, but you would also be able to say if queried yeah, this is similar to a circle, which is I think what we'd want. Yeah. But yeah, anyways. I don't think this is, at least it's not the problem that I was trying to get at. But yeah. I'm getting hungry, so maybe we Alright, Yeah. A pause if nothing else. I think it's a, good sign that we're moving from like staplers to things like balloons and, I squishy objects 'cause that means we're, or yeah, we're, really solving the hard problems now. Yeah. Solving more problems than we set out to. we always, the latest problem always seems like the hard problem, so it's like really hard problems we've already solved. They don't seem hard anymore.