[
    {
        "text": "Yeah.",
        "start": 6.959,
        "duration": 0.3
    },
    {
        "text": "Thanks everyone for, Coming.",
        "start": 7.259,
        "duration": 2.625
    },
    {
        "text": "so today I'm going to be talking\nabout, transformer networks, and how",
        "start": 10.424,
        "duration": 5.4
    },
    {
        "text": "they relate to Monty, what kind of,\nsimilarities there might be between",
        "start": 15.824,
        "duration": 4.11
    },
    {
        "text": "them, and, kind of key differences.",
        "start": 19.934,
        "duration": 2.37
    },
    {
        "text": "and yeah, obviously, ask any\nquestions as we're going along,",
        "start": 23.264,
        "duration": 3.97
    },
    {
        "text": "as always, and I guess, yeah.",
        "start": 27.724,
        "duration": 1.86
    },
    {
        "text": "What's the aim of this\nto, see if we can help?",
        "start": 30.274,
        "duration": 2.61
    },
    {
        "text": "Yeah, so there's a couple of aims.",
        "start": 33.374,
        "duration": 2.06
    },
    {
        "text": "So the first aim is.",
        "start": 35.444,
        "duration": 1.12
    },
    {
        "text": "identify possible cross pollination\nbetween these, That's right.",
        "start": 37.369,
        "duration": 4.77
    },
    {
        "text": "what can we do to, maybe improve\ntransformer networks while we continue",
        "start": 44.059,
        "duration": 4.57
    },
    {
        "text": "to create the true Monty system?",
        "start": 48.629,
        "duration": 3.13
    },
    {
        "text": "as well as, maybe there's some,\nideas from transformers that,",
        "start": 53.559,
        "duration": 3.26
    },
    {
        "text": "would be useful for, Monty.",
        "start": 57.099,
        "duration": 1.72
    },
    {
        "text": "In general, in the discussion,\nI'm going to focus on, the current",
        "start": 59.219,
        "duration": 2.79
    },
    {
        "text": "implementation of Monty as it stands,\nrather than, the thousand brains",
        "start": 62.059,
        "duration": 3.67
    },
    {
        "text": "theory as it, generally, exists.",
        "start": 65.749,
        "duration": 3.19
    },
    {
        "text": "and the, main reason for that is\nit just makes it easier to talk",
        "start": 69.529,
        "duration": 2.57
    },
    {
        "text": "about, concrete details and, draw,\nmore, yeah, explicit parallels.",
        "start": 72.109,
        "duration": 5.83
    },
    {
        "text": "and then just in general, yeah, we\nidentified at the last by the bay that",
        "start": 80.544,
        "duration": 3.5
    },
    {
        "text": "we want to improve communication between\nMonty and Mega team and just general",
        "start": 84.044,
        "duration": 3.93
    },
    {
        "text": "understanding of what we're each working\non and get influence from each other.",
        "start": 87.974,
        "duration": 3.89
    },
    {
        "text": "And if, Monty is a big, epic castle on\na mountain, Mega is this transformer.",
        "start": 92.264,
        "duration": 7.21
    },
    {
        "text": "And we want this kind of, I don't\nknow, cyberpunk castle on a mountain.",
        "start": 99.964,
        "duration": 3.87
    },
    {
        "text": "All right, that's a stretch.",
        "start": 105.154,
        "duration": 1.05
    },
    {
        "text": "And then, Did you use thermal\ndiffusion to change that picture?",
        "start": 108.234,
        "duration": 2.72
    },
    {
        "text": "It was a Dall-e 2.",
        "start": 111.644,
        "duration": 1.65
    },
    {
        "text": "and then I think it'll also just be\ninteresting to maybe contextualize",
        "start": 117.629,
        "duration": 3.38
    },
    {
        "text": "some of the recent games and, hype\naround transformers, if they have just,",
        "start": 121.019,
        "duration": 6.86
    },
    {
        "text": "through their, design incorporated some\nconcepts from, the thousand brains,",
        "start": 128.129,
        "duration": 4.89
    },
    {
        "text": "then, maybe that explains why, they have\nbeen able to achieve certain things,",
        "start": 133.329,
        "duration": 4.02
    },
    {
        "text": "but of course are still, limited.",
        "start": 137.349,
        "duration": 1.71
    },
    {
        "text": "and, yeah, in terms of limitations,\nobviously, there's many, but, just",
        "start": 140.544,
        "duration": 3.74
    },
    {
        "text": "the sheer amount of data that these\nthings require, all this, it might be",
        "start": 144.284,
        "duration": 4.28
    },
    {
        "text": "possible to understand, a bit better.",
        "start": 148.754,
        "duration": 2.4
    },
    {
        "text": "And, on this talk, topic as well\nis just recent, work in this area.",
        "start": 151.584,
        "duration": 5.44
    },
    {
        "text": "transformers being used as embodied\nsystems is becoming, an active",
        "start": 158.464,
        "duration": 4.68
    },
    {
        "text": "area of research, and so I'll be\ntalking about a couple papers.",
        "start": 163.144,
        "duration": 3.17
    },
    {
        "text": "in that area.",
        "start": 167.199,
        "duration": 0.62
    },
    {
        "text": "so just a quick preview, so these\nare like some of the similarities",
        "start": 170.519,
        "duration": 3.63
    },
    {
        "text": "that I'm gonna touch upon.",
        "start": 174.149,
        "duration": 1.43
    },
    {
        "text": "It's important to emphasize that\nsimilarity does not mean equivalence.",
        "start": 175.979,
        "duration": 3.13
    },
    {
        "text": "I'm not saying that these things are\nidentical, but that these are just some,",
        "start": 179.149,
        "duration": 3.89
    },
    {
        "text": "kind of things where you can definitely\nsee, relationships between the two.",
        "start": 184.139,
        "duration": 3.41
    },
    {
        "text": "yeah, the connectivity, that\nexists, this kind of concept of,",
        "start": 188.349,
        "duration": 3.95
    },
    {
        "text": "a common representational format.",
        "start": 192.829,
        "duration": 1.65
    },
    {
        "text": ". how kind of voting operates, and how that\nrelates to self attention, which will",
        "start": 195.529,
        "duration": 4.805
    },
    {
        "text": "be the thing I spend the most time on.",
        "start": 200.334,
        "duration": 2.74
    },
    {
        "text": "kind of reference frames and how that\nmight relate to positional encodings",
        "start": 204.884,
        "duration": 3.41
    },
    {
        "text": "and transformers, and then, yeah,\nembodiment, which as mentioned is now a",
        "start": 208.294,
        "duration": 3.62
    },
    {
        "text": "topic of research in, with transformers.",
        "start": 211.914,
        "duration": 2.8
    },
    {
        "text": "but obviously there are many\ndifferences, and, Yeah, although",
        "start": 216.114,
        "duration": 3.775
    },
    {
        "text": "this list isn't exhaustive, the\nconnectivity can be similar, but",
        "start": 219.889,
        "duration": 3.42
    },
    {
        "text": "it's also different in many ways.",
        "start": 223.309,
        "duration": 1.89
    },
    {
        "text": "of course, the kind of, whether\nthere's explicit, object models, is a",
        "start": 225.919,
        "duration": 4.49
    },
    {
        "text": "huge one, and how learning is taking\nplace is another really big one.",
        "start": 230.409,
        "duration": 3.15
    },
    {
        "text": "and then, yeah, when I talk about\nembodiment, you'll see some of the",
        "start": 234.719,
        "duration": 2.86
    },
    {
        "text": "approaches that are currently being\nadopted with transformers are just a",
        "start": 237.579,
        "duration": 2.58
    },
    {
        "text": "bit Kind of curious, if you compare\nit to the thousand brains mentality.",
        "start": 240.159,
        "duration": 5.48
    },
    {
        "text": "before I get into it, so I guess,\nI'm, hoping everyone's fairly",
        "start": 249.349,
        "duration": 3.88
    },
    {
        "text": "familiar with Monty and fairly\nfamiliar with transformers, but",
        "start": 253.229,
        "duration": 2.43
    },
    {
        "text": "I will just cover them a bit.",
        "start": 255.659,
        "duration": 1.34
    },
    {
        "text": "Obviously, if, either of them are\nmore new to you, then, stop me and we",
        "start": 257.369,
        "duration": 4.85
    },
    {
        "text": "can go through the, in more detail.",
        "start": 262.219,
        "duration": 1.82
    },
    {
        "text": "But yeah, just to emphasize that, with\nthe kind of learning modules in Monty",
        "start": 264.879,
        "duration": 3.96
    },
    {
        "text": "at the moment, we're getting these,\nfeatures, with a pose, to update the kind",
        "start": 268.839,
        "duration": 7.13
    },
    {
        "text": "of evidence for these, explicit, kind of\ngraph like models, and in order to process",
        "start": 275.969,
        "duration": 5.62
    },
    {
        "text": "that, we have kind of a buffer of, of\nwhere we were before, and given kind of",
        "start": 281.589,
        "duration": 4.71
    },
    {
        "text": "our new, position, how we move through\nspace, that gives us a displacement",
        "start": 286.299,
        "duration": 3.7
    },
    {
        "text": "that tells us where we're moving.",
        "start": 290.539,
        "duration": 1.66
    },
    {
        "text": "Each hypothesis along this kind of\nspace, and these hypotheses are based",
        "start": 292.554,
        "duration": 3.44
    },
    {
        "text": "on our kind of memory of these graphs.",
        "start": 295.994,
        "duration": 1.55
    },
    {
        "text": "These hypotheses can also be used\nto vote with other learning modules.",
        "start": 298.484,
        "duration": 3.03
    },
    {
        "text": "And then we can output essentially\na pose of, where we, the kind of",
        "start": 302.114,
        "duration": 5.46
    },
    {
        "text": "object, and, the pose of that object,\nas well as, potentially an action.",
        "start": 307.574,
        "duration": 4.69
    },
    {
        "text": "and then a transformer, is, composed\nof these, kind of key blocks, repeated",
        "start": 316.164,
        "duration": 7.28
    },
    {
        "text": "throughout, and really the kind of key\nthing that makes transformers, what they",
        "start": 323.454,
        "duration": 4.85
    },
    {
        "text": "are is, this self attention operation.",
        "start": 328.304,
        "duration": 1.92
    },
    {
        "text": "And so I'll, I will go into a bit\nof detail about how this works.",
        "start": 330.224,
        "duration": 3.81
    },
    {
        "text": "just so everyone's on the same\npage, but I'll do that later",
        "start": 335.014,
        "duration": 2.16
    },
    {
        "text": "when I talk about, voting.",
        "start": 337.174,
        "duration": 1.54
    },
    {
        "text": "But the kind of general, thing to, be\naware of is, yeah, these were created",
        "start": 339.644,
        "duration": 4.82
    },
    {
        "text": "in the context of natural language\nprocessing, so where you'd have a series",
        "start": 344.464,
        "duration": 3.1
    },
    {
        "text": "of token representation, so for example,\nwords in a sentence, these are processed",
        "start": 347.564,
        "duration": 6.575
    },
    {
        "text": "in parallel, by the self attention\nmechanism, which is essentially comparing",
        "start": 354.139,
        "duration": 3.79
    },
    {
        "text": "all of these words, and how they relate\nto one another, that gives you this kind",
        "start": 357.929,
        "duration": 4.53
    },
    {
        "text": "of new output, which, will be done for\nkind of many different sets of weights.",
        "start": 362.459,
        "duration": 4.58
    },
    {
        "text": "And then this kind of feedforward\nsystem is just a way of",
        "start": 367.889,
        "duration": 2.57
    },
    {
        "text": "combining that information.",
        "start": 370.659,
        "duration": 1.24
    },
    {
        "text": "and then you essentially just repeat that.",
        "start": 372.789,
        "duration": 1.47
    },
    {
        "text": "But you get this kind of, stream\nwhere you're just, any given",
        "start": 374.839,
        "duration": 5.26
    },
    {
        "text": "token is passing through, in this\nkind of given stream on its own.",
        "start": 380.119,
        "duration": 4.59
    },
    {
        "text": "But then whenever there's a self\nattention operation, it's, that's where",
        "start": 384.749,
        "duration": 2.98
    },
    {
        "text": "it's looking at the information around\nit and combining that information.",
        "start": 387.819,
        "duration": 3.84
    },
    {
        "text": "So this is very different from kind of\nthe typical fan in and CNNs and so forth.",
        "start": 392.379,
        "duration": 4.06
    },
    {
        "text": "And then it's also just important to note\nthat with transformers at the input you",
        "start": 399.599,
        "duration": 3.55
    },
    {
        "text": "have this positional encoding, which,\nbecause, yeah, otherwise the sequence,",
        "start": 403.159,
        "duration": 6.27
    },
    {
        "text": "there's no information in\nthe kind of where these items",
        "start": 411.649,
        "duration": 5.73
    },
    {
        "text": "are relative to one another.",
        "start": 417.379,
        "duration": 1.21
    },
    {
        "text": "They're going to be processed in parallel\nall at the same time, so you need to",
        "start": 418.969,
        "duration": 3.14
    },
    {
        "text": "give some sort of positional information.",
        "start": 423.099,
        "duration": 1.78
    },
    {
        "text": "and for example, in the original paper,\nthis was using, some kind of sinusoidal,",
        "start": 425.864,
        "duration": 3.85
    },
    {
        "text": "and cosine, functions, and was literally\nadded to, the vector of the, feature",
        "start": 430.974,
        "duration": 6.56
    },
    {
        "text": "representation, to capture that.",
        "start": 437.534,
        "duration": 2.41
    },
    {
        "text": "How would encoding be encoded\nin a sinusoidal or a cosine?",
        "start": 440.484,
        "duration": 4.01
    },
    {
        "text": "I don't understand.",
        "start": 445.114,
        "duration": 0.72
    },
    {
        "text": "essentially, it's you can imagine,\nit's a little like grid cells, to be",
        "start": 446.204,
        "duration": 4.34
    },
    {
        "text": "honest, where you imagine you have a\nbunch of, sine functions of different",
        "start": 450.544,
        "duration": 3.47
    },
    {
        "text": "frequency, And so let's say you're the\nfourth element of, the sentence, then,",
        "start": 454.014,
        "duration": 6.625
    },
    {
        "text": "you are going to plug in four into\nthese different sinusoidal functions.",
        "start": 461.779,
        "duration": 4.94
    },
    {
        "text": "Those will give different amplitudes,\ndepending on their kind of frequency.",
        "start": 466.769,
        "duration": 4.87
    },
    {
        "text": "And, it's those values that are, form a\nvector, and that's what's, what's used.",
        "start": 472.189,
        "duration": 7.53
    },
    {
        "text": "So is it, does that vector\nactually represent a position",
        "start": 481.529,
        "duration": 3.06
    },
    {
        "text": "in the, in a sequence of words?",
        "start": 484.599,
        "duration": 2.14
    },
    {
        "text": "Yeah, so it, it represents\na, an absolute position.",
        "start": 487.849,
        "duration": 3.45
    },
    {
        "text": "Yeah.",
        "start": 492.089,
        "duration": 0.42
    },
    {
        "text": "So the sinusoidal line Or you can,\ninfer, a bit like how from, a set of",
        "start": 493.079,
        "duration": 5.15
    },
    {
        "text": "grid cell modules, you can infer from\nthese, functions where your position is.",
        "start": 498.229,
        "duration": 4.91
    },
    {
        "text": "Yeah.",
        "start": 503.139,
        "duration": 0.279
    },
    {
        "text": "Okay.",
        "start": 504.799,
        "duration": 0.38
    },
    {
        "text": "And the reason they chose that actually\nwas because, yeah, it can, that works",
        "start": 505.659,
        "duration": 4.15
    },
    {
        "text": "for, sequences of arbitrary length.",
        "start": 509.809,
        "duration": 2.1
    },
    {
        "text": "And it sounds like nearby positions will\nalso have slightly similar encodings.",
        "start": 512.739,
        "duration": 4.87
    },
    {
        "text": "Yeah, exactly.",
        "start": 519.639,
        "duration": 0.8
    },
    {
        "text": "I guess it would be interesting sometime\nto understand that mechanism, more",
        "start": 521.069,
        "duration": 3.72
    },
    {
        "text": "explicitly because you say they're,\nit's a little bit like grid cells are",
        "start": 524.789,
        "duration": 4.1
    },
    {
        "text": "adding sinusoids, but those sinusoids and\ngrid cells are representing vectors of",
        "start": 528.889,
        "duration": 3.96
    },
    {
        "text": "movement, and, and you're trying to update\na static, a representational location.",
        "start": 532.939,
        "duration": 6.42
    },
    {
        "text": "I just don't know how parallel that is.",
        "start": 539.359,
        "duration": 2.01
    },
    {
        "text": "I don't know if we can get into\nit now, but it'd be interesting.",
        "start": 544.044,
        "duration": 2.0
    },
    {
        "text": "Yeah, I feel like it's, from\nmy understanding of it, of how",
        "start": 546.044,
        "duration": 4.79
    },
    {
        "text": "it's implemented, at least,\nI think it's pretty similar.",
        "start": 550.844,
        "duration": 1.93
    },
    {
        "text": "I think, the key thing is it's one\ndimensional, and this is like a common",
        "start": 553.074,
        "duration": 3.29
    },
    {
        "text": "theme throughout transformers that, I\ndon't know if it's because they originated",
        "start": 556.364,
        "duration": 5.04
    },
    {
        "text": "in natural language processing, but\neven when, 2D, images are processed.",
        "start": 561.424,
        "duration": 7.415
    },
    {
        "text": "They tend to just treat it as a\nraster, and use one dimensional,",
        "start": 568.849,
        "duration": 3.86
    },
    {
        "text": "embeddings rather than, 2D.",
        "start": 573.239,
        "duration": 2.59
    },
    {
        "text": "I don't want to get on topic here\ntoo much, but this might be a very",
        "start": 576.129,
        "duration": 2.73
    },
    {
        "text": "important aspect of the whole idea you're\npresenting today is, it's one dimensional.",
        "start": 578.859,
        "duration": 5.79
    },
    {
        "text": "But, there's different ways, but it's\none dimensional, it's not clear if",
        "start": 585.064,
        "duration": 6.46
    },
    {
        "text": "they're representing, is it just an\nordinal, order, like first, second,",
        "start": 591.664,
        "duration": 4.63
    },
    {
        "text": "third, and fourth, or are these, are\nsort of temporal aspects to this?",
        "start": 596.294,
        "duration": 2.74
    },
    {
        "text": "I don't think it would be, because\nthey're not processing this in time.",
        "start": 599.034,
        "duration": 3.33
    },
    {
        "text": "No, it's just a position.",
        "start": 602.694,
        "duration": 2.06
    },
    {
        "text": "So think of this as Let's say four\ndifferent 1D modules with different",
        "start": 604.754,
        "duration": 5.205
    },
    {
        "text": "frequencies, and then together\nyou're representing a position in 1D.",
        "start": 609.959,
        "duration": 5.159
    },
    {
        "text": "That's basically it.",
        "start": 615.119,
        "duration": 0.81
    },
    {
        "text": "Where do those frequencies come from?",
        "start": 615.929,
        "duration": 3.79
    },
    {
        "text": "It's just a parameter.",
        "start": 620.539,
        "duration": 0.89
    },
    {
        "text": "Yeah, it's just a parameter.",
        "start": 621.949,
        "duration": 1.13
    },
    {
        "text": "Who determined that parameter?",
        "start": 624.389,
        "duration": 1.12
    },
    {
        "text": "Hyperparameter.",
        "start": 625.509,
        "duration": 2.329
    },
    {
        "text": "I don't understand it.",
        "start": 628.219,
        "duration": 1.03
    },
    {
        "text": "It's okay.",
        "start": 629.259,
        "duration": 0.53
    },
    {
        "text": "I don't understand it.",
        "start": 629.789,
        "duration": 1.213
    },
    {
        "text": "It's the same with grid cells.",
        "start": 631.002,
        "duration": 2.167
    },
    {
        "text": "How do you determine the\nspacing of grid cells?",
        "start": 633.769,
        "duration": 1.86
    },
    {
        "text": "It's just . That's spacing.",
        "start": 636.549,
        "duration": 2.22
    },
    {
        "text": "But, grid cells, I can, say are\nrepresenting, a position in space.",
        "start": 639.014,
        "duration": 4.665
    },
    {
        "text": "Same here, which is updated.",
        "start": 643.979,
        "duration": 1.5
    },
    {
        "text": "and sequence, I think about sequences\nthat's not, you can say one dimension, but",
        "start": 646.334,
        "duration": 3.975
    },
    {
        "text": "you can have a one dimension order, and\nyou can have a one dimension or true one",
        "start": 650.309,
        "duration": 2.97
    },
    {
        "text": "dimension where the, words are like some.",
        "start": 653.279,
        "duration": 2.1
    },
    {
        "text": "spatial distance, like notes in a\nmelody, there's some gap in between.",
        "start": 655.999,
        "duration": 3.36
    },
    {
        "text": "They're not just a sequence, right?",
        "start": 659.549,
        "duration": 1.33
    },
    {
        "text": "It's not just an order,\nit's an order plus time.",
        "start": 660.919,
        "duration": 2.35
    },
    {
        "text": "Yeah, so it's up to you, there's no\nreason you couldn't use this to, here it's",
        "start": 663.379,
        "duration": 6.41
    },
    {
        "text": "sentences, so yeah, so the, it's ordinal\nin, so I guess it's ordinal out, but it's",
        "start": 669.789,
        "duration": 5.51
    },
    {
        "text": "a continuous mapping, it's a continuous\noutput, so if you had a continuous",
        "start": 675.299,
        "duration": 4.11
    },
    {
        "text": "input, you could, you could do that.",
        "start": 679.419,
        "duration": 1.97
    },
    {
        "text": "Yeah, but you could use this for\n2D images too, you could have a 2D",
        "start": 681.819,
        "duration": 3.41
    },
    {
        "text": "sinusoidal thing, which would be much\nmore like what we think of as grid cells.",
        "start": 685.509,
        "duration": 4.7
    },
    {
        "text": "Yeah.",
        "start": 690.609,
        "duration": 0.49
    },
    {
        "text": "But, and I guess one thing with grid\ncells, I think you can't remove them from,",
        "start": 691.779,
        "duration": 4.63
    },
    {
        "text": "or think of them separately as a movement.",
        "start": 696.649,
        "duration": 1.68
    },
    {
        "text": "There's no movement.",
        "start": 699.484,
        "duration": 0.65
    },
    {
        "text": "I know, so I'm just trying to where the,\nYeah, the, when, maybe when I get to",
        "start": 700.134,
        "duration": 4.6
    },
    {
        "text": "the embodied stuff, that'll make sense,\nbut, yeah, one, and I'll talk a bit",
        "start": 704.734,
        "duration": 3.76
    },
    {
        "text": "more about like the different flavors\nof positional encodings, but just to",
        "start": 708.494,
        "duration": 2.93
    },
    {
        "text": "highlight, the most common approach is\nto literally add the positional encoding.",
        "start": 711.424,
        "duration": 4.619
    },
    {
        "text": "which is interesting because, yeah, I\nthink that's very different from the",
        "start": 717.054,
        "duration": 3.31
    },
    {
        "text": "kind of way we would think about it\nin Monty because, then you're really",
        "start": 720.374,
        "duration": 3.36
    },
    {
        "text": "entangling this positional representation\nwith, the feature representation rather",
        "start": 723.974,
        "duration": 5.01
    },
    {
        "text": "than it being explicit and on its own.",
        "start": 728.984,
        "duration": 1.76
    },
    {
        "text": "and I don't think that's done for\nany kind of, like, it is possible to",
        "start": 732.334,
        "duration": 4.7
    },
    {
        "text": "concatenate them instead, but it's just,\nin order to keep, computations, low.",
        "start": 737.054,
        "duration": 5.08
    },
    {
        "text": "They basically do that.",
        "start": 742.444,
        "duration": 0.67
    },
    {
        "text": "okay.",
        "start": 746.104,
        "duration": 0.34
    },
    {
        "text": "So a bit of background then, done.",
        "start": 746.444,
        "duration": 3.04
    },
    {
        "text": "So then onto kind of the first\nthing, which is just this",
        "start": 749.504,
        "duration": 2.39
    },
    {
        "text": "architecture, kind of connectivity.",
        "start": 751.944,
        "duration": 2.49
    },
    {
        "text": "And what I'm showing here\nfirst is Monty and the kind",
        "start": 755.294,
        "duration": 3.96
    },
    {
        "text": "of general architecture of it.",
        "start": 759.254,
        "duration": 1.18
    },
    {
        "text": "and I'm trying to emphasize in\neach one of these units, the kind",
        "start": 761.924,
        "duration": 4.39
    },
    {
        "text": "of, the representations that are\nbeing, computed and, handled.",
        "start": 766.314,
        "duration": 4.39
    },
    {
        "text": "Okay.",
        "start": 770.704,
        "duration": 0.07
    },
    {
        "text": "Which is why I've broken it up into\nthis kind of pose and ID representation,",
        "start": 771.479,
        "duration": 4.08
    },
    {
        "text": "because it'll just make it a bit\nclearer kind of the similarities.",
        "start": 775.999,
        "duration": 3.02
    },
    {
        "text": "But yeah, in Monty you have multiple\nlearning modules, potentially with",
        "start": 779.869,
        "duration": 4.77
    },
    {
        "text": "this kind of hierarchical relationship.",
        "start": 784.639,
        "duration": 2.48
    },
    {
        "text": "I'm not showing all connections here, but\nas we've currently implemented it, You",
        "start": 787.569,
        "duration": 5.95
    },
    {
        "text": "get this, the sensory modules coming in.",
        "start": 793.769,
        "duration": 2.02
    },
    {
        "text": "And just one important thing to\nhighlight, as well as the kind of",
        "start": 796.479,
        "duration": 3.23
    },
    {
        "text": "voting, these lateral connections, the,\nyou have a one to one connection, going",
        "start": 799.719,
        "duration": 5.59
    },
    {
        "text": "from a lower level to, a layer above.",
        "start": 805.319,
        "duration": 2.46
    },
    {
        "text": "At the moment, we don't have any kind of\nfan in connections, and it's something",
        "start": 807.789,
        "duration": 2.92
    },
    {
        "text": "we've discussed, but, in general, the\nkind of view at the moment is, All of",
        "start": 810.709,
        "duration": 4.365
    },
    {
        "text": "the kind of interactions between, across\na layer of processing are primarily",
        "start": 815.074,
        "duration": 5.53
    },
    {
        "text": "going to be handled by the, voting.",
        "start": 820.604,
        "duration": 2.44
    },
    {
        "text": "And then you have the Oh yeah.",
        "start": 824.504,
        "duration": 2.29
    },
    {
        "text": "A small detail.",
        "start": 826.844,
        "duration": 1.2
    },
    {
        "text": "I think, one thing we do allow is to have\nmultiple lower level modules fan into",
        "start": 828.044,
        "duration": 6.11
    },
    {
        "text": "a high level module if they are within\nthat high level module's receptive field.",
        "start": 834.154,
        "duration": 3.92
    },
    {
        "text": "So if Oh yeah, that's true.",
        "start": 838.074,
        "duration": 1.53
    },
    {
        "text": "Yeah.",
        "start": 839.604,
        "duration": 0.28
    },
    {
        "text": "Higher module has a larger receptive\nfield and all the lower ones have like",
        "start": 839.884,
        "duration": 4.11
    },
    {
        "text": "small parts of that receptive field.",
        "start": 843.994,
        "duration": 1.77
    },
    {
        "text": "they can all input features into the\nhigher one, but the higher one can't",
        "start": 846.629,
        "duration": 3.64
    },
    {
        "text": "distinguish their locations from each\nother, because, Yeah, so it's It's, yeah,",
        "start": 850.269,
        "duration": 6.42
    },
    {
        "text": "I guess if this was broken down into\nthree submodules, yeah, thanks, yeah,",
        "start": 856.689,
        "duration": 6.59
    },
    {
        "text": "but then the kind of in, the location\ninformation is still this one to one,",
        "start": 863.279,
        "duration": 5.16
    },
    {
        "text": "mapping, and yeah, on that, then there's\nthis kind of almost skip connection",
        "start": 869.079,
        "duration": 3.83
    },
    {
        "text": "where, as well as getting the kind of,\nlearning module one gets this direct input",
        "start": 872.909,
        "duration": 3.58
    },
    {
        "text": "from a sensory module, learning module\nfour gets, direct input from a sensor",
        "start": 876.489,
        "duration": 4.65
    },
    {
        "text": "module, not necessarily sensor module\none, but of, the same location, space.",
        "start": 881.139,
        "duration": 4.56
    },
    {
        "text": "and then a transformer, if you ignore that\nfeedforward layer I was, in, just, it, can",
        "start": 889.944,
        "duration": 6.63
    },
    {
        "text": "still fit into this, but, just put that to\nthe side for the moment, you can imagine,",
        "start": 896.574,
        "duration": 4.92
    },
    {
        "text": "potentially each token as a column where,\nthese tokens kind of communicate laterally",
        "start": 902.564,
        "duration": 5.63
    },
    {
        "text": "with the self attention mechanism.",
        "start": 908.244,
        "duration": 1.78
    },
    {
        "text": "And as mentioned, whenever a\ntoken is being processed, other",
        "start": 910.664,
        "duration": 2.53
    },
    {
        "text": "than self attention, it's.",
        "start": 913.194,
        "duration": 1.07
    },
    {
        "text": "Predominantly in this kind of like\nsingle feedforward, single track",
        "start": 914.624,
        "duration": 5.08
    },
    {
        "text": "highway, rather than again having\na massive kind of fan in operation.",
        "start": 919.704,
        "duration": 5.88
    },
    {
        "text": "And, and yeah, and we have\nskip connections here as well.",
        "start": 926.434,
        "duration": 2.71
    },
    {
        "text": "and then similarly the\nspatial representation is",
        "start": 930.804,
        "duration": 3.53
    },
    {
        "text": "being injected at the start.",
        "start": 934.334,
        "duration": 2.57
    },
    {
        "text": "where the kind of sensory\ninput, quote unquote, may be.",
        "start": 938.004,
        "duration": 3.85
    },
    {
        "text": "What does pose mean in this case?",
        "start": 942.964,
        "duration": 2.17
    },
    {
        "text": "Just the positional encoding,\nI think you're Just the,",
        "start": 945.284,
        "duration": 2.29
    },
    {
        "text": "yeah, the positional encoding.",
        "start": 947.574,
        "duration": 1.439
    },
    {
        "text": "It's not, the same as pose where\nyou're thinking in Monty's, right?",
        "start": 949.564,
        "duration": 3.22
    },
    {
        "text": "It could, it's not in the sense\nthat, yeah, there's no explicit",
        "start": 952.834,
        "duration": 4.08
    },
    {
        "text": "graph or anything like that.",
        "start": 956.914,
        "duration": 1.24
    },
    {
        "text": "it, like these will, these kind of\nembeddings will, learn what they learn.",
        "start": 959.354,
        "duration": 5.56
    },
    {
        "text": "and, yeah, I think there's reasonable\nevidence that they can learn to represent,",
        "start": 966.324,
        "duration": 4.62
    },
    {
        "text": "positions in space and things like that.",
        "start": 973.224,
        "duration": 1.74
    },
    {
        "text": "But yeah, the, only kind of explicit,\nspatial input is, here at the start.",
        "start": 975.264,
        "duration": 6.11
    },
    {
        "text": "And as mentioned, even that, it's\nnot that explicit because it's",
        "start": 981.744,
        "duration": 2.49
    },
    {
        "text": "normally just added on top of the\nrepresentation that's, for the feature.",
        "start": 984.234,
        "duration": 3.63
    },
    {
        "text": "you have the lateral connections\nbeing self attention?",
        "start": 989.914,
        "duration": 2.69
    },
    {
        "text": "Yeah.",
        "start": 993.354,
        "duration": 0.37
    },
    {
        "text": "it's not a voting type of thing, right?",
        "start": 996.364,
        "duration": 1.61
    },
    {
        "text": "Is it, isn't the lateral connections\nmore just deciding, where your",
        "start": 998.824,
        "duration": 5.71
    },
    {
        "text": "attentional heads are at that point\nin time or something like that?",
        "start": 1004.574,
        "duration": 2.36
    },
    {
        "text": "Yeah, I'll, I'll, get more into\nattention later, but basically voting.",
        "start": 1007.254,
        "duration": 4.52
    },
    {
        "text": "can be considered a special\ncase of self attention, assuming",
        "start": 1012.214,
        "duration": 5.88
    },
    {
        "text": "everything's set up appropriately.",
        "start": 1018.094,
        "duration": 3.0
    },
    {
        "text": "But they also do an n\nsquared thing, right?",
        "start": 1021.574,
        "duration": 2.03
    },
    {
        "text": "Every token is Yeah, but voting, voting\nin Monty does the same, but I guess",
        "start": 1023.654,
        "duration": 8.33
    },
    {
        "text": "the key difference is, there's not\nlearned weights that are doing that.",
        "start": 1031.984,
        "duration": 2.89
    },
    {
        "text": "It's, just based on the,\nrelative displacement.",
        "start": 1035.274,
        "duration": 1.96
    },
    {
        "text": "It's, and it's just a question of, yeah,\nhow, much of the tokens or the columns",
        "start": 1037.654,
        "duration": 3.75
    },
    {
        "text": "around you, you want to compare to?",
        "start": 1041.404,
        "duration": 1.58
    },
    {
        "text": "Yeah.",
        "start": 1043.924,
        "duration": 0.3
    },
    {
        "text": "You say those aren't learned?",
        "start": 1044.274,
        "duration": 0.99
    },
    {
        "text": "The attention mechanism?",
        "start": 1046.134,
        "duration": 1.04
    },
    {
        "text": "No, they are learned.",
        "start": 1047.174,
        "duration": 0.73
    },
    {
        "text": "No, that, that's the thing.",
        "start": 1048.194,
        "duration": 0.85
    },
    {
        "text": "The attention, Monty uses the graphs that\nare internally learned within each column.",
        "start": 1049.124,
        "duration": 5.41
    },
    {
        "text": "Yeah.",
        "start": 1054.534,
        "duration": 0.11
    },
    {
        "text": "Whereas, because transformers\ndon't have this explicit graph",
        "start": 1055.174,
        "duration": 3.23
    },
    {
        "text": "representation, it's implicit in the\nweights and how it's transforming",
        "start": 1058.404,
        "duration": 4.91
    },
    {
        "text": "between, one token and another.",
        "start": 1063.344,
        "duration": 2.52
    },
    {
        "text": "yeah, I'll, self attention is\ndefinitely a weird one, so I'll, I",
        "start": 1068.894,
        "duration": 3.56
    },
    {
        "text": "think that'll make more sense with\nthe, The later visualizations, but,",
        "start": 1072.464,
        "duration": 3.44
    },
    {
        "text": "but yeah, just to ask one more, one\nmore question to that point, so yeah,",
        "start": 1077.064,
        "duration": 5.09
    },
    {
        "text": "cool visualization and comparison, but\nyeah, when you put object representation",
        "start": 1082.184,
        "duration": 4.71
    },
    {
        "text": "into these purple boxes and compare\nit to a learning module for example.",
        "start": 1086.894,
        "duration": 5.08
    },
    {
        "text": "Does that mean in there it can represent\nentire objects like models, like things",
        "start": 1092.484,
        "duration": 6.2
    },
    {
        "text": "that cannot be seen in one input, or\nis it basically it can recognize the",
        "start": 1098.684,
        "duration": 5.54
    },
    {
        "text": "embedding input and then everything\nit learns are like in the connections.",
        "start": 1104.234,
        "duration": 5.29
    },
    {
        "text": "yeah, checking if I understood, yeah, so\nit's, it's, literally just like a vector.",
        "start": 1112.895,
        "duration": 6.884
    },
    {
        "text": "So it's very, like it, it can\nrepresent like a, yeah, I guess",
        "start": 1119.799,
        "duration": 4.5
    },
    {
        "text": "whatever a vector could represent.",
        "start": 1124.299,
        "duration": 1.24
    },
    {
        "text": "So it can't, do a whole graph, but\nyeah, it could represent an object and",
        "start": 1125.539,
        "duration": 3.99
    },
    {
        "text": "I guess some fuzzy features about it.",
        "start": 1129.529,
        "duration": 2.09
    },
    {
        "text": "in terms of if I understand what\nyou're asking, How it would do",
        "start": 1133.049,
        "duration": 4.955
    },
    {
        "text": "object recognition or whatever.",
        "start": 1138.014,
        "duration": 1.47
    },
    {
        "text": "Do you mean is it going to, do something\nthat's not bag of features to recognize",
        "start": 1139.814,
        "duration": 6.39
    },
    {
        "text": "how parts compose it and things like that?",
        "start": 1146.434,
        "duration": 2.75
    },
    {
        "text": "Yeah.",
        "start": 1152.644,
        "duration": 0.27
    },
    {
        "text": "one of the principles of Monty is\nthat each of the columns can learn",
        "start": 1153.654,
        "duration": 3.4
    },
    {
        "text": "complete models of objects while\nthe input to a column never sees",
        "start": 1157.054,
        "duration": 4.26
    },
    {
        "text": "the entire object at one time.",
        "start": 1161.344,
        "duration": 1.65
    },
    {
        "text": "It can still recognize it.",
        "start": 1163.044,
        "duration": 1.38
    },
    {
        "text": "By moving over it over time.",
        "start": 1164.819,
        "duration": 1.51
    },
    {
        "text": "yeah.",
        "start": 1166.949,
        "duration": 0.42
    },
    {
        "text": "Would this kind of transformer\narchitecture be able to do that as",
        "start": 1167.409,
        "duration": 4.15
    },
    {
        "text": "well, or it can only recognize the\nobject in conjunction with, through",
        "start": 1171.559,
        "duration": 4.87
    },
    {
        "text": "the weight matrices that connect it?",
        "start": 1176.429,
        "duration": 1.63
    },
    {
        "text": "Yeah, so no, it has to use, it has\nto use these lateral connections.",
        "start": 1178.059,
        "duration": 3.61
    },
    {
        "text": "So this is the kind of, one of the funky\nthings about transformers, and I think one",
        "start": 1181.689,
        "duration": 3.18
    },
    {
        "text": "of the key differences is, there's no self\nrecurrence in each one of these tokens.",
        "start": 1184.869,
        "duration": 6.1
    },
    {
        "text": "and so there's no way for it to\nupdate its representation other",
        "start": 1192.614,
        "duration": 2.89
    },
    {
        "text": "than by these lateral inputs.",
        "start": 1195.504,
        "duration": 1.26
    },
    {
        "text": "So it's almost like if you if you were\nlike, we're not going to have any kind",
        "start": 1196.774,
        "duration": 4.66
    },
    {
        "text": "of sprawl world type recognition, where\nyou're building up models over movement.",
        "start": 1201.434,
        "duration": 4.53
    },
    {
        "text": "We're just going to focus on, voting,\nand voting's going to do everything.",
        "start": 1206.274,
        "duration": 4.07
    },
    {
        "text": "It's changing a bit, though.",
        "start": 1210.704,
        "duration": 1.34
    },
    {
        "text": "the naive transformer stuff, there's\nno, it doesn't maintain any state.",
        "start": 1214.794,
        "duration": 3.35
    },
    {
        "text": "So when the next input comes in, It's\ncompletely fresh, so there's no question,",
        "start": 1218.154,
        "duration": 4.365
    },
    {
        "text": "there's no concept of successive inputs.",
        "start": 1223.059,
        "duration": 2.39
    },
    {
        "text": "Yeah.",
        "start": 1226.159,
        "duration": 0.41
    },
    {
        "text": "instead the idea is you just flatten\neverything, you just flatten time",
        "start": 1227.569,
        "duration": 4.48
    },
    {
        "text": "into a single vector that's in a\nfeed forward way, but that's not",
        "start": 1232.049,
        "duration": 4.62
    },
    {
        "text": "the case with the GPT architectures\nand these so called autoregressive",
        "start": 1236.669,
        "duration": 4.4
    },
    {
        "text": "transformers, you get, you're\nsuccessively feeding the input in and",
        "start": 1241.099,
        "duration": 4.25
    },
    {
        "text": "actually maintaining the previous state.",
        "start": 1245.349,
        "duration": 1.76
    },
    {
        "text": "Yeah, so concatenating stuff, so yeah.",
        "start": 1248.244,
        "duration": 3.15
    },
    {
        "text": "So you're talking about yeah.",
        "start": 1251.394,
        "duration": 3.34
    },
    {
        "text": "So there's, self recurrence\nin that kind of sense.",
        "start": 1254.734,
        "duration": 2.94
    },
    {
        "text": "but it's a weird kind of self recurrence.",
        "start": 1258.484,
        "duration": 1.53
    },
    {
        "text": "It's weird because it's if the brain\nkept expand, if, you assume that these",
        "start": 1260.014,
        "duration": 5.16
    },
    {
        "text": "are cortical columns, it's like you\ndeal with sequences and time by just",
        "start": 1265.174,
        "duration": 3.54
    },
    {
        "text": "adding cortical columns or recruiting\nmore cortical columns through time.",
        "start": 1268.719,
        "duration": 3.325
    },
    {
        "text": "to deal with, new inputs, rather\nthan, yeah, saying, okay, each one",
        "start": 1273.684,
        "duration": 5.37
    },
    {
        "text": "of these tokens is going to, try and\nrepresent an object and integrate",
        "start": 1279.054,
        "duration": 3.56
    },
    {
        "text": "and, maintain a stable representation.",
        "start": 1282.994,
        "duration": 1.86
    },
    {
        "text": "Niels, when we're dealing with long\nsequences, I don't think that's",
        "start": 1286.154,
        "duration": 4.1
    },
    {
        "text": "quite true, because basically,\nyou're concatenating what it,",
        "start": 1290.254,
        "duration": 4.87
    },
    {
        "text": "evaluated previously with the current\none, and then processing them, and",
        "start": 1297.164,
        "duration": 5.24
    },
    {
        "text": "then I think there is a form of\nrecurrence that goes on in that sense.",
        "start": 1302.414,
        "duration": 3.71
    },
    {
        "text": "Do you mean like transformer XL?",
        "start": 1306.474,
        "duration": 1.88
    },
    {
        "text": "Or like perceiver?",
        "start": 1308.634,
        "duration": 1.989
    },
    {
        "text": "Okay, yeah, I would need to look at that.",
        "start": 1311.904,
        "duration": 2.01
    },
    {
        "text": "but definitely transformer XL, it was\nagain, I mean it was, that's the key",
        "start": 1314.554,
        "duration": 4.07
    },
    {
        "text": "thing is that they always concatenate.",
        "start": 1318.624,
        "duration": 1.86
    },
    {
        "text": "I think, which is very different from,\nthey didn't, at least when I looked at",
        "start": 1320.554,
        "duration": 6.3
    },
    {
        "text": "the Onyx transformer, which basically\ncan take many forms, on the ops.",
        "start": 1326.854,
        "duration": 7.52
    },
    {
        "text": "Yeah.",
        "start": 1334.374,
        "duration": 0.06
    },
    {
        "text": "there was the, a place there where\nit, could add, but there was only",
        "start": 1335.604,
        "duration": 3.87
    },
    {
        "text": "a certain amount it could add.",
        "start": 1339.474,
        "duration": 2.07
    },
    {
        "text": "In other words, I don't know, me double\nwhat the normal one was, but then yeah.",
        "start": 1341.544,
        "duration": 4.68
    },
    {
        "text": "that would be.",
        "start": 1346.709,
        "duration": 0.35
    },
    {
        "text": "Somehow saved on out and then re\nrepresented on the next cycle.",
        "start": 1347.729,
        "duration": 4.05
    },
    {
        "text": "I think, so I think it was a finite\nwindow that was moving across.",
        "start": 1353.329,
        "duration": 5.45
    },
    {
        "text": "Okay.",
        "start": 1360.269,
        "duration": 0.52
    },
    {
        "text": "Yeah, that sounds interesting.",
        "start": 1361.279,
        "duration": 0.79
    },
    {
        "text": "I'll definitely, yeah, I'll\ncheck out, Perceiver And, double",
        "start": 1362.089,
        "duration": 4.745
    },
    {
        "text": "check that, because, yeah, the,",
        "start": 1366.834,
        "duration": 1.89
    },
    {
        "text": "just to quickly show, this is the,",
        "start": 1372.154,
        "duration": 1.68
    },
    {
        "text": "yeah, the transformer Excel kind\nof passes in these representations",
        "start": 1376.504,
        "duration": 3.6
    },
    {
        "text": "here from the previous step, this is\nthe kind of fixed, window, but when",
        "start": 1380.444,
        "duration": 3.85
    },
    {
        "text": "it calculates self attention, it\nconcatenates this representation here,",
        "start": 1384.294,
        "duration": 3.56
    },
    {
        "text": "but that's, yeah, it's concatenated,\nbut, but okay, but I'll, yeah, I'll",
        "start": 1388.634,
        "duration": 3.58
    },
    {
        "text": "definitely check out, Perceiver.",
        "start": 1392.334,
        "duration": 2.08
    },
    {
        "text": "That'd be interesting if they do that.",
        "start": 1394.589,
        "duration": 1.52
    },
    {
        "text": "And that's one thing that, yeah, I'm sure\nsomeone has tried something like that.",
        "start": 1396.239,
        "duration": 4.23
    },
    {
        "text": "Because it's essentially just saying\neach one of these is an RNN as well.",
        "start": 1400.949,
        "duration": 3.14
    },
    {
        "text": "that's about the only way they can get\npast the N squared thing is Perceiver has",
        "start": 1405.129,
        "duration": 5.58
    },
    {
        "text": "A limited amount of N squared and then\ntracks across the thing so they can put",
        "start": 1411.564,
        "duration": 5.38
    },
    {
        "text": "in these much larger, token sequences and\nactually derive something that has more",
        "start": 1416.944,
        "duration": 6.39
    },
    {
        "text": "scope to it, but just not all at once.",
        "start": 1423.344,
        "duration": 2.41
    },
    {
        "text": "It's not self attention all the time.",
        "start": 1425.764,
        "duration": 2.06
    },
    {
        "text": "It's building up.",
        "start": 1427.824,
        "duration": 1.95
    },
    {
        "text": "I could be wrong about how\nmuch they're retaining, in that",
        "start": 1430.809,
        "duration": 3.33
    },
    {
        "text": "particular The, cross attention.",
        "start": 1434.139,
        "duration": 1.92
    },
    {
        "text": "cross attention usually comes\nin on the, on the decoder,",
        "start": 1438.029,
        "duration": 3.04
    },
    {
        "text": "side, But yeah, you're right.",
        "start": 1441.839,
        "duration": 3.03
    },
    {
        "text": "In a way, it's you're right.",
        "start": 1444.929,
        "duration": 2.17
    },
    {
        "text": "I think they use cross attention at\nthe input to basically say, 10k tokens",
        "start": 1447.269,
        "duration": 5.42
    },
    {
        "text": "is going to be like cross attention\nor something like that, and then,",
        "start": 1452.689,
        "duration": 4.75
    },
    {
        "text": "yeah, okay, cool, but, But in any\ncase, even, if they don't specifically",
        "start": 1457.589,
        "duration": 5.95
    },
    {
        "text": "do it, you could see how you could\ndo something like that, right?",
        "start": 1463.539,
        "duration": 4.15
    },
    {
        "text": "As long as you have some, state\nbuilt on up that you feed in on",
        "start": 1467.709,
        "duration": 4.55
    },
    {
        "text": "the next cycle and become part\nof the, the, input if you wish.",
        "start": 1472.259,
        "duration": 4.69
    },
    {
        "text": "yeah,",
        "start": 1479.279,
        "duration": 0.28
    },
    {
        "text": "okay, yeah, but",
        "start": 1481.979,
        "duration": 0.56
    },
    {
        "text": "unless, yeah, there was any more kind of\nquestions on that, I was just going to",
        "start": 1485.459,
        "duration": 3.05
    },
    {
        "text": "then focus on just briefly some of the\nkind of, differences that immediately",
        "start": 1488.509,
        "duration": 4.36
    },
    {
        "text": "jump out is, yeah, firstly, the, what\nI was already saying, and which will be",
        "start": 1493.099,
        "duration": 4.66
    },
    {
        "text": "broken down into more detail about how\nthese kinds of, the lateral operation",
        "start": 1497.759,
        "duration": 3.44
    },
    {
        "text": "that's going on is very much dependent\non learned transformations, learned",
        "start": 1501.199,
        "duration": 3.89
    },
    {
        "text": "weight sets in, in transformers.",
        "start": 1505.089,
        "duration": 3.11
    },
    {
        "text": "How would you think about\nattention heads here?",
        "start": 1509.479,
        "duration": 2.11
    },
    {
        "text": "Would that be, considered a model?",
        "start": 1511.599,
        "duration": 4.08
    },
    {
        "text": "so if you have 10 attention heads,\neach column here can have 10 models?",
        "start": 1516.759,
        "duration": 4.62
    },
    {
        "text": "Yeah, I've, I was wondering this.",
        "start": 1522.419,
        "duration": 1.68
    },
    {
        "text": "I think the reality because of, how,\nI guess neural networks can operate",
        "start": 1526.469,
        "duration": 6.26
    },
    {
        "text": "in such kind of, weird spaces.",
        "start": 1532.729,
        "duration": 3.34
    },
    {
        "text": "I'm, sure they can do\nmany more than 10 models.",
        "start": 1536.359,
        "duration": 1.93
    },
    {
        "text": "but yeah, but I think there would\nbe a relationship between the number",
        "start": 1539.999,
        "duration": 3.03
    },
    {
        "text": "of transformer heads and like the\nnumber of models that can be learned.",
        "start": 1543.029,
        "duration": 2.99
    },
    {
        "text": "attention heads, but I'm not sure\nit's nearly a one, or clearly",
        "start": 1546.939,
        "duration": 4.43
    },
    {
        "text": "like a one to one mapping.",
        "start": 1551.369,
        "duration": 1.01
    },
    {
        "text": "I think what happened, at least in\nsome of them, the, each head, The",
        "start": 1553.039,
        "duration": 4.85
    },
    {
        "text": "various levels learn some aspect, noun,\nverb, syntax or something like that.",
        "start": 1557.889,
        "duration": 7.4
    },
    {
        "text": "And then by whatever mechanism, the\nother heads learn other parts of it.",
        "start": 1565.739,
        "duration": 4.78
    },
    {
        "text": "it's just how it kind of trains,\nit differentiates during training,",
        "start": 1570.839,
        "duration": 4.56
    },
    {
        "text": "time so that they, there seems to\nbe some functionality on each head.",
        "start": 1576.029,
        "duration": 4.22
    },
    {
        "text": "So I'm not sure it's a complete model.",
        "start": 1580.249,
        "duration": 2.49
    },
    {
        "text": "It could be more akin to a feature.",
        "start": 1582.759,
        "duration": 2.6
    },
    {
        "text": "And that those features are then being,\nwhen they, when it goes from the attention",
        "start": 1586.679,
        "duration": 4.06
    },
    {
        "text": "head up to the fully connected layer,\nthen they all get scrambled together",
        "start": 1590.739,
        "duration": 4.29
    },
    {
        "text": "and then produces new inputs and then\ngoes to the next level and then it",
        "start": 1595.029,
        "duration": 4.2
    },
    {
        "text": "tries to discern stuff from that.",
        "start": 1599.239,
        "duration": 1.68
    },
    {
        "text": "So the actual structure of what happens\nin that fully connected layer and how it",
        "start": 1603.589,
        "duration": 5.75
    },
    {
        "text": "consolidates information to be fed in\nas input to the next one is something",
        "start": 1612.089,
        "duration": 5.84
    },
    {
        "text": "that I've not seen people talk about\na lot, but you can imagine how somehow",
        "start": 1617.939,
        "duration": 5.27
    },
    {
        "text": "you're managing to create these streams\nof capabilities that at the very end, if",
        "start": 1623.949,
        "duration": 5.17
    },
    {
        "text": "you're classifying, there's no real model.",
        "start": 1629.119,
        "duration": 2.4
    },
    {
        "text": "it's, a collection of features at the\nvery end, I'm not sure how, whether",
        "start": 1632.969,
        "duration": 6.37
    },
    {
        "text": "imputing a model to the heads actually\nis something that they embody, at",
        "start": 1639.759,
        "duration": 6.36
    },
    {
        "text": "least in the current incarnation.",
        "start": 1646.119,
        "duration": 1.43
    },
    {
        "text": "Yeah, I think the closest thing to a\nmodel is and yeah, I'll talk about this",
        "start": 1649.509,
        "duration": 5.285
    },
    {
        "text": "when talking about, kind of reference\nframes is yeah, a bit like how capsule",
        "start": 1654.794,
        "duration": 3.85
    },
    {
        "text": "networks they don't have an explicit\nreference frame about, this is where",
        "start": 1658.644,
        "duration": 4.97
    },
    {
        "text": "you're going to, we're going to represent\nthis feature relative to this feature.",
        "start": 1663.614,
        "duration": 2.52
    },
    {
        "text": "It's more the, weight\ntransformations that are learned.",
        "start": 1666.344,
        "duration": 2.63
    },
    {
        "text": "learn, okay, I've got a nose, this is\nwhere I expect the face, I've got an",
        "start": 1669.604,
        "duration": 3.3
    },
    {
        "text": "ear, this is where I expect the face,\nwhatever, it's more, like that with",
        "start": 1672.914,
        "duration": 4.62
    },
    {
        "text": "the, learned, yeah, attention heads\ncould be capturing, or I guess, like you",
        "start": 1678.214,
        "duration": 5.27
    },
    {
        "text": "say, also maybe the, feedforward layer.",
        "start": 1683.494,
        "duration": 1.76
    },
    {
        "text": "but yeah, but there's certainly\nno kind of explicit, spatial map",
        "start": 1686.649,
        "duration": 3.56
    },
    {
        "text": "that things are being laid down to.",
        "start": 1690.209,
        "duration": 1.46
    },
    {
        "text": "Okay.",
        "start": 1692.929,
        "duration": 0.25
    },
    {
        "text": "and along with that reference frame, there\nseems to be, maybe I'm missing it, maybe",
        "start": 1693.949,
        "duration": 4.63
    },
    {
        "text": "I'm going to get to it later, but the\nwhole idea of removing your sensors under",
        "start": 1698.579,
        "duration": 4.14
    },
    {
        "text": "volitional control of the brain through\nspace is, a huge part of this, right?",
        "start": 1702.829,
        "duration": 4.31
    },
    {
        "text": "We're not just something\nnot being passed to us.",
        "start": 1707.139,
        "duration": 2.21
    },
    {
        "text": "We are actively exploring space and\nmoving in different directions and",
        "start": 1710.254,
        "duration": 2.68
    },
    {
        "text": "processing motion, processing movement,\nkeeping track of where you, which",
        "start": 1713.494,
        "duration": 4.01
    },
    {
        "text": "requires reference frames to do that.",
        "start": 1717.544,
        "duration": 1.98
    },
    {
        "text": "so it's more than just reference frames.",
        "start": 1720.484,
        "duration": 1.77
    },
    {
        "text": "I think it seems like we're missing\nthis whole sensorimotor aspect here.",
        "start": 1722.254,
        "duration": 5.08
    },
    {
        "text": "For sure.",
        "start": 1727.864,
        "duration": 0.47
    },
    {
        "text": "Okay, I just want to, it's\na, that's a huge part.",
        "start": 1729.034,
        "duration": 3.16
    },
    {
        "text": "It comes with reference\nframes, but yeah, no, for sure.",
        "start": 1732.264,
        "duration": 3.11
    },
    {
        "text": "that is very much missing.",
        "start": 1735.704,
        "duration": 1.3
    },
    {
        "text": "can these work across modalities too?",
        "start": 1739.484,
        "duration": 2.11
    },
    {
        "text": "I will get to that, I think.",
        "start": 1742.734,
        "duration": 2.08
    },
    {
        "text": "Yeah, on the next slide.",
        "start": 1745.604,
        "duration": 1.06
    },
    {
        "text": "Okay.",
        "start": 1747.424,
        "duration": 0.29
    },
    {
        "text": "but yeah, anyways, no top down\ninfluence, no explicit reference",
        "start": 1749.004,
        "duration": 2.52
    },
    {
        "text": "frame, no self reference.",
        "start": 1751.534,
        "duration": 0.86
    },
    {
        "text": "Those are just some of the differences,\nbut just highlighting at the architectural",
        "start": 1752.434,
        "duration": 3.84
    },
    {
        "text": "level, the kind of the connection level.",
        "start": 1756.274,
        "duration": 2.17
    },
    {
        "text": "Okay, so yeah, common\nfeature representations.",
        "start": 1760.114,
        "duration": 1.92
    },
    {
        "text": "To answer your question in a\ncouple words, yes, they do.",
        "start": 1762.474,
        "duration": 4.63
    },
    {
        "text": "so of course, common feature\nrepresentations across modalities, this is",
        "start": 1767.794,
        "duration": 3.51
    },
    {
        "text": "important for enabling multimodal, models.",
        "start": 1771.304,
        "duration": 2.98
    },
    {
        "text": "and in the, recent work that's trying\nto move beyond just language into these",
        "start": 1776.174,
        "duration": 4.69
    },
    {
        "text": "kind of multimodal, large multimodal\nmodels, LMMs, they do exactly this.",
        "start": 1780.864,
        "duration": 6.28
    },
    {
        "text": "and the basic idea is\npretty straightforward.",
        "start": 1787.634,
        "duration": 1.9
    },
    {
        "text": "You just take whatever it is you're trying\nto represent, that could be a pixel patch,",
        "start": 1790.144,
        "duration": 4.78
    },
    {
        "text": "that could be a, word token, whatever.",
        "start": 1794.924,
        "duration": 3.29
    },
    {
        "text": "And you just do some kind of learned\nprojection of that representation",
        "start": 1798.584,
        "duration": 3.79
    },
    {
        "text": "into a fixed size vector embedding.",
        "start": 1802.594,
        "duration": 2.18
    },
    {
        "text": "And so as long as those kind of\nvector embeddings are of, they're",
        "start": 1805.604,
        "duration": 3.95
    },
    {
        "text": "all 64 dimensions, say, then you can\nessentially treat them all the same",
        "start": 1809.554,
        "duration": 5.46
    },
    {
        "text": "in your transformer and assume that\nit's going to learn to deal with the",
        "start": 1815.034,
        "duration": 4.59
    },
    {
        "text": "fact that one of them is, An embedding\nof a pixel patch, and one of them is",
        "start": 1819.624,
        "duration": 3.92
    },
    {
        "text": "an embedding of a, of a English word.",
        "start": 1823.544,
        "duration": 2.25
    },
    {
        "text": "and this is just an example showing\nthat in practice from, so Palmy,",
        "start": 1828.324,
        "duration": 4.95
    },
    {
        "text": "is an embedded, embed embodied,\nembedded, embodied, sorry, multimodal,",
        "start": 1833.634,
        "duration": 5.49
    },
    {
        "text": "language model, from Google.",
        "start": 1839.334,
        "duration": 1.74
    },
    {
        "text": "that, came out pretty recently.",
        "start": 1841.914,
        "duration": 2.16
    },
    {
        "text": "this was actually one of the main, papers\nthat the, the DLCT, presentation was",
        "start": 1844.464,
        "duration": 6.355
    },
    {
        "text": "on a couple of weeks ago about scaling\nrobotics using, large language models.",
        "start": 1850.819,
        "duration": 4.19
    },
    {
        "text": "And yeah, basically they, take the\nkind of query, they turn it into",
        "start": 1855.919,
        "duration": 4.08
    },
    {
        "text": "these tokens, they take the image,\nthey turn it into these tokens, and",
        "start": 1859.999,
        "duration": 2.92
    },
    {
        "text": "then all of that can be passed into\nthis, model, and used together.",
        "start": 1862.919,
        "duration": 5.419
    },
    {
        "text": "but yeah, you'll, see the, how the\nkind of the, embodied part of it is a",
        "start": 1871.249,
        "duration": 3.94
    },
    {
        "text": "bit odd, but I'll come back to that.",
        "start": 1875.189,
        "duration": 1.5
    },
    {
        "text": "I just wanted to start with, Those two\ncomparisons just to ground the rest",
        "start": 1877.469,
        "duration": 7.33
    },
    {
        "text": "of the discussion, and then, yeah.",
        "start": 1884.799,
        "duration": 2.13
    },
    {
        "text": "It seems like a fairly limited way of\ndoing multimodal integration, but I,",
        "start": 1886.929,
        "duration": 5.465
    },
    {
        "text": "can't say that for certain because I\nhave to study it more, but the way I",
        "start": 1892.394,
        "duration": 4.47
    },
    {
        "text": "think about the brain is much more,\nit's much more flexible than this.",
        "start": 1896.874,
        "duration": 3.22
    },
    {
        "text": "but maybe I just don't understand this\nyet, is it that different from There's",
        "start": 1901.404,
        "duration": 4.91
    },
    {
        "text": "a lot of things here, there's a lot\nof things I think we'd be careful",
        "start": 1906.314,
        "duration": 2.41
    },
    {
        "text": "saying, oh look, it does this too, oh\nlook, it has objects, it has poses,",
        "start": 1908.724,
        "duration": 2.92
    },
    {
        "text": "when you get to the details, they're\nreally quite different in some ways,",
        "start": 1912.114,
        "duration": 2.8
    },
    {
        "text": "and so we have to be careful not to,",
        "start": 1915.294,
        "duration": 1.7
    },
    {
        "text": "just be careful with that language.",
        "start": 1919.539,
        "duration": 1.47
    },
    {
        "text": "but is, is that, that different\nfrom taking, a sound wave and, a",
        "start": 1921.929,
        "duration": 5.42
    },
    {
        "text": "few kind of Rod Cone responses or\nwhatever, and encoding that as an SDR.",
        "start": 1927.349,
        "duration": 6.295
    },
    {
        "text": "Yeah, but we don't do that.",
        "start": 1935.604,
        "duration": 1.1
    },
    {
        "text": "We never do that.",
        "start": 1938.174,
        "duration": 0.97
    },
    {
        "text": "We only we That would be a\nterrible way of doing it.",
        "start": 1941.214,
        "duration": 4.589
    },
    {
        "text": "the way we, the way the brain\nconverges is purely through voting.",
        "start": 1946.534,
        "duration": 3.82
    },
    {
        "text": "And they're trying to, they're trying\nto correlate objects with objects.",
        "start": 1950.964,
        "duration": 3.41
    },
    {
        "text": "And it's not a one to one correlation.",
        "start": 1954.394,
        "duration": 2.42
    },
    {
        "text": "A sound doesn't have to be\ncorrelated with a single object.",
        "start": 1957.054,
        "duration": 2.031
    },
    {
        "text": "And a single object can\ncorrelate with a single sound.",
        "start": 1959.085,
        "duration": 2.139
    },
    {
        "text": "It's a much more,",
        "start": 1961.719,
        "duration": 0.9
    },
    {
        "text": "dynamic and",
        "start": 1965.049,
        "duration": 0.49
    },
    {
        "text": "method.",
        "start": 1968.129,
        "duration": 0.15
    },
    {
        "text": "You mean with the, spatial pooler, or?",
        "start": 1968.319,
        "duration": 1.87
    },
    {
        "text": "No, I, we have never done this.",
        "start": 1970.579,
        "duration": 1.7
    },
    {
        "text": "I'm, just saying how we've described\nit and how we've written about it.",
        "start": 1972.679,
        "duration": 3.55
    },
    {
        "text": "it's all done through voting.",
        "start": 1977.049,
        "duration": 1.31
    },
    {
        "text": "It's not, there's nothing\ndone in the spatial pooler.",
        "start": 1978.409,
        "duration": 1.88
    },
    {
        "text": "There's no, common, it's just voting.",
        "start": 1981.159,
        "duration": 2.93
    },
    {
        "text": "It's one set of auditory models\nor a set of tactile models can",
        "start": 1984.089,
        "duration": 4.37
    },
    {
        "text": "inform a set of visual models.",
        "start": 1988.949,
        "duration": 2.14
    },
    {
        "text": "but it's not one to one, and\nit's not always appropriate.",
        "start": 1991.799,
        "duration": 3.52
    },
    {
        "text": "I'll just leave it at that, is that I, I\nlook at this and say, oh, this is cool,",
        "start": 1995.509,
        "duration": 3.16
    },
    {
        "text": "but I, it's, to me, it's not the same as\nI envision multimodal integration frames.",
        "start": 1998.669,
        "duration": 4.85
    },
    {
        "text": "I just want to point that out, that at\nsome level you can say, oh yeah, there's",
        "start": 2003.519,
        "duration": 3.64
    },
    {
        "text": "objects in these modules, and there's\nposes, but they're really not the same.",
        "start": 2007.189,
        "duration": 3.22
    },
    {
        "text": "Yeah.",
        "start": 2010.949,
        "duration": 0.44
    },
    {
        "text": "Just really, we can use\nthe same words for them.",
        "start": 2011.639,
        "duration": 2.32
    },
    {
        "text": "Yeah.",
        "start": 2013.959,
        "duration": 1.879
    },
    {
        "text": "Yeah.",
        "start": 2016.019,
        "duration": 0.4
    },
    {
        "text": "Yeah.",
        "start": 2016.419,
        "duration": 0.519
    },
    {
        "text": "So I guess we can be agnostic how,\nlike this whole, embedding operation,",
        "start": 2016.939,
        "duration": 6.03
    },
    {
        "text": "yeah, we're not certain exactly what\nwe would do, but I guess the main thing",
        "start": 2023.489,
        "duration": 5.47
    },
    {
        "text": "then is once, these are input, then\nthroughout this stream, at each level",
        "start": 2028.959,
        "duration": 5.45
    },
    {
        "text": "of processing, I you're you started\nwith a visual feature here, you started",
        "start": 2034.409,
        "duration": 6.375
    },
    {
        "text": "with a language feature here, whatever.",
        "start": 2040.784,
        "duration": 2.31
    },
    {
        "text": "You're going to continue going up\nthose paths, and they're going to",
        "start": 2043.554,
        "duration": 2.63
    },
    {
        "text": "continue interacting laterally.",
        "start": 2046.194,
        "duration": 2.12
    },
    {
        "text": "I'm just highlighting that because\nthat is similar to but it seems",
        "start": 2049.004,
        "duration": 3.557
    },
    {
        "text": "like we've told them that these\nthings are one and the same.",
        "start": 2052.561,
        "duration": 2.567
    },
    {
        "text": "and that's not the way it is in the brain.",
        "start": 2056.079,
        "duration": 1.72
    },
    {
        "text": "They're not one and the same.",
        "start": 2057.809,
        "duration": 0.89
    },
    {
        "text": "There are different models and different\nmodalities which can, under, at different",
        "start": 2058.699,
        "duration": 5.48
    },
    {
        "text": "times, help each other, but they're\nnot, we're not trying to figure out",
        "start": 2064.179,
        "duration": 2.61
    },
    {
        "text": "what's in this picture and they'll\ncorrespond the language with the picture.",
        "start": 2066.789,
        "duration": 4.23
    },
    {
        "text": "It's, that's not what's\ngoing on in the brain.",
        "start": 2071.499,
        "duration": 1.51
    },
    {
        "text": "Yeah.",
        "start": 2073.049,
        "duration": 0.55
    },
    {
        "text": "And that sounds what they're doing here.",
        "start": 2073.599,
        "duration": 2.9
    },
    {
        "text": "They're saying, hey, can we just,\ncan we use these two modalities to,",
        "start": 2076.539,
        "duration": 3.74
    },
    {
        "text": "combine them in a single representation?",
        "start": 2081.259,
        "duration": 2.24
    },
    {
        "text": "I don't think that's\nhappening in the brain.",
        "start": 2083.499,
        "duration": 1.479
    },
    {
        "text": "and there's a real advantage as I'm\nnot doing it, but this could, this",
        "start": 2085.979,
        "duration": 3.51
    },
    {
        "text": "solves a certain set of problems if you\nwant to have, described images and if",
        "start": 2089.489,
        "duration": 4.67
    },
    {
        "text": "you want to take words and make them,\nit is a fine, do a good job of that.",
        "start": 2094.229,
        "duration": 3.33
    },
    {
        "text": "Okay, I just, again, I just want\nto just, I'm just throwing it out,",
        "start": 2098.309,
        "duration": 2.17
    },
    {
        "text": "we just shouldn't accept these\nwords as meaning the same thing.",
        "start": 2100.539,
        "duration": 2.58
    },
    {
        "text": "Yeah.",
        "start": 2103.739,
        "duration": 0.49
    },
    {
        "text": "No, thanks.",
        "start": 2105.169,
        "duration": 0.52
    },
    {
        "text": "okay.",
        "start": 2110.999,
        "duration": 0.2
    },
    {
        "text": "Yeah.",
        "start": 2111.199,
        "duration": 0.22
    },
    {
        "text": "voting versus self attention.",
        "start": 2112.269,
        "duration": 1.53
    },
    {
        "text": "so I'll just yeah, briefly go through\nvoting, in Monty and self attention",
        "start": 2115.879,
        "duration": 4.6
    },
    {
        "text": "in transformers, and then try and get\na bit more into the concrete details",
        "start": 2120.489,
        "duration": 4.35
    },
    {
        "text": "of how they're potentially similar.",
        "start": 2124.899,
        "duration": 1.31
    },
    {
        "text": "Monty, we, let's say have, two\nfingers that were touching this mug.",
        "start": 2129.209,
        "duration": 4.23
    },
    {
        "text": "We're going to get two different\nfeatures at poses, and these are",
        "start": 2134.104,
        "duration": 3.07
    },
    {
        "text": "going to be processed by different\nlearning modules, which are going",
        "start": 2137.174,
        "duration": 2.96
    },
    {
        "text": "to develop different hypotheses for\nwhere we might be on that object.",
        "start": 2140.134,
        "duration": 4.25
    },
    {
        "text": "And we, have this information\nabout the kind of displacement,",
        "start": 2146.689,
        "duration": 4.16
    },
    {
        "text": "including the, rotation between,\nthese two sensor modules.",
        "start": 2151.649,
        "duration": 3.4
    },
    {
        "text": "and this is key, to, the kind of\nvoting process where we essentially",
        "start": 2156.189,
        "duration": 5.69
    },
    {
        "text": "take the hypotheses, coming out\nof a particular learning module.",
        "start": 2161.909,
        "duration": 3.16
    },
    {
        "text": "And depending on which learning module\nthat information is going to, transform",
        "start": 2165.819,
        "duration": 3.61
    },
    {
        "text": "it by the appropriate, displacement.",
        "start": 2169.429,
        "duration": 2.6
    },
    {
        "text": "And, this is going to give us a new set\nof points for, where we would expect to",
        "start": 2173.049,
        "duration": 4.53
    },
    {
        "text": "be, on that object based on, where we\nare on, in this kind of hypothesis space.",
        "start": 2177.589,
        "duration": 5.74
    },
    {
        "text": "And that's what all these\nkind of clouds exploding out.",
        "start": 2183.329,
        "duration": 2.16
    },
    {
        "text": "are doing, where there's only a small\nset of points, related to where we were",
        "start": 2185.829,
        "duration": 5.12
    },
    {
        "text": "before, that with this displacement are\ngoing to move to this point on the handle.",
        "start": 2190.959,
        "duration": 5.08
    },
    {
        "text": "And that point on the handle, if\nyou look at your nearest neighbor,",
        "start": 2197.169,
        "duration": 4.1
    },
    {
        "text": "you're essentially going to add\nthe evidence that's coming in,",
        "start": 2201.889,
        "duration": 2.36
    },
    {
        "text": "which may well be negative, that's\nassociated with your neighbor.",
        "start": 2204.249,
        "duration": 3.67
    },
    {
        "text": "and you'll get these kind of hotspots\nforming, essentially where these",
        "start": 2208.839,
        "duration": 2.87
    },
    {
        "text": "are consistent with one another.",
        "start": 2211.709,
        "duration": 1.84
    },
    {
        "text": "self attention in transformers,\nis basically a way to figure out,",
        "start": 2218.269,
        "duration": 5.13
    },
    {
        "text": "representations, figure out what, other\nrepresentations are Looking for them,",
        "start": 2224.679,
        "duration": 5.605
    },
    {
        "text": "and related to them, so it can bring that\ninformation together and condense it.",
        "start": 2230.944,
        "duration": 4.82
    },
    {
        "text": "when I first talk about this, it\nwon't seem to have anything to",
        "start": 2236.574,
        "duration": 2.17
    },
    {
        "text": "do with voting, just to be clear.",
        "start": 2238.744,
        "duration": 2.61
    },
    {
        "text": "But, that's why when I talk about\nthis as a special case, hopefully",
        "start": 2241.634,
        "duration": 4.1
    },
    {
        "text": "the, parallel will be more obvious.",
        "start": 2246.274,
        "duration": 1.84
    },
    {
        "text": "And, when you do this self\nattention operation, you",
        "start": 2249.664,
        "duration": 2.84
    },
    {
        "text": "have, three important things.",
        "start": 2252.514,
        "duration": 1.41
    },
    {
        "text": "You have a query, you have\na key, and you have a value.",
        "start": 2253.924,
        "duration": 2.57
    },
    {
        "text": "And for every token, and in this\ncase, we're going to say every word",
        "start": 2256.814,
        "duration": 4.1
    },
    {
        "text": "in this sentence is going to be\nrepresented by a token embedding.",
        "start": 2260.914,
        "duration": 2.66
    },
    {
        "text": "For every token embedding, you\nwill, have a key, query, and, value.",
        "start": 2264.014,
        "duration": 5.1
    },
    {
        "text": "and so if we just focus on this word,\nit, for now, so this is the word that",
        "start": 2270.644,
        "duration": 4.34
    },
    {
        "text": "we're processing with self attention,\nso it's going to get, it's going",
        "start": 2274.984,
        "duration": 4.17
    },
    {
        "text": "to project a query, it's going to\nbasically be asking, okay, what other",
        "start": 2279.154,
        "duration": 3.91
    },
    {
        "text": "representations are there around me that\nmight be related, that might be relevant?",
        "start": 2283.544,
        "duration": 5.11
    },
    {
        "text": "And in kind of answering that call, every\nother token is going to produce, a key.",
        "start": 2289.684,
        "duration": 5.84
    },
    {
        "text": "And these, so the reason for these\narrows is these are, vectors.",
        "start": 2295.784,
        "duration": 3.84
    },
    {
        "text": "and so you can just think of them\nas a point, somewhere in space.",
        "start": 2300.794,
        "duration": 3.57
    },
    {
        "text": "And, to determine how much agreement,\nor like how appropriate the two",
        "start": 2305.164,
        "duration": 6.38
    },
    {
        "text": "are for each other, you literally\nare just doing the dot product.",
        "start": 2311.544,
        "duration": 3.32
    },
    {
        "text": "and then when you do the dot product,\nso yeah, so the query dot with this",
        "start": 2316.359,
        "duration": 3.45
    },
    {
        "text": "key is going to be a larger value.",
        "start": 2319.849,
        "duration": 1.51
    },
    {
        "text": "and then this is just passed\nthrough a softmax, and this is",
        "start": 2322.319,
        "duration": 2.23
    },
    {
        "text": "what gives the, attention, mask,\nthe, self attention, layer.",
        "start": 2324.549,
        "duration": 5.38
    },
    {
        "text": "and so here, the, value, the, this\nkind of, probability associated to",
        "start": 2331.939,
        "duration": 3.98
    },
    {
        "text": "the, this, the animal, say, is higher.",
        "start": 2335.919,
        "duration": 4.72
    },
    {
        "text": "And this kind of fits with the idea\nthat, It is, okay, it's, it knows",
        "start": 2341.429,
        "duration": 4.865
    },
    {
        "text": "it needs to look for something like,\na noun or an object or something",
        "start": 2346.294,
        "duration": 4.0
    },
    {
        "text": "that it could be referring to, and,\nanimal in this sense has learned",
        "start": 2350.304,
        "duration": 5.65
    },
    {
        "text": "that it needs to look for words like\nit that might be referring to it.",
        "start": 2355.954,
        "duration": 3.64
    },
    {
        "text": "But what we want to actually But\nthe word street, you could make the",
        "start": 2361.054,
        "duration": 2.98
    },
    {
        "text": "same argument for the word street.",
        "start": 2364.034,
        "duration": 1.46
    },
    {
        "text": "So how does it distinguish between those?",
        "start": 2365.514,
        "duration": 1.44
    },
    {
        "text": "Yeah, this, yeah, this is, a toy example.",
        "start": 2366.984,
        "duration": 4.75
    },
    {
        "text": "But just in general, like, all\nright, that's the trick, right?",
        "start": 2372.854,
        "duration": 2.82
    },
    {
        "text": "Is it referring to street or\nis it referring to animal?",
        "start": 2375.744,
        "duration": 2.27
    },
    {
        "text": "I guess another thing is, This, voting\noperation, it's not just based on,",
        "start": 2379.014,
        "duration": 5.91
    },
    {
        "text": "or it's based on this embedding, and\nthis embedding is both based on the",
        "start": 2385.234,
        "duration": 2.97
    },
    {
        "text": "kind of feature, whatever that is, the\nobject kind of type representation,",
        "start": 2388.204,
        "duration": 3.54
    },
    {
        "text": "as well as its position encoding.",
        "start": 2392.064,
        "duration": 1.89
    },
    {
        "text": "and so maybe it has learned, that,\nimmediately before it, so early",
        "start": 2395.084,
        "duration": 6.365
    },
    {
        "text": "before it, it's not going to be\nfinding anything, it follows usually",
        "start": 2401.449,
        "duration": 6.15
    },
    {
        "text": "later, in the sequence relative to\nthe word that it's referring to.",
        "start": 2407.599,
        "duration": 3.34
    },
    {
        "text": "that's funny because, I could make the\nsame, I could say the animal didn't cross",
        "start": 2411.049,
        "duration": 3.45
    },
    {
        "text": "the street because it was too wide, and\nin that case it would refer to the street.",
        "start": 2414.499,
        "duration": 5.404
    },
    {
        "text": "so it can't be that simple.",
        "start": 2421.144,
        "duration": 1.18
    },
    {
        "text": "Yeah, it's impressive.",
        "start": 2424.614,
        "duration": 1.23
    },
    {
        "text": "It does.",
        "start": 2425.844,
        "duration": 0.31
    },
    {
        "text": "I just don't understand how it does it.",
        "start": 2426.274,
        "duration": 1.49
    },
    {
        "text": "yeah, I'm not accepting a simple\nexplanation because it leaves me",
        "start": 2428.764,
        "duration": 3.36
    },
    {
        "text": "still wondering how it does it.",
        "start": 2432.304,
        "duration": 1.06
    },
    {
        "text": "Yeah.",
        "start": 2434.144,
        "duration": 0.4
    },
    {
        "text": "Yeah, what were you going to say, Vivien?",
        "start": 2434.934,
        "duration": 0.93
    },
    {
        "text": "Yeah, basically the same thing.",
        "start": 2438.114,
        "duration": 1.26
    },
    {
        "text": "If you substitute the last word for too\nwide or too dangerous, it just depends",
        "start": 2439.374,
        "duration": 5.1
    },
    {
        "text": "on the last word, what it refers to.",
        "start": 2444.474,
        "duration": 2.3
    },
    {
        "text": "Yeah.",
        "start": 2447.284,
        "duration": 0.5
    },
    {
        "text": "No, it's a good point.",
        "start": 2448.114,
        "duration": 1.57
    },
    {
        "text": "And that's why this kind of, I guess\none to one comparison is a bit odd",
        "start": 2449.684,
        "duration": 4.86
    },
    {
        "text": "that it's doing and how it integrates\nthis, but it may be a hierarchy thing.",
        "start": 2454.584,
        "duration": 4.07
    },
    {
        "text": "I think this is literally\nextracted from an actual network.",
        "start": 2458.674,
        "duration": 2.97
    },
    {
        "text": "I don't think someone\nmade these values up.",
        "start": 2461.644,
        "duration": 1.65
    },
    {
        "text": "I don't know if, it's here, they've\nsaid that the embedding is for the",
        "start": 2465.364,
        "duration": 4.45
    },
    {
        "text": "word it, but maybe actually the\nembedding is for the word it after",
        "start": 2469.814,
        "duration": 3.14
    },
    {
        "text": "it's been processed by five layers.",
        "start": 2472.954,
        "duration": 2.56
    },
    {
        "text": "And by that point, it's understood.",
        "start": 2475.874,
        "duration": 2.88
    },
    {
        "text": "Yeah.",
        "start": 2478.804,
        "duration": 0.3
    },
    {
        "text": "You can look at evolution from layers.",
        "start": 2479.379,
        "duration": 2.11
    },
    {
        "text": "I, have that same library and I, to look\nat from layer 1 to 12, it changes a lot.",
        "start": 2482.009,
        "duration": 5.09
    },
    {
        "text": "So probably on the first layer,\nit was referring to street, but",
        "start": 2487.109,
        "duration": 3.99
    },
    {
        "text": "then at the second one, it already\nencompasses the entire, the spousal.",
        "start": 2491.099,
        "duration": 3.98
    },
    {
        "text": "That information is helping decide whether\nit refers to frame or it refers to script.",
        "start": 2495.814,
        "duration": 5.64
    },
    {
        "text": "And you're one of layer 5, so I'll\nwrite mine to 5 layers, but you can,",
        "start": 2501.904,
        "duration": 5.53
    },
    {
        "text": "clearly see the evolution within layer.",
        "start": 2507.994,
        "duration": 1.83
    },
    {
        "text": "Okay, that helps.",
        "start": 2510.234,
        "duration": 0.72
    },
    {
        "text": "so yeah, so what are we\ngoing to do with this?",
        "start": 2516.789,
        "duration": 1.52
    },
    {
        "text": "Oh yeah.",
        "start": 2518.529,
        "duration": 0.28
    },
    {
        "text": "I want to make sure I'm just clear.",
        "start": 2519.139,
        "duration": 1.31
    },
    {
        "text": "The token for the word it, there's nothing\nelse besides just the token, right?",
        "start": 2520.679,
        "duration": 4.52
    },
    {
        "text": "There's no, meaning to that thing other\nthan its associations with other words.",
        "start": 2526.639,
        "duration": 3.43
    },
    {
        "text": "Is that correct?",
        "start": 2530.149,
        "duration": 1.06
    },
    {
        "text": "I have, I, when I say the word\nstreet, I have all kinds of",
        "start": 2532.699,
        "duration": 3.97
    },
    {
        "text": "associations with the word street.",
        "start": 2536.669,
        "duration": 1.41
    },
    {
        "text": "I can, and I can visualize\nit, I can think about it.",
        "start": 2538.869,
        "duration": 3.9
    },
    {
        "text": "Particular streets, I can imagine\ntouching it or crawling across it,",
        "start": 2543.084,
        "duration": 2.9
    },
    {
        "text": "whatever, I get, I have a lot of,\nit's not just a, it's not just a word.",
        "start": 2545.984,
        "duration": 3.5
    },
    {
        "text": "That's just a token.",
        "start": 2550.199,
        "duration": 0.9
    },
    {
        "text": "And so I've always had the impression\nthat in, these large language models,",
        "start": 2551.549,
        "duration": 4.92
    },
    {
        "text": "the tokens are in some sense meaningless.",
        "start": 2556.559,
        "duration": 2.77
    },
    {
        "text": "The only meaning comes from what\nthey're associated with other tokens.",
        "start": 2559.329,
        "duration": 5.35
    },
    {
        "text": "So it's a token referring\nto a token, but there's no",
        "start": 2564.939,
        "duration": 2.19
    },
    {
        "text": "grounding what these tokens mean.",
        "start": 2567.139,
        "duration": 1.22
    },
    {
        "text": "I think there's a learned\nrepresentation for each word.",
        "start": 2568.819,
        "duration": 3.44
    },
    {
        "text": "So street and road\nmight have very similar.",
        "start": 2572.269,
        "duration": 2.0
    },
    {
        "text": "But there's still, it's\njust all between tokens.",
        "start": 2574.999,
        "duration": 3.08
    },
    {
        "text": "There's.",
        "start": 2578.099,
        "duration": 0.36
    },
    {
        "text": "There is nothing else besides,",
        "start": 2579.314,
        "duration": 2.12
    },
    {
        "text": "it's just all correspondence\nwith other tokens.",
        "start": 2586.024,
        "duration": 2.43
    },
    {
        "text": "All the definition of what a token\nis, how it relates to other tokens.",
        "start": 2588.514,
        "duration": 3.85
    },
    {
        "text": "And nothing else besides that.",
        "start": 2593.309,
        "duration": 1.59
    },
    {
        "text": "Yeah.",
        "start": 2595.279,
        "duration": 0.09
    },
    {
        "text": "Okay.",
        "start": 2595.759,
        "duration": 0.3
    },
    {
        "text": "But that's true for every aspect of this.",
        "start": 2596.209,
        "duration": 2.13
    },
    {
        "text": "It's just, I know, I just,\nhuge, not even multimodal ones.",
        "start": 2598.549,
        "duration": 4.08
    },
    {
        "text": "So have an association of\nthree and I image of three.",
        "start": 2602.629,
        "duration": 5.395
    },
    {
        "text": "But even those who just tokens, right?",
        "start": 2609.439,
        "duration": 1.92
    },
    {
        "text": "They're, I make the same argument,\nthe tokens and the image processing.",
        "start": 2611.359,
        "duration": 4.32
    },
    {
        "text": "it would be associating, it would see\nwhat sort of images are associated with",
        "start": 2617.209,
        "duration": 3.93
    },
    {
        "text": "what sort of words in a stream purpose.",
        "start": 2621.139,
        "duration": 1.68
    },
    {
        "text": "Yeah.",
        "start": 2622.819,
        "duration": 0.3
    },
    {
        "text": "yeah, would the brain have something\nsimilar, just with neural activity at",
        "start": 2624.719,
        "duration": 4.77
    },
    {
        "text": "a certain I think, I think the brain, I\nthink language is, comes after it, right?",
        "start": 2629.489,
        "duration": 5.66
    },
    {
        "text": "It doesn't start on you.",
        "start": 2635.149,
        "duration": 1.1
    },
    {
        "text": "And we start with a model of the\nworld, and dogs have it, cats have",
        "start": 2636.949,
        "duration": 3.75
    },
    {
        "text": "it, and birds have it, and we have it.",
        "start": 2640.699,
        "duration": 1.53
    },
    {
        "text": "And then some animals like us\nhave the ability to express",
        "start": 2642.459,
        "duration": 4.53
    },
    {
        "text": "that model through tokens like,\nbut it goes in that direction.",
        "start": 2647.049,
        "duration": 5.56
    },
    {
        "text": "It goes from a model of the\nworld to a communication token.",
        "start": 2652.619,
        "duration": 3.92
    },
    {
        "text": "We're here, we start with the\ncommunication tokens, and there's",
        "start": 2657.059,
        "duration": 2.75
    },
    {
        "text": "no model of the world other than the\nrelationship of the communication",
        "start": 2659.809,
        "duration": 2.89
    },
    {
        "text": "tokens to other communication tokens.",
        "start": 2662.699,
        "duration": 1.44
    },
    {
        "text": "And, there's no tie to specific\nmovements or specific behaviors",
        "start": 2664.799,
        "duration": 5.19
    },
    {
        "text": "or anything like that.",
        "start": 2670.099,
        "duration": 0.59
    },
    {
        "text": "It's just tokens to tokens.",
        "start": 2670.689,
        "duration": 1.54
    },
    {
        "text": "That's how it works really\nwell, but that's to make sure",
        "start": 2672.299,
        "duration": 2.88
    },
    {
        "text": "I understood it correctly.",
        "start": 2675.349,
        "duration": 0.68
    },
    {
        "text": "Yeah, no, I think, that is fair to say.",
        "start": 2677.059,
        "duration": 1.8
    },
    {
        "text": "And yeah, exactly.",
        "start": 2678.939,
        "duration": 1.31
    },
    {
        "text": "there's no explicit models.",
        "start": 2680.469,
        "duration": 0.9
    },
    {
        "text": "There's no kind of sensorimotor\nreally understanding of them.",
        "start": 2681.379,
        "duration": 2.82
    },
    {
        "text": "What if you learn a model in a\nsensorimotor environment, and then",
        "start": 2684.689,
        "duration": 4.086
    },
    {
        "text": "you associate it with language?",
        "start": 2688.775,
        "duration": 1.712
    },
    {
        "text": "that's what we do.",
        "start": 2690.829,
        "duration": 1.53
    },
    {
        "text": "but you could do that here too.",
        "start": 2693.719,
        "duration": 1.29
    },
    {
        "text": "if you have that sensorimotor model, yes,\nbut there is no sensorimotor model here.",
        "start": 2696.139,
        "duration": 3.43
    },
    {
        "text": "some models do, yeah.",
        "start": 2700.349,
        "duration": 1.14
    },
    {
        "text": "yeah.",
        "start": 2703.579,
        "duration": 0.4
    },
    {
        "text": "Let's get to that.",
        "start": 2704.309,
        "duration": 0.31
    },
    {
        "text": "It's very hard to understand sensorimotor\nmodels without reference frames, so",
        "start": 2706.629,
        "duration": 3.0
    },
    {
        "text": "I need to understand that better.",
        "start": 2710.049,
        "duration": 1.439
    },
    {
        "text": "Okay.",
        "start": 2713.949,
        "duration": 0.29
    },
    {
        "text": "But just, yeah.",
        "start": 2714.239,
        "duration": 0.78
    },
    {
        "text": "Oh, yeah.",
        "start": 2715.689,
        "duration": 0.48
    },
    {
        "text": "Yeah.",
        "start": 2718.309,
        "duration": 0.19
    },
    {
        "text": "Sorry.",
        "start": 2718.499,
        "duration": 0.22
    },
    {
        "text": "Go ahead, Vivien.",
        "start": 2718.719,
        "duration": 0.54
    },
    {
        "text": "this might be more of a side question,\nbut, is it predetermined where a",
        "start": 2721.619,
        "duration": 4.29
    },
    {
        "text": "token starts and where it ends?",
        "start": 2725.929,
        "duration": 1.65
    },
    {
        "text": "Or can it also learn that on its own?",
        "start": 2727.609,
        "duration": 2.58
    },
    {
        "text": "for example, tired split into two tokens.",
        "start": 2730.359,
        "duration": 2.75
    },
    {
        "text": "Yeah.",
        "start": 2734.159,
        "duration": 0.35
    },
    {
        "text": "So that's, yeah.",
        "start": 2734.509,
        "duration": 1.305
    },
    {
        "text": "So there's, when you are starting\nwith like sentences, like natural",
        "start": 2735.814,
        "duration": 5.075
    },
    {
        "text": "language sentences, you have two steps.",
        "start": 2740.889,
        "duration": 1.53
    },
    {
        "text": "You have tokenization and\nthen the token embeddings.",
        "start": 2742.419,
        "duration": 3.2
    },
    {
        "text": "The token embeddings is, what\ntransforms it into a vector of",
        "start": 2746.179,
        "duration": 3.78
    },
    {
        "text": "a, fixed length or whatever.",
        "start": 2750.139,
        "duration": 1.65
    },
    {
        "text": "Tokenization is how you decide to,\nyeah, basically parse sentences",
        "start": 2752.529,
        "duration": 4.33
    },
    {
        "text": "into, these kind of components.",
        "start": 2756.869,
        "duration": 2.67
    },
    {
        "text": "And there's a variety of\ndifferent ways you could do it.",
        "start": 2760.059,
        "duration": 2.2
    },
    {
        "text": "You could do it at a character level.",
        "start": 2762.269,
        "duration": 1.44
    },
    {
        "text": "but then your sequences are\ngoing to get super long.",
        "start": 2764.699,
        "duration": 2.51
    },
    {
        "text": "And so that's really computationally\ninefficient for transformers.",
        "start": 2767.279,
        "duration": 2.56
    },
    {
        "text": "Or you could do it at a word level,\nbut then the kind of vocabulary, that",
        "start": 2770.344,
        "duration": 4.39
    },
    {
        "text": "you're dealing with, becomes enormous.",
        "start": 2774.734,
        "duration": 2.68
    },
    {
        "text": "So in practice, most of them do\nsub word, tokens, so they split",
        "start": 2777.794,
        "duration": 5.08
    },
    {
        "text": "up, I think, almost syllables.",
        "start": 2782.874,
        "duration": 5.46
    },
    {
        "text": "And that's what's used, I think, in\nall the kind of recent GPT models.",
        "start": 2789.809,
        "duration": 3.71
    },
    {
        "text": "Is that true?",
        "start": 2793.719,
        "duration": 0.72
    },
    {
        "text": "syllables?",
        "start": 2794.689,
        "duration": 0.5
    },
    {
        "text": "This is showing a couple,\nthat didn't work entirely.",
        "start": 2795.189,
        "duration": 4.41
    },
    {
        "text": "This is verbs that, you\nknow, or concatenation.",
        "start": 2800.849,
        "duration": 2.45
    },
    {
        "text": "This is definitely, I\nthink, a simplified one.",
        "start": 2804.969,
        "duration": 1.96
    },
    {
        "text": "Do they really work at\nthe level of syllables?",
        "start": 2807.259,
        "duration": 2.96
    },
    {
        "text": "Yeah, normally, Lucas or someone else\ncan correct me if I'm wrong, but I'm",
        "start": 2811.749,
        "duration": 4.44
    },
    {
        "text": "pretty sure that's, almost all of them.",
        "start": 2816.299,
        "duration": 2.19
    },
    {
        "text": "it's actually, they start with\nsingle characters, but then",
        "start": 2818.929,
        "duration": 3.6
    },
    {
        "text": "there is a fixed vocabulary, so\nI can only have 50, 000 tokens.",
        "start": 2822.539,
        "duration": 4.12
    },
    {
        "text": "So then they aggregate.",
        "start": 2827.079,
        "duration": 1.32
    },
    {
        "text": "The least common ones.",
        "start": 2828.999,
        "duration": 1.23
    },
    {
        "text": "So you're going to end up with some\ntokens which are long, which are",
        "start": 2830.309,
        "duration": 2.75
    },
    {
        "text": "like a full word, but they rarely\nappear, but some ones would just",
        "start": 2833.059,
        "duration": 3.29
    },
    {
        "text": "be like one or two characters.",
        "start": 2836.349,
        "duration": 1.6
    },
    {
        "text": "Interesting.",
        "start": 2837.999,
        "duration": 0.54
    },
    {
        "text": "So they say we got 50, 000 tokens,\nhere's a whole bunch of corpus",
        "start": 2839.379,
        "duration": 3.17
    },
    {
        "text": "of text, let's figure out some\nefficient way of picking out 50,",
        "start": 2842.549,
        "duration": 3.569
    },
    {
        "text": "000 tokens that covers all the text.",
        "start": 2846.119,
        "duration": 3.08
    },
    {
        "text": "It's like a clustering operation.",
        "start": 2849.199,
        "duration": 1.14
    },
    {
        "text": "Yeah, it's really weird because that's\nreally interesting, but it's also weird.",
        "start": 2850.349,
        "duration": 5.195
    },
    {
        "text": "I can see why that would be good.",
        "start": 2858.014,
        "duration": 1.48
    },
    {
        "text": "I can see why it would work, but\nit's just all very statistical.",
        "start": 2859.754,
        "duration": 3.18
    },
    {
        "text": "so the tokens themselves, may\nsurprise you when you look at them.",
        "start": 2864.574,
        "duration": 5.135
    },
    {
        "text": "Yeah, it's an interesting\nway of, tokenizing language.",
        "start": 2869.709,
        "duration": 8.795
    },
    {
        "text": "But once again, it seems like\nit might move you away from any",
        "start": 2879.944,
        "duration": 2.92
    },
    {
        "text": "sort of, what we might think\nabout, meaning of these tokens.",
        "start": 2882.864,
        "duration": 4.95
    },
    {
        "text": "I have some strange little letter\nsequence that has no meaning on",
        "start": 2888.414,
        "duration": 4.01
    },
    {
        "text": "its own, but it's useful in the\ncontext of building models like this.",
        "start": 2892.424,
        "duration": 2.66
    },
    {
        "text": "again, it proves that you're really\ngoing towards a very statistical",
        "start": 2898.124,
        "duration": 3.17
    },
    {
        "text": "approach to this whole problem.",
        "start": 2901.294,
        "duration": 1.88
    },
    {
        "text": "You're just taking it that way.",
        "start": 2904.124,
        "duration": 1.0
    },
    {
        "text": "Okay, that's really interesting.",
        "start": 2905.784,
        "duration": 0.89
    },
    {
        "text": "I didn't know that.",
        "start": 2906.674,
        "duration": 0.51
    },
    {
        "text": "That's fascinating.",
        "start": 2907.234,
        "duration": 0.9
    },
    {
        "text": "so I have a question which was brought up\nearlier, this is at layer five, so by this",
        "start": 2909.084,
        "duration": 6.69
    },
    {
        "text": "time, whatever was the token animal, is\nnow some conglomeration of something else.",
        "start": 2915.774,
        "duration": 7.705
    },
    {
        "text": "the fact that they're pointing backwards\nthrough the discrete beginning token,",
        "start": 2925.729,
        "duration": 4.03
    },
    {
        "text": "I think is what's taking Jeff off\ntrack on what's actually possible.",
        "start": 2929.789,
        "duration": 4.959
    },
    {
        "text": "That's a fair point.",
        "start": 2934.749,
        "duration": 0.54
    },
    {
        "text": "Yeah.",
        "start": 2935.759,
        "duration": 0.43
    },
    {
        "text": "No, it is a bit misleading.",
        "start": 2936.259,
        "duration": 1.08
    },
    {
        "text": "I, I don't know, I didn't follow that,\nKevin, but I think I understand this.",
        "start": 2938.149,
        "duration": 5.69
    },
    {
        "text": "I think it's important you\nshould understand that this",
        "start": 2944.419,
        "duration": 3.29
    },
    {
        "text": "is not a direct mapping.",
        "start": 2947.709,
        "duration": 2.06
    },
    {
        "text": "This is an inferential mapping.",
        "start": 2949.819,
        "duration": 1.58
    },
    {
        "text": "By layer five, you won't see the token\nbee or animal or anything like that.",
        "start": 2951.589,
        "duration": 4.41
    },
    {
        "text": "You'll see it's gone through five\nlayers of transformer that have been",
        "start": 2955.999,
        "duration": 5.61
    },
    {
        "text": "processed and there's A more abstract\nrepresentation of all this stuff.",
        "start": 2961.609,
        "duration": 5.775
    },
    {
        "text": "So you can go backwards and look\nat what the receptive field was.",
        "start": 2967.984,
        "duration": 4.76
    },
    {
        "text": "But, What you're actually seeing there\nwhen it's saying it's, these things",
        "start": 2973.404,
        "duration": 4.425
    },
    {
        "text": "all, how they're related to it is a\nlittle, is more abstract, you've already",
        "start": 2977.829,
        "duration": 6.23
    },
    {
        "text": "probably parsed out whether it's a noun\nand a verb and a bunch of other things.",
        "start": 2984.189,
        "duration": 3.57
    },
    {
        "text": "So I just want to make sure that\nthis, the, what they're trying to",
        "start": 2988.249,
        "duration": 6.39
    },
    {
        "text": "convey here is a concretization of\nan abstraction which is, much richer.",
        "start": 2994.649,
        "duration": 8.05
    },
    {
        "text": "Yeah, they're putting words\non it so we can make sense of",
        "start": 3003.249,
        "duration": 2.04
    },
    {
        "text": "it, but it's not like this.",
        "start": 3005.289,
        "duration": 1.01
    },
    {
        "text": "yeah, it's that literal.",
        "start": 3008.054,
        "duration": 1.03
    },
    {
        "text": "Great,",
        "start": 3013.124,
        "duration": 0.34
    },
    {
        "text": "yeah, so just the kind of final key\nelement of the self attention operation",
        "start": 3016.064,
        "duration": 3.34
    },
    {
        "text": "then is, so we, found out first, with\nthe queries and the key, the query and",
        "start": 3019.404,
        "duration": 4.37
    },
    {
        "text": "the keys, how, how much this, token,\nshould attend to these other ones",
        "start": 3023.774,
        "duration": 7.31
    },
    {
        "text": "and integrate information from them.",
        "start": 3031.594,
        "duration": 2.665
    },
    {
        "text": "And in addition to, associated with\neach one of these tokens is this",
        "start": 3034.259,
        "duration": 4.485
    },
    {
        "text": "third representation, again, a vector,\nwhich is what's called its value.",
        "start": 3038.744,
        "duration": 3.85
    },
    {
        "text": "And this is meant to be capturing more,\nI guess it's semantic meaning, directly.",
        "start": 3043.409,
        "duration": 4.4
    },
    {
        "text": "And what we're then going to do\nwhen, updating the token for it,",
        "start": 3048.719,
        "duration": 5.26
    },
    {
        "text": "or whatever this is at this point.",
        "start": 3054.069,
        "duration": 1.37
    },
    {
        "text": "I'm sorry, what did you say?",
        "start": 3055.589,
        "duration": 1.229
    },
    {
        "text": "You said a semantic meaning?",
        "start": 3056.819,
        "duration": 1.6
    },
    {
        "text": "Yeah.",
        "start": 3059.789,
        "duration": 0.5
    },
    {
        "text": "Or, yeah, essentially what the\nkind of the representation that",
        "start": 3060.479,
        "duration": 3.92
    },
    {
        "text": "we want to pass on, onwards.",
        "start": 3064.399,
        "duration": 1.62
    },
    {
        "text": "So key and query are about This\ncrosstalk, like how do you figure",
        "start": 3066.309,
        "duration": 4.405
    },
    {
        "text": "out who you should be talking to?",
        "start": 3070.714,
        "duration": 1.62
    },
    {
        "text": "Value is about how are you going\nto encode your own representation,",
        "start": 3072.664,
        "duration": 3.59
    },
    {
        "text": "then pass on to the next, level.",
        "start": 3076.714,
        "duration": 2.45
    },
    {
        "text": "All right, and how does that work?",
        "start": 3080.504,
        "duration": 1.09
    },
    {
        "text": "to get from the kind of current,\ntoken representation, I'll show",
        "start": 3083.844,
        "duration": 3.18
    },
    {
        "text": "that a bit more on this next, slide.",
        "start": 3087.024,
        "duration": 2.03
    },
    {
        "text": "You, just, yeah, do a matrix,\nmultiplication with a learned, Kind of",
        "start": 3089.084,
        "duration": 5.71
    },
    {
        "text": "set of weights to get that value, but\nthe kind of important thing about self",
        "start": 3094.874,
        "duration": 4.07
    },
    {
        "text": "attention is it's going to take all of\nthe values, and essentially combine them",
        "start": 3098.944,
        "duration": 5.04
    },
    {
        "text": "based on these attention, values here.",
        "start": 3104.324,
        "duration": 3.16
    },
    {
        "text": "So essentially the larger the,\nprobability from here, the more",
        "start": 3107.874,
        "duration": 4.08
    },
    {
        "text": "influence that value is going to\nhave on the updated representation.",
        "start": 3111.964,
        "duration": 3.82
    },
    {
        "text": "But where did the value come from?",
        "start": 3116.234,
        "duration": 1.43
    },
    {
        "text": "How does it determine?",
        "start": 3118.124,
        "duration": 1.24
    },
    {
        "text": "It, the network learns what a good,\nkind of value representation to use is.",
        "start": 3120.314,
        "duration": 4.38
    },
    {
        "text": "and it learns that through what mechanism?",
        "start": 3126.339,
        "duration": 2.233
    },
    {
        "text": "Backpropagation.",
        "start": 3128.572,
        "duration": 0.756
    },
    {
        "text": "The backpropagation.",
        "start": 3129.609,
        "duration": 0.58
    },
    {
        "text": "Okay.",
        "start": 3130.529,
        "duration": 0.21
    },
    {
        "text": "Yeah, all of this, the key and\nthe query transformations are also",
        "start": 3131.739,
        "duration": 3.11
    },
    {
        "text": "learned through backpropagation.",
        "start": 3134.849,
        "duration": 1.24
    },
    {
        "text": "it's a, parallel operation.",
        "start": 3140.199,
        "duration": 1.43
    },
    {
        "text": "they're, the token comes in, the vector\nrepresentation comes in, and each of",
        "start": 3142.399,
        "duration": 5.23
    },
    {
        "text": "them go through their own separate\nmatrices to produce these values.",
        "start": 3147.629,
        "duration": 5.11
    },
    {
        "text": "that gives you the\nability to, any one stage.",
        "start": 3153.699,
        "duration": 3.18
    },
    {
        "text": "represent, produce a query key and value\nfor any incoming token, so to speak.",
        "start": 3158.129,
        "duration": 6.6
    },
    {
        "text": "Yeah.",
        "start": 3167.959,
        "duration": 0.41
    },
    {
        "text": "Okay, as soon as you tell me it's,\nlearned by backprop, then I stop",
        "start": 3169.419,
        "duration": 4.98
    },
    {
        "text": "thinking of it as in terms of any\nkind of thing I can think about.",
        "start": 3174.399,
        "duration": 3.59
    },
    {
        "text": "Yeah, correct.",
        "start": 3179.199,
        "duration": 1.29
    },
    {
        "text": "Correct.",
        "start": 3181.484,
        "duration": 0.44
    },
    {
        "text": "Correct.",
        "start": 3182.474,
        "duration": 0.2
    },
    {
        "text": "It's an encoding scheme\nif you want to wish.",
        "start": 3182.674,
        "duration": 3.49
    },
    {
        "text": "It's basically saying how do these things\nrelate to each other in some kind of",
        "start": 3186.214,
        "duration": 4.16
    },
    {
        "text": "statistical sense, and but there's a huge\namount of context associated with that",
        "start": 3190.374,
        "duration": 5.17
    },
    {
        "text": "in when you do that, when you, because\nthey, pull things apart into query key and",
        "start": 3196.034,
        "duration": 6.11
    },
    {
        "text": "value run through what they call multiple\nheads, which are kind of Representations",
        "start": 3202.144,
        "duration": 5.87
    },
    {
        "text": "of the self attention, then they all\nget mixed together in another layer,",
        "start": 3208.014,
        "duration": 3.51
    },
    {
        "text": "and then repeat, for multiple layers.",
        "start": 3211.774,
        "duration": 3.26
    },
    {
        "text": "So there's a huge amount of abstraction\nthat you go through in this thing.",
        "start": 3215.344,
        "duration": 3.33
    },
    {
        "text": "That, through the magic of\nbackpropagation, it learns how",
        "start": 3219.104,
        "duration": 5.73
    },
    {
        "text": "things are related to each other at\nmultiple levels of representation.",
        "start": 3224.864,
        "duration": 4.22
    },
    {
        "text": "Yeah, but I agree with you that the\nwords query, key, and value are actually",
        "start": 3229.244,
        "duration": 3.65
    },
    {
        "text": "more confusing than they're helpful.",
        "start": 3232.894,
        "duration": 1.53
    },
    {
        "text": "You just, if you just said\nthis is a value, this is a",
        "start": 3234.524,
        "duration": 3.13
    },
    {
        "text": "vector that was determined by\nbackprop, I'm like, okay, fine.",
        "start": 3237.724,
        "duration": 4.349
    },
    {
        "text": "The only thing that determines, one\nof the mathematical operations it",
        "start": 3242.074,
        "duration": 3.17
    },
    {
        "text": "does, to come up with an output.",
        "start": 3245.244,
        "duration": 2.58
    },
    {
        "text": "It's not really a query key.",
        "start": 3248.384,
        "duration": 1.78
    },
    {
        "text": "And it's not, you can't really\nsay it's semantic value.",
        "start": 3250.414,
        "duration": 2.51
    },
    {
        "text": "I don't know what you'd call it.",
        "start": 3253.004,
        "duration": 1.479
    },
    {
        "text": "Everything's semantic.",
        "start": 3254.484,
        "duration": 0.89
    },
    {
        "text": "It's just something I've determined\nby the backprop out there.",
        "start": 3256.774,
        "duration": 2.24
    },
    {
        "text": "Yeah, but I guess, it will probably tend\nto, cluster similar representations.",
        "start": 3259.994,
        "duration": 4.7
    },
    {
        "text": "we can go look at it later\nand try to figure this out.",
        "start": 3265.534,
        "duration": 2.88
    },
    {
        "text": "Sure.",
        "start": 3268.654,
        "duration": 0.3
    },
    {
        "text": "And I guess determine\nsomething interesting about it.",
        "start": 3270.074,
        "duration": 4.95
    },
    {
        "text": "yeah, and let's say the animal or\nwhatever, its value is some vector or",
        "start": 3275.784,
        "duration": 5.57
    },
    {
        "text": "whatever and then you have the adjective\nblue or whatever that's some other vector",
        "start": 3281.354,
        "duration": 4.0
    },
    {
        "text": "And through the self attention, it decides\nto prioritize those two and combine that.",
        "start": 3286.124,
        "duration": 4.43
    },
    {
        "text": "Then you can imagine that the new\nvector direction maybe is this",
        "start": 3291.024,
        "duration": 3.75
    },
    {
        "text": "combination of these two, concepts.",
        "start": 3295.254,
        "duration": 3.58
    },
    {
        "text": "or at least an interpolation between them.",
        "start": 3300.214,
        "duration": 1.78
    },
    {
        "text": "Obviously, it's an open question\nhow useful that actually is, but",
        "start": 3302.664,
        "duration": 3.35
    },
    {
        "text": "that's the basic idea of,\ntransformer self attention.",
        "start": 3310.644,
        "duration": 2.93
    },
    {
        "text": "Just to briefly show it again, just, in\na more kind of, yeah, vectorized, matrix",
        "start": 3313.954,
        "duration": 5.72
    },
    {
        "text": "free, format, we have our embeddings here.",
        "start": 3319.694,
        "duration": 3.64
    },
    {
        "text": "these are the, kind of vectors coming in.",
        "start": 3324.684,
        "duration": 1.84
    },
    {
        "text": "this, yeah, as pointed\nout, this is layer five.",
        "start": 3327.634,
        "duration": 2.8
    },
    {
        "text": "they would actually be more abstract\nat this point than word embeddings.",
        "start": 3331.224,
        "duration": 3.01
    },
    {
        "text": "But, those are the ones coming in, and,\nwe can, transform them by these, learned",
        "start": 3334.839,
        "duration": 6.75
    },
    {
        "text": "weight, matrices to get the query vectors,\nthe key vectors, and the value vectors.",
        "start": 3341.589,
        "duration": 5.62
    },
    {
        "text": "And you see there's one for each,\nword or each token coming in rather.",
        "start": 3347.249,
        "duration": 4.65
    },
    {
        "text": "and this is just showing the\nsame operation that basically",
        "start": 3353.249,
        "duration": 2.55
    },
    {
        "text": "we're gonna do the dot product\nbetween the query and the key.",
        "start": 3355.799,
        "duration": 2.72
    },
    {
        "text": "You do that for the, token itself as well.",
        "start": 3358.519,
        "duration": 2.89
    },
    {
        "text": "So it also asks.",
        "start": 3361.409,
        "duration": 1.01
    },
    {
        "text": "Should I attend to myself, basically, and\nyou do it for, basically all of the other,",
        "start": 3362.949,
        "duration": 5.84
    },
    {
        "text": "tokens that you're going to compare to.",
        "start": 3369.769,
        "duration": 1.63
    },
    {
        "text": "Then you do the softmax, you get the\nprobability, associated with that.",
        "start": 3372.169,
        "duration": 4.04
    },
    {
        "text": "And then the output of all of\nthis is, yeah, I said the, scaled",
        "start": 3376.779,
        "duration": 5.69
    },
    {
        "text": "sum, of all of these, values\nbased on their, softmax values.",
        "start": 3382.469,
        "duration": 5.41
    },
    {
        "text": "and then the last thing I just wanted\nto say is on this is as Kevin mentioned,",
        "start": 3391.939,
        "duration": 4.01
    },
    {
        "text": "there's multiple heads involved in this.",
        "start": 3395.959,
        "duration": 1.9
    },
    {
        "text": "So what does that mean?",
        "start": 3397.869,
        "duration": 1.04
    },
    {
        "text": "So as in, you can learn one of\nthese weight operations, but that's",
        "start": 3398.909,
        "duration": 4.06
    },
    {
        "text": "going to be reused at every point.",
        "start": 3402.969,
        "duration": 2.16
    },
    {
        "text": "So it's going to be, for every token,\nyou're going to use the same kind of",
        "start": 3405.519,
        "duration": 5.74
    },
    {
        "text": "operation to do a query to generate\nthe query vectors and so forth.",
        "start": 3411.259,
        "duration": 4.14
    },
    {
        "text": "But you could imagine you'd want to\ndo, Different types of transformations",
        "start": 3416.199,
        "duration": 4.01
    },
    {
        "text": "to yeah, represent different\nthings, different kind of spaces.",
        "start": 3420.299,
        "duration": 3.27
    },
    {
        "text": "And so that's what these\nmultiple heads are doing.",
        "start": 3424.199,
        "duration": 2.57
    },
    {
        "text": "So they're all going to, each input token\nis going to get processed by all these",
        "start": 3426.769,
        "duration": 4.38
    },
    {
        "text": "different, attention heads, separately\nin parallel, and that's going to produce",
        "start": 3431.149,
        "duration": 7.155
    },
    {
        "text": "a bunch of these, different, zeds.",
        "start": 3438.304,
        "duration": 2.97
    },
    {
        "text": "and then it's that, feedforward layer\nyou saw at the very kind of start,",
        "start": 3442.254,
        "duration": 3.41
    },
    {
        "text": "that's essentially just using a, a multi\nlayer perceptron to, to combine those.",
        "start": 3445.984,
        "duration": 4.06
    },
    {
        "text": "Okay, finally on to then,\nsome comparisons of this.",
        "start": 3454.939,
        "duration": 4.42
    },
    {
        "text": "yeah, the kind of proposal is that\nself attention can do something",
        "start": 3460.729,
        "duration": 2.91
    },
    {
        "text": "like, voting as a special case with\nobviously some, significant caveats.",
        "start": 3463.659,
        "duration": 5.07
    },
    {
        "text": "And, yeah, again, just reminding the\nreason I'm, talking about all this is not",
        "start": 3469.639,
        "duration": 4.74
    },
    {
        "text": "to say that, oh, transformers have already\nsolved Monty or something like that.",
        "start": 3474.389,
        "duration": 3.81
    },
    {
        "text": "I'm, covering this to, to try and\nhighlight what are the things that we can",
        "start": 3479.039,
        "duration": 4.2
    },
    {
        "text": "maybe make transformers more Monty like.",
        "start": 3483.279,
        "duration": 1.93
    },
    {
        "text": "and, yeah, that's the main thing,\nand but if we can first understand",
        "start": 3486.714,
        "duration": 4.39
    },
    {
        "text": "what similarities there might be, I'm\nhaving trouble parsing this sentence.",
        "start": 3491.104,
        "duration": 4.39
    },
    {
        "text": "So it says, are you saying that\ntransformers, the self intention,",
        "start": 3495.884,
        "duration": 3.85
    },
    {
        "text": "is doing something like voting, or\nthat it could if we modified it?",
        "start": 3499.784,
        "duration": 3.62
    },
    {
        "text": "it can, assuming the\nright weights are learned.",
        "start": 3505.244,
        "duration": 2.95
    },
    {
        "text": "Okay, so maybe that's not happening\nnow, but maybe it could be made to",
        "start": 3509.019,
        "duration": 3.53
    },
    {
        "text": "do something like voting, is that it?",
        "start": 3512.559,
        "duration": 1.54
    },
    {
        "text": "Yeah.",
        "start": 3514.129,
        "duration": 0.38
    },
    {
        "text": "Okay.",
        "start": 3515.009,
        "duration": 0.42
    },
    {
        "text": "Yeah, and there's a reasonable chance\nthough that it is because the weights",
        "start": 3515.979,
        "duration": 3.18
    },
    {
        "text": "are basically the identity weights.",
        "start": 3519.159,
        "duration": 1.46
    },
    {
        "text": "So hopefully that'll become clear.",
        "start": 3520.869,
        "duration": 1.26
    },
    {
        "text": "I'm just showing this again\njust this sounds slightly",
        "start": 3523.489,
        "duration": 2.554
    },
    {
        "text": "It does sound conceptually different\nbecause in Monty, in voting, two models",
        "start": 3528.794,
        "duration": 6.01
    },
    {
        "text": "that have learned models of completely\ndifferent modalities can still vote on",
        "start": 3534.854,
        "duration": 4.97
    },
    {
        "text": "like object ID and pose, but here it seems\nlike the entire model across everything",
        "start": 3539.834,
        "duration": 5.42
    },
    {
        "text": "would have to be multimodal, like the\nwhole weight matrix and everything.",
        "start": 3545.264,
        "duration": 3.73
    },
    {
        "text": "the entire weight matrix\nwould have to be multimodal.",
        "start": 3551.224,
        "duration": 1.94
    },
    {
        "text": "They're not separate models\nthat vote, they're like one",
        "start": 3553.394,
        "duration": 2.47
    },
    {
        "text": "big model that has multimodal.",
        "start": 3555.864,
        "duration": 1.36
    },
    {
        "text": "Is that what you're saying?",
        "start": 3558.534,
        "duration": 0.82
    },
    {
        "text": "Yeah.",
        "start": 3560.334,
        "duration": 0.52
    },
    {
        "text": "as long as the representations, have,\nyeah, look similar, there's no reason",
        "start": 3564.014,
        "duration": 6.29
    },
    {
        "text": "they I can't communicate, or yeah, maybe\nI'm not understanding what you're saying.",
        "start": 3570.314,
        "duration": 5.225
    },
    {
        "text": "Or I guess, in Monty, in\nvoting, we never communicate",
        "start": 3576.479,
        "duration": 5.24
    },
    {
        "text": "any model details to each other.",
        "start": 3581.729,
        "duration": 1.76
    },
    {
        "text": "features are never\ncommunicated through voting.",
        "start": 3583.909,
        "duration": 2.19
    },
    {
        "text": "you don't know if the votes that come in\ncome from a touch model or a vision model.",
        "start": 3586.729,
        "duration": 4.15
    },
    {
        "text": "But here it seems like it does\nexplicitly tell you what it is",
        "start": 3591.499,
        "duration": 4.51
    },
    {
        "text": "sensing, like the, Embedding.",
        "start": 3596.019,
        "duration": 2.695
    },
    {
        "text": "Yeah.",
        "start": 3599.904,
        "duration": 0.29
    },
    {
        "text": "I think even in Monty, you could argue\nthere's a certain degree of that in that,",
        "start": 3600.694,
        "duration": 3.35
    },
    {
        "text": "I don't know, a 3d model of a mug isn't\ngoing to vote with the abstract concept",
        "start": 3605.154,
        "duration": 5.02
    },
    {
        "text": "of a family tree or something like that.",
        "start": 3610.174,
        "duration": 2.47
    },
    {
        "text": "like there's always going to be\na degree of, can things vote?",
        "start": 3614.184,
        "duration": 2.84
    },
    {
        "text": "but if we assume that the tokens are\nboth like object level representations,",
        "start": 3617.304,
        "duration": 3.55
    },
    {
        "text": "like one is, like in a multimodal\ntransformer, one's learned mugs through,",
        "start": 3620.864,
        "duration": 5.21
    },
    {
        "text": "what you call it, yeah, vision one's\nlearned it through touch or whatever.",
        "start": 3627.214,
        "duration": 2.37
    },
    {
        "text": "Then, then those can vote at least\nat the object level representation.",
        "start": 3629.899,
        "duration": 3.52
    },
    {
        "text": "Yeah, I, yeah, definitely to vote, you\nneed to know model of the same object.",
        "start": 3636.659,
        "duration": 5.51
    },
    {
        "text": "but I just mean like the, in Monty,\nwhat is communicated through votes",
        "start": 3643.169,
        "duration": 4.91
    },
    {
        "text": "seems a bit more limited than what\nis communicated in self attention.",
        "start": 3648.239,
        "duration": 4.41
    },
    {
        "text": "But yeah, it might just be more of a\nYeah, let, yeah, maybe bring that up again",
        "start": 3653.099,
        "duration": 5.97
    },
    {
        "text": "when, I've gotten to the, slide later.",
        "start": 3659.599,
        "duration": 1.86
    },
    {
        "text": "Because Yeah, yeah.",
        "start": 3661.469,
        "duration": 2.795
    },
    {
        "text": "And that's, maybe why I'm\nemphasizing special case, cause, self",
        "start": 3664.404,
        "duration": 4.68
    },
    {
        "text": "attention is doing a lot of things.",
        "start": 3669.084,
        "duration": 1.28
    },
    {
        "text": "and yeah, which I think, yeah, it relates\nto what you're saying, that it's, it",
        "start": 3671.744,
        "duration": 6.18
    },
    {
        "text": "can send a lot of information basically.",
        "start": 3677.924,
        "duration": 2.14
    },
    {
        "text": "but yeah, this is just bringing back\nthis kind of columnar view of, kind of",
        "start": 3681.684,
        "duration": 3.3
    },
    {
        "text": "Monty with these, Yeah, voting, we're\ngoing to compute these online, online,",
        "start": 3684.984,
        "duration": 4.72
    },
    {
        "text": "we're going to compute these relative\ndisplacements using the sensor modules.",
        "start": 3689.704,
        "duration": 2.83
    },
    {
        "text": "Basically, for each graph, we're going to\ngo through, and send the transformed poses",
        "start": 3693.234,
        "duration": 4.8
    },
    {
        "text": "using this relative displacement, find\nthe nearest neighbors to the receiving po",
        "start": 3698.034,
        "duration": 3.81
    },
    {
        "text": "points on the object, the receiving poses,\nand then add this incoming evidence,",
        "start": 3702.354,
        "duration": 4.71
    },
    {
        "text": "based on your kind of nearest neighbor.",
        "start": 3707.644,
        "duration": 1.34
    },
    {
        "text": "and then this is the kind of, transformer\nwith this kind of column view,",
        "start": 3712.399,
        "duration": 4.47
    },
    {
        "text": "and this, by the way, this, might be a\nlittle confusing, but hearing this reminds",
        "start": 3718.889,
        "duration": 5.45
    },
    {
        "text": "me, there's really two types of voting.",
        "start": 3724.339,
        "duration": 1.45
    },
    {
        "text": "We've never, we actually haven't\nreally teased them out before,",
        "start": 3725.789,
        "duration": 4.65
    },
    {
        "text": "We're saying, okay, we're like multiple\nfingers touching the mug at the same",
        "start": 3730.889,
        "duration": 2.735
    },
    {
        "text": "time, we need to know the relative\nposition, they're in the same modality,",
        "start": 3733.624,
        "duration": 3.11
    },
    {
        "text": "which is different, but that's not going\nto occur if I'm going across audition",
        "start": 3738.004,
        "duration": 4.99
    },
    {
        "text": "and touch or something like that.",
        "start": 3742.994,
        "duration": 2.12
    },
    {
        "text": "where you're really, you're not relying on\nknowing the relative poses of the sensors.",
        "start": 3746.824,
        "duration": 5.09
    },
    {
        "text": "you're really doing purely\nobject level voting.",
        "start": 3753.439,
        "duration": 2.94
    },
    {
        "text": "so I just want to point out that there\nwas this voting, we have to, in a",
        "start": 3757.169,
        "duration": 4.94
    },
    {
        "text": "particular modality, touch or vision,\nThis relative pose of the sensors",
        "start": 3762.159,
        "duration": 4.135
    },
    {
        "text": "is very important, but I don't think\nit's important across modalities.",
        "start": 3766.314,
        "duration": 3.16
    },
    {
        "text": "I think there's actually different,\nI've always argued that there's",
        "start": 3770.844,
        "duration": 2.53
    },
    {
        "text": "different levels of voting\ngoing on in a cortical column.",
        "start": 3773.394,
        "duration": 2.96
    },
    {
        "text": "There's multiple layers of cells and\nlayers two, three, and I think they're",
        "start": 3776.454,
        "duration": 2.62
    },
    {
        "text": "doing slightly different things.",
        "start": 3779.074,
        "duration": 1.24
    },
    {
        "text": "I just want to point it out that here\nwe're going to talk about voting, it's",
        "start": 3781.024,
        "duration": 2.64
    },
    {
        "text": "not going to go across modalities.",
        "start": 3783.664,
        "duration": 1.11
    },
    {
        "text": "It's just not exactly the same as\nwhat's happening in this image.",
        "start": 3784.804,
        "duration": 2.54
    },
    {
        "text": "this isn't a single modality, and we need\nto know, we need to know the features",
        "start": 3789.254,
        "duration": 4.35
    },
    {
        "text": "at, relative positions to return.",
        "start": 3793.604,
        "duration": 2.31
    },
    {
        "text": "It's a, I hope that wasn't confusing,\nbut it, they're not exactly the same.",
        "start": 3798.154,
        "duration": 4.13
    },
    {
        "text": "But yeah, Yeah, no, that's a good point.",
        "start": 3803.844,
        "duration": 3.21
    },
    {
        "text": "Yeah, okay, how could self\nattention be like voting?",
        "start": 3808.414,
        "duration": 3.85
    },
    {
        "text": "assume, that these, weight matrices\nwe normally learn are all the identity",
        "start": 3813.134,
        "duration": 4.77
    },
    {
        "text": "matrix, slash, we just don't use them.",
        "start": 3817.904,
        "duration": 2.17
    },
    {
        "text": "the key query, and value beddings are\ngoing to be the same, they're just",
        "start": 3821.174,
        "duration": 3.66
    },
    {
        "text": "going to be the original, vector, kind\nof representation, that was, coming in.",
        "start": 3824.834,
        "duration": 6.63
    },
    {
        "text": "What this means then is the more closely\nthose embeddings, so the X vectors,",
        "start": 3832.729,
        "duration": 4.83
    },
    {
        "text": "are already to each other, the more\nthey're going to attend to each other.",
        "start": 3837.789,
        "duration": 4.37
    },
    {
        "text": "and so what that basically means is, the\ntoken here is going to attend to itself,",
        "start": 3843.679,
        "duration": 5.49
    },
    {
        "text": "and about equally it's going to attend\nto this token, and similarly vice versa,",
        "start": 3849.269,
        "duration": 5.28
    },
    {
        "text": "they're both going to ignore this token.",
        "start": 3854.779,
        "duration": 2.24
    },
    {
        "text": "And, so what you'll get as the output of\nthis, the kind of value, is essentially",
        "start": 3857.559,
        "duration": 4.64
    },
    {
        "text": "just blending of these two similar\nones, which, maybe might correct for a",
        "start": 3862.199,
        "duration": 4.51
    },
    {
        "text": "bit of, pose disagreement between them.",
        "start": 3866.709,
        "duration": 2.07
    },
    {
        "text": "and so that's yeah, where, the\nsimilarity starts coming in.",
        "start": 3872.309,
        "duration": 3.16
    },
    {
        "text": "but you might reasonably wonder,\nokay, maybe this only works when",
        "start": 3876.499,
        "duration": 3.38
    },
    {
        "text": "they agree strongly with each other.",
        "start": 3880.129,
        "duration": 1.66
    },
    {
        "text": "what if there's legitimate\ndisagreement about objects?",
        "start": 3883.289,
        "duration": 2.22
    },
    {
        "text": "So we've talked about this before that,\nif, if the pose is consistent, mutually",
        "start": 3885.519,
        "duration": 5.235
    },
    {
        "text": "consistent for two different columns.",
        "start": 3890.754,
        "duration": 1.54
    },
    {
        "text": "let's say, you have a mug and\na phone next to each other.",
        "start": 3892.414,
        "duration": 4.15
    },
    {
        "text": "this phone isn't going to be telling\nthe column processing this mug.",
        "start": 3897.414,
        "duration": 3.23
    },
    {
        "text": "You should be seeing a phone because\nwith that relative displacement,",
        "start": 3900.674,
        "duration": 3.22
    },
    {
        "text": "then, you would, you would expect\nthat you'd be on a different, object.",
        "start": 3904.604,
        "duration": 4.83
    },
    {
        "text": "And so I'm not claiming transformers are\ndoing this because as mentioned, their",
        "start": 3910.144,
        "duration": 4.41
    },
    {
        "text": "kind of positional embeddings are a bit,\nare very fuzzy and not at all explicit.",
        "start": 3914.554,
        "duration": 5.77
    },
    {
        "text": "So But in theory, the kind of the actual\nself attention, mechanism has no issue",
        "start": 3920.324,
        "duration": 5.345
    },
    {
        "text": "with this, in that, if these different\ncolumns disagree completely with each",
        "start": 3925.669,
        "duration": 4.11
    },
    {
        "text": "other, then you will, using this kind\nof, format where, the, embedding is,",
        "start": 3929.779,
        "duration": 6.14
    },
    {
        "text": "the original embedding is the, key query\nand the value, then you're only going",
        "start": 3935.929,
        "duration": 4.47
    },
    {
        "text": "to intend to yourself, when you, yeah,\nwhen you differ from everyone else.",
        "start": 3940.399,
        "duration": 5.8
    },
    {
        "text": "So if the weight matrices.",
        "start": 3947.569,
        "duration": 1.24
    },
    {
        "text": "Oh, sorry.",
        "start": 3949.724,
        "duration": 0.65
    },
    {
        "text": "Oh, no, go ahead.",
        "start": 3951.304,
        "duration": 0.58
    },
    {
        "text": "if they're just the identity\nmatrix, how can the mug be",
        "start": 3952.804,
        "duration": 4.17
    },
    {
        "text": "recognized in the first place?",
        "start": 3957.004,
        "duration": 1.8
    },
    {
        "text": "Because the input will not be mug,\nit will be like a feature on the mug.",
        "start": 3958.844,
        "duration": 3.91
    },
    {
        "text": "So how does it even get to that stage?",
        "start": 3962.904,
        "duration": 2.89
    },
    {
        "text": "So there's a couple,\nyeah, answers to that.",
        "start": 3966.274,
        "duration": 1.8
    },
    {
        "text": "So one is, you could have this\nas a different attention head.",
        "start": 3968.074,
        "duration": 3.82
    },
    {
        "text": "You would have an attention head that\nis the identity matrix, and that's",
        "start": 3972.719,
        "duration": 3.42
    },
    {
        "text": "doing, voting like this, but you'd\nhave other attention heads that are",
        "start": 3976.139,
        "duration": 4.55
    },
    {
        "text": "doing more, more interesting things.",
        "start": 3980.699,
        "duration": 2.69
    },
    {
        "text": "and yeah, I'll get to that later.",
        "start": 3985.229,
        "duration": 1.64
    },
    {
        "text": "The other thing is, yeah, you're\nabsolutely right, because we have",
        "start": 3986.899,
        "duration": 3.5
    },
    {
        "text": "no kind of reference frames that\nare updated over time through",
        "start": 3990.489,
        "duration": 2.73
    },
    {
        "text": "movement, all this kind of stuff.",
        "start": 3993.219,
        "duration": 1.12
    },
    {
        "text": "We're relying entirely on voting\nfor, for understanding anything, or",
        "start": 3994.684,
        "duration": 5.53
    },
    {
        "text": "rather relying entirely on lateral\nconnections for understanding anything.",
        "start": 4000.214,
        "duration": 2.94
    },
    {
        "text": "So if you're only doing literally\nvoting, then, yeah, you're",
        "start": 4003.154,
        "duration": 4.09
    },
    {
        "text": "not going to get anywhere.",
        "start": 4007.244,
        "duration": 0.85
    },
    {
        "text": "but I think the most important question\nis, what if you have a source of noise?",
        "start": 4011.074,
        "duration": 3.82
    },
    {
        "text": "it's all well and good that if you\nalready decide you're a mug, You",
        "start": 4015.384,
        "duration": 3.02
    },
    {
        "text": "just say you're a mug, and whatever.",
        "start": 4018.404,
        "duration": 2.43
    },
    {
        "text": "That's essentially happening\nin both these cases, so not",
        "start": 4020.834,
        "duration": 2.48
    },
    {
        "text": "that particularly interesting.",
        "start": 4023.314,
        "duration": 0.82
    },
    {
        "text": "What's interesting about voting\nis, what if a column is unsure?",
        "start": 4024.134,
        "duration": 3.27
    },
    {
        "text": "can the consensus that's emerged around\nit in other columns help it to, become",
        "start": 4028.374,
        "duration": 5.68
    },
    {
        "text": "?\nmore certain.",
        "start": 4034.424,
        "duration": 0.705
    },
    {
        "text": "and so in this example, assume this\ncentral token has equal evidence",
        "start": 4036.679,
        "duration": 4.11
    },
    {
        "text": "for car and mug, and this is, yeah,\nagain, represented by maybe two,",
        "start": 4040.789,
        "duration": 4.39
    },
    {
        "text": "orthogonal, the kind of combination\nof two orthogonal, vectors.",
        "start": 4046.169,
        "duration": 4.61
    },
    {
        "text": "Basically, when you do this self\nattention, it's going to attend to",
        "start": 4054.684,
        "duration": 5.96
    },
    {
        "text": "itself because of course it's itself.",
        "start": 4060.644,
        "duration": 3.85
    },
    {
        "text": "But it's also going to attend a\nreasonable amount to these other objects",
        "start": 4065.814,
        "duration": 4.45
    },
    {
        "text": "around it, because it has some evidence\nthat it might be representing that.",
        "start": 4070.264,
        "duration": 3.42
    },
    {
        "text": "And what that means is when you\ndo the softmax, you're going to",
        "start": 4074.274,
        "duration": 3.2
    },
    {
        "text": "get, some kind of non zero values\nfor all these other columns.",
        "start": 4077.474,
        "duration": 3.96
    },
    {
        "text": "And basically, when you then produce\nthe new value for this token,",
        "start": 4082.184,
        "duration": 4.41
    },
    {
        "text": "it's going to be a, blending of\nthese, these different columns.",
        "start": 4087.124,
        "duration": 4.57
    },
    {
        "text": "Depending on how unsure or certain this\ncolumn is about whether it's a mug will",
        "start": 4091.744,
        "duration": 5.9
    },
    {
        "text": "depend on how much it ignores those\nthings around it versus how much it",
        "start": 4097.644,
        "duration": 3.66
    },
    {
        "text": "decides to integrate their evidence.",
        "start": 4101.314,
        "duration": 1.61
    },
    {
        "text": "So if it was like, 80, 90 percent sure\nit's a mug, but a little uncertain, it's",
        "start": 4103.684,
        "duration": 4.82
    },
    {
        "text": "going to much more quickly take in, that,\nor it's going to have a more significant",
        "start": 4108.504,
        "duration": 4.92
    },
    {
        "text": "kind of influence from the others.",
        "start": 4113.424,
        "duration": 1.09
    },
    {
        "text": "If it was only 10 percent sure it\nwas a mug, it's probably going to",
        "start": 4115.114,
        "duration": 3.13
    },
    {
        "text": "mostly attend to itself and just\nignore the other ones, around it.",
        "start": 4118.244,
        "duration": 3.18
    },
    {
        "text": "And actually, as the number of other\ncolumns, like adjacent columns that",
        "start": 4122.414,
        "duration": 3.0
    },
    {
        "text": "it's self attending with around it\nincreases, it's the more of an effect",
        "start": 4125.414,
        "duration": 4.98
    },
    {
        "text": "those, columns are going to have.",
        "start": 4130.394,
        "duration": 1.58
    },
    {
        "text": "does that, yeah, make\nsense, before I move on?",
        "start": 4135.934,
        "duration": 3.06
    },
    {
        "text": "Or does anyone take issue with",
        "start": 4141.324,
        "duration": 1.97
    },
    {
        "text": "Okay, yeah, because then I just\nsaw some fun random trivia.",
        "start": 4148.044,
        "duration": 3.69
    },
    {
        "text": "so that's, yeah, hopefully convinced\nyou that, self attention can,",
        "start": 4152.714,
        "duration": 3.97
    },
    {
        "text": "under these special circumstances,\ndo something like, voting.",
        "start": 4156.704,
        "duration": 4.72
    },
    {
        "text": "I then came across this interesting quote.",
        "start": 4161.874,
        "duration": 1.83
    },
    {
        "text": "the cortical columns are like,\nattention weighted interactions between",
        "start": 4164.404,
        "duration": 3.44
    },
    {
        "text": "Different word fragments in a multi head\ntransformer, but they are simpler because",
        "start": 4168.684,
        "duration": 2.49
    },
    {
        "text": "the query key values are all identical.",
        "start": 4171.174,
        "duration": 1.71
    },
    {
        "text": "And the role of the inter column\ninteractions is to identify these",
        "start": 4173.474,
        "duration": 4.17
    },
    {
        "text": "identical kind of embeddings.",
        "start": 4177.644,
        "duration": 1.14
    },
    {
        "text": "Yeah, any guesses on who said this?",
        "start": 4180.284,
        "duration": 2.06
    },
    {
        "text": "Geoffrey Hinton.",
        "start": 4182.344,
        "duration": 3.519
    },
    {
        "text": "Yes, that's correct.",
        "start": 4186.224,
        "duration": 0.933
    },
    {
        "text": "Regarding the GLOM architecture.",
        "start": 4187.157,
        "duration": 2.347
    },
    {
        "text": "Geoffrey Hinton.",
        "start": 4191.034,
        "duration": 0.66
    },
    {
        "text": "But I just think it's interesting\nbecause, yeah, when he came out with",
        "start": 4192.344,
        "duration": 3.71
    },
    {
        "text": "this GLOM architecture, a lot of people\ndrew parallels to Numenta's work.",
        "start": 4196.054,
        "duration": 3.23
    },
    {
        "text": "and which he, doesn't mention\nat all in the paper, even though",
        "start": 4200.669,
        "duration": 4.65
    },
    {
        "text": "he talks a lot about columns\nand basically describes voting.",
        "start": 4205.319,
        "duration": 2.85
    },
    {
        "text": "but he, he does compare\nhis work to transformers.",
        "start": 4209.629,
        "duration": 3.76
    },
    {
        "text": "And so I just thought, Yeah, it\nwas the same kind of point that the",
        "start": 4214.239,
        "duration": 4.39
    },
    {
        "text": "key query and value vectors are all\nidentical to the embedding vector.",
        "start": 4218.739,
        "duration": 3.11
    },
    {
        "text": "but then, there's definitely\nsome caveats to this.",
        "start": 4225.569,
        "duration": 2.75
    },
    {
        "text": "yeah, 100 percent the positional\nencodings are less explicit.",
        "start": 4228.829,
        "duration": 3.45
    },
    {
        "text": "They're all entangled in this.",
        "start": 4232.289,
        "duration": 1.6
    },
    {
        "text": "There's no, yeah, real clear notion\nof space in the same way as in Monty.",
        "start": 4233.889,
        "duration": 6.36
    },
    {
        "text": "this assumes pose, votings\nat the object level.",
        "start": 4242.119,
        "duration": 2.7
    },
    {
        "text": "I think this kind of gets to what you\nwere, saying earlier, Vivian, that",
        "start": 4244.819,
        "duration": 3.05
    },
    {
        "text": "yeah, like in Monty, where, we're\nvoting, what do you call it?",
        "start": 4250.039,
        "duration": 4.76
    },
    {
        "text": "Yeah, at the object level,\ndo we agree, mug, but,",
        "start": 4255.239,
        "duration": 3.56
    },
    {
        "text": "oh yeah, but sorry, but then the pose,\nin, Monty, we're where are we on the mug?",
        "start": 4261.099,
        "duration": 4.66
    },
    {
        "text": "Whereas what I've represented\nhere, we would be more saying like,",
        "start": 4266.069,
        "duration": 2.97
    },
    {
        "text": "where's the mug in space, like\nmaybe in body centered coordinates.",
        "start": 4269.049,
        "duration": 3.99
    },
    {
        "text": "and then, yeah, and there's yeah,\nagain, no recurrence within each token.",
        "start": 4278.084,
        "duration": 4.78
    },
    {
        "text": "I feel like, yeah, this kind of has\nan interesting, outcome in that if you",
        "start": 4283.804,
        "duration": 4.72
    },
    {
        "text": "imagine, let's say you are doing this\nand you're relying on this to converge",
        "start": 4288.524,
        "duration": 4.37
    },
    {
        "text": "towards a certain representation and\nactually use voting for this purpose.",
        "start": 4292.894,
        "duration": 3.79
    },
    {
        "text": "let's say you're 50 percent sure,\nbut, like when you, do the softmax,",
        "start": 4298.229,
        "duration": 5.2
    },
    {
        "text": "this might only get nudged, a\nlittle bit, towards, certainty.",
        "start": 4304.919,
        "duration": 4.73
    },
    {
        "text": "And let's say, you want to iterate and,\nmove closer or whatever, basically you",
        "start": 4310.019,
        "duration": 5.98
    },
    {
        "text": "will need multiple layers of, processing\nto do that, so you would need, a very deep",
        "start": 4315.999,
        "duration": 5.51
    },
    {
        "text": "transformer, to handle that situation.",
        "start": 4321.519,
        "duration": 2.96
    },
    {
        "text": "and yeah, how deep are these networks?",
        "start": 4326.609,
        "duration": 2.2
    },
    {
        "text": "in general, like these\nlarge language models.",
        "start": 4329.714,
        "duration": 2.77
    },
    {
        "text": "good question.",
        "start": 4334.174,
        "duration": 0.43
    },
    {
        "text": "I think, what was the original one?",
        "start": 4334.864,
        "duration": 1.65
    },
    {
        "text": "It was like 6 or 8 encoder and then\nsomething similar for decoder, or 12?",
        "start": 4336.514,
        "duration": 4.47
    },
    {
        "text": "But I don't, I have no idea how\ndeep the, the latest ones are.",
        "start": 4341.689,
        "duration": 2.87
    },
    {
        "text": "Do birds have 12 layers, for example?",
        "start": 4345.289,
        "duration": 3.09
    },
    {
        "text": "These ones are like over 100.",
        "start": 4349.549,
        "duration": 1.77
    },
    {
        "text": "I'm not sure of the number.",
        "start": 4351.519,
        "duration": 0.87
    },
    {
        "text": "that's very interesting, but 12\nis really different than 100.",
        "start": 4353.189,
        "duration": 2.33
    },
    {
        "text": "Because I remember, going back to the\nconvolutional networks for image, right?",
        "start": 4355.519,
        "duration": 2.76
    },
    {
        "text": "They were, they're big, they had\nhundreds of layers or something",
        "start": 4358.379,
        "duration": 2.17
    },
    {
        "text": "like that, hundreds of layers.",
        "start": 4360.549,
        "duration": 1.15
    },
    {
        "text": "so here, 12 layers doesn't mean,\nit means 12 transformer blocks.",
        "start": 4362.589,
        "duration": 4.67
    },
    {
        "text": "Each block has multiple layers.",
        "start": 4367.269,
        "duration": 1.25
    },
    {
        "text": "And GPT has, I think,\n96 transformer blocks.",
        "start": 4371.359,
        "duration": 3.14
    },
    {
        "text": "Which GPT?",
        "start": 4375.559,
        "duration": 0.79
    },
    {
        "text": "GPT 3.",
        "start": 4376.379,
        "duration": 0.6
    },
    {
        "text": "Yeah.",
        "start": 4377.169,
        "duration": 1.299
    },
    {
        "text": "Oh no, the four.",
        "start": 4379.249,
        "duration": 0.74
    },
    {
        "text": "So without this recurrence, and there's\nno feedback, are you always relying on",
        "start": 4381.109,
        "duration": 7.1
    },
    {
        "text": "the entire stack to get your answer?",
        "start": 4388.239,
        "duration": 1.83
    },
    {
        "text": "Yeah.",
        "start": 4390.869,
        "duration": 0.35
    },
    {
        "text": "You are.",
        "start": 4391.429,
        "duration": 0.36
    },
    {
        "text": "There's a lot of work of actually, doing\nfast inference by exiting early, that",
        "start": 4393.419,
        "duration": 5.03
    },
    {
        "text": "you don't need to go through, 96 layers\nof consolidation to get your answer.",
        "start": 4398.489,
        "duration": 3.97
    },
    {
        "text": "Yeah.",
        "start": 4402.699,
        "duration": 0.31
    },
    {
        "text": "So there's a lot of work doing that.",
        "start": 4403.049,
        "duration": 1.19
    },
    {
        "text": "But they train it.",
        "start": 4404.239,
        "duration": 0.9
    },
    {
        "text": "You've got to train the whole thing, and\nthen you could maybe do faster inference.",
        "start": 4405.289,
        "duration": 4.56
    },
    {
        "text": "Yeah.",
        "start": 4409.849,
        "duration": 0.019
    },
    {
        "text": "That's interesting.",
        "start": 4413.439,
        "duration": 0.57
    },
    {
        "text": "Yeah, it's, it's either that's\nnot to be diffing, it's just",
        "start": 4414.009,
        "duration": 5.31
    },
    {
        "text": "the fundamental differences.",
        "start": 4419.769,
        "duration": 1.44
    },
    {
        "text": "without the recurrence, without\nthe feedback, you have this system.",
        "start": 4422.119,
        "duration": 4.33
    },
    {
        "text": "it's quite different.",
        "start": 4428.129,
        "duration": 0.87
    },
    {
        "text": "And maybe that's not nothing new, just\nsaying that, just remind ourselves,",
        "start": 4431.389,
        "duration": 3.36
    },
    {
        "text": "yeah.",
        "start": 4439.659,
        "duration": 0.54
    },
    {
        "text": "honestly, we, spend a lot of time\nthinking about like compositional objects.",
        "start": 4441.064,
        "duration": 3.475
    },
    {
        "text": "Objects made of other objects and\nreusing models inside of other models.",
        "start": 4445.864,
        "duration": 4.08
    },
    {
        "text": "Is there anything equivalent to that here?",
        "start": 4450.664,
        "duration": 1.92
    },
    {
        "text": "Yeah, it's a good question.",
        "start": 4455.554,
        "duration": 0.86
    },
    {
        "text": "That was one of the main things I didn't\nget around to a satisfying answer.",
        "start": 4456.824,
        "duration": 4.41
    },
    {
        "text": "So I don't touch on that here.",
        "start": 4461.234,
        "duration": 2.7
    },
    {
        "text": "But I could get back to you on that.",
        "start": 4464.274,
        "duration": 1.76
    },
    {
        "text": "there's, I will talk about",
        "start": 4466.704,
        "duration": 1.63
    },
    {
        "text": "Yeah, on the next object, like how\nyou recognize an object composed of",
        "start": 4471.854,
        "duration": 3.63
    },
    {
        "text": "parts, but that doesn't necessarily\naddress the situation of arbitrary",
        "start": 4475.744,
        "duration": 6.72
    },
    {
        "text": "like fast binding, between things.",
        "start": 4482.464,
        "duration": 3.18
    },
    {
        "text": "Self attention, can do that, in the\nsense that, yeah, let's say, yeah,",
        "start": 4487.879,
        "duration": 8.01
    },
    {
        "text": "let's say, this is, the word is blue,\nand, and then this is, I don't know,",
        "start": 4495.889,
        "duration": 4.49
    },
    {
        "text": "Volkswagen or something, and, so that's\nlike a novel combination that was never",
        "start": 4500.829,
        "duration": 3.61
    },
    {
        "text": "encountered in the, In the training\nset, maybe it's such that these kind",
        "start": 4504.439,
        "duration": 8.475
    },
    {
        "text": "of key query values have been learned\nsuch that Volkswagen knows to look for",
        "start": 4512.914,
        "duration": 5.52
    },
    {
        "text": "adjectives, Blue knows to look for nouns,\nand so we can come up with a kind of",
        "start": 4518.444,
        "duration": 4.76
    },
    {
        "text": "vector embedding that combines those two.",
        "start": 4523.214,
        "duration": 2.88
    },
    {
        "text": "But that's a very kind of like\nmushy just add everything together",
        "start": 4527.909,
        "duration": 3.99
    },
    {
        "text": "way of representing things.",
        "start": 4531.909,
        "duration": 1.0
    },
    {
        "text": "It's not compositional either, right?",
        "start": 4532.909,
        "duration": 1.93
    },
    {
        "text": "I think, I don't think there's an\nexplicit mechanism for compositionality.",
        "start": 4535.889,
        "duration": 4.75
    },
    {
        "text": "It's more if for some parts of the\ndata it would be more efficient at",
        "start": 4541.149,
        "duration": 3.61
    },
    {
        "text": "compositional representations, it\nmight learn it for some aspects.",
        "start": 4544.869,
        "duration": 3.53
    },
    {
        "text": "It's just, but it's not like it's\ngenerically doing compositionality.",
        "start": 4549.169,
        "duration": 4.07
    },
    {
        "text": "But if I learned a model of something,\nlet's say I was doing revision network,",
        "start": 4553.449,
        "duration": 2.65
    },
    {
        "text": "I learned a model of something, okay,\nthat's And now that appears as part of",
        "start": 4556.169,
        "duration": 5.315
    },
    {
        "text": "a larger image of not a larger object,\nit's not a separate thing anymore,",
        "start": 4561.484,
        "duration": 3.95
    },
    {
        "text": "it's not part of a larger object.",
        "start": 4565.454,
        "duration": 1.58
    },
    {
        "text": "will I say, oh yes, this larger thing\nhas the model I learned earlier,",
        "start": 4569.604,
        "duration": 5.48
    },
    {
        "text": "or should I say no, I got a new\nlarger thing and I'm just going to",
        "start": 4575.084,
        "duration": 2.94
    },
    {
        "text": "learn the statistics of its parts.",
        "start": 4578.024,
        "duration": 1.84
    },
    {
        "text": "Does it, if I, let's say for\nexample, I knew what a bicycle",
        "start": 4580.084,
        "duration": 3.34
    },
    {
        "text": "pedal crank looked like.",
        "start": 4583.444,
        "duration": 1.43
    },
    {
        "text": "And I know it, and I know that it's,\nand I've learned what pedals look like.",
        "start": 4585.519,
        "duration": 3.99
    },
    {
        "text": "And now I see it in different machines.",
        "start": 4589.509,
        "duration": 2.01
    },
    {
        "text": "It might be a stationary bike,\nit might be a, a human powered",
        "start": 4591.519,
        "duration": 4.58
    },
    {
        "text": "wine press, I don't know.",
        "start": 4596.309,
        "duration": 1.07
    },
    {
        "text": "But I say, oh, that's a, would it know\nthat, yes, I recognize there's a pedal",
        "start": 4597.609,
        "duration": 4.29
    },
    {
        "text": "and there's, affordances, or is it going\nto say, here's a line press and, this",
        "start": 4601.899,
        "duration": 5.05
    },
    {
        "text": "is just some statistical part of it.",
        "start": 4606.949,
        "duration": 1.54
    },
    {
        "text": "I think he can do the, I\ncan, he can do some of it.",
        "start": 4608.559,
        "duration": 3.61
    },
    {
        "text": "Some.",
        "start": 4612.259,
        "duration": 0.38
    },
    {
        "text": "Yeah.",
        "start": 4613.069,
        "duration": 0.27
    },
    {
        "text": "I think that's why.",
        "start": 4613.829,
        "duration": 0.95
    },
    {
        "text": "All these answers are going to be fuzzy\nlike that because I think in a lot of",
        "start": 4614.819,
        "duration": 4.64
    },
    {
        "text": "cases from, because it's trained on so\nmuch data, it will just realize it's",
        "start": 4619.459,
        "duration": 3.83
    },
    {
        "text": "more efficient to think about panels and\ncranks and apply it to different things",
        "start": 4623.289,
        "duration": 3.92
    },
    {
        "text": "and it'll do it, but if it's only seen.",
        "start": 4627.209,
        "duration": 2.45
    },
    {
        "text": "Very few pedals.",
        "start": 4631.049,
        "duration": 0.81
    },
    {
        "text": "It's not gonna do that.",
        "start": 4631.859,
        "duration": 1.02
    },
    {
        "text": "if I see, if I saw a machine I didn't\nrecognize before, it was a wine press",
        "start": 4634.934,
        "duration": 4.575
    },
    {
        "text": "and it was human power with bicycle\npedals attached to it, I might be",
        "start": 4639.629,
        "duration": 5.04
    },
    {
        "text": "able to figure out what's going on.",
        "start": 4644.669,
        "duration": 1.65
    },
    {
        "text": "I say, oh, there's some human's gonna\nbe using this thing to provide power.",
        "start": 4646.319,
        "duration": 3.36
    },
    {
        "text": "And then where's the chain go?",
        "start": 4649.679,
        "duration": 1.29
    },
    {
        "text": "okay, it goes over here.",
        "start": 4651.014,
        "duration": 0.975
    },
    {
        "text": "It looks like it's gonna turn\nthis tub or something like that.",
        "start": 4651.989,
        "duration": 2.105
    },
    {
        "text": "I'm just thinking that I would\ngo through, because it's built of",
        "start": 4655.694,
        "duration": 2.73
    },
    {
        "text": "components that I've seen before,\nand I might figure out how they work.",
        "start": 4658.504,
        "duration": 3.01
    },
    {
        "text": "is there a way we could test\nthat with just language?",
        "start": 4663.184,
        "duration": 2.33
    },
    {
        "text": "Because then we could\njust input it to ChatGPT.",
        "start": 4665.594,
        "duration": 2.17
    },
    {
        "text": "I don't know.",
        "start": 4667.804,
        "duration": 0.6
    },
    {
        "text": "I think Yann LeCun put them together and\nsee Yann LeCun recently had this, thought",
        "start": 4670.974,
        "duration": 6.72
    },
    {
        "text": "experiment, which was, like, putting a\nbunch of cogs that are rotating either",
        "start": 4677.714,
        "duration": 4.44
    },
    {
        "text": "clockwise or anticlockwise in, series.",
        "start": 4682.274,
        "duration": 2.38
    },
    {
        "text": "And then asking, okay, one cog is\nturning, what is the, the direction of",
        "start": 4685.144,
        "duration": 6.11
    },
    {
        "text": "turning of, the nth cog, or whatever.",
        "start": 4691.254,
        "duration": 2.29
    },
    {
        "text": "A cog for you is a gear?",
        "start": 4693.704,
        "duration": 1.16
    },
    {
        "text": "Is that the British term?",
        "start": 4695.014,
        "duration": 0.82
    },
    {
        "text": "Yeah, sorry, like a gear.",
        "start": 4695.854,
        "duration": 1.06
    },
    {
        "text": "And maybe, yeah, French, English, no.",
        "start": 4698.334,
        "duration": 2.41
    },
    {
        "text": "but, Yeah, and there was a bunch\nof tweets about whether it could,",
        "start": 4701.964,
        "duration": 5.595
    },
    {
        "text": "sometimes it solved it, but only\nif you told it Yann LeCun was",
        "start": 4707.829,
        "duration": 3.07
    },
    {
        "text": "wondering whether it could solve it.",
        "start": 4711.189,
        "duration": 1.47
    },
    {
        "text": "and stuff like that.",
        "start": 4714.789,
        "duration": 0.63
    },
    {
        "text": "but, yeah, I think definitely it\ncan do general computation, right?",
        "start": 4717.949,
        "duration": 5.71
    },
    {
        "text": "It's not Turing complete.",
        "start": 4723.669,
        "duration": 1.39
    },
    {
        "text": "So there's, there's problems that\nrequire polynomial time to solve or",
        "start": 4725.689,
        "duration": 4.67
    },
    {
        "text": "linear time to solve, and it's not going\nto be able to do those things, right?",
        "start": 4730.389,
        "duration": 3.7
    },
    {
        "text": "It can't.",
        "start": 4734.089,
        "duration": 0.42
    },
    {
        "text": "It's if like sorting requires\nn log n time and you give it a",
        "start": 4736.349,
        "duration": 3.21
    },
    {
        "text": "list of a million numbers, it's\nnot gonna be able to solve it.",
        "start": 4739.889,
        "duration": 2.37
    },
    {
        "text": "But yeah.",
        "start": 4743.949,
        "duration": 0.45
    },
    {
        "text": "But yeah, I guess this probably was like\ndesigned to point, but it can, yeah.",
        "start": 4744.399,
        "duration": 3.36
    },
    {
        "text": "It's not a general computing.",
        "start": 4748.269,
        "duration": 2.25
    },
    {
        "text": "Yeah.",
        "start": 4750.579,
        "duration": 0.09
    },
    {
        "text": "then again, I don't like using in\ncomputing because we think about",
        "start": 4751.299,
        "duration": 2.58
    },
    {
        "text": "like digital computers, but I do\nthink it computing in the sense",
        "start": 4753.879,
        "duration": 4.33
    },
    {
        "text": "that the more abstract notion of\ncomputation yeah, Like maybe, okay.",
        "start": 4758.209,
        "duration": 5.31
    },
    {
        "text": "Some problems just.",
        "start": 4763.519,
        "duration": 1.14
    },
    {
        "text": "are known to require polynomial\ntime to come at an answer.",
        "start": 4764.969,
        "duration": 3.53
    },
    {
        "text": "And it probably won't be able,\nlike traveling salesman problem.",
        "start": 4769.114,
        "duration": 2.79
    },
    {
        "text": "But no one can solve that.",
        "start": 4772.204,
        "duration": 1.44
    },
    {
        "text": "Nothing can solve the\ntraveling salesman problem.",
        "start": 4773.914,
        "duration": 2.49
    },
    {
        "text": "Yeah, of course you can.",
        "start": 4776.584,
        "duration": 0.66
    },
    {
        "text": "Computers can solve, it just takes time.",
        "start": 4777.394,
        "duration": 1.26
    },
    {
        "text": "not perfectly well, big a problem.",
        "start": 4778.744,
        "duration": 2.35
    },
    {
        "text": "They, we don't have amount\nof time in the universe.",
        "start": 4781.094,
        "duration": 1.44
    },
    {
        "text": "but up to a certain point it can solve it.",
        "start": 4783.674,
        "duration": 1.77
    },
    {
        "text": "Whereas I think transformers won't be\nable to do arbitrary touring computation.",
        "start": 4785.444,
        "duration": 4.33
    },
    {
        "text": "it's funny because we\nuse the word attention.",
        "start": 4789.914,
        "duration": 1.53
    },
    {
        "text": "I think it's completely different.",
        "start": 4792.309,
        "duration": 1.2
    },
    {
        "text": "it's not completely different,\nbut it's certainly, again,",
        "start": 4793.669,
        "duration": 1.935
    },
    {
        "text": "it's one of those words that.",
        "start": 4795.604,
        "duration": 0.691
    },
    {
        "text": "It sounds the same and maybe similar,\nbut it's really quite different.",
        "start": 4796.954,
        "duration": 3.02
    },
    {
        "text": "If I see something I don't understand,\nit's a composition of components",
        "start": 4799.974,
        "duration": 3.63
    },
    {
        "text": "I haven't seen before, I narrow\ndown my attention until I, I see",
        "start": 4803.604,
        "duration": 3.46
    },
    {
        "text": "various things I do understand.",
        "start": 4807.094,
        "duration": 1.5
    },
    {
        "text": "And, and then I say, okay, what\nare the relationships to those,",
        "start": 4809.424,
        "duration": 3.28
    },
    {
        "text": "things and say, can I see parallels\nto other things I've seen?",
        "start": 4813.084,
        "duration": 2.35
    },
    {
        "text": "So it's, not this big,\nfeed forward process.",
        "start": 4815.724,
        "duration": 2.27
    },
    {
        "text": "It's an interactive process where I\nhave to literally attend the different",
        "start": 4818.014,
        "duration": 3.09
    },
    {
        "text": "parts of an image of different parts of.",
        "start": 4821.194,
        "duration": 1.55
    },
    {
        "text": "Something that's happening in my life.",
        "start": 4823.219,
        "duration": 1.57
    },
    {
        "text": "And here they have the word attention,\nand it's, but it seems like a very",
        "start": 4825.339,
        "duration": 3.52
    },
    {
        "text": "feed forward, there's no recurrence\nto this, I can't walk through",
        "start": 4828.859,
        "duration": 4.12
    },
    {
        "text": "hypotheses sterially or something.",
        "start": 4832.979,
        "duration": 2.07
    },
    {
        "text": "the chat with the autoregressive stuff,\nit's continuing to feed things back in.",
        "start": 4835.559,
        "duration": 4.49
    },
    {
        "text": "But that's the thing about it, it's\njust running this, it's running",
        "start": 4841.684,
        "duration": 2.97
    },
    {
        "text": "the same algorithm, but it's taking\nan output and putting it back in.",
        "start": 4844.654,
        "duration": 2.29
    },
    {
        "text": "Yeah, it's active, it's what\nwe call active prediction.",
        "start": 4846.944,
        "duration": 2.24
    },
    {
        "text": "And so it can diverge very easily,\nso it's taking a prediction, sticking",
        "start": 4849.874,
        "duration": 5.36
    },
    {
        "text": "it back in, treating it as ground\ntruth, going and predicting the next",
        "start": 4855.234,
        "duration": 3.37
    },
    {
        "text": "thing, treating it as ground truth.",
        "start": 4858.624,
        "duration": 1.06
    },
    {
        "text": "That's interesting, it's not exactly what\nI was talking about, but it's interesting.",
        "start": 4859.684,
        "duration": 2.51
    },
    {
        "text": "Yeah, that's the thread that\nNiels and I have on Slack.",
        "start": 4863.644,
        "duration": 3.96
    },
    {
        "text": "That's one of Yann LeCun's\nmain, that's why he says these",
        "start": 4868.524,
        "duration": 3.12
    },
    {
        "text": "are fundamentally limited.",
        "start": 4871.644,
        "duration": 1.19
    },
    {
        "text": "It's just essentially only\ndoing active prediction.",
        "start": 4873.609,
        "duration": 2.28
    },
    {
        "text": "It's not grounded in any way once\nyou start, once it starts going.",
        "start": 4878.109,
        "duration": 3.63
    },
    {
        "text": "Yeah,",
        "start": 4882.169,
        "duration": 0.35
    },
    {
        "text": "That leads to its hallucination.",
        "start": 4885.359,
        "duration": 1.79
    },
    {
        "text": "hallucination, yeah.",
        "start": 4887.219,
        "duration": 1.73
    },
    {
        "text": "Because there's no additional information.",
        "start": 4888.949,
        "duration": 1.83
    },
    {
        "text": "You and I would look at something we\ndon't understand and we would then,",
        "start": 4891.029,
        "duration": 2.85
    },
    {
        "text": "we would try to, attend it in parts\nand try to get my hypothesis serially.",
        "start": 4894.539,
        "duration": 3.95
    },
    {
        "text": "but we, but each time we\nget more sensory input.",
        "start": 4900.174,
        "duration": 2.21
    },
    {
        "text": "we get more sensory input\nand we don't just confabulate",
        "start": 4902.814,
        "duration": 2.69
    },
    {
        "text": "our way through the results.",
        "start": 4905.504,
        "duration": 1.6
    },
    {
        "text": "I didn't, I don't say, I think it's a dog.",
        "start": 4908.294,
        "duration": 1.6
    },
    {
        "text": "if it's a dog, then it's going to\nbark, and if it didn't bark, then it's",
        "start": 4910.054,
        "duration": 2.99
    },
    {
        "text": "going to say this, and it's whatever,\nIt's just once you give it the query,",
        "start": 4913.044,
        "duration": 5.07
    },
    {
        "text": "it's just, the input is just going\nto hallucinate from that point on.",
        "start": 4918.124,
        "duration": 4.17
    },
    {
        "text": "essentially, I'm really trying to\nget a sense for the fundamental",
        "start": 4926.294,
        "duration": 3.81
    },
    {
        "text": "component differences here.",
        "start": 4930.164,
        "duration": 1.58
    },
    {
        "text": "And it does feel like this idea that\nwe're moving, sampling the world,",
        "start": 4932.454,
        "duration": 3.64
    },
    {
        "text": "not randomly, but we're moving\nthe sampling world constantly.",
        "start": 4936.334,
        "duration": 2.45
    },
    {
        "text": "And sampling the world is not\njust Moving our senses, but it's",
        "start": 4938.784,
        "duration": 2.61
    },
    {
        "text": "attending to different parts of\nthe world, and we're building these",
        "start": 4941.394,
        "duration": 4.26
    },
    {
        "text": "models dynamically in real time.",
        "start": 4945.654,
        "duration": 2.3
    },
    {
        "text": "that part I think it's doing in brute\nforce, if we're given an image, we",
        "start": 4948.404,
        "duration": 4.47
    },
    {
        "text": "attend to different parts of it.",
        "start": 4952.874,
        "duration": 0.95
    },
    {
        "text": "When it's given an image, it\nattends every possible combination.",
        "start": 4954.724,
        "duration": 3.39
    },
    {
        "text": "And so it is In the\nsuperset of the 1080 H4.",
        "start": 4958.854,
        "duration": 3.67
    },
    {
        "text": "that's been the history of technology\nin some ways, that we can build",
        "start": 4963.054,
        "duration": 5.81
    },
    {
        "text": "machines that are better than humans\nin a lot of capabilities, right?",
        "start": 4968.864,
        "duration": 2.91
    },
    {
        "text": "and, in some sense,",
        "start": 4972.664,
        "duration": 1.89
    },
    {
        "text": "one could argue, I always use\nthe word, we have calculators,",
        "start": 4977.144,
        "duration": 2.89
    },
    {
        "text": "they're superhuman, right?",
        "start": 4980.034,
        "duration": 1.375
    },
    {
        "text": "They're really great.",
        "start": 4981.409,
        "duration": 1.454
    },
    {
        "text": "and then somebody, sometimes you\ncan argue that, computer vision",
        "start": 4983.474,
        "duration": 4.28
    },
    {
        "text": "should be better than human, right?",
        "start": 4987.754,
        "duration": 1.98
    },
    {
        "text": "Because you can do this, you can\ndo what you just described, right?",
        "start": 4989.734,
        "duration": 2.56
    },
    {
        "text": "You can look at all at once,\nyou can train like a billion",
        "start": 4992.294,
        "duration": 2.19
    },
    {
        "text": "things, but humans can't do that.",
        "start": 4994.484,
        "duration": 1.619
    },
    {
        "text": "so it shouldn't be a surprise that,\nThings, even like these chatbots, are",
        "start": 4997.394,
        "duration": 3.85
    },
    {
        "text": "really, good at faking it in some sense.",
        "start": 5001.684,
        "duration": 2.74
    },
    {
        "text": "The question is, are they really, do\nthey have any, is it more than faking it?",
        "start": 5004.924,
        "duration": 3.5
    },
    {
        "text": "Is there really true knowledge\nabout the world inside that",
        "start": 5008.424,
        "duration": 2.55
    },
    {
        "text": "can be taken advantage of?",
        "start": 5010.974,
        "duration": 0.92
    },
    {
        "text": "so far I don't think so, but I'm\nnot certain that, I don't know.",
        "start": 5013.794,
        "duration": 3.4
    },
    {
        "text": "I'm asking questions trying\nto get a better sense.",
        "start": 5018.914,
        "duration": 1.839
    },
    {
        "text": "It's just very brute force, and\nyou can get, do really well.",
        "start": 5022.924,
        "duration": 4.22
    },
    {
        "text": "And we, of course, we as humans look\nat language, which is, language is",
        "start": 5031.214,
        "duration": 5.48
    },
    {
        "text": "already a very It's a, it's an abstract\nway of communicating an internal",
        "start": 5036.714,
        "duration": 5.755
    },
    {
        "text": "model state that I have to you.",
        "start": 5042.469,
        "duration": 2.02
    },
    {
        "text": "And, so it's abstract from the beginning.",
        "start": 5045.389,
        "duration": 3.44
    },
    {
        "text": "it's already post understanding.",
        "start": 5049.399,
        "duration": 2.12
    },
    {
        "text": "It's we're coming up with description,\nwhere the tokens represent my",
        "start": 5051.639,
        "duration": 3.36
    },
    {
        "text": "internal representations that\nI'm going to communicate to you.",
        "start": 5054.999,
        "duration": 2.07
    },
    {
        "text": "Which could be completely fake.",
        "start": 5057.729,
        "duration": 1.28
    },
    {
        "text": "but here we're starting with\nthat, and that's all you get.",
        "start": 5061.359,
        "duration": 2.04
    },
    {
        "text": "you don't have any of the ground\ntruth or knowledge about how the world",
        "start": 5064.179,
        "duration": 3.08
    },
    {
        "text": "actually feels and looks and sounds.",
        "start": 5067.259,
        "duration": 1.68
    },
    {
        "text": "You just have what the\nwords correspond to.",
        "start": 5068.989,
        "duration": 2.28
    },
    {
        "text": "But anyway, we're really good at\nassigning, oh, a human will say,",
        "start": 5071.469,
        "duration": 3.53
    },
    {
        "text": "oh, this language is really good,\nthere must be a human on the other",
        "start": 5075.039,
        "duration": 2.12
    },
    {
        "text": "side of it, there must be something\nreally smart on the other side of it.",
        "start": 5077.179,
        "duration": 2.13
    },
    {
        "text": "In the same way we might look at the\noutput of a calculator here, there's",
        "start": 5080.484,
        "duration": 2.08
    },
    {
        "text": "a really great mathematician on the\nother side, but until you get used to",
        "start": 5082.584,
        "duration": 2.43
    },
    {
        "text": "it, you say, oh no, it's just doing its\nthing, I don't really understand what,",
        "start": 5085.014,
        "duration": 4.63
    },
    {
        "text": "I'm just rambling.",
        "start": 5093.434,
        "duration": 0.48
    },
    {
        "text": "What else do you have?",
        "start": 5094.844,
        "duration": 0.94
    },
    {
        "text": "Yeah, I guess a couple,\nyeah, a few different things.",
        "start": 5096.514,
        "duration": 2.78
    },
    {
        "text": "Just one last thing to say on this kind\nof question of compositional objects.",
        "start": 5099.294,
        "duration": 3.26
    },
    {
        "text": "I just thought it was worth highlighting,\nYeah, I think, the, where these things",
        "start": 5102.574,
        "duration": 5.38
    },
    {
        "text": "excel, seemingly, is, yeah, they are\ngood at this kind of fuzzy combination",
        "start": 5107.954,
        "duration": 3.67
    },
    {
        "text": "of these, vectors which might represent\ndifferent things, and that's why I",
        "start": 5111.624,
        "duration": 3.9
    },
    {
        "text": "guess it's impressive that, something\nlike Dall-e or stable diffusion, you",
        "start": 5115.524,
        "duration": 3.86
    },
    {
        "text": "can say, I want a hedgehog made out\nof lettuce, and it seems to be able",
        "start": 5119.384,
        "duration": 3.95
    },
    {
        "text": "to take fuzzy concept of lettuce, the\nfuzzy concept of hedgehog, and create",
        "start": 5123.334,
        "duration": 4.39
    },
    {
        "text": "something that seems like a reasonable\ninterpolation of these two things,",
        "start": 5127.724,
        "duration": 2.97
    },
    {
        "text": "but on the whole kind of compositional\nthing, it's, it's relying on this self",
        "start": 5131.444,
        "duration": 5.21
    },
    {
        "text": "attention, which doesn't have a very\nexplicit sense of space, and that maybe",
        "start": 5136.654,
        "duration": 4.22
    },
    {
        "text": "is why they, on the other hand, struggle\nwith, something as simple as, I want a",
        "start": 5140.874,
        "duration": 5.18
    },
    {
        "text": "blue cube next to a red sphere, with a\nyellow pyramid on top of the blue cube.",
        "start": 5146.244,
        "duration": 5.96
    },
    {
        "text": "it will get the colors, associated\nwith each object wrong, as",
        "start": 5153.099,
        "duration": 4.72
    },
    {
        "text": "often as it gets them right.",
        "start": 5157.819,
        "duration": 1.36
    },
    {
        "text": "Is that right?",
        "start": 5159.539,
        "duration": 0.36
    },
    {
        "text": "Because, yeah, at least Dall-e.",
        "start": 5159.899,
        "duration": 2.37
    },
    {
        "text": "I don't, I don't know Dall-e 2.",
        "start": 5162.269,
        "duration": 1.2
    },
    {
        "text": "I don't know about Mid Journey 5 and\nstuff like that, but that was definitely",
        "start": 5163.469,
        "duration": 4.01
    },
    {
        "text": "an interesting example of, Dall-e 2 could\ndo all these amazing things, and it can't",
        "start": 5167.829,
        "duration": 4.06
    },
    {
        "text": "do three colored, geometric objects.",
        "start": 5171.889,
        "duration": 2.5
    },
    {
        "text": "Wow.",
        "start": 5175.549,
        "duration": 0.34
    },
    {
        "text": "yeah, so it does really\nwell if there's, one thing.",
        "start": 5177.749,
        "duration": 4.13
    },
    {
        "text": "This is, related to the\ncompositionality thing.",
        "start": 5181.879,
        "duration": 2.559
    },
    {
        "text": "It's yeah, a hedgehog made of lettuce\nis one thing, but these three objects",
        "start": 5184.439,
        "duration": 4.52
    },
    {
        "text": "with three different colors and\nthree different positions . Yeah.",
        "start": 5188.959,
        "duration": 4.41
    },
    {
        "text": "And and it needs to keep track\nof where they're in space and",
        "start": 5193.369,
        "duration": 3.875
    },
    {
        "text": "what's associated with what.",
        "start": 5197.424,
        "duration": 1.02
    },
    {
        "text": "And that's where self\nattention seems to struggle.",
        "start": 5198.444,
        "duration": 2.76
    },
    {
        "text": "And if mid Journey five has solved\nthat, I imagine it's just through brute",
        "start": 5201.204,
        "duration": 3.98
    },
    {
        "text": "force rather than, anything because\nthey are famously have better hands now,",
        "start": 5205.184,
        "duration": 6.01
    },
    {
        "text": "although they're, still not perfect.",
        "start": 5211.229,
        "duration": 1.435
    },
    {
        "text": "But, and that was another thing\nwe talked about recently, which",
        "start": 5212.664,
        "duration": 4.05
    },
    {
        "text": "I think is a similar pro problem.",
        "start": 5217.014,
        "duration": 1.86
    },
    {
        "text": "It's one thing to have a fuzzy\nrepresentation of a hand, but to have",
        "start": 5219.359,
        "duration": 3.08
    },
    {
        "text": "a very clear sense of okay, I'm going\nto do five fingers, you need a more",
        "start": 5222.449,
        "duration": 4.41
    },
    {
        "text": "explicit representation of space.",
        "start": 5226.859,
        "duration": 1.46
    },
    {
        "text": "Yeah.",
        "start": 5230.239,
        "duration": 0.34
    },
    {
        "text": "But yeah, I do have a few things.",
        "start": 5232.024,
        "duration": 2.76
    },
    {
        "text": "yeah, let's see if we can\nget through the rest of this.",
        "start": 5236.664,
        "duration": 2.56
    },
    {
        "text": "Let's get to the conclusion here.",
        "start": 5239.244,
        "duration": 3.729
    },
    {
        "text": "okay, yeah, reference frames.",
        "start": 5245.224,
        "duration": 1.47
    },
    {
        "text": "Yeah, so do transformers\nwork with reference frames?",
        "start": 5246.754,
        "duration": 2.56
    },
    {
        "text": "they use positional encodings at input,\nwhich is a small step, maybe, towards",
        "start": 5249.904,
        "duration": 6.675
    },
    {
        "text": "something like reference frames.",
        "start": 5256.589,
        "duration": 1.27
    },
    {
        "text": "and yeah.",
        "start": 5258.349,
        "duration": 0.29
    },
    {
        "text": "This is what we did with\nthe temporal pooler, right?",
        "start": 5258.769,
        "duration": 1.72
    },
    {
        "text": "it's the same thing.",
        "start": 5261.099,
        "duration": 0.74
    },
    {
        "text": "We said, okay, let's solve\nthis simpler problem of one",
        "start": 5262.039,
        "duration": 3.87
    },
    {
        "text": "dimensional reference frames.",
        "start": 5265.909,
        "duration": 2.09
    },
    {
        "text": "and, then before moving on to the\nmore difficult problem of, not n",
        "start": 5271.614,
        "duration": 5.82
    },
    {
        "text": "dimensional, three dimensional, so it\nis a significantly simpler problem.",
        "start": 5277.554,
        "duration": 4.54
    },
    {
        "text": "yeah, and,",
        "start": 5284.584,
        "duration": 0.62
    },
    {
        "text": "there's some evidence they have better\nkind of spatial awareness, which",
        "start": 5287.724,
        "duration": 2.13
    },
    {
        "text": "I'll get into, and yeah, there are\nsome, with a kind of neuroscience",
        "start": 5289.854,
        "duration": 3.33
    },
    {
        "text": "psychology background who have, compared\nkind of their computations a bit to",
        "start": 5293.194,
        "duration": 3.67
    },
    {
        "text": "grid cells, the fact that they have\nthese kind of, spatial encodings.",
        "start": 5296.864,
        "duration": 4.13
    },
    {
        "text": "I was just going to say, I don't think\nit's due to their success, new models",
        "start": 5303.004,
        "duration": 3.28
    },
    {
        "text": "don't have positional encodings, right?",
        "start": 5306.284,
        "duration": 1.49
    },
    {
        "text": "They use Alibi, they just replace\nit for a learned bias instead.",
        "start": 5307.854,
        "duration": 3.802
    },
    {
        "text": "What does that mean?",
        "start": 5311.656,
        "duration": 1.087
    },
    {
        "text": "You get rid of positional\nencodings of words?",
        "start": 5312.744,
        "duration": 1.78
    },
    {
        "text": "Yeah, you have a learned positional\nencoding instead, you don't have",
        "start": 5314.584,
        "duration": 3.76
    },
    {
        "text": "the version that Neils was showing.",
        "start": 5319.184,
        "duration": 1.36
    },
    {
        "text": "Yeah, but it's still a, it's still\na learned positional encoding.",
        "start": 5321.194,
        "duration": 2.774
    },
    {
        "text": "Yeah, it's still position encoding, but\nit's just not, like, How does it learn?",
        "start": 5324.209,
        "duration": 3.38
    },
    {
        "text": "Don't I, aren't I feeding\nin text one word at a time?",
        "start": 5327.589,
        "duration": 2.53
    },
    {
        "text": "Or no?",
        "start": 5330.119,
        "duration": 0.64
    },
    {
        "text": "How's your thing going?",
        "start": 5330.769,
        "duration": 1.27
    },
    {
        "text": "Something like that.",
        "start": 5332.039,
        "duration": 0.43
    },
    {
        "text": "you have, a bias in case.",
        "start": 5333.529,
        "duration": 2.08
    },
    {
        "text": "where that would be in the sequence, but\nthat you're not, giving it beforehand.",
        "start": 5337.214,
        "duration": 5.05
    },
    {
        "text": "You're just initializing and letting\nit learn for backpropagation.",
        "start": 5342.694,
        "duration": 2.67
    },
    {
        "text": "So it's not exactly a position encoding.",
        "start": 5345.804,
        "duration": 2.61
    },
    {
        "text": "encoding.",
        "start": 5348.794,
        "duration": 0.11
    },
    {
        "text": "Okay.",
        "start": 5349.554,
        "duration": 0.14
    },
    {
        "text": "I don't know if you it's a mapping\nfrom, let's say it's a sequence.",
        "start": 5350.174,
        "duration": 4.01
    },
    {
        "text": "isn't it, let's say, again, it's\nthe fourth token in the sequence.",
        "start": 5354.524,
        "duration": 2.57
    },
    {
        "text": "a learned encoding would just\nbe like, yeah, mapping from four",
        "start": 5357.989,
        "duration": 3.99
    },
    {
        "text": "onto a certain vector, and you\njust learn, you learn the weight",
        "start": 5362.009,
        "duration": 3.75
    },
    {
        "text": "transformation that does that vector.",
        "start": 5365.759,
        "duration": 1.38
    },
    {
        "text": "that vector presumably is still\ncapturing information about fourthness.",
        "start": 5369.889,
        "duration": 3.92
    },
    {
        "text": "It's just, yeah.",
        "start": 5374.269,
        "duration": 1.513
    },
    {
        "text": "yeah, I'm not suggesting it's specifically\ncosine or whatever, just the fact that",
        "start": 5376.455,
        "duration": 4.874
    },
    {
        "text": "there is positional information that,\nyeah, is maybe But when you train,",
        "start": 5381.329,
        "duration": 3.39
    },
    {
        "text": "when these systems are trained, they're\nnot, they are trained with, the only",
        "start": 5385.019,
        "duration": 4.55
    },
    {
        "text": "information you have on the tokens\nand their positions relative to each",
        "start": 5389.569,
        "duration": 3.1
    },
    {
        "text": "other, there's nothing else, right?",
        "start": 5392.669,
        "duration": 1.23
    },
    {
        "text": "There's, no other, there's\nnothing to be learning on.",
        "start": 5394.449,
        "duration": 2.5
    },
    {
        "text": "there's no other additional\ninformation about the world.",
        "start": 5398.249,
        "duration": 1.86
    },
    {
        "text": "It's just These tokens and, where\nthey are relative to each other in",
        "start": 5400.699,
        "duration": 4.66
    },
    {
        "text": "a one dimensional reference frame.",
        "start": 5405.359,
        "duration": 1.5
    },
    {
        "text": "I'm stating that to see if I'm\nwrong about it, but that seems like",
        "start": 5408.689,
        "duration": 2.32
    },
    {
        "text": "sort of fundamental truth here.",
        "start": 5411.019,
        "duration": 1.25
    },
    {
        "text": "That's all you've got\nfor your training data.",
        "start": 5412.269,
        "duration": 3.67
    },
    {
        "text": "So there's no more position, there\ncan't be any other positional Other",
        "start": 5417.109,
        "duration": 3.775
    },
    {
        "text": "than inform, this thing was in\nthis position relative to this one.",
        "start": 5422.009,
        "duration": 3.975
    },
    {
        "text": "Yeah.",
        "start": 5425.984,
        "duration": 0.06
    },
    {
        "text": "the language is a sequence, I dunno\nwhat happens when you do this on images.",
        "start": 5428.174,
        "duration": 3.48
    },
    {
        "text": "Yeah.",
        "start": 5432.364,
        "duration": 0.51
    },
    {
        "text": "and you could do a variety\nof things, as mentioned.",
        "start": 5433.204,
        "duration": 2.22
    },
    {
        "text": "Yeah.",
        "start": 5435.424,
        "duration": 0.21
    },
    {
        "text": "Often they just do a, raster kind\nof, where they just flatten the image",
        "start": 5435.634,
        "duration": 3.66
    },
    {
        "text": "essentially into a bunch of concatenated.",
        "start": 5439.294,
        "duration": 3.15
    },
    {
        "text": "But, but they, there is also\nwork where they do a more like 2D",
        "start": 5442.894,
        "duration": 4.46
    },
    {
        "text": "what you call it, relative displacement\nand X and y and things like that.",
        "start": 5450.304,
        "duration": 2.82
    },
    {
        "text": "But even that's not what you\nreally want about the world, right?",
        "start": 5454.784,
        "duration": 2.69
    },
    {
        "text": "Because what's next to each other on your\nretina has absolutely nothing to do with",
        "start": 5457.474,
        "duration": 4.83
    },
    {
        "text": "what's next to each other in the world.",
        "start": 5462.334,
        "duration": 1.65
    },
    {
        "text": "Yeah, yeah, I can get into this here\nthat, yeah, you have all these different",
        "start": 5464.794,
        "duration": 4.95
    },
    {
        "text": "flavors, and it can be absolute, it can\nbe relative, and yeah, but just to say",
        "start": 5470.004,
        "duration": 5.8
    },
    {
        "text": "yeah, reference frames, so there's some\nevidence, for example, that transformers,",
        "start": 5476.154,
        "duration": 5.0
    },
    {
        "text": "visual transformers have more of a\nshape bias than, for example, CNN, are",
        "start": 5481.374,
        "duration": 5.1
    },
    {
        "text": "getting closer to humans, maybe, but\nit is interesting to note, if you look",
        "start": 5486.474,
        "duration": 4.22
    },
    {
        "text": "at this graph, The difference between\nVGG or ResNet 50 in general is much",
        "start": 5490.694,
        "duration": 7.525
    },
    {
        "text": "smaller than the difference between\nthese visual transformers and humans.",
        "start": 5498.219,
        "duration": 3.12
    },
    {
        "text": "Sorry, I should explain this figure.",
        "start": 5502.134,
        "duration": 1.09
    },
    {
        "text": "So this is, here we have different\nshapes, and the more you go to the",
        "start": 5503.224,
        "duration": 5.37
    },
    {
        "text": "left, at presentation time, you\ntake these different shapes, and",
        "start": 5508.594,
        "duration": 4.06
    },
    {
        "text": "you put a random texture on it.",
        "start": 5512.654,
        "duration": 1.52
    },
    {
        "text": "So you might have a, an airplane,\nbut with the texture of an elephant",
        "start": 5514.184,
        "duration": 3.88
    },
    {
        "text": "skin, and you basically ask the\nsystem or the human to classify it.",
        "start": 5518.064,
        "duration": 4.32
    },
    {
        "text": "and humans will classify based\non the shape most of the time,",
        "start": 5523.204,
        "duration": 3.08
    },
    {
        "text": "which is what you're seeing here.",
        "start": 5526.284,
        "duration": 1.15
    },
    {
        "text": "but, other systems, particularly\nlike old CNNs will tend to focus",
        "start": 5528.534,
        "duration": 3.82
    },
    {
        "text": "the texture, and just completely\nignore the global shape.",
        "start": 5534.439,
        "duration": 2.46
    },
    {
        "text": "This gets back to our morphology\nmodels versus, Yeah, exactly, so it,",
        "start": 5538.229,
        "duration": 7.04
    },
    {
        "text": "seems yeah, okay, maybe adding these\npositional encodings is doing something",
        "start": 5545.269,
        "duration": 3.65
    },
    {
        "text": "for transformers, but, but it doesn't\nseem to be enough, maybe because it's not",
        "start": 5548.919,
        "duration": 4.24
    },
    {
        "text": "explicit enough, What, are transformers\non that, chart too, or are those Yeah,",
        "start": 5553.159,
        "duration": 5.81
    },
    {
        "text": "they're the, pyramids, these yellow and\norange ones, so you can see they're the",
        "start": 5559.479,
        "duration": 2.81
    },
    {
        "text": "best, of all the different networks.",
        "start": 5562.299,
        "duration": 3.16
    },
    {
        "text": "The other ones are CNNs.",
        "start": 5565.459,
        "duration": 1.08
    },
    {
        "text": "Yeah, okay, that's a pretty telling chart,\nYeah, I suppose maybe one caveat to add",
        "start": 5568.279,
        "duration": 5.63
    },
    {
        "text": "to that, there's been a few different\npapers around this kind of question,",
        "start": 5573.909,
        "duration": 2.64
    },
    {
        "text": "and some of them suggest that it's also\njust the fact that transformers tend to",
        "start": 5576.549,
        "duration": 4.12
    },
    {
        "text": "be trained on a lot more data, so it's\nnot totally clear whether it's really",
        "start": 5580.679,
        "duration": 5.75
    },
    {
        "text": "something special about transformers, but,\nyeah, but I think this one controlled for",
        "start": 5586.519,
        "duration": 5.46
    },
    {
        "text": "that, but, okay, yeah, This kind of, yeah,\nthen gets into, okay, how can we do yeah,",
        "start": 5591.979,
        "duration": 10.655
    },
    {
        "text": "recognizing, let's say, a mug, based on\nthis kind of self attention, operation.",
        "start": 5602.724,
        "duration": 6.72
    },
    {
        "text": "And again, this is going to assume\na simpler form of self attention",
        "start": 5610.054,
        "duration": 4.56
    },
    {
        "text": "than, a, a special case, rather\nthan the full, one that's possible.",
        "start": 5614.624,
        "duration": 4.745
    },
    {
        "text": "So basically assume that, a set of,\nthe, like the set of the, key query",
        "start": 5620.819,
        "duration": 6.38
    },
    {
        "text": "and vector, weights are the same,\nwhich is to say that, when we're",
        "start": 5627.199,
        "duration": 4.65
    },
    {
        "text": "doing a particular, attention head,\nwithin that attention head, the, Oh,",
        "start": 5631.849,
        "duration": 7.055
    },
    {
        "text": "actually, yeah, I think that's right.",
        "start": 5638.904,
        "duration": 1.75
    },
    {
        "text": "Yeah, within that attention head,\nwe're going to see, basically",
        "start": 5640.954,
        "duration": 3.93
    },
    {
        "text": "the same, key, value, and query.",
        "start": 5644.884,
        "duration": 4.04
    },
    {
        "text": "and then, yeah, basically we have\nthese, input representations, let's say",
        "start": 5652.004,
        "duration": 3.31
    },
    {
        "text": "they're rim and handle with associated\nkind of positional embeddings.",
        "start": 5655.314,
        "duration": 3.284
    },
    {
        "text": "and if you have these kind of, different\nbut reused, weight operations, you could",
        "start": 5660.129,
        "duration": 4.73
    },
    {
        "text": "imagine them learning to transform,\nfor example, from rim at, a particular",
        "start": 5664.859,
        "duration": 4.88
    },
    {
        "text": "location to mug at a particular location.",
        "start": 5669.739,
        "duration": 2.66
    },
    {
        "text": "And this kind of gets very much to,\nwhat we've explored in the past, which",
        "start": 5672.399,
        "duration": 3.99
    },
    {
        "text": "is this more kind of capsule network\ntype operation, where everything is",
        "start": 5676.389,
        "duration": 3.46
    },
    {
        "text": "just directly learning to predict, given\na feature at a location, what is the",
        "start": 5680.599,
        "duration": 3.68
    },
    {
        "text": "relative position of the object, to it.",
        "start": 5684.279,
        "duration": 3.33
    },
    {
        "text": "but, once that's done, once you have\nthose operations, then again, or like",
        "start": 5689.779,
        "duration": 5.51
    },
    {
        "text": "the outputs, then again, you could just\ncompare these, and, they would attend to",
        "start": 5695.289,
        "duration": 4.68
    },
    {
        "text": "one another if they, had good agreement.",
        "start": 5699.969,
        "duration": 2.83
    },
    {
        "text": "so yeah, so then about positional\nencoding, embeddings, yeah, just to",
        "start": 5707.859,
        "duration": 3.47
    },
    {
        "text": "say basically there's a whole bunch of\ndifferent, varieties, that you can do.",
        "start": 5711.329,
        "duration": 4.75
    },
    {
        "text": "As, Lucas said, they can be learned\nor they can be hardcoded as the",
        "start": 5716.409,
        "duration": 2.55
    },
    {
        "text": "one I described at the start was.",
        "start": 5718.959,
        "duration": 1.33
    },
    {
        "text": "they can be added, they can be\nconcatenated, they can be in",
        "start": 5721.294,
        "duration": 2.51
    },
    {
        "text": "relative coordinates, absolute.",
        "start": 5723.804,
        "duration": 1.2
    },
    {
        "text": "they can even be recurrent, which seems\na bit odd given the mass parallelization",
        "start": 5725.934,
        "duration": 4.27
    },
    {
        "text": "that transformers, do, but, yeah,\nyou can essentially have the position",
        "start": 5730.254,
        "duration": 5.86
    },
    {
        "text": "encoding at the kind of nth position be\nrecurrently dependent on the position",
        "start": 5736.144,
        "duration": 4.25
    },
    {
        "text": "encoding at the n minus one position.",
        "start": 5740.394,
        "duration": 2.58
    },
    {
        "text": "and, but a kind of key thing is\nthey're typically at the input only.",
        "start": 5745.114,
        "duration": 3.42
    },
    {
        "text": "Okay.",
        "start": 5748.534,
        "duration": 0.019
    },
    {
        "text": "And yeah, I was, one thing I was trying\nto research but didn't get around to,",
        "start": 5749.034,
        "duration": 4.91
    },
    {
        "text": "was like how much, and so I'd be curious\nif anyone knows, like, how many, how",
        "start": 5754.284,
        "duration": 5.72
    },
    {
        "text": "much work is there, out there, yeah,\nusing, concatenated representations",
        "start": 5760.004,
        "duration": 7.03
    },
    {
        "text": "and making sure that there's a\nseparate, representation at each layer.",
        "start": 5767.034,
        "duration": 4.089
    },
    {
        "text": "Because it seems like there's a risk\notherwise that the kind of spatial",
        "start": 5771.674,
        "duration": 3.0
    },
    {
        "text": "representations get washed out as you\nget deeper and deeper, unless there's",
        "start": 5774.674,
        "duration": 3.93
    },
    {
        "text": "a really good objective function that's\nensuring that they're, absolutely",
        "start": 5778.604,
        "duration": 4.4
    },
    {
        "text": "important, and that maybe this contributes\nto the kind of, yeah, they're, I",
        "start": 5783.004,
        "duration": 5.97
    },
    {
        "text": "don't think rotary invariants are\ninput only, I didn't get implemented,",
        "start": 5788.974,
        "duration": 3.78
    },
    {
        "text": "so I didn't read the paper fully.",
        "start": 5792.754,
        "duration": 1.474
    },
    {
        "text": "But I know for sure it's not an\ninput only because you got to",
        "start": 5794.499,
        "duration": 2.41
    },
    {
        "text": "change every attention layer.",
        "start": 5796.909,
        "duration": 2.14
    },
    {
        "text": "So there is something going\non at every layer level.",
        "start": 5799.349,
        "duration": 2.32
    },
    {
        "text": "Wait, which one is that, sorry?",
        "start": 5802.879,
        "duration": 1.06
    },
    {
        "text": "Rotary.",
        "start": 5804.329,
        "duration": 0.61
    },
    {
        "text": "That's the one they\nwere using before alibi.",
        "start": 5806.339,
        "duration": 1.6
    },
    {
        "text": "Oh, is that the one that's used in PAL ME?",
        "start": 5809.749,
        "duration": 1.63
    },
    {
        "text": "I think, yeah.",
        "start": 5812.279,
        "duration": 0.65
    },
    {
        "text": "Where they, rotate the matrix?",
        "start": 5812.939,
        "duration": 1.76
    },
    {
        "text": "I'm not, I know it's not embedded\nreally, but I didn't want to get too",
        "start": 5816.524,
        "duration": 3.8
    },
    {
        "text": "into it, but take a look at that.",
        "start": 5820.324,
        "duration": 1.56
    },
    {
        "text": "Okay, cool, thanks.",
        "start": 5822.634,
        "duration": 0.87
    },
    {
        "text": "Yeah, there was definitely one that I\nsaw from Google where they, rotate the",
        "start": 5824.064,
        "duration": 4.28
    },
    {
        "text": "embedding instead of, adding to it,\nwhich was interesting, but, okay, cool.",
        "start": 5828.534,
        "duration": 6.12
    },
    {
        "text": "And then, but yeah, just to point out,\nyeah, versus Monty, at the input we're,",
        "start": 5834.654,
        "duration": 5.08
    },
    {
        "text": "At least at the moment, and likely to use\nabsolute, body centric coordinates, that",
        "start": 5840.204,
        "duration": 5.89
    },
    {
        "text": "are in kind of six degrees of, freedom.",
        "start": 5846.094,
        "duration": 2.18
    },
    {
        "text": "But then the kind of real computing,\nis happening at these object",
        "start": 5848.644,
        "duration": 4.45
    },
    {
        "text": "level internal representations.",
        "start": 5853.094,
        "duration": 2.09
    },
    {
        "text": "all of this is very explicit.",
        "start": 5856.024,
        "duration": 1.38
    },
    {
        "text": "and I guess we also have kind of\nstronger skip connections for positional",
        "start": 5859.109,
        "duration": 3.2
    },
    {
        "text": "information, so although the skip\nconnections might help the preservation",
        "start": 5862.309,
        "duration": 3.8
    },
    {
        "text": "of some of this for transformers, I\nthink that's, yeah, again, because we're",
        "start": 5866.109,
        "duration": 5.87
    },
    {
        "text": "keeping the spatial pose and everything\nlike that very explicit and separate.",
        "start": 5871.979,
        "duration": 3.35
    },
    {
        "text": "I think it's more likely that will make\nit deep into the architecture in Monty.",
        "start": 5875.869,
        "duration": 3.77
    },
    {
        "text": "yeah, this is the limitations\nI already just talked about.",
        "start": 5884.689,
        "duration": 2.66
    },
    {
        "text": "I think just the one last one to mention\nis, yeah, that these transformations,",
        "start": 5887.429,
        "duration": 3.53
    },
    {
        "text": "all these weights and stuff need to be\nlearned for transformers to be able to",
        "start": 5890.979,
        "duration": 3.75
    },
    {
        "text": "do any of these kinds of, operations,\nwhereas Monty learns very quickly",
        "start": 5894.729,
        "duration": 5.89
    },
    {
        "text": "based on the sensorimotor movement\nand then uses those graphs to just",
        "start": 5900.629,
        "duration": 2.55
    },
    {
        "text": "directly determine, okay, what is the\nposition I expect to be over the pose.",
        "start": 5903.279,
        "duration": 5.19
    },
    {
        "text": "okay, so this will hopefully\nbe an interesting one.",
        "start": 5912.569,
        "duration": 1.85
    },
    {
        "text": "Sorry if I'm going a bit quicker,\nI just want to make sure there's",
        "start": 5914.579,
        "duration": 1.78
    },
    {
        "text": "time for everything and then I can\nalways, I guess we'll come back.",
        "start": 5916.359,
        "duration": 3.89
    },
    {
        "text": "but yeah, so embodiment, so this\nis, yeah, now starting to become",
        "start": 5921.499,
        "duration": 2.74
    },
    {
        "text": "a thing with transformers, and so\njust two kind of prominent examples.",
        "start": 5924.239,
        "duration": 3.78
    },
    {
        "text": "One is Gato, so this was from DeepMind.",
        "start": 5928.019,
        "duration": 2.32
    },
    {
        "text": "And this one was interesting.",
        "start": 5930.974,
        "duration": 1.12
    },
    {
        "text": "as, as Subutai was saying, these, systems\ncan be autoregressive if they're of",
        "start": 5932.314,
        "duration": 5.8
    },
    {
        "text": "the generative variety, and, and they\ntook that into an embodied setting",
        "start": 5938.114,
        "duration": 5.85
    },
    {
        "text": "where basically You know, you might\nhave some, fixed prompt, and then you",
        "start": 5943.964,
        "duration": 5.355
    },
    {
        "text": "it receives some sensory input, let's\nsay, the output of a game screen, that",
        "start": 5950.159,
        "duration": 4.79
    },
    {
        "text": "outputs, an action, and all of this is\nrepresented, so it's, it for example,",
        "start": 5954.949,
        "duration": 6.04
    },
    {
        "text": "receives, the game input tokens, then it\noutputs As the next token, it predicted",
        "start": 5961.039,
        "duration": 6.38
    },
    {
        "text": "was its action, and then it did another\naction, and then it gets, receives then",
        "start": 5967.419,
        "duration": 5.11
    },
    {
        "text": "the next sensory input, and so forth.",
        "start": 5972.529,
        "duration": 1.89
    },
    {
        "text": "And so this can, yeah, it can,\nIs that done, I'm sorry, is",
        "start": 5975.559,
        "duration": 3.58
    },
    {
        "text": "that done serially, or is that?",
        "start": 5979.139,
        "duration": 1.28
    },
    {
        "text": "That is done, yeah, that will be\ndone serially at, at inference time.",
        "start": 5980.419,
        "duration": 5.81
    },
    {
        "text": "Yeah.",
        "start": 5986.839,
        "duration": 0.23
    },
    {
        "text": "it might get a batch of these\nat once, I don't know, with",
        "start": 5988.469,
        "duration": 2.29
    },
    {
        "text": "the, like the visual input, but,",
        "start": 5990.759,
        "duration": 2.57
    },
    {
        "text": "but, What's interesting is then, yeah, it\ndoes have a sense of recurrence in that,",
        "start": 5995.349,
        "duration": 4.78
    },
    {
        "text": "obviously it can just keep concatenating\nthese, but of course there's a limit to",
        "start": 6000.149,
        "duration": 5.04
    },
    {
        "text": "how long you can continue this before\nyou completely forget about your past.",
        "start": 6005.189,
        "duration": 4.82
    },
    {
        "text": "and it's also just very computationally\nintense because, yeah, you're",
        "start": 6011.449,
        "duration": 4.04
    },
    {
        "text": "essentially, just doing all of these\noperations, at every, next step.",
        "start": 6015.489,
        "duration": 4.65
    },
    {
        "text": "yeah, in order to deal with this, they\nused a, an architecture that's designed",
        "start": 6020.749,
        "duration": 4.26
    },
    {
        "text": "to have a longer distant horizon, but\nI guess the important thing is there's",
        "start": 6025.009,
        "duration": 4.16
    },
    {
        "text": "no real, there's no recurrence really\nwithin the network, it's, recurrence",
        "start": 6029.169,
        "duration": 5.29
    },
    {
        "text": "with the input that it's dealing with.",
        "start": 6034.489,
        "duration": 2.49
    },
    {
        "text": "and that's quite similar for this,\nmore recent work, PaLM-E, so PaLM was",
        "start": 6040.529,
        "duration": 5.02
    },
    {
        "text": "this huge language model from Google,\nand PaLM-E was their subsequent work",
        "start": 6045.549,
        "duration": 3.64
    },
    {
        "text": "to try and leverage this for robotics.",
        "start": 6049.189,
        "duration": 1.92
    },
    {
        "text": "With a multimodal, model.",
        "start": 6051.784,
        "duration": 1.59
    },
    {
        "text": "And actually, yeah, this was\npresented recently, at DLCT.",
        "start": 6054.024,
        "duration": 5.14
    },
    {
        "text": "and yeah, so they just explore, for\nexample, can you get a robot to,",
        "start": 6059.854,
        "duration": 3.83
    },
    {
        "text": "pick up these, a bag of chips, like\nthis based on kind of a natural",
        "start": 6063.894,
        "duration": 4.91
    },
    {
        "text": "language prompt and an image.",
        "start": 6068.804,
        "duration": 2.16
    },
    {
        "text": "So it's multimodal in that sense.",
        "start": 6071.144,
        "duration": 1.38
    },
    {
        "text": "And then, the model's job is to output.",
        "start": 6073.334,
        "duration": 3.78
    },
    {
        "text": "More natural language, which will\ncondition the robot's kind of",
        "start": 6077.709,
        "duration": 4.04
    },
    {
        "text": "action sequences that it can take.",
        "start": 6081.779,
        "duration": 1.56
    },
    {
        "text": "for example, like grabbing something.",
        "start": 6083.969,
        "duration": 1.47
    },
    {
        "text": "but again, the kind of the\nway that they're dealing with",
        "start": 6088.629,
        "duration": 2.34
    },
    {
        "text": "embodiment is I want to be clear.",
        "start": 6091.589,
        "duration": 1.67
    },
    {
        "text": "They take the original prompt, then\nthey create a series of language steps.",
        "start": 6093.359,
        "duration": 6.73
    },
    {
        "text": "Yeah.",
        "start": 6100.999,
        "duration": 0.35
    },
    {
        "text": "They actually go through that, they\nactually generate these language",
        "start": 6101.799,
        "duration": 2.67
    },
    {
        "text": "Sentences, and then those become,\nthose are mapped to movements.",
        "start": 6105.124,
        "duration": 5.14
    },
    {
        "text": "yeah, and the model, as part of the\nwhole like end to end learning, the",
        "start": 6112.204,
        "duration": 4.29
    },
    {
        "text": "model has to, like the language model,\nwhatever, has to learn like what are the",
        "start": 6116.494,
        "duration": 3.99
    },
    {
        "text": "reasonable outputs that will actually\nmap onto actions that the robot can take.",
        "start": 6120.484,
        "duration": 5.98
    },
    {
        "text": "they don't do anything to\ndirectly constrain that.",
        "start": 6129.614,
        "duration": 2.06
    },
    {
        "text": "They're going back.",
        "start": 6132.794,
        "duration": 0.43
    },
    {
        "text": "But yeah.",
        "start": 6133.334,
        "duration": 0.26
    },
    {
        "text": "They're going back through\nlanguage to do that again.",
        "start": 6133.594,
        "duration": 2.83
    },
    {
        "text": "It's interesting.",
        "start": 6136.604,
        "duration": 0.8
    },
    {
        "text": "I don't have to say to myself,\nopen the drawer and, reach",
        "start": 6138.434,
        "duration": 6.25
    },
    {
        "text": "for the green bag out of it.",
        "start": 6144.694,
        "duration": 1.39
    },
    {
        "text": "It looks like it's interesting, but Yeah,\nbut then when it's, once it generates the",
        "start": 6146.084,
        "duration": 4.11
    },
    {
        "text": "language instruction, does it then go off\nand do the entire sequence or does it keep",
        "start": 6150.194,
        "duration": 4.85
    },
    {
        "text": "Yeah, so this is the thing.",
        "start": 6157.254,
        "duration": 2.06
    },
    {
        "text": "So it, at any given time point,\nit has access to, the instruction.",
        "start": 6159.334,
        "duration": 7.77
    },
    {
        "text": "So that's the start of the sequence.",
        "start": 6167.994,
        "duration": 1.63
    },
    {
        "text": "It has all of the actions it's taken\nitself, like all the actions it's",
        "start": 6170.134,
        "duration": 4.15
    },
    {
        "text": "generated in natural language, and\nthen it has the current visual input.",
        "start": 6174.284,
        "duration": 5.96
    },
    {
        "text": "But what happens, what if you can't open\nthe draw, or the, or, I don't know, I",
        "start": 6180.524,
        "duration": 7.22
    },
    {
        "text": "guess something, that, that, requires,\na change in plan or something, right?",
        "start": 6187.744,
        "duration": 6.1
    },
    {
        "text": "Yeah, so they show that with\nthis, the human knocks the rice",
        "start": 6193.844,
        "duration": 3.72
    },
    {
        "text": "chips back into the drawer.",
        "start": 6197.564,
        "duration": 1.13
    },
    {
        "text": "And so I guess that at that time point,\nat the next time point, so it already",
        "start": 6199.494,
        "duration": 5.19
    },
    {
        "text": "said take the rice chips out of the\ndoor, but then the rice chips are back,",
        "start": 6204.694,
        "duration": 4.81
    },
    {
        "text": "and then it's, what you call it,\nbut it sees the image, so yeah, so",
        "start": 6211.804,
        "duration": 3.29
    },
    {
        "text": "they're still there, so it basically\njust outputs the same action.",
        "start": 6215.094,
        "duration": 2.79
    },
    {
        "text": "So it's not like it, I don't know,\nit doesn't understand why it's",
        "start": 6218.724,
        "duration": 3.97
    },
    {
        "text": "gone back or anything like that.",
        "start": 6222.694,
        "duration": 1.21
    },
    {
        "text": "Yeah, a classic example might be I try\nto open the drawer and maybe there's",
        "start": 6223.904,
        "duration": 4.45
    },
    {
        "text": "a latch and I didn't know there was a\nlatch or button I have to press, so it",
        "start": 6228.354,
        "duration": 3.91
    },
    {
        "text": "says open the drawer, it doesn't work.",
        "start": 6232.274,
        "duration": 1.38
    },
    {
        "text": "it's possible, that, it would have\nlearned that, okay, I tried this.",
        "start": 6236.819,
        "duration": 3.67
    },
    {
        "text": "And I see the image, and okay,\non the next one it might attend",
        "start": 6241.004,
        "duration": 3.63
    },
    {
        "text": "more to the lock or something.",
        "start": 6244.644,
        "duration": 1.74
    },
    {
        "text": "I don't think it's inconceivable\nthat it could learn to do that.",
        "start": 6246.904,
        "duration": 2.37
    },
    {
        "text": "But I think, yeah, it feels like the main\nkind of weird thing about this is there's,",
        "start": 6249.324,
        "duration": 6.68
    },
    {
        "text": "yeah, there's just a bit of a, a\nmissing, yeah, world model and just",
        "start": 6258.534,
        "duration": 6.2
    },
    {
        "text": "continuous, continuous representation,\nfor example, of visual space.",
        "start": 6264.734,
        "duration": 4.05
    },
    {
        "text": "it literally just has, the actions it's\ntaken and its current visual input.",
        "start": 6268.924,
        "duration": 4.41
    },
    {
        "text": "which is of course very different\nfrom how we would interact with and",
        "start": 6274.284,
        "duration": 4.41
    },
    {
        "text": "again, they're just like concatenating\nall of this on top of each other",
        "start": 6278.694,
        "duration": 3.31
    },
    {
        "text": "and then just doing the full,\ntransformer operation across all of it.",
        "start": 6282.014,
        "duration": 3.5
    },
    {
        "text": "again, it's not clear to me that,\nthey might be able to get this",
        "start": 6286.254,
        "duration": 3.36
    },
    {
        "text": "kind of system working really\nwell under certain environments.",
        "start": 6289.614,
        "duration": 2.84
    },
    {
        "text": "it's not, it's, I don't know, it's\nhard to say how good they can get.",
        "start": 6292.954,
        "duration": 4.27
    },
    {
        "text": "This is just the beginning.",
        "start": 6297.234,
        "duration": 1.06
    },
    {
        "text": "or maybe I'll run into\nfundamental issues, I don't know.",
        "start": 6299.489,
        "duration": 2.41
    },
    {
        "text": "Yeah, but yeah, so\nbasically, yeah, go ahead.",
        "start": 6302.779,
        "duration": 3.89
    },
    {
        "text": "Do they then generate, sub policies to\nactually translate these sentences into",
        "start": 6307.679,
        "duration": 5.26
    },
    {
        "text": "motor commands, or joint movements?",
        "start": 6313.169,
        "duration": 2.82
    },
    {
        "text": "no, I don't think so, but, the, this\nkind of thing itself is I can't remember",
        "start": 6318.219,
        "duration": 5.02
    },
    {
        "text": "if there's also a step where it it\nfirst chains together these things.",
        "start": 6323.239,
        "duration": 4.08
    },
    {
        "text": "Thanks.",
        "start": 6327.319,
        "duration": 0.084
    },
    {
        "text": "I, it was either this or another\npaper where, bring me the rice chips",
        "start": 6330.124,
        "duration": 4.27
    },
    {
        "text": "from the drawer, and then it's meant\nto first create, a bullet point of",
        "start": 6334.394,
        "duration": 2.46
    },
    {
        "text": "all the actions it's gonna take.",
        "start": 6336.864,
        "duration": 1.28
    },
    {
        "text": "And then it says okay,\nfirst action, do this.",
        "start": 6338.889,
        "duration": 2.09
    },
    {
        "text": "so yeah, no, but I don't think\nthey, do anything with, sub",
        "start": 6342.899,
        "duration": 6.13
    },
    {
        "text": "policies, but, yeah, no, it's not.",
        "start": 6349.279,
        "duration": 2.63
    },
    {
        "text": "just from my experience, it seems\nlike it's, so much easier to take",
        "start": 6353.409,
        "duration": 3.29
    },
    {
        "text": "seven steps than to actually take\nthe rice chips out of the drawer.",
        "start": 6356.829,
        "duration": 3.64
    },
    {
        "text": "That seems like a lot more difficult\nthan just writing out seven steps.",
        "start": 6360.729,
        "duration": 4.11
    },
    {
        "text": "Yeah, and what if the robot arm was in\na different position than it usually",
        "start": 6365.564,
        "duration": 3.67
    },
    {
        "text": "was, or, things are rotated, would it,\nit's not, these could be very simple",
        "start": 6369.234,
        "duration": 7.15
    },
    {
        "text": "robotic commands if everything's fixed.",
        "start": 6376.424,
        "duration": 2.05
    },
    {
        "text": "Yeah, I think the problem.",
        "start": 6378.474,
        "duration": 3.92
    },
    {
        "text": "yeah, I don't know if they considered\nthat or maybe it's something they're,",
        "start": 6382.734,
        "duration": 4.87
    },
    {
        "text": "doing now but, yeah, I think in\ngeneral they were trying to like really",
        "start": 6387.604,
        "duration": 4.18
    },
    {
        "text": "lean heavily on the language model.",
        "start": 6391.794,
        "duration": 2.13
    },
    {
        "text": "Yeah.",
        "start": 6393.924,
        "duration": 0.06
    },
    {
        "text": "But I guess with the caveat that\nit's, yeah, maybe not very good at",
        "start": 6394.444,
        "duration": 5.11
    },
    {
        "text": "describing these lower level things,",
        "start": 6399.964,
        "duration": 1.48
    },
    {
        "text": "but yeah,",
        "start": 6404.654,
        "duration": 0.5
    },
    {
        "text": "yeah, that's, I'll just skip over that,",
        "start": 6409.814,
        "duration": 2.96
    },
    {
        "text": "yeah, just the other kind of really big\ndifference to mention is, yeah, obviously,",
        "start": 6414.884,
        "duration": 4.21
    },
    {
        "text": "You have either rapid, sensorimotor,\nreference frame building, few shot based",
        "start": 6419.409,
        "duration": 3.81
    },
    {
        "text": "on real world inputs, or you kind of\nbackprop on the, a significant chunk",
        "start": 6423.219,
        "duration": 4.38
    },
    {
        "text": "of the internet and absorb all the kind\nof horrible things, that are out there.",
        "start": 6427.599,
        "duration": 4.22
    },
    {
        "text": "that's just, yeah,\nanother, big difference.",
        "start": 6434.009,
        "duration": 2.8
    },
    {
        "text": "The last thing I just was, yeah, hoping\nto talk about is how can we maybe",
        "start": 6437.834,
        "duration": 3.7
    },
    {
        "text": "bring some of these ideas together.",
        "start": 6441.754,
        "duration": 1.48
    },
    {
        "text": "two caveats to just mention, yeah, I'm\nsure some of these things have been",
        "start": 6444.954,
        "duration": 3.58
    },
    {
        "text": "tried in the literature, but maybe not\nin combination, maybe not, particularly",
        "start": 6448.534,
        "duration": 4.82
    },
    {
        "text": "in the context of embodiment, but, but\nyeah, that would definitely be a next",
        "start": 6453.644,
        "duration": 2.83
    },
    {
        "text": "step if we do want to think about this.",
        "start": 6456.474,
        "duration": 1.63
    },
    {
        "text": "Firstly, just check and maybe\nsomeone here already knows.",
        "start": 6458.124,
        "duration": 2.77
    },
    {
        "text": "But yeah, but I think turning\nthese token representations into",
        "start": 6461.614,
        "duration": 3.99
    },
    {
        "text": "something a bit more like an RNN.",
        "start": 6465.604,
        "duration": 2.11
    },
    {
        "text": "within that token, it's still part of\na transformer, but just it, it also",
        "start": 6469.659,
        "duration": 4.24
    },
    {
        "text": "updates itself based on its, past history.",
        "start": 6474.079,
        "duration": 2.76
    },
    {
        "text": "and in particular, if there was maybe\nsome sort of separation out of pose",
        "start": 6478.374,
        "duration": 4.06
    },
    {
        "text": "embeddings from feature embeddings,\nthroughout the architecture,",
        "start": 6482.704,
        "duration": 3.51
    },
    {
        "text": "then I think this would get, yeah, a\nbit closer to, something more Monty,",
        "start": 6488.374,
        "duration": 4.92
    },
    {
        "text": "and, would make more sense for something\nlike embodiment where kind of, rather",
        "start": 6496.134,
        "duration": 4.39
    },
    {
        "text": "than having to constantly concatenate the\nfull input at every, like the, full past",
        "start": 6500.524,
        "duration": 6.855
    },
    {
        "text": "and the full, future, when you are, doing\nembodied, learning, you'd, because this is",
        "start": 6507.379,
        "duration": 7.05
    },
    {
        "text": "what I meant by this approach is a little",
        "start": 6514.429,
        "duration": 1.89
    },
    {
        "text": "like, they just keep adding, sorry, like\nthis one, it's like they keep adding",
        "start": 6518.709,
        "duration": 4.31
    },
    {
        "text": "kind of new columns onto the brain if, or\nrecruiting new columns if you imagine that",
        "start": 6523.019,
        "duration": 3.9
    },
    {
        "text": "these are, columns, whereas if we assume\nwe have a fixed number of columns, we",
        "start": 6526.919,
        "duration": 4.35
    },
    {
        "text": "can deal with, you can still process like\nthe visual input and whatnot in parallel.",
        "start": 6531.269,
        "duration": 3.74
    },
    {
        "text": "but you keep track of history\nthrough this kind of more, self",
        "start": 6536.634,
        "duration": 3.19
    },
    {
        "text": "occurrence within a given column.",
        "start": 6539.824,
        "duration": 1.76
    },
    {
        "text": "And that would be more similar\nto what, Monty would, do.",
        "start": 6541.584,
        "duration": 4.32
    },
    {
        "text": "and then, yeah, making this kind of\nvoting, this kind of special case.",
        "start": 6549.884,
        "duration": 4.63
    },
    {
        "text": "So it might already be happening,\nbut it requires that the, attention",
        "start": 6554.514,
        "duration": 3.45
    },
    {
        "text": "has learned the identity operation.",
        "start": 6557.974,
        "duration": 1.99
    },
    {
        "text": "I didn't get time to check whether that's\nlike an observation that ever happens.",
        "start": 6561.204,
        "duration": 3.16
    },
    {
        "text": "That would be interesting if it does.",
        "start": 6564.404,
        "duration": 1.67
    },
    {
        "text": "But.",
        "start": 6566.104,
        "duration": 0.25
    },
    {
        "text": "But,",
        "start": 6567.154,
        "duration": 0.27
    },
    {
        "text": "maybe settings, like adding some\nadditional attention heads to the ones",
        "start": 6569.654,
        "duration": 3.49
    },
    {
        "text": "that typically are there, could just\nmean that's like an extra computation",
        "start": 6573.144,
        "duration": 4.22
    },
    {
        "text": "that the system has access to, the\nkind of the feed forward layer could",
        "start": 6577.374,
        "duration": 4.0
    },
    {
        "text": "ignore it if it feels, relevant.",
        "start": 6581.374,
        "duration": 2.61
    },
    {
        "text": "And this maybe would be a bit like\nskip connections, how they do the",
        "start": 6584.844,
        "duration": 3.715
    },
    {
        "text": "identity operation, just like passing,\ninformation through the system.",
        "start": 6588.559,
        "duration": 3.65
    },
    {
        "text": "it would also be parameter free,",
        "start": 6593.429,
        "duration": 1.42
    },
    {
        "text": "because there's no,\nyeah, weights involved.",
        "start": 6597.389,
        "duration": 1.57
    },
    {
        "text": "Or these kinds of other weights\nI was talking about for the kind",
        "start": 6599.429,
        "duration": 2.14
    },
    {
        "text": "of more transformer type, or\nsorry, capsule object processing.",
        "start": 6601.569,
        "duration": 4.77
    },
    {
        "text": "Because again, the key query and\nvalue in this case is shared for a",
        "start": 6607.129,
        "duration": 2.95
    },
    {
        "text": "particular attention head, then I\nguess that would be like one third",
        "start": 6610.079,
        "duration": 2.58
    },
    {
        "text": "the typical parameter count cost.",
        "start": 6612.659,
        "duration": 1.57
    },
    {
        "text": "so yeah.",
        "start": 6616.359,
        "duration": 0.38
    },
    {
        "text": "these could maybe be interesting,\ninductive biases to try,",
        "start": 6617.779,
        "duration": 3.67
    },
    {
        "text": "and, yeah.",
        "start": 6624.649,
        "duration": 1.33
    },
    {
        "text": "some of these changes could be made to\npre trained networks and then further",
        "start": 6627.089,
        "duration": 3.63
    },
    {
        "text": "fine tuned, but obviously there's a lot\nof detail that would need to be done to",
        "start": 6630.719,
        "duration": 3.62
    },
    {
        "text": "make, to think through whether it would\nactually work or what problems there are.",
        "start": 6634.339,
        "duration": 3.44
    },
    {
        "text": "but some of the kind of more complex\nthings to, to introduce, would be, yeah,",
        "start": 6640.229,
        "duration": 5.41
    },
    {
        "text": "these kinds of things like top down\nconnections, I think, yeah, that would",
        "start": 6645.639,
        "duration": 3.29
    },
    {
        "text": "maybe cause issues for backpropagation,\njust explicit spatial models, how",
        "start": 6649.119,
        "duration": 5.61
    },
    {
        "text": "to do that, I don't know if the kind\nof recurrence and the, and that sort",
        "start": 6654.729,
        "duration": 3.23
    },
    {
        "text": "of thing would help have that emerge\nnaturally, it would be a bit similar",
        "start": 6657.959,
        "duration": 4.54
    },
    {
        "text": "to then, like the representations\nwould be a bit similar to some, In the",
        "start": 6662.499,
        "duration": 4.535
    },
    {
        "text": "literature where they found like grid\ncells emerged just through learning.",
        "start": 6667.034,
        "duration": 3.41
    },
    {
        "text": "And then what you were talking\nabout, Jeff, like this whole kind of",
        "start": 6672.184,
        "duration": 2.49
    },
    {
        "text": "compositional objects like rapid binding\nbetween reference frames and features.",
        "start": 6675.604,
        "duration": 3.52
    },
    {
        "text": "It's not clear that there's any\nkind of easy solution again without",
        "start": 6679.794,
        "duration": 4.61
    },
    {
        "text": "kind of explicit spatial models.",
        "start": 6684.404,
        "duration": 2.439
    }
]