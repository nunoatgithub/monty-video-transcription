Welcome to the second video in the core video series. This one is a follow on to the last video where Jeff presented the idea of the AI bus for the first time. And that idea was really the key spark that started this whole thousand brains project and those are the ideas that this entire project is based on. And the video that's about to come now was a follow on a couple of weeks later after we had some more in depth research discussions about the ideas that Jeff presented. Here he presents those outcomes to the entire company and also clarifies a couple of misconceptions. He also goes over the advantages he sees and why he thinks this is such an interesting path to take. And I must say, after a few years, I still agree that this is a super interesting path to be on. And it's really been a great path. Again, he talks about the AI bus, which we now call the cortical messaging protocol. And the idea of communicating information in a common reference frame, which is exactly what we're doing today in our code implementation. He also talks a bit more about the details of voting.

Another thing he mentions is that this approach is maybe a little bit different to what we've done so far in the sense that for some parts of the implementation, we might actively decide to deviate from how we know the brain is doing things. This is such a huge undertaking, and we want to get the whole framework but we know that some of the details are implemented differently in the brain. But we can't do everything all at once. for example, something we decided, and something we've been doing until now, is to use explicit graphs to represent models of objects and also to use Euclidean reference frames which we know the brain doesn't use. But we use those because they make it easier for us to debug and to visualize what exactly is happening in the system and where maybe things are going wrong and also to figure out ourselves what is the kind of information the system needs to do what we needed to do instead of working with some learning black box algorithm, where we don't really know what's going wrong and we can't really visualize what's happening inside. And this approach has worked out quite well for us. It helped us to figure out the whole framework around it, this whole sensorimotor learning framework and the cortical messaging protocol. And by now we feel quite confident in this framework around it. And you can now take these quite explicit modules and replace them with more biologically plausible ones. and that's the idea here, that you can take these learning modules, you can implement custom learning modules that work on very different principles, as long as they implement this cortical messaging protocol. You can mix and match them, you can swap them out, you can try different implementations of learning modules. So without further ado, take a big swig of coffee or tea, and focus all your 150, 000 cortical columns on this video. Cheers

alright, let's just jump into it. This is not in the images I sent for the purpose of the meeting today, and, I have two purposes to doing this meeting today. One is to correct a misconception that I created, that somehow we're abandoning the neuroscience in our work. That's not true. I'm going to make sure that's clear. And then the other one is to better explain why I'm excited about the AI bus, the opportunities for it. This turned out to be pretty difficult to do, pretty difficult to explain why I'm excited about it. And. I had to spend a lot of time on it to think about it. This is often the case where you can know something, but it's difficult to explain it. expressing it in words. And in the end, I felt the way to do this was to actually go through a sort of summary of Numenta and what we've achieved and where we're going, and then in that context I think it becomes clear. So hopefully, I'm going to do my best to clarify that. I know there's a lot of confusion about that. just a few things. We're not trying to make any decisions today. There's absolutely no reason to decide anything today. This is just an information sharing thing and It can be interactive. I, I tend to get in a sort of presenter mode, but please make questions and welcome I'm going to avoid any technical questions. So if there's questions that, let's say, Christy or Charmaine or even our new employee can't understand, we shouldn't be asking those.

I'll just make sure we don't do that. So that's the purpose of today. You don't have an image of that, but those are the two things I wanted to correct today. All right. This is the agenda, this little image right here. I'm going to go through these in order, our company goals, talk about that a bit because it's important, how we tackled goal number one, what we learned, talk about a roadmap, talk about the AI bus, which you'll see now, I'm sometimes calling it the cortical communications protocol because the bus term is confusing to some people.

I'm then going to do a little thing about the birth of industries, mostly about Amazon. and I'm going to talk about the second road map. This is not to replace the third road map. This is really of a way to think about the AI bus, what we might do in a road map for the AI bus. And, and then talk about options as a company, what we might do next, and these are just options and it may not be inclusive list, but, to discuss the different things we have on the front. So that's just the, I'm going to go through these in order. Each one of these has a little box here where you can see number one, number two, number three.

this is a method that I use quite a bit, and I think Marcus uses it now, too, to some extent. put these in boxes when you're working with somebody. I thought recently I would do this in a PowerPoint presentation, but it's just too hard.

All right, so if there aren't any questions, let's just get into it, all right? number one, goals. I paraphrase our goals. Everyone should know this. We have two goals as a company. the first one is biological goals, reverse engineering of the neocortex. And that one is really a neuroscience goal. It's not a machine learning goal. It's a neuroscience goal. And I often use the word biologically constrained.

Which one of them is not muted? It's my fault.

so I often use the word biologically constrained because from a neuroscience theory, we don't have any theories about what's going on in the brain that, that conflicts with known neuroscience. there's occasionally we'll accept something, we say, this isn't quite right, we know that, but we'll move on anyway. But, so this is really, it. It's not the goal here is to explain everything going on in the cortex of the brain, but the things we do explain can't violate any of the principles we know or any of the details we know exist. So that's how that is viewed there. The second goal is really about machine intelligence, and it relates to the first goal, but it's about machine intelligence. There's different ways of expressing this. We've talked about being a leader in the future of AI. based on what we've learned inside brain. I always like to use this phrase. This is one I used from the very beginning, and it's not the one we typically show, but, I like it. Which is to be a catalyst for the age of intelligent machines.

I think in this century, we're going to see this tremendous explosion of truly intelligent machines and robotics. And it's going to impact the world in the same way computers did in the last century. Maybe even more probably more And, the idea of being a catalyst is this is going to happen one way or the other, right? Something is going to happen whether we're here or not. All you can do as an entrepreneurial team is to accelerate it. You can just don't think it's going to happen anyway. You can make it go faster or make it happen sooner. and that's how I view it here. A catalyst can accelerate a reaction. many fold, and that's what, I think we've got this, potential to do. There's a bunch of assumptions underlying this, and I think it's worth pulling them out. one assumption is today's AI is not intelligent. It's not really where we're going to be. it's really dumb in some ways. Work, as spectacularly stupid. Was that one strikingly stupid was one of the people and you called them. it's not that it's not useful. It's not cool or anything like that, but it's not really intelligent. It's not like where it's going to be. The industry of AI is going to be something different in the future. and, so the future of AI will be based on different principles that we have today. It's not like we have to throw away all the stuff that's been going on deep learning and so on, but it's insufficient. Then the second assumption was that the fastest way to discover those new principles was to start with by reverse engineering the brain. And this was a very, unpopular view. I, have, I ran into this when I applied to MIT many years ago, and they said, Oh, the brain is just a stupid computer. There's no reason to study it. That's literally what I was told by professors at MIT. that was a close to verbatim quote.

like that is a, it's not clear, it could be that a lot of smart people working in machine learning and don't know anything about the brain could discover these principles and, there's nothing to say they couldn't. And so this was a bet really. It was a bet that says, I know it seems like a long way to go to figure out how the brain works first, but I'm betting that's actually the quickest way. And other people said, nah, we'll just go right there, and we'll just go start building these things. this is not a foregone conclusion. it was a bet, and it was an assumption. So far, I think it's held pretty good. and, it's not so clear. the machine learning world is converging on some of the same ideas that we've discovered in the brain, and so on. But, anyway, I thought it was worth spelling out those assumptions.

I was once asked by one of our employees, which of the two of these is the most important if I can only pick one to succeed at which one they thought I was going to pick this one, but I said no, totally this one. So I personally want to understand how the brain works and, and we can do this too. That's amazing. That's great. It's like super good. Okay, so those are our goals. Any questions about that?

Yeah, thank you. Yeah. To know that today's AI is not intelligence and we want to decide intelligence is when we know what is missing, what is that gap. Yeah, we don't know that. So that's the way to start, if you could study the brain and figure out what the brain does and how it does it, then you could look back and say, Oh, this is what we missed. I, sometimes the question I asked when people said, I said, how do you know what, when you start in the brain, how do you know which principles are important? I said, I don't know, but you'll know it when you figure the whole thing out.

it wasn't even a fundamental question of what is intelligence? that's a bigger question. But we have to answer that question, right? We have to answer, what is intelligence? It's not oh, we need this feature or that feature. No, it's what is it? It's not about playing Go or solving some machine learning problem. It's what is the essence of intelligence? And, and my book is all about that. and, I think we've really got a really good handle on that. But we started out not knowing that at all. My original interest in getting in this field was like, I was interested in understanding what my brain was doing, and I had, there was zero information. What the hell is it doing? It's oh yeah, it's obvious, it speaks, it doesn't know, but what are the core principles at work on? How's information organized? what's it function? And that was a real black hole. nobody knew any of that stuff. So did I answer your question? Yeah. Anything else? We'll go on to the next one. So now we're going into box number two for those online who didn't follow. Okay. Along. so how do we go about tackling? Oh, anyway, there's an order to this right order is do number one first, then do number 2 there's a point you can't work on number two.

So then, so how do we tackle number one so just worth reiterating how we went about it. I said, we're going to focus on the neocortex, not the whole brain. and fortunately, the cortex, as you all know, is made of many identical cortical columns. all mammals have cortical columns. They're all very similar. We have more than other mammals and so on. We've got 150, 000 roughly in our brain, with rats and dogs and cats, even, even non mammals have something, even birds have something equivalent to a cortical column. we can then break the question into two parts. One is, what does a cortical column do and how does it do it? Because he got a whole lot of them. And Vernon Mountcastle himself suggested, hey, if you could figure out what a cortical column does, that would be pretty important. then the second question is, even if you know what it does, how do they work together? Because you got a whole bunch of them, thousands of them, how do they communicate with each other? What do they do to work together? And so here too, he said, there's not really much point in thinking about this one. How do they work together until we figure out what they do? And the analogy I sometimes use is it's like you're trying to understand what society does or how society works, you better first understand what the person does. basically something about them. so we focused on this one. We really focused on what does the cortical column do and how does it do it?

we had no overall picture understanding this when we started this We had absolutely no idea what a cortical column does. There's some fuzzy ideas in the neuroscience world about oh, it's extracts features But we knew it was much more complicated than that.

we didn't know any of the things we know today in the thousand brains theory So, we did the way we did it we identified Subfunctions things we knew that a column had to do and try to tackle them one at a time So you think about it this way. If the neocortex is made of copies of identical units, then everything that the neocortex does, in some sense, has to be done by each unit. You can't have, and so the idea is, if I know that the brain can learn sequences, and all the columns are doing the same thing, then all the columns have to be able to learn sequences, otherwise they wouldn't be the same. And that was the idea there, if we could pick things that we knew they had to do, and we focused on prediction, because we knew the brain made, the cortex made predictions. We started thinking about sequence memory, and I'm not going to go through all of this. we then had to think about how do we represent inputs in different contexts, we had to figure out how we do uncertainty, and, so we started tackling these things, and this, these led to theories about, how we came about with the different theories and papers we published. Then we moved on to how we make predictions based on movement. Again, not really understanding what cortical column does overall, but knowing these are pieces of what it does. we have to study other brain regions too, as needed. So if you've been around here for a while, you know we've spent a lot of time talking about the thalamus, the entorhinal cortex, the hippocampus. cortex doesn't exist in isolation, and as we needed to, we would bring in these things. Now we have, we, the neuroscience world itself has a lot of knowledge now about what's going on in the entorhinal cortex and hippocampus, which relates to the cortex. we have theories about the thalamus. We haven't really resolved those yet, so we, there's still a bit of a mystery about that.

and, then, as we thought about this, we mostly restricted our studies to columns that get direct sensory input. in the cortex, there's a lot of columns that get input from the eyes, ears, and the skin. There's other columns that just don't directly get sensory input, they get from other parts of the brain. And, we focus less on the ones like, okay, your touch or your vision or something like that. Any questions about that? That's how we learn about them. That's what we've been doing for the last, I don't know, 10 years.

Literally 10 years. I think maybe more than that. Any other questions about that? Luiz not, you recognize this hopefully. But I want to put in the context because there's a logic to this. It was, we didn't lay this out like this up front, but this is what we were doing basically. All right, let's go on to column number three. What did we learn? I'm not going to go through this in detail, but I'm going to start with the big picture because we start off all these little details, but then in the end with the thousand brains theory, we have this big picture. So each column is like a miniature brain, and associated with each column is something that is like a sensor patch. That's the way I think about it, you've got like a patch of your skin or patch of your retina or part of your ear. Cochlea, and that little patch is moving around in the world. and the trick was that the column knows where that sensor patch is relative to the thing it's sensing. That was the key insight that will come from the thousand brain theory, and that required a reference frame. So for a column to know where it's finger is relative to the coffee cup, or my ear know where, it is relative to some sound that you're hearing here outside, that was a total surprise, to me at least. that it could do that, but once we realized that, then the whole thing came, came apart, or it came together. therefore, as a sensor moves, the column can learn a model of the structure of something in the world by moving and sensing and moving and sensing, because it knows where it is. And, and so the idea that even columns in the low sensory regions of the cortex can learn complete spatial models of objects through observation and movement over time was like the big insight of the Thousand Brains Theory. Okay, I want to just talk about a couple of things here because I think it's relevant to the AI Bus. here's the way we've been thinking about this recently, work Marcus and myself and others have done here. here's a picture of two, out here we have our chairs and the little circular tables out here, remember? So here are two chairs like that, here's a little circular table, and this is another little table I just threw in there for good measure. So imagine this is some, this is an abstract representation of what's going on in a cortical column. You've got this arrangement of things out there and you want to learn that arrangement of things. And what happens is, a model is basically, a representation of things in the world at relative positions and orientations. So I want to have, I want to know what that seating area is out there. And I do, I have a picture in my head right now what it looks like, right? You do too, probably, if you've been here a while.

how's that represented in the brain? the brain represented essentially by saying, there's a chair at some orientation position to the table, there's another chair around the table. And these green links here represent the relative, what we call displacements. That's the model. And, it basically is composed of locations and orientations of chairs and tables relative to each other. And that's how everything is learned in the world. There's like things relative to each other. That's what is going on in your head. Everything is composed this way. Bicycles and tables and buildings. It's how we learn things. Now, the interesting thing about it, and this really boggled my mind for so many years, is how, because our sensors are moving around relative to this thing. And your fingers are different than your eyes, because your fingers are touching something, your eyes are not, and so on. But here's the basic idea. You have a sensor patch. That's what these little rectangles represent. This could be your finger, it could be an eyeball. In this case, you can think of it as part of the eyeball. And the way you learn this model is you're at some position, and you observe these components, and as you observe these components, the brain calculates the links to the relative displacements. And if I was observing the same chairs and tables from this position, I'd end up with the same model. It's in the model, it's independent of where the sensor is, but the cortex has to know where the sensor is. That's incredible, it has to know where the sensor is at any point in time, but the model it builds is independent of that sensor location. it's this interaction between this, local knowledge of where I currently am and the model that you're building, which is independent of that. And, and then we also know that, so we, talked about the displacements and, now we think we're operating on a different principle than we did before, but they're still there. that's basically a lot of what's going on in a cortical column, is that columns are sensing things in different locations and orientations and building models. And so later on, you can learn it from different positions. You can infer it from different positions. I can learn the arrangement from here and I can observe it and recognize it from here. And I can extend it by another time by observing it from a different direction. We also know that columns, we have to learn the behaviors of things in the world. Some things move by using staplers, an example, or a smartphone, things move and change. And so the, relative, the models here of things can change and things can move. So we know that columns also move in a sequence of these displacements. To learn how something behaves, you're basically learning how these things are moving relative to each other. And so there's sequences of how these things are changing. It's not random. Stapler opens and closes in a certain way. Now, there's a lot of details we don't know about this, but the big picture is pretty clear.

so I said down here at the columns, perform this, these very complex functions. It's, really complex. it was very hard to get our mind around. And we know a lot about how they do this. And we started with these details, and ended up with this big picture. But we learned a theory of sparsity, we didn't know that when we started here, why sparsity is important, roles it plays, and representations. We had to come up with theory of dendrites and how neurons, use their dendrites. that was, to me, is one of the biggest discoveries we made at Numenta, a real theory of dendrites and neurons changing that. we had to come up with theory of, in some senses, some parts of what many columns do. If we have a laminar circuit, that would be our temporal memory circuit. We've also now included things like grid cells and vector cells, all things we didn't discover, of course, but we were one of the first people to realize that they're inside of every cortical column. And we made the prediction that every cortical column would have these grid cells, which come from the entorhinal cortex, before we knew any evidence for it. There was evidence, but we didn't know it. So I think it still counts. that was a really remarkable prediction in some sense. And I don't think anyone, I think even today people think V1 is going to have grid cells? And yes it is, and even when people are starting to get evidence for it, other people dismiss it.

and we have a lot of details here. I'm not going to go through all these details. We spend hours and hours reading ourselves all these things. The point is that there are many things that still are confusing to us. Even though we have the big picture, and we have lots of details, We don't understand it all. Just, be frank about it. there's a lot of things we just don't really quite get our calls, we sense things, I'm looking at Marcus, we see many bridges.

that doesn't take away from what we've done, it's just, it's being honest. One of the things that came out of the Thousand Brains theory, this idea that columns can do all this stuff, was like, oh, look at this. We think they can vote. We talked about voting. We said, oh, that wasn't even our focus. But voting is actually is, it's in the question number two area. How do columns work together? And up to that point, we haven't been thinking about that at all. There's no concept, we're just not even thinking about it. We're just thinking about cortical columns individually. And we said, oh, look, columns can vote. They can share this, This their hypothesis about what they're sensing. So my eyes and my skin finger can agree on or that multiple fingers can agree on what they're sensing. We did a lot of simulations of this, a lot of ones that Luiz did. they share an hypothesis about what object they are sensing. And this explains a lot of things. It explains how we can sometimes recognize something with a single glance, or, instead of having to move our eyes or finger grasp like this. And importantly, voting works across modalities, meaning Voting works between sense different types of sensory modalities like touch and vision and hearing it can do that. So it's a pretty cool idea And so we modeled that and we simulated it and put it in our papers we did know that there was a hole in the voting. The simplest way to explain it is again touching the coffee cup You want multiple fingers touching the coffee cup they all vote on what they think they're sensing But, but we also now know that they need to know their relative position to each other. In machine learning world, if you didn't know your relative position to each other, it's called a bag of features. It works, but it's not very powerful. And so we knew that, we put it in our papers, it's in my book. we didn't know how it worked, we just said somehow they're going to know where they are relative to each other. So we just put it all to the side. so that's, this kind of summarizes what we've learned in the last 10 years. Any questions about that? Disagreements? Additions? No? All right. All righty, go on.

So now we're going on to number four here, the roadmap. For those who are online, I didn't take a picture of the roadmap. So you don't have this little picture down here. It is in the thing I posted on Slack, though. Okay, but it is pretty much the roadmap we're all familiar with, so it shouldn't be a surprise to you. We have the really great idea, what do we do with all this stuff, right? are we just going to go out and start building a brain? we don't really know how to do that. There's a lot of things we don't know yet, but maybe we can start making, how do we go forward implementing this stuff and what do we do with it? the idea is let's just start applying some of these, I, these principles we learned to existing deep learning networks. That was the idea. that's the DNNs here. So that's a great idea, and it's worked really well. So we started with sparsity, and say, hey, we can make, you can use sparsity maybe for robustness, for speed, and now we've shown, hey, it really does work, and we've been working on that, and we've made great progress on that. It's really exciting. I think the work that everyone's been doing on that in the hardware team is just really amazing. so that's exciting. That's worked out well. The next thing we did is we worked on applying dendrites to deep learning networks. And that's also worked out well. we've done really cool things. this is related to continuous learning and robustness. And, the work there is exciting. and we've been showing. But this is all applying to DNNs deep learning networks. The next point on our roadmap was to take the idea of reference frames and applying it to deep learning networks. And we were ready to start, and we didn't really know how to do that. we didn't, we were struggling. We didn't understand, maybe we could figure it out. Maybe we spent some more time to figure out how it, I think some people are trying to do that. Jeff Hinton is trying to do something like that. First was, with his Capsuls. Now it's Glom. it's also related to, slam type of robotic stuff. So maybe we could do that. But we were struggling with that a bit. And so if you recall, we had a meeting a month or so ago. We all put in, put an idea about how we might add reference frames, what we might do next type of thing. And we were trying to figure out what to do there. and then a couple of people mentioned. Hey, what about voting? That's a cool idea. and it was like, oh my god, that was like a real awakening up for me Because I said, yeah, what asking about voting? oh I should point out. On our roadmap. We had another category over here. It was a catch all category, which was Jumping over to the thousand brains theory, right? All right, we're doing all this stuff, machine learning, boom Thousand brains, building machines, that are intelligent, right? So it's a, put it all over there. voting was part of that, right? but what struck me was, We hadn't thought about question number two. We hadn't been thinking about, how the columns work together. We've been so focused for so long, just understanding what a cortical column does, that we just didn't remind ourselves that we should now maybe look at this again. That's how it felt to me. Okay? So I walked away from that meeting we had a month ago saying, Oh, let me think about voting. And, cause that seems pretty interesting. And maybe there's some things to do there. So that's all the lead up to now the AI bus. Any questions about that? Yeah, thank you. It means like we're not doing the fast learning reference frames? no, it means we didn't have to proceed. It's just a question mark. We didn't know what to do. We didn't have a specific task. Or a specific way of implementing it. I didn't have it. I don't think anyone else had it. So you're right. That's my question. It's not that we didn't want to do it. We just didn't know what to do. It wasn't as clear. It was clear here, how we could apply these principles to existing DNNs. But it wasn't clear. Maybe there is. And I'm going to come back to that. This is still a viable option here. We just didn't know what to do. It wasn't like a question of doing it. It was like, no, we don't know what to do. What task? What are you going to start on tomorrow? What are you going to work on? You know what? What? What's the machine learning goal or benchmark or something like that? We're thinking about it, but we didn't have a clear. If someone disagreed with me, speak up. But as far as I could tell, we didn't have it. We couldn't figure it out. I couldn't figure it out. we didn't know what to do there. but remember, this is always it. This is always like on the path to doing this. It was never the end goal in itself. And, and it was a way for experimenting and learning, take this, sparsely down CPU. That's going to be important for the final theory too. it's all good. And I think the dendrite itself is going to be important for the final implementation of the reference frame is going to be important for that. So the idea of this is breaking off things along the way that can add value both from maybe a monetary value, both from PR communications value, but ultimately we really want to be over here. We don't want to be confused that we're not in the business of enhancing DNNs. that's not what we want to do here. We want to do much more. That's too small a goal.

That clear? Yeah. Yeah. Can you talk about the relationship between voting and learning? that's a, I can. That might be a longer conversation.

almost everything here is learned. There's not really anything in here that's not learning something. So when you say the word learning, you might have a specific idea like what it means to learn as a human. but columns have to learn how to talk to each other doing voting. they have to learn their reference frames. They have to, from a neuroscience point of view, a machine learning point of view, all these things are constantly learning and adjusting to each other. if you're talking about like how I learned something as a human, I think I'll adjust that more. Maybe we could just leave that question for later. I want to get some bigger picture. I think I'll cover that. But what the voting really was at this point in time was a way of different sensory patches, parts of your skin or eyes and ears, whatever, to work together to reach a common consensus. So that literally, if I touch this coffee cup with one finger, I have to move it to recognize a coffee cup. But if I grab it with multiple fingers, I may not have to. How does that work? If I look at the world through a straw, I have to move around like this. That's one column in your cortex. But if I show you an image like this, somehow they all, everyone knows what it is. How is that? so one of the basic things we, I know from lab work, but now understand is that the brain works basically by moving its sensors. And under certain circumstances, we can recognize things without, but those are the exception, not the common. The common method is moving, sensing, moving, sensing. All right, let's put that aside. Good question. I don't want to discourage questions, but I'll get to that later. All right, now we're going on to, number five. the AI bus. I do, we were just joking earlier, I'm also calling it the cortical communications protocol, but now Marcus says that's a communist party or something. Okay, what I meant by this is that, I just threw this name out. We don't have the right names for these things. You know what, a lot of people here didn't even know what a bus was. So, it's oh, that's an old computer term. if you've been around the early computers, you know what buses are, but software engineering might not. so there's a lot of confusion about that. I just, what it is basically, it's a communications protocol. It doesn't have to be anything physical. In the brain, it's these long range connections between columns. So you have all these columns, and there's these long range connections. Some of them go elsewhere and come back, but there's these long range connections. And, so it's physical something in the brain, but it's really just a communications protocol. So if you're going to implement this in machine learning, it doesn't have to be. it's just like a way of saying, here's how we're going to talk to each other. This is how you put information in a certain format.

this is a marketing problem to figure out what you want to call this. so I want to talk about that next. That comes over here, up to this point. so again, what we're dealing now is, question number two from over there is how do columns work together? And like I pointed out, voting was incomplete. for voting to work, the columns need to know where their sensors are relative to each other. And, and the voting system that we outlined and tested and implemented didn't do that. It was a bag of features.

just thinking about that, and we already had all the pieces here. We've already been talking about these components. that led to this idea of this AI bus. So I'm not going to be able to go through it in detail. I'm going to try to give it enough detail to get a flavor for it, and a lot of the details we don't know yet. But there's an idea that these are like cortical columns. we're, now starting to call them modules because we think they actually might be more than one column. They might be like a where column or what column. So it's a little bit confusing about that. So from a machine learning point of view, I might say it's module, because I don't think there's a direct correspondence now with the cortical columns that there could be like, two different columns combined with each other. So I'm going to use that language interchangeably, unfortunately. And the idea here is each of these columns has a sensor patch, that's this little thing here, which is moving around, because it can move around. it needs to know, it's, observing some object, it needs to try to guess what that object is, and it knows it has, it's at, the sensor is at some location to the object, and the object is at some orientation to the sensor. So this kind of information, along with a lot of other information, inside each column. Now, this is the object being sensed, and this is the location and orientation of the object to the sensor. these locations and orientations are all different. It's one finger's over here, another finger's over here, my eye's out here. These are all different positions in the world, different orientations. There's no, there's, at this point, there's no commonality between them. they're all little worlds here. And, what I realized, what we realize now is that in addition to sending, specifying, sharing what they think the object is, that was the voting we wrote about in our papers, that they share something else. They share a sense of location and orientation. And the way they do that is they pick a common point. You might think of it as your body, and say, okay, relative to the body, where is my sensor? And this guy says, relative to some central point, the body, where is my sensor? And relative to some central point, where is my sensor? And they can now, that's, that is what they communicate to each other. They say, oh, I'm seeing some object at some location or orientation relative to a common point, and I have to convert, each column has to convert its local sense of location and orientation into this common one. And then they can also take this common one and convert it back into a local one. So this way everyone knows where everyone else is, or at least they don't know, I didn't, they don't actually know where everyone else is, but they all agree that we're all looking at the same thing at the same point in time in space.

and, this struck me as, this was like a real revelation to me. This was like, it reminded me of, when we realized that there was a reference frame in each cortical column. Initially, we just proved that there was a reference frame, but we didn't know about how the columns learned or anything else. And when I came to, understood there was a reference frame in each cortical column, then I realized, oh my God, this is going to explain how, this is going to be the core piece to explain how the columns work and how they build models. And that turned out to be true. I feel the same way about this. It's a little hard to explain why I feel that way. but I feel like this is the key thing that's gonna break open the whole thing. So let me walk through some of the things you get from this, architecture. Again, imagine one thing, what we're saying here again, is that I might be looking at this cup, and I say there's some feature over here, at some position relative to my body. And now, the column that's representing my finger knows where it is relative to that feature too. That's what this tells me. I say, I know I am relative to that, I'm going to do it relative to the body. This guy knows where he's relative to the body, so if this guy wants to touch it, he knows how to get there. He says, yeah, I know, and when he gets there, he knows what he's going to feel. Even if I close my eyes, I said, Okay, I'm gonna touch it oh, yeah, because I saw it there and that was communicating what it doesn't reveal. Yeah, I'm going to feel the same thing you just saw in the same location. yeah, please do. Please do. is the communication between the columns passive in a sense they're just throwing information on the bus and sending information to the bus, or is active in the sense that they can actually request specific information to other nearby I think it's active. I'm not sure they're going to request, but I'll give some examples of this in a moment.

I'm going to talk about attention. I think it's, I think it's more active. in the sense that, these columns have models of things, right? Like the chairs over there. And, but they all don't have models of everything. An example I've used a lot, is like in my house, I have a visual model of the, rooms in my bedroom to the bathroom, but I don't go around touching it, so I don't have a tactile model. But in the middle of the night, I can take the visual model in my head. And I can't see anything because it's dark. But I know that at some point I should be feeling something at this point. And so essentially the visual system is saying, you know what, I need someone to go out at this point and find out where that edge is. And my fingers. Okay, I'll do that. Oh, I found it now I know where I am. They can share knowledge about the models. They can inform each other. You can request information. I'll talk about attention in a second, which is not for an active But it's an active system, and I don't understand all that yet. It's just, I don't understand, but I think it's active. All right, let's talk about this. Is that good enough for now? Okay, anything else?

Some of the things you get from this is one thing you call it sensory fusion, right? But this means I can build a system an animal or a machine It doesn't really matter And I can combine any number of sensors any number of sensor patches few or many I can put in any location meaning there can be any location of my body Even outside of my body. We'll come to that in a moment and in any modality. and they all work together almost seamlessly. It's as long as they all agree that they're going to talk about, share the certain type of information with each other, it's all going to work. Talk about modalities for a second, this is small, I apologize, I'll read it. in nature, there's all kinds of senses we have in, mammals. There's vision senses, which, there's lots of types of eyes, right? There's eyes that move sideways, there's eyes that go forward, there's eyes that see black and white, there's eyes that see color and black and white, cats have slitty eyes, which helps them. There's different types of eyes. and same with touch, there's multiple sensors in your skin, it's not one thing. of course, hearing, bats use echolocation, which is another type of sense. Rats use whisking, their whiskers are active senses. they can feel objects with their whiskers. It's not just oh, there's something over there. No, they can recognize objects with their whiskers. Each whisker goes into a single cortical column. And it's the clearest example in the world where you have a sensor patch thing. In this case, the sensor patch is a single hair that's sweeping back and forth. And that, when it sweeps back and forth, it's going to detect something at some distance. It can tell how far away that thing is and where it is relative to its body. It knows how to get this stuff. and and in a sense can see with its whiskers. Yeah. That's not like a cat. A cat just has, not like this, but a rascal like this. There's another, if you haven't seen it, take a look at the, star nosed mole. It's unbelievable. It's really ugly. but it has this other sensory system with these like probes coming out of his face. It's like his face exploded, but it just plugs into the cortex like anything else. The point is, you can build almost any sensory system as long as you just can provide the right information, it's going to work. And of course, when we build intelligent machines, we don't have to be restricted to the things biology has. we're right here.

Oh, radar. That's like radar or ultrasound or, all kinds of weirdo sensors you could come up with that doesn't really matter. Anything that can basically you can do a location and sense something and figure out what feature of it. as long as you can, isolated in space that's really important. Even sound is isolated in space and you hear something you know where it is. that's essential. Okay, and when it comes to, these are biological things, but think about what it would be if I was building, applying this to certain type of machine intelligence applications, or just machine applications. Imagine, imagine you have, this is, you are a robot walking around in the room, right? And so you've got some sensors, you can feel things on your limbs, you've got some eyes that can see in the distance. Maybe you got some hearing. I could add new sensors to that robot. I could say, Oh, we're going to give it an infrared eyeballs. We're going to give it, radar eyeballs or something like that. But you could also do the following. I could put a camera. In the corner of the room. And I'm in this corner. And as long as the sensors on that robot, and the sensors in the camera know where they are relative to each other. These become eyes for the robot, it would be seamless, it'd be like putting your hand out someplace and touching something. You can literally have some of these sensors moving around, some of these sensors stationary, as long as the camera can, attend to some part of the visual scene and know where it is, this could work. It's mind blowing when you start thinking about this, how sensors can be arranged in different places. You could have two robots walking around. In the space, each with a set of sensors, but if they unite their sensor, their point of common point of, this, this, common point, then they act as one. It's like how your two arms work as one. Why do they work as one? They're really two separate arms, right? They move around, but they work as one because, They share this common central, idea of where they are relative to each other. And so all of a sudden you could start, building a temporary arms of robots. it's crazy stuff to think about, but I think it's actually going to happen. the next thing I think I realized, and another thing about this, it answers the question, but yeah. Sensor fusion is just a byproduct of voting, right? not voting, it's a byproduct of the AI bus. Not just voting. The key thing here is not the actual object ID as much as it is the relative, the location and orientation of the object you're sensing. each column has its own specific sensor, and if you're fusing the output of the columns, you're also in a way fusing the sensor input. Yeah, but the key to making that work Is they all know that they're attending to the same thing and they all know their location orientation to that thing, right? It's not just what the thing is. It's the whole this may take a while to sink in maybe it's obvious to some people maybe it's not but the Key thing is actually much more important thing than the object is actually the location and the orientation that they all know where they are So they all can say i'm sensing we and i are sensing the same thing from different positions You and therefore, we can vote on what it is, and we know how to move relative to it together and things like that. It's, a, it binds all these columns together into a single entity. Okay, so you're talking about voting and on the object ID, I'm thinking of voting just as a way of aggregating information, which includes also location and orientation. okay, if you want to view that, they, yes, if you're, if you want to think of it this way, not everybody is certain about their orientation and location, right? but they can help each other determine that. So a perfect example is, you're watching me on the talk here or whatever, and then you hear some noise over here. you don't know exactly where the noise was over there. So what happens is, I'll talk about this in the attention system. The auditory system says, somewhere over here I heard something. And your eyes come in immediately over there and go, Oh, I see that someone just opened the door, right? And now I've got much more precise location. So these things play together, right? I have a fuzzy idea of what I'm sensing, a fuzzy idea of the location, a fuzzy idea of the orientation, but other people can contribute to that and can do this voting. They all come to a common agreement, but I think it's much more than voting. I think the bus idea is much more than that. And this is the thing I'm going to struggle to explain. So let me go on to the, if that's okay, I'll go on to the next thing. These are all brand new ideas. There will be new ideas coming up as well soon. But the idea of attention. I've always struggled with exactly what attention is. It seems like you're, isolating your input to some part of your sensory array, what I used to think of it as. And now I realize that's not really true. What it is, attention is a way of communicating an area of interest in the space of, that everyone's agreeing about, everyone's talking about. It's the same. But I want everyone who can to look at this location, and that might mean that in my eyes, I have to converge or I have to, reduce the amount of input from the eyes. it may mean I have to move my hand there. It means I have to turn my head to attend to that. But it's a broadcast thing that everyone can listen to and say, look, if you can attend to this location in space, you're attending to a location in space. Everyone knows how, if they can, how to look at that point in space. it's less about restricting input as opposed to directing people where to restrict their, their, sensory input from in the common space. So I gave that example, if you hear something and you turn to look at it, or if we look over here and I see something move over here, we immediately turn our head to that, or if I'm feeling something, something's unusual, I'll look at it. so this is like an automatic function. Anybody could put out an attentional signal saying, please, everyone, there's something going on. I don't understand over here. Everyone who can please look at it and let's even figure out what it is. So automatic system for that. I also realized that the bus played into a lot of other very high level concepts. let's talk about episodic memory. In the brain, you've got, let's say we have the cortex and get all these columns. They're pretty identical, and, the cortex is, of course, a sheet of neural tissue. If you follow the sheet of neural tissue in your brain, it folds underneath. At some point, it turns into something else. It turns into a few other structures. One is the entorhinal cortex, and then another one is the hippocampus.

And these are structures that are not cortex, but they're they have similarities, and of course we've been borrowing, we've been thinking that the way cortical columns work is picking up in ways that these things work. But the point here is the hippocampus is associated with the episodic memory. So if you're not familiar with this, if I say to you, what did you do today? What did you do this morning? What did you do yesterday? All that information is stored in the hippocampus. None of it's in the cortex. In fact, everything you can tell me about the last three or so weeks is in the hippocampus. It's not in the cortex. And we know this because from a few unfortunate people who lost their hippocampus, they could no longer form new memories. And they remembered everything up to about three or four weeks before the surgery. But after that, no new memories. Nothing. Every day it was like, waking up, 1934, whatever it was that person, right? Groundhog Day. But they had no memory of the day before. And literally, you could talk to me, they could be talking to me, a famous patient at you might have heard of. I could be talking to Luiz, and I don't have a hippocampus, I turn away, I turn back, who are you? That, that profound loss, right? But, we know that over time, long term memories over here, belong over here. So if I say to you, Jeff, Jeff, remember where you grew up, the house you grew up, and so on, that's stored here, but if I went to a new house tomorrow, it would be stored here, and if you remove the hippocampus, we never form these new long term memories. So there's been a long theory that somehow, in the brain, these are very fast memories, and that somehow these memories are transferred back to here. over time. I never believed that hypothesis, mostly because I didn't see how it could work. I couldn't imagine how a hippocampus could transfer memories to something else. there was evidence. It's not quite that simple anyway. There's evidence to suggest that these memories, restore anyway, but you do need the hippocampus. I think it's complex, but the point I'm getting at here right now, it became very clear how this might work. the information going over the bus. Is really the, it's the sort of the things you are perceiving from a body centric point of view, over time. So this is like, Oh, I'm picking up the coffee cups in front of me. Then I moved it to the side over here. And then I saw Luiz doing this and so on and that stuff going over the bus. Like things you're absorbing at different positions relative to the body is exactly the kind of memories you can have in your episodic memory. your episodic memory is not like what you ate this morning isn't I generally eat muffins in the morning. It was like, oh, this morning I had a couple, I had this and then refrigerator was empty and ran out of milk. And it's very, momentary, specific, very egocentric viewpoints, if I'm, what I'm saying. So if you implement a fast memory here and just record what's going on in the bus, and you can play it back. And now I realize that you, the hippocampus could transfer memory back to the cortex because it's just going to transfer back in this format. This format is what's needed to learn models out there. If I, if you tell me there's this object at this location, this object at this location, this object at this location, you can actually build these models in these columns. there's a long way of saying that you can implement episodic memory this way, and it really, it provides a very clear way how this would work. In a, machine intelligence system. Today's AI doesn't do anything like this at all, right? you can't ask, a deep neural network, hey, what were you thinking a moment ago? It doesn't exist. But this kind of system, you can do that. You can say, what are the steps you got to get here? What were you doing? What were you thinking? And in the brain, this is a limited amount of memory. Memory is very, difficult to do. Fast memory is difficult to do. So we have a pretty limited, depth of our episodic memory. But in a computer you can dig as deep as you want. You can go deep, deep, episodic memory if you wanted to.

I also realized that language is going to come out of this as well. now these are partial ideas. There's not, I'm not saying I understand language. But I, believe that the bus idea gives us the framework in which to build language. If you think about what language, at least some of the language is you want to, if I want to communicate to you something through language, whether it's spoken or written, doesn't matter, I'm literally, I need to take what's, if I can take what I'm thinking, what I'm imagining right here on this thing, I want to put it out in a form that language is actually capturing this view. It's saying, I'm, saying, okay, I'm on the phone and you're saying, yeah, I'm looking at the numenta coffee cup, it's in front of me. The handle's turned away from me, and now I want to understand, why did you put the logo on or something like that. And you literally, I can, by putting this information on your bus through language, I can have you imagine and view the same thing I'm viewing. And so it can go both ways. I can share what I'm experiencing with someone, or my memories, whether it's the memories are coming from the episodic memories or long term memories, I can express it to you in a way that goes into your brain, and you can put it into your short term memory. Or you can use your models to understand it. It's a way of, it's a way of, language is a way of sharing between entities this kind of information. And this information can be associated with words and so on. It's complicated, but the framework is there for doing this. and, language is not some part of the neocortex. You have to take 20 steps to get to. It's no, everyone's looking at the same stuff most of the time, and, and you can say, I'm going to express what I'm seeing or not. I'm going to express what I'm thinking or not. I'm going to express what I've memorized or not. So it gives you the, again, the foundation for language. Then finally, I think the AI Bus is going to give us a foundation for robotics. And why do I say that? I don't know much about robots, I'll be honest with you. I know very little about it. But, I know that today's robots are pretty bad, and you know that. they just don't, they're not fluid, they can't do things very well. And, but it struck me as well, look, cortical columns all have a sense of location in their space. They all have a motor output, which I didn't draw here. That is, they can move themselves. And if you think about moving the sensor, but moving the sensor could be moving your head or eyes, but it could also be moving your hand. All robotic movements are basically coming out of cortical columns. And the question is, how do they coordinate? The problem with robotics is how do all these cortical columns, where each one can just know a little bit about its own self, how how do they coordinate to do something unified? And I don't know the answer to that question. But I just said, I understand now how they could coordinate and unify their sensory experience. How they can all see something and know what it is together. And how they can coordinate. And somebody says, we need to attend to this, move your hand there, or you move your hand there, or something like that. And, and therefore, it's, robotics is going to be built on the same concept. to me, I'm saying, oh, I bet you if we start spending some time on this, we'll figure this out. Yeah, where before I would have no idea where to begin. So these are some of the basic capabilities that AI bus gives you. And I feel like it's completing the entire picture in some sense, not that we know all the details yet, we don't, but it's completing the picture of okay cortex is a bunch of cortical Columns, what do they do? Now how they work together. Oh, we have the basic understanding how they work together. It's all about locations. It's all about locations, common locations, and manipulating, understanding space together. and that is going to be the unifying concept of doing all of those things.

I'm not going to go further on that. but let's just keep going. Unless there's questions. Yeah. You mentioned motor output. Yeah, because that's something part of the bus or that stuff is separate. we know that every cortical column has an output. That's local. And of course, it has to know how to move itself locally, but they have to coordinate, right? the same sort of translation between local knowledge and shared knowledge That would be that translation It's the same translation for robotics. I don't know how it's going to play out yet. I don't even know what, how to think about it yet, but it's going to happen. It's going to be like, globally we're trying to do something. Push the button. But locally, who can push the button? who is close enough to push the button? And maybe I want to say, use your left hand. Or I say, use your right hand. I can do that, and then this guy will translate into his local signal. I know where I am relative to the button. Therefore, I'm going to push the button. Where up here, we're just saying, somebody needs to push the button. on the arrow, there will be another rectangle that says motor, motor command. Yeah, I don't know. I don't know yet. I'm not going to go there. I don't know yet. Just, I don't want to pretend I can figure that out right now. I know, I just saw the arrow. Okay, alright, yeah. but, you realize sensor fusion, remember, even the sensor fusion involves movement. Everyone has to attend. That's a form of movement. Whether you move your finger, or you turn your eyes, or you just, I don't know. Covertly attended something. That's a form of movement. So already sensor fusion involves movement. but it's not. But now we just need to extend it to movement that is designed to change the state of the world. or to do something. And so it's very much related. I've always thought that robotics and AI were going to be one and the same, because that's the way it is in the brain, and so now I'm beginning to see how that's going to play out.

yeah. Alright, that's the end of that one, unless someone has more questions on that one. You look like you're scratching your chin, Alright, I'm now going to switch to a something completely different. It's the birth of industries. I don't have any slides on this. Okay.

this doesn't applied to all industries, but it applies to some. And, and I just, I want you to at least understand what I'm thinking about this so you're not confused by it.

let's start with Amazon. Jeff Bezos saw the internet early on, and Amazon, I think, was founded in 1994. There were just barely some beginning web browsers. There was no protocols. It was really crude, right? And he said, I want to create, he saw an opportunity to create the largest marketplace in the world. That was his opportunity. I'm going to create the world's largest marketplace. Now, at that time, Web browsers are terrible, the protocols are terrible, and no one would want to put their credit card on them, into a computer, that would be just like, oh my god, who's going to do that? You two don't do none of this, trust me, that's what happened.

and, how was he going to do that? He chose to start with books. He wasn't a bibliophile, he didn't care about books. He wasn't, I love books, we're going to sell books. There were other online bookstores that were doing that. He says, no, I'm putting it in the world's largest marketplace. Books is a good place to start. Now, it didn't, it was a disconnect. He's talking to some people out there in the world's largest marketplace, but then he's like selling, books. And I'm sure there was a lot of, he was losing money hand over fist, and there was a lot of ragging on him on Wall Street. People were saying like, this guy's losing, he's crazy, he's never going to make any money, he's just spending all his money, why, how come he can't make a profit selling books? And it's because he didn't want to sell books. He wanted to create the world's biggest marketplace and he's using books as a way of building out infrastructure. So books for him were like, Oh, I have to have regular relationships. People have to write things for me. I have to figure out how to do rankings. I have to figure out how to do recommendations. I have to figure out how to do logistics and shipping. All the things I need to do, books are a good way to practice. And yet if you work for Jeff Bezos, you might think this is boring. He's look, what happens in the big marketplace? What happens in the world? Amazon is the world's largest river, and that's why he picked the name. He said, it's the world's largest river. Collects the most water anywhere in the world. That's us. We're the Amazon of, retail. That's literally why I picked the word Amazon. so he knew this, but he was doing something which looked pretty boring. And there were other books, vendors and people. And I think someone's a book vendor is better than Amazon. He said, I don't care. I'm not building a bookstore. It looks like I am, but I'm not. So that's a, that is a common property when you're doing really big things, you're often on the edge of what's feasible, and then you have to pick some subset. to get started. We had our own experience of that at Palm. Palm was formed to be the represent, literally the one line summary of Palm and its inception was, we're going to build the future of personal computing. That was it. I used that phrase over and over again. Future of personal computing. Everyone's going to have one of the things. There's going to be 10 times more, hands on computers sold than desktop computers. It's going to be the dominant personal computing platform. but we couldn't really build that initially. That was the vision. And it turns out that it wasn't possible. It was just too many things that were missing. So we ended up building an organizer. And I had a lot of employees that were pretty disappointed about this. some were, basically ready to quit, essentially. Because, an organizer, what? Address book and calendar? This is the future of personal computing, it's not even independent of the PC. You have to plug it in. It's an accessory. I said, no, trust me. We're really independent from computers. We can't do all that other stuff yet, but here's where we can get started today and just not forget that when I'm in the organizing business, that's not what we're doing. and I remember a colleague who was on the board, he was the VP of marketing, and we had a big debate about what to call the first product. And I wanted to call it a hands on computer, and he wanted to call it a connected organizer. He wanted to sell something, and he said, that's what it is. I said, no, yes, but Ed, it's not that. I don't want to mislead people. I want everyone to know we're building the future of computing. And he won. we called it the connected organizer. And, I don't think it matters. He's probably right. I want to go for the big thing. He says, don't worry about that. We need to sell something today. So we'll get there. And now I have a big debate with Donna about, should we, expand sales internationally? Should we put our engineers on doing international versions or doing the development kit? So independent developers could write apps for the organism, right? And what are they going to write for us? I don't know, they'll do games, they'll do something, they'll figure things out, but we have to go in that direction. So I think in the end we ended up doing both, but there was that kind of dynamics that we had to deal with, right? so I think we're in a similar situation here. I want to explain to you why I think this is such a huge business opportunity. Yet today, you have to get started at something that might seem a little dull or boring. and that's the point of that story. And I don't want anyone to get discouraged by that. I don't want anyone to think, oh, what happened to that big vision of, the future of AI? Even because we have to do something that doesn't look like that vision for now. Okay, that was the end of number six. let's go on to number seven. I'm going to propose sort of a roadmap, but this is the AI bus roadmap. It doesn't replace this roadmap. This roadmap is still here. but it's a different sort of roadmap. I feel like, okay, what would we do with the AI bus? The first thing to focus on is sensor fusion. That's the thing I think you can do today, right? and to do this, we have to start building something and test it. I said that we can define prototype bus protocols, and we've already started that. And the prototypes, we're going to change them, right? Like what is actually being sent here? What's representation? How do people talk to it and things like that? We don't know yet. We know what it is. we also need to build We need to build test systems to evaluate the protocols. Do they work? This is not a product. This is not Deep machine learning. This is like just seeing did we get this right? It's along the lines of what we did with Luiz created the the virtual robotic arm that touched And you know when we first, it's almost embarrassing. So Luiz started emulating the robotic arm. Literally, as soon as the robotic arm started touching something, it didn't work at all. I was like, what the hell is going on? And it was so obvious. I don't know if we missed it, but we missed it. which was that we didn't have any concept of orientation of the fingertips. So what it sensed would be different depending on its orientation. And we didn't have that. We just missed it completely. But we didn't discover that until Luiz made the robot, and we were like, oh, it doesn't work. so I don't know if it's gonna be like that, but we have to do some things like this, just to start testing out, oh, we forgot about this, or we didn't do that. And this required, and to do this, to evaluate the test system, like, all right, we're gonna have three modules, and we're talking to each other, something like that. We have to create test modules. We have to have something to plug onto the bus, something equivalent to these. Now, this is where I misled a lot of people. I said, in my mind, I'm thinking we're gonna build these test systems. And I have to have modules to plug into it to test the protocols. I don't really care how those modules are built. They can be hacked together, they can be kludged, doesn't really matter. And so we have this conversation in this room where I said, build it any way you want, as long as it works with protocol, I don't care. That was construed as we're abandoning the neuroscience. And that is not true. That was just what's required to test the protocols. This is all neuroscience. We're trying, this whole AI bus is neuroscience. It's derived from neuroscience, and I just misled people there. I apologize for that. and that's what I meant there. It's not that we're going to use the ultimate. These can be hard coded. I don't know what the right answer is. We do unit tests and so on. It doesn't really matter too much at the moment. We're trying to understand the bus and its protocols and how it works, and how we build the modules and plug onto it. We're not going to be adhered to any, strict, purity system here. And that's what I did there, and I apologize for creating that. We're going to probably have to iterate this a few times, just to experiment. Just like we did with the other systems we built. We iterated with the sequence memory, we iterated with the, the sensorimotor learning. this I think we should do. we should do it. There's no rush to do it, but we should do this because I think it's going to be really important, and we can start that today. On the robotic side, as I said, it's a natural extension of sensor fusion. It's just really, it's just an extension of it, but I don't really know enough, and I don't know if anyone here knows enough to do this, but this is mostly an education. Exercise. we need to educate ourselves about robotics. What are the core problems in robotics, current path approaches? I don't know any of this stuff. Maybe some of you do. I personally feel I need to get educated on this is I can't build anything here. It's like we're doing basic research. and, I think one of the things we're going to have to figure out to do robotics is we need a better understanding how the bus, how we represent state of objects in the, world is a stapler, open and closed. What's the state of your phone. We don't really know how to do that yet. So that's going to be required to that. So we can start thinking about this now, but there's nothing, there's no code to be built or anything like that, but it's something we can work on. ultimately, in the end, we're going to have to do, figure out how this is going to work with cognitive and abstract thinking. I don't want to think about that now. It's, it gets too far out. we'll get there, but right now we need to really work on these components here. Mostly, we can start implementing this and testing it, and this we just have to think about and start brainstorming.

so that's the, how I view the, what we need to do here, what we should do. in some sense that's filling this column over here on the original roadmap. It's a way of attacking the thousand brains theory.

number eight. We have options what to do next. We're not going to decide anything today, and, let me just go through some of the things. we are going to continue to work on sparsity on GPUs and CPUs. That's really important. It's going to also be important for the AI bus too, so and that has to continue on. and, and so we're doing that. It's great. on the dendrites and deep learning networks, we can continue working on deep learning networks if we have a PR or dollar benefit. It was really clear what we're going to get out of working around dendrites, for, deep learning networks. And we may decide that there is an opportunity there, but that's not where we want to be. We probably, we want to fold, eventually fold that into the, into the AI Bus opportunity. And give you an example, right at the moment, we don't know how the AI bus is going to work. My current vision is it's going to work probably like the brain, but that requires sparsity and dendrites. So maybe we're going to use that in the bus, right? Maybe our bus will be based on principles of sparsity. And then maybe we'll figure out another way of doing it. I don't know, but at the moment, that's my assumption. And so it's not like the work we've done here is, it's got a near term benefit, but it has a long term benefit. And so that was always the plan. it was always a plan to test these things here, but bring them over here. So let's continue on with that. there's less clear if there's a dollar or PR benefit for the current work than there is for, sparsity. that's why we're willing to invest more in this one. we had this, this question of, rather than phrase it over here, reference frames in, in, deep learning networks. Should we work on that? we didn't, we couldn't identify what to do there. So I said, we can pursue it if we can identify something to do that's useful in the deep learning networks.

is there some benefit PR or dollar benefit? if we could figure that out, we might want to work on it there. If not, we'll just fold it into the modules because all the models are based on reference frames and the whole thing we got here is translated from one reference frame to another. So the whole system is built on reference frames, so Even if we don't pursue that in deep learning networks or deep neural network, we're going to still use it. it's just the question is we couldn't figure out what to do with it in DNNs. and then, and then of course, we need to, we need, now we're saying we're going to do this AI bus research, which is just this stuff here. So we're going to, we're going to add that to our list of, options for what to do next. I'm proposing we add that to our list of options.