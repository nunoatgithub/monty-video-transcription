Yeah. Thanks everyone for, Coming. so today I'm going to be talking about, transformer networks, and how they relate to Monty, what kind of, similarities there might be between them, and, kind of key differences. and yeah, obviously, ask any questions as we're going along, as always, and I guess, yeah. What's the aim of this to, see if we can help? Yeah, so there's a couple of aims. So the first aim is. identify possible cross pollination between these, That's right. what can we do to, maybe improve transformer networks while we continue to create the true Monty system? as well as, maybe there's some, ideas from transformers that, would be useful for, Monty. In general, in the discussion, I'm going to focus on, the current implementation of Monty as it stands, rather than, the thousand brains theory as it, generally, exists. and the, main reason for that is it just makes it easier to talk about, concrete details and, draw, more, yeah, explicit parallels.

and then just in general, yeah, we identified at the last by the bay that we want to improve communication between Monty and Mega team and just general understanding of what we're each working on and get influence from each other. And if, Monty is a big, epic castle on a mountain, Mega is this transformer. And we want this kind of, I don't know, cyberpunk castle on a mountain. All right, that's a stretch.

And then, Did you use thermal diffusion to change that picture? It was a Dall-e 2.

and then I think it'll also just be interesting to maybe contextualize some of the recent games and, hype around transformers, if they have just, through their, design incorporated some concepts from, the thousand brains, then, maybe that explains why, they have been able to achieve certain things, but of course are still, limited. and, yeah, in terms of limitations, obviously, there's many, but, just the sheer amount of data that these things require, all this, it might be possible to understand, a bit better. And, on this talk, topic as well is just recent, work in this area. transformers being used as embodied systems is becoming, an active area of research, and so I'll be talking about a couple papers. in that area.

so just a quick preview, so these are like some of the similarities that I'm gonna touch upon. It's important to emphasize that similarity does not mean equivalence. I'm not saying that these things are identical, but that these are just some, kind of things where you can definitely see, relationships between the two. yeah, the connectivity, that exists, this kind of concept of, a common representational format.. how kind of voting operates, and how that relates to self attention, which will be the thing I spend the most time on. kind of reference frames and how that might relate to positional encodings and transformers, and then, yeah, embodiment, which as mentioned is now a topic of research in, with transformers. but obviously there are many differences, and, Yeah, although this list isn't exhaustive, the connectivity can be similar, but it's also different in many ways. of course, the kind of, whether there's explicit, object models, is a huge one, and how learning is taking place is another really big one. and then, yeah, when I talk about embodiment, you'll see some of the approaches that are currently being adopted with transformers are just a bit Kind of curious, if you compare it to the thousand brains mentality.

before I get into it, so I guess, I'm, hoping everyone's fairly familiar with Monty and fairly familiar with transformers, but I will just cover them a bit. Obviously, if, either of them are more new to you, then, stop me and we can go through the, in more detail. But yeah, just to emphasize that, with the kind of learning modules in Monty at the moment, we're getting these, features, with a pose, to update the kind of evidence for these, explicit, kind of graph like models, and in order to process that, we have kind of a buffer of, of where we were before, and given kind of our new, position, how we move through space, that gives us a displacement that tells us where we're moving. Each hypothesis along this kind of space, and these hypotheses are based on our kind of memory of these graphs. These hypotheses can also be used to vote with other learning modules. And then we can output essentially a pose of, where we, the kind of object, and, the pose of that object, as well as, potentially an action.

and then a transformer, is, composed of these, kind of key blocks, repeated throughout, and really the kind of key thing that makes transformers, what they are is, this self attention operation. And so I'll, I will go into a bit of detail about how this works. just so everyone's on the same page, but I'll do that later when I talk about, voting. But the kind of general, thing to, be aware of is, yeah, these were created in the context of natural language processing, so where you'd have a series of token representation, so for example, words in a sentence, these are processed in parallel, by the self attention mechanism, which is essentially comparing all of these words, and how they relate to one another, that gives you this kind of new output, which, will be done for kind of many different sets of weights. And then this kind of feedforward system is just a way of combining that information. and then you essentially just repeat that. But you get this kind of, stream where you're just, any given token is passing through, in this kind of given stream on its own. But then whenever there's a self attention operation, it's, that's where it's looking at the information around it and combining that information. So this is very different from kind of the typical fan in and CNNs and so forth.

And then it's also just important to note that with transformers at the input you have this positional encoding, which, because, yeah, otherwise the sequence, there's no information in the kind of where these items are relative to one another. They're going to be processed in parallel all at the same time, so you need to give some sort of positional information. and for example, in the original paper, this was using, some kind of sinusoidal, and cosine, functions, and was literally added to, the vector of the, feature representation, to capture that. How would encoding be encoded in a sinusoidal or a cosine? I don't understand. essentially, it's you can imagine, it's a little like grid cells, to be honest, where you imagine you have a bunch of, sine functions of different frequency, And so let's say you're the fourth element of, the sentence, then, you are going to plug in four into these different sinusoidal functions. Those will give different amplitudes, depending on their kind of frequency. And, it's those values that are, form a vector, and that's what's, what's used. So is it, does that vector actually represent a position in the, in a sequence of words? Yeah, so it, it represents a, an absolute position. Yeah. So the sinusoidal line Or you can, infer, a bit like how from, a set of grid cell modules, you can infer from these, functions where your position is. Yeah. Okay. And the reason they chose that actually was because, yeah, it can, that works for, sequences of arbitrary length. And it sounds like nearby positions will also have slightly similar encodings.

Yeah, exactly. I guess it would be interesting sometime to understand that mechanism, more explicitly because you say they're, it's a little bit like grid cells are adding sinusoids, but those sinusoids and grid cells are representing vectors of movement, and, and you're trying to update a static, a representational location. I just don't know how parallel that is.

I don't know if we can get into it now, but it'd be interesting. Yeah, I feel like it's, from my understanding of it, of how it's implemented, at least, I think it's pretty similar. I think, the key thing is it's one dimensional, and this is like a common theme throughout transformers that, I don't know if it's because they originated in natural language processing, but even when, 2D, images are processed. They tend to just treat it as a raster, and use one dimensional, embeddings rather than, 2D. I don't want to get on topic here too much, but this might be a very important aspect of the whole idea you're presenting today is, it's one dimensional. But, there's different ways, but it's one dimensional, it's not clear if they're representing, is it just an ordinal, order, like first, second, third, and fourth, or are these, are sort of temporal aspects to this? I don't think it would be, because they're not processing this in time. No, it's just a position. So think of this as Let's say four different 1D modules with different frequencies, and then together you're representing a position in 1D. That's basically it. Where do those frequencies come from? It's just a parameter. Yeah, it's just a parameter. Who determined that parameter? Hyperparameter. I don't understand it. It's okay. I don't understand it. It's the same with grid cells. How do you determine the spacing of grid cells? It's just. That's spacing. But, grid cells, I can, say are representing, a position in space. Same here, which is updated. and sequence, I think about sequences that's not, you can say one dimension, but you can have a one dimension order, and you can have a one dimension or true one dimension where the, words are like some. spatial distance, like notes in a melody, there's some gap in between. They're not just a sequence, right? It's not just an order, it's an order plus time. Yeah, so it's up to you, there's no reason you couldn't use this to, here it's sentences, so yeah, so the, it's ordinal in, so I guess it's ordinal out, but it's a continuous mapping, it's a continuous output, so if you had a continuous input, you could, you could do that. Yeah, but you could use this for 2D images too, you could have a 2D sinusoidal thing, which would be much more like what we think of as grid cells. Yeah. But, and I guess one thing with grid cells, I think you can't remove them from, or think of them separately as a movement. There's no movement. I know, so I'm just trying to where the, Yeah, the, when, maybe when I get to the embodied stuff, that'll make sense, but, yeah, one, and I'll talk a bit more about like the different flavors of positional encodings, but just to highlight, the most common approach is to literally add the positional encoding. which is interesting because, yeah, I think that's very different from the kind of way we would think about it in Monty because, then you're really entangling this positional representation with, the feature representation rather than it being explicit and on its own. and I don't think that's done for any kind of, like, it is possible to concatenate them instead, but it's just, in order to keep, computations, low. They basically do that.

okay. So a bit of background then, done. So then onto kind of the first thing, which is just this architecture, kind of connectivity. And what I'm showing here first is Monty and the kind of general architecture of it. and I'm trying to emphasize in each one of these units, the kind of, the representations that are being, computed and, handled. Okay. Which is why I've broken it up into this kind of pose and ID representation, because it'll just make it a bit clearer kind of the similarities. But yeah, in Monty you have multiple learning modules, potentially with this kind of hierarchical relationship. I'm not showing all connections here, but as we've currently implemented it, You get this, the sensory modules coming in. And just one important thing to highlight, as well as the kind of voting, these lateral connections, the, you have a one to one connection, going from a lower level to, a layer above. At the moment, we don't have any kind of fan in connections, and it's something we've discussed, but, in general, the kind of view at the moment is, All of the kind of interactions between, across a layer of processing are primarily going to be handled by the, voting. And then you have the Oh yeah. A small detail. I think, one thing we do allow is to have multiple lower level modules fan into a high level module if they are within that high level module's receptive field. So if Oh yeah, that's true. Yeah. Higher module has a larger receptive field and all the lower ones have like small parts of that receptive field. they can all input features into the higher one, but the higher one can't distinguish their locations from each other, because, Yeah, so it's It's, yeah, I guess if this was broken down into three submodules, yeah, thanks, yeah, but then the kind of in, the location information is still this one to one, mapping, and yeah, on that, then there's this kind of almost skip connection where, as well as getting the kind of, learning module one gets this direct input from a sensory module, learning module four gets, direct input from a sensor module, not necessarily sensor module one, but of, the same location, space.

and then a transformer, if you ignore that feedforward layer I was, in, just, it, can still fit into this, but, just put that to the side for the moment, you can imagine, potentially each token as a column where, these tokens kind of communicate laterally with the self attention mechanism. And as mentioned, whenever a token is being processed, other than self attention, it's. Predominantly in this kind of like single feedforward, single track highway, rather than again having a massive kind of fan in operation. And, and yeah, and we have skip connections here as well. and then similarly the spatial representation is being injected at the start. where the kind of sensory input, quote unquote, may be. What does pose mean in this case? Just the positional encoding, I think you're Just the, yeah, the positional encoding. It's not, the same as pose where you're thinking in Monty's, right? It could, it's not in the sense that, yeah, there's no explicit graph or anything like that. it, like these will, these kind of embeddings will, learn what they learn. and, yeah, I think there's reasonable evidence that they can learn to represent, positions in space and things like that. But yeah, the, only kind of explicit, spatial input is, here at the start. And as mentioned, even that, it's not that explicit because it's normally just added on top of the representation that's, for the feature.

you have the lateral connections being self attention? Yeah.

it's not a voting type of thing, right? Is it, isn't the lateral connections more just deciding, where your attentional heads are at that point in time or something like that? Yeah, I'll, I'll, get more into attention later, but basically voting. can be considered a special case of self attention, assuming everything's set up appropriately. But they also do an n squared thing, right? Every token is Yeah, but voting, voting in Monty does the same, but I guess the key difference is, there's not learned weights that are doing that. It's, just based on the, relative displacement. It's, and it's just a question of, yeah, how, much of the tokens or the columns around you, you want to compare to? Yeah. You say those aren't learned? The attention mechanism? No, they are learned. No, that, that's the thing. The attention, Monty uses the graphs that are internally learned within each column. Yeah. Whereas, because transformers don't have this explicit graph representation, it's implicit in the weights and how it's transforming between, one token and another.

yeah, I'll, self attention is definitely a weird one, so I'll, I think that'll make more sense with the, The later visualizations, but, but yeah, just to ask one more, one more question to that point, so yeah, cool visualization and comparison, but yeah, when you put object representation into these purple boxes and compare it to a learning module for example. Does that mean in there it can represent entire objects like models, like things that cannot be seen in one input, or is it basically it can recognize the embedding input and then everything it learns are like in the connections.

yeah, checking if I understood, yeah, so it's, it's, literally just like a vector. So it's very, like it, it can represent like a, yeah, I guess whatever a vector could represent. So it can't, do a whole graph, but yeah, it could represent an object and I guess some fuzzy features about it. in terms of if I understand what you're asking, How it would do object recognition or whatever. Do you mean is it going to, do something that's not bag of features to recognize how parts compose it and things like that?

Yeah. one of the principles of Monty is that each of the columns can learn complete models of objects while the input to a column never sees the entire object at one time. It can still recognize it. By moving over it over time. yeah. Would this kind of transformer architecture be able to do that as well, or it can only recognize the object in conjunction with, through the weight matrices that connect it? Yeah, so no, it has to use, it has to use these lateral connections. So this is the kind of, one of the funky things about transformers, and I think one of the key differences is, there's no self recurrence in each one of these tokens. and so there's no way for it to update its representation other than by these lateral inputs. So it's almost like if you if you were like, we're not going to have any kind of sprawl world type recognition, where you're building up models over movement. We're just going to focus on, voting, and voting's going to do everything. It's changing a bit, though.

the naive transformer stuff, there's no, it doesn't maintain any state. So when the next input comes in, It's completely fresh, so there's no question, there's no concept of successive inputs. Yeah. instead the idea is you just flatten everything, you just flatten time into a single vector that's in a feed forward way, but that's not the case with the GPT architectures and these so called autoregressive transformers, you get, you're successively feeding the input in and actually maintaining the previous state. Yeah, so concatenating stuff, so yeah. So you're talking about yeah. So there's, self recurrence in that kind of sense. but it's a weird kind of self recurrence. It's weird because it's if the brain kept expand, if, you assume that these are cortical columns, it's like you deal with sequences and time by just adding cortical columns or recruiting more cortical columns through time. to deal with, new inputs, rather than, yeah, saying, okay, each one of these tokens is going to, try and represent an object and integrate and, maintain a stable representation. Niels, when we're dealing with long sequences, I don't think that's quite true, because basically, you're concatenating what it, evaluated previously with the current one, and then processing them, and then I think there is a form of recurrence that goes on in that sense. Do you mean like transformer XL? Or like perceiver? Okay, yeah, I would need to look at that. but definitely transformer XL, it was again, I mean it was, that's the key thing is that they always concatenate. I think, which is very different from, they didn't, at least when I looked at the Onyx transformer, which basically can take many forms, on the ops. Yeah. there was the, a place there where it, could add, but there was only a certain amount it could add. In other words, I don't know, me double what the normal one was, but then yeah. that would be. Somehow saved on out and then re represented on the next cycle. I think, so I think it was a finite window that was moving across. Okay. Yeah, that sounds interesting. I'll definitely, yeah, I'll check out, Perceiver And, double check that, because, yeah, the, just to quickly show, this is the, yeah, the transformer Excel kind of passes in these representations here from the previous step, this is the kind of fixed, window, but when it calculates self attention, it concatenates this representation here, but that's, yeah, it's concatenated, but, but okay, but I'll, yeah, I'll definitely check out, Perceiver. That'd be interesting if they do that. And that's one thing that, yeah, I'm sure someone has tried something like that. Because it's essentially just saying each one of these is an RNN as well. that's about the only way they can get past the N squared thing is Perceiver has A limited amount of N squared and then tracks across the thing so they can put in these much larger, token sequences and actually derive something that has more scope to it, but just not all at once. It's not self attention all the time. It's building up. I could be wrong about how much they're retaining, in that particular The, cross attention. cross attention usually comes in on the, on the decoder, side, But yeah, you're right. In a way, it's you're right. I think they use cross attention at the input to basically say, 10k tokens is going to be like cross attention or something like that, and then, yeah, okay, cool, but, But in any case, even, if they don't specifically do it, you could see how you could do something like that, right? As long as you have some, state built on up that you feed in on the next cycle and become part of the, the, input if you wish.

yeah, okay, yeah, but unless, yeah, there was any more kind of questions on that, I was just going to then focus on just briefly some of the kind of, differences that immediately jump out is, yeah, firstly, the, what I was already saying, and which will be broken down into more detail about how these kinds of, the lateral operation that's going on is very much dependent on learned transformations, learned weight sets in, in transformers. How would you think about attention heads here? Would that be, considered a model? so if you have 10 attention heads, each column here can have 10 models? Yeah, I've, I was wondering this.

I think the reality because of, how, I guess neural networks can operate in such kind of, weird spaces. I'm, sure they can do many more than 10 models. but yeah, but I think there would be a relationship between the number of transformer heads and like the number of models that can be learned. attention heads, but I'm not sure it's nearly a one, or clearly like a one to one mapping. I think what happened, at least in some of them, the, each head, The various levels learn some aspect, noun, verb, syntax or something like that. And then by whatever mechanism, the other heads learn other parts of it. it's just how it kind of trains, it differentiates during training, time so that they, there seems to be some functionality on each head. So I'm not sure it's a complete model. It could be more akin to a feature. And that those features are then being, when they, when it goes from the attention head up to the fully connected layer, then they all get scrambled together and then produces new inputs and then goes to the next level and then it tries to discern stuff from that.

So the actual structure of what happens in that fully connected layer and how it consolidates information to be fed in as input to the next one is something that I've not seen people talk about a lot, but you can imagine how somehow you're managing to create these streams of capabilities that at the very end, if you're classifying, there's no real model. it's, a collection of features at the very end, I'm not sure how, whether imputing a model to the heads actually is something that they embody, at least in the current incarnation. Yeah, I think the closest thing to a model is and yeah, I'll talk about this when talking about, kind of reference frames is yeah, a bit like how capsule networks they don't have an explicit reference frame about, this is where you're going to, we're going to represent this feature relative to this feature. It's more the, weight transformations that are learned. learn, okay, I've got a nose, this is where I expect the face, I've got an ear, this is where I expect the face, whatever, it's more, like that with the, learned, yeah, attention heads could be capturing, or I guess, like you say, also maybe the, feedforward layer. but yeah, but there's certainly no kind of explicit, spatial map that things are being laid down to. Okay. and along with that reference frame, there seems to be, maybe I'm missing it, maybe I'm going to get to it later, but the whole idea of removing your sensors under volitional control of the brain through space is, a huge part of this, right? We're not just something not being passed to us. We are actively exploring space and moving in different directions and processing motion, processing movement, keeping track of where you, which requires reference frames to do that. so it's more than just reference frames. I think it seems like we're missing this whole sensorimotor aspect here. For sure. Okay, I just want to, it's a, that's a huge part. It comes with reference frames, but yeah, no, for sure. that is very much missing.

can these work across modalities too? I will get to that, I think. Yeah, on the next slide. Okay. but yeah, anyways, no top down influence, no explicit reference frame, no self reference. Those are just some of the differences, but just highlighting at the architectural level, the kind of the connection level. Okay, so yeah, common feature representations. To answer your question in a couple words, yes, they do. so of course, common feature representations across modalities, this is important for enabling multimodal, models. and in the, recent work that's trying to move beyond just language into these kind of multimodal, large multimodal models, LMMs, they do exactly this. and the basic idea is pretty straightforward. You just take whatever it is you're trying to represent, that could be a pixel patch, that could be a, word token, whatever. And you just do some kind of learned projection of that representation into a fixed size vector embedding. And so as long as those kind of vector embeddings are of, they're all 64 dimensions, say, then you can essentially treat them all the same in your transformer and assume that it's going to learn to deal with the fact that one of them is, An embedding of a pixel patch, and one of them is an embedding of a, of a English word.

and this is just an example showing that in practice from, so Palmy, is an embedded, embed embodied, embedded, embodied, sorry, multimodal, language model, from Google. that, came out pretty recently. this was actually one of the main, papers that the, the DLCT, presentation was on a couple of weeks ago about scaling robotics using, large language models. And yeah, basically they, take the kind of query, they turn it into these tokens, they take the image, they turn it into these tokens, and then all of that can be passed into this, model, and used together.

but yeah, you'll, see the, how the kind of the, embodied part of it is a bit odd, but I'll come back to that. I just wanted to start with, Those two comparisons just to ground the rest of the discussion, and then, yeah. It seems like a fairly limited way of doing multimodal integration, but I, can't say that for certain because I have to study it more, but the way I think about the brain is much more, it's much more flexible than this. but maybe I just don't understand this yet, is it that different from There's a lot of things here, there's a lot of things I think we'd be careful saying, oh look, it does this too, oh look, it has objects, it has poses, when you get to the details, they're really quite different in some ways, and so we have to be careful not to, just be careful with that language. but is, is that, that different from taking, a sound wave and, a few kind of Rod Cone responses or whatever, and encoding that as an SDR. Yeah, but we don't do that. We never do that.

We only we That would be a terrible way of doing it. the way we, the way the brain converges is purely through voting. And they're trying to, they're trying to correlate objects with objects. And it's not a one to one correlation. A sound doesn't have to be correlated with a single object. And a single object can correlate with a single sound. It's a much more, dynamic and method. You mean with the, spatial pooler, or? No, I, we have never done this. I'm, just saying how we've described it and how we've written about it. it's all done through voting. It's not, there's nothing done in the spatial pooler. There's no, common, it's just voting. It's one set of auditory models or a set of tactile models can inform a set of visual models. but it's not one to one, and it's not always appropriate. I'll just leave it at that, is that I, I look at this and say, oh, this is cool, but I, it's, to me, it's not the same as I envision multimodal integration frames. I just want to point that out, that at some level you can say, oh yeah, there's objects in these modules, and there's poses, but they're really not the same. Yeah. Just really, we can use the same words for them. Yeah. Yeah. Yeah. So I guess we can be agnostic how, like this whole, embedding operation, yeah, we're not certain exactly what we would do, but I guess the main thing then is once, these are input, then throughout this stream, at each level of processing, I you're you started with a visual feature here, you started with a language feature here, whatever. You're going to continue going up those paths, and they're going to continue interacting laterally. I'm just highlighting that because that is similar to but it seems like we've told them that these things are one and the same. and that's not the way it is in the brain. They're not one and the same. There are different models and different modalities which can, under, at different times, help each other, but they're not, we're not trying to figure out what's in this picture and they'll correspond the language with the picture. It's, that's not what's going on in the brain. Yeah. And that sounds what they're doing here. They're saying, hey, can we just, can we use these two modalities to, combine them in a single representation? I don't think that's happening in the brain. and there's a real advantage as I'm not doing it, but this could, this solves a certain set of problems if you want to have, described images and if you want to take words and make them, it is a fine, do a good job of that. Okay, I just, again, I just want to just, I'm just throwing it out, we just shouldn't accept these words as meaning the same thing. Yeah. No, thanks.

okay. Yeah. voting versus self attention.

so I'll just yeah, briefly go through voting, in Monty and self attention in transformers, and then try and get a bit more into the concrete details of how they're potentially similar.

Monty, we, let's say have, two fingers that were touching this mug. We're going to get two different features at poses, and these are going to be processed by different learning modules, which are going to develop different hypotheses for where we might be on that object.

And we, have this information about the kind of displacement, including the, rotation between, these two sensor modules. and this is key, to, the kind of voting process where we essentially take the hypotheses, coming out of a particular learning module. And depending on which learning module that information is going to, transform it by the appropriate, displacement. And, this is going to give us a new set of points for, where we would expect to be, on that object based on, where we are on, in this kind of hypothesis space. And that's what all these kind of clouds exploding out. are doing, where there's only a small set of points, related to where we were before, that with this displacement are going to move to this point on the handle. And that point on the handle, if you look at your nearest neighbor, you're essentially going to add the evidence that's coming in, which may well be negative, that's associated with your neighbor. and you'll get these kind of hotspots forming, essentially where these are consistent with one another.

self attention in transformers, is basically a way to figure out, representations, figure out what, other representations are Looking for them, and related to them, so it can bring that information together and condense it. when I first talk about this, it won't seem to have anything to do with voting, just to be clear. But, that's why when I talk about this as a special case, hopefully the, parallel will be more obvious. And, when you do this self attention operation, you have, three important things. You have a query, you have a key, and you have a value. And for every token, and in this case, we're going to say every word in this sentence is going to be represented by a token embedding. For every token embedding, you will, have a key, query, and, value. and so if we just focus on this word, it, for now, so this is the word that we're processing with self attention, so it's going to get, it's going to project a query, it's going to basically be asking, okay, what other representations are there around me that might be related, that might be relevant? And in kind of answering that call, every other token is going to produce, a key. And these, so the reason for these arrows is these are, vectors. and so you can just think of them as a point, somewhere in space. And, to determine how much agreement, or like how appropriate the two are for each other, you literally are just doing the dot product. and then when you do the dot product, so yeah, so the query dot with this key is going to be a larger value. and then this is just passed through a softmax, and this is what gives the, attention, mask, the, self attention, layer.

and so here, the, value, the, this kind of, probability associated to the, this, the animal, say, is higher. And this kind of fits with the idea that, It is, okay, it's, it knows it needs to look for something like, a noun or an object or something that it could be referring to, and, animal in this sense has learned that it needs to look for words like it that might be referring to it. But what we want to actually But the word street, you could make the same argument for the word street. So how does it distinguish between those? Yeah, this, yeah, this is, a toy example. But just in general, like, all right, that's the trick, right? Is it referring to street or is it referring to animal? I guess another thing is, This, voting operation, it's not just based on, or it's based on this embedding, and this embedding is both based on the kind of feature, whatever that is, the object kind of type representation, as well as its position encoding. and so maybe it has learned, that, immediately before it, so early before it, it's not going to be finding anything, it follows usually later, in the sequence relative to the word that it's referring to. that's funny because, I could make the same, I could say the animal didn't cross the street because it was too wide, and in that case it would refer to the street. so it can't be that simple.

Yeah, it's impressive. It does. I just don't understand how it does it. yeah, I'm not accepting a simple explanation because it leaves me still wondering how it does it. Yeah. Yeah, what were you going to say, Vivien?

Yeah, basically the same thing. If you substitute the last word for too wide or too dangerous, it just depends on the last word, what it refers to. Yeah. No, it's a good point. And that's why this kind of, I guess one to one comparison is a bit odd that it's doing and how it integrates this, but it may be a hierarchy thing. I think this is literally extracted from an actual network. I don't think someone made these values up.

I don't know if, it's here, they've said that the embedding is for the word it, but maybe actually the embedding is for the word it after it's been processed by five layers. And by that point, it's understood. Yeah. You can look at evolution from layers. I, have that same library and I, to look at from layer 1 to 12, it changes a lot. So probably on the first layer, it was referring to street, but then at the second one, it already encompasses the entire, the spousal. That information is helping decide whether it refers to frame or it refers to script. And you're one of layer 5, so I'll write mine to 5 layers, but you can, clearly see the evolution within layer. Okay, that helps.

so yeah, so what are we going to do with this? Oh yeah. I want to make sure I'm just clear. The token for the word it, there's nothing else besides just the token, right? There's no, meaning to that thing other than its associations with other words. Is that correct? I have, I, when I say the word street, I have all kinds of associations with the word street. I can, and I can visualize it, I can think about it. Particular streets, I can imagine touching it or crawling across it, whatever, I get, I have a lot of, it's not just a, it's not just a word. That's just a token. And so I've always had the impression that in, these large language models, the tokens are in some sense meaningless. The only meaning comes from what they're associated with other tokens. So it's a token referring to a token, but there's no grounding what these tokens mean. I think there's a learned representation for each word. So street and road might have very similar. But there's still, it's just all between tokens. There's. There is nothing else besides, it's just all correspondence with other tokens. All the definition of what a token is, how it relates to other tokens. And nothing else besides that. Yeah. Okay. But that's true for every aspect of this. It's just, I know, I just, huge, not even multimodal ones. So have an association of three and I image of three. But even those who just tokens, right? They're, I make the same argument, the tokens and the image processing. it would be associating, it would see what sort of images are associated with what sort of words in a stream purpose. Yeah. yeah, would the brain have something similar, just with neural activity at a certain I think, I think the brain, I think language is, comes after it, right? It doesn't start on you. And we start with a model of the world, and dogs have it, cats have it, and birds have it, and we have it. And then some animals like us have the ability to express that model through tokens like, but it goes in that direction. It goes from a model of the world to a communication token. We're here, we start with the communication tokens, and there's no model of the world other than the relationship of the communication tokens to other communication tokens. And, there's no tie to specific movements or specific behaviors or anything like that. It's just tokens to tokens. That's how it works really well, but that's to make sure I understood it correctly. Yeah, no, I think, that is fair to say. And yeah, exactly. there's no explicit models. There's no kind of sensorimotor really understanding of them. What if you learn a model in a sensorimotor environment, and then you associate it with language? that's what we do. but you could do that here too. if you have that sensorimotor model, yes, but there is no sensorimotor model here. some models do, yeah.

yeah. Let's get to that.

It's very hard to understand sensorimotor models without reference frames, so I need to understand that better.

Okay. But just, yeah. Oh, yeah.

Yeah. Sorry. Go ahead, Vivien.

this might be more of a side question, but, is it predetermined where a token starts and where it ends? Or can it also learn that on its own? for example, tired split into two tokens. Yeah. So that's, yeah. So there's, when you are starting with like sentences, like natural language sentences, you have two steps. You have tokenization and then the token embeddings. The token embeddings is, what transforms it into a vector of a, fixed length or whatever. Tokenization is how you decide to, yeah, basically parse sentences into, these kind of components. And there's a variety of different ways you could do it. You could do it at a character level. but then your sequences are going to get super long. And so that's really computationally inefficient for transformers. Or you could do it at a word level, but then the kind of vocabulary, that you're dealing with, becomes enormous. So in practice, most of them do sub word, tokens, so they split up, I think, almost syllables. And that's what's used, I think, in all the kind of recent GPT models. Is that true? syllables? This is showing a couple, that didn't work entirely. This is verbs that, you know, or concatenation. This is definitely, I think, a simplified one. Do they really work at the level of syllables? Yeah, normally, Lucas or someone else can correct me if I'm wrong, but I'm pretty sure that's, almost all of them. it's actually, they start with single characters, but then there is a fixed vocabulary, so I can only have 50, 000 tokens. So then they aggregate. The least common ones. So you're going to end up with some tokens which are long, which are like a full word, but they rarely appear, but some ones would just be like one or two characters. Interesting. So they say we got 50, 000 tokens, here's a whole bunch of corpus of text, let's figure out some efficient way of picking out 50, 000 tokens that covers all the text. It's like a clustering operation. Yeah, it's really weird because that's really interesting, but it's also weird.

I can see why that would be good. I can see why it would work, but it's just all very statistical. so the tokens themselves, may surprise you when you look at them. Yeah, it's an interesting way of, tokenizing language. But once again, it seems like it might move you away from any sort of, what we might think about, meaning of these tokens. I have some strange little letter sequence that has no meaning on its own, but it's useful in the context of building models like this.

again, it proves that you're really going towards a very statistical approach to this whole problem. You're just taking it that way. Okay, that's really interesting. I didn't know that. That's fascinating. so I have a question which was brought up earlier, this is at layer five, so by this time, whatever was the token animal, is now some conglomeration of something else.

the fact that they're pointing backwards through the discrete beginning token, I think is what's taking Jeff off track on what's actually possible. That's a fair point. Yeah. No, it is a bit misleading. I, I don't know, I didn't follow that, Kevin, but I think I understand this. I think it's important you should understand that this is not a direct mapping. This is an inferential mapping. By layer five, you won't see the token bee or animal or anything like that. You'll see it's gone through five layers of transformer that have been processed and there's A more abstract representation of all this stuff. So you can go backwards and look at what the receptive field was. But, What you're actually seeing there when it's saying it's, these things all, how they're related to it is a little, is more abstract, you've already probably parsed out whether it's a noun and a verb and a bunch of other things. So I just want to make sure that this, the, what they're trying to convey here is a concretization of an abstraction which is, much richer. Yeah, they're putting words on it so we can make sense of it, but it's not like this. yeah, it's that literal.

Great, yeah, so just the kind of final key element of the self attention operation then is, so we, found out first, with the queries and the key, the query and the keys, how, how much this, token, should attend to these other ones and integrate information from them. And in addition to, associated with each one of these tokens is this third representation, again, a vector, which is what's called its value. And this is meant to be capturing more, I guess it's semantic meaning, directly. And what we're then going to do when, updating the token for it, or whatever this is at this point. I'm sorry, what did you say? You said a semantic meaning? Yeah. Or, yeah, essentially what the kind of the representation that we want to pass on, onwards. So key and query are about This crosstalk, like how do you figure out who you should be talking to? Value is about how are you going to encode your own representation, then pass on to the next, level. All right, and how does that work?

to get from the kind of current, token representation, I'll show that a bit more on this next, slide. You, just, yeah, do a matrix, multiplication with a learned, Kind of set of weights to get that value, but the kind of important thing about self attention is it's going to take all of the values, and essentially combine them based on these attention, values here. So essentially the larger the, probability from here, the more influence that value is going to have on the updated representation. But where did the value come from? How does it determine? It, the network learns what a good, kind of value representation to use is. and it learns that through what mechanism? Backpropagation. The backpropagation. Okay. Yeah, all of this, the key and the query transformations are also learned through backpropagation.

it's a, parallel operation. they're, the token comes in, the vector representation comes in, and each of them go through their own separate matrices to produce these values. that gives you the ability to, any one stage. represent, produce a query key and value for any incoming token, so to speak.

Yeah. Okay, as soon as you tell me it's, learned by backprop, then I stop thinking of it as in terms of any kind of thing I can think about. Yeah, correct. Correct. Correct. It's an encoding scheme if you want to wish. It's basically saying how do these things relate to each other in some kind of statistical sense, and but there's a huge amount of context associated with that in when you do that, when you, because they, pull things apart into query key and value run through what they call multiple heads, which are kind of Representations of the self attention, then they all get mixed together in another layer, and then repeat, for multiple layers. So there's a huge amount of abstraction that you go through in this thing. That, through the magic of backpropagation, it learns how things are related to each other at multiple levels of representation. Yeah, but I agree with you that the words query, key, and value are actually more confusing than they're helpful. You just, if you just said this is a value, this is a vector that was determined by backprop, I'm like, okay, fine. The only thing that determines, one of the mathematical operations it does, to come up with an output. It's not really a query key. And it's not, you can't really say it's semantic value. I don't know what you'd call it. Everything's semantic. It's just something I've determined by the backprop out there. Yeah, but I guess, it will probably tend to, cluster similar representations. we can go look at it later and try to figure this out. Sure. And I guess determine something interesting about it. yeah, and let's say the animal or whatever, its value is some vector or whatever and then you have the adjective blue or whatever that's some other vector And through the self attention, it decides to prioritize those two and combine that. Then you can imagine that the new vector direction maybe is this combination of these two, concepts. or at least an interpolation between them. Obviously, it's an open question how useful that actually is, but that's the basic idea of, transformer self attention. Just to briefly show it again, just, in a more kind of, yeah, vectorized, matrix free, format, we have our embeddings here. these are the, kind of vectors coming in. this, yeah, as pointed out, this is layer five. they would actually be more abstract at this point than word embeddings. But, those are the ones coming in, and, we can, transform them by these, learned weight, matrices to get the query vectors, the key vectors, and the value vectors. And you see there's one for each, word or each token coming in rather. and this is just showing the same operation that basically we're gonna do the dot product between the query and the key. You do that for the, token itself as well. So it also asks. Should I attend to myself, basically, and you do it for, basically all of the other, tokens that you're going to compare to. Then you do the softmax, you get the probability, associated with that. And then the output of all of this is, yeah, I said the, scaled sum, of all of these, values based on their, softmax values.

and then the last thing I just wanted to say is on this is as Kevin mentioned, there's multiple heads involved in this. So what does that mean? So as in, you can learn one of these weight operations, but that's going to be reused at every point. So it's going to be, for every token, you're going to use the same kind of operation to do a query to generate the query vectors and so forth. But you could imagine you'd want to do, Different types of transformations to yeah, represent different things, different kind of spaces. And so that's what these multiple heads are doing. So they're all going to, each input token is going to get processed by all these different, attention heads, separately in parallel, and that's going to produce a bunch of these, different, zeds. and then it's that, feedforward layer you saw at the very kind of start, that's essentially just using a, a multi layer perceptron to, to combine those.

Okay, finally on to then, some comparisons of this. yeah, the kind of proposal is that self attention can do something like, voting as a special case with obviously some, significant caveats. And, yeah, again, just reminding the reason I'm, talking about all this is not to say that, oh, transformers have already solved Monty or something like that. I'm, covering this to, to try and highlight what are the things that we can maybe make transformers more Monty like. and, yeah, that's the main thing, and but if we can first understand what similarities there might be, I'm having trouble parsing this sentence. So it says, are you saying that transformers, the self intention, is doing something like voting, or that it could if we modified it? it can, assuming the right weights are learned. Okay, so maybe that's not happening now, but maybe it could be made to do something like voting, is that it? Yeah. Okay. Yeah, and there's a reasonable chance though that it is because the weights are basically the identity weights. So hopefully that'll become clear. I'm just showing this again just this sounds slightly It does sound conceptually different because in Monty, in voting, two models that have learned models of completely different modalities can still vote on like object ID and pose, but here it seems like the entire model across everything would have to be multimodal, like the whole weight matrix and everything.

the entire weight matrix would have to be multimodal. They're not separate models that vote, they're like one big model that has multimodal. Is that what you're saying? Yeah.

as long as the representations, have, yeah, look similar, there's no reason they I can't communicate, or yeah, maybe I'm not understanding what you're saying. Or I guess, in Monty, in voting, we never communicate any model details to each other. features are never communicated through voting. you don't know if the votes that come in come from a touch model or a vision model. But here it seems like it does explicitly tell you what it is sensing, like the, Embedding. Yeah. I think even in Monty, you could argue there's a certain degree of that in that, I don't know, a 3d model of a mug isn't going to vote with the abstract concept of a family tree or something like that. like there's always going to be a degree of, can things vote? but if we assume that the tokens are both like object level representations, like one is, like in a multimodal transformer, one's learned mugs through, what you call it, yeah, vision one's learned it through touch or whatever. Then, then those can vote at least at the object level representation.

Yeah, I, yeah, definitely to vote, you need to know model of the same object. but I just mean like the, in Monty, what is communicated through votes seems a bit more limited than what is communicated in self attention. But yeah, it might just be more of a Yeah, let, yeah, maybe bring that up again when, I've gotten to the, slide later. Because Yeah, yeah. And that's, maybe why I'm emphasizing special case, cause, self attention is doing a lot of things. and yeah, which I think, yeah, it relates to what you're saying, that it's, it can send a lot of information basically. but yeah, this is just bringing back this kind of columnar view of, kind of Monty with these, Yeah, voting, we're going to compute these online, online, we're going to compute these relative displacements using the sensor modules. Basically, for each graph, we're going to go through, and send the transformed poses using this relative displacement, find the nearest neighbors to the receiving po points on the object, the receiving poses, and then add this incoming evidence, based on your kind of nearest neighbor.

and then this is the kind of, transformer with this kind of column view, and this, by the way, this, might be a little confusing, but hearing this reminds me, there's really two types of voting. We've never, we actually haven't really teased them out before, We're saying, okay, we're like multiple fingers touching the mug at the same time, we need to know the relative position, they're in the same modality, which is different, but that's not going to occur if I'm going across audition and touch or something like that. where you're really, you're not relying on knowing the relative poses of the sensors. you're really doing purely object level voting. so I just want to point out that there was this voting, we have to, in a particular modality, touch or vision, This relative pose of the sensors is very important, but I don't think it's important across modalities. I think there's actually different, I've always argued that there's different levels of voting going on in a cortical column. There's multiple layers of cells and layers two, three, and I think they're doing slightly different things. I just want to point it out that here we're going to talk about voting, it's not going to go across modalities. It's just not exactly the same as what's happening in this image. this isn't a single modality, and we need to know, we need to know the features at, relative positions to return.

It's a, I hope that wasn't confusing, but it, they're not exactly the same. But yeah, Yeah, no, that's a good point. Yeah, okay, how could self attention be like voting? assume, that these, weight matrices we normally learn are all the identity matrix, slash, we just don't use them. the key query, and value beddings are going to be the same, they're just going to be the original, vector, kind of representation, that was, coming in. What this means then is the more closely those embeddings, so the X vectors, are already to each other, the more they're going to attend to each other. and so what that basically means is, the token here is going to attend to itself, and about equally it's going to attend to this token, and similarly vice versa, they're both going to ignore this token. And, so what you'll get as the output of this, the kind of value, is essentially just blending of these two similar ones, which, maybe might correct for a bit of, pose disagreement between them.

and so that's yeah, where, the similarity starts coming in. but you might reasonably wonder, okay, maybe this only works when they agree strongly with each other. what if there's legitimate disagreement about objects? So we've talked about this before that, if, if the pose is consistent, mutually consistent for two different columns. let's say, you have a mug and a phone next to each other. this phone isn't going to be telling the column processing this mug. You should be seeing a phone because with that relative displacement, then, you would, you would expect that you'd be on a different, object. And so I'm not claiming transformers are doing this because as mentioned, their kind of positional embeddings are a bit, are very fuzzy and not at all explicit. So But in theory, the kind of the actual self attention, mechanism has no issue with this, in that, if these different columns disagree completely with each other, then you will, using this kind of, format where, the, embedding is, the original embedding is the, key query and the value, then you're only going to intend to yourself, when you, yeah, when you differ from everyone else. So if the weight matrices. Oh, sorry. Oh, no, go ahead. if they're just the identity matrix, how can the mug be recognized in the first place? Because the input will not be mug, it will be like a feature on the mug. So how does it even get to that stage? So there's a couple, yeah, answers to that. So one is, you could have this as a different attention head. You would have an attention head that is the identity matrix, and that's doing, voting like this, but you'd have other attention heads that are doing more, more interesting things. and yeah, I'll get to that later. The other thing is, yeah, you're absolutely right, because we have no kind of reference frames that are updated over time through movement, all this kind of stuff. We're relying entirely on voting for, for understanding anything, or rather relying entirely on lateral connections for understanding anything. So if you're only doing literally voting, then, yeah, you're not going to get anywhere.

but I think the most important question is, what if you have a source of noise? it's all well and good that if you already decide you're a mug, You just say you're a mug, and whatever. That's essentially happening in both these cases, so not that particularly interesting. What's interesting about voting is, what if a column is unsure? can the consensus that's emerged around it in other columns help it to, become? more certain. and so in this example, assume this central token has equal evidence for car and mug, and this is, yeah, again, represented by maybe two, orthogonal, the kind of combination of two orthogonal, vectors.

Basically, when you do this self attention, it's going to attend to itself because of course it's itself. But it's also going to attend a reasonable amount to these other objects around it, because it has some evidence that it might be representing that. And what that means is when you do the softmax, you're going to get, some kind of non zero values for all these other columns. And basically, when you then produce the new value for this token, it's going to be a, blending of these, these different columns. Depending on how unsure or certain this column is about whether it's a mug will depend on how much it ignores those things around it versus how much it decides to integrate their evidence. So if it was like, 80, 90 percent sure it's a mug, but a little uncertain, it's going to much more quickly take in, that, or it's going to have a more significant kind of influence from the others. If it was only 10 percent sure it was a mug, it's probably going to mostly attend to itself and just ignore the other ones, around it. And actually, as the number of other columns, like adjacent columns that it's self attending with around it increases, it's the more of an effect those, columns are going to have.

does that, yeah, make sense, before I move on?

Or does anyone take issue with Okay, yeah, because then I just saw some fun random trivia. so that's, yeah, hopefully convinced you that, self attention can, under these special circumstances, do something like, voting. I then came across this interesting quote. the cortical columns are like, attention weighted interactions between Different word fragments in a multi head transformer, but they are simpler because the query key values are all identical. And the role of the inter column interactions is to identify these identical kind of embeddings. Yeah, any guesses on who said this? Geoffrey Hinton. Yes, that's correct. Regarding the GLOM architecture. Geoffrey Hinton. But I just think it's interesting because, yeah, when he came out with this GLOM architecture, a lot of people drew parallels to Numenta's work. and which he, doesn't mention at all in the paper, even though he talks a lot about columns and basically describes voting. but he, he does compare his work to transformers. And so I just thought, Yeah, it was the same kind of point that the key query and value vectors are all identical to the embedding vector.

but then, there's definitely some caveats to this. yeah, 100 percent the positional encodings are less explicit. They're all entangled in this. There's no, yeah, real clear notion of space in the same way as in Monty. this assumes pose, votings at the object level. I think this kind of gets to what you were, saying earlier, Vivian, that yeah, like in Monty, where, we're voting, what do you call it? Yeah, at the object level, do we agree, mug, but, oh yeah, but sorry, but then the pose, in, Monty, we're where are we on the mug? Whereas what I've represented here, we would be more saying like, where's the mug in space, like maybe in body centered coordinates.

and then, yeah, and there's yeah, again, no recurrence within each token. I feel like, yeah, this kind of has an interesting, outcome in that if you imagine, let's say you are doing this and you're relying on this to converge towards a certain representation and actually use voting for this purpose. let's say you're 50 percent sure, but, like when you, do the softmax, this might only get nudged, a little bit, towards, certainty. And let's say, you want to iterate and, move closer or whatever, basically you will need multiple layers of, processing to do that, so you would need, a very deep transformer, to handle that situation.

and yeah, how deep are these networks? in general, like these large language models. good question. I think, what was the original one? It was like 6 or 8 encoder and then something similar for decoder, or 12? But I don't, I have no idea how deep the, the latest ones are. Do birds have 12 layers, for example? These ones are like over 100. I'm not sure of the number. that's very interesting, but 12 is really different than 100. Because I remember, going back to the convolutional networks for image, right? They were, they're big, they had hundreds of layers or something like that, hundreds of layers. so here, 12 layers doesn't mean, it means 12 transformer blocks. Each block has multiple layers.

And GPT has, I think, 96 transformer blocks. Which GPT? GPT 3. Yeah. Oh no, the four. So without this recurrence, and there's no feedback, are you always relying on the entire stack to get your answer? Yeah. You are. There's a lot of work of actually, doing fast inference by exiting early, that you don't need to go through, 96 layers of consolidation to get your answer. Yeah. So there's a lot of work doing that. But they train it. You've got to train the whole thing, and then you could maybe do faster inference. Yeah.

That's interesting. Yeah, it's, it's either that's not to be diffing, it's just the fundamental differences. without the recurrence, without the feedback, you have this system. it's quite different.

And maybe that's not nothing new, just saying that, just remind ourselves, yeah. honestly, we, spend a lot of time thinking about like compositional objects. Objects made of other objects and reusing models inside of other models. Is there anything equivalent to that here?

Yeah, it's a good question. That was one of the main things I didn't get around to a satisfying answer. So I don't touch on that here. But I could get back to you on that. there's, I will talk about Yeah, on the next object, like how you recognize an object composed of parts, but that doesn't necessarily address the situation of arbitrary like fast binding, between things.

Self attention, can do that, in the sense that, yeah, let's say, yeah, let's say, this is, the word is blue, and, and then this is, I don't know, Volkswagen or something, and, so that's like a novel combination that was never encountered in the, In the training set, maybe it's such that these kind of key query values have been learned such that Volkswagen knows to look for adjectives, Blue knows to look for nouns, and so we can come up with a kind of vector embedding that combines those two. But that's a very kind of like mushy just add everything together way of representing things. It's not compositional either, right? I think, I don't think there's an explicit mechanism for compositionality. It's more if for some parts of the data it would be more efficient at compositional representations, it might learn it for some aspects. It's just, but it's not like it's generically doing compositionality. But if I learned a model of something, let's say I was doing revision network, I learned a model of something, okay, that's And now that appears as part of a larger image of not a larger object, it's not a separate thing anymore, it's not part of a larger object.

will I say, oh yes, this larger thing has the model I learned earlier, or should I say no, I got a new larger thing and I'm just going to learn the statistics of its parts. Does it, if I, let's say for example, I knew what a bicycle pedal crank looked like. And I know it, and I know that it's, and I've learned what pedals look like. And now I see it in different machines. It might be a stationary bike, it might be a, a human powered wine press, I don't know. But I say, oh, that's a, would it know that, yes, I recognize there's a pedal and there's, affordances, or is it going to say, here's a line press and, this is just some statistical part of it. I think he can do the, I can, he can do some of it. Some. Yeah. I think that's why. All these answers are going to be fuzzy like that because I think in a lot of cases from, because it's trained on so much data, it will just realize it's more efficient to think about panels and cranks and apply it to different things and it'll do it, but if it's only seen. Very few pedals. It's not gonna do that.

if I see, if I saw a machine I didn't recognize before, it was a wine press and it was human power with bicycle pedals attached to it, I might be able to figure out what's going on. I say, oh, there's some human's gonna be using this thing to provide power. And then where's the chain go? okay, it goes over here. It looks like it's gonna turn this tub or something like that. I'm just thinking that I would go through, because it's built of components that I've seen before, and I might figure out how they work. is there a way we could test that with just language? Because then we could just input it to ChatGPT. I don't know.

I think Yann LeCun put them together and see Yann LeCun recently had this, thought experiment, which was, like, putting a bunch of cogs that are rotating either clockwise or anticlockwise in, series. And then asking, okay, one cog is turning, what is the, the direction of turning of, the nth cog, or whatever. A cog for you is a gear? Is that the British term? Yeah, sorry, like a gear. And maybe, yeah, French, English, no. but, Yeah, and there was a bunch of tweets about whether it could, sometimes it solved it, but only if you told it Yann LeCun was wondering whether it could solve it.

and stuff like that.

but, yeah, I think definitely it can do general computation, right? It's not Turing complete. So there's, there's problems that require polynomial time to solve or linear time to solve, and it's not going to be able to do those things, right? It can't. It's if like sorting requires n log n time and you give it a list of a million numbers, it's not gonna be able to solve it. But yeah. But yeah, I guess this probably was like designed to point, but it can, yeah. It's not a general computing. Yeah. then again, I don't like using in computing because we think about like digital computers, but I do think it computing in the sense that the more abstract notion of computation yeah, Like maybe, okay. Some problems just. are known to require polynomial time to come at an answer. And it probably won't be able, like traveling salesman problem. But no one can solve that. Nothing can solve the traveling salesman problem. Yeah, of course you can. Computers can solve, it just takes time. not perfectly well, big a problem. They, we don't have amount of time in the universe. but up to a certain point it can solve it. Whereas I think transformers won't be able to do arbitrary touring computation. it's funny because we use the word attention. I think it's completely different. it's not completely different, but it's certainly, again, it's one of those words that. It sounds the same and maybe similar, but it's really quite different. If I see something I don't understand, it's a composition of components I haven't seen before, I narrow down my attention until I, I see various things I do understand. And, and then I say, okay, what are the relationships to those, things and say, can I see parallels to other things I've seen? So it's, not this big, feed forward process. It's an interactive process where I have to literally attend the different parts of an image of different parts of. Something that's happening in my life. And here they have the word attention, and it's, but it seems like a very feed forward, there's no recurrence to this, I can't walk through hypotheses sterially or something. the chat with the autoregressive stuff, it's continuing to feed things back in. But that's the thing about it, it's just running this, it's running the same algorithm, but it's taking an output and putting it back in. Yeah, it's active, it's what we call active prediction. And so it can diverge very easily, so it's taking a prediction, sticking it back in, treating it as ground truth, going and predicting the next thing, treating it as ground truth. That's interesting, it's not exactly what I was talking about, but it's interesting. Yeah, that's the thread that Niels and I have on Slack. That's one of Yann LeCun's main, that's why he says these are fundamentally limited. It's just essentially only doing active prediction.

It's not grounded in any way once you start, once it starts going. Yeah, That leads to its hallucination. hallucination, yeah. Because there's no additional information. You and I would look at something we don't understand and we would then, we would try to, attend it in parts and try to get my hypothesis serially. but we, but each time we get more sensory input. we get more sensory input and we don't just confabulate our way through the results. I didn't, I don't say, I think it's a dog. if it's a dog, then it's going to bark, and if it didn't bark, then it's going to say this, and it's whatever, It's just once you give it the query, it's just, the input is just going to hallucinate from that point on.

essentially, I'm really trying to get a sense for the fundamental component differences here. And it does feel like this idea that we're moving, sampling the world, not randomly, but we're moving the sampling world constantly. And sampling the world is not just Moving our senses, but it's attending to different parts of the world, and we're building these models dynamically in real time. that part I think it's doing in brute force, if we're given an image, we attend to different parts of it. When it's given an image, it attends every possible combination. And so it is In the superset of the 1080 H4. that's been the history of technology in some ways, that we can build machines that are better than humans in a lot of capabilities, right? and, in some sense, one could argue, I always use the word, we have calculators, they're superhuman, right? They're really great. and then somebody, sometimes you can argue that, computer vision should be better than human, right? Because you can do this, you can do what you just described, right? You can look at all at once, you can train like a billion things, but humans can't do that. so it shouldn't be a surprise that, Things, even like these chatbots, are really, good at faking it in some sense. The question is, are they really, do they have any, is it more than faking it? Is there really true knowledge about the world inside that can be taken advantage of? so far I don't think so, but I'm not certain that, I don't know. I'm asking questions trying to get a better sense.

It's just very brute force, and you can get, do really well.

And we, of course, we as humans look at language, which is, language is already a very It's a, it's an abstract way of communicating an internal model state that I have to you. And, so it's abstract from the beginning. it's already post understanding. It's we're coming up with description, where the tokens represent my internal representations that I'm going to communicate to you. Which could be completely fake.

but here we're starting with that, and that's all you get. you don't have any of the ground truth or knowledge about how the world actually feels and looks and sounds. You just have what the words correspond to. But anyway, we're really good at assigning, oh, a human will say, oh, this language is really good, there must be a human on the other side of it, there must be something really smart on the other side of it. In the same way we might look at the output of a calculator here, there's a really great mathematician on the other side, but until you get used to it, you say, oh no, it's just doing its thing, I don't really understand what, I'm just rambling. What else do you have? Yeah, I guess a couple, yeah, a few different things. Just one last thing to say on this kind of question of compositional objects. I just thought it was worth highlighting, Yeah, I think, the, where these things excel, seemingly, is, yeah, they are good at this kind of fuzzy combination of these, vectors which might represent different things, and that's why I guess it's impressive that, something like Dall-e or stable diffusion, you can say, I want a hedgehog made out of lettuce, and it seems to be able to take fuzzy concept of lettuce, the fuzzy concept of hedgehog, and create something that seems like a reasonable interpolation of these two things, but on the whole kind of compositional thing, it's, it's relying on this self attention, which doesn't have a very explicit sense of space, and that maybe is why they, on the other hand, struggle with, something as simple as, I want a blue cube next to a red sphere, with a yellow pyramid on top of the blue cube. it will get the colors, associated with each object wrong, as often as it gets them right. Is that right? Because, yeah, at least Dall-e. I don't, I don't know Dall-e 2. I don't know about Mid Journey 5 and stuff like that, but that was definitely an interesting example of, Dall-e 2 could do all these amazing things, and it can't do three colored, geometric objects. Wow. yeah, so it does really well if there's, one thing. This is, related to the compositionality thing. It's yeah, a hedgehog made of lettuce is one thing, but these three objects with three different colors and three different positions. Yeah. And and it needs to keep track of where they're in space and what's associated with what. And that's where self attention seems to struggle. And if mid Journey five has solved that, I imagine it's just through brute force rather than, anything because they are famously have better hands now, although they're, still not perfect. But, and that was another thing we talked about recently, which I think is a similar pro problem. It's one thing to have a fuzzy representation of a hand, but to have a very clear sense of okay, I'm going to do five fingers, you need a more explicit representation of space. Yeah. But yeah, I do have a few things. yeah, let's see if we can get through the rest of this. Let's get to the conclusion here.

okay, yeah, reference frames. Yeah, so do transformers work with reference frames? they use positional encodings at input, which is a small step, maybe, towards something like reference frames. and yeah. This is what we did with the temporal pooler, right? it's the same thing. We said, okay, let's solve this simpler problem of one dimensional reference frames.

and, then before moving on to the more difficult problem of, not n dimensional, three dimensional, so it is a significantly simpler problem.

yeah, and, there's some evidence they have better kind of spatial awareness, which I'll get into, and yeah, there are some, with a kind of neuroscience psychology background who have, compared kind of their computations a bit to grid cells, the fact that they have these kind of, spatial encodings.

I was just going to say, I don't think it's due to their success, new models don't have positional encodings, right? They use Alibi, they just replace it for a learned bias instead. What does that mean? You get rid of positional encodings of words? Yeah, you have a learned positional encoding instead, you don't have the version that Neils was showing. Yeah, but it's still a, it's still a learned positional encoding. Yeah, it's still position encoding, but it's just not, like, How does it learn? Don't I, aren't I feeding in text one word at a time? Or no? How's your thing going? Something like that. you have, a bias in case. where that would be in the sequence, but that you're not, giving it beforehand. You're just initializing and letting it learn for backpropagation. So it's not exactly a position encoding. encoding. Okay. I don't know if you it's a mapping from, let's say it's a sequence. isn't it, let's say, again, it's the fourth token in the sequence. a learned encoding would just be like, yeah, mapping from four onto a certain vector, and you just learn, you learn the weight transformation that does that vector.

that vector presumably is still capturing information about fourthness. It's just, yeah. yeah, I'm not suggesting it's specifically cosine or whatever, just the fact that there is positional information that, yeah, is maybe But when you train, when these systems are trained, they're not, they are trained with, the only information you have on the tokens and their positions relative to each other, there's nothing else, right? There's, no other, there's nothing to be learning on. there's no other additional information about the world. It's just These tokens and, where they are relative to each other in a one dimensional reference frame. I'm stating that to see if I'm wrong about it, but that seems like sort of fundamental truth here. That's all you've got for your training data. So there's no more position, there can't be any other positional Other than inform, this thing was in this position relative to this one. Yeah.

the language is a sequence, I dunno what happens when you do this on images. Yeah. and you could do a variety of things, as mentioned. Yeah. Often they just do a, raster kind of, where they just flatten the image essentially into a bunch of concatenated. But, but they, there is also work where they do a more like 2D what you call it, relative displacement and X and y and things like that. But even that's not what you really want about the world, right? Because what's next to each other on your retina has absolutely nothing to do with what's next to each other in the world. Yeah, yeah, I can get into this here that, yeah, you have all these different flavors, and it can be absolute, it can be relative, and yeah, but just to say yeah, reference frames, so there's some evidence, for example, that transformers, visual transformers have more of a shape bias than, for example, CNN, are getting closer to humans, maybe, but it is interesting to note, if you look at this graph, The difference between VGG or ResNet 50 in general is much smaller than the difference between these visual transformers and humans. Sorry, I should explain this figure. So this is, here we have different shapes, and the more you go to the left, at presentation time, you take these different shapes, and you put a random texture on it. So you might have a, an airplane, but with the texture of an elephant skin, and you basically ask the system or the human to classify it. and humans will classify based on the shape most of the time, which is what you're seeing here. but, other systems, particularly like old CNNs will tend to focus the texture, and just completely ignore the global shape. This gets back to our morphology models versus, Yeah, exactly, so it, seems yeah, okay, maybe adding these positional encodings is doing something for transformers, but, but it doesn't seem to be enough, maybe because it's not explicit enough, What, are transformers on that, chart too, or are those Yeah, they're the, pyramids, these yellow and orange ones, so you can see they're the best, of all the different networks. The other ones are CNNs. Yeah, okay, that's a pretty telling chart, Yeah, I suppose maybe one caveat to add to that, there's been a few different papers around this kind of question, and some of them suggest that it's also just the fact that transformers tend to be trained on a lot more data, so it's not totally clear whether it's really something special about transformers, but, yeah, but I think this one controlled for that, but, okay, yeah, This kind of, yeah, then gets into, okay, how can we do yeah, recognizing, let's say, a mug, based on this kind of self attention, operation. And again, this is going to assume a simpler form of self attention than, a, a special case, rather than the full, one that's possible. So basically assume that, a set of, the, like the set of the, key query and vector, weights are the same, which is to say that, when we're doing a particular, attention head, within that attention head, the, Oh, actually, yeah, I think that's right. Yeah, within that attention head, we're going to see, basically the same, key, value, and query.

and then, yeah, basically we have these, input representations, let's say they're rim and handle with associated kind of positional embeddings. and if you have these kind of, different but reused, weight operations, you could imagine them learning to transform, for example, from rim at, a particular location to mug at a particular location. And this kind of gets very much to, what we've explored in the past, which is this more kind of capsule network type operation, where everything is just directly learning to predict, given a feature at a location, what is the relative position of the object, to it.

but, once that's done, once you have those operations, then again, or like the outputs, then again, you could just compare these, and, they would attend to one another if they, had good agreement.

so yeah, so then about positional encoding, embeddings, yeah, just to say basically there's a whole bunch of different, varieties, that you can do. As, Lucas said, they can be learned or they can be hardcoded as the one I described at the start was. they can be added, they can be concatenated, they can be in relative coordinates, absolute. they can even be recurrent, which seems a bit odd given the mass parallelization that transformers, do, but, yeah, you can essentially have the position encoding at the kind of nth position be recurrently dependent on the position encoding at the n minus one position.

and, but a kind of key thing is they're typically at the input only. Okay. And yeah, I was, one thing I was trying to research but didn't get around to, was like how much, and so I'd be curious if anyone knows, like, how many, how much work is there, out there, yeah, using, concatenated representations and making sure that there's a separate, representation at each layer. Because it seems like there's a risk otherwise that the kind of spatial representations get washed out as you get deeper and deeper, unless there's a really good objective function that's ensuring that they're, absolutely important, and that maybe this contributes to the kind of, yeah, they're, I don't think rotary invariants are input only, I didn't get implemented, so I didn't read the paper fully. But I know for sure it's not an input only because you got to change every attention layer. So there is something going on at every layer level. Wait, which one is that, sorry? Rotary. That's the one they were using before alibi. Oh, is that the one that's used in PAL ME? I think, yeah. Where they, rotate the matrix? I'm not, I know it's not embedded really, but I didn't want to get too into it, but take a look at that. Okay, cool, thanks. Yeah, there was definitely one that I saw from Google where they, rotate the embedding instead of, adding to it, which was interesting, but, okay, cool. And then, but yeah, just to point out, yeah, versus Monty, at the input we're, At least at the moment, and likely to use absolute, body centric coordinates, that are in kind of six degrees of, freedom. But then the kind of real computing, is happening at these object level internal representations. all of this is very explicit. and I guess we also have kind of stronger skip connections for positional information, so although the skip connections might help the preservation of some of this for transformers, I think that's, yeah, again, because we're keeping the spatial pose and everything like that very explicit and separate. I think it's more likely that will make it deep into the architecture in Monty.

yeah, this is the limitations I already just talked about. I think just the one last one to mention is, yeah, that these transformations, all these weights and stuff need to be learned for transformers to be able to do any of these kinds of, operations, whereas Monty learns very quickly based on the sensorimotor movement and then uses those graphs to just directly determine, okay, what is the position I expect to be over the pose.

okay, so this will hopefully be an interesting one. Sorry if I'm going a bit quicker, I just want to make sure there's time for everything and then I can always, I guess we'll come back. but yeah, so embodiment, so this is, yeah, now starting to become a thing with transformers, and so just two kind of prominent examples. One is Gato, so this was from DeepMind. And this one was interesting. as, as Subutai was saying, these, systems can be autoregressive if they're of the generative variety, and, and they took that into an embodied setting where basically You know, you might have some, fixed prompt, and then you it receives some sensory input, let's say, the output of a game screen, that outputs, an action, and all of this is represented, so it's, it for example, receives, the game input tokens, then it outputs As the next token, it predicted was its action, and then it did another action, and then it gets, receives then the next sensory input, and so forth. And so this can, yeah, it can, Is that done, I'm sorry, is that done serially, or is that? That is done, yeah, that will be done serially at, at inference time. Yeah. it might get a batch of these at once, I don't know, with the, like the visual input, but, but, What's interesting is then, yeah, it does have a sense of recurrence in that, obviously it can just keep concatenating these, but of course there's a limit to how long you can continue this before you completely forget about your past. and it's also just very computationally intense because, yeah, you're essentially, just doing all of these operations, at every, next step. yeah, in order to deal with this, they used a, an architecture that's designed to have a longer distant horizon, but I guess the important thing is there's no real, there's no recurrence really within the network, it's, recurrence with the input that it's dealing with.

and that's quite similar for this, more recent work, PaLM-E, so PaLM was this huge language model from Google, and PaLM-E was their subsequent work to try and leverage this for robotics. With a multimodal, model. And actually, yeah, this was presented recently, at DLCT. and yeah, so they just explore, for example, can you get a robot to, pick up these, a bag of chips, like this based on kind of a natural language prompt and an image. So it's multimodal in that sense. And then, the model's job is to output. More natural language, which will condition the robot's kind of action sequences that it can take. for example, like grabbing something.

but again, the kind of the way that they're dealing with embodiment is I want to be clear. They take the original prompt, then they create a series of language steps. Yeah. They actually go through that, they actually generate these language Sentences, and then those become, those are mapped to movements. yeah, and the model, as part of the whole like end to end learning, the model has to, like the language model, whatever, has to learn like what are the reasonable outputs that will actually map onto actions that the robot can take.

they don't do anything to directly constrain that. They're going back. But yeah. They're going back through language to do that again. It's interesting. I don't have to say to myself, open the drawer and, reach for the green bag out of it. It looks like it's interesting, but Yeah, but then when it's, once it generates the language instruction, does it then go off and do the entire sequence or does it keep Yeah, so this is the thing. So it, at any given time point, it has access to, the instruction. So that's the start of the sequence. It has all of the actions it's taken itself, like all the actions it's generated in natural language, and then it has the current visual input. But what happens, what if you can't open the draw, or the, or, I don't know, I guess something, that, that, requires, a change in plan or something, right? Yeah, so they show that with this, the human knocks the rice chips back into the drawer. And so I guess that at that time point, at the next time point, so it already said take the rice chips out of the door, but then the rice chips are back, and then it's, what you call it, but it sees the image, so yeah, so they're still there, so it basically just outputs the same action. So it's not like it, I don't know, it doesn't understand why it's gone back or anything like that. Yeah, a classic example might be I try to open the drawer and maybe there's a latch and I didn't know there was a latch or button I have to press, so it says open the drawer, it doesn't work.

it's possible, that, it would have learned that, okay, I tried this. And I see the image, and okay, on the next one it might attend more to the lock or something. I don't think it's inconceivable that it could learn to do that. But I think, yeah, it feels like the main kind of weird thing about this is there's, yeah, there's just a bit of a, a missing, yeah, world model and just continuous, continuous representation, for example, of visual space. it literally just has, the actions it's taken and its current visual input. which is of course very different from how we would interact with and again, they're just like concatenating all of this on top of each other and then just doing the full, transformer operation across all of it. again, it's not clear to me that, they might be able to get this kind of system working really well under certain environments. it's not, it's, I don't know, it's hard to say how good they can get. This is just the beginning. or maybe I'll run into fundamental issues, I don't know. Yeah, but yeah, so basically, yeah, go ahead. Do they then generate, sub policies to actually translate these sentences into motor commands, or joint movements?

no, I don't think so, but, the, this kind of thing itself is I can't remember if there's also a step where it it first chains together these things. Thanks.

I, it was either this or another paper where, bring me the rice chips from the drawer, and then it's meant to first create, a bullet point of all the actions it's gonna take. And then it says okay, first action, do this. so yeah, no, but I don't think they, do anything with, sub policies, but, yeah, no, it's not. just from my experience, it seems like it's, so much easier to take seven steps than to actually take the rice chips out of the drawer. That seems like a lot more difficult than just writing out seven steps. Yeah, and what if the robot arm was in a different position than it usually was, or, things are rotated, would it, it's not, these could be very simple robotic commands if everything's fixed. Yeah, I think the problem. yeah, I don't know if they considered that or maybe it's something they're, doing now but, yeah, I think in general they were trying to like really lean heavily on the language model. Yeah. But I guess with the caveat that it's, yeah, maybe not very good at describing these lower level things, but yeah, yeah, that's, I'll just skip over that, yeah, just the other kind of really big difference to mention is, yeah, obviously, You have either rapid, sensorimotor, reference frame building, few shot based on real world inputs, or you kind of backprop on the, a significant chunk of the internet and absorb all the kind of horrible things, that are out there.

that's just, yeah, another, big difference. The last thing I just was, yeah, hoping to talk about is how can we maybe bring some of these ideas together. two caveats to just mention, yeah, I'm sure some of these things have been tried in the literature, but maybe not in combination, maybe not, particularly in the context of embodiment, but, but yeah, that would definitely be a next step if we do want to think about this. Firstly, just check and maybe someone here already knows. But yeah, but I think turning these token representations into something a bit more like an RNN. within that token, it's still part of a transformer, but just it, it also updates itself based on its, past history. and in particular, if there was maybe some sort of separation out of pose embeddings from feature embeddings, throughout the architecture, then I think this would get, yeah, a bit closer to, something more Monty, and, would make more sense for something like embodiment where kind of, rather than having to constantly concatenate the full input at every, like the, full past and the full, future, when you are, doing embodied, learning, you'd, because this is what I meant by this approach is a little like, they just keep adding, sorry, like this one, it's like they keep adding kind of new columns onto the brain if, or recruiting new columns if you imagine that these are, columns, whereas if we assume we have a fixed number of columns, we can deal with, you can still process like the visual input and whatnot in parallel. but you keep track of history through this kind of more, self occurrence within a given column. And that would be more similar to what, Monty would, do.

and then, yeah, making this kind of voting, this kind of special case. So it might already be happening, but it requires that the, attention has learned the identity operation. I didn't get time to check whether that's like an observation that ever happens. That would be interesting if it does. But. But, maybe settings, like adding some additional attention heads to the ones that typically are there, could just mean that's like an extra computation that the system has access to, the kind of the feed forward layer could ignore it if it feels, relevant. And this maybe would be a bit like skip connections, how they do the identity operation, just like passing, information through the system. it would also be parameter free, because there's no, yeah, weights involved. Or these kinds of other weights I was talking about for the kind of more transformer type, or sorry, capsule object processing. Because again, the key query and value in this case is shared for a particular attention head, then I guess that would be like one third the typical parameter count cost.

so yeah. these could maybe be interesting, inductive biases to try, and, yeah. some of these changes could be made to pre trained networks and then further fine tuned, but obviously there's a lot of detail that would need to be done to make, to think through whether it would actually work or what problems there are.

but some of the kind of more complex things to, to introduce, would be, yeah, these kinds of things like top down connections, I think, yeah, that would maybe cause issues for backpropagation, just explicit spatial models, how to do that, I don't know if the kind of recurrence and the, and that sort of thing would help have that emerge naturally, it would be a bit similar to then, like the representations would be a bit similar to some, In the literature where they found like grid cells emerged just through learning. And then what you were talking about, Jeff, like this whole kind of compositional objects like rapid binding between reference frames and features. It's not clear that there's any kind of easy solution again without kind of explicit spatial models.