so yeah, today we're gonna spend probably five or 10 minutes just going over the repeat feedback unless, Vivin, did you get a chance to go over this during the, code meeting? no. Yeah, if I can have, yeah. That okay. Yeah. So we'll start off with that. and then, last week we brought up, some potential issue and like a solution to object distorting behaviors. I put the link to like her writeup in, of VRA in the research channel.

yeah, we can join in on that when that happens. and then as time permits, I think the presentation will be short, but, I already have questions that I came with mine, so depending on how that goes, it, that may be the main topic of, today's research meeting. But as time permits we can also continue on, to the Google. I can't imagine that topic's gonna be short. It's it's very confusing. I don't think we understand it very well. At least I don't. Yeah, oh, actually, yeah. Yeah. Now that you that say that, if if you could also go over what problem is trying to solve and the distortion model, like just introducing that concept again, that might be helpful to, I guess Tristan who might watch this later, and also probably everybody who's watching this on YouTube. Yeah. I updated the X College robot this morning to add a bit of context first, again, to recap the pro a little bit the problem, but mostly the kind of what I presented on Friday of the brain streaming week.

all right, so this is a bit of a recap, but I thought it might be useful to give some context. So basically, where we started out was that we had a solution to modeling object behaviors or we have a solution to modeling object behaviors. We also had a really nice idea for how we can use an object behavior and apply it to make predictions about morphology. That was that idea of using kind the movement, stored in the behavior model and sending them to the child column, to move you through that reference frame the same way you use the sensorimotor movement to move through the reference frame. and I still think that's a really nice idea, but one of, one issue with that idea was that it didn't work with, object distortions or behaviors where a lot of parts on the objects move in different directions. Like not the entire child object moves together, but yeah, all the parts on the child object or all the parts on the object move separately. And so I call that object distortions generally. And so when I, restarted thinking about it, I thought about just object distortions in the static case first. So no behaviors right now. Basically just thinking about what are all the ways we can recognize this cup? how can it have all the different distortions? So we might have a model of a for coffee mug, and we can recognize that coffee mug again, we can recognize it in a different orientation or location in the world and we can recognize it at in a different scale. And all of that is already taken into account by our theory. we can also imagine a scale in one direction where you just scale the movement vector by something in only the X direction or something. But then you can also have these more funky distortions, like different scale at some locations or different scale and orientation, at different locations on the model. so basically different transformations that would need to be applied to the movement vector depending on where you are on the object. and for this object. Deformation, we have a solution proposed already, which is the idea of the, I should have put a picture of that in here. but like the bent logo on the cup where you have the logo and then at some point it, it turns its orientation and bends. And we put that on the cup. And the way we learn that is that, we use hierarchy and we learn the child object, location, orientation, and scale on a location by location basis. So it's not like the child object has one orientation, location and scale, but it's associated at different location on the parent objects. It can have different orientations or different scales. and this is just a sketch, but hopefully it's simple enough to follow. Basically we would have, the Menta logo or TBP logo being modeled down here and then. we would send its, object ID up to the parent column. So the parent column could model the cup and on different locations on the cup, we would assign the logo id, but we would also calculate the relative orientation of the logo to the cup and store that as well in the, parent model. And that relative orientation can be different at different locations on the parent model.

does that make sense so far?

Okay. So then I extrapolated a bit on that idea.

so one thought was the map of the translations, orientations and scale could be independent of the specific object. So instead of saying we have a relative orientation of the logo to the parent, and that relative orientation is different on different locations, we could have that map independent of the. Logo on Cup Parent Object, it could just be a general deformation model that kind of models relative orientations at locations. And you can apply that same deformation to different objects, to basically then say, oh, now I have this different logo that bends. and you can imagine that immediately without ever having learned that different logo with a, in the bent variation.

and then instead of associating object specific locations with each other in the backward connections, we would apply a movement command to the morphologies reference frame. So that's a requirement if we would want to learn these general defamation models, we couldn't use the, feedback connections that associates, location in the parent model with a location in the child object. Because those locations are unique to the specific objects, we would need a more general solution that we can apply to any object. So we could use this mechanism that we talked about or came up with a couple of weeks ago where we just apply a movement command to the Child Objects reference frame instead. That is general, independent of the location, specific location. SDRs, I don't know if you wanna start having discussion yet. Sure, yeah. Yeah. So I just on this last point, I was thinking about the Bent logo and he said, oh yeah, I can apply it to another object. And I was trying to imagine, okay, I could imagine that I could show you a, cup with the bent thousand Brains logo and I say, okay, we're gonna put another logo on there. I can imagine that, but I wouldn't be able to make, necessarily make specific predictions about exactly where the bend occurred. I, it would be like, I wouldn't be certain, I would have to think about a lot to say, oh yeah, maybe how far across the thousand Brains logo did the bend occur? And then try to imagine how far across the other would be. it wouldn't be something I could just easily do. and, it seems like when I learn it on the first time, I learn on a specific logo, I actually noticed exactly where it occurs. Oh, it occurred right here at the two, you in the ass, or something like that. And so I kinda learned that. So my thinking here is that, it may be too much to, instead of throwing away that particular solution, the one we have already, and throwing away because it's hard to make a specific prediction for a novel logo. I'm not sure that's, a sufficient reason to look for an alternate solution in that particular case. Because it could be that, Yeah. I get the general idea of I'm gonna bend, bend image here, but I won't be able to make specific predictions about it, and it feels like, it really does feel like I, when I learn it, when I learn the original Ben logo, I'm not imagining some movements I'm just, or behavior. It's just oh, I just look at where exactly on the logo the thing is bending. And that's where I learned those points. So I guess I'm not convinced that this particular example requires, an alternate solution. I guess that's what I'm trying to say. Yeah. I, guess just to clarify, I wasn't thinking of this as like throwing away the previous idea. It was more like generalizing the current mechanism. So we would still calculate the relative orientation of the parent to the child and still send that information up here and still invoke minicolumns and learn that, that he, the difference would be that those relative orientations would be in a different reference frame than the reference frame of the, TVP cup with logo model. So it would be like a separate map, like the behavior models. Okay. That's fine. But then the idea that we might, imply the movement commands, is weird to me. That's the part that struck me as Hey, it doesn't seem like I do that. Maybe I didn't understand it correctly, but, I don't see why I need to think about movement commands in this particular example. Yeah, I guess it just seems like a big ask to always have to learn the backward connection between the specific location, every object. I, I obviously I couldn't, right? I couldn't learn the backwards connection. yeah, I could learn, it seems to me I could learn, so the backwards connection a are tying it to a specific location on a specific child object. So let's say I can't learn that. Maybe I just don't have that. Yeah, I could still couldn't, I still calculate the relative orientation at that, at those points. And so I say whatever is going on there, it's gonna change orientation.

so can I do that? Lemme just put the backwards arrow on here so we know what we're talking about. But, it's that green one there, right? We just add that. So basically the, oops, this is a very thick line.

so this would be the backward connection that goes up here and then threads. And we have the activation here in layer six A of the locate the location, the child object and the location and the parent object.

So I'm saying, look, we, that's not, we can't, we haven't learned that. We can't rely on it, right? Yeah, exactly. but we can still calculate or could we still calculate the relative orientation change at that point? 'cause that's not object specific. yeah, so we know the relative orientation in the parent column and one thing we, talked about using the layer six cortico cortical connections. This is dark green error to communicate that to the child column. so yeah, I think that's still a valid general mechanism that would still apply in, right? So the solution I propose, the thing that would require applying a movement command is if the location of the child object relative to the parent is changing. because if the logo bends, the features will also be at different locations now.

so imagine, we just didn't learn the purple backwards connection. we learned it in a specific case. We learned it on with one logo, with one object on the particular, the logo on the cup and now we're gonna see something else in the cup.

and but so we would say, okay, this cup, I'm expecting the change in orientation at these locations, so that's gonna be stored. I just won't be able to predict a specific object or a specific location of that child object.

I guess it just feels like I, can't do that. It's like I, have, I really don't have an ability to do that. I just, I could take a wild guess, but I don't have an ability to say, oh, this is where it bends, or This is what the stuff is on the left of the bend and the right of the bend. I have a general impression that the, new logo bends someplace and occupies the same space.

I could say, oh, there's another logo about the same overall size here.

it, it just feels like I don't, it just feels oh, yeah, I don't have enough information to make a very specific prediction about where it bends.

but it's, I do know it bends and, and that's all I can say about it. I, could, I don't know specifically.

I, have a, yeah, do you wanna go first? Okay. I was just gonna say, I think you, I would say you can make pretty specific predictions. I think the example of the bend is like not knowing where exactly the bend is more, because you don't. Know how to anchor the distortion yet, like you don't know where the lo what's the location in the distortion reference frame yet before you actually see the bend in the logo. But if you think of, a distortion that is more uniform, let's say it's an arc shaped logo. There's like a bend in one location, but it's just an arc. You can imagine applying an arc to any kind of logo out of the box without ever having to learn these, associations. It's funny when you think about the arc, I also feel like, oh, I didn't learn that. It, feels like I wouldn't know exactly which letters in the logo or at different points in the arc.

it's sort you wouldn't know roughly.

I feel like I could make pretty good predictions. but you wouldn't know exactly. it's like I think about the old Intel logo, which had the dropped E, right? If you're familiar with that, and like you look at it and you go, oh, there it is, the e is dropped.

and from then on, I know exactly where the lo the logo's supposed to change. It's where the E drops. and I think here too, like here I might say, oh look, I, you just drew it between the U and the M. But I wouldn't know that specifically for another logo that might be there. Another word I could guess I could try to imagine it, but I wouldn't know. I wouldn't have learned that specific. Point there. Yeah. That's what I think is about the anchoring. once you anchor it, where the distortion reference, where in the distortion reference frame you are. Like once you see the bend, you know how to predict all of the letters after the bend. Alright, I'll just, I'll let you continue here. I'm gonna make a, I'm gonna just a lodge a, point. I won't go far as calling an objection, but I'll call it I don't like it or it bothers me slightly. Yeah. Which is the idea that we're gonna somehow use these, movement commands to which we came up with for behaviors. We came up with 'em for object behaviors. Yeah. and I'm still, good with 'em for object behaviors. I think that's, good. I am gonna, I'm just not comfortable applying that same mechanism now to a non behaving, deformation and in the back of my mind, I can think of all these reasons why I don't like it. But we can keep going. I'm gonna come back to it. I'm just saying, I'm not sitting here write today going, oh, that's a great solution. I'm thinking like, ah, so happy about it. Yeah. Actually, so once I get to next, I was gonna talk about the proposed solution, but then there's an issue with it and for dealing with the issue, it actually goes further away from applying the movement command again. So maybe, yeah, I know, I was reading your stuff this morning. It's like you have to do this sort of a cumulative movement vector, right? Is that what you're referring to? not, no. Alright, I'll let you keep going. Yeah. Hojae, you, wanna say something? You think? yeah, I'll just make it real quick. okay. I think I have on the same boat as Jeff in terms of, I, agree with both of you. Like for example, if we have a new, like bed logo Oh, sorry. Kind snaps in and I wanna, predict the tp, like you, it could be like non-overlapping. I don't know why Thet is not showing my, to refresh your screen on Zoom. I can see it just fine on my sal, but, Oh. Oh, just for the sake of, okay. I don't, okay, but my tea is gone so I can just on your teeth.

all So this might be what we want to predict, but also, again, we don't know where we anchor, so it might be like slightly overlapping. We rotated at a different point. But again, like as you say, if we, that's because we don't know where we're anchoring this kind of change. I think if it's over overlapping like that, it wouldn't be the same, distortion. I would say this is like a different type of distortion where the bend also includes a translation of the banded part towards the non-band part.

if I'm, for example, like, bending at oh wow, this is really big, like at this, yeah, I don't, yeah. but, I think what I'm trying to get at is, the general deformation. Is, that more related to, so if I'm on, Jeff's side of I don't know if we can make like prediction, like we can make some general, like this is a general defamation of my head. See, it's two generations. Yeah. Your behavior suddenly has changed drastically. Yeah.

Yeah. so my sense, and I'm not sure if we can make like with prediction of where this B or p like might be, but maybe the more I still want this general deformation model. To be learned in the sense that we want to learn some general behaviors. if we see stapler hinge, like stapler opening, like we wanna recognize this as a hinge behavior so that we can potentially apply this to a laptop and realize that the laptop opening is also like a hinge behavior. Like Yeah. I'm not sure if it's like related to that or not, but Hello. Okay. I think, okay. I can see that my point is lost and no, I'm sorry. I don't wanna distract everyone, but this is this is what's going on. This is what's going on in my life. This, these few weeks. All right, I'm gonna dismiss them now. It's okay. Do you wanna learn bit object behaviors? They can't either. they're, I'm sure learning Lizzie's pointing at the screen going, wow, look at all that stuff. It's also a do full grandparent mode. Yes. Full grand. now half grandparent mode. There's only two of the four.

It's a lot of people.

yeah. Sorry, Hojae, do you wanna just briefly that again, brief entertainment, please agree with everything that just to, yeah. Yeah. I, guess those, the more interesting question is can we recognize Similar behaviors across different objects is maybe what we're, I'm not sure if that's ultimately what we're trying to get at. That stapler opening and the laptop opening is essentially the same behavior, and we want to recognize that in the same behavior.

and it's related to this in a sense that oh, if we learn the, stapler behavior slash deformation, I guess it not the same thing that, that we can theoretically apply this to laptops. I think that it implies that we recognize success, same behavior. Yeah. I guess how I was thinking, like why I was starting to go down this road is because to me, like the object distortions seem similar to, object behaviors in the sense that they're independent of the specific object you're applying it to. Like you can imagine bending all kinds of objects. You can imagine breaking objects, you can imagine stretching objects. It's not dependent on like when you have a coffee cup, or maybe that's a bad example because it's not bendable, but if you have a balloon, you can imagine it inflating with all kinds of, print on it. You, you can just imagine a logo on it inflating with the balloon. and so seems like that kind of distortion is similar to object behaviors. just the way that features can change their location, orientation, and scale, but it's independent of the specific features that are there and changing their location, orientation scale. what it feels to me like is it's like in object behavior. we definitely have this time flow, right? But, it's more like when I'm looking at these distorted coffee mugs, it's more like a, a key frame or, a learned state of a behavior in some sense. as a, and therefore it's, I don't need to do the path integration of, or the time integration over time of, the behavior. it's, more oh yeah, like when I look at all, if I look at especially the Bend the Bent, the one that the coffee cups tilting over, or even any of those. it's like I imagine them, all right, it's different. Like the one on the right here is definitely, oh, that's not normal. I'm applying some kind of weird behavior that I've learned some behavior or some distortion I've learned elsewhere. there might be behavior related, and I'm applying it here. the middle one? No, that's just that's a form of cup. I see. I don't see that as a distorted cup. I would say that's a di that's a, standard cup. I've seen those. And same one on the left. So the one on the right feels more like a key frame of a behavior. The other two don't they just, unless I see the, but you like the DO logo distort with the cup. So yeah, the cup gets broader and the logo gets broader as well. But I would still just learn that, I would still think of that as, okay, that's just our standard compositional model. yeah, I guess the point is if you then put a different logo on here, you would know how it would look like. with the kind of scale getting larger towards the top of the logo.

It's all very confusing. I'm sorry, I find this all very confusing, because some of these feel like, oh yes, this is a defamation, and others are like, no, that's not a defamation. That's a, I've learned that maybe like the cup that has the middle one here that you're looking at, regardless of, it just looks like, oh, that's a flavor of cup. I know there's no way I've seen lots of 'em like that. So it just accept it. I probably, if I saw a lot of cups that were bent, like the one on the right, I would maybe look at that the same way. I'd say, oh yeah, that's the flavor I have, but the first time I see it. I might think oh no, that's like a bend. Somebody bent it somehow.

I, I don't, I guess I'm not, I don't see clarity of the, I'm, bringing up lack of clarity in my thinking about these things. I don't think they're all in the same class or there are different points in learning. I don't think it's simple that these are all sort of flavors of the same thing. maybe I think I could explain growing skills. 'cause I was trying to make it really look like the cup is changing scale towards the top and everything on the cup is changing scale with it. So In this case, the only thing that you would really indicate that would be the, logo. and then that's just putting a distorted logo on a standard cup.

now obviously if I saw cups behave like this, if I saw 'em start cylindrical and then became conical, that would be a different thing. I'm still going back to my basic objection to the or uncomfortableness of saying, oh, I'm gonna solve these prob somehow these problems have to do with, behaviors.

although the one on the right feels like it's more like a behavioral type of thing. oh, I've seen things bend before. so this looks like a bent, this looks, it's a result of a behavior. But even then it looks more like a key frame. And that's oh, I'm, I have to imagine what the, I have to imagine my movement commands have to get bent to follow this. You know what I'm saying? It doesn't feel like that to me. Yeah, I would describe the model more like a key frame as well, yeah.

And, then, can I just throw something else? I, this, is really confusing topic. So I've often thought about often we can take one, two dimensional object and wrap it around a three dimensional object. so I can take, oh, imagine, our logo, going across the top of a car from the left side to the right side, something like that, or van, I can imagine that. And so I'm, I have the morphology of the, van, and I can take a very big logo and have it wrap around the top of it. I, or I can do that with the face. I can. So it is just a general property of, applying one model onto the distorted, or not the distorted, but applying it to the image of another model in the same way we talked about the logo wrapping around the, cylinder. anyway, I'm just, I'm expressing my confusion. I don't know if anyone else is finding this confusing, but I'm not, understanding what you're saying. I, I think I saying Viviane, I'm just, my confusion is, I'm just not happy with description of it so far. but I feel like the example of wrapping a logo around the car or just wrapping it around the cup is a good example of why. We would need a general distortion model instead of having to learn that specific, like those specific associations for a specific logo and shape combination. I, I don't know if, I think it was a distortion model. It's just, I need, the morphology of one object and, that's not a distortion. in some sense I can take a two dimensional object and wrap it around the morphology of a three dimensional object. Yeah. which is, it's not a distortion model, it's just, I'm, just applying it. yeah. but the problem with that is that it, to make predictions about the two dimensional model, then we have to learn these explicit feedback connections between the locations on the morphology of the cup and the locations on the 2D logo. And then if we take a new logo where we haven't learned these feedback connections, it wouldn't work well again, I'm not sure it wouldn't work.

what if, R one Okay. R one is some long skinny logo and R two is a top of the minivan or the van. Okay. And, or we could just stick to the cup, I, Matt, first of all, R one knows it's looking at the new logo, the new image. So it's gonna keep, assuming that's the case, it's, it says I know that. So all I need to know is what, so I don't have to, it doesn't have to be told that over and over again. It says once, once you've recognized it, it says, Hey, I'm looking at the new logo. and then, so we don't need to pass the ID back. We just, the only thing, and the only thing we're, gonna struggle with, we have to pass back. any kind of scale and orientation and specific location. And I argued earlier, you might not be able to know the specific location, you could guess, but you wouldn't know specifically.

but based on just the movements of the sensorimotor, you would move in the 2D reference, in the 2D model of the logo. And those would be getting more and more off with the bent of the cup. So you make wrong predictions about the features you should be sensing at locations. I think you can't make it, I think you, it's very difficult to make an accurate prediction on That's my point.

if I saw some, if I saw, the, thousand brains logo wrapped over a top of the car and I'm looking at, I'm looking at it, and, in one image I'm looking at the side of the car, one image, it, changes, it's, it, the, it bends at the, between the O and the J and project. And another time you showed me bending between the N and the D and thousand. I wouldn't know either one of those is wrong. I would say, oh, that's wrong. I would, I wouldn't be able to predict that. I would say, oh, the logo is going around the top and Okay, that's where it breaks, I wouldn't make a prediction where it's gonna break. Unlike if I really had learned the object and learned a specific thing, I might learn where the prediction, where, if I would then, I guess I'm just saying, I don't know if we can predict that. Yeah. I think that's a valid, just so that I'm like understanding this correctly as well. the problem that we're trying to solve is we have this, canonical cup and then let me copy over the scaled one. that, if we observe the behavior of, I don't know, the logo, like rotating, and we saw a different, like a deformed version of the cup that without having to learn the rotation again, that we can, we'll be able to predict if this, if we have observed this behavior in, in the standard that you know, that we are assuming that we can predict what's gonna happen if we apply that behavior, but just to a different morphology. I thought the problem was. Goes back prior to this. Maybe I'm wrong, or maybe my memory's bad, I haven't been thinking about this for a couple weeks. but I thought it was more like we're trying to deal with basic deformation of objects and, extreme versions like a t-shirt there in between versions, maybe a flag waving than even like the very simple one. I kept saying, what's an egg versus a circle? That, and it was very fun. It was very, it was something along those lines, if, my, is my memory incorrect, about that we, were trying to say, Hey, I have an object and now it's deformed somehow.

like an egg is a deformed circle or something like that. Yeah. And I guess there are two parts to that. One is being able to recognize the object in its deformed state, which is the static version of the problem. And then the one we started with was the deformation happening as part of a behavior that you learned. So like a ball bouncing and squishing, like the circle turning into an egg or the balloon deflating and you're learning how you go through the deformation or how the def, how the object is deforming over time.

Those seem to be separate things. One is recognizing deformation of, non-moving deformed object. The other is saying, oh, being able to know the behavior of the object between sort of those key states, those key frames. I can imagine a ball and a squish ball, and I can imagine, oh, I can imagine the ball bouncing. That's a behavior I've learned. But I can also imagine, a ball instantaneously turning into a squish ball or a ball popping out like a cartoon into a cube or something. I don't know. where, I, I'm just saying, I can imagine different behavior. Maybe. I can imagine d maybe I could put, I can imagine playing a lot of pressure to a ball, a sphere or a ball, and it doesn't collapse right away. It just moves a little bit and then it all of a sudden bum jumps down. So my point is, the actual behavior between these two key frames is not dic dictated. I'm just trying to, again, separate out the idea that I have two images of the same thing and that's, and I don't always have to have the behavioral secrets between them. yeah.

Yeah. I think that's consistent with how my proposal, that you'd have kind of key frames of distortions that let you make these kind of predictions about what to sense wear. But then for the behavior model, you still have everything, how we discussed it before, that it basically stores changes. Time at locations and you can recognize the behavior and predict, So I'm, all I'm saying is I, feel uncomfortable with the idea that we're, if we're not observing a behavior and we're just observing the distorted result that we're gonna use the mechanism for behaviors to, to help us out. they don't, yeah. So this is not Yeah, I'm not proposing that what you are, that's using the movement command. okay. Yeah. The movement command is kind, yeah. Okay. I was thinking of that more as like a general mechanism we could use for different things, but, not, necessarily behavior speci, okay. If that, if you mean that then Yeah. I did steal that mechanism to use it. and maybe I said I thought it was a great idea when you first mentioned it. I don't know.

yeah, maybe I'll go actually into what I'm Yeah. Proposing and then we can talk. Keep going. Yeah. Sorry. Sorry. Go ahead. I'll try to keep that brief, if that's possible.

How this relates to object behaviors. Object behaviors can distort the object like the balloon as the behavior is distorting the object. We want to continue to make correct predictions about the morphology, so we wanna be able to move somewhere else on the balloon and know what to expect at that location. proposal. Could we learn both the stepwise and the distortion map of an object? Maybe I'll, yeah. Okay. So basically, I think we walked through this last time as well. The stepwise one is the basic behavior model as we talked about it for the past months, storing changes at locations over time. it's used to recognize a behavior and to predict changes to observe. Next, the commutative one would be a separate thing. It would be what I'm talking about as the distortion model. So it would store the difference between the original morphology of the object and the current time step, or just generally between the original morphology and a distorted, ver version of the object. It doesn't, this one doesn't actually represent anything in time, but I was thinking of it as you could learn an association between a point in time in a behavior sequence and such a model. so I would call this a defamation model, which can be used to make correct predictions about morphology. I. After the forming behavior has happened, it can also generally be used to recognize deformed objects and make predictions about them without learning a new model. so it would be it wouldn't represent anything with time. It wouldn't be the same as the behavior model, but we could associate this with a point in the sequence and be like, all right, if we went through this behavioral sequence, now we expect whatever more, whatever object we have to be in this distorted state. And so if we wanna keep making correct predictions about the features of this object, we have to apply these distortions to our existing model. And so basically what we would store in this distortion map is, if we are on a location, oh, there's no red dot, I copy pasted this from the other, whiteboard. so if we're on a location, in the objects reference frame, we need to, apply the invert inverse of what's stored here to the incoming wolf vector. Or it would be, this is confusing now without the red dot, but basically it means, given any morphology model, what kind of, transformation do I need to apply to get from. The current location in that objects reference frame, that's based on how my sensorimotor moved to the correct location, where the feature is stored that I want to invoke. So let's say I have a model of the balloon. It has all these features stored of like a logo that's on there. and, but now since it inflated, those features are new locations now, and the deformation map would basically tell us how these new locations map onto the original locations in our model that we learned. so maybe I'll do this example here. Let's say we have the thousand base project logo and it distorted. So the logo mark moved to the left, the thousand moved up and Brains project rotated and fell down a bit. and so this would be the distortion map for it. It would basically tell you, if you are here on the logo, the feature you, would be expecting is actually down here in your model. assuming this is the model that you learned. so then let's say I'm moving my sensorimotor from the T down to the P here. We'd apply that movement in the distortion, reference frame, distortion maps, reference frame from here to here. We would have this kind of compensating transformation stored there and apply that to the movement and that would then tell us. where in the objects reference frame we would expect to be. So that would be the p So it would tell us, all right, now we expect to be here on the logo, and that means we expect to sense the p and that works independent of where you're coming from or how you're moving. So another example, if we have this kind of bending cup, let's say first the red movement.

some of these errors have changed since I copied this. so basically we are moving from here up to here. We apply exactly that movement of the sensorimotor to the reference frame of the distortion model like this at the location. In the distortion model, we have stored a kind of a compensating movement. We add that in and then this resulting, vector tells us where we would expect to be on our original model of the cup. And then as another example, if we do the screen movement, we are at the bottom of the handle.

and we are moving again to the edge of the cup. So let's say we are here, we would also apply that to the reference frame of the distortion model. Add this orange arrow to it that's stored there, and that would tell us the correct movement in our models reference frame.

so in summary. This idea would, make use of the idea to apply movement transformations in addition to the incoming movement vector from the sensorimotor would apply translation and rotation so that the feature location under deformed model corresponds to the location of the same feature on the non deformed model and the stored post transformation. So what's stored in here would have to be absolute differences between the current time step or the current deformed state and the default model that we have learned, in, if this is in within a behavior, I think it'll be probably too much to ask to calculate this on the fly. So I was thinking of it more as like key frames that we store that are independent of a specific object. and these distortion key frames could also be used to recognize distorted objects independent of what behavioral sequence led up to it or if there was even any behavior that led up to it that we observed. and they would reduce the need for storing morphological states specific to objects. So we would, if we would have this solution, we don't have to learn key frames of specific objects anymore. like, we had discussed before, I have a question. Yeah. When thinking of a single column, how does a single column know how to integrate distortions in other places of like at different locations? If you can only observe one location on that behavior, like it would need to know the full behavior to know how to, like how does it come up with this cumulative map of distortions everywhere? And we were only just thinking of a single column. Oh. So this map, it, would take a, long time to learn this. So like you wouldn't observe everything at once. this would be, yeah, like learning a cup. You have to observe all of these locations, as they're distorting. And I one way I imagine this could be learned is that if you are doing smooth pursuit and tracking a feature as it moves, you try to have a constancy that this feature keeps getting moved to, keeps getting mapped to the same location in the objects reference frame. similar to how, we discussed maybe the thalamus learns that like as an object rotates, features should still be mapped to the same location, as in the original object model or Basically preserving this feature constancy. But yeah, you would have to learn this over time. Same as the behavior model where you can't observe all the moving parts at once, but you can have to see it many times. Would it maybe have like template behaviors? because my problem is, I'm thinking of this as, you observe a few features on the behavior and you're basically able to extrapolate what's gonna happen on different parts by just predicting or assuming you know that, oh, I see this is the scale behavior or this is that behavior. And then you can maybe see, say, okay, this location, assuming that my behavior is correct, this is my prediction, this is what's gonna happen. So I'm thinking, are these cumulative distortion maps maybe leading into, if we, cluster them somehow, or if we come up with a way to, categorize them that we can come up with template behaviors that we can apply and easily just infer and use. or are they just like going to be a different distortion map for every behavior? we just have to learn every single distortion on a, on, a map.

so those don't tell you anything about a behavior themselves. They just say how. The features of a model remap to new locations and orientations. So there's no temporal aspect to this, there's no sequence or anything, so I don't think it could replace or help with modeling object behaviors. it would replace storing key frames for objects, for specific objects. But, if I understand your question I don't think it could help with, learning behaviors in my mind. They're still describing behaviors.

like these distortions are basically, another way of describing what behavior has happened. or what is this about? that was the point I was trying to make. Roman, it doesn't feel like that to me. when I look at this, I don't feel like, oh, that's a, an object that's undergone some behavior necessarily. I do with the bent cup like this, but I don't do it with the conical cup.

yeah, maybe like you do get the feeling if you see this, that you imagine it bending at least either do. But might be because you associated this distortion with a behavioral sequence maybe. 'cause I've never seen, I've seen bent things, but I've never seen a bent cup like that. So again, as I was saying, if I, it's just because in this case I just don't see cups like this, but I do see something, things that looked bent like this. And so where the conical cup, maybe I, what if I've never saw a conical cup before? they're all cylindrical, but I have seen things that are made conical by, some behavioral sequence or something. I might, feel the same way about it. I, guess I'm gonna come back and lodge my general overall thoughts about this. It, this obviously seems like it would work, but when you thought through it, but I don't think it's, it feels just somehow, I don't think we're getting at the essence of what's, I think just we're confused. I'm confused. I don't think this is quite capturing what's going on. I'm, it's hard for me to even put my finger on it right now, what it is, but it just feels no, I don't think this is right. It just feels something, there's too many odd things, too many corner cases, which I don't know, it just feels like I'm struggling. I feel there's a simplest solution. or, or that's what it feels like. There's a, simplest thing that goes on somehow that I don't understand yet. That's what it feels like. So that's not a very strong objection. it's strong in my mind, but it's not very good argument. I don't like, it doesn't feel right. Yeah, maybe it feels a bit complex because it solves more than the original problem we set out to solve. maybe it's also, don't I, have to learn these. what are we calling these morph distortion maps. Distortion maps. First, I'd have to have a lot of 'em.

Because there could be a lot of different distortions. Yeah. Isn't that right? Yeah, that's right. And, then so gosh, how many distortion maps do I have? And, but would cut down on having to learn key frames of objects. But it seems like I do learn key frames of objects, at least many of 'em. again, if I've never seen a couple like this, I would agree with you. oh, I have to somehow infer that this is a, it is not something I recognize right away. but it seems like the general case is if I started seeing a bunch of cups like this, then I would say, oh yeah, I've got, that's my, I have a new model for that. I would, it's, the general case. We call 'em key frames or just, static, models of some sort.

but yeah, I don't know if we would need that many distortion models like we. It seems like we have models of very general distortions, but once you get to more complex stuff like a t crumpled t-shirt, we wouldn't have a specific model for that. Like you wouldn't have a model for every possible complex distortion, you would have models for bending and scaling in different But even bending, I'd have to have different ones at different points, at different amounts of bending, wouldn't I? Yeah. But I feel like that wouldn't be like too many models compared to how many, I don't know. I don't know. I don't know. It just doesn't feel quite right to me. it feels I don't how to put my finger on it. we made this transition once from thinking about objects, models as existing features at locations, and then we made this sort of important but subtle extension of that saying, oh no, it's really orientations at locations. And then we can assign specific features at those locations, but there's a morphological model, which is independent of the specific feature. That was a huge leap, conceptually, to get to that. And I think it's held up pretty well, even if we haven't maybe worked out all the details of it. But I think it's held up pretty well. I feel like a similar sort of conceptual step is required here somehow that. like in the end by the way, that, idea that we have this morphological model didn't add any really new complexity to the, our biological circuit. It was just like, oh, we had Minicolumns represent orientation. We've always had minicolumns. it's always, it fit. There was no real additional machinery to do that.

here I feel oh gosh, this feels like a lot of additional stuff and it just, I feel that, I feel there's gonna be a simpler answer to this. it's, a gut feeling. So I'll, I'm just gonna have to keep saying until I com become comfortable one way The other, yeah, I think that's a good point. That if we, have a morphological model that's basically just orientations at locations for logo on cup, that doesn't tell us anything about the idea id of the logo. So if we're storing that, like the child logo as a different location than orientations on this parent cup, then if the idea of the logo, there's suddenly a new logo on it changes that, just, that's not really changing, which Minicolumns would get activated here. But in that case, it would actually work quite well already. I think the thing that bothers me a bit is like the backwards connections of how we inform, like where we would expect to be on the logo. and maybe that's just a case of we need to work a bit more through the details of that part. maybe we don't need to add anything like distortion models and, this already works for it all, but again, at least to me, like making predictions about the child objects, that object then, if we have learned the parent on a location by location basis is still not super clear. Yeah, there's a lot of things that aren't clear to me. I'm still struggling. I keep going back to my egg and circle problem, and trying to see this, does this inform me about that? Like how do I even learn that basic morphology model is out of this? Is it egg a distorted circle or is it not? I, it feels like it isn't, but somehow, I don't know.

I'm just, I just feel this, it is, it's messy enough and confusing enough and enough missing pieces that I, my, my intuition strongly says we're not, there yet. There's something else. There's some conceptual idea that we're missing here. That's just how it feels to me.

yeah, it doesn't feel like it's a hard conceptual idea, just that we haven't thought of it yet. What you think of it seems easy, but, I guess have the question about the cute, like how big these behavior models would have to be. And this is something I've been thinking about in the back of my mind and I don't really, I've never brought it up, so I'm not sure if it's, Let's see if I can get over to where you're, if we had a simple situation like a stapler, oh yeah. It's really simple. We, okay, I'm sorry. six months. Six months. Trying to understand the stapler. simple in the sense that, this thing isn't inflating while we, so it's just a rotating thing, right? do we, would we really need like a key frame for every No, because it seems like you could have something like a vector field that can apply anywhere throughout that sequence. It's just that is the behavioral sequence, right? Isn't that Scott, isn't that the behavioral secret? That's the behavioral model. Behavioral model Is that, vector field? Yeah. And it's it only exists once. It's, there isn't like a but then, if, the staplers stops the po the, basic theory we have right now is the staplers stops, then you immediately start learning it as a, key frame is the poor term, but as long as you all understand what we're meaning by it. We learn this is a different morphological state of the stapler.

Is this touches a little bit back to what we were just talking about in Vivian's idea. Here is when I see the open stapler, I don't go, oh, to predict what I'm gonna see here, I have to imagine the flow through space. I don't have to do that. I've wonder what open stapler looks like and that's, that, that's what it looks like, So it's like you do learn this flow field, but then if it stops, you start learning a morphological key frame if you want. I don't know. I'm sorry. Just reviewing what we've said doesn't mean it's Right, Yeah. But, so you, I think you is your point that you think it's too much to learn all that? no, not at all. I'm just thinking about sometimes we draw these behavior, maps, like they exist, like each one at each time point. It only relates to the locations that are currently in play on the state. okay. I guess that, is right. if, in some sense you drawn all these vectors in one plane, but, they wouldn't there be in one plane, they'd be in different slices of time. So maybe I would jump to jump ahead there. so this, isn't how we've been talking about it. We've been talking about each row of those arrows would be at a different point in time. It's the same model though, right? So in my, the way I've been thinking about it is, can't we just, if we have the whole flow field, it can apply every to any point in the sequence. So long as we like. Know which arrows to use based on what there, there is matter at this location. For example, at this, I can't, let me throw a counter example, which might throw a monkey wrench into that. Imagine the stapler goes halfway up, huh? A quarter way down. And then all the way up again.

So, it doesn't make a one smooth transition. It's if you can see my armor goes and that's just behavior and you can easily learn that. But now there are two points in time where the stapler is moving up in the same space, but there are different points in the sequence, right? And one point I'm gonna predict it's gonna come down again and another point I'm gonna pick it up again, but the flow field at that point is the same that sa, the arm of Sapa is moving to space at this location, at this speed. But I have to have a unique representation at, otherwise I wouldn't be able to predict if it's gonna go all the way up or if it's coming up back down again. So that's why you can't have this flow field in one space like that. It has to go through time because the representation for the each of those points is gonna be unique. It's like a sequence, a melody sequence where you have the repeating notes. You gotta, so words are conjoined in some way because let's say I let's say I. I'm just playing with a stapler, so I'm not passively observing this example where you okay. Where it goes like this and then, it's got like a, there's like a hysteresis problem. you need to remember it's not history. It's a, unique point in time problem. So let's say you're playing with the stapler, you're learning the stapler. Alright. I was describing a stapler had a behavior on its own. So that's, it wasn't like I'm doing it. The staple has this behavior on its own, but Okay, but now we're not doing that. Now we're playing with the staple. Yeah. So I'm just thinking, I'm learning what just the basic morphological rules of this hinge. So anytime, even then would, at any point in time then the hinge could go up and could go down, right? Yeah. So you don't know which way it's gonna go. I don't, but let's just say this is reversible. Like the arrows. Arrows are reversible. if we wanna move the stapler down, we just flip the arrow or the arrows or something. But either way I can learn this flow field by just two successive neighboring observations. So if it's here and then it's here in the next minute, I've just learned a little part of that flow field so I can build this vector field of possible motions. and that's a compact representation of the hole.

All the more, yeah, but I just, poked a few holes in it. Sky. I'm wondering if that, let's say that was in the base, that's the base model. But we could apply a second layer on top of it to, add in particular cases of how you might move through That case might move through those trajectories. Something that's like more compact than, than having to store like a behavioral key frame that's unique to every point in the sequence.

Try this here.

He might even learn a sequence of, not being able to hope isn't sidetracking too much, but it seemed like it might tie into talking about how dense our behavior models have to be, but Right. So, we can be too dense, Obviously. Yeah. Or we can't have so many too many points.

so this could be the base behavioral model that it's very parsimonious. It doesn't require much, and you learn through interaction. But afterwards, let's, I think we're for starters We're mixing two things here. And, we have to be just careful that we're not mixing two things. Okay. When I've been talking about up to now, we, when we're talking about the behavior of the stapler, always in my back of my mind, it's, like the stapler doing. Its on its own. It has its own behavior. that is, it's, we're not interacting with it. it's going through some behavioral sequence on its own. and that's quite different than an object that can move that doesn't do anything on its own and I move it up and down. those are really very different things and so we have to be careful. We have to solve both of those. Yeah.

and maybe we have, up to now, even though when we actually operate a stapler, we have to use our hands to do it. That's not how we've been thinking about it. As an example. I don't know about other people. I've been thinking about it automatically does its thing on its own and we're just observing it.

so if I have to interact with it, then we're getting into the sort of, maybe, we should start thinking about that because we have to do that, real estate, but don't move on their own. at least most of 'em don't. yeah. And that's a different problem. Or maybe it's a related problem. I don't know. I think that's under, goes under the actions in our actions and maybe causality and the model logic behaviors.

and I think that's another big topic that we need to have, come back to either like in the future research meeting or in open focus thing. I'm curious if Viviane can go over the issues that. Because I, I think I understood the model object behavior. like I, we went over this and I, I understand that, like how this could work, but now I'm curious like what the issue that she thought she saw it was. Yeah. Yeah. I'd be happy to go over that, only if, you think it's already too complex, then talking about the issue will not help. maybe you'll help, you'll make my case from me then, huh? Yeah, I think the mechanism itself is very clear. I guess I, I think where, it's unclear is like do we need this kind of, it's more of that sense instead of okay, like I can understand like the, composition of movement vectors to predict what feature to expect. yeah.

a good suggestion, Jose. Let's hear, Vivian's issue. Yeah. Okay. I'll try to be brief and then we can see if it's, anything we need to go down deeper.

as I was thinking through this some more, I realized that all of the examples I gave and played through only the one movement and that the whole thing doesn't work anymore if you then do a second movement and a third and a fourth movement because the, distort sheets. what do you mean? like the exploding logo? Is that what you mean? Or what, do you mean? Second movement, third movement. Oh, like the sensors moving se several times over the Oh, I see. So okay, so let me just show you. so here's again, the example. We're on the T and we're moving down to the P. and so we go to the distortion map. We move here, we retrieve the distortion and apply that, in addition. So then we correctly moved in the objects reference frame. We correctly predict the p. Now we do a second movement. So we are at the P and then we move to the n. so now in the behaviors reference frame, we here, we moving up, we retrieve this vector here, at that vector. And now you just look at the red one. If we now add that vector, we're overshooting. We're up here now because the location in our objects reference frame has already been compensated by the previous distortion, and now we're adding up distortions over time. so what we would basically need to do is we still move with the actual movement vector that our sensor's doing in the objects reference frame. So the green one here, we use this green vector. We, are at this location. and then we apply the transform that's stored.

to, to retrieve the correct feature, which is the n. this is the vector we apply, which is the green one, but we apply it from this location instead of this location. so that's, a little weird because yeah. So basically what we would need to do is when we move through the objects reference frame, we still use the correct, movement of the sensorimotor. We don't apply anything directly to the sensorimotor movement, but instead at every time step we, we take the location that we are at given the sensorimotor movement and temporarily retrieve a feature at a different location. But we are not actually updating our location in the reference frame. We are just retrieving a feature at a different location when we make a prediction.

so yeah, lemme just read this also. we keep track of the story location in the object reference frame and apply the movement to that every time. we use the actual movement vectors in the objects reference frame here, which is the green part, but invoke features stored at the offset locations.

I'm not sure how practical as this, the compensating movement would not be treated the same way as the sense movement. It would just be applied temporarily to retrieve the correct features, but not used for path integration over time.

Would another solution be to reset the cumulative distortion map every time we make a movement and then start building a new one? I feel like focus, yeah, we would, that would get into a lot of other problems and like then the distortions would be different and different locations. We'd have to learn a bunch of different distortion maps depending on how we move through the space. I think just temporarily, like basically just, using the distortions to retrieve a different feature would be the easiest way to solve it. So we have, we only store one distortion map and we can use that and we can move arbitrarily through that space, but we still retrieve the correct features. If I understand, I feel the memory replay, if we need to reset the cumulative thing, then I think just, going through the sequence of behaviors through memory replay is easier. No, we don't have to reset this at all. Basically we move through the objects reference frame, just using the movement of the sensorimotor the same way we did before. Nothing changes. we are not applying any extra movement commands from the distortion map. we are just moving through the objects reference frame, but then when it comes to predicting which feature we're gonna sense at that location, we need to take, we would need to take, So the, we're moving through the objects reference frame and through the, distortion reference frame at the same time. so both the green and like both this, the movements through both those, reference frames are in sync. Same thing happening here and here. So in both of them, we are at this location. At this location. Now we retrieve, the compensating thing, but that's not applied to change our location in the objects reference frame. It's just used to tell us which feature we should expect. So it just retrieves the feature at a different location.

so it seems like this one, yeah. Yeah. Is this one like combining? So when we do two steps in the sensorimotor, like one movement and another movement, we, instead of doing movement compensate, which will lead to, in the incorrect prediction of the features we do, we add up the movements of the sensors first, and then wherever it ends up it, we apply that one offset at that location. so this, yeah. If I see this move and is a, and this movement is B I'm just gonna label everything. So, c. D Yeah, before we were doing A plus C and then do a D and then D but now we're doing basic it, it looks like we're doing just adding up the, movements first and then doing a offset. Yeah. That, that would work too. But, I feel like that's more complicated than what I'm proposing. Like you would have to keep track of the movements, you would have to replay them after every next. So that would involve a lot more than basically just temporarily applying these. yeah, I mean you're definitely right that would work as well. It would, make the correct predictions if we do that, but yeah, to me it just seemed a bit more difficult. Okay. Okay. I thought you, it was this solution, so I'm not understanding your solution. Hopefully. Okay. I'm gonna try to read again, but, you can, but you can continue explaining, I thought this was your solution, which I was like, okay, I think this is the same thing as replay or, yeah, so you say it again. My solution, it comes out to whatever calculation this would come out to, but we are not replaying, we are just keeping track of the location in the objects reference frame, same as we all always do. So A plus B is. Basically moving us from here to here. But as we make the second movement, we don't have to repeat the first movement. We're already here in the objects reference frame. So we just apply the next movement to know the next location. Okay. Okay. But do you have to, does it require looking up a second location for the, features stored at the offset locations? I'm confused.

I'm just trying. Yeah. So basically I think what you wrote out here, Hojae is very helpful to explain this better. So the, what I'm describing as temporarily, applying the offset is basically not applying plus C in between in the sequence, but instead, as we do the first movement, we do A plus C and make a prediction, but only the A is, persistent in our location in the objects reference frame. So we stay at this location, which is off the object in the reference frame. We make the correct, but, I'm just trying to understand, is there two, you have to look up two things and you do, you, do you have to keep track of two points in the reference frame to make this where you currently are and some other object, the storage location or something like that? yes. You have to keep track of your location in the objects reference frame. Then you have to apply the offset to make a prediction about the feature. But you don't have to keep track of the offset. It's just at this time step, yeah. Like you don't have to keep track, but, but it, then, or there's, two reference frames. One for the, one for the displacement or something like that.

these two reference frames, that's the reference frame. Yeah. But that's a problem in its own right, isn't it?

we're asking columns to have a lot of reference frames. Yeah. Yeah. That's, actually my main concern with this is that now we have three reference frames and Yeah. I'm beginning, I was worried of just having two. Yeah. now three, it just seems too much to ask. or it seems unlikely. It's not impossible, but it seems unlikely, right? Yeah. So basically just to wrap it up, this is the last part I have, but mapping it on a cortical column, there would be two reference frames plus the behavior reference frame, which I'm not showing because it's independent of it. we would have the distortion model and the object model, separate reference frames. The distortion model would store like transformations that need to be applied, Orientation changes or location changes. and a location in that reference frame will be associate with a specific transformation. And then we would have to apply that transformation to the location in the optics reference frame to retrieve a different feature. So in our example, our location in that reference frame would be here, and that's what we would keep for the next time step. but the feature stored there is, in this case, nothing 'cause the logo doesn't exist there. we apply this kind of offset that we store in this model temporarily to retrieve whatever stored at this location, which is the p and then we move again. But the next movement will be applied starting from this location. in theory neurons could do things like this. it's asking a lot of 'em, but it could potentially like, I have to look up an offset in one set of column minicolumns or something, and then apply that offset to the location in another one. But when I think about like grid cells, I have no idea how you would do that. it's yeah, it seems complicated. I, again, we could implement it. This I'm sure the software, you guys could make this work in software. Yeah, in software. I think it would be pretty straightforward. But I agree. I don't know how this would be done with grid cells. to be honest, and I guess maybe just as a side note, I show it in one column, but maybe some problems would be simplified if it's, if we do this hierarchically where the distortion model is in the parent column, and then we can do some kind of Yeah. Transforms in the thalamus. But I'm not sure if I, yeah, I wrote, I'm not sure if this would solve any problems just to, throw out that possibility if you think this would solve any problems. So again, I'm always worried that, oh, this would work, we could implement it, but I'm always worried that it has consequences down the road that we like it just digging a hole and we're gonna have trouble getting out of it, in the future. 'cause it's gonna, try to make it work for the next thing and the next thing. So I'm gonna state again, I think this is a great proposal. Great to make us think about the problem. but I'm not on board on the solution yet. I feel like we're missing some things or a conceptual just thinking. It's more like we're missing, we're not thinking about the problem correctly. Not, that the solution is incorrect. It's more like we're not quite grasping the entirety of the problem yet. and this problem is really broad, right? We start off with. With morph. We started off with, object models and we'd split into morphological opponents and then the specific features. Then we did compositions, then we did distorted compositions. And then, now we have behaviors and there's all these, and then I'm still worrying about the egg in the circle, and, and, now we have these deformation. all these things have to be solved. I'm not, I just feel like, oh, it's something we're missing here. It's just, it's not all, it doesn't feel like, oh, this wraps it all up for me. Isn't that beautiful? It feels oh, it's thick and hammer and nails where everything see holes and patching things.

Yeah. Don't think it, yeah. I think to me, if we would, if this was the solution, it would solve a lot of problems. Like even problems we didn't try to solve, distorted objects, like recognizing distorted objects. But I agree, the solution still seems a bit too complex for it, and especially too complex to be implemented in grid cells or cor cough, yeah. Yeah. but yeah, I thought it was useful to just zoom out a bit and. Think about distorted objects in general. 'cause behaviors seem to be related to it, so maybe there's a general mechanism we can use for both. But yeah, I agree with all that. I think we need to zoom out a little bit further. I have trouble relating this particular problem and understanding in the broader context of all of our problems.

just like I was talking about, it's not clear to me. One is an object distorted. One, is it just another object?

and then's got you brought up the issue of oh, the staple really has two things. There's like a behavior that when we interact with it, but it could have a behavior on its own. And those are separate things. We we're trying to mixing these things together, in a way that feels like, makes me feel confused, eh, I don't think we got it quite right.

I wanna try to like, because I'm still also trying to understand the problem. I'm going to try to share, the other, the brainstorming atra.

so is this sharing? No. Should be, is this the one about from the brainstorming week? Yeah, I looked at that. yeah. but Viviane you took a lot out of that. One second. It's looking into a new one, right? yeah. Yeah. So this is a different ra. the link is also still on Slack. There's a. Two links for Excel. That's the second one. but so I, the object distortion, I think the most direct question that I was trying to answer was this, if we have behavior model and initial morphology model, what features should we expect in the morphology model after the behavior is finished and like implementation wise, like in software, like we can implement the, this dissolution map or, amp replay. I, I think both are doable in software.

and then, but I think like the question that maybe we were trying to get at that is more difficult is this one, can we use this independent of the object? It was learned and applied to another object. this is taken a little bit out of context, but basically if we have seen something change from cylindrical, the conical in a mug, and we see another object, like a completely different, I don't know, object that goes from that is in let's say linical form, but goes clinical then can we predict the features of that new object that goes clinical? Like basically how do we generalize, the, yeah, I guess that's the how do we generalize behaviors without having to Learn it again. And I'm not sure if we can, I guess that's my current position right now, but, yeah, so yeah, so this gamma map i, I drew there that, that would be independent of behaviors. It would just be a distortion. But, that, yeah, the general question is valid that, like I was wondering if we can apply these, distortions to new objects, if we have learned them on one object, then apply them to another. Same way as we can learn behaviors on one object and then apply them to another. So I think both the question that you stated, down there, was the question that I started out with how do we take a behavioral model and an initial morphology model, go through the behavior sequence and then make predictions about morphology? And in particular, how does that work if there was an object distortion in the behavior? and yeah, that's a nice summary of like we talked about hippocampal replay, using that for it and this proposal like the distortion maps, which both doable and software both might be difficult to do in biology. And then, yeah, this kind of scale map to me is. You mentioned taking it a step further in the sense that this now talks about a separate problem, like this doesn't solve the initial question, that we set out with anymore, this kind of extrapolates, and says that we can also use this for like static object distortions, where there no behavior happen, but now we see Ben Mark and we can still recognize it.

I'm not following all this, but I'm, gonna try to play back some of it just 'cause it seems like you had this idea back in the research, the offsite we had been like, oh, we can apply these.

then lemme start over again. Figure what I just said. the idea here is that on a location basis, we can, apply scale, we can apply orientation, and in a hierarchical fashion. And that seems really good. And that could explain some of these scale things, right? Like the skinny cup versus the non skinny cup, something like that.

and, I think that idea really is a good idea. Like we can do this on a point. And, getting back to Scott's concern about the models being too big, we don't have to store every point, 'cause the system has to extrapolate in between points. but it feels like this basic general mechanism on, at certain points we rec, we memorize, oh, for the, current scenario, we have some sort of orientation scale change.

I think that's what we're talking about. Was that what we were just talking about oj Yeah. Yeah. And then I guess this question that, is underneath that map is like we, we discussed this mechanism already, like learning things on location by location basis for a specific child parent object relationship, but pretty general. So I was wondering I agree. And so to me the, object, the specific object ID that's passed in layer four, layer three, it's always optional, right? It was optional in our morphology that you don't need it for the morphology model. And so I think that's always optional. So just like we have a morphology model, which is not specific to IDs, feature IDs. I think these, features too can be, the orientation scale can be not specific to particular child object IDs and the system should work. It's just, extending that idea.

So I, don't know why that, I'm not sure what the, why that doesn't apply work in all these scenarios. It seems like it somehow should work in all these scenarios. And it's not too much of a learning problem. 'cause we don't have to store every point. We don't have to store enough to extrapolate between them.

so I'm, sorry I wasn't following everything you were saying Hojae, but that seems to me, that seems like the essence of, how we're gonna solve a lot of problems. And I'm not sure where, it's not clear to me now where it fails. So maybe I need to think about it more, but maybe Viviane can or somebody can explain to me why I'm, what am I missing?

I'll give an example. what, where's the, is there like a stapler picture? Okay, there we go.

Coming, yeah. It's got below stapler. Yeah. so let's say we learn this opening. If I see a laptop, how do I apply this opening to the laptop? I guess that's alright. that's a behavioral model now, right? Yeah.

we, I thought we had that pretty well worked out. I, we have the idea that I. The behavioral model itself, the, how we've described it. I'm still with that. I don't mind having, I think having two models, one for one for morphology and one for behavior makes a lot of sense. and I think we can make that work.

but the behavioral model's independent of a specific, like it's independent of any specific, object. Its, yeah. So yeah. just starting basis, our behavioral model should work with multiple things because it doesn't really care about what, object is. And we then we, we spend a lot of time trying to predict what a specific novel situation, what you'd predict. And I, thought we had worked that out. I thought that was, I go back and think, if we did, then I don't recall. I guess my, for example, this like a, the stapler model, like just, we have to somehow map, say this is a closed position.

We have to somehow map this to this permission of the laptop and this to, like another, like towards the end of the laptop and, this is a location basis for the stapler. So how do we stretch it out for laptop That, okay. that's a separate question about scale.

we're gonna run outta time here soon. Here. Here's a parting thought, maybe I think about this. We started out even before the Thousand Brains Project was going on, when we first came up with the idea of, of using reference frames, we didn't understand the importance of orientation. We just started putting features at locations, and it was always about what features at this location, what features at that location. Maybe the way to think about this is that the actual features of IDs are secondary to everything that maybe the entire system can be understand by modeling morphology and orientation changes and scale changes independent of actual specific features. So our models of static objects, our models of behavior, all can be understood as things that are not tied to specific ideas specific.

often we in the world we don't see color. If it's dark, we don't, we can do these different modalities we do. It's sort like maybe the world is really structured as, edges and orientations and movements of edges and orientations and scale. I. Oh, as an afterthought, we can apply specific ideas to things that helps us out a lot. then we can recognize specific objects, specific flavors of these things. So Upton, I've always started with the specific objects and we're adding these more generic features to them. Maybe the way to think about the whole thing is to start with the generic features. Those just understand how do we understand behaviors in morphology, independent specific features, and then say, how do we apply that to specific features afterwards? It, that's like a mind shift a little bit. and we're doing that anyway, I think, but I, we never, I don't think I've ever rec, I've never stated that. I've never recognized it. I don't think anyone ever said that's what we're doing.

does that make sense to anybody at all? Does that, does that sound Yeah, I think that's a, I'm more about that and if that can solve some of our additional problems. I guess the first thing that comes to mind is that we still somehow want to make, want to be able to make predictions about features. if I, apply a behavior to a stapler, I'll be able to predict it's color, or the color I'm gonna sense at different locations correctly. Yeah. We still have to do that. We're not ignoring feature IDs. We're just saying the base model is independent feature IDs, just like we said, the base model of a face. Independent of specific feature id. 'cause I can see a face with using fruit, right? I can see a face drawn with dots. I can see a face, made of anything. because it's orientations and positions and scale and things like that. but in the end face, the noses and ears and eyes really do matter. And, but we can add that later. And that's why, it's just a flipping of priorities. it's like saying, let's solve all the pro, I'm gonna, I'm gonna think about this, like how to solve all the problems we've talked about as a sort of abstract plat platonic, not platonic, that's the wrong word, but abstract ideas, of morphology and changing morphology, and everything. we've already added that for, we've already done that for, behaviors we've done added the morphology model. and maybe I'll just start thinking about everything in that regard and make sure we can solve everything using that mindset. Then we come back later and say, okay, how do I predict what's the actual feature I'm gonna see? how do I make predictions when I can make predictions? I can't always make predictions, but when I can make predictions, what would it be? yeah, I guess just to add a bit more to this sta stapler and laptop example and like how to frame the open question, at least how it's in my mind.

I thought we already have solution, a full solution for the stapler and the laptop and making predictions about it. I thought we did too. The, the last missing piece was applying the movement vector to the child objects reference frame, and that solved everything, applying the behavior and recognizing it on a new object and making predictions on the new object. To me, the main missing piece was how that mechanism works on an object that's distorting with the, stapler. We don't have that problem because yeah, the whole child object, the whole top piece moves together and we can just change its location and orientation, but with something like the balloon that's, we can't do that. So that in my mind was the main remaining open question. but then you slid in, you were spent a lot of time today talking about, oh, how does the logo distort as the objects are starting and, yeah, because yeah, I guess I do, I thought that's might be related just generally object distortion. But the, change in attitude I'm suggesting right now is, don't worry about that yet. Yeah. Just not worry about it. In fact, I even pushed back a bit, can I really make a prediction about that? I'm not sure I can really make a prediction what that thing's gonna look like as it's distorting yeah. I think it's still, even if we don't make predictions about features, we still need to be able to deal with distortions in morphology. I agree with that. and that's, and to me, that brings up the egg, egg circle problem. yeah. And, and, all your cup examples and so on. I don't, I look what I'm proposing here is not a solution, it's just, it's maybe a little bit of a mindset. It's like saying, okay, let's deal with the balloon problem, but let's not worry about how I'm gonna predict what the logo looks like. maybe I need to predict, maybe I need to predict the distortion of any, the surface in general.

I think the reason they brought in the logo is because, in the, in some past meetings, an argument for the balloon was that you could just solve it with rough scale and, or generally like putting the logo on the balloon makes it easier to show the issue that you can generalize that, deformation to new objects, like to, you can apply the same deformation to the logo or to any new logo. And that's why I brought it in to, to show like the generalized, I fine, it was a good, exercise, but now I'm arguing that, now that we've gone through that, I think, I don't wanna go through that right now. I Okay. Yeah. I think if we all agree that like these properties need to be solved, then yeah, I think it, it would be much easier not to think about the, to me the balloon is, just a more weirder case of the egg and the oval. it's a more extreme case, But I, I think it was, it's a great exercise. We did everything we talked about today, it was great. it, but doesn't mean we, if it, if today's exercise means it gives us a, some time to thinking about these problems slightly differently, then maybe that's good. yeah, definitely gonna go away and think some more, it's such a weird thing now to think oh, it, this idea of reframing all these problems is just sort of morphology problems in a broad sense, is very, liberating in some ways. So maybe next week we can talk about eggs and ovals. Yeah. I was gonna say that I was just, and yesterday I was trying to write a little writeup about the nature of the world and the structure in the world and, Try to really get down to the base of what is actually going on in the world? what does the world consist of and how are we modeling it? and this is, this idea is really interesting, helpful to me to think about okay, let's think about the world in terms of, its basic morphology and changes in morphology.

'cause so much of the world is like that. So much of the things we see are like that. And then these specific details, like even things like color are not persistent, right? A green isn't always green. And so I can't say if it's this color, it's, a mean apple because in different real scenarios it isn't. but the shape is always the same.

I'm just, thinking out loud. So to keep everyone, I think it's a liberating way of thinking about it.

Okay. So maybe we can try to think about behaviors again, but without any features for location, but just in terms of, I guess think of it as a black and white world with just edges and Right. I think we had to put, as Viviane just said, Hojae, we had a pretty good solution for behaviors. Yeah. I think the challenge that Vivian's bringing up is I think the one of distortions, and I don't even think we have a good de definition of distortions. Because I was arguing earlier, but, so I think what we really wanna be able to do is let's think about the entire collection of things. We've come up with the Thousand Brains Project, thinking of us first starting up this, morphology based view of the world, and then how do we, and then, and solve things like distortions and behaviors and so on, which many of us restored, solved already. And then ask how do we fold back in specific behavioral IDs as we go?

so I, think we got behaviors pretty well. Pretty close. Yeah. Actually, maybe it was too much to bring into distorting behaviors when we don't even have a solution for distorted static objects. it might actually be like a separate problem.

yeah. yeah, I, one area that I'm particularly interested in here with this, behavior model is the distinction between the cumulative version and like the totally differential version. I'm not a fan of the cumulative version, right? Yeah. So I, we didn't, that was a new idea that Viviane brought up.

I, yeah, I think it came up last week as well. I'm also a fan of the differential version. It makes more sense to me in terms of like how you'd learn it, and it doesn't require necessarily having a, starting, like a supervised starting point. Now we go thing. I like the idea of a. It also seems to comport with, like if you asked me to sing a melody, I couldn't just start it anywhere. if you asked me to start halfway through, I'd probably have to go back to the beginning and then, that's, I think I like the idea of these differential models. So I, don't know, maybe that's like an area that, would be rich for future discussion. and, when you say differential models, do you mean like how we've been talking about behaviors be so far? Like change, like step by step? Yeah. Step by step application. Having to basically integrate through the whole model and, yeah. So I'm also a big fan of that. I, don't wanna anyone to think that I'm saying we should not do that anymore. this is definitely still how I'm thinking. We are learning behaviors and recognizing behaviors. And it in my mind that's totally independent of the distortion maps, which yeah. Are, and it's something on top component at all. They wouldn't replace a behavior model. they would just be there to, be able to remap locations in the morphology model to, to predict the correct features as it is distorting.

Okay. Yeah.

what would be helpful for me for or in general next week is if we can end with, Either a question that we can think about for next week if we can come up, or if we cannot really, say that concretely now to at least post on Slack relatively like not to say, I guess earlier, so that we can get a chance to think about it. Yeah. I think I'm gonna think somewhere about the egg and oval example. Okay. To just focus on morphology. Okay. Okay. And see if that can, if there's a simple solution to this kind of distortion. I think, the, whole idea of distortion isn't, is both unanswered, but un poorly defined. Yeah. so yeah. So I guess my question is what about the egg and oval? what? the egg and does the balloon, the bending coffee cup, the t-shirt, these are all variations of distortion and I don't think we have a crisp definition of one something's distorted and one something isn't distorted. Oh, okay. The flavors of it. So I think the problem that Vivian's been working on is still a good problem. I just don't think we have a clear clarity of what the problem is. we have some examples, but it's confusing if they're all the same or if they're different or they're on a scale. I don't know.

If I had to define it, in my mind it's defined as the same feature now appears at different locations and orientations in the objects reference frame. Okay. Okay. If we're just thinking about morphology models, then they could forget about features. We could say that there are changes in orientation at different points to the reference. in little cases. but Okay. And it, yeah. The, object parts of the object will now exist at different locations and orientation. But that's also, that also applies to behaviors, I could say behaviors do the same thing. Yeah. and I think that we're talking about defamations of something different behavior or, flavor or something different than behaviors. Yeah. So I don't think that's, I don't think that's a good enough definition yet. Maybe it is. I don't know. I need a definition which, clarifies all these things and says, oh yeah, this is, here's how they're related. This is A and B and C. These are all, they'll fit into some scheme.

okay. I think, Hojae, we could say we need to spend more time thinking about deformation. We need a better def. I think we got bigger just through our proposal definition. I'm going to see if I can improve upon that. other people can do that too. We can come up with more examples of deformation so we can have a more of a spectrum of things. Maybe we're missing some. Good examples that would help us clarify things.

to me that would be the most important thing is to have a better definition of what we, what are the set of problems that deformations relate to? And behaviors could be in that spectrum, right? Behaviors could be deformation that occur over time versus defamations that don't occur over time. I don't know. Yeah. Okay. Alright. That, that clarifies at least for me what to focus on for the week as much as we can. Yes. Sometimes it's really hard to do this.

Alright, are we done? Yeah, I think we're at a good point.