Okay. yeah. Should I start then and then we go through, the thoughts and question? Okay.

all right. Do you see my screen? Yeah. Okey dokey. okay, so first of all, what I wanna show builds on a couple of assumptions and things that we talked a lot about before. but just to state them explicitly, it assumes that we have two types of models in each column or learning. Module one is an object model, which models features at locations. that's what we currently have implemented in Monty. and then we have the behavior model, which uses the exact same mechanism, but it models changes at locations in a sequence. both model types have a unique reference frame for each model instance. So each object model has a unique reference frame and each behavior model has a unique reference frame and they're independent of each other. and both model types can be recognized at arbitrary location, orientation, and scale. By applying transforms to the incoming movement vectors. and like post dependent features as well. So yeah, basically rotating the incoming, movement and features by to transform them into that unique reference frame.

And then both model IDs and changes of those IDs can be associated hierarchically on a location by location basis. So the model ID and pose can become a feature at a location in the higher level object model. the logo can become a feature on the cup and then also model ID and post changes can become a change at a location in a higher level behavior model. Can you, just leave this screen up for a second while I read it? I don't want you to go on. Sure.

So this is one of the things I'm, gonna, we'll come back to perhaps is it wasn't clear to me that the, that the, behavioral model has to have a feedforward projection. And I think that's what your second bullet point here at the bottom is saying, or maybe not all ID and post change is that the behavioral model there?

so at the higher level, this would be the behavior model. So the, at the second level, we have the behavior model and that one only stores changes as by the not here. Yes. So that one only STA stores something if the. Model ID at the lower level changes. So for example, if the, logo suddenly changes from one, one logo to another, I have a, it's not necessarily behavior that's being passed up. It has, it just a change becomes, it's just a change in what is represented in the low level column. Okay. All right. it's if a feature changes. Alright. Alright. Those, two bullet points are the same thing then, right? They're basically saying, oh, you have a model ID pose and that model ID pose can change. yeah, it now it gets to the top two sides. Yeah. They're mirroring these two. So basically this is the main rule I'm following with this, that object models only store static features. I understand those two points. Yeah. And then I got this, it's just this ones at the bottom. You added some more words and I'm confused by what you're saying there. So now at the bottom one, I'm thinking of the higher level, learning module or column. and there, the first bullet is basically the object model at the higher level, which stores features, which can, in this case now be child object IDs. and then the second bullet corresponds to the behavior model at the second level in the hierarchy. And there the changes that it stores, don't just have to be changes of low level features, but they could also be changes of the child object ID or post. Okay. So so if the logo rotates or Right. Alright, so logo changes. It's a bit confusing. are you just trying to say there that, I've always did that. Anything can change in the model, anything. As part of a behavior, so the ID of the child could behave, change the orientation. If it can change its scale, could change its position, it can change. is this saying anything more than that or is this just saying you just spelling that out? I'm, it's not a that at all. It agrees. basically I'm just saying that we, learn, we can learn hierarchical models on a location by location basis. And how is, how, which information goes into which higher level model is dependent on whether it's changing or not changing?

I maybe it's, obvious. I need to take that again. Yeah. there was a lot of words in there and I, and that it wasn't obvious. That's what you're saying. That's just basically saying. It's, the same as the first two bulletin or almost the same, right? Yeah, exactly. Yeah. It's the same as that. But I just felt like it would be useful to state it again, because at least in my mind, I was a bit confused about that in the beginning. Trying to think about like, how do we as learn hierarchical behaviors, are the behaviors made up of beha other behaviors? but I feel like now it's pretty clear to me that no behaviors at higher levels are still just changes. And they could be changes in the object ID or the object orientation or changes in the behavior id. so it's not like it remains like a flow of behaviors hierarchically composed, but it's more like even at a higher level, it's behavior models are only changed. Yeah. Maybe like a clarifying point is, which yeah. Seems to be kinda what we're converging on is like behavior is not passed up as a feature. it's interesting because last week that was one of the things you proposed. You saw the behavioral model being passed into layer four and Yeah. So it is past, and I comment on that, but I, concluded that didn't make sense this last week. So are you saying you're also abandoned that? No, so I would still say that the behavior ID is passed up as a feature, but it becomes a feature on the higher level object model. Since it's a static feature, it's not something that's changing and that's how we assign behaviors to different locations on an object.

Okay. we'll come back to it, but I guess if a behavior can be a static ID, then it can also change. So I don't know. You could imagine the stapler goes from. It's opening, closing to its, yeah, on fire or something. but maybe I'll just go to the next slides because I show examples exactly of those things. These are, a lot of, these are a lot of words and yeah, that's the very text every slide. and it's hard from you. I think I understand this stuff pretty well, but I'm not on, the words are confusing to me. Yeah. Maybe I'll just go to the, concrete examples.

okay, so here's the basic mechanism again, just showing it to have the new way of showing it.

explained once, so we have a sensorimotor, the Sensorimotor movement and features, and those get transformed, from the body's reference frame into the objects reference frame. and then they go into the lower level learning module or column. and the movement moves you then through the objects reference frame. So you're like, you might be thinking you are in the middle of the logo and then the movement arrives and now you expect to be, I don't know, up here. And then the feature is then either laid down here when you're learning or compared to what's stored there.

and that way you can do inference. So in Monte, for example, during inference, this transform, there are multiple different transforms. We apply, basically testing several hypotheses at the same time. saying, okay, the object might be in this orientation or that orientation. So I orient the movement, in two different ways and see which one matches, with my model. and then once we actually recognize the object, we can just apply only one transform and keep making predictions in that model's reference frame.

And so the model of the object, is features ad locations and Monte, this is locations in Euclidean space.

so I, see you, you abandoned the sort of a literal column interpretation here, right? This is what you're referring to earlier. like you don't show the thal of the layers of the cells and so on. Yeah, exactly. I don't know if that works for other people. I find it a little dis jarring because we've been looking at the other images for so long. Now I'm looking at this image and I have to interpret it. So I, I'm just giving you my personal opinion about that. Other people can speak up on their own. Maybe this is helpful for other people. For me, it's a little bit harder, but maybe just 'cause I'm so used to the other way. Yeah, I just tried to boil it down to the conceptual mechanisms that are happening. you could say that this little black.is the thalamus or is a relay cell in a thalamus. Yeah. So I got all that. I, so maybe, it'd be interesting to hear from other people who are not as familiar with the neuroscience, whether this is easier. I just wanted to point out that, oh, it's not making easier for me, but it's okay. I can do with it. It's easier for me. Yeah. I think it's pretty clear, but, all right. That's good.

Yeah, I guess I'm trying to make it a bit more esque right now, to, draw that comparison. I got it. But yeah, we can switch between the two. I still try to keep the spatial arrangement the same, that this is a, like the object orientation is down in layer six and Yeah, the, I'll, stop. But, to me, like the column diagram is like a circuit diagram, right? it's like looking at a chip, you can see where the information flows, and that's helpful For me, this is much more conceptual. There's blobs and things are just relating to each other without real connections. but I'm good. If it helps other people, then we should keep doing it.

I, think it helps from the perspective of describing what is, but it is absent of all the constraints that help to inform any feature discussion of what ought to be.

so the purpose of. it's missing constraints for the purpose of, exploring alternatives, but with the constraints being absent, it's straightforward to explain what is. Okay. That's good that, that's a good input then. Yeah, and I'm not saying we should keep this way of showing it, for the future when we brainstorm and stuff. I was just trying to simplify it a bit to explain the mechanism.

but yeah, I guess one, simplification I, made here is that instead of showing the object model as associative connections between layer, foreign layer six, I just showed it as an object model, which is maybe a bit more, how it is in Monte, where it's like a kind of a point cloud of, feed feature points at locations in 3D space, or, graph so yeah, basically this blue box would in the, circuit diagrams be this associative connection between locations and features.

yeah. So then, if we take two of those learning modules or columns and we stack them hierarchically, we have lower level one and higher level one. first of all, something that we assume is that they are co-located in the space. So both of their sensorimotor patches are at the same location. It's just that the high level one has a larger receptive field.

so both of these actually can get raw sensory, input and movement information, and they move together through space. but then since it's a higher level, column, it also can get input from the lower level learning module or column. and there the input is basically the pooled object id. So it doesn't get any information about this object model that is, learned here. It doesn't get any information about that structural graph or point cloud or, model that was learned. It just gets information about this object ID and this object ID then can become a feature on this object. So we would then, for example, say, okay, here and here at these locations, the logo exists on the cup. The logo is a feature on the cup at this, at these locations. and then additionally we wanna tell it the orientation of the logo relative to the cup. So we can take the orientation of the logo and the orientation of the cup and calculate the relative orientation and also store it at these locations. So each of these points in the model of the cup would then store. Okay. At this location, there is a cup feature and, that also has a relative orientation.

and this is in Monte today. So object ID and pose are the, CMP output of a learning module, and they become the input of the next learning module, and as they become the input of the next learning module. They get processed exactly the same way as this input gets processed, which means that the object ID then becomes a feature adipose in the higher level learning module. and so this is how Monty is today.

that really helps me. That was really good. Cool.

and then one thing that's not yet in Monty, but on the roadmap for kind of completing modeling compositional objects in Monty is, the feedback signal, which is, if this column has recognized the TBP cup, it wants to tell this column to expect the logo at a certain orientation and to expect on a certain location on the logo. So basically, here, come here, we get again, the constraints of the neuroscience that come in. If you are just coming from an engineering perspective, you might think of a different solution. But, basically how we think this happens in the neocortex is that this, TBP cup, learning module that recognized it or column, can send a feedback signal, which is like a context signal for the lower learning module, which this one can then associate with the logo and the location on that logo. and, the post to expect the logo at. So basically, this one doesn't know anything about the logo itself. It doesn't know the structure of the logo or where on the logo, this column here is. It, just knows I'm detecting a cup, and so then this, location on the cup, which is unique to the cup can be sent down and then this one can associate that. Okay, whenever I'm getting this specific signal from the high level learning module. I am at this location on this logo. It's like a coincidence thing that always happens. Whenever I got this input, I am here and I'm sensing this. And so that can then when, this one hasn't recognized the logo yet, bias it to, to recognize that I think you said this, but it's worth reiterating. I think that this, feedback is not for the cup, it's for a specific location on the cup. And so in this particular case, the patch in the higher level module would be actually on the logo area of the cup. And then, it says you should be seeing a logo in the lower region. it's because at one point you said, oh, it's context for your, but it's that feedback changes. Yeah. The location, That's a good point. Yeah. I should have drawn it a more explicit dot. So basically, Yeah. That there's a location that we are on the cup and then that biases a specific location on the logo. And since the two, receptive fields of those two are co-located, those will always appear together. when the sensorimotor goes over, over the cup so they can form a, this one can just associate, okay, whenever this column sends that particular location, I don't know what it means. I don't have a model of the cup, but whenever this column sends this, I am at this location and orientation on the logo.

it's not just like voting where it's oh yeah, whenever you sense the cup, I, sense the logo kind of thing. Another way to put that, if you were looking at a different part of the cup, maybe on the bottom or the handle area, it wouldn't predict the logo at all. It wouldn't, say anybody. Only when your eyes move or your sensorimotor moves to the area where the logo actually is, then the low level learning module says, oh, there should be a logo.

Yeah. and I guess, yeah, one thing we might implement when we're adding this to mirror the anatomy is columns that this one might not be exactly co-located with. It can still more generally predict the presence of a logo, and that would be the equivalent of the L one, connection, the, spreading axon, terminal point. so that you, in the context of this diagram, you're both predicting a specific location in a specific low level learning module, but you're also saying, okay, I, I'm a high level column, so I see a wide receptive field. A lot of these other columns. That are lower down should be seeing the same object since I'm seeing the subject or, since I'm at this location, rather.

Yeah, it's funny that spread in layer one that you just referred to, it's not something, it's actually super required. It's something we just observe and we can explain. But, if I would design this from scratch, I wouldn't have, maybe I wouldn't have predicted that. I wouldn't say, yeah, I just need to know point by point what should be predicted. but it definitely would help, with noisy input and the obs obscured and, in voting and all that kind of stuff. the key point is that on a point by point basis, as your sensorimotor moves on the cup, it predicts different things in the lower level learning module. Yeah. And, I assume, like how wide that spread on you, L one has an influence on columns. Can depend on how predictive the signal is for a particular object. So if the logo's rather small, then that spread might be small because the further away columns, it's not predictive for those. So they wouldn't form a connection to, the signal. But if it is like bigger logo, then it might have, more, columns that further around there that would connect to the signal. the physical sp would be the same, but which synapses are formed. Yeah. It would only come synapses if there was, core.

Yeah. is there, are, we associating a single position in the higher level column with multiple positions, the lower level column? Or is it because we, have only a single position for the logo in the higher level column, if I understand it correctly. but these are sending feedback to, it will be sending also to a single position, the reference frame of the logo in the lower level. So is it, are we associated with multiple positions in the lower level or also a single one? Just a single one. So it'd be one location on the cup associated with one location on the logo. but that would normally happen multiple times.

yeah. So there wouldn't just be like a single association, but when an association's formed, it's between one location in each reference, right? Yeah. So same as the logo exists at multiple locations on the cup, each of those locations on the cup can then also have a backward connection here and bias different locations on the logo, but each connection itself would be a one-to-one connection. And yeah. Oh, sorry, go ahead, Romy. I was just saying how, does it bias multiple locations on the logo if, wouldn't it need to have associative connections to multiple locations on the logo and the lower level to be able to bias the different locations? But that's the thing. We only want it to bias one location in the logo. There's just a, that's, so it's a one-to-one, mapping. if we're at a particular location on the mug, then we're at a particular location on the logo.

and then in case this helps, like in terms of how many of those one-to-one connections we form between them, it depends on how complex the compositional relationship is. So if it's something really simple like a small logo on a flat surface or something, it might just be a couple connections or sufficient to describe that relationship. But if it's like the logo with a 90 degree bend or something like that, then you need more associations to know, given I am on the mug, where am I in the logo space? And also what is the orientation of the logo or the child object. So maybe a follow up question as we move with the sensorimotor on the, in the lower level, reference frame. Does the location in the higher level reference frame change, or does it stay the same? It changes. I mean you're, the sensor's moving in both reference frames. Okay. the only complication, which, we could try to avoid for now is that if, the two columns have different, inherent scales, like the size of the locations are bigger in one than the other, then it's not exact one-to-one match. But if you just assume that they're the same, then as you move the sensorimotor it moves in both the logo, the lower reference frame, and the upper reference frame the same. they're co-located constantly. Got it. Thanks. yeah. Try to show that a bit with those, like the pink errors, basically, they both get the same movement input 'cause they're moving together. so if one changes the location, the other one will as well.

it might be useful. I don't know. I find this fascinating that. To think what you, when you look at something like this cup, your eyes are constantly moving around and attending to different things, and every time you move around, it makes a prediction. What should be there. you would be like, oh, there should be the logo, and now there should be the word thousand. And by focus on thousand, it predicts it should be an A. So you're just, you're constantly boun bouncing around between objects and their features, objects and their features, and it feels seamless to you. It feels like you're just looking at this cup. But in reality is you're, constantly looking at, these, the hierarchical arrangement between features and a large object features and a large object. And it could be like, it's, just feels, it's just a seamless, oh, you, say, oh, I just know there's a logo there, but you'll know there's a logo there because your eyes move to that location, or you're anticipating moving to that location. Then, there's a logo. I just, I find this process fascinating in terms of this, your mental thoughts, what's actually going on as you're moving your sensors. it feels like one thing, but it's actually bouncing around all the time between these two different models.

Yeah.

yeah. Any other questions on this one?

Okay. then not two behaviors. I don't have to go into too much detail again, but, it's basically the same image here, but the big changes that, we get, movement and changes instead of movement and static features as input. So whenever the sensor's actually detecting a change, that gets routed into the behaviors reference frame. And then we store changes at locations. And I did show this in this diagram 'cause I didn't wanna make it more complex, is that there's also like a temporal component. So that's indicated by this being an animation. So there are changes. At different locations, but also at different points in a sequence. just to be here to, be clear here, the, the fact that it's already a change, like the difference computation occurs pretty far upstream, like at or at the level of the thalamus, right? There's no difference being calculated to at the level of the cortex or no?

yeah, I think it's a, bit mixed evidence like where, for example, direction selectivity emerges like in some species there, there's direction selectivity very early on, even retinal ganglion cells. But then also like layer, I think four B alphas also, either extracting or getting direct directional selective input.

but I think it's. It's generally quite early on. So I would, put it in the thalamus as like a general rule of thumb. I would, I, I'd say even earlier, if you go back to the idea that there's these, two basic types of outputs of the retina, the cellular, and the magnocellular.

and they have different response properties. one responds quickly and one responds slowly. so one could argue that, the quick responding ones are detecting chains and the slow ones only become active when the, when something's not changing, it becomes static. So the idea you're the center on center surround receptor fields coming out of the retina, they're divided into zones that are best detecting movement and ones that are best detecting, static patterns. It occurs there. And then in the cortex is where it gets converted into, oh, this is an edge moving in this direction, or something like that.

Yeah, I think that's a, maybe a way of putting it is that like some of the input is better suited for one versus the other. But then the place, like you said, where it gets actually a bit more interpreted as this is moving in this direction versus that direction, is then like combining a set of center surround cells.

hey, this is Jamie. He's about to go to school. Hey Jamie. Bye bye. Have fun at school. I do not remember gonna school that young.

Okay. that makes sense to me.

yeah, it makes sense to. So I guess what I was trying to think of is like the feature is like a little bit higher level than something that's just motion, at some point a feature. it's just a little more complicated than, just, I don't know if this is important to, the discussion today, but the way I view this is you start off with these raw, these very simple, imagine these centers round fields. Some are detecting changes and some are detecting, static, things. And these are just any a center round receptor field. It just detects anything that's not uniform, that's what they respond to, anything that's not uniform. And so then this is sent to the cortex, the thalamus. There's very little evidence. The thalamus does any transformation on this, so it gets to the cortex and very quickly it's converted into things like minicolumns that represent edges or moving edges or things like that. So that would be an output of a spatial pooler. So you start off with raw bits, like changing bits and static bits, meaning these things are, there's some sort of change occurring at this point, or there's some sort of a non-uniformity that's changing, or there's some non-uniformity. And then in the cortex it figures out what to represent. it's not clear to me that the idea that there is, directional sensitivity that's determined by the cortex. it's not determined by the retina or the thalamus.

Yeah. And then maybe getting, and I'm not sure if that's where you're getting it, Scott, with your question, but I think if you're thinking of extracting change at a higher level, like if the object ID changes or object orientation changes, that, that could be using the exact same mechanism of basically in the cortex detecting small temporal offsets of where a pattern changes. and yeah, basically the same mechanism you use with the on off, cells, you.

Jamie sing. Are you gonna stay with grandpa? No, he's saying goodbye. I'm skipper. I'm not grandpa, I'm skipper. Oh, okay. Bye Jamie.

But yeah, does, that make sense, Scott? Yeah, that makes sense. I have some kind of thoughts or questions about, the memory this, of things that receive, magnocellular versus parvocellular, but there's not really pertinent to this, conversation, so I'll just hang on to it.

Yeah, it's def it's also something I wanna read a bit more into and talk a bit more about. So maybe, at a future meeting or later we can go into more depth on that. Yeah, in general, I have a few kind of backlog of a few papers on optic flow and movement detection that I've been meaning to read. it could be an interesting topic for a kind of a session. yeah. I also read a couple, on that last week, so I could also, I should probably send a brief summary in what I learned this week, last week.

but yeah, anyways, back to the general mechanism, somehow we are extracting this different movement information and, storing these changes, in the reference frame of the object behavior and the same way. We convert, movement and features in from the body's reference frame into the, models reference frame. We do this, here to get into the behaviors reference frame, by applying the behavior orientation, to that input.

And so if we were to implement this in Monty, it would be pretty analogous to what we do for object models. Basically, we'll be laying point down points at locations. so like we call it graphs right now, but it's really more point clouds. 'cause we, don't use edges right now. but basically instead of at those, points in the graph, or points in that reference frame, instead of storing static features like color and whatnot, we would store changes at those locations and then we would need, another dimension, ba basically for time. so storing like a, graph of changes for each step in a temporal sequence.

Does that, that make sense?

Okay. Now, how does, ways of, thinking about this, solves, particular problems we've been talking about? so now I'm just going into the, new solutions I presented last week. the previous ones were included in the last slides already, but basically, one of them is we wanna be able to associate objects and behaviors. So if I see a stapler, I wanna know that it has a certain behavior. and also if I see a behavior that might bias, what object I think I'm perceiving. so a proposal I made last week is that we can associate those, by using hierarchy. So we have, in this example, a behavior model in the lower level and an object model in the higher level. here we have a hinge behavior in the stapler. And then this hinge id, the behavior id, is one of the outputs of this column that becomes a feature in this object model. So we would then do this again on a location by location basis. So each of these, circuit locations on the staplers could be, associated with this particular behavior, as a feature at that location. plus the orientation of the behavior, the relative orientation to the stapler using exactly the same mechanism that we would use if this was, like a child object. It's just a child behavior. It's just, again, a feature at locations and relative orientations at those locations.

Doesn't that make sense? What, does this model give? Because the other thing we've discussed is where it's the reverse, where the behavior model exists, the higher level and the. Morphological model is at the lower level? I think so that way around, I was proposing as a solution for predicting morphology, but this way around assigning behaviors to objects, because like we mentioned before, usually behaviors like the object is the larger thing. And the behavior applies to either the whole part of the whole object or parts of the object. It, the behavior is never bigger than the object itself. So it makes sense that it is the child of the object model.

are we, but I guess in, in this instance, why does, it's not clear to me that it needs to be across columns or yeah. Oh, because of the relative orientation, calculation here. I think that makes it easier and because we wanna do it on a patient basis. So is this an okay time to delve into more complex, confusing topics or do you. I guess because I have a, an issue with this too, and I, haven't, I wrote up this alternate example which I could talk about, which suggests maybe that the behavior isn't in the lower, column. Is it, should I reserve that to later or should I dive into it now?

yeah, if you, wanna, bring, okay. Alright. So let me, in the little writeup I put last night, I gave an alternate example. I said, imagine we had a cup with logo on it and the logo rotated. Okay. So it's just like we had before, but now the logo rotates. Behavior is magically, the, logo on the cup rotates some 30 degrees and then it rotates back.

this, in this example, the, it's different than the staple. In the staple, we have the confusion where parts of the stapler are moving and parts are not, in this case. The thing that's changing is just the logo. The logo itself isn't going under any behavior. it's just presented at different positions relative to the cup. So now the question is, okay, I can learn this. Where is the behavioral model of the logo rotating on the cup? Is it part of the logo or is it part of the cup?

if I were to say it's part of the logo and, it's, so like the logo could be just rotating on its own or, and I would just compensate for it to the, to that, to our, to Phil mechanism we have. And I would, wouldn't, I wouldn't necessarily say that's behavior. I would just say that the Rogo Hass got a different orientation now, so there's no behavior associated with it. yeah, I, think I'll have a slide on that, not with this specific example, but, basically. Since in that case, the relative orientation is changing, this would become an input to the behavior model here. So we have the but, the question is, would there be a behavioral model in the lower level column? No. Alright. So there's an example where there isn't a behavioral model, not relying on one at least, in the low level column. And then I asked myself, could that be the, way to think about all these situations? And, then I said to myself, if I think about the stapler, imagine the stapler is like the cup except the top of the stapler is the logo and the bottom of the staple is the cup. Okay. So now I have a part, that's a, separate component that's moving relative to the other components. it's not self-contained on the cup like the logo is, but, or, but it's one, there's one part that's stable or one part that's moving. And so that's very analogous to the cup on the logo. And, then I said, do I, do I need a a, behavioral model in the lower level, a mo learning module? Yeah. I said I don't think I do. I would say in that it's a bit of a different example because the logos, on the cup is already a compositional object. So we already need composition to model that. And then you have the child object change relative to the parent. So if I would draw this up, I would probably draw three levels if I would wanna show it all. But, I would still say at the lowest level, there is a behavior, which is just the local optic flow that's being detected by the sensorimotor. but it doesn't say any, it's not related to the child object itself, changing its orientation. It's just, but, is that a learned behavior or is that just a moving object? It's just okay, so the, so I have an object that's moving. Okay, it's rotating, but is that a behavior? Is that gonna associate with something. Yeah, I would still say like that's a general behavior model of how, bits are changing if something is rotating that, that could be, although I was working on the, alternate hypothesis, and I don't know which is right. The alternate hypothesis is that the logo, the staple example is confusing because it, we have the same object. It's parts moving and parts not moving. and the idea that the, logo and the cup could rotate is a simpler one. and I didn't feel like yet that I needed to introduce any kind of behavioral model associated with the logo. it's just that there's, it's a feature on the cup and, its position is changing, but the cup could learn that. and the stapler just the way to think about the stapler is imagine if immediately, as soon as I see the top moving, I isolate the top.

imagine I have a, morphology model. The stapler in the lower level column and the higher level column, they're both there. And as soon as it starts moving, I'm only gonna focus on the top of the stapler on the left column. So I, and now it's like a logo. It just, it's all I'm doing is a tend to the part that's moving and I ignoring the rest of it. So now it's just, I immediately, I split this into two parts. And on the upper column, I'm just, I'm only gonna be, I'm gonna be anchoring to the logo on the left column. excuse me. On the upper column I'm gonna be anchoring to the base of the stapler. On the left column I'm gonna be anchoring to the top of the stapler. And now I'm just back into the, it's just like the cup and the logo again. so I think this, the staple example was very difficult for us to get our heads around, but I think if I think of it the way I just described, it's pretty simple. Then, it's just like a logo changing on the cup. And then I asked myself, do I do, why do I need to have, Model of behavior in the lower column. It's just that the feature in the left column is, just changing its position relative to the, it's just, but the, I don't actually learn to lead the behavior there, to me, like why I put it like this hierarchically is because it seems so analogous to compositional objects because the parent object might have different behaviors at different locations and orientations relative to the parent. So like I had this animation of the mailbox where you can open, the door of the mailbox, you can put that little signpost indicator up and down. you can do it like one object can have many behaviors and the behaviors are in different locations and orientations on that object. So it seemed like the using the compositional object mechanism is like the perfect fit for it. I agree with that. I'm not abandoning, I'm not suggesting abandoning the compositional object mechanism, but Yeah, I was just gonna say, I think it might. Be possible to do it within a column because I can't right now come up with an example where at least as far as I remember, one of the big reasons why we model composition objects like that is because the child object might change is, orientation and scale midway. So we need to do this like very dynamically, but I don't, I'm not sure if there are good examples of where the behavior would change its orientation. I'm, not, I don't wanna abandon that. I think it's hierarchical. The question I'm asking is very specifically, we have these two models. We have the morphology model and the behavior model. And what I'm suggesting is, like in the logo on the cup example, we have we have a model of the logo and we have a model of a cup and we're positioning the logo on the cup in different positions and changing in orientations that we're keeping all of that. The question now is where is when, that relationship changes? Such as if the logo rotates, or the logo changes in some other way or some other feature, the cup changes. where is that? Where is the actual behavioral models that's describing that change? Stored? It could be on the, it could be associated with the left column, which case in the logo. Example would be, oh, logos rotate. And it's in a, there's a behavioral model of something rotating, or it could be on the upper, the higher learning module, which is saying, I have an object, and some part of it is rotating. Yeah. Some part of it's changing, which it feels like it has to be since it's relative to something. It's if it's just rotating then relative to what, so yeah. I, think, or Right. Go ahead. I was just finish up the thing I wrote last night was trying to get at the heart of this thing, which is that if I, can think about. Is it, This whole thing is just a series. We have composition objects. In fact, every object we've ever created in the system is a composition object. it, sometimes the trial features are very simple, but they're all compositional objects. And now what behaviors are, is that the features that comprise the composition object can change in various waves. and they can change their id, they can change their orientation, they can change their position, locations and so on.

that could all be stored as part of a, I have a composition object called a stapler or a composition object called a cup with a logo. And the, and I now need to keep track of how the features change on that composition object. And that would've suggested in the right column, not the left column.

yeah, and for what it's worth, I think in terms of discussing hierarchy, it's also useful to think even the lowest level column. Is in a sense, a hierarchy relative to the incoming, information from the sensorimotor models. From the, kind of, I would say it's com cortical stuff. It's compositional, right? Yeah. But even in that case, it's compositional. It's like it's features at location. It just features are even simpler, say, I would say it's, composition. It's okay. Yeah. It's, but it's always compositional. So I think you could argue, like in the case of the stapler, like we can learn, a model in multiple levels of columns. You can learn the stapler kind of the inefficient way in terms of all of this local feature changes happening at many different locations. Or you can learn it and represent it more as a compositional object, where it's it's the top of the stapler that's moving. It's the same mechanism. One requires the, former requires more neural representations because it's lots of different changes, being represented.

Yeah. Yeah, that's a good way of phrasing it, I think. So I don't have a very strong opinion about this. I'm just pointing out that when I think about it, the way I've been thinking about it, it's not clear to me that we need to use a behavioral model in the, lower level column in this example. or I, it's, it might be, I just, it's not clear to me you have to do that. so I, that's all 'cause if, I have a behavioral model on the left side, it's gonna be behavioral model of the top of the stapler. 'cause that's the equivalent to the logo on the cup. The logo on the cup doesn't, yeah, it's just that I object on the cup. So the object itself has to have this behavior, the logo would have to have a behavior rotating or moving. Whereas I could describe that as, every time I see the logo, maybe I'm gonna imagine it rotating. But maybe every time I see a cup, I imagine the thing on the cup rotating. yeah, so, I think maybe actually the next slides will help a bit because I'm also not saying it needs to only rely on this there, we would still have a, behavior up here for the stapler, which is much more rich than this one down here. This behavior is really just local, like optic flow and, or like local bits of changes. Whereas the behavior in the top level column really represents oh, the top of the stapler is changing its location orientation now. so there can be two behavior models or there would be two behavior models, and at least how I'm thinking of it and, the top level would be more richer. I, get that. I understand it. I'm not necessarily disagreeing with it. I'm just questioning do we actually have to have the lower model, the behavior like that. would it be sufficient, that the behavioral model is only in the April model? You probably don't need a. I don't think I would say we need it in all cases. I guess I'm just trying to illustrate a mechanism, and maybe in this example we wouldn't need it if we have learned a behavior of the top moving relative to the stapler model. But I, think I would also find it helpful to just work through. Yeah. What, in terms of this representation, what that helps us predict. 'cause yeah, it's, not about predicting morphology, so it's presumably about predicting, like feature changes. yeah, so basically, so what this, I'm trying, what I'm trying to achieve with this is associating object and behavior. So this one I showed so far is basically learning. So we learn that, on this location, on the stapler, the hinge behavior exists. So then when we see the stapler. this kind of backward context signal can bias us down here to expect the hinge behavior so we know, oh, there's stapler. I can push it down and open it and close it without it actually moving at the, time. at least that's, yeah, the main benefit I see in it.

if, the morphology of the child object changes, in a way that we cannot represent it as change in orientation, with thalamic transformations that would require us to build behaviors in the lower level column. Like for example, if the top, bends or the top of the stapler bends or something like that, we cannot represent that with change in orientation relative to the higher level column. And we'd have to learn that as a behavior of the lower level, object.

a, an example of that would be, imagine you have the logo and every two seconds, two of the letters on the logo swap places. Okay. So there's a behavior of the logo.

and wherever we have a logo model, that behavior would have to be represented. So it, wouldn't be, you wouldn't be able to represent that as part of the, mug. It's 'cause it's not the mug. It's doing that. It's the, it's not a mug behavior, it's a logo behavior. I think that's what you're saying. yeah. Yeah. there, so I think that the transformations that, the, like a change in orientation, that's happening in the, Alam is it's, very limited in terms of what it can represent as a behavior in the higher level column. Like something that, that changes orientation. I put you, I, don't think the thal is learning anything along behaviors. I think the Thal is just doing a transformation of orientation. That's it is no capacity to learn. But these, transformations are, are very limited in terms of what kind of behavior we could learn. yeah, that's what I, disagree with that. We can learn anything, right? Can we learn any kind of change? I think that's what I'm trying to get at with that open question of like, how do we represent changes in locations instead of just changes in, relative orientations. So how, do we represent that locations of part of the object are changing?

by the way, I can't, it's, hard to imagine any example where the locations are not changing at the same time as orientation, like in the stapler top as it moves, locations on the staple top are moving to location, right? That transformation to the whole child object. So it's, when we apply a transformation in the Tels to a child object, it's applied to the whole object. as a, with this, child ID and everything. But if we, if we want to apply, but that only, that's only sometimes Romy remember the, there, there are, we came up with examples where, the, let's say the top of the stapler as it went up, it bent in the middle or something, or, you can come up with all kinds of weird examples where the, even a change of orientation isn't applied to the entire, com, feature. Yeah. But then that's one example, that's one example where it would need to be in the lower level column as a behavior. maybe I, it's, I'm confused by it, so I, I'm, more expressing like. I don't think we understand it very well because I can think of examples where I, it's not obvious, like this idea that the, this idea that the logo rotates on the cup. Is that really, is that a behavior of the logo or is it just oh, maybe the cups rotate things. I can make arguments both ways. Let's say every time I saw the Thousand Brains Project logo every time I saw it, it would, rotate every two seconds. then that, so that seems to be a behavior of the logo.

but relative to what, it's like, because, if I just rotate the logo, I can just compensate that with the thalamic mechanism. It's oh, it's just the objects in different position. it would be more like, just for what it's worth, behavior tells the thalamus how, when to compensate, like how to compensate. The rotation. Like you don't have to re recognize the orientation of the logo all the time, but instead the theus is directly told, okay, now the orientation should change and the speed.

I guess I'm just trying to understand what are the behavioral models in the left column and what are the behavioral models in the right column? It seems to me if there's a behavioral model in the left column, it's, gonna be fundamentally different than the one in the right column.

yeah. For what it's worth, the rotation and the IC transformation. If it's, I feel like it's helpful to also just clarify that like, although we are robust to it, like in variant to it, if you either rotate your head or you rotate the text, we can still read, but we are also aware of the change in orientation. you're aware that the line is at an angle or that your head is at an angle.

So maybe, I think it's not because the thalamus is, undoing it that it like, is invisible to us. like my head's tilted right now and I'm not aware that, I had to ask myself if my head tilted as it, I would say what you become aware is if that thing moves relative to you.

if, I move my head, I'm less aware of it, but if the object moves, I'm very aware of it.

it's oh look, some something's rotating here.

anyway, I, I'm not, I'm just countering points, not because I have any fundamental belief, Niels, I'm just, I think it's still confusing. It's, yeah, I agree. I agree. It's still hard to know, like where would you store that information? I, agree. That's confusing, but Yeah. at least the sort of the information I guess exists at some level. But it's if that's a learned behavior, how do you store that long term? That, is weird.

it's not the, behavior is not gonna be stored in the thumb. it'd be, it's just I can imagine that a of the logo has a behavior that whenever I see the logo, it rotates, it's rotating relative to something. So it'd be rotating to a background that, that isn't rotating or it's, just, it maybe there's some apparent object that it's just, that I, that's invisible in some sense. It's got a reference frame and the position of the logo in that reference frame is changing. it's just fitting relative to the body or relative to the sensorimotor?

but, maybe, it's, but it's really relative to itself because if I held the whole cup was rotated, then the logo would be at an angle and it would rotate, but not relative. The, amount of rotation wouldn't be relative to the, to my eye it would be relative to the cup or its certain location, but not, I feel like a rotation of the logo is. It doesn't require a parent object. It's the same as the way that I'm able to recognize any object in different orientations. it's the same thing, same mechanism, But, if I'm gonna, if I'm gonna say it's rotating, our definition of a behavioral model, behavioral object is you've got a reference frame, and then there are changes in that reference frame. So if I wanna have a, a logo that's rotating, I can't rotate the reference frame. I have to say, oh, the position of the logo in some reference frame is changing.

o otherwise I'm just inferring it at different angles. It's, I can recognize the logo at different angles. That's not a behavior. It's somehow it has to move relative to something. Like I need to have a reference frame that says, oh, the logo is now in its rotated position and now it's in its UNT rotate position. Yeah. May maybe the next slide will actually, maybe I can move on to the next slide and it might help with some of these things. Yeah, maybe so because I'm talking now about actually relative orientation changes.

and now also about, behavior models in the high level column. So ba basically, if the stapler opens, it's it's a child object that's changing its orientation. I know it's also changing locations, and it's a bit more complex than the logo in the cup. So maybe I should have used that, example. But we talked more. We could just say it's like the logo in the cup. It's alright. but basically we have, the hinge orientation changes relative to the stapler. So the difference between the orientation of the stapler and the orientation of the stapler top. It's now changing. So previously when we just learned a compositional object, we just had a relative orientation and we would just store this in the save the model. But now that it's changing, this era goes into the behavior model reference frame. So now we lay down that change as a point in that reference frame. and so now this kind of head level hinge behavior, or whatever you would call it, represents the sequence of changes in relative orientations between, the stapler and the stapler top.

I'm, I confused. Are you, we don't, we're not, it's not the tabletop orientation is changing. it's on a location by location basis, isn't it? It's like I have to look at the orientation at each location.

it's not like I can apply rotation of the entire top. Yeah. Yeah. So I'm thinking of one time step the sensors at a certain location. that's why in this behavior reference frame, there are multiple points that are being laid down, but Right. it's just illustrating. So technically here, as it rotates the location, all the locations are changing. and if I go to one of the new locations, it's, got an orientation, which was different than what a, what that same equivalent was before. But yeah, guess so. I'm, struggling with this idea that we can separate out orientation at all. It's, just it's an orientation, each location and whatever it is, that's what it is, yeah. But the orientation at a given point in time will be common to the child object.

it, assuming we're attending to things that are moving together. But, we, one of the big insight with the competitional models is that we can't, we have to do it on a point by point basis. that was the big insight. And we can interpretate in between points, but we can't, assign an orientation to a whole component, a whole set of things. It's like it's on a point by point basis. We have an id, we have an orientation, But it's easier when, like the logo on the mug or the notebook or whatever can be learned much easier if it is a common orientation. But that's just because we can interpretate between points. it, and I don't need as many points, but it is still point by point. But also if they rotate in different ways now, like if they, if parts rotate clockwise and part clockwise, we would segment this into different child objects.

yes, but the, But I'll say it again there, we, the be there isn't an orientation applied to the entire child object. There's no way the brain would know that. It just goes on a point by point basis. There's this idea of a child and it's at this orientation, at this location.

that was the, big insight of the compositional models. I, so yeah, so I'm, I don't think what I'm showing here is against that. It's basically saying we have a relative orientation between these two. Previously we would store that in the object model at a location, but now this is actually, there's actually a change being observed. So instead of storing that relative orientation at a location here, we store that relative orientation change at a location here. And then what was, where was the second one?

Oh, and on the, hint, the one that ENT. you moved your cursor. I couldn't see where you moved it. Okay. Oh, so sorry. I don't know if, yeah, I got it. I got it. yeah, so, that relative orientation change could be different at a different location in that, behavior reference frame, but it would be a different location. Oh, sorry. It would be at a different location. I just don't see you can, I don't think you can separate location and orientation out the, you can't, say the orientation's changing. It's any point can have a, the behavior is at any point can have a, a can have a new ID and a new orientation. any point.

I agree with that. So we're, so this idea that we're we changing the orientation at the top? Yes. That's you. And I would say that mechanistically, we can't say that mechanistically, we would say, I. The system's gonna learn some points and it can interpolate between them. Yeah. So I'm not talking about changing the whole orientation of the whole object down here. I'm talking about laying down one point in this reference frame, associating with change orientation at one location. Okay. the problem is the language we sometimes use is oh, we're changing the orientation of the top of the stapler. which implies that the orientation applies to the entire top of the stapler. Logically in our heads, that makes sense, but that's not what's going on.

assumption just reacting to that and then the idea that somehow the orientation we can store change the orientations at some location doesn't make sense to me. It, basically, we have, you can't separate out orientation change and location change. There. There's, no way you can do that. I don't see how, you could do that. I, is it maybe worth just clarifying again what we mean by location changes and Yeah. The example of something where like it can just move up. So because yeah, these two parts of an object can be, I'm right now explicitly not talking about child objects moving location relative to the parent object because that's the big issue that I pointed out last time and that, I'll try to propose a solution for in the later slides. But I, I'm explicitly leaving it out here because I'm not sure what the solution is to that part.

so when you say change at locations, do you just mean look different locations in this, behavior reference frame have different changes in orientation, Stuart? Or do you mean. whenever something changes orientation, it will also have a movement associated with it through space. Yeah. Like you could argue this logo could rotate or on a point, or it could go up and down in some sense when it's rotating, there is changes in location, but it's not like the location of the child object is, moving. It, this again, I think we're, this is the big insight. When we started looking at the logo on the cup, that was change. It had a different orientation in the middle or different shapes. is that, you can't think of it as a single thing. It's just a point by point thing and you interpret it in between. So in this case, when I think about the behavioral model, imagine I have the behavioral, the, I have a morphology model and and then a moment later the morphology model is changed. 'cause of behavior, right? the actual positions of the features and the orientations in time one is different than positions of features and orientations in time. Two.

there you and I could say, oh, that's the top of the staple, top moving, or whatever. It, doesn't know that. It's just a set of morphology descriptions that flow through time. And, then what we wanna do is learn a behavioral model that says, I don't know the specific, details of what's going on here, but I can predict how the morphology model would change. I can predict what would be the new morphology model, if I had to predict one, and I'm gonna use the behavioral model to do that.

it's, just, I'm trying to get very abstract here. There is no concept at all in the high level model of. There's no assumptions about what the child objects will do or what they are or how they're gonna move. It doesn't know anything about that. It just says, I have an id, I have an orientation at this location at this time.

and, the fact that there is a, child object that has its own reference frame and it's rotating, we'll take advantage of that. But the parent object doesn't know anything about that. it's, not, it doesn't know that the top of its stapler is rotating. It just says, I, in a series of times, I have different, morphology models and I'm trying to learn how to close between them. Yeah, These language, this language is so hard to use.

I think I'm agreeing with everything you're saying. I guess just to double check, we are still communicating the orientation of the child object here. I, no, we're not. we're communicating the orientation of a child object at one location. It's not the object itself, it's just at some point where I'm focusing or attending to on the top of the stapler, what is the orientation at that point, and what is the location at that point? yeah, but it's still the, global orientation that this lower level column applies to the movement input it receives to move through it. That's, that, yeah. that is under the assumption that the lower child object is not changing. That it's, it is a, single thing that has a reference frame and we're gonna take advantage of that. But the parent doesn't know that. Parent doesn't know that there is a single thing that's moving. It's just detecting any kind of change. isn't that kind of the point though of subscribing the breaking off the child object with attention that it's like, it, the lower, the higher column can assume that it's one thing because the lower level column is only going to communicate. at any given point about like a unified thing? I don't know. I think, I'm not sure how that mechanism would work, Neil. somebody's clearly saying this, thing is, moving and I need to, treat it as a singular object.

but I don't know how that's happening. I, this is why I like thinking about just sub, this is why I went to the example of the logo rotating on the cup. It could avoid that whole issue, which we can come back to later and say, okay, how did it work? In the case of the staple where I had to break off a piece, it's it, makes it harder. Maybe. Yeah. Maybe it makes sense to, move on to the, next slide.

maybe it'll be easier to make. Talk about, Yeah, after the whole problem set up is stated, it would also be nice for you, be able to get to the Yeah. The movement correction stuff, just so that everyone can think about that. Yeah. And also, I have a log on a cup example on the next slide. it's not rotating, but, I guess in this, I, just want for completeness to show, learning, feature changes. So basically we have, the no matter logo or Hang on. we, have a logo on a cup. I don't know why it's not animated. here we go.

basically, yeah, I'll just show the slate. we have a logo on a cup and the logo can change between the menta and the TVP logo. Oh, I think you're muted. You're muted. Sorry. So that's the behavior we're talking about. the little a graphic is changing. Yeah. So the logo on the mug is changing. So we have at, a certain time interval, like each logo is shown for two seconds and then it switches and it switches back, back and forth. So the low level column can recognize both of these logos.

is the entire logo changing or just the graphic? Are the words changing too? no, just the, mark, just the graphic. Okay. 'cause when you have this logo change over the n and t, I'm trying to figure what the hell it is.

for simplicity's, just say the mark or, there's no text on the mark, whatever. Got it. but basically there's this TVP end cup that has this, specific behavior where the logo keeps, switching back and forth. And, whenever the logo changes, that means the. That kind of object ID feature that's coming into the high level column is changing. So that needs to be routed to the behavior model in that case. and so that change between these two object IDs from the lower level column would be modeled in the behavior model up here. That model would basically learn the temporal frequency of when it changes back and forth and data changes. And then, this behavior can then also be used as like this feedback contact signal for the lower alarm to, as know when to predict which logo at what orientation.

Does that make sense? Yeah, I, have a lot of ing questions about it. Yes, there's a lot of assumptions that, that unfortunately would need to be exposed.

I can do that if you'd like, but yeah, we, only have 13 minutes left, Okay. if we're gonna try to finish that. So keep going. I'm just thinking Viviane, if, you wanted to get to the, thing that you had in your document, because then yeah, maybe I'll get to that and then, we can go back home again because then people can ruminate on it, the next week. Okay. all So last, just, real quick, how could we use, this mechanism to predict morphology during a behavior? basically the idea being that we have this, behavior model up here, that stores, changes. So for example, relative orientation changes of the top of the stapler relative to the, whole stapler. And then this, feedback signal could then be used as a way to. Inform the lower level column, how to update the expected orientation of the child object, and if that expected orientation of the child object is updated, then again all of the input for the child object is, in the correct reference frame and all the predictions about its morphology would be correct. It has a detailed model of the, stapler top so it can predict all of the morphological features of it. We wouldn't have to learn any key frames or anything like that. we would just use this, behavior to know when to how to modify the incoming movement vector to still be correctly located in the child's reference frame.

so now you noticed already that all my slides just talked about orientation changes when, usually there's also location changes or in a lot of cases there are also location changes. and currently with, just compositional objects, not behaviors, locations are encoded by assuming co-located receptive fields and then making location by location associations.

however, now a child object can move relative to the parent, and that movement will be stored in the behavior model.

the behavior model in the high level learning module will need to update the object model in the lower level to update its expected orientation and location in the reference frame of the child object. like what I just showed, we need to have some way for the behavior at the high level to inform how we need to adjust the incoming movement vector so we correctly move through the child's reference frame and can make correct predictions about its morphology. it's relatively straightforward for orientation, like I just showed. You can just update that expected orientation in layer six. but we currently don't communicate or store relative location changes of the child. Object to the parent. and so that's the problem set up and, potential solution one, which I just added at the end of the document. I, it's not my favorite, but I just wanted to put it out there first. Is, to communicate the object location relative to the body. So basically similar to how we can calculate the relative orientation of the child to the parent, we could then, since they're both in a common reference frame, we could calculate the relative location of the child and to the parent and then store that relative location for one in here. And you could then recognize changes in the relative location and store that in the behaviorist reference frame. And it would be very straightforward to implement that in Monty as well. to get the location of the child, relative to the body. You basically just take the location of the sensorimotor relative to the body. The sensorimotor location is already in body reference frame, and you add it to the location on the object and you output that. So that would be a pretty easy mechanism. I could implement that in Monte in a day. I say now, but, I think.

it, would be a pretty straightforward mechanism, but I am, I think we had reasons why we didn't do this before. Maybe it would be worth revisiting, but I just wanna get to the other idea real quick. so yeah, the second idea would be instead to rely on motion information from the sensorimotor. So basically, this high level module still gets, the direct sensory input and that sensory input will communicate also movement in, the sensorimotor sensors receptive field. And that kind of optic flow could, inform location changes up here. and then those optic flow location changes could be applied to the chat objects reference frame. And the big assumption this makes is that the. High level receptive field is large enough to track the entire Child Objects movement. So this is a, something we talked before, talked about before, how like the biggest receptive field is to detect the movement of the sensorimotor. Like it's a global movement. But then if you have more local movement that can, indicate movement of the object itself and basically this one would assume that like this movement of the child object would actually be like raw se detected from the raw sensory input and stored in a high level learning modules, behavior.

and then to apply that motion, no matter whether we get it from. having body centric location information communicated or from the optic flow information? we basically have two scenarios. One is applying motion to, one is the sensorimotor is following the object, and the other one is the object is moving through the sensors location. So the sensorimotor is static. in the first scenario, the sensorimotor location relative to the body changes. So the sensorimotor is moving, but the location on the child object remains the same. So it's always on the same location at the stapler top. In the second scenario, the sensorimotor location relative to the body remains the same. It's not moving, but the location on the child object changes and we need to be able to deal with both of those scenarios.

so for scenario one, where we are following the object with the sensorimotor patch.

we are basically getting, movement input up here, but also movement input here to the higher level, reference frame. And that movement would go into the behaviors reference frame to move through that behavior reference frame. And basically, I tried to indicate it with this, red circle. We, as the sensorimotor patch moves here, we are also moving in the, behaviors reference frame. But in the Child objects reference frame, we don't want to be moving. We, want to stay on the same location in the Child objects reference frame. Even though our patch is moving and we're actually getting movement input, we don't want to move in this reference frame. We wanna move in this one only.

so how do we do that?

A way to do it could be to take the stored location and orientation changes that are stored here and apply them to the child objects reference frame. So one for orientation, we can just adjust that expected orientation. But then more importantly for location changes, we can apply that to the incoming movement vector. And in this case, they would cancel each other out. So basically the movement of the sensorimotor following the movement of the stapler is exactly the opposite of the movement that's stored in the behavior model. So we get exactly what we want, which is that we don't move in the child objects or reference frame.

even though the sensorimotor is actually moving, in the second scenario where the sensorimotor is not moving, but the, the object is moving, we are actually not getting any movement input up here, but we actually want to be moving in the child objects reference frame to make the correct predictions of which features we wanna sense at what point in time. we're also not, moving in the behaviors reference frame again because there's no movement input, from the sensorimotor patch 'cause it's static. but, the sensorimotor patch is detecting optic flow so it can detect this kind of local movement location change of the child object.

And that would be stored in this, model up here. and again, we can take the stored location and orientation changes and apply them to the Child Objects reference frame. So same mechanism to apply to the orientation, but then for the location, we again just take whatever location transform is stored in this model, we invert it, and then that becomes the movement input here. since we don't have movement of the sensorimotor, nothing cancels out. We actually are moving in the Child Objects reference frame, which is what we wanted, to do. That's it. little bit of a fast.

but, yeah, no, I, mean I think it'll take a while for that to sink in, but I think actually this worked well 'cause last time. You gave a nice overview of everything up to now and then it was quick, 20, 30 minutes at the end. The new ideas, it's been similar today where we spent more time talking about last week's stuff, but I think the discussion we had today wouldn't have been possible the first time around. So I think it's, I think there's a system here. I think next time we can maybe dig into these details, reminds, reminds me a lot of, when we worked on compositional structure, which if you recall, on and off, that went for years, but wasn't, it was not steady. and we, just spent so much time arguing about what the problem was, what problems we're trying to solve, and we kept introducing new complications that the solutions wouldn't work. And, I think that's where we're all here. I, don't, my sense is that this isn't right yet that. We're, thinking about this wrong, there's some things we're missing and that the final solution will be somewhat more orthogonal or, simple in some ways. but we're in the process where we're arguing, the analogy to the composition object when we're saying, yeah, but the logo could bend, or, we could have, this on top of this, on top of this. And, how do we understand, how many levels of hierarchy are there and blah, blah, blah, blah, blah. And then had came up with a pretty straightforward solution. I think we're not there yet. I think we're still in the argument of what the hell the problem is, and, it does remind me a little of displacement cells. that was the simplest, completely wrong example we had, but it was the first attempt, right? Yeah.

so as, yeah, it's good progress, but I don't think we're there yet. I. Yeah, I definitely think it, it's an interesting idea and yeah, it would just be good to, yeah, maybe next time work through the details. Yeah. Maybe, one high level observation, I just wanted to make before we wrap up, but, I know Viviane if you can just show one of your slides again with the showing the kind of like animation of the movements and stuff.

yeah, that, that's perfect. Thanks. No, Like that other one you just said. yeah, thanks. I think it's maybe just worth thinking about how at the moment, we talked a lot about how we don't want key frames, and I, think that's true, but in some sense at the moment our behavioral model is still like a key frame in that like we're learning changes at locations, throughout time. Like it, it's quite a densely sampled model. If you imagine even just something as simple as a stapler open. There's gonna be ch changes here, that feels like that kind of emphasizes the importance of hierarchy maybe that like, if there's any way to do that efficiently, we can't represent like a bunch of tiny local changes everywhere. That, it doesn't necessarily mean that it's just one single object moving, but I don't know it, just thinking about that again, it feels Yeah. the alternate solution to that is, and I keep saying is it is you have to interpolate, interpolation has to work really well because obviously anytime we have something which is not uniform, let's say the top of the stapler breaks into 10 pieces and each one flips over or something, then you have to learn a lot more to, to learn that behavior. and, but when the whole top moves at once, you don't have to learn much. I don't think that, to me, the solution to that problem is not, is, inter interpolation. that you can't sample all the points, you certainly don't want to for simple behavior and somehow, so to me, the, answer to this whole thing would be, I will, I need to interpretate within the behavioral model. Like maybe I can't, I'm not gonna store all the points in the behavioral model, but somehow be able to, I interpolate between them so that I can make the correct predictions. It's just what you're saying though, this, it's not a dense model, but it's got a lot of points in it. A lot of story points. Yeah.

that's a general problem. Yeah. Anyways, I just thought that was a bit of like a small elephant in the room to maybe think more about as well. And, I've argued before that even our morphology model has the same problem. We know we, we sample much more densely than we, really should. And so we've just been avoiding it, at least in that case, we're hoping hierarchy, it's probably not an either or. I feel like hierarchy and interpolation are both right. Hierarchy certainly helps a great deal, right? Yes. you, could argue that maybe you, could take a, problem and you bring into lots and lots of hierarchical components, and that would be more efficient. But then you have a lot more of storage of data in the hierarchy. So there's no free lunch here, right? You got, so I think you're right. The hierarchy is gonna get big component of it, but interpolation is gonna be an a component to it. It just, feels like it has to be. and I'm not worried about that too much because I think we can get the interpolation part to work.

but I, agree with your point though. Hierarchy is a com is gonna be an important component of this.

All right. I'm jazzed up to think about this. just hope I can keep Yeah. Still focused on, sorry, I didn't want to, just present the whole meeting again. that's all right. I didn't even plan on making slides, but then I felt like with like composition object, it's easy to just draw it on the, on a whiteboard or whatever during the meeting, but with, behaviors, I feel like I need this temporal component to show what I mean. So then I decided to make some animations, and I decided, oh, I need to make some watch slides to explain them. And it ended up being, yeah, animations were nice. I don't know how you did the one with the, hands drawn staple. That was pretty cool. Yeah. I loved the, like Walt Disney style stapler. I was like, I've sitting, I'm sitting here thinking oh my God, am I getting too old? Do I have to learn how to do this, this the new requirement. oh shit, I don't even, I don't even know where to begin on that. Robbie did some amazing stuff in PowerPoint. I don't know. He did. That would great. Yeah, la last time I made them all with, after Effects, but it's like such a complex program. So then I was talking with Bill the other day, how 15 years ago there was this really easy program and, we found a similar one that's just like a web interface. And so I tried using that today. It doesn't have all the features I wanted to have, but it worked reasonably well.

Oh gosh. All right. I'm gonna try to work on this.