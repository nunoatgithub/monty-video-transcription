Okay, we are going to get started here. So thank you very much for turning up in whatever time zone you're in. I know a lot of people are in a workday right now. So we really appreciate you coming to join us live. This is going to be an exciting presentation. So let's get started.

quickly, the agenda, we'll go through some welcomes and introductions, and then Jeff's going to give an overview of the thousand brains theory, and Viviane Clay and Niels Leton will be showcasing the implementation, the work so far. and then I'm going to go over, how you can get involved in the community ecosystem out there. And then we're going to have a Q A session at the end. so that's, what's on the plan. And so let's jump in. So a couple of administration points. this meeting will be recorded as we just mentioned, and we will be putting this on YouTube. So you'll have access to everything. So if you do have to leave halfway through because you're at work, that's totally fine. You can submit questions throughout the presentation through the Q A feature by clicking on the little Q A button in your Zoom toolbar. and we may address some of the questions as they come up and we may hold some of them till the end. just depends on whether we can answer them quickly or not inside the Zoom chat.

So please feel free to post questions all the way through. And with that, we'll start with some introductions. Okay, so if you're on this webinar, probably you know Jeff Hawkins. He doesn't really need an introduction, but I'll do my best. so Jeff founded Numenta about 20 years ago. he has devoted his life to understanding how human intelligence works. and he has two seminal works. on intelligence and a thousand brains theory. These books, I highly recommend them define and refine intelligence, what it is to be intelligence. And it's his life's work. He won't talk about some of the other accomplishments in his life. So he invented the Palm Pilot. He founded Palm. He founded the Redwood Neuroscience Institute. He invented the graffiti language, recognition language. And these are his side projects. That got us here. and so this is Jeff. next you'll meet, Dr. Viviane Clay, who is the director of the Thousand Brains Project. She holds a PhD in computational cognition from the University of Osnabrck, and she's won so many awards, I'm not going to list them all, but just a top one. Top three, so the Mendeley Fairset Data Award, the IT Talent, Loa Saxony for her Master's Thesis, the award for Extraordinary Master's Thesis, awarded by the Rosen Group and so on. so this is Viviane and you'll hear about, our current project from her later on. then we have Niels Leton, also a doctor, in this case, an actual medical doctor who, graduated from the University of Bristol. but he found that being a doctor wasn't challenging enough for him. So he went back and got a PhD in neuroscience, from Oxford. And, Neils is our lead researcher here at the Thousand Brains Project.

just a quick shot of the rest of the team. So this is, a candid shot where we captured everyone thinking about something. We're not sure what we were thinking about. That has been lost in the midst of time. But everyone has a very thoughtful face on here. I'm Will Warren. I'm down here on the bottom center. So I'm the community manager for Numenta and your MC today. and then honorable mention for Tristan, who wasn't in the screenshot because he was off camera. He says he was probably eating at that time. but Tristan is also a member of the team here.

Okay. And with that, I'll hand it over to Jeff and we can get started. Now, Jeff, whenever you're ready.

Jeff, you're muted.

Sorry about that. am I visible? I see, I'm seeing the, okay, great. anyway, thank you everyone for being here this morning or evening or wherever it is. whether you're here live, hope that'd be nice. If you're watching this some future date on recording, that's great too. we appreciate you taking your time to learn about the Thousand Brains Project. We're pretty excited about it. As you probably know, but it's worth stating again, the Thousand Brains Project is a new open source project for creating a new type of AI technology, based on, brain theory. really replicating how, humans and other mammals think and understand the world. as Will mentioned, it's been undergoing for, many decades actually, and it's a culmination of several decades of research, primarily at Numenta for the last 20 years. So it's, we call it sensorimotor learning. I will explain what that means. I'm going to give you an introduction to the theory, but I just thought, let's just jump right into it. I'm going to give you an introduction to the theory. So I'm going to share my screen here. And, and I'm going to start, with a few quotes, hopefully everyone can see that. let me make the presentation here.

brains work in very different principles than, any kind of machine learning technology we have today, and, 20, 30, 40 years ago we didn't know what those principles are, but now we do, and we can say definitively that brains work on different, very different principles. I'm going to tell it to you in a moment. And it lets us, it gives us perspective that today's AI technology is really, in our view, is, as exciting as it is and as powerful as it is, it's not really on the path to artificial intelligence, true intelligence. we believe that the correct way to do this is to really understand what those principles are by studying brains. And we're going to share those with you today. So we're not the only people feel this way. So here's some quotes from other people who've, basically have expressed similar concerns or feelings. And, and I'm not going to read these to you, but, I think we, we have a proposal and an alternate view for what, how to build intelligent systems. And that's basing them on, the way the brain works and sensorimotor learning. So let's just start with an overview of all learning systems. It's a very simple way to view this. all learning systems, whether they're biological or, Or, machine learning systems, have the following structure. They have some sort of training data that's fed into the system. The system learns a model of the data. We often refer to it as a model of the world, but it really isn't. It's just a model of the training data. And then the model is used to generate an output, whatever we want the system to do. Historically, AI, the AI world, has focused on the output. We've judged the success of our systems by what they do. How well can it label images, or how well can it translate languages, or how well can it play a game like Go or chess? and how the system works internally, although important, was secondary. a few years ago, convolutional neural networks were the hottest thing, and now transformer. Deep learning networks are the hottest thing. So whatever got you the best result was really matter. But this is not really the way to look at it. We want to understand if a system is truly intelligent or what it will be capable of doing, or what its risks are and rewards, really understand what is going to happen, what will happen in the future. We have to look at The other parts of the system, how the training data is collected, where does it come from, and how does the model represent, data. We can think of this, there's an analogy in the computer world. When we think about what's a computer, we don't look at what the computer does. We say a computer is something that has a CPU, memory. program, data storage, and things like that, and then we could call it a universal Turing machine or a computer, and it can be applied to all kinds of problems, large or small. I think in the future we're going to think about AI that way as well. The AI system is going to be defined by how they work internally. An intelligence system is going to be defined by how it works internally, and they can be applied to many different problems, some of which things Humans do, and some of which we don't do. So let's talk about sensorimotor learning, which is the core of the Thousand Brains Project, which I'll show a little picture of the neocortex up there just to remind you of that. But brains are sensorimotor learning systems. So what does that mean? First of all, the data we get that we feed into this system is coming from sensors, that model, that actually detect aspects of the physical world. So the primary sensors in humans are vision and hearing and touch. We have other sensors too. Other mammals have different types of sensors such as electric field sensors and seeing in ultraviolet and infrared, things like that. But it's always about sensing some physical attribute of the world. And an output of the sensorimotor system is movement. that's how, that's everything that a human does is based on movement. My voice and my speech right now is really muscle contraction. So we have sensor data in and movement out. Now there's an issue here. As we move through the world, the input on our sensors changes. In fact, changes very rapidly. As we walk, as we turn our heads. As we, touch objects with our hands and move them over the surface, and as we move our eyes several times a second, the sensory data changes completely. And the brain can't make sense of that if it doesn't know how it's moving. So the sensorimotor systems have to take account of how movement is occurring, how it's moving its body through the world, how it's moving its sensors through the world. And that becomes a second input to the system. I'm showing that here in red. So we have sensory data coming in, features of the world, and we have how the body is moving, how the sensors are moving through the space. Internally, what's going on in the brain, in sensory motor learning systems, is that this is stuff that the research has shown, so not everyone understands this, if you follow our work you might, but not everyone understands this yet. That internally, you have a sense of location, that the system knows where the sensor data is coming from, where it is located relative to something else in the world. Refer to that as a reference frame, so there could be reference frames relative to objects you're touching, reference frames relative to the rooms you're in, reference frames relative to your body. There are basically all information that comes into the brain is associated with some location in space. And how models are learned is by associating features with locations. And so as you move your fingers over an object, or you turn your eyes and look at different aspects of a scene, your brain is essentially associating what you're seeing with where it is. It's really obviously, if you think about it, it's obvious to observe this. When you just look around the room, you know where things are. that seems so simple, but it turns out that's not so simple. The brain has to calculate that, and it has to represent that internally, and so you're constantly just matching features of location. In this very simple, specific example, you can think of it like this. here's a tip of a finger. Let's say you're imagining you're touching this cup with the tip of your finger. As you move your finger over the surface of the cup, maybe on the rim, you might feel an edge or curvature. You might feel a texture, that's the sense feature. But as your finger moves, the brain keeps track of where it is in space. And therefore it is able to build a three dimensional, basically model of the cup. It's very much like CAD modeling. This is actually going on in your brain all the time. So we build models of things, in this case a model of a cup, by just associating what was sensed where. And, the movement of the sensor is actually fed in and it's updating this location. in a reference frame using a system called path integration which basically says if it knows where it is and it knows how the finger is, what direction it's moving and how fast it's moving, it can update its location, much like dead reckoning and if you know what ships do, used to do. All right, so now this just shows an example of a single fingertip, and this finger, in your brain, your single fingertip could learn a model, something like a copy. I could literally put your hand in a black box and you could feel around with a finger, single fingertip and build a model of something by moving it like that. But that's now turn to brains. 'cause brains are actually, they do this in a very interesting way. oh, I should, I guess I should mention this before I go on, just in summary. we learned by moving and sensing and moving and sensing and knowledge is represented by associating, observations with locations and reference frames. and this way we learn models of, structured models of objects, places, and concepts. By the way, it's easy to imagine how you can learn models of objects and places this way. It's a little bit harder than how you learn models of concepts like mathematics or democracy or language. But, as we'll see in a moment, this is how the brain does it. And although we don't understand all of it, it's very clear that this is, it's using the same method for knowledge representation for everything that humans and other mammals do. Okay. Now, if we look inside, one neuroscience slide here, if we look at the brain, this is a picture of the neocortex, which is about 75 percent of your brain. it is the organ of intelligence, meaning all the things we associate with intelligence from, basic perception of touch and hearing and vision to language, the thoughts, and so on, all occurs in the neocortex. And, it is, all mammals have a neocortex. Ours is particularly large. But all mammals do, they have the same structure, and, if you were to look at this structure in detail, you see it's actually a sheet of neural tissue, it's about three millimeters thick, it's about the size of a large dinner napkin, about 1, 500 square centimeters, so you can imagine it's a large sheet, and if you could look and cut through that sheet, imagine now you're looking at this, these columns, They're being shown here, the thickness of the neocortex, which is about three millimeters thick. The neocortex is composed of these columns. They're called cortical columns. they're about a millimeter in area. we might have quite a few more than these. No one really knows. You can't actually see them like this in a brain, but we know they're there. The neuroscientists have determined these columns there. And the structure of these columns is nearly identical everywhere in the brain. So if you look at anywhere in the brain, you see the same column of structure. In the same detailed architecture in each column, whether those columns are doing touch, or vision, or hearing, or language, or anything else, the brain, your brain is made up of copies of the same basic neural circuit over and over again. And other mammals have the same column of structure, they just have fewer of them than we do. All right, it turns out that in the brain, those cortical columns, are mimicking exactly what I just talked about. Each cortical column is a sensorimotor learning system. Each cortical column builds reference frames and builds models. basic by movement and, features coming in. So in our, in the thousand brains project, we call these learning modules. And so you'll hear the term learning modules throughout the presentation today. they're equivalent to what's going on in the brain in a cortical column.

it's a highly distributed system, 150, 000 minimum in human of these sensorimotor learning systems. This is, the primary reason we call this the thousand brains theory and the thousand brains project, because we have lots of these little things that are just like little brains, but they're just replicated. And by the way, the human brain got large by making, evolution decided to make more copies of this basic circuit. Okay, so how do these things work together inside? this is tricky, but, this diagram will illustrate it. Here in this image, you're going to see more of these images later. we're representing, you can see there's, here I'm showing eight of these learning modules. each one's showing a little reference frame, and a little box above it. they're attached to different sensors, which are represented by those little blocks on the bottom. And these learning modules are connected in two basic ways, two ways of distributing them.

And we know this from brain research. One is that there are these lateral connections between learning modules that allow them to reach inference faster, and they lead to a unified perception. So when you actually look at something, there are thousands of these cortical columns in your brain. Each looking at a part of the visual world, each looking at a part of the retina, nobody looks at the whole thing. And so each one is able to infer what's going on through movement, but in, but if they talk to each other, they can say, I think it might be this and I think it might be that, and they can vote and reach a consensus very quickly. And this is why you can flash an image in front of somebody's eyes and they can tell you roughly what it is, because these columns communicate with each other, and allows them to reach a conclusion about what's going on. without movement. If you didn't have this kind of voting then they'd all have to move. It'd be like looking at the world through a straw or touching object only with one finger. If you do that you have to have a lot of movement to infer and learn, but if you do this voting you can make inference much faster. The second type of communications between these learning modules is a hierarchical connections and that's shown here. We're showing two levels of hierarchical connections. These exist in the biological neocortex, and they exist in the thousand brains project as well. And these connections allow the system to learn models composed of objects composed of other objects, and that's how the way the world is structured. words are composed of letters, sentences are composed of words, cars have, they're composed of doors and windows and tires and so on, wheels composed of tires and rims and so on. We learn this compositional structure by having this kind of hierarchical connections like this. Okay, so these are the basic ways you can build, the brain and biology has learned to build intelligent systems. Sensory motor systems built of these, individual learning modules are each sensory motor learning systems. They're replicated in various ways and they're connected in both horizontal connections and vertical connections. And this is how you essentially, you understand the world. Your brain is structured like this. Niels and Viviane will get into this later. But one of the beauties of this system, is that it makes it so as we build intelligent systems, we only have to really figure out how these learning modules work. And then once you've got them working, which we haven't to some extent, we do have them working now, to a large extent, you can replicate them and make systems of various types. You can also You can apply them to various types of sensors. it's a very flexible system. it's, it's, it'll come, incredible, variety of, AI or applicant intelligence systems will be made this way in the future. I'm going to end, with a few thoughts.

just to get everyone excited, at least on the same page, or at least on know what page we're on, machine intelligence is going to transform the 21st century, similar to how the computing transformed the 20th century. I think that's a really good analogy. And, and we're just starting that. we're like 1940 or 1950 in the computing era. That's where we are in AI right now. It's going to be a really exciting and fast, Growth in this field, and it's going to lead to many benefits. I believe that sensorimotor learning, and not today's AI, not deep learning or convolutional networks or transformers, will be the technology at the heart of this transformation. Not that those other techniques are going to go away, I believe we'll have transformer networks for a long time, and they'll be an important part of our, technology. But I don't believe they will be this core of AI, what people in the future will think of as really AI, we'll be based on sensorimotor learning, and I should mention that it's also the core of robotics. This is how we're going to do, truly robotic systems, using these techniques. And finally, I believe, or at least I hope, this is more of a, aspiration, is that the Thousand Brains Project, will be a seminal event in the creation of truly intelligent machines. As far as I know, this is the first place that anyone can really, we have all the material, We understand what's going on for the first time, we're starting to build this stuff for the first time, and, and this will be the seminal event to do this. I guess I forgot to mention earlier, and I should do that right now, I don't want to forget this, that We are, we've been working on this for several years. the theory has been going on for decades, but we have been working on the code for several years, mostly Viviane and Niels, and we proved that we can actually do this. So we're not starting from scratch. And if we come into the thousand brains project, you'll see there's a lot of code already, and a lot of stuff that's working, and a lot of documentation, and a lot of videos, and a lot of, there's a lot of stuff to digest. It's not easy. In the sense that the concepts involved here are new, there's a bit of a learning curve to get, to understand these concepts. It's not like any other machine learning system you might be familiar with, but it's very rewarding and it's fun, and it's very exciting. So hopefully you find it exciting too. and I hope some of you, or all of you, or as many as possible can help contribute to the Thousand Brains Project. we put together a really great system. We're hoping to have a lot of contributors. and, with that, I'm going to pass it over back to, I guess Will. Who is, who's in the master of ceremonies here. Thanks so much, Jeff. That was a great overview. just to remind everyone on the call that, you can go and ask questions in the Q A section and you can vote for other people's questions as well. I don't think I mentioned that, so you can upvote questions and then the top ones will answer. and now I'll hand it over to Viviane for the next part of the presentation. All right. Thank you. Share my screen. And slight control with you, Niels.

all So Niels and I are going to talk about, Monty, which is how we call our first implementation of such a sensorimotor learning system. we call it Monty in reference to the neuroscientist Vernon Mountcastle, who first proposed this idea of the cortical column as a repeating general computational unit in the brain. and I just put this little link down here to Make this point again that this project has a very concrete implementation already, and it's all open source, and it's all, you can find it at this URL and take a look at the code yourself, run it yourself, or even contribute to it.

Making this presentation was tricky because there's so much you could go into, and we could talk about all these, every slide in depth, but, this presentation, this meeting is, scheduled for three hours, and still we're only going to scratch the surface.

as Jeff already mentioned, it's a bit of a learning curve if you first come to this project, because we're not just taking an existing AI system. We're not just taking transformers and adding a little incremental improvement to it. We're really building a fundamentally different system. And that means to understand this, you're going to have to. Dispense some of your preconceptions, some of the ways you currently think of AI, for example, thinking that you need a huge labeled dataset or just a huge internet scale dataset. And, start from scratch and have a fresh mind when you come to this project. And I'm going to repeat a few things that Jeff already said again, just because it takes some time for these concepts to sink in and for this approach to sink in and make sense. again, like Will said, feel free to ask questions, as I present this.

all So what is a thousand brain system? The thousand brain system is, we call it that because it's based on a thousand brains theory. And as Jeff mentioned, it's called thousand brains because we have this idea of, the cortical column, which is found in the brain as the general repeatable computational unit that can learn, can do a lot on its own already. and we, about three years ago, we took this theory and the principles in there that are outlined in Jeff's book and started building a system based on these principles and we proved to ourselves that this is really a good approach and this actually works and now we decided to take it to and make it open source and publish everything we found out so far. And it's been really exciting for, especially Niels and me, who've been working on it since the beginning, the past two weeks, since we published all of this. It's just been so much fun to be able to talk about all these ideas and this really cool approach. So I. I hope you'll be excited too. so let me go through the key principles of these systems.

first and foremost, it's based on sensorimotor learning and inference. So as Jeff mentioned, everything is about sensing and moving, both learning and inference is sensing and moving. And also the models that are being learned are made for this kind of data. They're models that, are explicitly designed to model a stream of sensorimotor input and to produce action outputs.

How are they designed for that? One of the Ways that you can model sensorimotor data is using reference frames. having an explicit representation of space, knowing how to path integrate in space. So if I go from A to B to C, I know how to get back from C to A, even though I never took that path. That's what you can use a reference frame for. And you can think of it as like Cartesian coordinates, for example. And as you're looking around a room, You can recognize objects and recognize where they are relative to each other, and you can remember that and model that using reference frames.

And then the cortical column as the general repeatable functional unit.

Each of these columns can learn full models on its own, so it's not like It just implements an edge detector or detects some kind of pattern or texture. It can actually model entire three dimensional objects and it doesn't actually sense the object all at one time, but it slowly moves. It can move over the object and then. Build a model of that larger object and recognize that object. And that's why I put this point, subpoint here, because this is a concept that I thought when I first came to this project was a bit hard to sink in. If you come from computer vision, you always think, oh, I'm going to give this photograph to my AI system and it will process this picture. but really, if you at the brain, there is no full image anywhere. You have The retina and the eye, there are tons of blood vessels going around everywhere, there's the optic nerve exiting here where you have actually a blind spot that you don't even perceive, you have neuron bundles going through the LGN, left and right visual field being separated, and so on and so forth. Each of the neurons in V1 actually perceives a very small part of the visual space, so if you look at your laptop, a neuron in V1 actually has a receptive field smaller than just a couple of millimeters, and so Each of these cortical columns always just sees a tiny patch of the input and doesn't get the entire image. But instead, you have to move around your eye saccade about three to four times a second and move around and recognize things that way and learn that way.

And then lastly, we have the, what we call the cortical messaging protocol, which is essentially a way of saying that all of these columns, no matter if they, model touch or vision or abstract concepts, they can communicate with each other using a common language. And, this makes it really, easy to have multimodal systems and to scale the system, stack these on top of each other, and I'm going to go into more detail on that as well, but I just wanted to outline these principles again. All of them Jeff also touched on already, but it's really Thank you. Good to keep these in mind. These were our compass for the past three years when we built the system and every design decision we made, we thought, does it follow these principles? Because looking at the neuroscience literature really showed that this is what the brain's intelligence is based on. And so we try to always keep these key principles in mind when building Monty.

All right, now I'm going to go into the actual implementation and concepts there. And I'll start with the cortical messaging protocol because it's really the glue that holds the whole system together.

so we start by defining what a message needs to contain. And the key The ingredients here are, it needs to contain a pose, which is basically a location and an orientation and a common reference frame that could be relative to the body or it could be relative to some external point in the world, but some common reference frame. And then it can contain a list of features, for example, color or amount of curvature that's being sensed. whatever that sensor is sensing as features at that. location. And additionally, we can have some auxiliary routing information like a confidence or sender ID, for some of the routing within the system, which is a bit more technical part.

and this messaging protocol is used All everywhere inside the Monty system. for instance, we have the sensor modules, which get the raw sensory input, and then they output an observed state, adhering to this messaging protocol. So it could output, I'm sensing this location and orientation here, and I'm sensing red and a curved, surface.

this goes into the learning module, and the learning module, tries to infer what larger object it is actually sensing, and then outputs this as the hypothesized state, so it might output, I'm sensing a red coffee mug, in an upright, orientation. it can also communicate, possible objects laterally, to other learning modules to speed up inference. Again, using exactly the same, And lastly, a learning module can also output goal states to the motor system, telling the motor system, how it wants it to move.

What does this kind of messaging protocol, give us? First of all, it, gives seamless multimodal integration. learning modules that learn to model. touch input can easily communicate with those that model vision input, or those that model lidar, or ultrasound, or any other modality. Doesn't matter, they can communicate with each other because they follow that protocol. it also allows us to easily model compositional objects. So whatever object and pose this learning module recognizes can just become the input to the next learning module. The coffee mug that's recognized down here just becomes a feature in the model of this learning module. And they don't even need to know where in the hierarchy they are. It doesn't matter for the learning module whether it gets input from a sensor or from another learning module, because it's all just in the format of features and poses.

And this makes the system very extendable and scalable. So if you have a very simple application, you can just use one learning module in one sensor. And you can do a lot with that. Actually, When we implement the system, for the past years we've mostly just used one learning module, and one of these units is really powerful on its own already. But if you have a large application or you have tons of sensors and they all need to communicate with each other, you can use thousands of these and you can easily scale them to whatever your application requires. And it's not like a transformer where you need billions of parameters to do anything useful. You can do a small solution with One or just a few of these modules, or you can scale it for tons of sensors or scale it up in depth for modeling more hierarchical compositional concepts.

it means that it's a very nice plug and play, framework, and I'll go into some more detail on that in a bit as well. parallelizable system. So you can use a very quick associative local learning in each of these. These modules themselves, you can run all of these individual modules in parallel. They don't need to wait on anything. They don't need to process anything in serial, and that way it can be implemented quite efficiently.

All right. That's the, messaging protocol. now to the two core components that are already mentioned, sensor modules and learning modules. So the sensor modules.

As I mentioned, they always see a small part of the object and they move. So here we're seeing a coffee mug. This is really just for visualization. The sensor module just sees a small patch of the object. So you can imagine like the tip of your finger moving over an object or a little patch on your retina.

And then the main job of the sensor module is to turn raw modality specific data into the cortical messaging protocol. for example, if you have an RGBD camera, the camera sees this patch, you could, you can extract a point cloud from that, depth image, from that you can extract, a location relative to the body or the camera or the agent or some external point of reference, and you can extract a rotation. So we usually define that by the point normal that points out of the surface and the two principal curvature directions. And then you can extract some other features as well, like what color is sensed here or the amount of curvature that's being sensed. And this is then, in the correct format to send it to a learning module.

important thing to say again, the sensor module needs to be able to extract a pose. Otherwise it can't send, something in the format of the cortical messaging protocol. So it needs to somehow be able to extract pose or movement information. It can be done in many ways, you could actually get a copy of the motor command, or you could extract it from optical flow, or proper reception, or other signals.

And then the learning modules, that's really the heart of the modeling system. So the purpose of the learning module is to model incoming streams of poses and features using reference frames. And using reference frames means, We're not just, the model is not just the bag of features. It's features at poses. So let's say we have a car and we first look at the tire and then the door and the hood of the car and, we sense those as features, but we associate these features with locations in a reference frame of that object. So the tire is at a relative location to the door and the door is at a relative location to the hood of the car. And that really defines And this is what we inferred from looking at the neuroscience and what the neocortex is doing. So the whole structure of the learning module is very heavily inspired by the structure of cortical columns in the neocortex. So if you take a slice of this neocortex anywhere, you find this layered structure of different types of neurons and cells. And this structure is found everywhere across the neocortex and it's remarkably similar everywhere. And we spend a lot of time looking at the neuroanatomy and physiological studies and what we know about the neocortex by now. And you can draw some analogies between how we implemented Monty and what's happening in the neocortex, although we don't Always explicitly talk about it in the code, but just as a bit of background.

in our current implementation, the learning module that we use most frequently is called the evidence based learning module. And I'll just give you a quick overview of how that actually works at a high level. We have a sensor module which converts the sensory input into the messaging protocol, which means, we get, features and pose and we get a displacement, which is like a movement in space. And then it also produces a motor output since it's a sensory motor system. And since it's interacting with the world, this happens over a series of steps. So each motor output leads to a new sensory input. And that gets processed, integrated again, leads to a new motor output, and so on.

as we get features and poses as input, we have some hypotheses in memory, and we can update the evidence for those hypotheses. So let's say we have four objects in memory, we are modeling these as features at locations, and as we're getting feature inputs and movements, we can say how consistent these The series of features and locations is with our models in memory, and for example here we might have gotten some, features and movements that made this bowl very unlikely, and all these cylindrical objects are so likely, but then even within an object, some locations are more likely than others. And the most likely of these hypotheses becomes, the kind of hypothesized pose and idea of the object. for instance, a coffee mug and an upright orientation and location relative to the body. And this would be the output of the learning module, which could become the input to another learning module, or it would just be the output of the system if we do a simple object and pose recognition. We can also share this. Information with other learning modules laterally to come to a conclusion quicker in a process that we call voting. And then lastly, we can also have model based policies where we use these hypotheses to generate goal states and tell the motor system how to move next. And Niels will go into more detail on those. And to see the system in action, I put an animation in here that I'm just stepping through very slowly, to explain this a bit more visually. So basically we have a little sensor patch here, that moves over a coffee mug. This zoomed out view is just for visualization. And we move down the coffee mug and then on the bottom of the mug. And with every step, we update our hypotheses. So by this time, we already decided it can't be the banana, very low likelihood. The dice, way too small. The bowl, also Not very likely. And then on the coffee mug, most locations on the mug are also unlikely, but there's this little red ring, red means highest likelihood, that goes around the coffee mug at about the height of where the sensor patches right now. And this is where we think we most likely are. We can't really determine yet where, around that coffee mug we are. But once we get some more distinct features like the handle, we can be pretty sure of where we are and what object we are sensing. And this will be eventually the classification. And importantly, we always have a most likely hypothesis. We don't need to sense the entire object to be able to say anything, but we always have a working hypothesis. And we Update it with every step and refine it with every step. And just another example, with another object. Here we have a dice, which is a bit smaller, we're moving on the dice. And since it's so small, we very quickly rule out these larger objects. and then on the dice, we have lots of evidence, since it's such a symmetrical object, it's a bit harder to nail down how it's actually rotated. and the system can recognize this actually, it can actually recognize that, there's some symmetry and then, in this case it actually detected a rotation that's not the rotation we showed it, but it's basically the same, look, so it's, it detected a symmetric, and this is what we would want in a system, we wouldn't want it to rely on these tiny artifacts on the, dice.

and, yeah, that's how the system works. Moving, sensing, updating hypotheses, moving, sensing, updating hypotheses, and at every step we have a most likely hypothesis of what we're sensing. And this was all one sensor and one learning module, but we can also use more of those, and then, vote between them. So we could have five patches that are sensing different parts of the MUG and moving across that MUG. And, we connect each of these patches to a sensor module, and then the output of the sensor module goes to five learning modules. And since they all get, different input, they might have different hypotheses. so they might think that they are on a different object, or they might think that they're on, The same object, but in a different orientation, but we can quickly resolve this by sending these lateral volts and then narrow down which object is being sensed much quicker and more robustly as well.

Okay. Now this one is a bit of a complex figure, but I think it just puts everything into one image. So bear with me. Again, the three components here, learning modules, sensor modules, and motor system. Here in this example, we have actually two sensors. We have an eye and a finger, and the eye is sensing two parts of the mug, and the finger is touching the table here. Each of those patches go into a separate sensor module, and the sensor module extracts features, so we might extract blue for the ones on the mug, and then a flat surface for the touch sensor on the table. And then these features and poses go into, learning modules, which then, through successive movement, build up hypotheses of what they are currently sensing. the ideally, these two would, sense a coffee mug and recognize that and the finger after moving a bit might recognize that it's sensing a table. And then, these again, since they're following the cortical messaging protocol, can become the input to another learning module. And that learning module might learn a compositional scene of the, coffee mug standing on the table, which are again, relative locations. So coffee mug on the table and the mug in the table then become features in that compositional object.

Additionally, we might also have skip connections, so we might directly send the output from the sensor module to a learning module. We also have these lateral connections for voting and quicker inference. We might have top down connections, so if we've already recognized a dinner table scene, that might bias the lower level learning modules to quicker recognize what's on the table. And then, every learning module has a motor output. This is again, something that we also see in the brain, that we have outputs to subcortical, motor areas everywhere across the brain, not just from motor cortex. And those go to the motor system, which then, oops, translates it into actual motor commands. And so all the dashed lines. are, don't need to follow the cortical messaging protocol. the motor system and sensor modules are basically the interface between Monty and the world, and they translate back and forth between the raw data and the cortical messaging protocol that's used everywhere within the system.

Okay. So the Monty, now actually to the code. so this is the Monty framework. If you go to our code base, you might find, some abstract classes with exactly these names and that's how they relate to each other. This is a bit of a simplified view. so we have a Monty experiment, which, an experimenter can use to set up the framework of what, you want to test, and what kind of tasks you want Monty to do. And you have, Monty, which can have as many sensor modules as you want, as many learning modules as you like, and a motor system, Monty kind of manages how the sensor modules connect to the learning modules and how the learning modules connect to each other and that helps us. Information routing is, managed in Monty. And then we have the environment. Right now, this is a bit more, complicated. It looks more like this, but we plan to simplify that. but basically we have an environment, and there's a bit more detail of, the learning module actually having memory and having object models as well. And the core idea is that you can customize each of these classes individually without changing any of the other classes. and, plug and play and compose, a Monty system however you like. You can use as many sensor modules or learning modules as you want. You can connect them however you want, but then you can also write custom classes for each of these boxes. And for instance, you can write a custom sensor module for a sensor that you have. Plug it into this framework, use exactly the same learning module, motor system, environment, and so on, and just test a different sensor. So for instance, we have a distance sensor, that's like an eye moving in a socket. We have a surface sensor that moves like a, kind of a hand. We have a sensor module that only sends the features when, only sends a message when features change significantly. We can mix in some noise for experimentation. But you might implement your own sensor modules, like one for sound, or lidar, or ultrasound, or whatever you might think of.

You can also customize a learning module. for instance, a lot of our work we've been doing with a graph learning module, where we use explicit graphs, to model objects because this makes everything very easy to visualize and to debug and for us to figure out all the plumbing in the system and what's actually going wrong exactly when something doesn't work and we've went through several iterations of different learning modules starting with a displacement learning module where we use displacements to recognize objects moving to a feature learning module where we Actually look at the nodes in the graph instead. And then now the current, I would say best version, the evidence graph learning module, where we actually keep track of continuous evidence values for different objects and poses. And they all have their strengths and weaknesses, generally getting, better from left to but it's just been very easy to plug in different types of learning modules to compare them while leaving everything around the same. We also played around with the learning module using, some of Numenta's previous, work, the hierarchical temporal memory in combination with a simulation of grid cells. You might think of, actually implementing a neuromorphic learning module, doing something in hardware. As long as it adheres to the, interface that we defined for the learning module, you can do whatever you like inside the learning module. Of course, it should be built for modeling sensorimotor data, so a reference frame inside a learning module is very useful.

if you have that, you can replace these, leave everything around the same, and you might come up with other approaches than our evidence graph learning module, and we might come up with other approaches in the future.

then you can also customize the environment. So in this case, the data loader and data set. we often work with the Habitat simulator, which is like a 3D simulation engine. We also implemented a real robots environment as a simulator. we have an Omniglot environment, which basically you kind of trace. The strokes of different handwritten alphabets. We have one where you cade on an image, basically moving a little patch across, to the image. and then for data sets, we often use the YCP data sets, which is like 77 household objects and render them. And Habitat recently put together scene data set where it's, 3D objects relative to each other arranged in a scene. we have a data set of real world images. And then, of course, you can think of tons of different ways you could test the system. And then lastly, the Monty experiment kind of defines, how, you put Monty into the environment and how you measure performance and all this. So a lot of the times we've been looking at object recognition, but we've also looked at generalization. Supervised training, profiler is just useful for optimizing the system. But you might think of tons of other ways you could experiment. Object manipulation, washing the dishes, mapping an area, detecting problems. the general idea is just that this is a core algorithm that applies to a huge range of areas and this framework easily allows you to do that. And I hope this was a little bit inspiring and gave you some ideas maybe of what could be interesting to test or which component you would find interesting to, test out or replace or try something, try a new environment. and with that, I'll hand over to Niels now.

Nice. Yeah. Thanks very much. Great, so yeah, I'm going to be talking about motor policies in Monty, and I just thought it would be useful to take a step back, because you've heard a lot about kind of a thousand brains, so you have all these different learning modules, which are each developing explicit structured models of objects in the world, and they communicate with one another and are semi independent to a certain degree, and those are the thousand brains, you've also heard a lot about how the system is sensorimotor, but I think it's really worth emphasizing, although this was touched on, earlier, that each learning module, each component of the system is a fully sensorimotor system in its own right. Because that's a really key concept to how it's going to operate and how it's going to act in the world. And this was, as Viviane alluded to, very much inspired by our knowledge of the brain. So I'm not going to go into, obviously loads of neuroscience detail, but just briefly, if you look at this view of the cells in a cortical column, which again, maps onto the learning module, there's this distinct layer that projects to the parts of the nervous system, the subcortical parts that control the body.

and you may have come across the concept of a kind of motor cortex and have this impression that's the part of the brain that controls how you move. And then everything else is, relatively inert as opposed, when it comes to kind of motor control. but that's not the case. So anywhere where, neuroscientists have looked for, these cells, they've found them, throughout the neocortex. So it's the entire organ. It's, every, every, cortical column and therefore in our implementation, every learning module. that is sending this, motor control, throughout the system. It's also worth emphasizing this because, this is, very different from how sensory motor control is implemented in any AI systems, that do have sensory motor control today. It's, a very different approach to have such a, highly distributed, semi independent, collection of units. providing the, sensorimotor control.

So I thought that was a useful perspective to just recap on, that's talked a lot about kind of motor control and, how, the system's acting, but, what exactly is being controlled? so if you look at our kind of documentation and our code, you'll see this concept of agents come up and, we were showing the example of this kind of eye and this hand, and essentially what we mean by an agent is just a system that can move, in a semi independent manner. from another system and has sensors associated with it. a good example is something like an eye that can move around at least relatively independently from a finger, and both of them have sensors associated with them.

And specifically in our code, we refer to two things, a distant agent and a surface agent. And roughly speaking, the distant agent maps on to what you'd think of as an eye, or a camera that pivots around. And the surface agent roughly corresponds to a finger or a robotic, digit that would, sample an environment.

and for example, the, distant agent can rapidly look around, a scene and depends on sensory information that, can propagate freely through space. life being the, clear one. whereas the distant agent needs, sorry, the surface agent needs to close the space to the objects it's sensing in order to sense them, but this confers additional advantages like additional sensory modalities, and for example, being able to manipulate the object.

It's also worth highlighting though that ultimately this distinction is a bit of a kind of false dichotomy, because, as we move away from, biological systems to, artificial systems, whether that's in simulation or something like robotics. Of course, we have a bit more flexibility, cameras are very cheap, and, you can imagine, for example, a robotic arm that also has cameras that are able to sense things like color, on the, actual appendage.

but that hopefully gives a bit of a flavor of, when I'm going to talk about policies now, what exactly is being, controlled. And going to then policies, this really refers to how the system, given the current state of the environment and of itself, how it decides to act in the world. and you can imagine that a policy could have a variety of, focuses of, what it's trying to achieve. a, clear important one is just inference, so recognizing what you're seeing and, where it is in the world. but you could also have a policy that's focused more on learning, so how do you efficiently gather new information in order to, build, new representations? And thirdly, you could have one, with manipulation as the core focus where it's all about how you can, change the environment. but our, aim right now, or our focus so far has really been on the, inference side and to a lesser degree with learning, but all of these are going to, become, something that we explore in the future.

And so when you talk about policies, there's two real ways that you can break them down and structure them. The first is what you would call model free in AI terminology, and I'll get into this in a moment. And the other one would be model based.

And so in Monty, the model free system is really referring to this part early in the architecture where you have this sensory information coming into a sensor module, here circled in purple, and then being sent to a motor system, and then you have an action output. And and this is called model free because this, circuit can operate, or in fact does operate without building any explicit models of, what exists in the world.

and this is, and this is represented by the fact that the learning modules, further down in the system, aren't involved in these kinds of decisions. And this is, analogous to how the subcortical structures, in the brain, can help us carry out actions without needing to involve, the kind of more deliberate cortical, thought processes. It's also worth mentioning that although this is a model free system and doesn't have model, explicit models, It can firstly still learn, and for that reason, it can also still be quite powerful. And it's definitely an important part of how the system can efficiently interact with the world.

And so when Monty first opens its, opens its eyes, so to speak, it, would just randomly move around on the surface of an object, just performing these kind of small saccades. but already in the beginning, we began implementing these, simple, you can think of as innate, model free policies, like if you move off of an object that you're looking at, and are just looking into the distance, try and move back onto that object, just to get a bit more of a sense of what exactly it was you were seeing that's, near you.

but then we began implementing, some more sophisticated policies. So for example, for the, surface agent, there's this policy that, enables us to, or enables it rather to. Move on to the surface of an object and then using the information that's coming into it. So again, it's, model free, it's, getting sensory information, but without having an explicit model of say, I'm sensing a coffee bug. Use that information to then, make movements that, keep it on the surface of the object and enable it to explore the surface.

But, in of itself, this process is going to either result in the agent just moving in a straight line, looping around the object, or just moving randomly, without any kind of, yeah, deliberate kind of goal. but a kind of early, then, model free policy we can do to make this a bit more, deliberate is, introduce this kind of curvature following. And so this is something that we already have, implemented, where in an object, something like a coffee mug, You have these dominant curvatures that exist at any kind of point on the object, and this is something that when you're feeling the surface of it, you can sense. But these kind of dominant curvatures give you some sense of where are interesting features or where kind of is the object changing in an interesting way if you follow them. And so again, Without the system needing to know what the actual shape of the whole object is, if it just senses these curvatures and decides to follow them, it tends to efficiently explore interesting things like, the rim of the coffee mug or the handle.

And so this is showing that, in practice, where, if you imagine, the kind of agent is starting at the bottom of the mug, which is upside down, and then it's moving down along its side, but when it reaches the, rim of the cup, it senses this, quite strong, difference in the curvature, and then decides to follow along that. And so It's, looking for essentially just interesting features that it's coming along.

I should perhaps mention that, so far these, model free policies are, I think alluded to earlier, innate policies that we think are important for the system to be able to efficiently interact with, the world. But you can also learn, model free policies, and that's, something we're, also exploring. And, any kind of learned policy can, of course, leverage whatever innate policies there might be as a, kind of, primitive.

but what we're really excited about is model based policies, because, this is really where, true intelligent action comes in. And the, name model based comes, intuitively from the idea that, The actions that are being proposed, as Viviane mentioned, we call them goal states because it's really a state that you want the world or another object to be in. Those goal states are being informed by an explicit model of the world that a learning module has. And as you can see again with these pink arrows, every learning module is going to project these kinds of goal states around the system in order to try and influence the state of the system, and therefore the state of the world.

and whereas the kind of model free was analogous to the subcortical structures, this is, analogous to the kind of more deliberate, thought out, action planning that your, cortex enables.

and one of the reasons we're excited about this is because, model based, policies in general are a major challenge, but something that, is a key goal, really, in artificial intelligence, in order to act, intelligently, and it's something that, mammals do very well. and, where there has been, for example, success, in AI with, AlphaGo that was, able to defeat, Lee Sedol in 2016, a key part of the success was actually providing, an explicit model, in the system of the, structure of a Go board. However, in, the real world, We don't have the kind of explicit structure like that just available, and, it's not some kind of simple, mathematical object that we can just inject into the system. So if you imagine in kind of a complex setting where a family's cooking, there's a huge array of different objects that are there, each with their own complex behaviors, their own complex structure, and different ways that they can interact with one another. And in order to act intelligently using these explicit models, the system first needs to actually learn, the structure of all these, objects.

and one policy that I can, tell you about that we already have implemented, which is a, model based policy, is what we call this kind of hypothesis testing policy. Because it's really a way for the system to, given the knowledge of the objects it knows, and given what it's sensing, propose, somewhere that it can sense in the world to efficiently disambiguate between what, objects it's seeing and also what their kind of pose in the environment is. So if you imagine on the left, these are the known objects, that the system has, it's, experienced before and has a, as a model of a spoon, a knife and a fork. And on the right is the observation that it's, experiencing. So it's moving along the handle of the spoon. but what we want for the kind of system to do is having just observed the handle and therefore being uncertain about which piece of cutlery it's observing, we wanted to intelligently be able to understand that the place to move to disambiguate these hypotheses is. is the, head of the object, in this case, the head of the spoon.

And, unfortunately, I don't have time to go into detail about the algorithm today, but at its core, what this is doing is taking the, models that the learning module has, developed, through learning, transforming them by, the information that it's receiving, its understanding of the, state of the world as it is now, and then comparing these representations in a way that it can propose where these, objects would be most different, if they are both present in the environment.

And seeing this in action, if you imagine, on the left here, we have, the spoon, as it's being perceived, again, with the surface agent, which is this, blue line with the kind of red joint, and it's the kind of fifth step in the environment, so the, surface agent has moved along for some time, and at least has a sense that it's on the handle of one of these pieces of cutlery.

But, it's uncertain about exactly what it's perceiving, so what's showing here in the middle is essentially the kind of mental representation of the system of its most likely hypotheses of what it thinks it's most likely in the environment right now. And here, the kind of top two contenders are the spoon, and the knife. And by, as I said, transforming these representations by the information it's being, it's receiving, it can then infer that the, place it should move to, to disambiguate these as quickly as possible, is this red point, highlighted here.

And then what's shown, on the right here is the next step in the environment where the agent has actually taken that information and then performed a rapid, essentially a kind of, jump, to that location, in order to then see that, okay, it is in fact the spoon.

And, this slide is showing essentially the kind of same thing, but with an animation and where, the surface agent is sensing a fork. on the left is a side view of this happening and on the right is a top view. What this also shows is a mixture of the model free policies at work and the model based policy that I was just describing. if you look, now as the surface agent first starts on the object, It senses, that kind of curvature I was talking about that, points it in the direction of, oh, there might be something interesting in this direction, and that causes the system to efficiently explore the length of the, handle rather than moving randomly along it, but without it needing to have any kind of explicit model of what it's sensing at that point. But as it reaches the end of the bottom of the fork, it basically then has a good sense of whether this is a fork, a spoon, or a knife, it has a sense of what it poses in space because it's, not sensed the other end of the object. and so at that point it can basically propose, this kind of new location that it should jump to, to disambiguate between these kind of final most likely objects, that it could be sensing. And so that's what we see, on the right there, where it, jumps to that kind of head of the sport. And then it actually continues its kind of model free exploration. there isn't a tension between the model free and the model based, policies, but rather they are working together to, to help the system, act efficiently in the world.

so Up until now, you've heard a lot then about learning modules, sensor modules, motor systems, and then the cortical messaging protocol that kind of binds it all together. what does this really unlock? that's where I'm going to hand over back to Viviane.

Alright, thanks. okay. So what are the current capabilities of the system? we're working, we're actively working on a paper on that as well, so keep your eyes open for that. But just a brief flavor, what the system currently does. So a lot of our experiments at the moment focus on object and pose recognition. So we show the system a bunch of different three dimensional objects in different orientations and Ask it and see how well it recognizes these objects and the poses of these objects. And the kind of setup we usually use is we have an episode with a series of steps. So each step is moving. So sending one action and getting one sensory input and updating the learning modules hypotheses once, and then next step is moving again, sensing again, updating hypotheses again, until the system is reasonably confident in what it is sensing. And that's when the episode ends. And then we would show a different object for the next episode, and it will start moving again on that new object. And then once we've shown all objects in the dataset once, Then that's what we call an epoch and then in the next epoch we would show all of the objects again in a different orientation, usually. And that's our normal experimental setup. there are also two kind of distinct phases within an episode, especially if we're learning. so we can have a matching phase, we call it, where the system is still trying to infer what it is actually sensing. So here we are moving along the cup and then eventually we recognize, okay, these sensations are consistent with my model of a cup. And then that can be followed by an exploration phase where the agent moves some more on that object and can particularly explore areas that are underrepresented in its model. And then it can use these additional observations to update its memory and its model of that object.

And, we have a set, if you go to our documentation, you'll find a section called benchmark experiments, where we have a couple of tables that report results on, different kind of experiments. here I'm just showing a, a table of results on the, YCB data set. So testing the 77 objects. in that data set and testing it under various conditions, so sensing it with different types of agents, testing, like showing the objects in random rotations that the system has never seen before, and also adding noise into the system. And the last row also shows, trying to recognize it with five learning modules instead of just one. And then we have a couple of other experiments like, unsupervised learning, evaluating that and, Monty Mead's world. I'll also show a brief teaser here. so there we actually tested the system on some real world data. We basically use the iPad camera to take an RGBD image. of some object, for instance here this Numenta coffee mug, and then we have a little patch that we move over that object, and based on this patch and its movement, the system tries to infer which object it is sensing. We also generated some 3D meshes of these objects, and have a bit of a challenging sim to real transfer there that we can test. and then we generated, five different conditions under which we test these images, a normal condition, and then a dark condition, a bright condition with a lot of reflection on the objects, some hand intrusion where other objects occlude this, the hand occludes the object, and then also having other objects in the frame, right next to the target object. And we also have results on all of those, in our documentation, and you can also run these experiments yourself if you like. We also put these datasets, up on AWS storage so you can download them and, all the code is in the repository if you'd like to give it a try.

so what are the current capabilities of the system? One, it can learn from very little data. So usually when we train a system, we show it 14 images of each object. 14, because that kind of covers all the different, features of the object. It could be visible. So we show it from each of its six sides and then from the eight edges of the object, and that's very little data. try training a CNN or a transformer on that amount of data. it's. Pretty quick, the learning. it can also learn with little or even no supervision at all.

in Monty, learning and evaluation are, Very intertwined. The only difference between, training and evaluation is that during evaluation, we don't update models. But also during training, we do inference. We try to recognize the object, and once we recognize it, we can add more points into this object. Since we know the object and its pose, it's very easy to do this. And that way, we also have a bunch of benchmark experiments that test, learning without any labels. where then the system might actually merge some very similar objects. So for instance, here, we showed a mug and a red cup, and they are quite similar. and in this case, the system actually decided to put them into one object model since we didn't tell it that those are distinct objects.

naturally the system can do continual learning without catastrophic forgetting because we learn, the object models, each object is learned in its own reference frame. we don't have this kind of interference that you would have with artificial neural networks. when we train the system we can show, The images in whatever order we want, at a later point we could show it a new object, just add it into the data set that the system knows about, and continual learning just naturally is possible with this approach.

As I showed before, it can recognize objects and their poses, and it can do this while sampling totally new points on the object that haven't been seen during learning, showing the objects in novel orientations or locations relative to the body. noisy observations, and while only sensing part of the object.

oh, I didn't, wanted to build that slide.

yeah, here's, an example of a noisy, object where you have noise in the locations that are sensed and the color and curvature that's being sensed and, we show them in random rotations too that were not seen before.

due to the cortical messaging protocol, cross modality transfer is also. comes out of the box. So we can for one vote between sensors of different modalities, but we could also train a learning module using touch and then take away the sensor module and plug in a sensor module from a vision sensor and then still do inference and still recognize the objects with a pretty reasonable accuracy.

it generalizes based on object shape. So how humans would generalize, here I'm showing, where we show a new object that was never seen before and then what object it would pick from memory to recognize. And those are the most similar objects based on shape.

and same here, if we do a clustering, they are clustered by their rough shapes with cups here and round objects here and so on. Whereas like for instance, deep neural networks often overly rely on texture or other low level features. not humans do, we can also, like Niels talked about, efficiently move across the object to recognize it faster. And lastly, lose, use significantly less compute than deep learning systems and generally just a little compute for learning and inference.

And now I'll hand back to Niels. Yeah. with that kind of overview of the current state of things, I was going to talk a bit about some of the future capabilities that we're working on, the research side. and then also, although applications isn't something we're working on at the moment, what are some of the kind of long term things that we imagine, Monty might be useful for?

and it's important to emphasize again, we're excited about making this open source because, we really want, all of you to be involved in this as well. and some of the, research, items I'm going to talk about on the next few slides. Often, for example, we might have, some reasonably concrete ideas about how we might implement it. but, we're a small team and we don't necessarily have, we haven't yet gotten around to implementing a lot of them. that's one thing where it'd be really exciting to get your involvement. But, of course, we also don't have all the answers and so we're also really excited, to hear what ideas people have, about how we can make improvements. thousand brain systems, reality, and even better.

so one example of something that we're really interested in is, hierarchy and composition, which is, something that's been touched on so far in, this webinar, but it's really our focus up until now has been on getting a single learning module to, really have the kind of capabilities, that we believe that a cortical column in the neocortex has, which is quite a lot. it's certainly more than, is often And so it's a, it's quite a powerful system, and now we feel quite happy that we have at least a lot of those, kind of elements there. and so now is an appropriate time to then start chaining them together, and really getting the benefits of hierarchy, which are, clearly important for how we understand compositional objects in the world. there could be anything like this, Segway that's composed of these wheels and this handle and understanding how those, relate to one another.

another thing that we're interested in and, all of these things you'll see occasional kind of research meetings where we're brainstorming or discussing them, but, one is, deformations and how, the first time in your life that you saw a Salvador Dali melting clock, you immediately recognized what it was. You didn't have to be trained on several examples of a clock like this to do and you're also not just relying on low level features. You are understanding that this is a clock, but it has also a particular unique shape, and building that full representation. And again, we have some ideas for how we might implement this, but it's not something that we have yet. and a kind of related concept is, how we deal with scale. So you can see a saw that's the size of a building, or one that's smaller, the sense, smaller than the size of your fingertip. and you can immediately recognize these, and understand their, relative scale. so this is kinda another interesting one, which, we're excited to start implementing some, solutions.

And then, another really important one is abstract spaces, and I think someone already asked a really nice question about this, so maybe I can touch on it a bit more. but, clearly the, cortex is, where the kind of, representation of these abstract spaces is, is, found. And, what's interesting, and I think Jeff touched on this already, is that, one might think you need some kind of totally new, algorithm or representational space to deal with these kinds of, concepts. But actually the, certainly the, the neuroanatomy shows that every part of the brain, including prefrontal cortex and all these different regions, using the same core, anatomy, the same core, structures, and we, believe that, the, same computational principles, which were, we've implemented in the learning modules, can apply to abstract spaces. And that could be anything from like a family tree on the left to, taxonomy to classify species, on the right. maybe getting a bit more, concrete since someone asked about it, I think, it's fair to say that we, generally feel like a cortical column is, capable of representing 3D space plus, temporal dimension where relevant. And that, certainly any abstract space that a human can comprehend, can be embedded in, in a 3D space. something like a family tree, even though our current learning modules are dealing with these kind of 3D explicit Cartesian coordinates, it's totally possible to embed a family tree, in that space. What really defines it as abstract is the way you move through that space. And the fact that there's this kind of transition of, you go from, child to parent and parent to grandchild, but then you could do grandchild to, grandparent, sorry, to grandchild, which would skip through those kinds of discrete movements. It's probably something about how, the composition is coming in and the changes that are, experienced in the, low level learning modules that are feeding into the higher level learning modules, how that, change in what's represented there is understood as movement. And then that movement is moving you through, the space. But the important thing is the, kind of the same types of reference frames, are still relevant. We don't need some kind of totally different, type of representational space, to deal with that.

and then another thing we're really excited about getting more into is hierarchical, model based policies. So I already talked about kind of model based policies, how these use explicit models of, what we understand in the world, objects at every scale, everything from, The keys on your keyboard all the way to, how you understood your bachelor's degree and how you were going to plan out your thesis, these are all objects that you have a structured representation of and that representation is informing your actions. But for it to be really interesting, any given learning module is only going to know some objects in the world. at these various levels of extraction. So to really plan complex, actions that we are capable of and, mammals are capable of, you need to be able to chain these together and essentially decompose complex tasks, in order to, in order to carry these out. and I alluded to earlier that, model based, policies are this kind of, thing that, everyone's, striving for in AI, but, we really believe that, the, unique architecture of a thousand brain system, the way, sensory information is coming in, the way motor information leaves, the use of reference frames, all of these things are really setting up to be able to develop these model based representations at these levels of abstraction, that are actually necessary to, carry out, model based, hierarchical policies. And this, one example we've talked about, there's certainly simpler ones and there's certainly more complex ones is, if you're going to make a cup of coffee, particularly if it's, let's say, a cup of coffee in a friend's home, where you've never used their coffee machine, maybe it's the first time you're seeing their kitchen, but you still have the kind of sufficiently rich representations at different levels of abstraction that you can decompose this goal into simpler states and kind of goal states and ultimately achieve, what you need to.

so that was just a taster of some of the things that we've been, brainstorming on and we've written up some ideas on. and as I said, we'd love, any and all kind of contributions from the community. so if you're interested, in the research side, please definitely check out, this kind of future work section on our, readme. com where we have some of the documentation about what we're working on, and what we would, love it for people to help with.

So that was more the fundamental research side, where, we're going and where we see Monty developing over the next few months and year. but, another question is, later down the road, what kind of applications, would Monty, shine in? and this isn't something we're working on at the moment, but, but we think this is, obviously, this is ultimately the aim is to really, enable us to, to, help people and, make useful systems. And I think a useful analogy is thinking about how any given technology has a kind of problem set that it maps well onto. And a relatively simple technology, like a calculator, is a, still a great achievement, but, is particularly useful for things like numerical operations. Whereas a, more general technology like deep learning, has found a variety of uses and as Jeff said, we don't believe it's, going away, anytime soon. at its core, it's, really about, being able to approximate arbitrary functions and then where appropriate, perform kind of generative sampling. And so that's useful for, a large range of domains, anything, everything from predicting the structure of a protein, to code assistance, which we ourselves use, and find very helpful. but really the thousand brains project is in thousand brain systems is where we believe, the, kind of technology that's going to be needed for, sensory motor learning in action, which is a huge space of, very challenging, domains that, are all around us. robotics is, the obvious one, but, I'll touch on more on kind of other examples of this, but in general, if you're ever wondering what is something that Monty would be good at. It's worth kind of thinking of, what are the kinds of things humans are good at, and, it's all these things Viviane already mentioned of, learning with small amounts of data, building structured representations, moving efficiently, things like that. What humans are not good at is, for example, crunching large numerical operations. And so that kind of gives some sense of like the scope of where thousand brain systems are really going to be, at their strongest.

And this is not an, application that we're working on, but just to give a concrete example, one that we've thought was an interesting, one that's certainly relevant is, medical ultrasound, which of course has a variety of important use cases around the world. and medical ultrasound is, inherently a sensorimotor form of, imaging in the medical world because, the sonographer has this probe that they are capturing this, 2D slice of the patient's tissue. But, firstly, the, thing that they are, trying to understand, the thing they're trying to structure, sorry, understand is a 3D structure. and any given, view with the probe is going to be suboptimal in some way. It's either going to, cut through the tissues in a certain way that you're not getting all the view you're, wanting to, or it might be obstructed in the same way that, if you look around a room, certain objects will be invisible because they are, or not visible because they are behind other objects. a sonographer, has to move this probe, in order to build up, an understanding of, the patient's health. and so there's a variety of ways, they can do this, but ultimately, what this is enabling is something, with an extremely complex structure like the vasculature system of, of the liver can be, understood, using these again, it's the, as Viviane talked about, it's the partial view of the world at any given time, but by building that up over time with movement, you can develop this more full representation.

And just revisiting some of the real strengths of thousand brain systems in this kind of view. as mentioned, sensorimotor, tasks are where, they are going to shine, and that doesn't need to be, physically embodied, that could be in, cyberspace, or it could be, some more abstract, control system, but it has to be sensorimotor at its core, and certainly, something like ultrasound is a good example of that. another kind of key, thing that thousand brain systems, excel at is dealing with small amounts of data. And, this is something that comes up in many, domains, but certainly in, in medical ones where patient data is hard to come by is one of them.

there's also the ability of the kind of thousand brain systems to perform inference with, very small amounts of, compute, certainly at least in compared to, many deep learning systems. and, this could have advantages, everything from more kind of green, AI systems to, being able to do computations at the edge, where, necessary. And then the representations that are developed, are structured, and this has, a variety of downstream benefits, but certainly one is interpretability, and again, there's many domains, but certainly, medical ones where it's really important to understand how the system is developing its representations, what those representations look like, and how those representations are influencing the kind of decisions or the outputs that it's making. and that's something that, that we think, the thousand brain system is, well suited for. but these structured representations are also useful for robustness and generalization. by developing these kinds of explicit models, it's easier to transfer from, training data, which might be relatively different from, or transfer to test data, sorry, which might be relatively different from what was trained on, which is currently a very challenging thing, to do. And related to this is the kind of importance of continual learning, so again, many domains where this is useful, but certainly, the medical field is constantly changing, and it's, it's not great if your entire model, you need to go back and essentially retrain the system on some batch data in order to update its representations.

And finally multimodality, which again, Vivian touched on, and there's different forms of this. one is transfer in modality. So you can imagine, a system, for, ultrasound that's trained on 3D models, for example, in anatomical atlases, but then can transfer, when it gets, actual ultrasound, into a different modality. And then there's also those voting connections, which enable information to be shared across modality at the same time. for example, medical imaging, could give some information about the health of a patient at the same time as, the sonographer, or rather the sonographic system was operating. and all of this can work together.

that was just an example of, where these strengths might come out, but, really, these are, it's a large range of, advantages that we think, are gonna have, a diverse range of applications, and of course there's others that, I didn't mention, as well as, small amounts of compute needed at inference, it's even doubly so when it comes to training because of how rapidly the system learns, and together with that, the system's able to learn, the vast majority of the representations it has in an entirely unsupervised way, which is obviously important for many domains, and then those kind of model based, hierarchical policies I was talking about earlier, those are really important for, being able to deal with novel problems and a diverse range of tasks.

so we're excited about this because, sensorimotor problems are everywhere and, it could be anything from, robotic use cases in domestic or industrial settings to, being able to control experimental, apparatus, in order to accelerate science to, navigating through, more cyberspace for, security purposes. There's, a large range of, problems that we hope this will really help with, but, Jeff often makes the remark, and, I think this is a useful one, that often it's hard to anticipate what the actual, use cases of a technology are going to be, when it's being developed, and we're really focusing on developing that core technology, making it easy to use, getting your help in, accelerating this development, and then we really hope that, yeah, it can go out there and, And bring a lot of benefit to the world.

So yeah, we're really excited to collaborate with all of you. and, particularly on that point, I'll hand over to Will, who will talk a bit more about that. All right. Thank you, Niels and Viviane. that was a lot. I'm sure a lot of you, that was a lot of new information. And your brain might be melting out of the side of your head. I'm going to talk a little bit now about the community and the engagement, that we would like from you guys. and so with that, we'll get into it.

first I wanted to start just by saying thank you to the community, so far we've had an overwhelming number of responses, positive congratulatory questions coming in. These are just the first week of YouTube comments that are being displayed here. So we were overwhelmed. We didn't expect this kind of turnout so quickly. we've also had great responses from, our community forum. so interesting and informed. Questions and discussions that are happening over there. I wanted to call out D Led specifically, because he is not only asking questions, but is also the first pull request that we merged from the community into our repository. thank you for that fix to our documentation. And again, to Humble Traveler, just thoughtful, robust kind of responses that are coming in. This one about qualia, and other interesting, Areas as well. And again, Raleigh has, trying to implement on a Raspberry Pi and ran into some issues there. and so this is exciting for us because if you're putting Monty on a Raspberry Pi, probably it's going to be strapped to a robot at some point. And so we're excited to see what the community does with this as we progress. And thank you to everyone that started our repository in the first few days. We grew exponentially really quickly to 120 stars. And this really helps with our trust level within the GitHub community and other systems as well. Thank you everyone so far who've contributed and thank you to everyone who's asked questions so far. We will get to them, in about 15 minutes or so. Okay, so I want to give you a quick lay of the land of the systems that are available to interact with us and interact with the community in no particular order. Here is our github repository. so thousand brains project organization, and then tbp. monty. this is where we store all of our code, all of our tests, and all of our documentation as well. So if you want to make changes to any of these parts of the project, this is the repository that you will do it on.

I also want to shout out to Monty Lab. So this is a repository that holds all of the experiments that we're building. So if you saw the video of Monty Meets World, the experiment is laid out in this project. you can go and have a look at that. and this is. Projects that use the TBP Monty code to run an experiment.

We also have our documentation server. This is an extensive set of documentation about all the concepts we've covered here. It goes in depth with videos, diagrams, and all kinds of information. It's broken into six major steps. So an overview of what the project is about, how to use Monty, about getting started, how to install it on your machine, how Monty work goes into all of the architecture behind the code and what that looks like, and there's a section about contributing, how to contribute code, how to contribute documentation, how to contribute tutorials, and then there's a section of community that has our code of conduct, our patent pledge, and then a section that Niels mentioned just now which is our future work. These are all the large blocks of work that we or someone in the community can take on and start developing against, so we're excited about that.

We also have our YouTube channel, so I'm sure a lot of you have been watching our videos. We have over 200 hours of video footage of our research meetings, and we've structured them and broken them down into different playlists. The primary one is the Quickstart playlist, which is the shortest number of videos where we can describe all of the concepts in depth so that you can get up and running with the Monty project. It's called Quickstart, but it is six and a half hours of stuff. It's a solid material to understand and Jeff mentioned to the learning curve of this project, it is steep. These are new concepts. The actual code and implementation is not complex, but the concepts are new and you will have to start to understand these. You have to think about the world in a vaguely different way. So that's the quick start series. And then we have the core series, which is essentially just like the quickstart series right now, but it will have more videos over time as we publish more of these videos, and you can go into much more detail about any of the concepts that are covered. And then we have two additional categories, which are a more raw format of the videos. These are brainstorming and reviews, brainstorming where the team comes together with a specific idea, and we talk and discuss through what that means and how we might implement something. And then reviews are where somebody comes to the team as a subject matter expert and presents on some state of the art in some technology field or discipline that might be reinforcement learning, it might be robotics, it might be a neuroscience paper that's come out that's interesting and talks about something that we haven't covered yet. So those are all available to you on YouTube and more will be coming over the coming months. Those are exciting. Then we have our discourse forum. So this is where all of our community members can chat and ask questions, post topics, get answers to questions. If you get stuck somewhere, you can come along here and ask a question about something that you have run into trouble with. The forum has four major categories. General, the questions and information that doesn't fit into any of the other categories. Videos discussions for, whenever we post a video, we'll also post a reference to it here, so you can have a threaded discussion about the concepts that are talked about and explained here. And then research theory and the Monty code are the two areas to talk about those disparate sides of, the Monty project as it's implemented.

we're also on various social media, so you can follow us on Twitter or on, BlueSky or on LinkedIn. There's our Linktree address at the bottom there, so you can connect with us, on those forums. And we'll post much more frequently on these forums. This is where we'll post every video, anything interesting that happens, any articles that are published, we will, feed those out through these social platforms.

Okay, so let's talk about getting involved. This is the most important part, for us, having the community involved is, a core reason that we made this whole project open source. so the problem space, what kind of problems, will we solve, are we looking for, what kinds of, issues. will this solve? So one of them is if you're just looking to get away from, the artificial neural network benchmarks. So there's a lot of, fluff out there right now. There's a lot of information about neural networks and deep learning and transformer networks. and it can be quite overwhelming. There is a lot of new direction that will come out of this project. So we're excited to see what your take on it is.

So you want to understand the principles behind human intelligence. So as you progress through your understanding of a cortical column and how it sees the world in these little patches, it really transforms your understanding of your interaction with the real world. And so that's a fascinating thing that I've found as I've gone through and learned these principles.

Do you have a limited compute budget but still want to do AI research, interesting AI research? if you want to run a deep learning system and train it on 20 million images, this costs a considerable amount of money to run on CPUs and GPUs. As you saw from the presentations, Monty uses Orders of magnitude less data to learn its representations and therefore is much cheaper and faster to run, and that will continue to be the case. and again, if you don't have access to the data sets that will allow you to build a deep learning network that can do something as well as a human, which is a huge number of tasks, so you don't have internet scale data to train on, then again, Monty would be a good fit.

again, do you have an application with moving sensors? as Niels mentioned, there are all kinds of different, applications with moving sensors, sonograms are one, there's all kinds of different problems out there that have these. we're also interested in novel industry applications, so if you have sensors that move around that we don't know about, we'd like to hear about those as well.

do you have an application where you have to integrate multimodal sensors? So maybe you learn an object with a vision, but you need to manipulate it with touch, or you have a combination of radar and lidar, and you need these things to form stable representations. And again, Monty can do this. It's very hard to get deep learning networks to do this. You need a system that learns quickly and continuously. With most deep learning systems, you have to have a separate training phase that can take, hours and weeks to run. and that is not applicable if you're in a novel environment and you have to learn on the fly. so again, Monty will be a good fit for these kinds of problems. and you, are you tired of just throwing a black box brute force approach at the world? Like you get a deep understanding of how the system is learning with Monty. and it makes sense to you as a human, as opposed to trying to understand how deep neural networks. Balance their weights out. They're always surprising when you try and take them apart and look inside. So that's the problem space. If you have all of those, that's great. If you have some of them, that's also good. and then some incentives. So why would you want to do this? so we think that you will be able to write publications based on this that will be widely received. we think that the various modules that we're going to build and that the community might build will be, can be the basis of a master thesis. you can write a master thesis on the entire thousand brains approach as well. you will become part of an awesome community as you saw not only the thousand brains project team, but also all of the people that are already contributing and asking questions.

you will have your project showcased. So if you do strap a Raspberry Pi to something exciting, please let us know and we will put it on our, documentation. We would love to see all of your applications, robotic or not. so please let us know how, you get on. if you do publish a paper, you can get us list, you can get you listed on our TVP papers page. and you can contribute code and start, adding to this active community. you can also unlock achievements. This is one of many. but when you submit an RFC, which I'll get to later, when it's merged, you get to choose a player icon so that on our roadmap, you can see which parts are being worked on by the community. Okay, and with that, let's have a quick look at the roadmap. so this is a representation of a spreadsheet that's available online. You can go and look at it right now. and each of the checked off parts of things we have already built and the majority of them are unchecked. And these are things that are on our roadmap, things we're going to build in the future. And we expect the community to help build these. We do not intend to build all of these ourselves. We would be thrilled if a community member took one of these and ran with it and tried to implement it. Let's have a look at some of these in a little bit more detail because they're small. so in the voting section, you heard about voting and how columns vote together to do faster inference. we currently don't use pose for voting, so that might be an interesting exploration, like how much can we, how much faster can we make inference if we send around pose?

there's various, pieces of the environment that we want to clean up. So one of the questions that came up just now is that, we're not compatible on windows right now because of our habitat environment. And so we want to decouple that. And so that anyone can run it on windows and, any other systems, operating systems as well.

there's lots to do on the motor system. We're very excited about this. This is the key to robotics, where we're going to go in the future. Bottom up exploration policies, top down exploration policies, calculating a salience map. And if you find it, it find. Areas of interest for learning objects. Then on the framework side, adding GPU support so we can speed, speed inference up, all kinds of interesting, neuromorphic hardware approaches that might be done in the future. Then heterarchy and hierarchy, more learning modules, more sensor modules, These are all things that are on our roadmap we haven't done yet and we would love for the community to get involved in.

Okay, I want to talk about two processes, like the actual act of making changes to the Monty repository. we're going to talk about pull requests, which are for essentially minor changes, so smaller work items. And also again, I'm going to talk about requests for comments. And this is for major changes to the system. Think a brand new sensor module or a change to the cortical messaging protocol. I'm going to talk about these in detail. so pull request is pretty simple if you're familiar with GitHub. the pull request process is almost identical for what you're familiar with. You fork the repository, and then you clone it, and then you make whatever changes you're going to make to the code, fix a bug, write a tutorial. If it's the first time you're committing for opening a pull request back to our repository, you'll be asked to sign the CLA, the Contributor License Agreement, and then you'll go through the normal PR review process as you do with any kind of code or software changes. That's pretty simple, pretty standard. I'm going to talk a little bit about the request for comments now. So this was new to me when I joined Numenta. but I've since come, love, love this process. and an important premise for this is the, Thousand Brain project team is committed to working in public. We want, your input. We want your feedback. We want your contributions. and in order to do that, we have to work in public. We can't just make decisions off on the side and then surprise you with them later. And the RFC process is a way of doing that. It surfaces, it makes visible, the major changes that we're going to make and why we're making them. And we will, solicit feedback from the community to see if they, think it's the correct thing to do, the right implementation, the right technology choice. we're excited to start doing this. We've been using RFCs internally for major changes, for about two months now, and they've surfaced some really great results. Changes to the system. So like I say, they represent a major change to the system. It's essentially just a document that describes what that change is going to be. And anyone can come along and comment on an RFC as it's in process. And anyone can create an RFC, both internally to the thousand brains project team and to the community at large.

So what do they look like? essentially when you create an RFC, you create a document. We have a template and that template will have kind of various sections with helpful text to help you understand how to fill them in. But essentially it's a summary of what you're changing, the motivation of why you want to make changes. this change, any rationale and alternatives that were found in your research, different ways of doing this prior art and references, which, if you're building a new sensor module, probably there's not going to be any prior art. This is pretty new technology, but if you're building something that speeds up using some kind of technique, then there may be prior art there as well than any unresolved questions and future possibilities. So you create this document and then you create a pull request against the repository, with this document in it, and then you'll get some comments. We'll go through the normal. change comment review process. And this will happen a fair number of times as we strengthen the idea, add questions, ask for alternatives, maybe try, tiny implementations of proof of concepts. And we'll come to this final comment period, which is, again, where you'll see, us as the thousand brain project team make these thinking faces where we'll decide, are we going to incorporate this? Is this something that we want to do in the future? and if it isn't, we'll close the RFC and if it is, we'll merge the RFC. And so now this document exists, inside a repository of something that we want to do. Nothing has changed in the code yet. Like we haven't built anything yet. We've just agreed as a group and a community that this is something that we want to do in the future. And then anyone in the community can have a go at building these. if you find an RFC and it's not yet implemented, we welcome you to try and implement it, and see how you do.

Okay, this is my last slide, so I wanted it to be an eyesore, but I wanted to talk a little bit about the future from my perspective as well, the presentation you just watched went into an enormous number of new ideas, and it can be overwhelming, but we believe, as a group, And we hope the community agrees with us, and this proves to be true, that sensorimotor learning is, the future of machine learning. We think that AI systems in the future will all be based on this or something very similar to it. Sensorimotor learning is the core of all of this that we do, and we think that the technologies that this will enable will, be the foundation of a new era of human progress. We think that the technologies that we'll be able to build on top of this platform will be incredible. so we're super excited about the potential of this project. The reason we have made this open source is twofold. One is that we think this technology should not be in the hands of large corporations that can afford to run huge deep neural network training sessions as it currently is. And we don't want this to be in the hands of state level actors. We think that everyone in the world should have access to this technology. And so that's part one of why we made it open source. The other part is that we're a small team, as Niels mentioned. There's only seven of us, right now that are working on this, and if we want to make this A reality. We're going to need help from the community. We need you guys to join us and to build this and to help us form the foundation in the best way possible. So we invite you to join us and we hope you do. Okay, that is it. It is on to the next section, which is our Q A section. So we have a lot of questions, and so I'm going to state those questions, and then the members of the team can answer them as they see fit. You guys all ready? We have over 126 questions. I don't think we're going to get through them all. we may have a separate recording session where we just answer questions and release that as a separate video, but we'll see, how we do. I'm essentially going to go through them in vote order. So I'm going to sort by voting and then ask those questions. And if you want to answer it, you can. Okay. here we go. So this one was from the beginning of the presentation. How does the thousand brains theory handle the integration of abstract concepts that lacked direct sensory motor grounding?

how are we, how are we going to decide who answers these questions? I guess I, I touched on this one a bit, so I don't know, was there anything you wanted to add, Jeff or Viviane? I guess I would just give a little historical context to this question. it was damn surprising. That the brain works this way, right? No one anticipated that all the things that, we were able to do as humans are derived from this simple algorithm, it's not that simple, but this common algorithm, but that appears to be the fact, so we have to start with that assumption that this is true, and then we're working at it, and Niels talked about it very nicely for a bit, I wrote a little bit about this in my most recent book, And I'll be honest, we don't completely understand it yet. We have, glimmers that we sometimes we think we understand it and then we say, maybe quite. So we're getting there. But I think, just adding on to what Niels said, the concept of movement and sensation do not have to be physical, Movement just means that the thing that you're detecting is, using to, measure something is, now in a different location, but that location could be, doesn't have to be physical space. It could be, it could be conceptual spaces. Neil's talked about it.

be honest, we haven't quite figured all Yeah. Yet, but we're, I'm confident. We'll, get in a short period of time we'll have, very concrete answers to these questions.

Okay, great. Next question from Roderick. How does the thousand brains theory differ from its earlier incarnation, hierarchical temporal memory?

We didn't talk about that here. hierarchical temporal memory was a term we used for a series of technologies, that we developed along the way or a series of ideas. I would say, if I was new to this project, I would, I wouldn't focus on the hierarchical temporal memory. It's not that it's wrong. but, we've changed the language, we've changed the way we think about these problems. one of the key aspects of the hierarchical temporal memory is, we came up with something called, came up with these models of sequence memory, that, models of neurons, which we still think are true in the brain. But they don't have any direct, we don't model the neurons, we don't model that mechanism in Thousand Brains Project. There's other ways of doing it. So the short answer to that, there's no simple answer to that question, but if you're new to this, I would just focus on the documentation that the team's put together for Thousand Brains Project, which is really excellent. And, and someday over a beer we can talk about, how we got there by doing the HTM. And I would just add to that, that those two are also not opposed to each other. They just look at the problem from different directions. So the thousand brains project looks at the higher level principles, like this repeatable functional unit, the learning modules or cortical columns and sensorimotor learning and reference frames. Whereas the hierarchical temporal memory and previous work at Numenta looked at a lower level, like Jeff mentioned, modeling neurons and dendrites and temporal sequences. And yeah, and maybe one other thing to add is just that, in the future, we are open to the possibility of bringing these ideas again into the Thousand Brains project, and so it was a very deliberate decision that, for example, the reference frames we use are these explicit 3D coordinates rather than simulating them with grid cells, because, here in this early stage of the project, it makes it much easier to visualize the system, to understand it, to describe it to other people and, for them to see how it's working. and so there may come a point where we feel like there's, such a definite, improvement from switching to these more neural representations that we're definitely going to go down that route. But, there's also advantages to these kinds of, higher level approaches. and so that's always, a balance where we're going to strike and, maybe to Viviane's point earlier, these learning modules can interact together. maybe there'll be, hybrid approaches as well.

Excellent. this question from Avinash, it's an AGI question. do you think that by scaling these systems to more than 150, 000 cortical columns, human intelligence can be surpassed?

I'll start again.

first of all, I don't think it's a goal of ours to surpass human intelligence and humans are very complex creatures, right? And we have, some people argue we have emotional intelligence, we have various types of capabilities that are controlled by the neocortex. so it's not a goal to recreate humans or recreate something that's just like a human. We are, our goal is to model the neocortex, which is a modeling system itself. and to be as smart as a human, you probably have to live a human life, you have to have human emotions, you have to have human drives and needs, and none of the systems we're creating have any of that. but is it possible to build machines that are, can do things that humans can't do? Sure, I can easily imagine robotic systems that are much more precise and accurate. and, capable than humans are, and can beat us in other categories as well.

yes, it's possible to do that, I don't, but that's not our goal. And as I mentioned at the beginning of this talk, I think many of the applications The vast majority of them will not be human like at all, they'll be embedded systems doing simple things, or things in areas we can't even imagine today, so I think the goal of replicating human intelligence is, it's not really one that we care too much about, it's more about creating this technology that understands the world, through different sensors, different modalities, different embodiments, that can be applied to do things that humans generally can't do.

I don't know if anyone wants to add anything to that, but All right. next question from Konstantinos. At which point is the concept of a cup cut out from the continuous stream of features at locations?

That will be a Viviane or Niels question.

yeah, I guess I could answer it on a conceptual level or on the algorithmic level, but essentially the model of the cup exists in the associations in the cortical column between layer four and layer six neurons. So in the implementation associations between features at locations in a reference frame that is specific to that object. So the cup. Model all the features on the cup, share a reference frame, so all the locations exist in the same kind of space that you can traverse for the cup. And then the point at which you actually recognize the cup or whatever model object you're sensing is when you have gotten enough sensation that you narrowed down the possible hypotheses to that object and excluded other objects that might be similar in some regards.

in the, cortical column, or if you are coming from the HTM terminology, it would be like the spatial pooler or layer two, three in, in the cortical column where that object ID is then represented.

Yeah, and maybe just one thing to add. I don't know if this is what the question is getting at, but I yeah, maybe if it's also asking like, how is unsupervised learning really help, happening? how is it understanding, okay, I got this stream of information, now I'm going to learn a cup, and then you get some more information, now I'm going to learn a separate object. At least this was a something that some of the other questions touched on. So yeah, unsupervised learning is, already working in the system in that, it can receive, this kind of. Information, and as Viviane touched on, if it senses something and then decides that, okay, the part I've seen is an object I saw part of before, I'm going to recover that model, and then I'm going to start adding information to it. but it's not a perfect system, and this is one of the things that we're looking forward to implementing soon, where we don't really have this sense of forgetting or kind of paring down information at the moment, but that's definitely something we think that's important. And so we have some ideas for how, over time you can, sometimes the system will make a mistake and it will merge two objects that were similar enough that it thought they should be one object, but in fact they shouldn't be. But we imagine that's the kind of thing that over time, you'll develop these better representations where the merging really does correspond to, what is the actual structure out there in the world, as opposed to the kind of one time you saw it from a bad angle and then that led you to an incorrect conclusion.

Great. Thank you, Niels Jeff. a question from Jeremy. And there are a few questions about language. So I'll summarize all of them in this one question. Do we have a vague or clear path to how language works in this system? And how do you think about language later on? And do you think it's important to the Monty system? Yeah. I can start on that one. so we did talk about language a fair amount of times. language is something that also in humans is something that comes last. So you first learn about how the world actually works, how we can interact with the world before you start actually producing language. We don't plan to start with language as like LLMs do, for example, but we think that, the system is certainly capable of understanding language and producing language as well, and would actually have a much more grounded understanding of language. as Jeff already mentioned, language is also a compositional construct. letters are made out of strokes. Words are made out of letters, sentences are made out of words, and then the actual words that you're hearing or reading, can be associated with the concepts that you've learned through other modalities. So if I hear the word cat, I can have an associative connection to, a learning module that actually modeled a physical, Object of the cat, like how the cat feels like when I pet it and how it looks like when I look at the cat. And that gives us a much more grounded understanding of language. I don't know if you guys want to add something to that. Yeah, I think, we should have more conversations about this topic because it's so fascinating to think about. the one thing I would add to what you just said is, we use language, this is something we use language for, very often we just, we use language to describe the physical structure of things. if I, if someone's visiting, say, how do I, where's the post office in town, I'll say, you go here, you see this, you go here, you see that, or I'll say, where's something in your kitchen, and so on, I think language probably started as a way of transferring Representational knowledge and reference frames from one person to another. Where did you get that food? it was on the other side of that hill over there, and you had to go so much further. And then, that's probably where it started. It's just a way of conveying the knowledge structure that's already in brains. and in some, if you think of it that way, it's pretty simple. It's a matter of taking the, outputs of these learning modules and instead of acting on 'em directly, you just, you, send representations of the different, outputs of the learning modules to some system, which then can be conveyed to someone else. but I think, Vivian said it correct, it's not our focus right now. I think ultimately the language, these systems will have language, they may not be all, many of them will have the language we use, they'll have their own languages. And if you think about it systemically, the goal of language is really to take one, Monty like system and another sort of Monty like system and have them share knowledge, and have them cooperate. And if you think of that way, it's not really hard to do. There's lots of ways you can do that. Anyway, I'm just rambling as we often do in our research meetings, but it's a great question, and I think it'll be fun to work on.

Great answers. Minimi says, can you give us a feel for how you would use Monty with time series data, or how do you think about time in general?

Yeah, it depends on what you mean with time series data, so everything that Monty learns from is a time series, since it's a sensorimotor system, there's always a time component in what we're learning, but there's also always a motor component. So it's not just a passive series of feature inputs, but it's actually features and movement that the system learns from. so learning from just a time series will probably be a, lead to a pretty impoverished model of basically just that time series. and we would focus on learning from sensorimotor data.

Maybe one thing to, again, I don't know if this is what the question's getting at, but, one thing we sometimes talk about is object behavior, and this is something that I think it's fair to say we're still fairly uncertain about, we're still trying to figure out the details, but, we often talk about the example of, opening and closing a stapler, as, a simple example of object behavior, but there's, lots of complex behavior out there, even just, at the extreme end, how do you understand a T shirt and how that can behave and things like that. and I think maybe just at a high level, like two of the main kind of ways we've discussed this is, which I think is what the question is alluding to, any kind of behavior systems or what's observed as being changing over, is changing over time, potentially dependent on the motor actions. And In some sense, that's a sequence and can be learned as a sequence. And that's where algorithms like HTM and things like that become relevant. but then we've also sometimes talked about object behaviors as you have, you can think of an object as the different components, the subcomponents of that object, and how they interact with one another, almost like a graph, and how the kind of edges connect these, These different subcomponents, and maybe there's some kind of way that the cortical column is understanding how a particular subobject influences another one. and maybe it's a mixture of these two things, that one is useful in some senses in some cases, and the other is useful in others. that just gives like a high level of where our thinking has been going about how we would model object behavior, which ultimately is about how objects change over time. Yeah, that's right. that's, the key of it, right? We were talking about it's, the world is changing over time. One of the funny things about this is one of the first things we ever did, And from a neuroscience point of view, try to figure out how neurons learn time sequences like melodies and so on, these high order sequences. And we came up with a really compelling theory for that. I think it's right. This is in our paper called Widening Neurons of Thousands of Synapses. And that paper's done extremely well. But we came to the understanding, and from a neuroscience point of view, this is not from the Monty point of view, from a neuroscience point of view, the same set of cells that can do sensorimotor sequences can also learn high order sequences. And going back to what Neil just said, you may, the first time you do something, we were just talking the other day about typing, the first time you learn to type, You, that's, you really go through this sort of model-based version of moving where to move your finger and how to press and so on. But if you do it enough times, you don't, you just learn the sequence. You don't have to think about it anymore. It just plays back the sequence like a melody. and so the same set of neurons in the brain can learn, can do both sensory motor and through practice. wrote sequence playback and recognition. but we haven't actually. that, that doesn't directly, those mechanisms don't play directly into, the thousand brains project today, but they will, as Niels talked about it, as we do behaviors, sometimes behaviors will be very model based, but then we can get good at them by practice, and I think Monty's systems will do that. Anyway, I guess in the bottom line I'm saying we really have a great model of sequence processing in neurons, but it's not exactly in Monty yet.

Okay, great. next question from Jeff, not for Jeff, necessarily. When each learning module builds a model of a cup, it has a different ID for cup. When learning modules vote, they use their own independent IDs. So how does that vote for the same object?

I feel like, yeah, Viviane, this may be a good one for you, but yeah, great observation. Yeah. Yeah. I can answer that one. If, unless you wanted to, Jeff, you look. The problem is my answer will be a neuroscience answer. So if you want to answer from a Monty point of view, you should do that. basically they just learn associations to whatever comes in. it's a bit of a neuroscience answer as well. If you imagine, a cortical column in the brain, receiving lateral inputs as votes, it just learns to associate whatever. These are inputs it gets as it's sensing this object, over time, and these associations can then be used. So there's no way we could have the same ID for an object everywhere across the brain, but instead they locally just associate whatever inputs they get when they sense this object.

And in the brain, these patterns that are associated are very sparse activations. So you might have several thousand neurons that represent object IDs, but they're sparse representations. So you're going to have a few hundred neurons that are active. And these, the properties of these kind of neurons. These kind of sparse properties allow the system to spread these patterns across large number of cortical columns. There is no, as Viviane said, there's no central agreement on what the ideas are. It's just some neural pattern, and each column has its own neural pattern, but they can all learn associatively to associate their neural pattern with someone else's neural pattern who's observing it at the same time. So if a bunch of columns are observing the same object at the same time, they'll learn to associate their outputs with each other. there's no, there does not be any agreement about what the call is saying, they don't even know who's else out there, they're all that's beauty, we've been touched on this, these learning models are really independent of each other, there's very little they have to know about the other ones, for the whole system to work. Anyway, you have to trust this, it's not clear, it really does work. It's also worth maybe mentioning on that, and maybe Jeff Thompson, you've looked at the code and this is why you're asking, because actually the way it's currently implemented, so that's our understanding of the algorithm, and that's what we're, aiming to implement. The way the code currently works is actually, there is this object ID, that is in some sense shared between the learning modules. And that's really just an implementation detail. Again, we've alluded to, we're a small team, we haven't had a chance, but if you go to the future work section on our readme, at least once the PR that's, outstanding is merged, there's a section exactly on this thing that we'd like to implement these associative connections. So that's a kind of cool example of if that's something you're interested in, we'd love, help with, implementing that. Actually, I knew it. I didn't know how it was actually implemented in the code. This is a great question because it brings up a general problem or general task we have. We started this whole project with all these deep neuroscience ideas, and so I can tell you exactly how the neurons do this, but then how do you implement this in the code? We have a lot of choices. We don't generally want to just model all the details of neurons. and so there are other ways of implementing it. So I guess what Niels has said is, we took a quick way of doing it right now, but we do want to get back with sort of the properties that the brain does. It doesn't mean we have to model exactly how the brain does it, but at least we know that how the brain does it and we know that it can be done. And so it takes a little bit of maybe clever engineering to say, how would I do that in, code about modeling neurons? So I think that's something we do a lot. We do a lot of, the tasks we have ahead of us are things that we, we, sometimes understand better from the neuroscience point of view. and, but we can't just directly implement it just like that.

Okay, great. I think we'll do a three or four more questions and then we'll call it. So the panelists can take a break. Ray asked, what is the basic learning algorithm being used here? Is it some sort of gradient based learning? Or is it based on Hebbian learning or something else? What do you call it?

Yeah, I would say that, you can think of it as a form of Hebbian learning. it's associative learning. and, I mentioned before, like right now our system is this kind of sponge that absorbs a lot of information, but it's not really forgetting things. but that's something we want to bring in as well. That there's essentially some degree of decay in, in memories and things like that. But, Yeah, core throughout it it is Hebbian. it's, not at the more extreme biological realism side of spike time dependent plasticity or anything. anywhere where we do think time is maybe important in the brain, there's normally been ways of abstracting that away. and it's also, it's definitely not backpropagation of error. we've had some instances where, we might use gradient descent within a learning module. and that's something we've debated from time to time, but we definitely don't want to be passing these kinds of errors, over long distance connections in order to do, yeah, essentially backpropagation.

so I guess, yeah, the TLDR is, you can think of it as Hebbian, it's associative learning. Yeah, and that, that's one of the things that, that make it so interesting. It's such a fast learning system because we have this fast associative learning, that the system use and local, very local learning, within each learning module. and that, also makes it robust to catastrophic forgetting for instance.

Okay. next question from Erica. I'm curious why curvature is prioritized but not planar intersections. My understanding of infant development is that the intersections of planes of the object are critical for the visual system to make a mental model of objects. My intersections serve as a relevant parameter for both curvature and tactile input with intersections of planes as a tactilely perceivable but also visually critical input. That was a long one, but I think useful. Sounds like a Niels question to me. Yeah, I'm not sure. To be honest, I'm not familiar with this, but it sounds super interesting. I feel like this is a great example of, something that could be an RFC. if, at least, yeah, in my case, I'm not very familiar with this. If you want to educate us about how this may be relevant, we're always looking at, this could be something where maybe the sensor module, could be updated, the core stuff will still stay the same, it's, poses at locations, but as Viviane alluded to, there's a whole bunch of other features that can be passed into the system, and so maybe something like this, could be one of those. Yeah, I would agree. I also don't have a lot of knowledge about this. so I think I'm going to look this up after this meeting, but, if you can send some more details on the discourse forum, that would be really cool. and yeah, To start, we were just looking for features that are simple to extract from a sensor patch and we just wanted to get something to get the system working and principal curvatures fit the bill since the two principal curvatures together with the point normal span up a reference frame and we can very easily use that to define three dimensional rotation. of that point in space. And that's why we picked that.

Okay, excellent. Bamshad asks, Does this algorithm have potential for optimization on current computer hardware, or would it require neuromorphic hardware for better performance?

I'll take a stab at that. It doesn't, at the moment, we don't see neuromorphic hardware in its traditional definition that's playing an important role. And now, the neuromorphic hardware generally, as it was originally conceived, is using these sort of analog properties of semiconductors, to model the analog properties of neurons, not using them in digital form. we're not going down that path, we're not modeling sort of analog neurons in our system right now. that could be, someone could do that in the future, as, Viviane said, you could create new learning modules. on the other hand, I think there'd be a lot of hardware accelerations. The, there are, there's lots of operations here. In fact, the whole system is ideally suited for semiconductor based computing because you have this basic learning module that you have to create lots of them. And so you could optimize hardware to achieve the purpose of these learning modules and then make all kinds of systems by replicating and putting more or less on a chip or different types of stacks. so I think hardware is going to play a role, a real important role in the future of this. How much neuromorphic computing hardware is going to play a role is yet to be seen.

Excellent.

another question from Allison. In sensorimotor learning, will it be required for a computer to have a reference frame of self? How it incorporates itself in the world? Niels, do you want to take that one?

Yeah, sure. yeah, this was something that I didn't get to in the policy section, but it's definitely something where we're interested in. And yeah, and we think it is important. In fact, I was talking a bit about how you have this more kind of almost classical concept of the motor cortex. And, that's where a lot of motor stuff is happening. But we think that's probably what kind of that part of the brain is specialized for is, how are you modeling the objects that are your own kind of body parts that can interact with the world? and so it's it's the same principle, you have these structured reference frames that, model the objects and there's some behavior associated with them. and then these would be representations that could be recruited, by other ones, by other learning modules. That's I guess at the kind of lower, more kind of tangible sin, what you call it, a spectrum of self. in terms of like theory of mind and modeling one's own mind, it's not actually something we've discussed that much recently, but I think it makes a lot of sense that, depending on the use case of a system, that obviously becomes more important, that's what enables primates to be such social intelligent creatures is because they essentially model their own minds and, those of others. and have that kind of theory of mind. so I guess it depends on the, use case.

Excellent. And, so one, one final question for the group here. as, the software is open source, do you worry about, the future of this technology falling into the wrong hands or people using this technology for evil? how do you think about the implications of bringing this technology to the world?

Did you want to? Just, take that one, Niels, what you wrote down earlier? Yeah, or yeah, go ahead. Yeah, I can also start off, I think it will actually make it safer, putting it into the atmosphere so that anyone can look at it, see what it does, how it works, and can help on making the technology safer versus it just being some proprietary technology that a large corporation owns, and this is me. Also, I want to mention, this is still an early research project, so it's not like this is like ready to go, robots are gonna start using this tomorrow. There's still time to figure out all these safety implications and we are hoping that people from the community will also help test the system in terms of safety and issues.

Yeah. And I guess just adding to that, yeah, there's a lot of reasons to believe as Viviane's alluding to, that it's going to take time before any system like this reaches a kind of significant level of capability. I think there's a lot of issues with these arguments about like an intelligence explosion happening overnight where, someone turns a machine on and suddenly the system has reprogrammed itself to, To some other level, a good example is, a very intelligent system can produce a lot of ideas about the world, but in order to actually gain new information, you actually need to test those ideas, and this is a problem that faces science today, that there's, hundreds or thousands of theories out there, but actually progress is really bottlenecked often by our ability to collect new information, and so even if a system has a degree of embodiment, it's not like it can just infer, mentally every possible kind of, prediction about the universe and therefore, turn itself into some superintelligent being. What that means in terms of kind of the, time that, it would take, for intelligence to, to reach anything near that is that it's probably going to be slow and, so We will benefit more from more people having eyes on this technology, contributing to this technology, so that we can make it safe, we can understand it well, make it interpretable. And that, as Viviane alluded to, everyone's benefiting from it. It's not just one large corporation that, that controls this. I'd like to just add a couple thoughts on this. It's a very complex question and Niels broke it down already. there's the, there's Niels and Viviane, there's the everyday sort of problems with people misusing a technology, then there's the sort of the existential risks of the technology and so on, which I don't believe exist. And if you're really interested in this topic, I have a couple of suggestions. One is I wrote extensively about this in my most recent book, A Thousand Brains. And, really, it's a topic I care greatly about, what is the future of humanity, what should we be striving to achieve as a species on this planet, how does this technology help us achieve goals that we might set ourselves as a species, and I think it's essential. And so it's a large topic, I'm not going to try to review it here, but if you want my musings on it, it's just mine, you can read the second and third section of my recent book. I also want to point out that, when I first mentioned that we were going to do this open source project, I did it at a little talk I gave at Stanford back in June. It was before we officially announced it. I just mentioned it. And I had several people came up to me talking about this issue. And they, and I, and they asked if they could contribute to the open source project along these lines, and not just from a technology point of view, but from a discussion point of view. And I said, that would be great. I think it would be great to have that kind of discussion. and so you don't have to be, I don't know if Will's got an accommodation for that yet or not. But I think we should be, have a place in our discourse forums, perhaps, to people discuss these topics and get into them. I know I'd be interested in participating in that as well.

Yeah, I think that's a great idea. We should definitely add that and start those conversations now. I think it would invite a lot more people to the conversation. Great idea. Okay. I'm going to wrap it up. I want to say thank you to Jeff and Niels and Viviane for the fantastic presentations and the time today. A big thank you to the audience. we had Over 300 people turned up for this presentation. We were blown away by the number of questions and the thorough reading that you've done beforehand to ask these really good questions. We hope to get to a lot more of them in a subsequent video. And with that, I will bid you a good day and have a good week. I'll leave you with a thousand brains QR code if you want to go and sign up for any of our information. Thank you guys. Thanks everyone for showing up. Thank you, everyone.