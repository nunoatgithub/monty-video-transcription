so yeah, really exciting today. We've got, two topics. first one, Rami's gonna be presenting some of the work, he's been doing, recently on a key stepping stone, getting us towards compositional objects. so I think that'll be really interesting. And then depending on how much time we have at the end, we were gonna talk about some of the changes we're thinking about making in the code, reflecting some changes we've had, on the conceptual side in terms of, model free, model based policies coming together and how that's orchestrated in sort of something like a subcortical structure. and that's something that kinda Scott will, Kinda take the lead on. but yeah. Rami, do you wanna, start? just, I throw in if we have time at the end, I have some thoughts about how we, start tackling sort of the broader picture of complex goal-oriented behaviors. okay. Nice. that sounds Yeah. Yeah. Relevant. It's just a, it's just an idea about how you're going about it.

cool. Yeah, I think we'll have a lot of time at the end. It's, my presentation's a little short. Famous was last words. Yeah. I feel like presentations are never short. It's, yeah. Open discussion. Okay. I'm, so this is hypothesis resampling and, I'm just gonna start with the motivation here. and the motivation for this is because we're starting to tackle, compositional object recognition, before we just had one object in one episode, and we're just trying to figure out what that object is. now we have, or we're trying to, we're starting to enable composition, object recognition, and, There are some changes that have, that we have to do in the code. and one of those changes is figuring out how to deal with multiple objects in the same episode. And, I'll explain what I mean by that, but, can I ask a question already, Rami? Sorry. Sure. You say recognition, I mean we have to learn composition objects and we have to in infer composition objects and predict composition objects, predict components. is this about all of those or is it, or, I'm not sure what composition object recognition is. That seems a broad term. this is mostly, about inference and not inferring, I'm inferring a, cup that with a, logo, it, it's differently than just a cup. Is that, the way to do that in this case? So a cup with a logo, the lower level, LM is going to be recognizing a logo and then the higher level is going to be recognizing the cup with the logo, at the same time. but this is more the motivation for what Rami's been working on. So basically what t's been working on is basically only the lower level. So you just have the lower level model and you can recognize the individual child objects and you can move the, your sensorimotor from one child object to another and switch. Your hypothesis. Okay, that's just, that's this object inference, That's right. That's this basic object, multiple object. Yeah. It's, basically improving object inference as a stepping stone to enable composition objects. So right now there's, no kind of results or anything with comal objects, but compositional objects would be impossible with, how, Monte currently is. Right? I just, I'm just, I, it's worth going into this a bit because when you say compass object recognition, you have two learning modules or two regions. One is recognizing a mug and one was recognizing the logo. And so both of those are just standard, what we've done already. And so it's really a matter of learning the composition and then if you're gonna say competition, object recognition, defining oh, this is the mug with the logo, which is different than the lu with, the mug with, something else on it. Okay. Yeah. That's what you mean. If you want to recognize the, cup, the, cup that has, child objects on it, the lower level, LM is going to have to switch between objects and That's Okay. Yeah.

yeah, this is just what I was saying. Even though we have co-located receptive fields, the, higher level, LM is, trying to recognize this one object, but the lower level LM is just moving between objects to be able to, give it different object IDs. and the, problem with that is, there are some, it's basically in the code base. We just have some, difficulties with some of the assumptions around how, Monty experiment is structured. and I'm just gonna go through these assumptions basically. just to give an overview, in Monty, we have an experiment, and this experiment is split into episodes. Every episode is a different object, and we have this, assumption that we have a single object per episode.

and, it's coded in such a way that, the, every, at the beginning of every episode, there is an explicit reset signal that resets everything, basically deletes all the hypothesis that we had, and it reit initializes a new hypothesis space. this is what looks like. So at the beginning of every episode, there is a, there's a very important step where we use the first sensory, observation to initialize a hypothesis space. And then we use that hypothesis space, for the rest of the episode. Then after the episode ends, we reset that hypothesis pace. And this actually has enabled us to do, to parallelize these episodes and do a lot of, very interesting things. But, yeah, every episode is very independent.

and yeah, like I said, there's very, there's too much reliance on the first observation, at the moment in Monty, because that's what we use to initialize the hypothesis base. And after that, the hypothesis base is fixed. We are just updating the evidence scores in here. So if the object has changed, or so if the, even if the object, yeah, so if we replace the object with something else, or if, the sensorimotor moves onto, another object or the object is swapped, we actually, that first observation is not valid anymore. and so this is the current problem, and like I said, with comal objects, we wanna be able to move between, objects in the same episode.

so the, first thing that we, started with is define a benchmark so that we can measure, out performance against that benchmark. And it, the benchmark is very simple. We just define an experiment where we have all these episodes, but we just remove the reset signal in between. Whatever first observation that we, come up with here that's going to be used for all of the, episodes. And we started looking at what kind of results that would, that would be, that would give us, so, what I'm showing here is the maximum evidence score for this is the best hypothesis that we have, for, just recognition. And, on the top here, I'm showing what, which episodes I'm, basically I'm giving to Monty. So the first one is, zoom in a little bit. First one is, the mustard is a strawberry and then, a banana. And then the, spam can, or the, potted meat can, as you can see, it does very well or reasonably well on the strawberry because this is the, hypo, the hypothesis based that we initialized was based on an observation that was on the strawberry and it accumulated enough evidence there to be able to confidently recognize it. but then as soon as we started giving it a different object using, and it's starting to recognize using, the invalid hypothesis, it cannot accumulate, much evidence on those. And it's just all of the hypothesis are just, decrementing in evidence.

This is a no, not, that's not good. Yeah, I was gonna say, this is what we were expecting to happen. it like, this is not a surprise, but this is just the benchmark showing that as we expected, Monty in its original version cannot handle this. And that's what Robbie solved. Okay. That's what, originally what we would have is just a reset signal here so that we can initialize a new right.

this is a different view of, what's happening. we're defining and, a different metric here called theoretical limit, which is basically just using the ground truth. We know exactly what the, how the object is oriented and everything. We just look at the best hypothesis that we have based on that pose error. And we just define what is the limit of Monty. Basically, if, Monty were to choose the best hypothesis, what would be the pose error? And you can see that as it switches, here the post error jumps. and that is because it basically is using its old hypothesis and they cannot really work well with the, new hypothesis with the new objects.

Did, does that metric make sense to everyone? 'cause I feel like it's, yeah, a bit of a, I didn't follow it. So it's if you imagine when you initialize, you have kinda a set of hypotheses. Those are, the nice thing about Monty is those are all informed by that observation. So If I feel a surface here, then, one of my hypotheses is the mug like this, but also a mug on its side and stuff like that. So that's the set of hypotheses that are initialized. and basically some of those, all, of those, even if they're good, like valid or not, they will all have a distance to the ground truth rotation. Like how close they are to the, real rotation of the bunk, which we can measure. And then we can look at across all the hypotheses, what is the hypothesis that is closest to the ground truth, even if it has no evidence for it. e even if Monty hasn't had a chance yet to explore, and so that hypothesis might not have much evidence for it. But if Monty was to pick it, it would be the best hypothesis of all the ones that it could pick. So that's why it's the theoretical limit, because it's, not guaranteed that Monty will select that hypothesis, but by existing, it's in the space of possible hypothesis that could accumulate evidence.

and so the, theoretical limit is like, what is the, lowest one of all. So at the start here, what you call it, like it, yeah, it'll be constant in these cases when we change the object, but so as we expect with the first object, we initialize our hypothesis based on that first observation. So the theoretical limit is good, but once we change to another object, the hypothesis space for that other object. Was initialized based on a totally different observation that had nothing to do with the actual existence of the object. So none of them, fit the, real existence of that object particularly well. So the theoretical, image kind of jumps up, around step 50. Yeah. Okay. So like even if Marty would do perfectly on the second object, it just has no chance of, getting the correct orientation because it's not in its hypothesis space.

I think I got the basic idea maybe.

Okay.

yeah, hypothesis resampling is just this idea of that, that we don't really need to rely too much on the first hypothesis. Instead at every step we're going to be resampling some new hypothesis based on that, the new observation, every observation that we get. And then, we also don't want the hypothesis based to explode in terms of its size. so we're also deleting some of the hypothesis that, have not accumulated evidence, in a while.

so, the first step is just basically, look at the new observation. So we assume that we already have an a hypothesis based that has been initialized. and then we just look at the new observation, and then we initialize a few more, hypotheses and add them to the hypothesis space. the just one little detail here is that we're prioritizing graph nodes that, that match with, the feature that we're seeing. because we're not going to sample the full hypothesis base, we're not at, every step, we're not really sampling all of the hypothesis that could be sampled, that could be, generated, but we're sampling some of them and we're prioritizing those based on the graph node features. so we're using those features to do that.

and then the next step of, this would be, removing the hypothesis, from the hypothesis space. And we do that.

the original thinking of doing that, or an naive way of doing that would be to just look at low hy low, hypothesis that have, low evidence scores and just delete them. But that really, is not gonna work very well because, you may have, once you swap, once you switch, from an object to another, you'll have some hypothesis that have very high evidence for the previous object. And those will continue to stay. We're not gonna be able to delete them. so the idea here is to remove based on the slope, of the, evidence accumulation rather than the actual evidence, the absolute evidence values. We'll be able to just track this slope and just delete the ones that we don't need. So over time, it's, like we're refining the hypothesis space over time. rather than keeping a fixed hypothesis space that is tied to the first object that we had.

just one note here. We, also had an idea of resampling based on like something called Offspring hypothesis that is based on, the evidence scores. So we would, at every step, we would look at, what, which hypothesis had the highest evidence scores, and then we would resample offspring hypothesis based on those hypothesis. It something like we, we would sample from the distribution of those high evidence, hypothesis, something like very close to it, which is, like we, we take this one which is like high, high evidence, and we just sample something like close to it. And that is just the tar, the, goal of this is just to, fine tune or like closely refine, the hypothesis space so that it goes towards, better and better theoretical limit. And it's informed by, or like one of the inspirations is the particle filter approach that's often used in like slam techniques and the advantages that you can start out with a pretty sparse sampling of the possible hypothesis space. But then once you have A certain location orientation becomes more and more likely that you can sample similar locations in orientations to re refine your estimate and make a very exact prediction about location and orientation, even though you didn't like densely sample all possible locations in orientations in the beginning. Yeah. and one note about this is that it wouldn't really help us very much with the switching from one object to another. It just assumes that we, we just have the object in the same, we're just trying to refine the hypothesis based to go to towards that same object. but because it's based on the absolute evidence scores, it's, just not, it's, relying on what it thinks that object is to sample new hypothesis. when once we switch from one object to another, that, that high evidence scores doesn't really mean anything. So it doesn't really help us get, switch from one object to another. So it's not really geared towards compositional objects, but it was, it is nice to have because it can make us recognize objects faster, which is something we might need with behavior. So we, might come back to it.

yeah, so just to, this is the whole idea is just instead of having, instead of free sampling a large hypothesis space at the beginning, based on that sensory information, we continue to refine the hypothesis space over time. at every step we're doing this.

And, these are the results with resampling. again, this is the same figure. We're switching between objects and as we are switching, this is the maximum evidence course, and we can see that we're able to switch from one object to another. and Monty's able to recognize these objects as we switch. So the, evidence for, at least for the fir first episode, we're expecting it to do good. so it's, accumulating evidence and everything. And then, this is where the, improvement is happening. Basically, the second, object, it's accumulating, it has, it now has a, hypothesis that it can accumulate evidence for, the second object and then for the third object as well. So as we switch, we're able to switch, Monte's recognition to the correct object.

Now, for the theoretical limit, it also makes, sense because. the theoretical limit, even though it has increased, at as soon as we change the object from one to another, but then it continues to decrease as we resample more and more ev scores. And then even as we see resample more and more hypothesis. and then, even Monty is able to do good on these hypotheses because, we now have hypothesis that we can accumulate evidence on. then we're doing good for all of the, objects, basically.

Yeah, I think this is the last slide, that's, this is a comparison on, the benchmarks that we defined at the beginning where we just, we do not remove, we, we do not do the manual reset of Monte between episodes and, the a i we ran the benchmarks, so we have no resampling and full resampling. And then also one thing that we're, investigating now is, using, a smaller hypothesis space, which would make multi, faster as you can see here. So the runtime, experiment when we're using a 70 of the, of the hypothesis space, multi can run faster. 'cause we're always refining that hypothesis space and we're using a smaller hypothesis space so Monte can run faster and it does not affect the performance much. as you can see here, it's like the same performance. I thought I'm confused about this. I thought that I would've ex oh, I see there's a third bar that's really low there. Yeah, I was gonna say yeah, I was just say gonna be the same. It doesn't make sense. Yeah. Okay. So I think you also, you wouldn't have been at our kind of normal Monday, meeting when Rami first presented these results, but in very understatement of the century, I think he was like, this is a nice improvement. But yeah, it's, we basically go from 0 accuracy on this benchmark to, close to a hundred percent. So it's, a, I didn't a huge jump. So yeah, this figure actually shows two really key improvements. One is being able to solve, switching between objects without any supervisory signal. And then the second is that now we can actually have a much smaller hypothesis space size before, right? If it wasn't in the hypothesis space and the first step, it wasn't possible to be recognized at any time during the experiment. But now as we are resampling all the time, it, we don't have to have everything in our hypothesis space on the first step.

it seems like we can make the hypothesis space quite a bit smaller without, much impact on the accuracy.

I think my question, so I realize the effects are small here, but, I was just wondering if you could speculate as to why for the distant agent, the 70 size went down a little bit and for the surface agent went up a little bit in terms of percent, correct. Yeah, I, so these are, like, I had, I don't have any idea. I basically, there is also, one thing here with the runtime surface agent is running, faster, which I also am not sure why. but, so even without three sampling, the surface agent is much faster. And this might already be also in the, original, benchmarks. So there, there are, yeah, I think, that's just for the, like the policy, it just, converges faster 'cause it explores the surface more efficiently. Yeah. could be, which means, yeah, I, guess that there's a proportional decrease in the number of steps. That's what I'm pretty sure we've seen before.

Yeah. And, we didn't get the, the, improvement like, so here we have a large improvement in distant agent, but we didn't get that much improvement in the surface agent, in terms of front time. so yeah, these are things that I'm still looking at, for wrapping up this project.

I'm just, I'm, gonna go over the, like an interactive visualization, just take a few minutes, but may, may make some things clear.

This is a new visualization that Rami put together. That's really cool. thank you.

okay, this is my other computer, so I, you'll be saying my side, of my face.

so, basically this is what, this is the simulator, this is the ground truth of, what Monty sees. and so we have a, just like we saw with the, previous, with these figures, we have the object that Monty is seeing in the simulator, on top here. So it, it is going to see a strawberry and then a banana, and then a mugga and so on. And then this is the evidence of, which object Monte thinks it's seeing over here. but then in the simulator view, that's what's actually happening in the simulator. And, and the most likely hypothesis, this is the, it's basically plotting the maximum evidence over here. So as we move between these. We are not seeing your cursor, I think, I'm not sure if you can turn on a laser pointer or something, or just be a bit more descriptive in what you're pointing at. Yeah.

yeah, like maybe just highlight, I guess in the simulator, Monty is the kind of red circle with the, and then the direction it's looking is that line. And then I'm not sure if you saw Jeff, but there was like a green dot on the MLH, side on the, hypothesized object. And that's basically where Monty thinks it is, on it. I missed that, but we're just gonna get to, some comments I have in a moment here about attention. Is the green dot over there? Yeah, you can see it just peeking around, about whatever it is. Okay. That was not visible to me. Okay.

Yeah. Sorry. I'm not able to, to have a, the, cursor shown. Not sure. Oh, it's all right. I think if you just say on the bottom X, then it'll be Right. Yeah, I wasn't aware. the bottom plot here, this is the one that we've seen before and this is the Monte's evidence. And at the top, of that chart, we also see which objects Monte is talking about. Resampling and changing. I was thinking I was gonna be looking at a cup. So, this red blob thing is, that's a strange looking cup. It looks more like a strawberry, I think, but I'm trying to, I'm trying to fit it, make a cup out of it, but god damn it is a strawberry. You didn't say that. Some strong priors on your behalf off the space, right? Because we were talking about cups the whole time, and I'm trying to make that damn thing a cup. And and at some point I go, wait a second, there's a green thing on top. It's a strawberry because it's small. My display and you zoomed in on it too. Okay. That's funny. I wasn't doing very well on my inference. We do have a cup in Scott's new dataset, so we can use cups in the future. It's just a cup. The mental cup doesn't exist in the YCE dataset. That's all. That's all right. It's fine. It's just funny. We, have a, mug, over here. Okay. It doesn't matter. Pick the strawberry's. Great. Okay. So I was just showing what, we're seeing is, this is the agent, and this is what the agent's looking at this being the left, the, pin looking thing, the sim in the simulator view, the red, sphere is the, agent position. And the black line is basically the, what it's looking at. So this is the patches. Yep. And, on the right view, we see the, in the most likely hypothesis, this is what Monty thinks the object is looking at what that object is, and also, where the agent is or where the sensorimotor is on the object. And that will be shown as a green, as a green sphere. so something like this basically thinks this is the, the sensorimotor, is that green dot represent the actual size of the sensorimotor patch? No, it's, that's 'cause it's way too small, I would think. It's, so these experiments have noise in them, so even sometimes you'll see on the right side, that the, patch location is not exactly on the surface. And the sphere is I'm just saying, I'm just trying to understand how much of the object is being sampled by this learning module or this sensorimotor module. Oh, So you can look at the whole, experiment basically like from the beginning. on the simulator object, on the simulator view on the top left, you can see how much it, how many steps it looks at. like I think you just, I think Jeff's asking about the receptive field size. Maybe if I, if, I share my screen, I can show you what it has been for other objects in the past, and then that seems, because I, this would be similar. The green dot, it seems way too small of a sensorimotor path. Yeah, it's, larger than that. It's not, it's maybe, I don't know, like a hard to describe, but, yeah, Ramy, if you stop sharing for a second. thanks. Because, yeah, this is actually something we sh we show in the paper. for example, with the mug, you can see, you'll see like a kind of the rim and just underneath and just, I assume that, I assume the red.in that, in that figure represents the size of the patch. I've always assumed that, is that not true?

no. Go back to the image. The image you just showed is so as in the, patch is something more this, so it's, still pretty small, right? But it's, but it's bigger than what it's, we just, it's a Yeah. Yeah. I mean because, alright, it's an important variable, the size of that patch. obviously the smaller it gets, the harder it is to learn and infer and. More details you get, but it makes, think of other things much more difficult, Yeah. I think that'll be an, interesting, I was just say, that'll be an interesting thing to, experiment a bit more with now that we're bringing compositionally and hierarchy. 'cause then we can have, like a higher level learning module have a larger receptive field. Was that, for a surface station or distant agent? Rami on your visualization Destin agent. Distant. Yeah. Although what the sensorimotor module sends to the learning module is just a, pretty basic summary statistic. So it just sends the center location of the, patch and the color at the center of the patch. And then the main thing that's influenced by the size of the patch is what kind of curvature is being extracted From locations in the ca in the patch. But it's still not saying anything about like, how things are arranged within the patch or anything like that. No, but I think it would, anything within that patch will get, averaged out in terms of its curvature and Yeah. And so you don't want to be modeling every single dimple of the strawberry. Yeah. I just mean it's not like you get a whole lot more information into the learning module if the patch gets No, but it, I think it's helpful. It's helpful for visualization to just imagine. I understand that, but it's also, it's confusing to me to look at and see that teeny little thing. It's okay, we're not really sampling that point. We're sampling an area around that of some area. And, then that's, otherwise you, would learn the curvature of every ins and out of every dimple if it was just, Yeah. You don't want that. So anyway, it's, that would be helpful. I, remember it's the first time I ever thought about this, 'cause in the papers and previously there's always been some sort of rectangle, like Niels just showed, which said, oh yeah, there's the patch. And I can correlate that to the image that, this is what the patch is seeing, but here it doesn't look like that here. It just there's a point and you're using the size of the green dot of some sort of error, But, that's a very different sort of representation than we had in the paper. Yeah, that's an interesting, I think it, it would be useful to add, even just the RGB view of what it's looking at or, like a window here, like a square. it would be personally, you do what you need to do, but from my point of view, it would've been helpful that green dot shows the area of the patch or, even the simulator. 'cause I guess the, patch only exists in reality on the left.

But yeah. okay. What, I don't, that doesn't make sense. And just to clarify, I guess the, the, green dot, I don't think it's size varies. I think it was just, I thought it was, I think it was clipping into the mesh. Sure.

because there's noise in the locations it receives. It's not going to, anyway, if you take a hypothesis and Yeah. Anyways, it just occurred to me right away. It's oh, we are moving the sensorimotor of the object and how much area of the object you're sensing on the sensorimotor is an important variable and it's, missing here. So it was confusing. Yeah, no, that's a good point. I think it would be nice to visualize and we could even have a, yeah, we did that in the paper figures and therefore I wasn't confused by it. Yeah. Yeah. Fair. Another quick question about that. You, said it's the distant agent, and so on the left where we're seeing a dot and then a line like presumably the agent's quite a bit further away than where that.is for a distant agent. So I'm wondering is that just some fixed distance from where the OB observation was taken? I'm not sure if it's, much further away than that.

that's a fair point. Is it possible surface really close your eye? Yeah. oh, strawberry Scott great. Found a great video of these guys like studying t-shirts. I think we, we should share it, but it's, the fever dream that Monty experiences.

I, this distance, does it really matter at this point? It's, not a practical distance for her. Like a vision system necessarily, but from a, theory and implementation point, does it matter at all at this point? It didn't seem like it would. Yeah. Not, really. I do think the Asian is probably a bit further away because the not zoomed in view, like the viewfinder gets a full image of the strawberry, which seems difficult at that distance. but yeah, since it is like a 10 x zoom in view for the patch that we send to the learning module, then, so I, yeah, I just, I used the agent position, which didn't seem to be displaced by that much. maybe it's something in, I, I can dig deeper and maybe the camera just has a wide feel of view or something. it could be, but yeah. Ram, I have a basic question for you. So you're talk, what we're doing about here is the lower learning module, deciding, oh, I, I'm no longer on the mug and now I'm on the, logo or something like that. And, what about the upper learning module? It's amping the same spaces, the same points.

why wouldn't it do the same thing?

I think this is, yeah, the, having this, the stacked, LMS on, on top of each other and both of them looking at this, at, the same observations. that's, I think that's, one of the things that we're still, working on in terms of how to enable. the, okay, so it's an open, question. Yeah. That's actually something we wanted to talk more about, maybe at one of the next research meetings, because I think we're all a bit unclear on that right now. Besides giving the higher one, a larger lower resolution receptive. I have a lot of thoughts about this right now, so we don't have to wait. Yeah. Maybe, Rami can wa walk through this visualization first, and then we can Right, I've been waiting, so I'll wait some. Okay. it, it is just, as we, move the, you can see that the line, the vertical line in the, bottom figure, it moves as I change the, which step we're on. And as I move, and we're still looking at the strawberry, it has basically the most likely, as long as the most, most likely hypothesis is the same orientation and object as the simulator, then Monty has a, good idea of what it's looking at. and then as soon as we switch the object to a banana, Monty still takes a some time to, delete the wrong hypothesis and refine the space. But, after some time it, it thinks maybe it's just different object, the Lego piece and then maybe a mug or whatever. And then, it thinks it's a banana, but it's in a different, different orientation and it starts. Looking at different parts of the banana until it figures out the correct orientation.

oh, same thing happens at the end with the mug. let me just fix the orientation a little bit and this is what it starts with. And then just sometime it gets the mug, but then it thinks maybe it's a inverted, mug. you can see that by looking at the, opposite side. So it thinks the sensorimotor is on the opposite side, over here, which would make sense if the mug was inverted.

and after it fixes that, we can see that it's now looking at the rim over here and it thinks yeah, on the rim.

and yeah, after sometime we get the same, things with all the objects. so yeah, it's able, and these figures, Monte is not getting any resets between the objects. It's able to switch between them, and quickly decay the incorrect hypothesis. yeah.

there is this some interesting, observation here with the, mustard, but, where it basically, it thinks that it's 180 degrees rotated, based on the observation that it sees because it's looking at it from the side. It's hasn't looked as much at the texture, like the, barcode and all the text. It's just, it's looking at it from the side. So it thinks that, it basically, thinks that it's 180 degrees because it's symmetrical along that axis. I, imagine as well, if you looked at the hypothesis, the relative differences, the hypothesis for the symmetric rotation would be quite similar. So even though this is technically the most likely hypothesis, they're probably quite close and it would be very quick to, to change to it once it looks at any, of the texture. if it sees red over here, it would be very quickly to switch to the other one where it has different.

so you can see that. Yeah. it thinks it's a different orientation, but it's very close to it. yeah.

this is it. Very nice visualizations. Thanks Jeff. Yeah, it's beautiful to see also how kind of the rotation hypothesis quickly refines itself and how even if it's wrong initially, it's wrong in like meaningful ways, a long axis of symmetry, because it hasn't seen the disambiguating evidence yet. So it's really nice. There were some, places where, it figures, out that it is the right orientation once it looks at the, discrimin features like a handle or the rim of the cup. And right after like just a, few steps after that, it just, just itself, sometimes it only takes one step, but it's also interesting to see that, multi figures out the correct, goal state where it wants to be to, to ba basically disambiguate this, the, orientation. So that's also, it was interesting to see that it picks the right places to look at, like the handle of the cup or the ribbon. Yeah. Nice. Yeah, no, overall this was all really nice work. Both, yeah. With the hypotheses sampling. Awesome. That could, solve that new benchmark. And then Yeah, I definitely agree the, that visualizations can be really useful. Like even just the kind of the thing that Scott spotted, I think that's an interesting thing about, being really close.

I just think it's gonna be super useful for debugging. 'cause that might be, I think actually partly from the jump to goal state where we have this desired distance and I think I was just using whatever the default parameter we had for the distant agent as well for that. But that might actually be closer than we want to be. we might wanna to, Jess point to smooth out the, the sensory observations we're getting. So it, might be something to revisit and I'm sure there'll be other examples like that. Yeah, I'll definitely look into it just to make sure. Yeah. It's also occurring to me that when you change the viewing distance, you are gonna change the size of that, how big that receptive field falls on that object as we were discussing. And if the stored model kind of assumes a certain kind of distance, like what the patch, how the patch size fall over, then all of a sudden you're much closer and you're seeing a bigger part of that object than the amount of curvature, like how you estimate the curvature could be pretty different because you're averaging over a larger area. Yeah. I feel like that's the beauty of, Monty is that it shouldn't be too sensitive to that. I agree that yeah, it might be unexpected, but in, in general it's, I think it's gonna be very different from that same distributional shift again, if the deep learning system saw that where yeah, it's used to like this zoomed in view or vice versa. but yeah, I'd say it's surprising like that, hasn't made any difference, any noticeable difference at all thus far. Like after a goal state is achieved and you're really close, it's like we don't see any noticeable like drop or, in performance or whatever after that fact. so yeah, seems very robust too. I think it would be better without it or, yeah, a bit further back, but yeah. Rami. Yeah. I'm curious, have you had a chance to look if this improves performance on the normal noisy experiments? Because I think we, expected that maybe as a side effect because it removes the reliance on the first observation that might also be noisy. Yeah. I'm still looking into that. We, I have some, I ran some experiments on the default benchmarks, and I have some ways and biases things to show, but, there is, there are some things with the, pose error where I've seen the pose error slight, like increase on some of these experiments. and I'm debugging it just to make sure, because it doesn't make sense to be, to be increasing. so that's, one area I'm looking into.

I have a bunch of thoughts. One, we're ready.

Yeah. I think could, you bring up the mustard could container again to share your screen? Yeah. All right. Just stop right there. That's good. Oops. It's going to adjust the, all right. That's good. All right. so I have two general thoughts.

I believe it feels very strongly to me that when we do. When we look at an object like this, so here's a mustard container. It has a bunch of stuff on it that there actually is, an event that occurs when we, it's an intentional event that occurs when we wanna focus on some part of this object. So it's not like the a V one and V two are just scanning over this thing, and V one is just by magically picking out LO logos and words. And B two is magically picking out the whole object. It's for starters, both V one and V two would be looking at the entire ob inferring the entire object, and then you would attend to some part of it. And when you attend to that part of it, only V one is attending to that part of it. And V two is still looking at the whole thing. and I'll go into that a little bit more detail. Also, we have to keep in mind that in the ultimate in these systems, we're gonna have many columns looking at these objects. So if I think about V one and V two, the entire object is gonna be somehow covered in V one, of many columns. And the entire object might be covered in V two with many columns, regardless of whether the columns are representing the same size of receptive field or not. So we have to make sure that, so it's like when I would look at this mustard thing, I might say, oh, that's a mustard container. I actually don't read the words. I don't see the words until I attend to them if I just flash it in front of me. You imagine I just flashed this image in front of you. You say, oh, it's a master container. And then I said, what would the words say? You said, I have no idea if I, on the other hand, put a little, and I've done this experiment, I put a little rectangle around the word frenches. So now imagine all is the rectangle. And I said, I'm gonna flash an image in front of you. I want you to attend to what's in the rectangle. And I'll flash that image. You'll see the word frenches. You'll know, oh yeah, it says frenches. The same exact input coming into the retina. I'm, flashing the, I'm flashing this image, but I'm telling you in advance to attend to some part of it or not. And the retina gets the exact same impression. I can have the same fixation point. I don't even have to be fixated in the rectangle for the, frenches. I can fix that anywhere. And you'll see the word frenches in one case, and you won't have any idea what it says in the other case. So this tells you that there's, an intentional mechanism going on to, to break out a part of this image so that you infer what that part is. so when I think about, looking at a compositional object like the, cup with the logo, I imagine at first you actually, it. I know it feels a little weird. You actually don't see the logo. You see it's there in the same way I could, when I flash this image of the mustard container, that there's text, but you don't know what it is. You don't, you haven't really inferred the specific details of the child objects. It's there's a general morphology or a model of this thing, which includes colored areas and so on, but the actual details of what's there you have to attend to.

so what I imagine the first time, like I'm, holding, a coffee cup in front of me right now, and I said, oh, look at, this is my cup. And then I said, oh, there's some text over here. I actually never noticed what that text says. I have to attend to it. And it says, oh, it's says Copco. I could have seen it a hundred times. My eyes could have actually fixated on those spots a hundred times. I've never seen the word. It's only when I attend to it do I see the word. So I think there's an intentional mechanism that's required to do this, and that attention mechanism is in some sense a reset, if you will. It says we are now gonna attend to some subset of the input space. we're expecting to see something different than I was seeing before, which is the cop now expecting something else. I, this, by the way, the work you did, I think is really useful in general, but I think it's not sufficient to solve this problem.

and so it, it just basically, I think what, when I look at an object with a composition object, I will first look at the whole thing and then I'll start attending to details, oh, what's over here, what's over here, what's over here? And each time I tender the details, I infer a child object completely.

and then I, and then somehow, the V two is not. Not attending to that subset, V two is still representing the entire object, but only V one would be attending to that subset. it seems to be something like that's necessary Yeah. To do this. Yeah. I feel like that's, yeah. No, that's really interesting. I think makes a lot of sense. And also it also, we have many columns in V one and some of 'em will be on the attended subset and some of 'em won't. So we have to tell which ones we wanna pay attention to. So it's yeah, that's what I was gonna ask was Yeah. If, yeah, if that's the, main mechanism most is like windowing the area of voting, right? And it seems initially that this is a model free process. It's I don't know what there, I don't know what the, there's a word that says frenches. I don't know if it says yellow or 40 or anything like that. I don't know that, there, somehow it says, oh, there's some stuff there. let's attend to it and see what it is.

and so it's not like these things are invisible to me without, attention. It's just the, it's somehow in the overall model of the, mustard container without the details of what the child object is. and then, and again, since I don't know what the details are, it can't be a model-based attentional mechanism, at least not initially when I'm learning it, because I don't know what's there. it could be, it's more oh, there's a border of red. Maybe I'll attend to that or there's border of blue, or I'll attend to that, or there's some letters I'll attend to that. So I think this is what we're doing when we're looking. This is happening con constantly. Every time we, move our eyes, we're attending the different subsets of the world. And, and then we're assigning those subsets to, as a child, to a larger, portion of the world.

and even V two might be on the retina. There might be the, there are multiple objects, like you might have the coffee cup and the mustard container. And V two is like attending to the subset, which is the mustard container. And V one is then attending to the subset, which is the red.

now that I attend even closer, I see it's a flag. I didn't notice that before. See, I've been looking at it over and over again. I didn't know it was a flag. now I tend to it more carefully and I was, attending to the word, I didn't see the flag, but now I tend to the red spot. I see it's a flag, but I don't really see the word.

so I think this, we have to have some kind of mechanism like this. And of course when we do this, we, are, inferring often we just do this inference of the flag or the word, using many columns. It doesn't require movement or very little movement. So I can just attend to quickly attend to the red area. Oh, it says frenches or attend to the words. There's 40. I don't have to like scan over it. I would if I had a single column. So we have to, we'll have to be thinking about multiple columns voting, to, to infer the word frenches or infer the word 40 or the word yellow or something like that, or the flag.

and so those are the two things I think we have to add to what you've done. I think we're gonna have to add this sort of attentional window, which initially is gonna be, somehow, model free. and then we have to make sure that we're, able to, coincident with that, be able to have multiple columns voting on that. So a real system is very, quick. I just recognize the logo on one inch, one flash. I know that logo, bam, because multiple columns voted on it. I know the word frenches 'cause I've seen that before. Multiple columns vote on it could be that, we reset based on confidence or prediction error. I think this is something that I've been discussing with Niels and Viviane, prediction error of what? So, if you move to a, different place or a 10 to a different place and you have a prediction of something there, then this is, and it's not there, then you start resetting or you sample very quickly, like sample more aggressively. May maybe. I, certainly, that seems to recur in some situations, obviously.

I'll show you something funny. I'll be right back. Two seconds.

Yeah. You say, in some sense your slow tracker is, a prediction. I, thought you guys might enjoyed this. Can you see this? You can see my mic here. Maybe I myself see up here if, you hold it higher. Okay, here we go. So I took this to the, my talk last week and I got home. I noticed something immediately jumped right out at me.

I dunno if you can see that it's chipped. Whoa. So it's a nice chip. It's really a nice chip. So it's almost like a, I would like to have a cup like this because now I can talk you, it didn't break the whole cup. This made this perfect little chip. Yeah, it's quite satisfied. Normally it like it's off the surface and it's ugly. That's more like that. That's weird. It looks almost like a cut. It didn't obvious. You see the inside, you can see it. I actually had the piece, I could glue it in, but I don't want to. so anyway, my point is that's a prediction error, right? I think and my, because if I had this cup all the time, and this is the cup I used every day, I'd take that outta my bag and I wouldn't think anything about it. We'd just say, yeah, that's the cup. You get used to it, right? Your model gets modified. But the first time I see it, it's like a prediction error. This is what on intelligence the book was all about. so I think there is an issue of, but I'm not sure. in some sense in this prediction error says there's like a new feature on this mug that I didn't see before somehow. But I think it's the morphology model. that's, messed up. I don't know. Yeah, I think the prediction error will be important in general, but for the kinda potential mechanism, it's more about gating, which it inputs we're actually giving to the learning module to infer the object, Aiding to really give to the red parts of the flag where to only, give the, French letter lettering. In fact, and we have to, be thinking about multiple columns, getting that gating right. which we're basically saying there's an area of the, I'm not sure if it's the area of the retina or an area of the space in the world, but we're giving it an area that says we need to attend to this area and ignore everything else. and what do you infer in that area? Yeah. It seems like that kind of ties into what, we were thinking about part of, Scott's work, which is this is like explicitly motor attention, so only moving your eyes in a certain area, but basically having a model free policy that makes sure you stay on a child object until you recognize it. So basically only circadian within the French.

it might be a little bit more, maybe I don't recognize it right away. And maybe it's a com, a composition object itself, like the flag plus the word, that I hadn't seen before. You have to really, you'll spend some time in that little area exploring building up models of that, and then you can pop out again.

yeah, and it is nice to have the same sort of model free mask or, whatever would, could inform voting as much as it can inform the policy. so that would, we can still, get it to work with more of a sequential thing, but it, would work even better with voting right now if, obviously it seems like once I've learned, if I really studied this French's mustard container, then maybe, I'm not sure if the attentional mass would be model based or not. I don't know. But in initially it can't be. So Yeah, I think we, we were thinking that, yeah, like you say, initially it's, it would be model free, but then like model base would probably be really useful for things like robustness to clutter and things like that, that I'm pretty sure I can see like a chair behind that other, that table or whatever. And so you have a hypothesis of a chair and Right. that kind of gives you some sense of where it is in space somehow. In fact, when I think more about that, it feels like attentional mechanisms are, you're attending to an area in space, not an area on the retina. it feels like you're, you're attending to a depth in space and an area that's exposed, and I don't know how that happens. I don't know how the brain does that, but that seems like what it's doing. We could start with attention to an area of the retina or area of the receptive, the array of, of central modules. but somehow it seems like it's more than that in, in brains. Somehow brains are able to say, I'm attending to the, I'm attending to that table, which is about 10 feet away, and I have something right in front of my eyes that is partially blocking the table. I don't see it at all. it's just not, I'm not attending to it. It's pay no attention to the things that are not 10 feet away. I'm saying it's like weird. Yeah. I think the things you're bringing up, like the, you, everything you brought up in the five, last five minutes is a lot of intense debate in the RFD that I'm working on about all of these issues. you basically identified like a big section of the kinds of issues that we've been talking about, like including like whether or not this is, we're attending in retinal space versus like world space, So yeah, we've been, yeah, it ties into this, other RFC really nicely. if, great. I, hope I'm not being redundant then. no. I, think it's, I think the nice thing is it's fitting with, I think what we were biased towards doing. But, but I think you're also bringing up stuff that we haven't Yeah, you're definitely bringing up stuff we haven't talked about, like, voting and, using the same attention mechanism for, voting, which I think would be quite easy to extend and would be, a nice, conceptually Go ahead. Yeah, just that we haven't really conceptualized it as attention so far. We've been talking about it as policies, which I feel like thinking of it as attention is a bit more general in the sense that we can also apply it at the high level modules that might not be influencing the policy as much. And we can, think, use it much more broadly. so yeah. Hey, it just occurred to me, given our distance sensorimotor, which has, it has distance built into it, right? we could do attending to volumes and space, right? Yeah. That's what we, yeah. So that's what we were thinking of doing. And it would also, it would make it easier. 'cause one thing we were thinking about, and this is maybe starting to segue into the next discussion, is like how you combine, let's say what your model three attention or sensory input is telling you to, look at with your model based one. and that's much easier actually if they are both working in space rather than in retinal coordinates, right? 'cause then your eye is telling you, oh, whoa, there was like a loud noise over there. Your model based thing is saying, oh, someone's doing machinery or whatever, and those will match in the space that they're talking about without actually working in 3D space. If you're trying to do in retinal coordinates or some binaural kind of auditory coordinates or something like that, it's much harder to get them to work together. that's great. in, biology with eyes, it's a, it's much trickier to tend to space. You have to use various sort of bi binocular clues and, as we've talked about, and also the, parallax issues and things like that. But given the kind of census we have, could be much easier to do it. So I think, so let's, combine this together. I think this is, Romy is great work you did. I think we have to, and I think it's gonna be useful all around and just this general inference, it's like we have to decide whether we're on a new object or not. but I think to solve the composition object problem, we have to assume that two regions are attending to different things. One is attending to a larger object and one's tending to a smaller object. but they're both, you can have to assume that both multi learning module arrays and they're both getting some sense the same input. We don't have to assume that B two is getting larger receptor fields in V one. That's not an assumption and, a requirement. And, therefore, V one has to attend to some. Volume in space or some point in space that V two is not attending to. And therefore, and then with the multi columns, we can infer that sub child object very quickly.

and in the ultimate system with monies running around the world doing things, they're gonna be just like us. They're gonna be going, oh, that's there, this area that's there, building up these models as they go. Yeah, I think that's a nice way, like the nice thing about thinking of it as attention is we can apply a different mask to the lower level learning module, then we apply to the higher level module. So if we want the lower one to and further logo, we can gate that we only send observations within a certain area of space. And then the higher level one can, get observations in a larger area of space. So that's, yeah, I, that's really, that's the first time I realized this. Now that I think both V two and B one both have to have their own potential spaces. they both have to be attending. So in some sense maybe that's a general principle that's going on all the time.

that, of course if you have, if I give you a blank screen and I say fixate on some point and I show you an image. I, you would know where to attend. I don't know what though. That would, what you would do, and maybe you'd tend to everything, but, very quickly, you're saying, oh, this V two is attending to the mug, and V one is attending to the volume of the, logo. And then, V two is attending the logo and, V one is attending to the letter, that kind of thing. a question about that could, the higher region be initiating the attention for the lower region? so well that would be a model based attentional mechanism, right? And that, and we talked about that. I, guess sure it could, but I don't think it can initially. Yeah, that would be after we've learned the compositional object, almost like in the backward connections maybe, or just generally if we have learned a composition object, we know where the child object should exist. So that would be easy information to use to, one, predict the child object, but also know which area to attend to. So if we're looking for some child object somewhere, we, that's where we, that's where the high region can initiate that attention. But we're also talking about model free attention, which cannot be initiated from high region.

Yeah. Yeah. I think that, I think that makes sense.

I'm gonna stop sharing. Yeah. Yeah. Thanks. yeah, so this feels like a nice transition into un unless, yeah. You had other stuff you wanted to talk about, Jeff. into the next part because yeah, basically we've been talking about bringing in more of these, model free policies to tell us how to, move to, to attend to things or to stay in an area of interest. and that's raised these questions of like, how do we coordinate that with these, kind of model free attention slash kind of policy signals with model based ones. and kinda leading to this, so the interaction between model free and model based action policy, is that what you're saying? Yeah. And this is kind leading to this idea of having something like, whether it's the basal ganglia or a mixture of subcortical structures, a sort of like goal state or action selector that's receiving these and then telling the motor system ultimately okay, this is what we should, do. is this due for inference or do for doing like a task goal or a task? And so, right now I guess it would be focused on inference. Yeah. So it's, not gonna be a complex like hierarchical, task. it'd more be like, you are, typing your laptop and, generally you're relying on your, model, base kind of signals for that. But then, something falls over and, you then attend to that. or you are. walking while talking some to someone. So there's a bit of a bottleneck of, how much can actually be done and potentially there's a system there that's saying, okay, these, are the things we're gonna prioritize, and actually allow to happen. It's really about picking where to attend to next. Yeah. At, this point. Yeah. It's, so you gonna present some work on this or are we just, discussing this? Yeah, so we, don't have any work on this. This is basically, something we were planning on working on, but we just thought it'd be useful to get your feedback before we, we get too far with it and, if something Scott's been brainstorming on it, it clearly there is, we've talked about this multiple times that, model free attentional mechanisms are really hard to resist. it's I've talked about the, the classroom or tell all the kids to look at the whiteboard and say, don't take your eyes off the whiteboard. And then someone opens the door on the side of the room and everybody heads turned. it's a, it's un unstoppable in some sense. it's really hard. So there's this, clearly there's, in some situations, unexpected behaviors. Now, maybe it's just in the periphery. Maybe it's a, an evolutionary argument that says, animals are about to attack you from the side. So when you see movement on the side of your periphery, you have to attend to it. I don't know if it's a general mechanism or not, but there are certain cases where the model free attention is dominant and, hard to overcome. But yeah, it feels kinda like it's a scale and it can vary because you can also, except for the most extreme kind of model free signals, it feels like you can also kind of block it off. if we, say, for example, a model free signal is, prominent colors on the screen to catch your eye, or, and things like that, that's something that if you are just seeing a new scene and you're curious about it, you're going to let those signals take over and follow them. But equally, if you're trying to do a task oriented thing, you're gonna block all that off and you're not going to let that drive you. It's hard, it, reminds me, I go to some, some websites that are just filled with ads and videos and little things popping up all over the place. And, of course in, in Firefox, I don't know about other browsers. There's a little, tab you can click. It basically gets rid of all that stuff. And all is the text of the web website. it's like I have real trouble not paying attention or not being annoyed or not. It's like a struggle to fight, to not pay attention, all that crap that's going on in the periphery. And it's such a relief when it all goes away. And I can just read the text. I'm just showing you as an example of it can be difficult in situations where, that stuff is extraneous, you don't have to pay attention to it. It's still. It is a challenge to just tune it out. and that's how advertising works, yeah, I think maybe it makes sense if, Scott goes through the basic proposal and oh, so you have a proposal. Yeah. There are like two mechanisms we're thinking about right now. One is saliency based and one is a bit more like defining the region of interest to stay within, like what's all. Sure. Okay. That'd be Great, great. I don't know, Scott, if you have some slides or just the pictures from the R ffc. Can I go fill my coffee cup again? Yeah. Anyone else wanna take a yeah, should we take a, quick break? I'll take a couple. Thanks. Alright. A couple minutes. Okay. So good enough. Present our, this RFC about model free and model-based policies. And so I guess just a very quick, have you heard the term RFC Jeff? it's request for commitment. How about that? Yeah, comment. Yeah, comment. It's just like a kind of a proposal to I wanna do something to the, to the code base and, we get to somebody vets it. Yeah. I thought you guys had it. See you. Oh, we do. Yeah. So here's the RSC and you can see like how many comments there are. Like, everybody weighs in basically, and you make adjustments and, If you guys want me to look at this, let me know. But generally I'm not looking at these things. I can drop in like a human readable version of this into the No, no, it's, you don't have to. I, if you want me to be a commenter on these things, fine. Sure. if you don't mind it doing it this way, it's easier for me. Okay. That's whatever works.

so the, it's a, it is a little non-obvious how this is related to, computational objects. 'cause this proposal has some new architecture that seems generic or useful in many situations. But the top level idea is that at first we were thinking something like a superior colus. So you're able to take a wild, wider corer field of view and maybe use that to identify, like they say the label area on a, child object. And then somehow based on these sort of model free signals you can choose to attend to the label area and stay within it. And, that, that's the idea, like the very course grant idea behind this RFC and how, and this is just what we were just talking about this, right? Pretty much Exactly. Yeah. Yeah. So this RSD has a, proposes a couple new items and, to fold into a couple new classes to fold into the code base to help support this.

So in the, I guess I, I could have a version here of Monty without the proposed, pieces, but normally we have observations of a cup or whatever and they go into sensorimotor module, which then goes straight into a learning module. And then the learning module can emit goal states, it doesn't have this goal state selector. And so the model based, goal, I don't know what a goal state, I don't know what a goal state selector is, if I need to know what that is, if the, I'll introduce it in a second. I just realizing maybe it would've been helpful here to have a version without the components, data components, but it's alright. Normally there's no goal state selector, this learning module just goes straight to the motor system, which can either execute for model-based things when they're, requested model-based actions or otherwise just take control and do model free actions. We have, correct me I'm wrong. Today we have no model-based action policies. Is that right? That's not true. The hypothesis testing policy. Oh, I'm sorry. Totally shouldn't That's alright. And but Right. Exactly. And, right now when that comes through, that always takes over. So it's almost the opposite of, I guess what you were describing before of like certain model free signals always will cause you to, a model, based, policy for inference. is gonna be better than a model fee policy for inference. I guess the earlier were talking about a model free policy for like totally look at something else, stop what you're doing. Yeah. And that's, why I feel like I, I think it's not gonna be black and white. I think we'll want situations, that we generally rely on model based signals, but there are going to be situations where a model free thing will break that through the sort of attentional, window and say look, this is really important. Okay. Yeah, and as Scott said that what he's gonna propose is like a, pretty general solution. So we started with a specific problem we wanna solve, but I think we settled on a really nice general solution that will solve a whole bunch of problems. For example, also right now, the model free policies are happening within the data loader and the motor system. So there's like a bunch of sensorimotor processing happening in places that shouldn't really be doing the sensory processing. and that's one of the things that Tristan has also been working on. I guess it's gonna be outside of the model. So where do you put is the question, right? Yeah. So that's what Scott's gonna, So you're suggesting it shouldn't be in the sensorimotor module? It should be someplace else? Oh, no, I'm suggesting it shouldn't be in the mortar system or in the data loader. It should be in the sensorimotor module. Oh, okay. I don't have a good sense what the data loader is, but fine. we don't want sensorimotor processing to be happening in the, motor system. Okay.

Yeah.

this proposal adds two new, items here is a salience map, sensorimotor module name subject to change. But that's what we have a call so far. And then something to help manage the, combined. so this thing generates model free states, the sense the salience map, it, is our stand in for superior colus in the sense that it just takes, in it, we expect it to make salience maps, perhaps some very primitive types of segmentation or at least segmentation within visual fields to maybe help you stay within an, a label area, some constrained area. this, of course, there's a lot of work on salience maps and vision, and I don't, I'm not up on it, but it seemed like it was it wasn't clear cut, oh yeah, these are actually happening all the time. Or, where it says, oh, maybe, it's a poor explanation of what's going on. I, don't, I'm just pointing out there's a lot of research on this Yeah. As to whether, these things really exist or not, but it's okay idea. Yeah. And I, think one thing just while we're on this topic, was interesting from a brief look at the literature. There were people arguing that at least at some point in kind of the CUI and stuff like that, these saliency are in kind of a 3D. more egocentric rather than t atopic coordinate space again, because of this problem of even if it's all model free, how do you combine signals from audition and, vision in terms of telling you where to attend and, it seems like the superior, rather superior and inferior click. I can do that. and so it would make sense that they're doing that in some shared space. So like you were saying earlier, maybe it doesn't matter so much for us because we can get 3D very early on, but it's just nice that might not be too crazy from a biological perspective either. No, that's not at all. Okay. Okay. And maybe just to clarify that, this is just a different type of sensorimotor module, which I think is one of the things I really like about this proposal that originally we thought about introducing a whole new component to Monty, which is like a subcortical sensorimotor processing area, but, then realized that this is something that we have sensorimotor modules for. And basically the only change is that sensorimotor modules can emit goal states as well. And those will be locations to attend to that will pick based on model three criteria. Okay. So these are, you can call 'em a sensory module, but they're not, they don't feed into a learning module, right? They're outside of the No, but they receive raw, sensory input. so they, they have that, kind of thing in common. Then, their output, like a, the earlier sensorimotor modules is also in the CMP format. 'cause the goal state is a CMP signal. It's, a, a location and pose in space. It's a location pose, potentially other information about, it's a location in space, but it's definitely a location. it can't have a pose and it can't have an ID because it's, model free.

yeah. it brings up some interesting thing. We, were thinking maybe there could be something a model free id, which is really just like a temporary, there's this stuff, which is this ID, and there's this stuff, which is this Id, I have no idea what these objects are. I'm not gonna learn about them or model them, but I'm just telling you that I think this is different from that. and in that as soon as you were use the word id, I think it has to be a model, right? there could be a subcortical model, right? There are, it's been shown that blue even, humans have this sort of thing that subcortical, that sort of detects shapes of snakes, just, it's not a really, it's a, very crude sort of long thing that wiggles a certain way and that's built into your genes, right?

so, you can have some sort of models down there, but, but they've have to be very crude. It can't be anything specific. Yeah, the general assumption, it even could be learned. the way I, the reason I like this, I think if we think about Monty as a basically learning modules and central modules, and there's this common architecture, this whole separate path you're proposing here is independent in some ways. it could work any way you want it to work, right? It could be, signal from the internet saying, go, look in the hallway down here. I don't know. It's just a separate system that says you need to pay attention to something over here and how it actually works should be independent of how learning modules and, sensorimotor modules work. Is that correct? Seems Yeah. I, think that's fair. I think it was just, there was a, it was a, kind of conscious decision or a kind of realization to have the output of this be goal stakes as well, because be location and space's we felt like that if it's a location, if it's in a location in the egocentric space, I'm all into that. I think it, that's has to be that. What else could it be? Yeah. But that's gotta be what it is now. That's what you mean by goal state. Fine. Yeah. I'm, I'm still struggling with that word. So you have, we'll have to get into it a bit more exactly what goal States are, and it is still an evolving thing. I, I think, unlike the other ones, the other CMP signals, we've sometimes said it can be a bit less defined as in Some of the features or kind of properties are optional, but I think location always has to be there. And so in that sense, it fits, I think with, this, right? So it, and it's a separate sort of, it's, we have to figure out the mechanism for this, what we call the sale team map in this diagram, but it's really anything that we want to use a model free, attentional based detector, and, and then how it interacts with learning modules or, how it interacts with, the model based action policies. Yeah, but I, wouldn't say it's independent of, sensorimotor modules.

at least how, I couldn't think, oh, why I like it is because it's basically, it is a sensorimotor module. So like we still have sensorimotor modules, learning modules and water system, and the sensorimotor modules are responsible for turning, like raw sensorimotor information into the CMP, and do the sensorimotor processing. Only now we are giving the sensorimotor module the capability to send messages to the mortar system directly instead of to the learning module and then the loan. I would, I think there should be separate sensorimotor modules. If you wanna call it a sensorimotor module, it should be separate. For example, I may be looking at something and I hear a sound. I'll turn my head to the direction where that sound occurred in my behind me or off to the side. So it has, it doesn't have to do anything at all with what the sensorimotor module is actually sensing at this moment in time. That's the one that's being fed into the, learning module. Yeah. But the, your sensorimotor that's detecting sounds can at any time also send the sounds to learning modules, but then if there's maybe not, maybe not. I'm, arguing the opposite. I'm saying you could think of it independent of that and saying I might be an animal that actually doesn't, understand sounds at all, but a really good vision system. But the sounds could still be very useful for me for attentional mechanisms. Yeah. there's no requirement that you send it to a learning module, but, I don't see, a sensorimotor module wouldn't be able to send its messages to a learning module. It could be, but, I guess the trick is you don't want people to think that sensorimotor modules that send signals, you could combine these two things in a single module, but conceptually they're not combined. Conceptually they're different. So don't want to fool people to think that a sensorimotor module has to do both. 'cause it may not, I don't know if a touch sensorimotor surface sensorimotor really has an equivalent to this.

I think it would like, feeling something hot or I, feel like a, sensorimotor module that is disconnected from Cortex would be like the enteric nervous system in, in the, gi like track. let's, go back to vision. A Agile have a vision system with a single, learning module and single sensorimotor module. So it's just like looking through the straw. My attentional mechanism is always gonna be bigger than that. It's gonna take the entire retina and, it's gonna be looking for, movements or something like that in everywhere in the receptive fields. So it's like that is a sort of independent sensorimotor ability than a single central module. And that's the same thing that's going on with touch. my, yeah, I could in touch, I could have an unexpected temperature, but, generally when I, the, if I have a single touch sensorimotor and it's moving over an object, the idea that if I feel a bite on my arm and I recoil from it, that's a different system altogether. It might, it may be combined with a single sensorimotor patch, a patch of skin, but it's almost certainly different actual sensors in the skin. Yeah, so that's why we drew this, the saliency map sm to get this large receptive field as input. But also I don't see a problem why this large receptive field sm couldn't send something to a learning module or why a small receptive field send a module, couldn't send something to the motor system like for small adjusted kind of micro cates or something like that. I guess I, I know I seems like you want to combine these. I think I would just caution against that. I don't know, if I feel a sharp pain in my, like a, bite on my arm, I'm not sure that is a useful signal to, to send into a learning module. it's, I would, wanna attend to it visually. I would also recoil from that. My body would automatically recoil and move away from that. Yeah. Those also more reflexes, right? but these are things, that's another category of things, right? So I'm just saying there's, I could take all that stuff and put into this bucket of, outside the cortex, model free action policies and they can be all kinds of things. It could be reflex reactions. I could have new sensory modalities that only work for these, attentional mechanisms. as I said, I could hear something even if I don't understand sounds. it's, I like the idea of separating 'em, and. Treating 'em as differently, although whatever it is, still gonna send in a location signal into the, into the motor system.

I think like reflex reactions are different, but maybe more, mostly because they wouldn't go through the goal stage selected, they wouldn't really compete with any other signals because they are like safety guards, reflexes that you have to perform no matter what else is going on. But then, yeah. And I think those don't use a location or send a location. Those actually Yeah, directly tie into a muscle contraction. But, if I feel a bite of my arm, I will automatically move away from it. But I'll also, but that's like a more primitive reflex. But I'll also automatically turn my eyes to look at that low, that location, and I'll turn my head and my eyes and I'll, visually attend to that spot. so I, they'll do those both simultaneously. for what it's worth, I'm not sure how important the distinction is at this point in that I feel from a code perspective, it makes sense to call it a sensorimotor module and just communication perspective. 'cause it, it takes in raw sensory input and it's outputting a CMP signal. then I, just don't want, I think it's, okay. I think that's great. I, just think when people are trying to learn about Monty, they have to understand it's not like other sensorimotor modules. It's different. it can sometimes play a similar role, but. It's really, it's, serving a different purpose in some sense. And, and it doesn't have to be high tied to that modality. It doesn't have to, it could have different, all anything about it can be different than a typical sensorimotor module. So I think even if we, even if it shares some commonalities with the sensorimotor module, or a lot of commonalities, we can definitely introduce it, to users and things like that in such a way that makes it seem pretty distinct. Even if we later on say, oh, by the way, under the hood, a lot of this is basically the same as a sensorimotor module. I, like that approach. you could just call it something like an attentional sensorimotor module or, pick some term that just differentiates it. this is a different class of sensorimotor module and if it works on the same CMP principles and that's great. but it really is, it's different, in many ways that we just talked about. Yeah, yeah, I don't know. In my head it's still not like categorically different. It seems like a continuous shift between like, when is the receptive field large enough for it to be an intentional one versus too small to be one? Like it's, it seems like a continuous, I don't see like it's in a human. Every part of our skin is tuned to this. there isn't a single part of our skin that can't. Under the certain, certain type of sensation, the different sensors. These are sensors that might be pain, heat, sharpness. those sensors are a completely different sensory system that covers the entire body. And, they're active all the time and envision the entire visual field, including the periphery, is active all the time. You can't attend away from it for these purposes. So I, in that sense, they, seem different to me. They're, the sort of very large global things that are occurring.

look, I think Scott proposed what's wrong with just calling it something else. Call it an attentional sensorimotor module. And, and then we can see if it makes sense to feed them into a learning module, act actually.

so this might actually, support the, point you're making. but we were talking about having this sensorimotor module produce potentially lots of goal states goal. Let me, can we just say what is a goal state? Is that a point in space to attend to? What is, It's a point in space to attend to. Yeah. That move My sensorimotor patch here. Is that what a goal state is? Okay. Whatever it takes it, it can be more than that, at least as we've conceptualized in the past. but that's what it is as a minimum. I think it should do that. It should generate that. that's the idea that I feel a bite on my arm and my, visual system will immediately attend to that, location in 3D body space or 3D world space. So yes, I, it is, it has to generate that.

yeah, so we don't necessarily want the regular sensorimotor modules to be emitting, doing all the work of emitting lots of goal states out here if they're not gonna be used, if they're just going through to a sensorimotor module. So I suppose just having some way to maintain a separation in the sense that, there's a lot of work here that we don't need to be doing all the time for these guys that this one would need to be doing. Yeah. how I was thinking of it is that sensorimotor modules basically the same as learning modules can have a goal state generator. And if Sensorimotor module isn't wired up to the, to connect to the motor system or isn't getting a large receptor field, or we don't want it to generate goal states, it doesn't have a goal state generator, or it just has a very primitive goal state generator, that, only emits goal states if there's like a very, like a sharp pain or something. but then the say what, personality? That's a, that's also a, goal state. Isn't it? So that one would have a pretty complex, when would the salience model, salient sensorimotor not generate a goal state? No. Yeah, that one would for sure have a goal state generator, and that one would be rather complex. And, processing the whole kind of visual filter that gets us input to determine the, goal states. So like our current sensorimotor modules that feed into the learning modules wouldn't necessarily need to have a goal state generator, but then the one that's like responsible for the attention would for sure need one, but might not need as much other sensory processing machinery that we have in the, regular sensorimotor modules.

1 1, 1 thing. So go ahead Jeff. No, interesting. one thing about the code organization, like I know that in a goal state selector like these, goal states, these modelly goal states coming from this sensorimotor module with goal state generator, they're privileged and they're being pulled apart. And, the goal state selector needs, what I've seen in RFC is it is able to distinguish between them. And so that would be a case for calling it something else, like a attention state or something else. If we're gonna, priori say these are distinct from what learning modules send, because the goal state selector can tell them apart. It could be, so a, another proposal could be, it's an attentional state. Maybe it has two modes. One where it's like advisory and the other one is, the, once you get to the reflexes, maybe there's attentional states that have to do that you can't ignore. But this could be, advisory attentional state or something like that. But then to, to another argument of this might be something different is that maybe it's like maybe looking at from the goal state selector, having to tell these apart might also be a relevant framing to think about. I don't see why I couldn't tell him apart. basically you could, say that, the, saliency attention sensorimotor, is, may not be active all the time. no, if it is like the pain sensorimotor would be, but like in a visual saliency, feature detector may, if it's happening in ulu, for example, it doesn't have to always be producing a saliency detect. it could, under only under certain conditions it could do that. But when it does, and then you would attend to it. I'm just saying you could always give preference to this, to the saliency, sensorimotor module, but the sensorimotor module doesn't always have to be outputting something, yeah, I think generally. Goal state selector wouldn't necessarily need to know where the goal states came from. It can just ba select based on the confidence or the, relevance that the sender gives to that message. I feel like there would be, 'cause as in, for example, it might treat or it would probably treat model free and model-based signals, differently. but I, don't think that's unreasonable from a, it doesn't have to, but, but I feel like, yeah, it doesn't have to, but I don't think it would be unreasonable from a kind of neuroscience point of view, because obviously some of these inputs are coming from hardwired connections, from subcortical structures. Some of these inputs are coming from cortex. So those are very different things coming across, very different axons. And so it's not hard for those to be treated differently, but it doesn't need to be told that it can just learn that certain signals are more reliable than others. I don't know. there's two ways you could do this, right? you could pass in some sort of confidence. And so based on the confidence, you could, the goal state selected would figure out what to do, or you could allow the saliency generator to make that decision, right? and say, I don't think this is important enough to tell you about, so I'm not gonna tell you about it. or This is really important, you should do it, or something like that. Those are two ways of doing it. I personally like the latter. I think you're, you like the former, Viviane, but, you could work both ways. I feel like it just feels to me like cleaner to just burden the, saliency detector. And so Monte in its current form, model-based is, really clean and, then, and it generates goal states. and then we build this other little system on the side. It's very embodiment dependent and sensorimotor dependent and, almost task dependent, system to say, okay, how much do we care about pain? How much do we care about unusual things? How much do we care about, whatever.

I don't know. There's two ways you could do it and burden the, saliency. Sensorimotor or, provide confidence is I, don't know if one is inherently better. Yeah, I think we need a combination of both. Like the saliency one shouldn't, output, thousands of those and then, and have to go say selector do all the work. But then again, there should also be some kind of. Confidence. if there is a very salient signal, sharp pain or something, it, would be sent with like kind of an alarm signal attached to it, to the ghost. Yeah. it still, it feels like, imagine, the saliency in the classic view, which was oh, there's some, high frequency components in this visual image over here. Okay, so go, look at that. Something's interesting over there. That's when I think about classic saliency and then there is a separate thing like, oh, there's pain on your arm over here. Go look at that. Those are really separate systems, and they're not really one, it's not one thing. It's like those are separate saliency kind of signals that come in and have different priorities. but I guess it, it would be nice just for simplicity to try and put those just on a spectrum within Yeah, I agree. Maybe you need different, as you were saying earlier, like task specific things that are like detecting looming is like a very primitive reflex that, small mammals have and stuff like that, like a looming shadow. And so I guess that's, a hardwired thing that's separate from detecting yeah, as you say, high frequency textures, but, from the point of view of the goal state selector, if it's all just signals with like various levels of, kind of confidence.

and then, it's just some can really scream out. Look, you need to act on me now. okay. I think you could, right? So you could do both. You could have a confidence signal coming in, but it has to be able to say, maybe we're gonna build some robot construction workers from Mars, and they don't care about shadows looming, they don't care. There's no predators, just ignore that. so hopefully on Mars, no. Yeah, that's right. Hopefully aliens.

whereas, a biological animal that's important. I'm, I, like the idea of saying, okay, we have this system called Monty. It models the world. And now under different situations, different embodiments, different tasks, different sensorimotor types of things, you can, you have to provide these saliency signals. And some of them would be graded, like maybe the high frequency, visual, components and, like where to attend visually. and some of 'em would be like binary, like danger, pain, something like that. and as long as the system can handle that variation, clearly in a, way that doesn't really impact the models in Monty, then I think we're, okay. it's leave this up to designer of the system to come up with any kind of saliency thing they care about.

it doesn't really impact the way learning modules and sensorimotor modules work, the general sensorimotor module work.

Yes, I think it's both. Both options are available to us in the future. We can just try the really simple confidence based thing and kind of work in a more separate stream if that seems, I think there's still just a lot of open questions in the RFC. you could do all with the confidence based, just let me, let's some central modules be able to just force their way and say, I get a hundred percent you have to do this. Yeah. and other ones would be like, Hey, right over here, you might, look at this. Maybe you don't, maybe you don't, Yeah. I mean I was, I think I was originally thinking about it as separate and, for just a first pass since I'll be, the one implementing it, I thought, oh, maybe we'll just do the confidence thing first. Because it also has the nice property that like, it just makes things simple for the goal state selector. It means that this way let's, if you fine, confidence is good as long as you allow someone to have a binary choice.

there's no, like this must be done. yeah. So it's almost like confidence is scaled from zero to one, but you can also send in, and if you send in, if you send infinity, then that one will always get acted on. it is just, you just need to, you need to have that ability. And by the way, even something like vision, you might have multiple vision. sensors that do com that, that do ency. you could have ones that are detecting motion and periphery. You could have other ones that are detecting, spatial frequencies, nearby where you're looking, within the objects boundaries that you're looking at. yeah, those are, really separate type of ideas are not really one thing. So now that we're talk, I, never thought about this before, so this is all new to me. You guys have been thinking about this, but now that I'm thinking about it, it seems to me like, oh, there's all these different systems that come to play here, and they're very much, application embodiment, sensorimotor specific, task specific almost. So we have to have a lot of flexibility for someone to come along, just like in embodiment in sensors. We have a lot of choices. What you're gonna use, you have a lot of choices about, as a system designer, what you care about in terms of this, these saliency sensors. Yeah.

Are you happy with that? Viviane? you seem unhappy about it. No, I'm happy with that. Okay. Yeah. Does that make sense everybody? Yeah. And the, goal state selector, maybe we don't really need to talk about it too much now, and, it's something I, think we can leave this to the future, but yeah, I've just been thinking about how much, hardwired intelligence we also want to put here in, in a model free sense still. But in terms of how it decides to yeah, the simplest thing is basically it just gets all these confidence signals and it just picks the, maximum one. But if it's something like, oh, I want to pay attention to this part of the world, or, I don't know. I probably, just need to think more about it before I think I can propose anything. But, yeah, it just feels like there, there might be some greater complexity there. and that, that might map onto something like the basal ganglia. I think also disagree. Were talking earlier, the goal state s lecture maybe should really be an area of interest lecture, a volume of interest select or something like that. It's really not a point, like we talked about attending, you're attending to some volumetric space in some sense.

And, that's that we might wanna start thinking about the goal state select that way. it would still work for a single learning module, and as long as the, sensorimotor module, lands inside of that space and the sensing in that volume, it'll work. but then if there's multiple learning modules in parallel, they, we wanna care about which ones are sensing in that volume. All of 'em have to, there's a bunch of in that. So just maybe expanding the idea of goal state selector to a. goal. I don't know, volume selector. Yeah. I think that was what we tried to show here with some of the circles also being bigger, like the learning watch, you could say, attend to this broader area in space and then based on kind of saliency within that area in the feature space, it would then pick a certain spot, specific spot. But yeah, I don't think we, but that would just generalize that too. It's always picking a volume of space. Yeah. And and at any point in time it could change its volume to be bigger or smaller depending on what on its needs.

Yeah, This, yeah, the motor system eventually has to move to a specific spot. But, yeah, that's an interesting question. Like in how, if I'm, let's say I'm moving a bunch of sensors like on the retina or a bunch of sensors on my skin, and I, two fingers, I want 'em all moving to same, some volume of space.

I, don't know what does that signal look like? I, don't know, it's oh, move all of these to this area. Each one's getting a different one, different, location in that space. That's interesting. Yeah. I think, part of the way we were thinking about was a combination of things. One is you can, yeah, have a set of goal states so they can in some sense tile space, but then also you can have as well as confidence. You can have a blurriness or almost like a Gaussian kind of like around a location and. That can be smaller or larger. So you can say I really wanna attend to this exact location in space where you can say, yeah, look approximately over here. And that also applies to like, if you need to place an object in an environment, sometimes you care, about a location very closely. Sometimes it just needs to approximately be satisfied. I don't know about that. the more I think about it, the more I feel like it's always a volume and your volume can get so small that it looks like it's a point, but it's always a volume. Yeah, okay. It is. I guess it's never a, mathematical point, but Yeah. Yeah. I wanna put, at least, I wanna put this cup on top of the trit on my, desk here. that's not a spec, that is an area of the tribute matching an area of the cup. It's, I, it's not like a point on the cup and a point on the trit. It's yeah. And even if you're threading a needle, which is probably about as precise as humans can do there, there's a, an eye, it's not like a single point, It reminds me, I saw this exhibit in this really freaky museum in LA with, and they had all these really curio type of things, and someone had made these sculptures, 3D sculptures of individuals that all fit within the eye of needles. And so there was the pope and there was Elvis, these tiny little things, and they're so microscopic. You need a, microscope to see them. So even the space of a needle can have a lot of, have a lot of detail. Okay. It called the Museum of Jurassic Technology. It's definitely worth seeing. Okay. just, we, were talking about the, goal state and kind of what it could do. That reminded me. Yeah. One thing we were talking about is maybe giving it definitely like a, short term trace. a bit like, a short term memory, but just of where it's attended to in the past so that it can bias itself not to go back there. I think the literature and vision shows that too. I think there's some sort of in, inhibition of previous Yeah. Inhibition of return. Yeah. I dunno what it's called, you already looked over here. Now look, someplace else, don't look there again. So again, that's, that will all be in the algorithm of the saliency detector I would think. not in the model. it's a, model free parameter. It's a model free, decision.

yeah. I love data that way. It sounds like you can just, we just burdened the, saliency central module of all this stuff that we don't wanna think about right now. Okay. So, that one, you reckon the traces and the saliency detector, 'cause it doesn't actually know what we've attended to the goal states selector knows what we've attended to, so it feels like it'd be easier to do it in the goal state selector.

Because all, the salesy ones are gonna be sending lots of different signals, but the goal stake Electric could say okay, I, went here, let's try somewhere new.

I don't know. I could make the argument either way. it feels like something that, that's independent, it's model free. It's really model free. Definitely. and the goal state selector is model free in any decisions it makes. Okay. then it could be, it can be informed by goal states that are model based coming from learning module, but it itself, it's model free. But you want the, salience detector to say, don't keep sending me the same thing. because I, know. I already saw that. So that's interesting. Yeah. That would, that could also maybe be like a recurrent, inhibition or something. Like it could say I don't know. yeah. Doesn't the sensorimotor module know what it look, where it looked at? That's how it can get information. Neil is arguing that the sensorimotor module, the real sensorimotor module knows what it looks looked at. But, the, but the saliency sensorimotor, I think it, yeah, I think that's a fair point, Tristan. Yeah. That, that it could get that from that. yeah. Yeah. Somebody has to know it. So I don't really care as long as it's not in the model system. Yeah. I think Tristan makes a good point. it should know it. just based on the sensory input it gets, it should actually know it better than the goal state selector because maybe the motor system failed in executing the goal state.

True. Yeah, that's a good point. I guess it, yeah, I guess the only, no, it should be fine because, yeah, 'cause the sensorimotor module would also have a sense of 3D space. I was just thinking if the sensorimotor module, 'cause yeah, it's gonna be outputting in three think the sensorimotor module, it basically, it needs to map like after it moves, obviously the, it's direct input is gonna change. It's, so it needs to somehow solve that correspondence between what it's suggested to attending to before and what it's seen now. But yeah, as long as that's like in 3D space, which the output at least is, then you could do that. Yeah, it's pretty complicated. if you look at someone's psycho cotting looking at her face, it might go back and forth between the eyes and multiple times over, again. Or eyes, nose, mouth, eyes, nose, mouth, eyes, nose, mouth. So it's not oh, I looked at, don't look again. Yeah, it's more comp, it's more complicated than that. Yeah.

so I was planning, on encoding like the don't return here, not so much as. Not sending goal states for areas that you've been to, but more just like reducing that confidence value. But just so my bias was towards sending a good amount of goal states, locations to look at and the confidence values can just essentially be weighted at some point or another by how recently you visited. But it would be up to the goal state selector and the way I conceived it to perform that weight though it could be done in salience, map sm but that is a, the decision to reweight goal states and confidence and things like that is something that could be like a component of the system that can be plug and modular, I guess the way to go ahead of this and, maybe this is a burden on Tristan or somebody I don't know, but, the way to go ahead with this is just give ourselves lots of flexibility here. 'cause it's clearly we haven't thought through all the issues and and we wanna be able to rapidly in the future, try different attentional mechanisms and so on. Yeah. And, and people will need, may even need to do those experimentation and final implementations. So let's not make sure what whatever we code, we don't code anything that's, that restricts us in the future of moving these things around and trying different strategies on them. For sure. Yeah, that's the best sense I could suggest right now. It just seems like we're just making this up right now. We don't really know enough, but Right. But, I guess another point or consideration here is we don't know what users are gonna want to build for, like whatever they make for a salience map sm or something. So I suppose from a sort of extensibility side, you're right. And that's the same with the sensorimotor module, right? We don't know what sensors are gonna use. Yeah, exactly. try so try not to hard code in too many decisions can, I think that's the beauty of the money architecture, it has these learning modules which are really generic. They, you're gonna be the same in theory, close to the same no matter what. And then you have these sensorimotor modules, which are, have this non-generic interface to the world and a generic interface to the, learning module. So The same thing would be, we'd want the same with these, saliency sensors that they have some common interface to goal state selector, or the attentional volume selector, whatever it is. And then people can fuss around with 'em all they want and change the, confidence and change the how they work and, yeah. That's all why I, like thinking of the sales map as is just another sensorimotor module that generally sensorimotor modules, you can attach a goal state generator to it. You can connect them to the motor system or to the learning module or both. You have all this flexibility. You don't have to do any of it, but, basically code wise, we allow sensorimotor modules to directly send messages to the goal State generator. But if I, if the, if, but if the, if thes of the sensorimotor, I don't see any point where it ever would send a signal directly to the learning module. And you don't have to, but maybe someone has a use case, like maybe, okay, but let's be careful. Maybe you have a small receptive field, sensorimotor module like we have today that sends message to learning module. But maybe you still wanna use that to perform some micros Cades to adjust to exactly the specific feature in that small patch. I think that's fine. Just in the documentation and the description, people could get really confused. And so I think that's saying that you're gonna have thes you think sending directly to the, to a learning module is a, corner case we haven't actually figured out yet. And so I would just don't talk about it right away, just because it's hard. These are really hard concepts and so I think that'll confuse people. but if mechanistically fine, And I have no problems with that.

Cool. I think, yeah, this was, really helpful. 'cause I, I think overall it's encouraging. It sounds yeah, you're on board with kind of what we had been thinking about. And I think it's just because it is a, bit of a new, shift in terms of how we structure it. We just thought it would be. Your thoughts? Yeah, we're gonna have to go through a very similar type of exercise when we talk about these model-based behaviors, right? we've touched on this before. There can be very specific elements of it that are not generic to learning modules and they have to do with embodiments and specific features of embodiments. And, so we're gonna go through a similar sort of exercise in terms of motor degeneration. so again, cortex is Cortex and all this stuff below it. We'll have to pick and choose the stuff we wanna do.

that's fun discussion. I'm glad, it's, I'm glad we're thinking about these things. Yeah. Yeah. Yeah. Thanks for sharing that, Scott. Yeah. Nice. I guess that's good timing unless there's anything else, for have I, I can present if you want. Oh, remember, I thought I had some thoughts. Yeah. Okay. Yeah, Sorry, I, no, I've forgotten. You don't have to, but I know. that would be nice. alright. Yeah, so I, just, I need to share my screen.

I guess that's, good enough here. And then I'm gonna go just, for in case anyone has meetings and stuff. How, long do you think, oh, I thought these meetings, I thought our meetings went beyond 10 o'clock now. They, can, but more as in if, if people who are maybe not on the research team want to stick around, I guess they just have a sense for how much longer this talking, the basic ideas can presented in just a few minutes. how much time you wanna talk about it. A separate question. Okay. cool. Okay. Can you see my screen?

You see, my, yes or no? Yes. Yes. Okay. So it's just a tech a word document. It's labeled goal, learning behavior. So I start, I want to think about, start thinking about the overall problem of, having a system, do your laundry, make coffee, build buildings, whatever it has to do, assemble IKEA furniture. how would I even think about this problem? How do I even begin been putting myself in a frame of mind to think about the problem? So these are just some thoughts along those, directions. Okay. So I'm calling that goal oriented behavior. That's what I mean by goal oriented behavior. yeah, we want this thing to really work and do something in the world. And then I say, how do, even, how do I even, what's the language of this problem? How do I, so anyway, that's what I started thinking about. So it starts with the models, right? Everything starts with the models and what they represent. Because the goals are, we, if we wanna specify a goal to a system, it has to be an expressed in the language of the models, because that's the only thing the models understand. So the, goals have to be in the language of the representations in each learning module. because, that's the language we can, talk to the system in. So I say goals and tasks must be expressed in the language of models. In other words, goals have to relate to representations found in a column. That's just a general statement. It's I can't ask it to do something that the learning module has no language for. The solutions are similarly restricted. The solutions are restricted to the language of the models. if the model doesn't represent something, we can't ask the solution to come up with some other way of doing it. It's, this doesn't mean that there are, there, there still will be model free motor policies and so on. But when we're talking about these goal-oriented behaviors, we're talking about things that are based on the model. So that's, this is a very simple statement, but it helps me think about it, to think okay, what, things can I express to a column, to tell? I want you to do something. Then I said, in this paragraph, I say, one way to tackle this problem, oops. Is to, is to define a base set of goal-oriented tasks that aren't very interesting in their own right. But these will be combined to solve real world problems in some sense. It's what are all the things that column can do? And then how do I turn those into a behavior, right? It's so it's the inverse of what columns learn and columns understand. It's inverse in the sense that you'll see in a moment, I that I want to take, okay? we understand how a column learn sequences, how can a column generate sequences, that kind of thing. So these are some of the, items that I've come up with. The first one here, you can see, if you see my cursor, we know how to infer the pose of an object to a sensorimotor. So a, something I might ask a column to do is move an object to have a desired pose to the sensorimotor, right? it's somehow, I don't know how to do this yet, but this is something reasonable to ask a column to do. and a column has the knowledge to do this. And so I can say, how would it generate behaviors to, to do this task? Similarly, I could say an inference task is in for a pose of an object to the body. we're not really, that this is something I know brains do. And so the goal task, it would be move an object to have a desired pose to a body which is similar to the sensorimotor, but not exactly the same. another one might be in infer the pose of a child object to a parent. and now the goal, the inverse of that would be to move an object, to have a desired pose to a parent. reorient this to, to, fix the, orientation of the silverware on the play setting type of thing. we also know how we're working on an inference task of infer the behavior of an object so we can recognize the behavior, learn and recognize it. Now, the goal task would be to cause a desired behavior to occur on an object. So I flip around saying, I've observed this, now I wanna make it happen. another one would be infer high order sequences, like, melodies or object behaviors. Could be high order sequences. And the goal task would be to create that desired sequence, the high order sequence, make the melody or make the thing, swim or run, that kind of thing. another might be infer, the inference task would be to infer an object id. And, inverse of that would be find an object with a desired id. instead of saying, what is this? Id saying I, here's an id go find it. where is it in the world? so this is a, this is not a comprehensive list. It's not really, I said, here, I made this as quickly, I wrote this last night. So it's a, in an order of 20 minutes. So it's not a lot of thought went into this. but I felt like this is a way for me to at least start thinking about the problem. we should be able to do all these things as these basic.

elements of behavior, where yeah. and then we can string 'em together. Okay. I can stop talking. I think that matches well with what we, I think we discussed this as like a brainstorming two years or one year ago or something. And we talked about, object, goal oriented behavior. And I think that's when we came up with the term goal state, which, we used, because basically it's the same, it's the same format as the internal state of the column. So the internal state would be like, or the, or what the column outputs to the next column. So that would be like detected object ID detected object post detected behavior detected sequence. And the goal inverse of that would be desired object id, desired object post. So basically if you send a goal state to a column, you tell it all right, I want you to do something so that you're, whatever you're detecting matches this goal state. I know these ideas, we've had these ideas around for a while, but they weren't crystal in my head before. that to me like, it's oh yeah, sure. I could, on apical dendrites, I can specify, what my desired location would be, or I can specify my desired id. What I have personally, what I haven't done. So if it's, if this is repetitive, then it is, that's, me. that's how I work. I didn't really said I, I could tackle these one at a time, like, asking how would layer five A cells do this? I have to move here. Every one of these things require movement in the world. Every, one of the things requires some physical movement. it's not intentional mechanisms here. and, and that connection, I've never done, I've never really asked how would, the column generate a signal in this layer five cells, which is a movement vector to achieve these individual tasks. Yeah. I, think that's an interesting, I, yeah. If I'm understanding, I think that part is new that like the kind of, the suggestion that there's a potentially finite list of sort of goal like primitives that Yeah. Uses the language of goal states as we've described before and, yeah. Requires on, you can only operate within a model that's known, yeah. That, we've discussed before, but the suggestion that yeah, maybe there's this finite list that gives us the expressiveness. We need to do pretty much any, anything. I started thinking about some simple tasks. I was thinking like, what if I had a bolt in a nut and I wanna screw the nut onto the bolt? That's really complicated. I have to find these parts, I have to align them relative to each other. I have to know that there's a behavior that's related to threads and a screw. all these things. It's that. And then I said wow, how do I start where I begin? I said, let's just break it down. So we could call these like goal states, but to me, breaking 'em into these little at atomic pieces is very helpful for me to think about.

and so yeah, that's, the only, that's the point of this conversation. I think it's ing I think it is. Yeah, it does give me some clarity in that. I think often, like for example, the hypothesis testing policy, we obviously implemented like this. It was just like, this is a primitive where we're like, okay, sometimes a column wants to tell. It's the most likely things. It's, sensing apart. And so it might have this kind of primitive for it. On the other spectrum was the idea of like behaviors and that, obviously a lot of, tasks are going to be dependent on the behaviors of the world. And so you would learn behaviors and then you would informed by those behaviors know how to, act. But, But we never, but I like the idea that, yeah, maybe we never connected those. Yeah. and that kind of, so obviously behaviors can be essentially arbitrarily complex, but that may be on top of behaviors. There is this kind of set of relatively simple finite list of, sort of subroutine, right? I think, to me, this is the way breaking it down into these components because we have to be able to do all of these under different, complex goal oriented behaviors. And so then I'm thinking like, okay, Colin, once you can do these base ones, you just combine them in different ways somehow. but they have to be able to do these. So the, this thing I always talk to, like, the, logo on the coffee cup, it's an example that exposes lots of problems. And so to me this exposes, it gives me something to shoot for. okay, how can a, what I, have no idea yet, but how can a column somehow move an object to have a desired post? how does it do that? I don't know yet, but at least I know it, it has to do it. So it's a problem that I can sink my teeth into. It's also interesting because I feel like the history of Monty has been like, we started off not having things implemented at a very neuro level, and then we've always been like, let's see how we far we can get without that. Because, it's easier to debug, easier to understand. But of course there would be long-term benefits to having a system that's fully interpretable and doesn't have a bunch of neural elements. And those, again, are everything from interpretation to safety and all this kind of stuff. but it, always felt to me like, yeah, once we get to this kind of stuff, it would probably become this fuzzy learning stuff that would maybe I'm not, Neil, I'm not suggesting I, I like the idea that maybe we don't need to, it can be a bit more hardcoded and it would almost be like, yeah, whether it's neural links that, there's certainly some of these, you could imagine how it's oh yeah, this is L five projecting to this layer, and that's what causes a sequence to happen and this is L five projecting to this layer, and that's what causes a particular ID to be realized, or, yeah. it doesn't have to be neural the solutions. It, helps me to understand the solutions, but what I what by thinking about the neuroscience, I think oh, these low layer five cells, they're almost certainly representing, movement vectors, in, in, at least in the modules we have today, movement vectors in the starting out in the reference frame of the object, and you get converted to the reference frame of the sensorimotor, the orientation. So how we implement it doesn't have to be neurons. but that's the base element. You've got a movement vector. and that's gonna be, because those, I'm pretty sure that's what the layer five cells represent.

I'm, I was confused about your comment about neuroscience. I don't think we have to, this doesn't have to be a neural solution, but Yeah, it's a. I'm just saying it's a positive. I think if we can come up with a list and then implement that without having to start having, less understandable representations like neurons with heavy and weights, or we don't wanna apply some sort of deep learning output on top of these things. Yeah. If that's what you're saying. no, it's, it really, we really should be able to completely understand, okay, if a column, the only thing a column can do is output some sort of movement vector. And that movement vector is going to be learned based on the movement inputs it has received in the past. So it's learning to model it. It basically learns to, it learns what movements exist in the world related to its sensorimotor, and now it's able to generate those movements. What can, how do we solve these problems? how do you do these individual parts, anyway, I think as we get into this, we're gonna find all kinds of interesting complications and challenges, but personally, this is now I feel like, oh, I can sink my teeth into these things. I now know where to begin my own personal, exploration where thinking about high level tasks like making coffee and folding laundry, I just, it's not, base enough. it's like It's, it's, it wasn't, I, need to decompose it into the atoms of movement, the atoms of behavior, and then we can rebuild it up later. Yeah. and I feel like at least some of them, when we've thought about it, feels like a nice thing. Is it feels again, just simple associations can do a surprising amount. the kind of light switch that it example of a, lamp or whatever that you, like when you learn that behavior, you attended to a particular location, the state of the lamp, the switch change, sorry. And then the state of the, lamp change, like we need to work out the details, but it's not too crazy to imagine that you move into a location in a behavioral sequence and oh, the light switch was in this orientation, but right now in the real world, it's in this other orientation now just output a goal state to invert that or whatever. And, yeah, I don't know. Yeah, that feels encouraging that it's, so I, was telling David the other day, I said, I, wanna have all the theoretical constraints of Monty done within like a year. Like all these things and these big theoretical ideas. And so to my mind, this is One of the, this is the big area to me personally, maybe not to you guys, but to me personally, this is the area. I have no idea where to begin. And, I have good ideas about composition now. I have good ideas about tension, I have good ideas about sequences. we came up with some great ideas together about, behaviors of objects. And so this thing has been looming in the background, oh my God, how are we gonna start thinking about how to make a cup a pot of coffee? And you've broken it down, Neil, but it wasn't broken down the way I could think about it.

I feel like, okay, we're starting something that might take a year, or if we're lucky, but we should be able to do this.

anyway, I, just don't wanna over promise. I'm just saying this is the beginning of how to think about this project, right? Yeah. I mean it, that's definitely helpful. The last big open question, I guess like after object behaviors, I still feel like there's abstract, spaces is, maybe the final. Yeah. I, keep, but hopefully it's simpler than, we think. I think Neil, I keep thinking like, somehow that'll become obvious along the way. So as opposed to focusing on it. 'cause again, I have no way of really attacking that right now. So don't worry about it. cortical columns or cortical columns, Mount capsule seems to be right. So somehow let's keep doing the things we know we have to do. And maybe that abstract stuff will just fall out. I, think it will. Yeah. It seems like it has to because it, should just be a matter of columns getting different kinds of input, instead of having to add another piece of machinery to the column itself. And maybe, yeah, it's just somehow where does that abstract movement come from? But, I don't know. it probably is fairly, it is probably gonna be something quite simple, just like t's changing, that's movement or, yeah. I don't know. I don't know. I'm not too worried about it, but, I'm not worried about it all actually. It's, more of an annoyance, oh shit, we gotta figure that out sometime. But it's, not gonna require oh, we have to introduce quantum effects to, to, to the sensorimotor module. And, I think one, one thing that I also find encouraging and yeah, it's just become more clear with the recent papers and stuff, is like, just how much can be done with, associative learning. 'cause I think it's so common in, machine learning or anything that, anytime you don't understand something, you try and reach for these, deep learning or whatever, a complex system that's gonna learn how to solve that, for you. But of course, when we actually look at how neurons learn, it's some variant of, just associative connections. So we've gotten so far relying on that. So that seems encouraging that the rest of EE, even if a lot of it's un inscrutable, the rest of it will be the same that we can, yeah.

But it'll, we'll figure it out, and if, we, and if we design robots and building houses on Mars and they don't have deep thoughts about philosophy, it'll be okay.

Fair point.