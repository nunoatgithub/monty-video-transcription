I thought we just pick up where we lost. Left last, left it.

and we're working on this idea that color could be represented by a set of meaning columns that are independent of say the edges, right? So like color blobs. And I was reading more about color, but I'm gonna correct myself. The thing I remember is colors mostly considered to be processed in V four with stripes in V two, but the blobs were in V one. So I was remembering the fact that they say color is not really processed or understood until V four, and I was mistaken. So when they say processed, I, what do they I don't, I just read it again. Okay. I'm saying this is where I didn't have time to read it. Oh, then I read it was just while you're writing here with John Chen, I read a paper by Catherine Rockland and, known like anatomy and, this is about blobs and it look extremely low level details about all these. Sh minute differences in structure within structure and it's like really mind boggling. So you, I can't make any sense of it, it's not like blobs. Her point is, blobs aren't just these blobs, there's all these a substructure, more structure inter connected and, but no explanation what anything means. also very creative name. excellent blob. An inter blob. There's a blob. And the thing not in the blob is the inter blob. Yeah.

anyway, and then those, remember we were looking at the big green color blob and layer three and then this other what, red, whatever the other word and the like. Those are just more examples of as I was looking at, okay, there's like little blob lets in layer four, but there's bigger blobs in layer three. Maybe they're all doing the same thing in, the sense, in the following way that we're, we started talking about. But here, I was just thinking about, here's a picture, actually I should draw this. Like there, so this is the basic model working on this, that we have two reference frames. they're both driven by the same motor behavior, so they're not, they're actually in sync. Most of the time that may talk about that a bit. and we're gonna assign one of those reference frames for behaviors and one of the reference frames versus not, we're changing things and static things. So wanted to be static objects and wanted to behavior modeling behaviors and modeling structure. So structure versus behavior, structure versus change. And then we were up to now, we were just less quick. And earlier we said, okay, oh, this is really cool because we have these edges, these mini columns that represent edges. And those mini columns then would represent, this would be the morphology of the object. And if those edges are moving, then it would be the change in the morphology of the object. And we could basically learn stick figures dancing or doing whatever staplers. It doesn't really what the color is, it's just about edges moving. And, and that's cool. But then we didn't have the color thing. So then we said, the next idea is, what if we represent a, another set of meaning columns with color? And again, this, these representations, you'll see it, these representations have to be, the representations have to be in minicom. meaning to represent orientation, I have to say this, orient meaning com is this orientation, this many comes, this orientation. So on you, it's like a, it's, there's not a lot of 'em. That's the point of it. you're limited number. It's a, it's the output of the spatial pool. It's k winners of a small number. So we'd have to do the same thing with color. We'd have to represent color as a set of mini columns where I don't have to say each color has to represent, each minicom has to represent a particular color, but I'd have to be able to represent my color spectrum with a set of mini columns.

and then the idea here would be, okay, these, what would you learn here? This would learn, oh, if I just looked at, imagine now this reference frame applies to all these upper layers. So we have a bigger reference frame. All these guys can look at the bigger reference frame, or you could argue that this repetitiveness is in the reference, if that would work too.

here we'd say, okay, it is oh, on a particular object. I'm learning like a particular stapler. I could, if there's a color, I could assign that color to the stapler. if the color of the stapler changed at diff, at any point in time, not, by location, but over time, then that would be captured here and that would then be fed into your behavioral reference frame along with anything else that might be seen. Maybe it's just colors that are changing.

so if the whole thing changes from red green to red green, it would learn that. but if it was associated with particular locations on the traffic light, then, it would be associated here with particular locations on the traffic light. You'd be going by the green, and then I said, this is the general purpose idea. You can have any attribute as long as you're represented by a set of mini columns.

And you had a static and a changing version of that, then you could learn some other attributes, change of color or change of, I don't just, what are these things we're saying here? This stuff.

and then it made me wonder and that, would work. I think then you could basically say you mix and match here. Like any one of these things could be enough to infer the correct behavior, right? It could be a combination of movement and color change or just color change, or just movement or something else that makes a sound. I, whatever it is we got to represent here. anything in this sort of column. and, of course on the original object, we might have assignments of these things, to the static reference frame. I, haven't worked out further than this, but my, the point I'm trying to get to is that, I could learn the behavior of an object using a combination of attributes that seem to be important. If the color changes as it goes and that seems to correlate to the other changes, then that would be, part of the behavior of the optic. maybe the color changing would be independent of the morphology changes. So the dissing changes from red or green every three seconds independent, whether I open, staple, or closer, something like that. So it was one, those as two separate behaviors. Then somehow, we, want to be able to say, okay, on a new object, maybe I recognize a new object by morphology. And, and I see the actual color of the object, but there's no changes in the color object. It's somehow this has to be, this has to lead to my prediction here, right? It's, because color is not tied to the, it's not tied to the particular location, it's just tied to, this section on the object is not changing. Therefore, if I open the stapler, there'd be no reason to this. This would have to be, doesn't have to be persistent, right? You have to say it's, if it was red, it's gonna stay red, something like that. Or if it was at, if, I'm looking at someplace and there's an edge here and the color changes, I don't, I'm just gonna assume it's the same, something like that. so I haven't worked through all the details, but it seems like you could transfer anything that's not changing. You would just assume it's constant because you didn't record a change there as part of this behavior of this, of a behavior or, or if, the color changed at different locations, that's not a behavior. then I, and I recognize another object. I might assume the same color change at the new object, something like that, right? not change the color. like if a top of staple is always blue. Then, then, and I see a new stapler has a different, has a, the same morphology I see. It's a stapler. Then I would say, oh, stapler, these locations, the stapler always blue, or something like that. I, haven't worked through this all, I'm just trying to test the ideas. Yeah. That's sounds, I think it sounds what I was suggesting, but it is. I'm developing it. So did something cha change in what you were thinking how neurons could be doing this or I just, what's the new thing for me today is the idea that I could, I have to segregate the many columns into these different attributes. Yeah. Up to now. When we always thought about column, I always thought somehow like to color or these other attributes to be integrated into the same layer four as the edges. It was like, I felt like an object has some morphology and then we put on top of it, we put some other attributes. I see this is saying objects. Actually this isn't even prefer the edges isn't even preferential here. It's just another type of attribute. So how does this help with the problem that the color needs to stay the same as the stapler moves through different locations? It doesn't yet solve the problem of, how do I even know it? The predicting where the stapler aren't top will be, but it does solve the problem, I believe, of color constancy or color changing because it's a separate column, not just a I have a, model of colors for this object. So in some sense I could have a static object with, color changing colors. Or I could have a changing morphology object with static colors. They're separate models. This is a model color model of the object. Yeah. And this is a edge model of the object. And this is the other attribute models of that object. And we've already had this idea in some sense, like combining like the sound of an object or the touch of the object is the feel of an ob, the vision of an object, like what it look like, what touch, but those, I always think those are in separate areas, right? Those different parts of the brain. This is all in, this is right here. We're still in one column here. This is one sensor patch at one location, sensing different things.

Yeah. So basically having a color in their own columns makes it possible for us to keep it constant. Because the color, part of this model dictates how colors change or don't change the edge part of the model is how us, how the edges move or don't move morphology and whatever other attributes we want to throw in there would do the same. And the nice thing about this is a, the next step is to say, this is nothing special. This happens to be what we see at low level visual and somatosensory regions. but elsewhere in the cortex, I may have nothing like that.

I, can, it frees my mind from thinking like, what's the morphology of concept space? I don't have to think about that. I just have to think about, there's a bunch of attributes that are changing, and this is a big difference in my mind, right? I divorcing things that we thought we had to have in every model, like morphology and edges or something like that. And to say, no, that's just, convenient down here in B one and S one, but it may mean nothing in, higher up. And it's just a different set of attributes that I, yeah, I feel like it's quite, in thinking in the sense that before we talked about features and morphology being treated very differently, but now it's there all mini columns, right? They're separate mini columns, but they're all right. Yeah, conceptually. So it's, this is like a, our old stuff is a subset of the broader super set of how a column works. And when we think about primary sensory cortex, especially vision and cortex, it's hard to, vision, touch, it's hard to avoid these edges as a feature, right? Just hard to avoid them.

there is, I, already have a problem with this, so I like this. And then while we were, you were in your meeting with John, I had a, I think, potential problem. I was trying to imagine, and this is the one you brought up earlier, changes in orientation. Like whenever I have a feature, like a little logo or something, and it just rotates. So it's a compositional object where I say, here's a feature at this location.

but the thing that changes the behavior is it, changes its orientation. So then you can imagine opening the stapler and there's a little logo on the side as you open the stapler, the logo spins around or something like that, right? Yeah. It's cool, but easy. Remember, I could easily see that. I have no trouble learning that. or maybe it points up or points to the left. I don't know what it is. But, so I was trying to, just, before we came back in, I was trying to figure out, how does orientation play into this? changes in orientation are part of the model. I, need to be able to represent that.

could that be done in a similar way as we do with normal changes in orientation of an object that basically the change model, the behavior model, encodes changes in orientation, and then those, those get sent up so, this word gets confusing. I said, okay, why don't we just add some more mini columns here and these are, these represent orientation. And then I can say, okay, there's some orientation coming in for some feature, and if that orientation changes and the feature doesn't change, then I would encode that here. and, but now then you'd have to send that orientation to theist and orient, make it orient the, something like that. And now, and then it got really complicated because. This orientation applies to, child objects. And I was thinking that what, that's what this was applying to. I thought, oh, this is gonna be, originally I said, oh, this is the orientation. And the child object was gonna be, just like colors. It's gonna be like somehow I'm infused into layer four, right? I have these orientation columns and somehow you go the child would be some sparse representation, which dictates layer four cells, but it doesn't work anymore than that. If I do this, that doesn't work anymore. so it's just, I'm not saying there's no, it's a big problem. I'm just saying I'm confused at this point in time as an hour ago, do I have a separate mini console orientation? I thought that's what these were. and, or was it the following? Is it like, it's a, if I say the logo is at a certain orientation, is it, this is the orientation and the logo's here separate mini console logo. and by the way, as a child object, the child object can be so many, you can't represent 'em in many columns. I can't take a child input and say, oh, that's a new simple attribute. Let's represent in 20 mini columns. It's, that's not possible. There could be millions of children objects and or hundred thousands of them. So I can't, represent, I can't take a child input and say, oh, that's just an attribute at this location. It seems like I have to do something else beyond that. Wouldn't it be a, just a protection of the temporarily pulled. Representation in the lower level. Yeah. But where do I, store it here? How do I, where am I gonna, I wanna be able to say there's a child object at some location. I know how to do that. I know how to, I know that I'm sending the representation for the child in SDR and I can calculate the orientation in the thalamus, the relative orientation. And, so the information's coming in and my assumption all along was that, oh, I'm just going to use that to dictate which cells are active in the oriented mini columns. this was the orientation of the child object. And, but I can't do that. I can't, do that here. This is just morphology. This doesn't, I can't add into the right. I'm, or changing the orientation of a specific object if I'm a bit confused. Did you mean to draw these, the other way around? 'cause here it's changing and there's static, but then Oh yes. Oh yes, I did. I did that wrong. Yeah. Yeah.

That would, the only one that these didn't specify. So these tied together.

I guess one thing that's a bit different in this, like static edges view is in, at least in Monty or in general, it seems like you don't need to store all these different edges. You would just need to learn a model of one edge and then have corresponding orientations of that edge that you apply to that one edge. What do you mean apply to it? Oh, you're saying like the feature is the edge and these are the only, yeah, the edge is the feature, and then it can be, it can occur in different orientations. I thought about that here. I maybe it works, still works, but I was having trouble about making, I, for a while I adopted the idea, but now I'm, struggling with it. And then whatever's egg quoted in the behavior model would be translations and orientations, that are being perceived and the, and translation orientation. There's many columns that I see that represent angled edges. Yeah, that's the, is that, a universal orientation or is that a feature at that angle?

I'm not sure why we're seeing that in the brain. I'm just saying from a algorithmic level, you would just need to store one edge and then be able to rotate it to whatever orientation you need. Oh, okay. Let's say that's what I adopted in this little picture here. In some sense, I'm saying that your feature isn't edge and we're showing it different orientations. I think I took, yeah, and basically you wouldn't need to learn, but, how do I now apply that to the logo? Child object. Yeah, so basically the behavioral model would just include translations and orientation, and then you can apply a translation orientation to any kind of feature. It could be an edge or it could be a curve, or it could be, okay, so let's say we just, the behavioral model applies that At the moment we have a, our, my mental models, I have a set of cells that represent the location in space. I'm a another set of cells, layer six B that represents the orientation. So I have two different, some sense of reference frames, right? I didn't show the other one here. Six A and six B is the idea there is that, would be, locations and directions, orientation.

and both of those are path integratable. By the way. Motion can change your orientation or something and motion can, change your location. but I don't know what to do with that. So let's just take, let's continue that here. So this is my reference, static reference frame. This would be six A is a six B. This is my, this is like location on the object. And this is my orientation of the sensor to the object. so I have these two layers of cells here. I'm still confused how I Yeah. These would be, those would be the, translation and orientation of the features on the object. And this is of the entire, That's the entire object.

This is a different, is it, no, this is, sorry of the sensor. This is the location on the entire object. This is the location of the sensor to the entire object, or, maybe at this point in time. Yeah, I guess I, yeah, those are very local things that just being applied locally to one of the features on the object. So versus the, can you a picture, can you make a picture that makes it all work?

yeah. I don't, is, it may not be hard. I just, I haven't gotten it yet. I was just working on it, so I haven't thought it through at all neither. So I don't know neither. That's why I'm stuck. It's not like I'm saying, oh, it can't be done. I'm just saying maybe we just haven't drawn it yet. I haven't thought about it clearly yet.

that's what I'm trying to do. I'm trying to come up with a picture that captures everything and says, oh, yes, now I see. It's all clear.

And so we would have, are you, are these both layer four now or, I didn't say where they are. Okay. So we have, so no, and the classic view would be the static one. It's layer four and the changing one is layer three. Okay. Okay. So we have, because layer four we know are those represent, edges not moving. And layer three represents edges moving.

That's a classic view. So we'll say that's a delta, the changing one. And and, just bear in mind when we say layer three and layer four, there, there could be multiple different types of cells. In layer three, there could be temple pooling cells in layer three and motion detection cells in layer three. And so we can't say, oh, we've allocated all layer three to do something. it's it could still be temple pooling and passing up to the next region. So yeah, don't, think we, these aren't figure answers to where these things are. So we have the, feature input, going to both of them. I guess. We get the change in it's, we have an input from the sensor. Some of that input dictates something's changing and something dictates it's not changing. Yeah. So we have the marking cell pathway, which would detect like motion changes or if you look, actually look at the cell, this is really key. If you look at the mag side of the cells, they are, center surround cells that detect motion, detect change. They do not detect motion. They detect change. Oh, interesting. Only, when you get to the cortex, does it look like they're detecting motion? that's exactly what we want, right? I think so, right? Yes. I mentioned each other, but encouragement that the inputs are center, surround, non-directional, but they detect change. And that's, a universal rule that there's two types of inputs. Ones that are, that are static, and, and I forget what they call it. And the other is that they're, they're change detectors. So it is, it, is clear according to the literature that the idea of motion only appears once you get inside the cortical column. Like once you like, like a motion to space or something like that. of course most language neuroscientists believe that it's hard coded or the balloon and it's genetic. But the more generic case would be you get a bunch of change bits, and then the cortex has to figure out what they represent, does a spatial pool on it and says, okay, I think that's more likely to be like an edge, moving edge. so yeah. So we would have the mark pathway going into layer three telling it what changes it's detecting. Then we have features going into layer four. We have movement of the sensor relative to, which is also magnet pathway, but over broader area. Yeah. actually, that would actually be total movement. Let's leave it way just like you said it right there. That's good.

and that would be transformed into movement relative to the object by the thumbs. By the thumbs using Okay. Protections. Okay. So that's pretty much what we had so far, except that we have the delta change going in here and having a separate behavior model here.

Was the separate reference frames. Did you say anything already about whether they would both also be in layer six? I have no idea. I'm totally, it would be, if I had to take a guess, I would say that they're both in layer six. 'cause you want 'em both to be driven by the same, motion vectors right here. Here's how I, here's how I imagine that would look.

Here's a set of, in this case I view the mini columns as representing. These are the things that are required by grid cells. They have a series of cells that all represent motion in some direction and they, the frequency, their voltage control oscillators, so the frequency of which they're firing is depending how fast you're moving in that direction. And there's a background frequency that does not change. So the, which of these cells lines up with the background frequency? That's your hot spot there. and as you move, that cell will move along this trajectory. Okay. That's, the basic theory about how grid cells might come about. 'cause this one's in phase, then this one's in phase. And the faster you move, the faster moves. So this is like a repeating one dimensional space vector. Like you, you're along here and then I can't, it repeats and repeats. Okay. So that's, and you've got a whole bunch of these. It's unique space. So what I was imagining now is you have two sets of these things. They're both being driven by the same motion vector, but this set anchors differently than this set.

anchors two and anchors, to the one. So this would be object and then, this would be behavior so that it really just all tied together. They look very, similar. The difference would be that the, moving selves doesn't continue down here. It would just would loop around here like this and this this. will loop around here. it doesn't really loop like that, it just keeps moving it off back on. And, but these would have different anchorings between them so that they, and they would, these would have associative connections to layer four and those two, layer three. basically I have two reference frames I can connect to whatever I need to connect to. So this, one would be driven, the behavior one would be driven by changes. Changes, right?

So I, would imagine those are both down the bottom there.

Say, be reference behavior, reference frame and object. Reference frame. Okay.

okay. So what, so the, like the sort of base stapler object that we apply changes to, is that what you're The object reference frame, the base stapler would be in the socio connections between these, between the reference frame for the object and the features, the, static features. So imagine this, you've learned a static stapler, that's just the, object reference frame and layer four pattern, right? Now something starts changing on the stapler that. Re anchors the behavioral reference frame. But doesn't touch the object reference frame. So now the behavioral reference frame saying we're starting a new behavior. And that behavior, although it's located on the stapler space, it's on the stapler, it occurs one place, but because we have its own little reference frame, it, we could apply it to other objects other times. so that's the key there. There's also, object ID somewhere in there. Three or layer two. Yeah. we're not worrying about that right now, but there's someplace. Yes. but the application of behavior to the, object model creates predations in layer four. we, we have Or what to be observed? we haven't said that yet. We don't, haven't said how we are gonna do that yet. What, It does say that if I recognize an object and I recognize, or if I recognize the behavior At some location on some object, I will now apply that behavior to those locations on the object, whatever the behavior model says. I, we have not yet worked out exactly how it leads to a prediction on that floor. That's where we got in trouble. Yes. There were two days ago. Yeah. So I guess zooming in on that's, what we stopped. Yeah. So we zoom in on this layer, there are four layers. Three, both of them. Okay. what you drew there, so we have, at the top, we have the delta changes, and here we have absolute values. So for the delta changes, we would have translations in space and we would have rotations in space. that's, this is a hypothesis because Yeah, I'm not sure if the rotations make sense. Yeah, but keep going.

that's a question. This is a hypothesis now saying maybe we'll have rotations there. Yeah. it seems like the mag cell, like you would be able to distinguish like the local flow from whether the object is just moving like this or whether it's moving like that. No question at all. That the cortex calculates changing in orientation from the mag, the large magnet of the pathway. Yeah. I'm not, I didn't even get to applying the orientation yet. I'm just saying those should be able to detect orientation changes of local features. I guess I'm saying this is the current hypothesis because I have no evidence in the biology that you see that, but I'm good with it.

the, other edges moving, we see that's well known. Have they ever tested the, like having an edge under, I don't know this. Maybe they have, I just don't remember it. All I do know is very well known that there's edge movements. Yeah. I mean it just seems like it's the exact same mechanism point. Is that the moment. Hypothesis because we don't have any evidence for I'm going, I'm good with it. I'm just pointing out that all neuroscientists would say, oh yeah, there's this edge moving mini columns. Yeah. 'cause they tested moving edges. But I mean it seems like the exact same thing. If you have, like I agree the past, you can detect that there's like local flow like this, but you could also in like with the exact same mechanism detect that there's like a flow field that rotate, What's interesting is that somehow we believe that they differentiate down in layer six, that somehow layer six, no size, tease those two apart and put 'em in two different representations. Where in some sense you could argue it's just be, it's just take a bunch of change bits and map it into a set, put 'em into a spatial pool and some of the, spatial pool or some of 'em would be rotations and some would be moved, whatever Yeah, exactly The weird thing about it is we're hypothesizing that they're, separate down here, so how do they get separate down here? And they're not separate up. I'm just, I'm right. Oh, you mean here? Yeah.

I guess down here it's, they're really different. They're not just a difference in whatever look flow you observe. Here we actually moving through the whole reference. I, but, remember it just, it's just, it is just center surround bits coming in. So the system has to figure out what those center surround bits mean.

Let's keep going. I actually went as far as saying maybe we're wrong, maybe the location and the orientation are combined together down there. Yeah. You mean, so basically if we have movement of sensor relative to the body, that movement would include detecting rotation of the sensor. And, if you think about in the grid cells and the anon cortex, there's these pure grid cells, but there's far more cells that are orientation and grid sensitive. Like they're, like, they're gritty in one direction. You know what I'm saying? Yeah. So that which is consistent with this hypothesis.

I'm just constantly keeping all the options open. I'm just, that's all I'm doing. I'm not trying to interrupt you. I just, sorry. I'm just saying, okay. If I accepted the, that movement and orientation movement in, directions and orientation change are just all universally the same, that just whatever motions were detected and put to a space pool, then I have to ask myself, why didn't that happen down here? 'cause here I have the same kind of input coming in, and yet I'm assuming that they're separated into location and orientation. And then I say, maybe that's incorrect. Maybe there's a spectrum between location and orientation and that seems more consistent with the biology. But let's keep going. Let's say there's orientation, just like we've drawn it up there and I'll stop. Yeah. I guess just on that point here, so I agree, it's an interesting idea of whether this movement of the sensor relative to the body actually also includes. Rotation of the sensor, but then here it's actually the inferred location on the object and the inferred orientation of the entire object. So it seems like that's, quite different than just No, Basically what you, the input would be turned into, the input would be turned into, for example, movement in some direction and rotation in orientation one way or the other. And then the good cell mechanism would turn into a specific orientation of this object or a, I don't know. Yeah. So basically this could be a location slash orientation input, but then that would still tell you how to move through that space. Okay. Let's put it aside for now. Can we, I, shouldn't have interrupted you. Can we just go back to the, go back to this? Yeah. this is hypothesis that maybe we're representing rotations in orientation as part of the, just like anything, any other type of change. Yeah. All the behavior, real changes we could have and just color as well and that would, color would be a different type of change, right? Yeah. Color change could be temperature change.

I don't know what else, like child object Id change in the future. It could be any kind of weird, we have to do child object, but, child objects don't work there. Because there are too many. That's the problem. You can't represent child object IDs with many comps.

That's the problem there.

Okay, so we have a bunch of changes. They're all treated the same way. Could be trans, but just to focus on translation and orientation right now. Yeah. We need to be able to, or it would be useful if we can apply those to the static features that are coming in. So if I'm sensing like an edge or a curve or spike, peak or whatever, it would be useful to be able to apply these behavior, local behaviors to these local features. And I guess that's just maybe the last thing I'm gonna say because I don't know how that would happen, but that's the crux of the matter here, right? Yeah. one thing that could be, if we have the ths and it can rotate the, things, we could have a connection here, tell, it how to rotate the static feature and, that, that way when it comes in here, it's already in the expected orientation.

Maybe let's say that orientation. I don't think you need that.

I don't think you need the ous for this.

here we're, at this point we're just talking about.

on, like edges of the object or on and off things, right? There has to be in, in a blank field. This, these wouldn't get activated, but only when I see some bits moving.

this might be a, think of it this way. This is an edge moving in a direction like this. This would be an edge that's rotating, inspect at the same point. Yeah. But does, would it have to always be an edge? Could it just be any local feature that's rotating?

not with this mechanism we've specified right now because I have no way of representing. the whole idea here is I take the simple representation and I form unique in context of the behavior. So I can't, I can say, oh, there's a rotating edge here, but I, all I can do is say it's unique in the context of the behavior. And I can predict, I can say that in, in the next moment it'll be in a position like this or next moment. All I can do is predict the edge at a different orientation, just predict the edge at a different location. Just predict the edge at a different orientation. It just seems that way you are tying the behavior to the feature. So if I now have an, ball shaped kind of thing and that is moving. I don't really wanna have to relearn all of these possible translations that this bull shaped objects could go through. I just wanna be able to apply translation orientation, how I learned it for other features to this. and it in its core form, this would work because I never saw this before. This is just a bunch of little, this is a bunch of little edges that this thing is seeing, right? Different columns are seeing different things. So you could say at this point, there's a slow movement of this edge, and this is it actually looks like it's, you could say, yeah, I was assuming that's like the smallest possible feature. All you can do is say, a time t it looks like this. And time T plus one, it looks like this. And, and all these have some movement vector. And, so I could learn this, set of movement vectors is followed by this set of movement vectors and followed by this set of movements. I, could represent this curving sea without knowing anything about a city. I gonna say it's a bunch of lines that are at this point, are moving in these directions. The next point I observe it, they're moving in this direction. The next line, it, they're moving in this direction. It doesn't necessarily allow me to fill in over time. Yeah. I, yeah, maybe when I said bowl, it sounded like this was actually a big object, but I just meant this is instead of having a flat surface, it had has a round surface. Yeah. I'm not sure if that's, if we see representations like that. Yeah, you basically, you would have to then decompose everything into tiny edges, even round surfaces.

this is where yesterday I started thinking maybe to solve this in an efficient way, I need multiple columns. I can solve this problem very inefficiently. The model could just say there's no concept of C shape structures. It's just a bunch of bits. Moving another bunch of bits and moving into bits and or changes. I can learn very complex behavioral model. How all those changes, what changes. I see the time, but it doesn't. and that would help me infer this behavior, but it doesn't, it's very inefficient. 'cause I have no way of moving the whole thing. I just have to learn every point at every point in time. Does that make sense? Yeah. So more comm, like composing, making a c shape out of several, right? 'cause what reality would be to this, have a single column. To do that, it would have to sample all these points and then sample all the next points and sample all the next points. It can take forever to learn because I can't do it in one pass. But if I have a bunch of columns observing the same thing, and these columns somehow know where they are l to each other, somebody's gonna be observing every part of this. at once. And that's where I started. And that's as far as I got because it seems like they could coordinate with each other saying, okay.

I, don't know how they would work, but they seem like they could coordinate each other to essentially move everything at once without me having to learn all the points in between. So this is all just based on the fact that people don't only observe at, moving edges in the brain. But it just doesn't seem possible. No. I imagine at a higher level in the hierarchy. Imagine the stapler where you have the logo on it that rotates in the opposite direction when you open the stapler. Yeah. You can learn that behavior and you can apply that rotation to the child object, the logo. Okay. So, that's a separate, I think that's a separate problem. We, there's two problems we haven't addressed yet. How do I deal with the fact that child objects can be changing and, child objects. There are many, child objects and therefore I can't represent child objects, many columns. That's one problem. Yeah. That's not other problem. And then the separate problem is I'm moving a bunch of stuff through space and that stuff is not changing other than it's moving and changing orientation. This is the top of the stapler. This is a separate problem. This is saying how do I, how do I represent that in an efficient way? Yeah. Which is not quite the same as the child object problem. yeah. I guess I'm bringing up the child object because you can start imagining learning the movements of edges and then decomposing things into edges, but it just falls apart once you need to apply movement to child. Like more complex things. I agree. So then I said, then we said to ourselves, okay, it edges are at orientations. That's like an object at orientations. And then I, that's one kind of a child object is an edge. Then I could have a logo at orientations and so on. But I can represent an edge simply at all. Its orientations. I can't rep, there's an infinite number of child objects to that I might represent and they're all very high dimensional vectors. And so how do I do, I can't take that child object ID and say edge is like one thing. Oh, I have an edge. That's one object. I can, rotate, I can represent it in different orientations. I don't, I can't do the same with a child object. 'cause there's so many about child objects. I can't have many comms dedicated to the child object.

The answer obviously has to be somehow a projection, from the parent child to parent to the child.

So for example, and just can we just brainstorm about this a little bit? Imagine, I think we did this the other day. I have two, two, regions, hierarchical follow hierarchical regions. They're both recognizing the stapler.

and then I see the staple top moving.

maybe what that does is it says, here's a region that's moving. I'm gonna project down here to that same region, only pay attention to that region and rotate the stapler. So I'm rotating the whole, sometimes I'm rotating the whole stapler down here, but I'm saying attend to only that part. So that's pretty simple. I have to pass the ID back. That's not hard, I think. And I just have to say, rotate. And then I also have to say, only pay attention to the part that's included in the behavioral. I always picture like a behavioral object is occupying a bunch of points in the space, and those are the only points that matter. It's like only in the physical object. The only points that matter in the TPO topology morphology object. The only points that matter is where there's an edge. Everything else is nothing here. The only points that matter is whether there's a change. Nothing else matters. So I could say, project down my change locations onto this guy and say, okay, you're, doing the following. You're, you're rotating this, but only pay attention to the observed, to the attended part. that's actually moving everything else you're gonna ignore.

so I could use the model here of the stapler, and all I'm doing is rotating this part of it. And, and, now I, I have this sort of parent-child relationship where I can use this to generate all the predictions I need. Does that make sense at all? The, yeah. the subject, the behavior of the mask should be possible to do, because we're at the top level, we're learning behavior, right On the location. If I look at, if I'm looking at the behavioral object, there's points in space of that behavioral object where there are changes occurring. I then go down to the child optic, which is co-located, right? There's columns more co-locating on those columns right now. those are the owners we care about, somehow move that subset or, I don't know. I, don't know how it work. I don't know. I'm trying to use, I'm trying to, I'm trying to solve the staple problem by saying I have another model of the staple here. and if I could just rotate this model, only pay attention to that part, then I can make predictions. Even though this is a novel position here, I've never seen it this way, but I could predict, I could say, what would this look like if I opened it all the way up? I could say, oh, I'll just rotate this part and now I have this object at a different orientation. And I could make predictions based on I'm using the, I'm using the model here. I'm using the, yeah, I'm rotating this whole space. I can't rotate this space, but I can rotate this space and therefore I can make predictions on it.

Yeah, that would definitely make it easier to rotate all features. It's gotta be something like that. I don't know what else it could be.

It's like when I open the stapler, you can just imagine, you open the stapler, visualizes, you open the stapler. It's you don't, it, is like the top of the staplers. Nothing changed. You just, it's just the top of the staple. You how to predict any kind of prediction on it. It's as if the whole stapler rotated and you're looking at the top of the staple. No problem.

It's not like I'm doing translations or moving and, calculating something. It just feels like I'm just looking at the top of the staple. There is, it's it's the whole stapler, right? Yeah. It has to be that way because with this idea that those behaviors would tell me, would rotate the features, that doesn't work anymore. Once I ate to different locations on that part of the stapler, like after the stapler has moved, I am not really in a movement part of the sequence anymore, but I still wanna make predictions about. The correct orientations of features. So in this one it would work 'cause we are rotating the entire stapler or top part of the stapler. Yeah. So we can make predictions about any point on here. Just like we normally do. Whereas here we are we past that point in the sequence.

I'm agreeing with you're saying without falling into details. Yeah. this gets back to the other problem we have with going to be take an object and break it into multiple, separate pieces. Talk about that a lot. It always bothering me.

this tells you if I see something move, like, the top of the staple move, it would automatically, it might automatically running the top of the staple as a separate object down here. It could just, you could say, I'm gonna create a child object of a subset of this object I had before and now this is its own child object and now I'm just rotating the child object up here.

so it basically says if you start changing independently of everybody else, I'm gonna split you off and you're gonna become your own thing. That's nice. Because it doesn't have, it works without doing that too. Like in the long run it tells you where, how to split out the right objects. It doesn't require from the start. Yeah. The first time you see it, you can still, make inference. I could, what it, yeah. I could see a static stapler. Just learn as a static staple, I'm very, happy. Then as soon as something starts moving, it tells me right away, this part's separate from that part. Maybe you should model that part separately and and then I suppose at some point, I, don't know, maybe you forget. I don't, but at least gives you the clues to start what to do. Alright. This, let's see. We could, I know this is really hard. I'm, everyone's being pretty quiet. Maybe because we're talking too much. I'm talking too much because it's really hard.

basic, I, this is part of the solution, not a complete solution. When things start moving here, we try to break off a piece, the part that's moving, we instantiate it down here in a full model, isolate the section somehow how it turn this into its own model.

and now I'm assigning that own model to this thing. That's the basic idea. It doesn't solve everything, but it's, that's a part of the solution, I would say. making it its own model is like a step that happens over time. It's not like the first time you see it, you can still do it with stapler being a complete model. You start with a complete model. It's not even clear that it becomes its own model. the mechanism we just described here. It doesn't require it being its own model, it just requires that I'm able to isolate a subset and treat it as a child. Yeah. In fact, it would be better if I didn't create my own model, because if I create a new own model, I have to relearn it. I have to relearn a new, reference frame. Okay, so let's, back away from that. Basically, I have two models of stapler. That's not too hard. We got that already.

it starts moving. the upper guy says, okay, I'm gonna, I'm going to, I'm just gonna tell whoever below me, to, isolate this section that's moving and, and make predictions based on that movies. It could even be like a dynamically determined child. That's the way I look at it. I haven't think about the staple. I don't actually ever think of the top of the stapler. Separate from the staple. I don't expect to see it over on the counter over there. It's always attached to the staple. It's not really its own object if you never really separated onto its own object. It's just, when it's moving, I treat it like it's its own object. so I actually don't have to learn as a new object. I can just dynamically on the fly, create a child and say, okay, this is the part that's changing. You just ignore the rest of this. I'm telling you to rotate and or it, you're rotating or whoever's doing it. rotate it, and now we can make predictions based on this rotated section relative to me up here. Nice. Because it's a bit like the bent logo you could have if you just have a hinge behavior, you can apply it at any part of the staple or Break any objects. But you can, you can imagine the hinge being here, and you can imagine being here, so you can apply the object to subsets of the full object. You can apply the behavior to arbitrary subsets very easily without ever seeing it like that before. I even imagine on the, top of the staple, instead of going up like this, what if it opened up like a set of doors like this? As it, goes up it goes, me ran. You can imagine that. I could learn that really quickly.

this guy doesn't really care. I says if this thing is moving, I isolate a subset down here. maybe I The whole No, that, that gets into problem. Okay. Okay. Sorry. It really, I'm saying a behavior that looks like stapler and then as you open the stapler it becomes like this. And so these little things can flip up like that.

This means attached to it. that would basically means, I don't know, just imagine it just like little p just opens up like a set of doors. There's a base plate here, and then the hinge here. And a hinge here. Okay. And there's a big H here, right? He goes, what?

I have to make separate predictions about this and separate predictions about this and separate predictions about this part, because the original model didn't know any about this movement. But to make predictions here, I have to assume this is all one piece. To make predictions here, I have to assume this is one piece. And so I'd have to say, I'm breaking this into three children that are behaving, have their own, or, I'm, it's three child objects now. There's this one and this one. And I'm learning how those trial objects are moving.

how they're moving. I'm, basically saying, take this part, isolate it. Imagine it rotated like this. Now make predictions about it. Take this part, rotate it. Imagine it like that. Now make predictions about it, and no more do I actually have to learn these as separate components, separate objects. I just temporarily, yeah, we don't have to actually rotate the incoming features because we, just rotating the reference frame of the child object. We're rotating, we have to do rotating the reference frame and attending to some subset of it. Yeah. And I, and what it also tells me from a child object point of view, I can't do all three of these at once. I have to, I'd have to. I'd have to rotate the child object in one position, in one area. Then to make predictions over here, I'd have to imagine this part being a child object and rotate it in, a position. Then I'd have to imagine this part being a child object, being rotate position, that I, would have to mentally slip between these two, these three things.

I think that's what you would actually would do. I, think you would, you'd have to attend to this and all of a also anchor onto it, attend to this, anchor onto it, attend to this anchor onto that section. just like you can attend to this and anchor onto it. It's, like a way of saying, what do you mean by anchor on, I to anchor the orientation in location? It's imagine I was, imagine if I was observing a stapler and largely occluded. So all I saw was a piece of it. I could still infer that's a stapler and I would say, oh, where I am a stapler and what orientation is a stapler? That's what I mean by anchor on. It's like saying, oh, I'm only seeing a part of the stapler here. Where's the rest? Where's the stapler? It's okay. The stapler. I've only seen this piece. I imagine, I would imagine it's kinda snapping your internal model of it onto, it's almost like the observed, right? Imagine I have, I only have a model of the closed stapler. Now I have, and that's exists down in, in the lower regions down here. Yes. And then what I'm doing, I'm saying like, it's like all of a sudden I'm saying, oh, there's the stapler, it's occluded and it's a, it's its location of disorientation. I can only see that section of it, but that's still a staple. I know what it is. Then oh, here's another part of the stapler, not another part. Oh, there's a stapler again. Oh, now it's a disorientation. We only see that part. The rest of its secluded. And, so it's the same thing. I would, I can do this with, a static model of the stapler and just say, oh, there, I can see an included part. And I can say, oh, I recognize what it is, and I see the orientation of it. that's what I meant by anchor onto it. It's it's like I, I'd have to, I have to basically anchor the location and the orientation of this section. The part I'm this guy's saying attend to, God, I'm trying to remember how, but it's, yeah, exactly how you said you forget the exact words you used. Snap the full model, over it I imagine it augmented reality. You try to do the object detection and then when you recognize the object, even though you see only part of the fork, you then project like the entire model of the fork over it with the orientation that you recognized. And that lets you know where to predict what if the ha if it wasn't occluded. And then I guess with this mask, it'd be like augmented reality, but you only project part of that, object model on there. And we don't wanna fill in the rest of it, but we're really basically saying it's the opposite of augmented reality. It's segmented. It's segmented reality. It's Only pay attention to this part. Forget the rest of the world. It's just this part. What do you see there? Okay. You see a stapler part at good. Ignore it.

would this solve all of our problems? What if I see the stapler opening and I wanna predict Yeah, it would work. the behavioral model is like a, point cloud where changes occur. And so as a stapler open that point cloud reflects the movement. It just reflects the movement of the stapler. Remember someone showed a, the random bit pattern, then you show, you could show movements through the random bit pattern. Remember that image? Yeah. And I saw, I coded that up when I was young.

my point of that's a little bit like this behavioral cloud. It's basically saying it's all nothing until I see a change, that's all you're gonna get, just points and change. So, it is like the, as the topless, the staple moves, those little points where the changing are occurring. So there's a line of changes that are occurring, and those points of change are being, can somehow define what we wanna see below and, tell it what you know then, the thing below says, okay, here's my specific predictions. About, it's very much a high hierarchy. It's same as a hierarchy. Excuse me, same as composition. Yeah. I have a question about, hierarchy and a behavioral hierarchical, if I had a higher level behavior and let's say, and if you want to project that behavior down to child objects, let's say I had, this is my parent up and it has four things here. Okay. Four chi, four children. Yeah. And I want to apply, and behavior in this parent object is that it rotates by 90 degrees. The whole thing. Yeah. The whole thing. Is that really a behavior of that object? That to me, I wouldn't define as behavior of the object. 'cause it's not changing the model of the object. Be careful. Okay. Remember I keep going back to this. I have to find behavior in a very specific way, which is not, maybe not sufficient, but. We're considering right now is that something in the model has changed? Not that the whole model has rotated relative to the sensor that's not changing. Okay. You could rotate the section. So the parts themselves have to change something in the model has to change. The model itself has to change, not the, not my orientation to the model, but the model itself has to change. So attach that square to another object, like, a propeller on an airplane, the propeller is rotating relative to the airplane model or behavior. I can probably redo this example. you could just take, just say you have to get messy. You could just draw another big, square round. Or could you just rotate the B element? Rotate. Yeah. That's how I would have to do it.

so this becomes a, B. Okay. Something like that. Yeah. That, would qualify as a behavior of the object. Okay.

and Vivian's, right? If you took that whole object and put it as part of another composite object, then moving, if that was the dinner plate and it's now on the table setting and you move the dinner plate. But yeah, that would be changing the dinner plate. changing the play setting. But on its own, the dinner plate doesn't change when you're just moving a different position. yeah. So maybe this is like the b is the dinner plate and you're rotated or something like that. Yeah. On the plate setting.

But I am not sure if this, I'm not sure if this is gonna, what were you gonna, what were you gonna say? what I was gonna say, I could probably try to break this out even further, but, what were you gonna say? The original idea. Okay. the original thought is, if I wanna take this, rotation and apply it to the child objects, how do I make sure that it does?

that's a, that's B, that's C, that's D Actually, I may have misplaced these, but I think it's, I got it right. That's C. Yeah. Yeah. That's supposed to be C How do I make sure it does that instead of doing, yeah. A I think we have that. I don't individually rotate, but the, entire thing because you're rotating the entire reference frame of the whole object. They all come with it. There's nothing different. Yeah.

you're not rotating each of the features on its own. Like everything that comes in here, same. You're just rotating the grid that you have for the object. The reference frame. Remember the way we did this is that the parent, the parent reference frame, parent home represents the whole thing and the child object is attending to these features at different points in time. If you rotated the whole thing, the child would also be seeing them rotated. It would be tending the B and the A and the C and the D in a rotated position. And nothing's really changed. They're both rotated and the mechanism is, it calculates the relative orientation of the child and the parent. That was this complicated thing that you and I worked out. And so the model says, I don't care if every rotates together as long as the A and the parent have the relative correct of relative orientation. It's the same. Nothing's really changed. And if I know, and if I know b, if I know the, if I know the parent object like this one, and I know its orientation, I can then calculate what the orientation of the trial audit is. Uhhuh. So that's part of the compositional model. Optic model. Okay. That's already in there. I have a question. The A, B, C, the example there, the one that Scott gave and he rotated the whole thing, how would we learn that change in orientation of, in, in the brain? Which change? The one that I being Yeah. If it's not a behavior of that object, if it's rotating this whole, why am I, what am I learning? What am I learning? that's the, whole object rotated. it's part of the model. I don't have to learn it. I just apply new orientation to the whole thing you just sent on, different orientation. Different orientation here. So all of the, all of everything is getting rotated before it even go, goes into the column. So you just keep making the correct predictions in the objects reference frame, but it's not changing in the object frame. No, the object reference frame is the same. It's just changing the reference frame of the sensor. It says of the sensor, but it doesn't have to be compositional. It doesn't have to be part of something else. It is compositional. the object is compositional, but everything rotates. Everything. No, it doesn't have to be a child of something else so that we can represent this. If, it's a child of something else, then you're, then we're learning a behavior, which is what we're just talking about. If, again, it's not, I don't to learn this and the different orientations, I don't change my model of anything. If the model is a table with a cup on it, and every time I come into this room, there's this cup on the table, this is part, this is a child object of the table. And now I say, oh, by pushing a button, this rotates like this, and it goes back like that. That's now a behavior of the table. All right? And the point is, I don't wanna learn a new object. I don't wanna learn a new model for the table. I don't wanna learn a new model for the cup. I wanna somehow saying the change, the, cup has now changed relative to the table and it's back again. So we would learn this in six B. Then I guess the, orientation with respect to the sensor, what I'm, if it's not, if it's not, if it's not a, part of the, table behavior, what I'm suggesting, if this occur, the thing that we just worked on, the board is saying, okay, two objects, table and cup. And, they're in this relationship and we were working through the idea, what if, I didn't even know this was a cup. It just looked like. Just part of the table. And then one day I say, whoa, look at that part of the table changed. I would, this would be all these change detectors here relative to the table. And, that's what we're dealing with. and now we're saying, okay, we were saying that when you do this, maybe it breaks out this part of the table model. And it's saying, okay, the table's still here, but part of the table has rotated, just append to this part of it, the part that rotated, ignore the rest of it. But I can still use my table model to which included the cup. This is a weird example, it's as if I know where I already know what this looks like. It's a good part of the table. It's part of the table now. It's like this, okay, that part of it. It's like I'm gonna invoke the model that I had before of this thing. It's as if the whole table rotated. But now I have two objects. I have the current object, which is the table, and now I've broken out a part of the table object as a separate little temporary right thing. And said, just imagine that part of it rotated now relative to the original thing. And because we have to be able to make all the predict correct predictions here, right? I can't really, I can't see that. I can't turn this and go, oh, that's something totally new. I never saw I, what the hell's that? It's oh, I knew what it's part of this table. And now it drug rotate. I still know what it is, but it's just separated from the table. It's got its own. It's, instantiating itself independently of the table, even though it was learned as part of the table.

At this point in time, the reference frame for the cup and the reference frame for the table are essentially one's in, one's in the parent column and one's in the child column. And the child column, it's the same reference frame as original table. In the parent column, it's the table. But in the child column, I've rotated the table and only attended to part of it. I know it's weird, but that, so we did the whole thing and then just all big. Okay. I'm just, we're walking as always, we're walking ourselves into deductive logic saying it must do this, as strange as it may seem. And then after a while you go, okay, that's not so bad. So it's not that we create a, separate morphology model of the cup. Not right away. Certainly not right away. Is it possible that we are copying? Can we copy the whole model into a lower level? I You can't, there's no in the brain, there's no copying. Yeah.

it's just that's not been a long time really yet. But it is. There is no copying. You can associate two things, but there's no way, there's no way of taking knowledge of what a couple looks like in one column and transferring to another column. they can learn simultaneously. You can be association, learning. You're various things you can do. but you can't like just take all this knowledge and it's not like we have data, buses and memory. You can copy and transfer it someplace else. It doesn't. Neurons don't do that.

which is good 'cause it's really quick and efficient, right? The brain figure how this really efficiently, this seems like a good start.

what are we gonna call this? We call this like a forking, I don't know. what is it? It's, it requires having two models of the object to begin with. It requires that there's a, that there's a parent object model and trial object model, of the same thing.

And so it's almost like the lower level model doesn't even need to learn the behavior. It's just a No, it doesn't. So it's basically select selectively applying a behavior to Theology of an option. I wonder, if that's the way to think about it. All the time.

Meaning behaviors like composition requires two levels to hierarchical levels.

and this would even work in the primary sensory region, Spotify. 'cause you could think of features coming from the retina. It's a very simple child. And, so behavior says, Wouldn't, this wouldn't apply to color changes. I don't think this really only applies to, definitely this applies to morphology changes like the staple of opening and closing. It definitely applies to all situations like that.

Does it apply to all being changes? What track light? The red, yellow, green.

we assume the whole traffic light changed color and you're just attending to the No, I'm just saying, do I have to have a child object? It's, there's no new orientation. There's it, is just some feature at the same location has changed.

This, two level thing would be, it would be necessary when, one part of an object moves relative to another part of an object, whether it's moving in direct one direction or changing orientation. If one part moves relative to the whole, then I need this two, two tier solution. If I'm just changing the color of something, it's not clear to me. I need to, to I was a objection. Yeah. I, don't think I need a separate child about it. Oh, one other thing, it's, I don't think he would actually need to send up a behavior Id. I didn't write that. You wrote that. Yeah, I know. I'm here. Just wanna be clear questioning what I wrote. Okay.

because these predictions that, what, whatever the model that recognizes the behavior of the stapler opening, needs to send down here is the, orientation of that model at a specific location. So that's, sorry, really contact words for it yet, but basically we have the stapler model as local changes.

Wouldn't this model, instead of having to tell this one, oh, I'm seeing the stapler behavior, it would have to tell it, alright, you should be translating, you should expect the stapler to be in a new location which is displaced in this way. say it again.

that, this, model, recognizes the stapler behavior, I assume. Yeah. But stapler, recognizing the stapler behavior is really only for the purpose of this model module itself, for it to know what changes to expect at the next time step. But then it would only need to send the changes expects down to this one to tell it. In which orientation it should, expect the stapler to be? it's, I think it has to be more than changes because Yeah, imagine if I, the staplers opening and it went through some occlusions and now it reappeared. I would Yeah. That this one can do all of that. Sure. I'm just wondering, it sounds like this one actually doesn't even need to recognize the behavior of the stapler. No, it doesn't. So it, all it needs to know is how it should orient the stapler model. It should say, all it needs to know is it says, so that means this one never needs to communicate the behavior id. Oh, it doesn't hold down. No, it doesn't have to be the behavior id down. I thought you meant up or upward. I went down. But I don't know if it needs to communicate up either. I this guy, the lower model at the moment, just the first time we do this, doesn't need, it has no idea that the behavior's occurring. it has a signal that says, pay attention to this subset of the model and, and it should be a dislocation. Disorientation. Yeah. And now make, no, it just basically it says it, it says at this location in your object mo model, you should wait. It says, it just rotate and you should expect it just rotates it. And it says, okay.

you're rotated, you're bounded. Don't go out of your bounds.

here's your, here's where you're observing it and go at it. As the sensor moves, it'll be automatically changed into the, local orientation. So as I move my sensor up here, it'll move, it'll be equivalent. It'll, just automatically move to correct direction down here then.

You follow what I'm saying? Yeah, I'm, yeah. I was just having another, okay. So basically the only thing it needs to know is, here's where you should be observing this object at this orientation to the sensor. And there's a bound, don't go outside of the bound or, yeah. And I, guess on the point of the bound, I wonder if it has to be like a, it defines an area or if it's more like this location by location associations. 'cause if you have a behavior that's not just like a solid part of the object moving like that, but more like a string that kind of swings like this, it's, it would expect different orientations of the object on at different locations. So maybe that's a bit too complex of an example. You lost me on it, but it's a good one.

maybe, let's the, words you said, maybe it doesn't have to convey the balance. Yeah. What I do know in the upper model is I know where changes are occurring at this moment in time. Another moment in time. I know the other changes and another moment in time, other change. So I, so that's what I know.

does that have to be transmitted down below? Not necessarily. All I need to do is to say, if the upper column could determine, okay, right now I need you to break off a piece at some orientation and a moment later says, no, I don't want you to do that. Pay no attention to that. You know what I'm saying? Yeah. It doesn't have to convey the whole bounds. It just has to, it, it just has to tell this one, you're here and you should expect this to be on the stapler at this orientation. It basically says, I'm telling you the location and the orientation to the center. Yeah, exactly. And it doesn't actually have to tell anything about other points. But somebody has to know not to go beyond this. And so if I went beyond that up here, I would have to stop telling this to do that. Yeah. Which this one would know because it has the model of the behavior so it knows. So for example, if I slip back onto the base of the stapler, then there's no change in then ba basically go back to the original orientation and that location. Yeah. and then we're in sync again, with the original model.

so yeah, I guess I, I don't, at least we don't have to communicate. No, I need to communicate. that's, it was a separate question about, Now that I have, basically I have a model where it's first, it exists in two bubbles. And now, but this can answer something too. we say that every column gets sensory input. every column gets motor input. It's not clear. And I don't think every column gets sensory input. Not every column gets input from V one or from the retina, right? So it may be that the final predictions might always occur at the lowest level, but we still get input from the retina. 'cause there's a motion input, right? It's like maybe the, maybe only the par is projected to V one and bang predicted V one two. So my point of this is maybe when I learn an object, I'm always making my predictions, the sensory predictions at the lowest level and making more conceptual predictions at the next level. Yeah. So you would learn optic behaviors higher up in hierarchy, maybe even in places where we only get movement info input and we would then apply them to lower level models that have learned the morphology model.

I, maybe I think what we're saying, just let's start in a very small thing. Maybe this is what you just said. Maybe you just said this. I have to assume that if I have to say, if I wanna learn some behaviors of, that change the morphology to two levels, these two odd things have to learn the same object. Or they have to form models of the same object. They're not be identical, but they have to at least be modeling the same thing in some ways. And then the higher level, the predictions always occur at the first one. That's where the sensory predictions occur. The second one makes predictions of what the first models, this V one V two predicts what V one's gonna tell it. But if I want to say what's, the actual century input's gonna be? It has to come from V two to V one to layer four. This makes sense. and, yes, so the, upper level can't make a sensory prediction. It doesn't Go ahead. I don't making all this stuff up. So this makes sense. The upper level doesn't get sensory predictions. that's, that was this, that's the second. Yes. We do know it gets input from the retina, so that's confusing. Oh, but what if it was only the Magnus input, not the prophecy output.

and just say what if it was, it would still work, but just all the predictions would occur down below. And that's gonna be necessary for behaviors. 'cause we just said if the object is changing its morphology, we need a lower level to make the correct predictions, based on the behavior up here. So the behaviors wanted here and then the actual predictions occur down here. So that's really nice. Really nice. Of doing it now that we tease it apart into two learning modules, can we fit it back into one? I feel like why the fact that the lower one doesn't need the behavior model and the, it just seems I think it, does.

it just seems like this. I can, I, I think, the way I've come to think of it, V one doesn't get input from other learning modules. It just gets it from the sensor, right? So let's say that when I think about child objects, this is how I think about it. What are the child objects? The V one, those are the primitives that can come from the retina, whatever the retina can represent, which is not very much, but something, right? It is. We know what it gets. It gets a bunch of, it gets a bunch of centers surround bits, and, it learns what the common patterns of those center surround bits are, whatever that is. It could be little wiggles and curves. This is brutal housing's work where they did like all these, they've shown that cells in V one aren't just edge detectors. There's ones that we onto to like grids and checkerboards, and it's basically what, are the different primitive patterns that the, that can be extracted from the retinal input and, so that those are the child objects and I can't do this mechanism we just talked about. And by just I can't change the orientation of the thing coming from the retina other than just the, coordinating Janet.

Okay.

I'm just saying the, bottom one, we can think of it doing all the same things as the upper ones. It's just got an impoverished child, if that makes sense. Yeah. I guess I'm just wondering if this projection, like from the behavior model of this one and where it would expect, like, how it would need to rotate that if this projection could be within one column We have the behavior model. Okay. That tells here how to rotate the input. So what that, would require. Oh, I see. Oh, I see what you're saying. I would, oh, I see what you're saying.

we're not, since we're not re-anchoring, would be dynamically changing the orientation of a set of the model here. Yeah. Oh, I like that.

I like that. That makes more sense. I feel like it would be cleaner.

although I, although the idea that this is a mechanism from breaking out child objects is still a good idea. Yeah. I still think this is a great mechanism. Seems like I still need that somehow, but maybe not. Maybe not. Maybe I can do all the mechanism you just said. No, I, like this mechanism. But maybe we don't have to, maybe we can do it just what you just said. Maybe we can do all in one column. Yeah. So what this means is that the original Wait, here's the problem that as I move through the original stapler space, I have to know my orientation of the, of the sensor to the big stapler. Now the top of the stapler starts moving. So now my orientation to the stapler is changed, but only for that section.

And then if I go back to another section staple, I have to change the orientation back again. It's not a problem. It just means the dynamic. I have to be constantly switching back and forth between these, what We have to do the same thing. You have to do the same thing there anyway. I, you're right. You have to do the same thing either way. But now we're asking one column to do that. that'd be, that's pretty interesting. I, like it. I, if we can make that work, I like it better.

It's a little confusing to me. Imagine I have, imagine my base stapler here, and now I'm going to, try to make some predictions about it here and make some predictions about it here and make some predictions about it here.

and so this is my original object model, and so I have to go from here to there. So oh, this location is, no, it's, oh, there's another problem too. Yeah.

it's hard to imagine how you do all that basically. It may not, may be hard to do here too. I don't, having a model of the optic behavior, like how, like this little, like how little movements or changes Yeah. Occur over time doesn't, directly tell us at what orientation you should expect the stapler top to be in. just because I am at this point in time in my change sequence doesn't directly provide me with a location orientation, for the stapler. I know that there's a, we have to, we have a problem going from points of change to projecting project orientation, whatever, just Yeah. To that. Yeah. But it seems like there has to be a solution to that.

I'm, not sure we know what the right representation for the change is. We just know that it, the model's gonna contain points of change, what it's actually representing at those points of change. I don't know. we're imagining it's some movement factor, which makes sense. but maybe it isn't that. but let's, and if it is that, how do we go from that to magically knowing, I don't know what the new orientation for the whole staple put or something like that. Yeah. You have an idea? No. Yeah. just trying to I have an idea. Go ahead. Go ahead. Go ahead. we have to solve the orientation problem anyway. That is, imagine you're looking at the whole retina and you rotate your head. we've hypothesized that there are flow beds out there that know, that can detect that pattern different from, eyes moving left and right. It's a sort of circular motion.

This, we, know that our theory says if I rotate in my head, I know my orientation has changed via the movement. Yeah. And therefore I can compensate for it. So we've already hypothesized that there are a bunch of flow bits in the, retina that are translated into changing of orientation of the sensor of the sensors to the object or, But it's, changing orientation between two things.

whatever that mechanism, wouldn't it also apply to the top of the stapler? If I was just looking at the top of the stapler, wouldn't I, wouldn't those same flow bits be able to determine the orientation of the stapler is changing? It's the same problem. it's not as global of flow is. No, but it's a local flow. Yeah. so you're saying, alright, so we have the retina, which detects global flow, which is movement of the sensor. We would have more local flow, which would detect movement of the stapler top. And then we would have very local flow, which is, I'm a bit very local. It, is, imagine this, if I had a little, a little feature that was a, like an emblem, like an a and it started rotating. I, i immediate I could, that the flow bits would be very strong right there, yeah, everything moving. If I have, I'm looking at an edge of the stapler going through here and is moving this way, my flow bits would be very marginal. Because, this, flow here and this flow here would be almost identical. This would be slightly more small, slightly smaller, but that would be, that motion. So locally, I can't tell this, I can't see this, but I still could have a, I could have all the flow bits relative to that stapler top. And if relative, if all the four bits relative to the stapler top we're not sufficient to turn change in orientation, I'd probably have trouble recognizing it initially.

maybe I'm just saying that can we use the same mechanism we use for, assume that exists for determining change in orientation, of the whole world to the sensor cannot be used here, the exact same mechanism. And I can imagine some situations where it would be very difficult, other situations would be very clear, that it would work. When you say the same mechanism, you mean the same mechanism to detect the change in orientation or to compensate for it? Detect it.

yeah, I think it has to be the same mechanism. It's just, I guess the question of if this, this column that models the stapler, if that would. Like how local is the flow that it's detecting? Is it, actually detecting quite large objects moving, like the whole stapler top or is it detecting very small parts of the stapler? imagine you have a field of flow bits at the bottom of the stapler here though those flow bits would be very much more clear, changing in orientation because they'd be real circular up at the end here locally. They wouldn't be, 'cause locally it would just, the end's kind of moving almost the same rate. So more, look, this locally here would look like it's just moving here. It would be obvious that it's, I'm, just, that would be true if I'm looking at the whole visual field, how do I know the whole visual field is rotating? Because probably there's some point in the visual field where, where I'm looking, where the flow bits are pretty close together and there's obviously I can, detect it locally, but from the distance I, it might be hard to detect it, Yeah. So it's a question where, how big of an area do you pull the flow over to get it in this case of the stapler, we can't pull over anything larger than the stapler top. The sensor could, but, there's nothing else moving relative. It's, those are the only things that are moving. Yeah. So if it's way too large, just wouldn't say that something's moving. But basically. When we first extracted these, movement vectors. Yeah. I at least assumed those are extracted from very local flow in the patch. extracting that there's an orientation is because I'm sensing something like this, but that, that works in some places, but that's someplace that works, someplace that doesn't work. If it's really far out on the staple arm, it's not Local information is really not. Yeah. Again, and then the question is like, where do we have that large patch that pools for and how big is that patch? I, would assume it depends on where this column gets input from. If it gets input from a large low resolution patch or a small high resolution, or maybe from a bunch of columns looking together. Yeah. But I would say, yeah. Yeah, it's all the same mechanism. It just really depends on where the column gets its input from.

Imagine a whole bunch of columns together and and some are looking at the, one end and some working at the other that together they have clear, they clearly have an infiltration to determine rotation.

we've just got a bunch of just one column looking at the end or a couple columns, just like at the end. I won't have a, I won't be able to determine that it's, not movement and it is not just translation, but it's, rotating, I think maybe the solution, Maybe requires it as I really raise the staple top, I actually look at the hinge area and say, yeah, it's rotating, I see it there. Something like that.

So I apply that rotation to every part of the object somehow. I dunno, I think, yeah, I think it would be very hard to have that as a necessary condition for each column that it gets, like for all the objects and models, it gets exactly the right size input to determine, no, I'm saying this is how multiple columns would collaborate somehow. Oh, where would you mean? it would be enough if I had a single column and I brought it down to the hinge and say, okay, what's happening there is rotating. Yeah. And then I could go and say, okay, where's that one? And then go down. But I don't, I might be able to learn it that way, but it is really ridiculously hard.

I think it, I might be able to get to work, but from a practical point of view, you might wanna have multiple columns looking at this. And I think, again, I can always get it to work with a single column, but we're getting to the point when these mo behaviors are involved, I don't have time to sample everything as the behavior's unfolding. So I can only sample different points in space time of the behavior and, that may be really difficult to build up a behavior. It might be very hard to learn it. Whereas if I could look at all the pieces at once. Every, somehow these guys are coordinating, you say, yeah, okay. Together, we, together we can help train each other somehow. But I don't see how that solves this problem. it helps with like practical inference. which, which problem? Let's be explicit. The problem of knowing how to rotate the model of the state model of the staple top, just based on flow. So you mean why, not? If I, if my, I've already got a mechanism to detect rotation from flow. I have to have that. The whole basic model require of that. It has to come from the, yeah, so I can, if I can do it over the whole retina, why can't I do it over a portion of the retina for loading? no. I'm just in general, the information is there, right? Yeah. Information is there. We didn't explain how we did it in the whole retina. We just assumed we did it somehow. what's going into the cortex? What's goes into the cortex is a bunch of, as far as we know, purely center surround changes. None of those are sufficient to detect anything else. Other than that, at this point, there was an edge moving this way. Somehow the cortex is able to, take all that information or the thalamus or somebody take all that information and understand how to, that, oh, this whole, the whole world is the whole sensor is rotating, or my patch is rotating. Even though my little section here doesn't look like it's rotating, I know it's rotating. And, so there's already a collective, somebody determining that there's an orientation change and it's, and even if I locally can't observe it, so I'm, just assuming the same mechanism here without saying what it is. but it's now looking at just a portion of them that you don't buy them.

maybe just, I don't know how, you would coordinate that if every co column just gets input from a patch, can't pick which patch, it just gets that input. let's be careful. what do we know where's can have a black one? And the, here's what we know. this is general neuroscience dogma. You have a column, the cells down in the lower parts of the column that are directly sensitive, and they prefer very large receptive fields. So these cells somehow respond best when the entire, there's a huge thing moving like this. And I read that it's even, they respond best, even the random patterns. And yet Neil couldn't find that paper, but I'm sure. 99 sure. I read, I sometimes wrong, but I'm pretty sure I read that. it's like, how would I come up with it?

but let's, even if it's not, it's basically saying somehow already in a local column we have these things which are detecting some large thing that's moving in, something like that.

and they respond less as you get closer and closer, but they still, they prefer bigger. And we know that up here they prefer shorter ones. Okay. So my thinking is if they can detect that somehow, I don't know how they do that. They must be getting converging. They inputs from Magnus cellular cells. They must be getting a whole bunch of mag cellular clo cells coming in and somehow figures out that they're also are gonna get a whole bunch of mag cells and figure out that, the orientation is changing. Or maybe someone, cor Subc cortically is doing that and sending an orientation segment. I don't know. That could be true. but as far as we know from the retina directly, it's all just center surround cells. So maybe someone else is processing that. Like the only thing I, the only thing else that could process it would be aper. So the th but my assumption has always been the cortex and somehow it, it could do whatever it, however, it does this, it takes that same field and figures out that.

So now we're just asking it to do the same thing over a smaller part of the visual field. We're limiting it to whatever the staple of top is.

I would say it could, that I just, it's just hard to imagine how it would adjust that size always to the object that's, or the behavior it's currently recognizing or, you mean it would you mean how to pick the bounds? Yeah. Or otherwise it could only learn objects. How about certain size? How, about this?

at any point in time the model predicts changes at multiple points, right? It is very rarely just one point. It would be, there's big cloud of points essentially outlining the morphology of the upper part of the staple.

What if, you could, in some sense say, imagine I was looking at that. I'm trying to say, what if I only look at the beha? I only look at, I'm only looking at the movement vectors with those points and the velocity. Is that not sufficient to do what we wanna do? what if I feed that into the mechanism for determining motion and I feed only those points into the mechanism for determining motion? Would it, get the right answer? saying instead of the whole world rotating, I'm just looking at some sample points of the world and see if they're rotate. Same mechanism, sub sampling. It seems like almost that's what you have to do.

Now. A single column would have trouble doing this because a single column can only look at one point at a time.

I don't, know how that works. Wait, can you just repeat again where that would be happening? The sub sampling or maybe you did. Okay. There's a, imagine the retina and there's these mag cells all around retina. Each one detects change. I take that massive cells and and I bring 'em into a and, essentially I can look at those cells and from those cells I can tell if the, system's orient Changing orientation. Yeah.

I'm still in abstract form right now. So now we don't wanna look at the entire retina. We wanna look at the subset of the retina where, the change bits are happening in the behavioral model. So rise the moment I have a bunch of change bits in my behavioral model. I look at the changes occurring on those change bits, which is it's like a, it's like a pattern that is, it's, I'm not looking at all the bits in the random, I'm just looking at bits that are aligned with those change bits in the, staple top. So those, things on the staple top are move the staple tops moving. So I've changed bits there and I want to calculate the same calculation, but only using that subset of bits. I'm sub sampling the entire space. Only look at those bits and say, can I determine the orientation change or movement by looking at a subsample of all the bits, only the ones that aligned with the staple, wherever change was occurring. And who would, communicate that subset? Where would the information is there? a, the problem is a single column would've trouble doing that. Just like a single column.

You could imagine all of those ganglion cells projecting on the single column to do global orientation change or global movement of the audit. I can imagine that, but I can't imagine single column selectively looking at those different bits based on parts of the model. It is not even looking at, the model will tell me, if I go to this point, I'll see this change. If I go to that point, I'll see this change, but I'm only looking at one point. So I don't all I know right now is this change occurring right here. 'cause that's all I'm looking at. But the other columns nearby would be looking at all those other points. So there's a constellation of columns that are looking at the whole retina, and somehow they collectively could say, oh, the parts that we're attending to, it almost seems like now we're moving the behavior model into this pooling mechanism. Like not pulling or selectively a multiple column. Yeah. It seems like at that point we don't even need the behavior. yeah, we do.

How do we do this pooling without having a reference? I didn't use the word pooling. Why are you saying pooling? But where's the pooling? Oh, you didn't, you say, did I didn't mean to you selectively picking. Oh, maybe I sloppily said pulling it.

or Yeah, at least applying a ma arbitrarily shaped mask onto a larger field of you. Imagine, the, with the fall, imagine, we have a retina here and you got all these bits out there. I'm not gonna draw 'em all. These are the p magnet side bits. At any point in time, any translation of the eye to the world would result in every one of these bits moving something, right? Either all moving the same direction, which would be eye moving, or there'd be some sort of rotational pattern here. which means the eye is changing, and, eye is rotating. The eye is rotating. What's that? The world's rotating, your head's rotating. Your eye is rotating relative to the world. That's what I meant head like this. You have not, the, not the eyes wrong me.

All right. It's clear that the brain is able to figure what this is and bring it to the cortex. That's part of the theory.

clearly a single column which is attending to one spot here or here, that attentional spot isn't sufficient. But it could be that, that, that column is also getting, although this is where the mag, the parvocellular cells are coming from, it could be saying, oh, the magnet cellular cells are, I'm getting all these magnet cellular cells from a much larger area. And they're, that would be the evidence in some sense that, oh, yeah, okay. I'm only gonna detect a feature here, but I'm detecting motion everywhere over a much larger area, and therefore I can know how my patch is moving relative to the world, because somehow I'm getting all this information. I bring it into some big spatial pooler. These receptor fields are quite large out here. a, single, magnocellular cell represents a large area out here, or single ide still, it's a very small area, so I have, I don't have to have, I don't have to have a million axons coming in. Maybe a thousand or 2000 sufficient to do the whole thing. And this column could, because say, given that input, which is wide, I can determine how things are changing. and Okay. Now we're saying, now we're, what we're trying to do is, this column is we don't want, we want, we wanna do the same thing, but we're saying, oh, we're on a stapler top.

So only look at the magnets outta the cells in the stapler top.

because these other ones aren't changing. These ones now aren't, the only ones that are changing are in here.

isn't the column sent, signals back to the TRN, which inhibits parts of the column. Good, We don't know what the TRN does, but that's a good hypothesis that, that the TRN, which is presumed to be related to attentional mechanisms, could play a role in that.

And in my mind, attention's always been a couple of things. It's been to restrict your area of input and, And to, try to recognize what's only, what's in that area of your mind. It's like latching onto, there's a, anchoring to that whatever feature, whatever object is in there. And I guess if it's coming from six a it or, if it knows what object is and what poses it can, it is theoretically able to restrict, what the tus sense at. Yeah. But, in our current model there, it wouldn't, how would I tell it to restrict it to the stapler top? it knows what the object is. It knows what it's supposed to look like and what, the s doesn't know, but I can't tell, but the column knows well, but the column wouldn't be able to take that associative model between features and locations and know how that would actually project onto the retina. what could happen is you have all these columns. Imagine you have all these columns here, you have all these columns that are looking at the stapler top, right? Each one, it's independently looking at the staple top. Each one knows where it is on the staple top. Each one knows that, that I'm gonna change thing here. If something's going on as a whole, they could project. Now, this is now a different picture. This is now the thalamus here.

let, forget about asbestos. It's no longer retina me.

there's this, TRN. Each one of these column has been, each one says I'm on an object at some location. I don't, know why it's telling you that, but it's possible. And these are, all, these connections are generally topographically mapped, I believe. I think they're, yeah.

could that, I don't know. What's the telling us that against, what was I say? Just restrict where, where it might say, cut out the input to from anywhere else. Yeah. Basically, because it's a loop, it says from six A goes to the TRN, which inhibits the, LG n from sending it back to the sending, something like that. So it says there is pass information from the, to these guys, but don't pass any other information. So it's like saying the world is occluded right now. All you can see is the top of the stapler. So the attentions on this, right? It's like saying, I, did this little, thought experimental. I had a picture of a street scene, and I had everyone stare at it. I, and, they couldn't see the screen. I flashed in front of em, just like a flashed inference. And, oh wait, that was the street. Same thing. And then I say, okay, what was at this location down here? I have no idea. So then I put a little rectangle. I said, you were at the fixate on the center dot. So they fixated on CenterPoint. Everyone was staying at CenterPoint. And then there's a little rectangle of that. And he said, now don't get your eyes off that center point, but attend to that spot. Oh, I did this from eye test. Did you? Yeah. Like they make you stare at a red, do in the middle and then click whenever you see something in the periphery. They're changing. Usually they have to be changing. Yeah. Yeah. It's not there. this is different. I said attend to this rectangle. Okay. And but don't look at the rectangle. But mentally attend to it. Okay. And then I flashed the exact same image, the exact same time and they said, oh, there was a dog lying down there. Now they had no idea there was a dog in the first image. 'cause they were attending here and they saw the whole scene. They didn't get any of the details. Then I did attention to another little thing and then just said, back again on the, street sign, they couldn't see, you wouldn't see any of that until you attend to. So my point with this was, if you have this ability to, even while you're looking here, isolate a subset, and if you attend to that subset, you actually don't see the rest of it. You just see what's in there. It's it was like, oh, there's a dog there. Yeah. Everyone saw the dog, but you didn't see the dog in the beginning. There's too much. You normally, you have to attend to these different spots moving your eyes around. So you could attend, you can just attend to one thing at a time, and then you see the thing. And that's basically what you're saying here. I, in some sense, I was telling the TRN, don't pass that information from anywhere else. Just from those locations. And then we'll do inference. And the difference is here, what object you want to separate out because you are already the, that signal's coming from the column so it knows, oh, I wanna be on that stapler. But in, in the experiment you're saying you, you're fixating on the angle. You don't know, here the dog is here, I actually, what the thing might actually have is there's changes at these locations, which is defining a boundary of something. And I'm not actually telling the TRN what it is. I don't think TRN can't know. It doesn't have enough neurons there. So Yes, the bounds for the column knows where the bounds are. 'cause the column knows the object. actually right now, the bounds are determine, we're trying to determine a subset of the object. All we know right now is that this, these read, these dots are changing that, that's the information we have right now. You and I can say, oh, that's the top, that's the stapler. But at that point, the signal for change is not stapler. The signal for change is there's it's an edge is not a stapler. A set of ages is a stapler. The change is not behavior. The set of changes is a behavior. But the set of changes would dictate a morphological region in some sense. theory. and, and so you're basically, this is what I did with the dog. I'm, it's saying, attend to this region here, which is bounded by changes.

And then, and then what you would infer is, oh, there's a stapler there at some orientation. Something like that. All right. I'm gonna have to go on the first line. oh yeah, that's fine. Yeah, I can still talk, but no, you wanna do one more thing before I go and then do, you have something? What? What? No, I just goes on forever. Oh yeah. Chance doesn't stop haven. You notice that? Okay. Yeah, I just, by the way, I just think I was thinking this morning, I'm thinking like through that weekly wrap up, all this within year, and that may not seem fast to year, but that seems super fast to me. That's like unbelievable. We could actually do, I think these things are accelerated. Me actually took a huge step forward this weekend, this last week. Yeah. It's if a third of the cortex is doing modeling of, morphology, we just added another third of, and then, there's, a few more things we have to do. We have to do behaviors all kind. oh man, that was a big, we're gonna do more. I wanna accelerate. 'cause you're getting close to the end. You don't wanna stop. You wanna get going. It's oh man, here, go ahead.

just, help me. I don't have any ideas. It just, I'm just trying to, I don't know why brain message. You can text it like I do. Okay. Okay. I'm just kidding. so just think about things in a get away and I just need some help translating.

so in this simple column, we have some change section, bound to location orientation. So I'm thinking about something for whatever attribute there is some change. Is dependent on location. it's observed that location. Yeah. yeah. It's observed that location.

We're inferring an object and you've, I'm assuming we've already inferring the object, like you're looking at the staple and it starts to move. Is that right? Is that what you're saying?

I'm just saying there's a change that's dependent on location and orientation. But it makes a big difference if we know what we're looking at or if it's just random stuff. okay. so okay, go ahead. There's, I'll let you finish it. Keep going. I'm sorry. That adds some loca, location orientation and let's say also time, it's not, there's a change. Part of the change could be orientation, right? That Yeah, there's a change. We'll start with the saying there's a change at some location. Okay. It may not have an orientation. It could be color, it could be, we don't know. Orientation is one of the possible changes. I, orientation of the sensor to, oh, there is an orientation, but that's not part of the Okay. Add an orientation in. Yeah. Okay. Yeah. So in the case of color, it doesn't matter what the orientation to the sensor is 'cause it's just gonna be color. I'm sorry, I'm gonna interrupt you again. Okay. Yeah. Is the location in object space? Yes. Then you don't have to mention orientation 'cause we've already taken care of that. Okay. the orientation was typically referred to as the orientation of the sensor to the object. If we are locked onto the object, then the, we don't think about orientation. It's Okay. and then the problem that we're trying to predict, it's if this change, how do we know that the color doesn't change? So we, so this orientation, in this, X, Y, Z, so the change, like basically we want to know the set of points that is in that is a subset of the object that is changing that we do know that was observed. Yeah. That's an observation. But we want to somehow know that this changing. Does that affect other attributes like color or whatever's? we know that some changes occurred at some locations. Yes. And we will note what those changes are. So if it was a change in color, it would be in the little color mini columns. Yeah. If it was a change in, something else, it would be other mini columns. So again, we've observed either a color change or not color change. Yeah. Yeah. We've observed either a edge moving or not moving. Okay. And the, problem that we want to find is how do we know what this sub region is? Is that right? Like in this kind of way of thinking? Okay. Or that is the problem. Although I think. By the very nature of our definition, how system's working, it's detecting changes. Yes. Whether it's changes in color or changes in Yes. Whatever changes. Yeah. And those changes all occur at some location. Yes. then, we would by definition the points in this behavioral space, which is the points of all changes occurring at this point in time Yeah. Would define a morphology. I have to define some sort of morphology. even if the whole object changed color, at least the color changes on, it's occurring everywhere. Yeah.

so it is the very nature of detecting a change determines the subset of the object that we need to attend to, or we need to concern ourselves.

Yeah.

Does that make sense? Yeah. Yeah. Just trying to think if can do that. I don't how you do respond, but, and, the thing we're struggling with now is that a single column can't imagine all those changes at once. Yeah. Yeah. It, it doesn't, it, its model has all that stuff in it. The model says, oh, I look over here, look at it. But I can't do that in real time, fast enough to keep up with things.

It could do cast enough when we're living static on it. Yeah. Take it all the time in the world. Nothing's changing, but if things are changing, you just don't have time to go sample everything. you have time to make predictions about everything.

I don't have a solution. I'm just Okay. makes sense to me at least. Translation you described is correct. it's, that's the problem. Yeah. I feel like we're close to, I think we are close to a solution to object behaviors, what is the part that you would want to know more detail about right now?

yeah. I, feel like I haven't been able to really follow this idea of getting, but that's maybe also because I haven't had any more coffee, but, okay. I guess my biggest question, how do we figure out you, 'cause a single column cannot see everything. How do we know the sub region of the project? lemme just, lemme pop up to 50,000 people. Okay. We've always focused on a single column 'cause I believe single columns have to have models of incomplete models of object. So we have to understand a single column can understand everything it might have said. That doesn't mean that we can act, actually make practical systems with the single column we've shown here that, I felt this even when we're learning the morphology model, this idea that we gonna trace over the whole thing. Yeah, it works, but it's slow. And maybe you already got this working in mind, so you can look at it all at once with a bunch of columns and they it somehow seems like the columns have to inform each other. somehow I have to be able to learn things about a model that I've never observed, because it just feels like that I can't prove it, but it feels like the column can't observe everything. but it needs to learn models even though someone else observe something. So there has to be some communication. This becomes even more important when we're dealing with, behavior behaviors, because I don't have as much, I don't have as much time to do observations. Yeah. and there's, a larger space to observe it. It's four dimensional faith now out of three-dimensional space. It's got time. Yeah. and it's really hard for me to build up a concrete model that everything's moving. I can't see all the different changes that are occurring. I'm gonna miss things.

it seals this really clear that, multiple comms are gonna somehow get involved in both learning and inferring and predicting, and I don't know how that happens. yeah, I can very straightforwardly imagine voting on behaviors and inferring it quickly by doing that if we, but that's, I need more than voting. How do I train in a column? How do I learn something I haven't observed?

why would you not? You mean applying where in situation? It's, I, can't imagine every column, mean columns can vote. That means, but how do I learn a model? I have trouble. Magic. Every column can observe all parts of a model. It just seems and maybe it involves hierarchy. I don't know. It just seems like I, I don't have enough. Somehow models have to inform each other. They have to, I don't know how, why we assume that for the morphology models, what do we assume that the, that during learning the column has even all Yeah, but it has time to do it. We can take all the time in the world. It works. You said that it can work and I, think it can work and it can work sporadically in the behavioral models too. If I had a really fast in the staple of, and I could sample a hundred points a second and move around, I'd catch everything. Yeah. Yeah. Yeah. But don't work like that. Yeah. So theoretically, yes. Yeah. Or what you were saying, if we keep looping back the behavior, then Then I tell we have again and we do it, which I think we even have to do, even with multiple columns. we have to, like the first time a child seal all this thing move, it doesn't get well then it does it again. Does it again. Does it again, is looking at different parts. Yeah. Yeah. So maybe I don't, but I still feel like on the behavioral part, it just seems like I can't sample fast enough to be able to accurately build that model. In theory, yes, I could do it quickly or I could do it over and over again. And if I had a perfect time signal and I knew had a perfect space signal, I knew exactly where I was at every point in time, I could just sample over and over again and fill in all the pieces of my model. And I get it. it might take days or hours to sample over a complex behavior to get all the points. It doesn't seem like that. It feels, I'm not sure. Here's not, it's not like we don't have a lot of time to learn the behaviors of the world. there are not that many complex behaviors that we can actually model accurately. or at least we are not. Like when have you last learned in totally new behavior that you couldn't take an existing behavioral model and just apply it every time we work on neuroscience, trying learn things we've never done before. That's why it's so hard. But that's more that's still and you don't have to integrate over a large area of many different things moving together. in some sense it's like that, like I, I can imagine learning a, simple behavior of like color changing on an app. Oh, that's easy. That's easy. But and even conceptual behaviors seem easier to learn. Like it gives, you have, you've got a lot parts if you already, if you need. So we need to explain that if we've already done a lot of, all the components you're learning and we're just reassembling 'em in a slightly different way. Yes, but I, the problem we're doing right here, we're trying to learn concepts about the brain. And we don't even have the foundational concepts. It's like we're starting as a child, we don't even understand what neurons do and how this works and that works. And so we're trying to figure out these little basic components and rearrange 'em a bit and try to get 'em, rearrange them this way. And every time we read a paper, we're looking at pieces that are in some sub-sample of the bigger space. And you say, oh, can I recognize that from this subsample? No. Then I look at another one. Oh, can I recognize what's going on from this sub sample? No. Can recognize no. It's like you bang your head against them all over and over again, and eventually it works. I love those analogies. I think, I don't know, there's, have you ever seen this, this little trick? They, there's a bunch of circles on the screen and there's some like lines through the circles and you look at it and say, what is that? Nothing? And then a moment later, the lines move. All of 'em move and in different ways and immediately, you know what it is? They Oh, that's a person walking behind it. Yeah. Yeah. The orientations and the lines it, the orientations and the movements. the real trick is when you can see the lines and they're not moving. You don't know what the hell's going on. But as soon as they move. Then you just, and oh, that's a person back there, and now I'm looking to see a bunch of holes and there's a person Before that, you're just looking at a bunch of circles of lines. You don't see anything, and then it stops moving again. You can't see it. Yeah. And so it's this is some sense, like why I bring this up. This is a sub sampling of the behavioral mask for that ob for that behavior. You're seeing some pieces of the behavior part, some pieces of the motion, not all of them, but enough that when, that, when you actually see how they're moving, it's, it then pops into place. So it's not enough from a feature to recognize morphology, but the sub sampling, the motion space is sufficient to recognize the behavior and the object that's creating that behavior. yeah. So this would be re-anchoring in the behavior, reference space. it's just all of a sudden have enough. This is just essentially what I'm getting at here. This is like somehow all this happens instantaneously and obviously different columns are sampling these things. So they're vote, maybe they're just voting. Yeah. maybe they're just voting's, not just happening with voting. Maybe they're just voting. Maybe they're, that's all it is, but they're voting anytime they look at this kind of stuff. With NI, I don't love FMRI, it's not my favorite measurement technique, but they always point to what a higher cortical region, of course they do for structure, for motion kind of stuff, right? It's like we do have multiple tiers of higher, we, because we don't have just. But the problem is FM RI would not show this in a single column and lower it down, just no. You don't see much. it's, it, you can't, if you see something special pop out, you can't set. first FM I is a terrible measurement as you point out. And second of all, even it's the, null result from V one or something doesn't tell you that it's not happening there. It just says FMI can't. So now, we do have all these visual regions, right? What are the, what's the purpose of so, we talked about this, Scott, it clearly in the brain you learn things top down, top first, and then you can relearn and lower down. And, but that doesn't mean that knowledge isn't eventually down lower and you can't invis mean we couldn't learn it lower down to begin with. Yeah.

those are not incompatible hypothesis. Our hypothesis here are not incompatible with that idea that you're going at the top and move down. we just don't wanna focus on that because it's gonna be learned every, it's gonna be learned in columns everywhere. And so we, it doesn't matter. It's easier to think about the low down ones first. I don't it, I'm just going to lodge my, yeah. Prediction that certain things like this are gonna require hierarchy. Yeah. I'm not going to, so I disagree, put money on it, but I disagree. Like certain higher level features. I think when we get to like multimodal, knowledge built on a lot of previous knowledge, you're right. But it doesn't, the mechanisms themselves, I don't think require, we do not require to have a, that big hierarchy to understand the basics going on. Just two levels in a high. But this, example itself is, I thought that's what the disks and the lines, that doesn't require, that's just multiple columns. Vivian's not even off of voting yet. She says multiple columns are gonna vote. I thought you were suggesting that it were No, I'm, hierarchy. no. I said multiple columns. When you say not voting, when I think voting, I'm thinking flat. Other suggestion is that there's a new mechanism that's not even political as I No, I'm, not arguing strongly for this, but it feels like learning has to be spread. There has to be another horizontal spread besides voting Uhhuh. That's what I'm arguing about. It just seems but it doesn't necessarily have to be hierarchy. No, It doesn't have to be hierarchy at all. I wasn't thinking hierarchy. I was thinking purely, I have this paper, I can show it you 'cause I know it's on my laptop. But they show these motion. they, they can show these activity that are occurring. You activate a single column, and I mentioned this the other day, and then you, the, you like, they activate this sensory to a particular column and you can see the, information flows up and down. Yeah. And then it starts spreading horizontally. Yeah. Why is it voting? What's. These other comms didn't get any input. It's and there's a certain pattern in which they're like, this spread's here and then this spread's here. It's it could be just voting, but there's nothing to vote on in that case. 'cause the other comms aren't getting any input. So it's are they sharing, what are they sharing? Is it, or what is it? Is it invoking the similar, is an adjacent column saying, I have no input. And this guy says, I'm figuring out something. I'll just tell you what I learned. I'll activate the same thing on your column so you can learn the same thing. 'cause you're not getting input. So I'll train you on what I learned. So then you might get eight times fast to learn it or something like that. Yeah. And once you have a mechanism like that, then maybe other stuff is happening. I don't, I'm just throwing it out. It seems really complicated to do this one column in a practical way. I think we call this term, some people call it voting. I think I called it propagating. And then now it's a sharing of some information that's not voting. I, voting was always about inference. I'm argue, I'm not even arguing. I'm, yeah. I'm asking or talking about something that's always bothered me that may or may not be a real problem. Which is not about inference, but about learning. Yeah. that's hence why he's propagating. Propagating. But it is all about learning. It's not propagate. Yeah. it's not just propagating something. It's about, I think it's about training other comms on things they didn't observe themselves. Yeah. which would make things much faster. Yeah. If you could do it. There is an issue of hierarchy too. For example, like if I have two columns and, hierarchical and they have different receptive field areas, one's a subset of the other, I'm like, it'd be more like, would've been like this, like two receptive field areas. maybe there's something going on there too where, even though there's only input here, this guy looks at the whole thing and maybe it's, we do know it's gonna tell these guys that should be looking at this, that they would be inferring the same thing. I don't, know. It could be like two levels of a hierarchy. Maybe it's not even necessarily maybe, Vivian's playing my role, I wouldn't normally role argue that you don't need anything else, just one column sufficient.

and maybe some, maybe it is just, so if it's just one column, we don't, maybe we just don't know because we have multiple columns to begin with and just don't know how it is. So, let's say it's one column, what that means somehow we have to be able to explain how a single column Would, once it learns this model Yeah. Would be able to make predictions about where the stapler head is at different points in time. And the, and even though it's not tracking that end of the stapler To be able to say, oh yeah. At this point in time, it's not like I'm, at this point in time, I'm predicting a change here. It's oh, at this point in time, the top of the stapler should have moved goalie up to here. I'm gonna make that prediction. Yeah. And that was my point about using the, occlusion thing. Yeah. Because I can't track it through that. Yeah. There's no way anybody can track it through there. It's like I've lost track of it. Yeah. But somehow I still know where it's gonna be. Even if I wasn't observing it, that they're either voting on state id, they would be able to like a single column that's looking at something that's stationary and all of the, and there are a lot of other columns that are looking at the top of the stater that's moving. Even that column that's looking at the stationary part would know that the, top of the stater is moving because they're voting on state id. Wait, are we still voting on states? We just voting on object id? They, no, you could argue that you could vote on an instantaneous moment in a state trajectory. Yeah. Remember, states are now four dimensional vectors, and so at any point in time it has some sort of morphology to that state. And you might be able to vote on that, but I don't know about that.

All right. I gotta go maybe a little one out. You have the video. is it? Yeah, I'm talking about this. The, the points, like if it's static, then we don't, here I think it's a easy example. This is not what I was talking about. Oh. but this, someone showing this the other day in the, in the Yeah. Will was pulling that up. Yeah. He had that in there. No, this is a thing where you've got, should, get it posted though. go to I know what you're talking about. Here we go. I'm just fucking find it. Okay. What would I see, if I go to YouTube and say, what I.

Figure motion. Yeah. I dunno how many hours that was, but that happened.

Yeah. you'll get to start the next session with all the questions that you have and objective objections.