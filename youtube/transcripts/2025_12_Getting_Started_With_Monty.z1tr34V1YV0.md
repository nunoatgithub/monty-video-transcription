Hojae Lee: My name is Hojae. For my part, I'm gonna be giving a short presentation on how to run Monty, and also how to configure Monty, so that you can run your own experiments. I'm gonna just jump right into it. First, our code is available at github.com slash thousandbrainsproject slash tbp.monty. Our documentation is available at thousandbrainsproject.readme.io. After you get the code from the GitHub, I highly recommend you to go to the documentation site. It has additional information on how to set up a conda environment or a programming environment for where you can run Monty. It also has link to download the YCB dataset. So the YCB dataset consists of household objects that you have seen in previous presentations from Scott and Niels, like the mug, bowl, the bananas, etc. And finally, it also has a link to download some pre-trained Monty models, so that you can immediately start playing around with Monty. highly recommend these two websites to get started. And with that said, I'm going to actually switch over to a different window to actually show you what a Monty run looks like, and what you can expect when you actually run an experiment. So I'm going to switch over.

Alright, so hopefully, you are looking at my code editor. Just to orient you a little bit, on the left, what you see here is a YAML-based configuration file that specifies the parameters of an experiment, and I'll explain what that experiment is. On the right is my integrated terminal, and I'll be typing in some commands to run a monty-experiment, the experiment that we specified here. This is going to be a very quick experiment, doing object recognition. So what this is going to do is it's going to evaluate a pre-trained Monty model, to identify two objects from the YCB dataset. So simply, I specified a bowl and a mug. And the goal of Monty, is to move around the object, gather features at different locations, and try to come to a conclusion about which object it is sensing. So this pre-trained Monty model, has already learned, actually up to 77 objects. We're gonna... here, we're gonna present with two, and, the Monty has to try to. Infer what that is. Okay, one last thing before I actually type in the command, on just how to read the configuration file. the configuration for experiments are basically divided into two parts. One called defaults. So defaults just mean, oh import these configurations from other files. And they're going to provide the default values. Hence the name, defaults. For any kind of parameters that's not specified under config. So if you're familiar with Python, these are like import statements, so it just says import all these configurations from different files. Under config are overrides that you can make to specific parameters, that are unique to that experiment. Okay, I know there's a lot of words here, but I'm going to move on to running the experiment itself, so let me activate the Conda environment first. tbp.monty and to run the monty experiment and you have to specify which experiment you have to run. That's the only required argument, so to do that you just have to pass you just have to specify experiment equals and the name of the experimental file. So in this case it's just demo. You don't need to specify the file extension. And press enter. So what this is gonna do, it's gonna print out a long list of configs. So this is the full configuration listed out.

And soon, it should pop up, should start the, evaluation.

Let's wait a couple seconds for that to pop up. There we go. Just gonna start some there we go. So you can see here, it's a kind of a crude visualization of Monty moving around looking at the bowl. It thinks that the most likely hypothesis, MLH, is a bowl with a certain amount of evidence. It's gonna move on to the mug. Again, it's gonna, it's gonna move around the mug, explore around different parts. I think sets the most likely hypothesis is the mug, So that was a quick experiment. I know it flew by. If we have time at the end maybe we can come back to this. But basically the experiment's gonna run until Monty detects a match or has timed out.

And another thing that I want to note for object recognition experiments, or inference experiments, they're gonna output, we usually save some summary statistics of how Monty has performed. So you can find this in the output directory and what you should see so this is actually my output directory. What you should see is a folder name with the name of the run name. just FYI, the run name doesn't need to be the same as the file name, it can be different. It can be the same, that's up to you. And the most important kind of file here is the eval underscore stats.csv. So if I just double-click and open this up. It's a little bit small. So you can see, this contains some statistics for each object or each episode that Monty encountered. in the first episode it encountered ground-truth object, bowl. It thought that the most likely object was bowl. We saw that but there's also other, many other, kind of columns, calculating the rotation error, how long did Monty take to get to this conclusion, etc... So this is how we measure performance of Monty and these are the kind of metrics that you'll see if you go to our benchmark pages. And yeah, so this is what you can expect when you run Monty. I'm gonna go back to the slides real quick.

Alright, I think, previous presentations has alluded to this already, but, this is another kind of view of the high-level, kind of classes that exist in Monty. So Monty itself is composed of one or more sensor modules, one or more learning modules, and a single motor system. It can then be placed in an environment, simulated or real, to run an experiment. you just saw an experiment for object recognition to identify YCB objects. And every one of these components are designed to be modular so you can easily swap in or out. So you don't need to just change particular arguments or parameter values but you can actually write whole new classes for data that you might have that you want to experiment with Monty.

As an example of that this is actually my own configuration for a project that I'm working on. So you'll not see... you'll not see this in documentation yet, because I'm not done with the project, but hopefully soon. But you can see that I have, went to Python, I've written up a custom sensor module class. I specify that here, and the sensor module class key. I have some custom arguments as well. And you can see that in the config I'm mostly importing, again, defaults, it's analogous to import, from this control experiment. But for this experiment I'm going to just override the sensor module class. this is kind of a tip for how to set up experiments. If you want to compare, two different experiments like a control and an experimental without accidentally overriding or changing other kind of irrelevant parameters. So in general, the configuration is written in YAML, so that hopefully you can read through and understand some of the major classes, even if you don't know what Python is. It's supposed to be composable and modular so that you can easily plug in your own sensor module. Plug in your own custom classes, such as the sensor module or learning module, or environment. Yeah, so that's actually it from my presentation. I know that was a lot of information, so if you didn't get everything, that's okay. Most of everything that I covered is actually written up in running your first experiment tutorial. This is actually one of many tutorials that we have, I highly recommend you to start with running your first experiment. Go through this, but there's also more advanced tutorials if you want to dig in deeper. If at any point you get stuck or have questions please feel free to post on Discourse or GitHub Issues. Even if you don't have questions, please come say hi, we love meeting new community members and interacting with you all. And for those who are advanced, got everything I said, went through all the tutorials in the past 5 minutes and actually want to use Monty immediately for your own application, and you need inspiration, I highly recommend you to check out the project showcase page. Alright, so with that, I'm gonna hand it over to Ramy. He's gonna show you some really cool interactive visualization tools that exist. And that's gonna give you a good sense of how Monty's brain work. Alright, thank you.