Okay, nice. so yeah, I just thought it'd be useful to talk through an example of, this is all very kind of high level, but, yeah, how we might conceptualize the hierarchical, action policies and the goal states and things like that. and, yeah. Yeah, we talked a bit about, making a cup of coffee, and, yeah, to your kind of point, Misha, each learning module has these, layers, the way we, just as a reminder, the way we generally break it up is L4 is where sensory features come in, L6 is where the kind of, reference frame is essentially represented, whether that's grid cells or something else, and so L4 kind of sends information down there, there's recurrent connections between those, and then L2, L3 is more the kind of like object level representation. L5 is the bigger kind of, mystery in terms of what we are going to implement and what we think is happening in the brain, but that's where direct, motor, subcortical projections go. So clearly it has some, importance in kind of motor and the main things missing from our, systems is Sort of planning part. So there's a good chance that would be there. And then what's not shown because it's very thin is L1, but it is important because L1 receives top down connectivity.

and yeah, sorry, I didn't have a framing slide just because I was expecting to show this at the end, but I guess to frame this discussion, we've just been talking recently about, our hierarchical action planning again, if you have some kind of abstract, complex task, Make coffee. how do you actually achieve that? And, and what would that look like in terms of the different learning modules? And I think what's the kind of exciting thing about Monty is, in kind of reinforcement learning and kind of policy planning more generally, you have this like divide between model free and model based, policies where model free is a simpler kind of trial and error style learning, which deep learning systems can generally do really well. And model based is where you have an actual explicit world model that you simulate things like the, like a kitchen scene and things like that, and you can essentially understand how that, that scene, how that model will change over time based on certain actions. and if you have this, explicit model, you can actually plan out, entirely novel, kind of actions, you can choose a different path through your kitchen every day, because you have this kind of structured representation of it, but what's particularly, promising, I think, about, Monty or the thousand brains theory is, each learning module is an object centric representation with reference frames, So it is ideally, suited to basically enable model based policies at that level of representation. And so then if we recognize that the learning modules are arranged hierarchically, as well as with lateral connections and stuff, then it's natural to think about how they might coordinate between each other to carry out more complex actions.

so then I guess getting to the actual example, that I put here, so yeah, imagine you have some very high level learning module. Which is, it's responsible for planning your day or something like that. It determines the abstract structure of what you're going to do over the course of the day. And the highest level goal coming in, I've just put hypothalamus, as a cop out. But, somehow, the cortex is being told like, okay, the body's very tired, let's get into a less tired state. and so that's the kind of received goal. And so the day planning LM. Given this goal of being in a less targeted state, maybe knows about, okay, if I make coffee, I can address this requirement. so it's going to recruit a, lower level learning module, but still a fairly abstract one that models, say, kitchens. with this kind of, and it's going to send, or at least one possibility is sending this information via L6, which predicts to L1, which in turn has, there are apical from neurons that are in L5. So that just means that there's a natural way, basically, for this information to go down through the hierarchy to cells in L5, if those are the ones that are making plans.

and then this kitchen modeling LM, it needs to achieve the target goal state of kind of coffee made.

And this might be a setting where rather than immediately to recruit, some other lower level learning module, we might make use of immediately subcortical recruitment, yeah, so you walk through the scene until you're at a coffee machine, so this is a simpler policy that can be more model free in terms of okay, you just need to be at a particular location, Maybe this, this learning model can send a series of target places to B so that, it actually plans the kind of route through the kitchen, but it's the subcortical structures that kind of deal with, okay, I need to be here. I'm going to move legs until we're at that location.

and then, so this has now been shifted up, if you imagine, so this is still the, this is the kitchen modeling LM. So this is assume this has now been achieved. We've walked until we're at the coffee machine. Then, we, this learning module might send a, another kind of broken down goal state to an even more kind of specific learning module, which is like coffee machine on state.

this is to the, sorry, the learning module that models coffee machines. And then here, subcortical recruitment might be something like look at the coffee, or find the coffee machine button, basically. And so this learning module that knows all about coffee machines. knows that in order to be in the on state, it needs to find the button. and because it knows the model of the coffee machine, it can, again, direct subcortical structures like those that affect the, orientation of the eye, but also moving the head, things like that. Until you basically find the copy button, and then again, shifting it up, then once that's achieved, then finally, there might be, say, a learning module that models kind of machine buttons, and you want that in the kind of press to the on state, and the subcortical recruitment in this case is pressure at a location, and then I guess, yeah, so what this is hopefully showing is how, depending on the kind of task, it either can be decomposed further into Basically by recruiting like a learning module that knows about that kind of object, or it can, that learning module is, will be in a setting where it's okay, I know the right amount of information to then just recruit subcortical structures, which are going to actually move the body, to achieve the required state.

Can I ask a question at this point? Yeah, please. so that recruitment mechanism, what's the selection mechanism that it picks from a plethora of behaviors such that it recruits that specific one to achieve a particular goal? Yeah, no, that's, a fair question. yeah, I think one of the biggest unknowns is, yeah, exactly how L5 would learn and Remember this, but I guess you can imagine if you've previously encountered, an environment like this, I guess you would have okay, coffee was made at a time when, sorry, let's say in the kitchen one, coffee was made at a time when, the coffee machine was, on. So you could have some association between kind of that state represented here and a location, but it's not any location. It's like a location associated with Yeah. Potentially a particular goal. But, but I guess the analogy is rather than moving to a location in physical space, we're moving to a location and expecting a particular state at that location. I it's definitely very fuzzy at the moment. It's, how exactly we would do it, but, it, would be some sort of associative learning. And so then in the future, if you want to reinstate that, state, then you have that association to say, okay, the learning module that was active at that time, when this happened was this one. So I'm going to try recruiting that one. So Jeff's, word doc was basically Outlining the fact of this, seeking sequential goals is tantamount to navigation in physical spaces.

the, with the physical spaces though, you could see that you have a kind of a limited configuration space of how you move yourself, body, obstacles, and stuff like that. Goal states being wildly more abstract, it would seem, you can, just in this, Kitchen scene alone navigating to any interesting point in there is finite, but all the behaviors you could do at any one of those points is, is another level of, complexity. I'm just I guess you're depending upon the hierarchy to, cut down the number of choices at any one particular point where, these are the options, I'm at this point, I want to go, and, somehow I've Yeah, exactly.

the, goal state that this learning module is going to be sending to this lower level one is going to be conditional on where you are, but also the goal state it's receiving from the higher level. So because the, yeah, it's the kitchen modeling, learning modules, as you say, there's a lot of stuff you can do in the kitchen, but it's received the goal state, copy made. And so once it's in the location where. it can interact with the coffee machine and conditional on this, then the kind of natural thing is okay, I remember at this time, the coffee machine needs to be in the on state. Okay. So just, to be clear, the, only path integration mechanism that we currently know about is through grid cells. Yeah. Do we, are we hypothesizing that is? necessary and sufficient for this type of navigation, or we just don't know yet, and maybe there's another mechanism involved, but we're just positing that something exists. Yeah, I think, yeah, I think it's fair to say in general, we think that in abstract spaces, it'll still be something similar to a path integration, so going with the example of a family tree or something, it's a very different, action space, like if, you have the parent relationship or the child relationship or grandchild relationship, but there's still some kind of rules about what steps will bring you back to the same location and things like that. but yeah, obviously whether kind of grid cells can do that, but in general, yeah, I think there's, firstly, there's evidence that, that's true. simple recurrent neurons that can model grid cells can also model those kinds of abstract spaces. But also like right now we're just using this kind of 3D Cartesian spaces and in general we think it'll be very simple to embed even abstract spaces as long as they're three dimensional or less in those. And so the, then the weirdness, this is more just about abstract spaces in general is like, how do you really learn the actions that are possible in that space? but maybe in some sense that just emerges naturally through a hierarchy that's going to be the movement information coming in and you're just going to learn, okay, this particular movement information is associated with this kind of displacement in the space, but, yeah, definitely very uncertain how exactly we would implement it. Okay. Thank you. Yeah. No worries.

Yeah, and then so I guess the other kind of thing to just touch on then is, with this hierarchy, you can imagine how, of course like making coffee isn't just about turning on the coffee machine, there's going to be a series of steps, of which turning on coffee in the kind of, say kitchen scene, Or even in the coffee machine space, there's only going to be one.

but as you break down the task, hierarchically like this, okay, you go to the coffee machine, you turn it on, it's on the on state. Your, you will then start receiving sensory information consistent with this being in the on state, and that's going to then come into here, in, this kind of the hierarchical connections that basically go in the opposite direction from L2, L3 here to L4 here. And so then I can tell this learning module, okay, the goal state I just generated, which was machine button press state, has been achieved because I'm getting sensory information from this lower level learning module. That it is in that state, so now L5 at this level can create a new, subgoal, which is maybe put a coffee pod in the machine, or, whatever it might be, So yeah, that's a high level overview of, I think, how we were generally thinking about, goal states the last time we discussed it, summer 2023.

Yeah. And just one, just to recap, that was a really great overview, Niels. but just so things in a bit more, when you conventionally think about hierarchical action policies, you think about decomposing an action into sub actions and into smaller actions. Yeah. But what we're proposing here is to, everything that's communicated between columns are states, not actions.

and that's everywhere. That's the cortical messaging protocol that we've defined. and learning module outputs, hypotheses, a hypothesized state of the world that it recognizes, oh, there's the coffee machine at this location and pose. And that's the input to the next higher learning module. It also outputs possible states through the votes. And then it gets goal states through these connections that Niels described. So everything between columns is in the format of states of the world. And then actually translating, these goal states into muscle movements is done subcortically.

Yeah, I know you already said that. Yeah, no, but no, thanks. Thanks for emphasizing that. I think you're right. I think that's important to emphasize. And that's part of what makes this kind of the feedback through the hierarchy of achieving goal states particularly simple, because then It's literally just a comparison of what you have achieved. So it's still in there at 9 a. m. All right, we're going to get started a little bit late, unless you want to do something else. Yeah, no, Niels just recapped some of the, Monty plans we had for, actual policies and things we talked about last year. let's just get going then. I'm going to share my screen if that's okay. And, hopefully, that's going to work. And then I want to bring up, let's see here. This is, this is the document that I posted on, on Slack yesterday. Do you see that?

Hello? Yeah, we see it. Okay, I don't know if everyone had a chance to read it or not, but I thought I would go through this document a little bit, just not read everything, and then I have some images to work from, if that's okay.

Anyway, so just a little bit of background here.

we, the last couple weeks Neil's presenting our action policies, but these are all related to inference and learning. and they're central and really important, but the output of the Sensory Motor System is, is behaviors not just for learning and inference, but to. Achieve change in the world and achieve goals. And we really haven't addressed that at all, in any kind of formal way so far in our theories and the code. And obviously for us to have a really working system, we have to do this.

we need this, I figured this, let's get started on it, in a serious way. And I think with the recent advances we've made in compositional, modeling, like modeling compositional objects, it was, that was an essential first part because compositional, because goal oriented behaviors are also composition, and the same relationships columns have when they, modeling objects are going to have in modeling compositional behavior. if that's clear. so I, that's what I, these notes I wrote up yesterday, if you had any questions about them, I'm just going to talk about them briefly before I go on, but, I do want this to be interactive, and so if you just didn't like something or didn't understand something, just speak up immediately and say so. I just, second paragraph is goal oriented behaviors are complicated. I just, that's obvious, but it's worth stating. This is going to be a very difficult thing for us to solve. I'm confident we can get it all done, but it's not something we're going to get done in a day or a week. It's going to take months to figure this all out, if we're lucky. And there's a lot of things that have to be put in place. I listed some of them here. But in the end, we're going to have to figure out how columns work together horizontally or laterally and hierarchically in achieving goal oriented behaviors, that's clear. But as always, I think we should start with a single column. And that's what I did today. I'm going to talk about what single columns, how we could think about goal oriented behavior in a single column. So I said I'm making good progress on this problem, but I'm making good progress on the problem of single columns. and as I said, we've talked about some of these ideas before. But they never, they weren't really crystallized in any way, as much as they feel like the way to me now, I'm not saying, oh, this, maybe there's some totally new things here, I don't know, I can't remember, but I do think that I've made progress in putting it all together. The main thing that we're going to talk about today, or at least as a starting point, is this idea of, we, we've always known that models of objects, or objects can have behaviors. of And we've talked about the stapler opening and closing, we've talked about apps on a phone changing and so on. And I'm trying to get serious about understanding how do we define, how does a model learn behaviors, because behaviors are going to be key to understanding, behaviors of an object are going to be key to understanding behaviors of an organism.

I, let me just, I don't know if I should keep going through this or just jump to my slides here. I'll just, keep just touching on this and the slides will reiterate this again. essentially today our models, are defined by a location space. And at locations in that space, the model can observe features at orientations, and that's how a model is built up. It's built up with location, space, features, and orientations.

The way we modeled it, based on the enthorhinal cortex, is that we're using grid cells and grid cells can re-anchor grid cell modules can re-anchor. So each, object has its own space in some sense. The, SDRs represent the locations in space for any object are unique. and so an object is associated with a particular, set of grid cell anchorings. But we're gonna introduce a concept of state, and this is my proposal. And state is another variable that says within the context of a particular. Anchoring of grid cells within a particular space, an object can have different, different features, different feature locations, different feature orientations. It can be completely different. That is, anything can change on an object based on its state. And so the state allows us to say, staplers don't always look the same or this object doesn't always behave the same or look the same. It can have different shapes and different features. Anything can change, actually. Yeah. and we're gonna, we, and yet it's still the same object. a stapler is a stapler whether it's open or closed. so we're gonna stick with the idea that the object is defined by its anchoring of grid cells, but a state lets us say, what does it look like now, and what does it look like a moment later, and so on. so I, that's why describing here, I have pictures of this we'll get to in a second. and the idea here is that these two variables converge. You have, if I think about what defines an object, I have a location in its base, I have some observed feature, and I have the state. And so what is predicted at any point in location will be determined by both the location and the state of the object.

there we go. And then I define the idea of a goal. A goal is essentially, a column is trying to get an object into a particular state, a state that it's not in right now, but we want to get into that state. And, this, again, this is a, very small part of goal oriented behavior, but it's what a column, an individual column can do. It says, okay, I'm observing an object, it's in one state, I'm supposed, I want to get into a different state. Someone told me that, I should get into a different state. And, And so this, this provides a basis for setting a goal for the particular object.

I then go on to what I hear called the behavioral space. there's how is this state represented? And I'm going to propose that states are represented the same way using grid cell like modules. That there is a space of behaviors, of the states of behaviors of an object. And, in that, this is a parallel space to the space of locations of an object, and it exhibits the same kind of behaviors. In the behavioral space, we can do path integration and we can do, we can set a goal and know how to get to that goal in behavioral space. I'll walk through this in a picture in a moment.

one of the advantages here is we know that the enthorhinal cortex, when an animal's in an environment, it can, it does path integration, but it can also do the inverse of path integration. The inverse of path integration says, hey, given where I am right now, and I want to get someplace else, what is the movement I should make to get to a location? So this can be a novel direction, like an animal can wander around an environment and say it wants to get home, and it can say, I know which way to go to get home, even though it's never traveled that way before. and we're going to assume the same thing is here too, that within the behavioral space, that, that I have an existing state and a desired state, and, the column will be able to say, how do I move in my behavioral state to that, desired goal?

so this is basically the columns of two sets of grid cell like modules, this is my proposal today. and then there's a lot of questions that have to come up, that we'll talk about in a second. what is the equivalent of movement in a state space, what does it mean to move in a state space? It's not a physical movement. What does it, what does that mean? what is the equivalent of path integration in the state space? How are movements in state space converted to movement in physical space? Because in the ultimate, the column has to do something physical, but it's going to be thinking both in terms of, a different space, a, state space.

as we're voting, and temporal pooling in this thing. And then and so on. just a little quick aside for those of you who read this, I thought this breaking, adding of the state to an object might be a solution for context. classes like, different mugs that are completely different that we both call mugs, but they don't trans, they don't change from one to the other. And so I was really excited about this might also solve the classes of objects problem, but I don't think it does. so I'll just leave that aside for a moment. Okay. I'm going to go on now, to this presentation. Before I do, is there any, I want to go through the same ideas again, but I'm talking about them in some images here. is there any questions before I do that?

Large or small little questions, big questions?

Are you still there? Yeah, no, no, I think nothing specific at the moment. I thought it would be interesting at some point to talk about like the grid cells and yeah, and how they might map on to states and stuff like that, but maybe it makes sense to wait until. I'm going to talk, I'm going to talk about it a little bit right here. Yeah. I'm going to try to get in there. I don't have answers to all these questions, but I do feel, we're getting pretty close. All right. So this little thing, I, labeled the slide, you can see my slide here somewhere, right? Two parallel systems. Yes?

I don't know. I'm just confirming that we can see it. I'm still trying to understand states and goals. I'm still, processing in my mind, like I, I only read this yesterday and It's still a big concept, so I'm seeing if this presentation will help me, the visual way might help me more, It might help you. I, Hojae, I, I've been struggling with these issues for years, and it's really hard for me to understand these things. And then I get clarity, and I don't even know how to describe it, right? It's like I'm trying to put words on this that, that are not really things I think about in terms of words. And so I expect this to be very hard to get right away, and I don't even get it completely. So I think it's just, we're going to go through it a couple of times, and we'll, it'll take a few presentations before it sinks in, or we'll find out where the holes are, what's good about it, what's bad about it. So maybe we just, I should try to present it here in this picture, and it might help a lot. I'll do that. I'm going to, this, there's an idea here that's been in neuroscience for a long time, like 50, 60 years. This is an observation made many, decades ago where people observed that the upper layers in the cortex and the lower layers of the cortex seem similar. There's a similar, there's a parallel construction here. I've talked about this in the past. it's, an idea that I read many, years ago and it, seems to be, it's intriguing but I don't really know what they're made of. So why do they say that it looks similar? Why does it look like there's two parallel systems? What's going in the upper layers that's similar to the lower layers? the first thing to notice is that there are, the most notable thing is that there are cells that are arranged in mini columns in the upper and the lower layers. they're not, it doesn't go all the way through the cortex, it's this, sort of layer four is a no man's land in between, but in these, cells in these minicolumns are directionally tuned, meaning they, they respond to movement, this is mostly vision, so it responds to movement, if they put a, a grating in front of the animal's eye, these cells in the minicolumn all respond to the same directional tuning of, an oriented line moving, or a grating moving in some direction.

and that in both the upper and the lower regions. I didn't show it on this picture, but there's also horizontal, long distance connections in layers two and three, which go to other columns. And there's also long distance arrangements in layer 5B, which go to other columns. So there's a, we would call this maybe a voting connection today. But that's not shown in this picture, but this was a striking observation. There's a lot of parallels here, and mostly it's around these minicolumns because this is an unexpected thing. It's also not clear why you would have, traditionally neuroscientists don't really have a good idea why you would have cells that are directionally tuned, for movement, that respond to movement. you can say, oh, maybe the object's moving. We've now said, oh, maybe, we're now saying that the lower cells are representing movement of the sensor itself. But this has not really been ever explained by most neuroscientists. One of the big observations about the difference between these upper and lower layers is the size of the receptive fields of these cells.

if I put a sinusoidal, a grating, like a sinusoidal grating across the retina of an animal, and this is a very artificial stimulus, you'll see that the cells in the lower layers respond to this movement in, one minicolumn, maybe a horizontal next minicolumn, maybe oriented 20 degrees differently or something. but they respond to the very large patterns on the retina. In fact, they, they really like large patterns on the retina. It's like these, gratings can, if you, put an oriented line and it's moving that, if you make it longer, it's better. Where the upper minicolumns, they're smaller RFs. In fact, some of these cells, many of them are what they call end stop, meaning if the, stimulus, the line stimulus, imagine that there's a line moving across your retina, if that stimulus gets above a certain size, the cells start being inhibited, they don't respond. So the big difference between the directionally tuned Motion sensitive cells that are arranged in minicolumns in the upper and the lower areas is one of the big differences, perhaps the biggest difference, is the size of the receptive fields that they respond to.

other than that, they're very similar in many ways. So this is an observation, that was made many years ago and people said, hey, maybe the same thing, something similar is going on in the upper region and the lower, upper layers and the lower layers, but there really hasn't been any theories about that I'm aware of. there's nothing that's stuck around, so this idea has been around a long time, and it's been for Sort of the unspoken, like, why would you see it like this? why would you see these mini columns like this? it's most neuroscientists just to say, oh, this is what it is. It looks like this. As opposed to like, why would it look like this? What's the theory behind this? and part of what I'm proposing today fits in with this very well, it fits in that says, okay, there's evidence here that these two things are doing something similar. And as we think the directionally tuned, as I talked about recently, the. Minicolumns, I've argued, fit a requirement that grid cells have. To make grid cells you need to have a series, you have to have a set of cells that are, that respond to movement in a particular direction, and they change their frequency based on the speed of that movement, and, And they are the precursors to grid cells, and so you need a set of these cells that are all spiring at the same time, but at different phases to, as to, this is a requirement to create grid cells, in essence. that was a theoretical, proposition, and the minicolumns look like that. Now, we don't know if they're phase, distributed in a phase, I don't, that would be, that's a strong prediction of the theory, but. but they look like that. They look exactly what grid cells would need, and we know that grid cells do exist in the cortex now. We, we, theoretically derive that need, and then it, now it's been seen in many parts of the cortex. So this is no longer, in my mind, speculative. Something's going on in grid cells, in grid cell like behaviors in the cortex. And, and it fits our theories and it fits the anatomy and physiology we see. So we've said, okay, those large RFs in the lower regions refer, they're going to capture the movement and say vision, they're going to capture the movement of the eye relative to the world. Because if your eye moves, then all the bits on the retina will be changing. All the pattern, the entire pattern will be moving. And so the entire pattern moving on the retina, that's a strong indication that the eye is moving and the world's not moving. Okay. And the smaller RFs in the upper region says, hey, these won't be responding to movements of the eye because it's only a small part of the retina that's changing. that's what the small receptive fields are telling us, that these cells are going to respond when not the eye is moving, but the thing I'm looking at is moving. The thing I'm attending to, this central area that this particular column is looking at, that must be changing or moving and, and therefore this is, somehow detecting motion of the object or changes in the object or something along those lines.

this is the state of the art until fairly recently and how we've talked about it. Before I go on, is there any questions about this?

This is helping me understand a little bit more, but I don't have questions yet. Okay, I'll give you an interject at any time if you want. Okay, so I'm now proposing the following things. I'm going to say, okay, the bottom layers are representing the location of the sensor in the space of the object. That's not new. I'm going to say the upper thing is representing a state of the object. Now, this is, it's. State of the object is like I described before. It's just going to say what the object actually looks like at, in a particular state. again, the state will open and closing on each point on, what each point predicts in the location of the space, the state of the space will depend on the state of the statement.

and now a model is, going to take advantage of all three of these things. The model is going to say, look, I have a location. I have some observed feature at that location, but I also have a state, and the state says, look, I have to have the location and the state to make the correct prediction or to learn the feature. They're, and if I change the state, I could have a very different prediction, a different set of cells. so we're just adding another variable that, that these have to, you have to have location and state to make the correct prediction, and when I'm learning the model, I will have some state variable. and a location variable, and I'll know how to assign the feature to the layer four cells, which layer four cells will be active at that point in time. So it's kind of sense, and sometimes it's a very simple addition to the model. There's complexities with it, but I won't go into it right now. But the basic concept is pretty straightforward, I think. if I have a different state, then this model, this middle object could look different. The model can be different anyway. There's no restrictions on how the object may change under different states, but they're all, what they all do is the stapler object all share the same space of the object. Locations, the same grid cell, in layer six grid cell module anchoring. So that's, that's just an extension of our current model. It allows us to, to basically learn predictions of, learn, the model of how objects change in different, at different times under different conditions. Yet still think of it all as the same thing. An open, a stapler and a closed stapler is still the stapler. We have no question about that. That's going to be determining by the anchoring of the grid cells. In some sense, the location space in the lower layers defines the object, and then the state defines what currently, how it currently appears in the world, what its current state is.

I just said that, so I don't have to say it again. You can read it.

All right, now I said here, that these, the way the neuroscientists classically will describe these cells in the minicolumns is that they're directionally tuned. So they'll say, okay, this minicolumn, all the cells respond to movement horizontally to the right, another one's, up at 20 degrees, up at 40 degrees and so on. And you, I, assume all of you have seen these pictures, I didn't include them here, but you can, they show that as you move across the cortex, the minicolumns, each minicolumn changed orientations from the next. So they essentially, the minicolumns represent a complete set of orientations of movement.

I think that's not actually correct. and there's, what I'm going to suggest is that, and this, took me a while to get around this, directionally tuned isn't the right answer, I'm going to suggest something else, that, that the minicolumns, we would say that they are, they're responding to some type of change, and one type of change is movement, it would be like a directional movement, but there are other changes that could occur, a color could change, occur, or an orientation of a feature could change. it's not that these things are just recording movements. In fact, think about what we've talked about in the, in the lower layers.

we have to, as, our eyes move, we have to update the location on the object.

as I pointed out before, if you're watching someone play a video game, you don't just see that the eye is moving up or down, you also know if the player is moving forward or backwards, or turning their head. In some sense, the information coming in from the retina is encoded into a series of movement vectors, that are not just constitute So, the classic ones of left, up, and down, there, there actually needs, there needs to be a movement vector that you're going into the screen or, you're going forward or you're going backwards. And that's not, that doesn't really, that can't be explained by directionally tuned. If you think about it, if you think about what the bits on the retina look like, that these are, if I'm moving forward, the bits are all changing, but the ones in the chin are changing almost none, but, and then there's a radiating field coming out. It's not going to be captured by your classic directional tuning. That's how they describe minicolumns. but we're absolutely certain that a cortex, the visual cortex is getting that information from the retina. There's no other way about it because you can just sit there, look at the screen, and as a, virtual player moves forward, you know it's moving forward. That information is encoded in the image on the retina and it's changing your sense of location in the game. So that information is coming in. and I'm going to argue the same thing is, going on in the upper layers. So this is a subtle point. We tend to think about these minicolumns as being directionally tuned, like they're just these, it's, just like a compass, you're going one of many directions. But I think what's reality is happening, it's more subtle than that, and the directional tuning is a subset of all the possible behaviors that can be exhibited, as in moving forward or backwards. also in the cortex, there are lots of minicolumns. S tend to report what they wanna see or what they're looking for. And there's lots of evidence that there are many cell responses in, in the cortex that don't match this classic, view of, directionally tuned, 10 different direction type of things. and you can go into the literature, you can find this, so you can talk to a neuroscientist. So it's not outta the question at all that, that what is classically reported, the huble and diesel type of model. it's actually just a subset of what's going on, and I think this is important to understanding a movement of the sensor in the lower layers, and it's going to be important to understand, what state space is in the upper layer. So I just wrote here supporting evidence, the magnocellular inputs, the cortex of cinema and RFs. Oh, this, is another point. So again, I'm assuming a lot of neuroscience knowledge here, sorry about that. but I, can't give the whole thing. So we've talked about in the past how the magnocellular, there's two types of inputs from the cortex, from the retina, two basic types, there's others, but there's the parvocellular and the magnocellular. The magnocellular cells have inputs, they all have center surround RFs. Meaning that if you look at what a red, an axon coming off the back of the retina represents, it doesn't represent movement. It just represents a change in a center surround, receptive field. So there has to be a line going through it or a dot in the middle or something like that. And it has to be changed. These, the magnocellular cells only respond when there's a change. They don't respond to static input. The parvicellular cells respond to a static input. But the magnocellular cells are saying, something has changed here. And, now if you think about it, you've got all these bits coming into the cortex, these magnocellular bits, and they represent different changes. they're not directionally sensitive. There's nothing about them that says, oh, the retina is moving this way or that way. It just says, at this point, this ganglion cell from the retina says, there's been a change in the, in relative to my center surround RF, my center surround receptive field. So no concept of movement has appeared yet. It's just change. This is important. It's just change. And they've determined the axons coming off the retina in primates, like humans, is just the senescentral RFs. as it exits the thalamus going to the cortex, it's just center surround, but immediately when you get to the cortex, you see directionally sensitive, so the directional sensitivity is something that the cortex has extracted from these center surround receptive fields. It wasn't something that was there, and it I'm going to argue that the cortex has to learn these things. We've talked about this before. We've talked about the cortex has to learn what movements actually are exhibited. So it gets these center surround RFs and it runs it through something like the spatial pooler, And it says, what are the top, components of, that I can represent the changes I'm looking at, and it would say, okay, I'm going to represent this, the kind of changes I've seen would correspond to the agent looking left, looking right, looking up and down, going forward, going backwards, and so on. These are things that have been observed, not things that are hard coded. So it has, the cortex, my assumption is the cortex learns from these changing bits, what are the movements that correspond to those changing bits. And we'll call them movements, although they don't, you'll see in a moment, they don't have to be physical movements. It's just, you're just taking a bunch of changing bits and running them through the spatial pool and becoming a bunch of minicolumns that rep, that says, this is what I'm going to call movements. These are the, this is the, abstraction of all the changing bits I've seen. I'm going to, I'm going to put it down into like an end winner take all, concept. So I'm going to say these are no longer directionally tuned, but they're change sensitive. So the minicolumns represent common changes that have been observed, In the inputs, they're not, and some of those changes will correspond to movements, but they don't have to. They're just common changes that are observed in the input. It's a subtle point. it's essential, though, to understanding behavioral spaces.

I've said a lot, someone, people must be confused, so I'm going to say something. I do have one question. The, magnocellular and parvocellular, project into the LGN, so they don't go, as far as I know, directly to the, neocortex.

But they've measured, they've determined the receptive fields of the relay cells. In the thalamus. And they have not changed. They are still, center surround. This is why they call them relay cells. They can't see what the hell the thalamus is doing, right? It just looks like you've got a center surround bit, RF coming in here, but center surround RF coming out. We now know what's going on there. It's remapping those to create a change in orientation. But the point is, they've determined that the inputs to the cortex themselves are still center surround. There's no movement in those fields. Now, there's, now that you've asked this question, Kevin, it's confusing because in rodents, They actually see the outputs coming off the back of the retina and coming out of the thalamus, having directional sensitivity. in rodents, there isn't this generic receptor, the center surround. It's tuned for movement, but in primates, it's not. And I view this as that, in evolutionary terms, In the beginning, all of the evolution carried out, is this animal moving, how is it moving, right? But now when we get to sophisticated movements, and especially when we get to state spaces we're going to talk about here, movement isn't the right term anymore. And change is the right term. And that's why we see in primates, including humans, that these bits don't represent movement, per se. That's a limiting, that's a limiting assumption. And we want to have an, we don't want to assume that, we want to assume that the cortex can learn any.

I tend to focus on numbers rather than on the number of cells in the cell. And the reason I do that is because I want to make sure that everything is right. I don't want to call it the number that's in there. And then what I want to do is move that into this, the, this is the ical data I'm looking for. I'm looking for these sequence numbers.

temporal shift in the spike or, the, receptor field got a little bit larger, a little bit smaller. there's a whole bunch of papers where they're trying to figure out what the thalamus is doing. And they tease out these very subtle changes without really changing the basic idea that, they're looking for what, the hell is the thalamus doing? And they're looking for really small things that they can put their, they can detect. In reality, they missed the big thing. The big thing is it's rerouting those signals. It's a multiplexer type of thing. And, and at least that's what we believe. I'm pretty sure it's going to be correct.

so, that's, they missed the big thing because they just, under any particular context, they'll say, okay, it's, a spike in, a spike out. What's the difference? the difference is under a different, orientation concept, it would totally remap the output from the input, and their research paradigms would just don't detect that. They just don't know how to detect that, because they weren't looking for it.

All right, so how do we understand what's going on in these two things here? So in the lower layers, we've already discussed this quite a bit, it is, we all know this is part of thousand brains theory. what it's, what the, these grid cells down there in the lower layers are doing is they're, they're detecting movement of the sensor patch and through path integration it updates the locate, the representation of the, of the location of the sensor patch relative to the object. There's nothing new there. That's what we're doing. what's going on in the upper layers here? Now, I have to admit, I'm still foggy about a lot of this, so don't, I don't have the answer to this, but this is, where my current thinking is. so I said, what is the equivalent here? I said, the equivalent here is that the feature is changing at this location. That is, there are cells in the retina that have detected a change. I'm not going to say it's a movement, I'm just saying it's a change, and that change is going to be a movement in the space of states, behavioral states, so maybe the, maybe this part of the, object changed color or maybe the orientation switched or maybe a new feature appeared, it's, we don't really know, it could be any of those things. So the feature is changing at whatever location the column is looking at, and that change will affect path integration. In the space of object states, it's going to be, it's going to, it's going to do the same principles, it's going to say, okay, I've got a bunch of vectors which are representing change, and they may not correspond to movement in the world, they're going to change to some kind of change in the object, and if I, set up the mechanism to represent the most common changes I observed, which would be in the minicolumns, then I will be moving through the state of The space of states of the object. I can say those words and still not be able to visualize it, but I can say the mechanisms would lead to this. It's still very confusing. I was just gonna say that part seems challenging for me, as in, in the, In integrating location, there's a fixed number of dimensions, and Kevin was when we were chatting before you joined, Jeff, but in, in integrating states, seems like there's an infinite number of dimensions possible, because we, the states could be anything. maybe. first of all, it's not the number of states. It's the way states can change. That's we're enumerating. but for each state, like maybe it's helpful to be more concrete about what a state is like. you said state could be color or Okay. A state. A state or like moving forward. A state is a point in the behavioral state space. It's an SDR, just like a location is an SDR in the location space. If you think about the location space and little layers, there's an infinite number of locations. no limit. You just keep pulling around. But a finite number of dimensions. You're only doing it in three dimensions. Now the dimensions, Now actually there's an infinite number of ways you could move. We don't, because, but, the, because we, can't think of it like Cartesian X, Y, and Z. I could say there's a movement that goes at one degree north and then another movement's one degree off of north and two degrees off of north. I could do that. I could say there's movements that are spiral, I could say there are movements that represent me doing somersaults, and, but, what we want to do is that you take all these changing bits and you put them into a, you run them through the spatial pooler, and you end up with a, number of, a set of movement vectors, and that's going to be your representation, that's, you're going to have to fit everything into that set of movement vectors. And so those movement vectors are going to be the most common things that are observed, And if you ever have some other movements, like someone doing a somersault and what's happening on your retina while you're doing a somersault, it's not going to be very well represented in those minicolumns. It's just not a very common movement, but almost all movements, the common movements, it will be represented in some way or another. A combination of the, let's say in a particular, let's say in a particular minicolumn, I might have 100 to 200, a typical column, I might have 100 to 200 minicolumns. That kind of puts an upper limit on how many different movement directions or vectors I could have. But I can represent a lot of different types of movements in that, and if I was, and so there's, it's a pretty big dimensional space. when it comes to behavioral states, There's an infinite number of potential behavioral states, but the number of ways you can move through the behavioral state will still be limited to a hundred minicolumns. So there's a hundred different ways an object might change at any particular location, and that's going to have to be sufficient, and that's going to cover the vast majority of almost anything we can imagine. Because think about it, at any point, all you have to have is a feature, you have an orientation, you might, a scale, whatever. a color perhaps, and those can only change so many different ways. and so I don't think there's a, representational space problem here. I think it's fine. At least I'm arguing it is. Yeah, I guess the other, or just, yeah, I'm just thinking related to that weather. Yes. I think to your point, Michael, yeah, in theory, there's like an infinite number of behavioral states or like kind of feature change directions you go through or whatever. But yeah, there will probably be some representational capacity in terms of like how complex an object you could learn. And practically, a column or learning module might just learn a couple kind of dimensions, of changes, and if that's insufficient, then you maybe need to start recruiting new, learning it almost as a compositional object or something if it's that complicated. But let's review again. There's an infinite number of locations. In the lower layers, there's an infinite number of potential locations, but there's a limited number of movement vectors. That limited number of movement vectors is in the 100 to 200, not 2 or 3. we don't know, but there are up to a, maybe typical 100 per cortical column. So that's your, base. That's a lot. There's a lot of movements that can be represented like that, and many of them are very similar. The same thing is true in the state space. The number of points in the state space is unlimited, but the number of ways you can move through the state space is going to be limited to, again, 100, maximum 100 to 200, probably much less than that because of being in redundancy, But it won't be two or three, so it's basically the column is going to say what are the patterns I've seen, it's going to be looking at that, those, centers around bits that coming, that are only restricted to a part of the retina, the, part of the object, a small part, and says what are the types of, as those bits change, how would I run it through a spatial pooler, I'll come up with n number of representations for that, And that's going to have to represent all possible changes I, can see, but it's not very limiting. It's, it is a limit, but it's not very limiting, just like the movements in space are not very limiting either. Maybe you've said this, but in the lower one, we're integrating movement to get location. In the upper one, we're integrating what to get what.

Indicating changes in, changes in the state, so could be changes in color. They're just coming off the retina, they're not states, right? Coming off the retina, they're just bits that are changing. And it says, okay, something at this location is changing, right? The fact that those center surround bits are changing, the fact that those ganglion cells are firing, tells you that something is changing. If the pattern on that part of the retina is static, the magnocellular cells don't fire. The only fire when something changes. So it says, okay, something has changed here. I'm going to, through learning, I'm going to turn that into some sort of what we might call movement factors, but they're not physical movements. They're just They're a way of representing the different ways that, that an object can change. That's what they represent, the different ways an object can change. And and then, I'm going to feed those into a grid cell module, which will essentially say, okay, I can path integrate in this space of states based on my movement, which are really just change, the way the different, the way the object can be changing, and I will path integrate and get to a new state. Which is could, what's jumping to my mind is we're categorizing the changes into common changes? Or common, yes. Most frequently occurring sets of changes. They're not changes to a particular object, they're changes that can occur to any object at this point.

We haven't, just like the location, just like when we path integrate, we're not saying, when we path integrate the locations in the lower layers, we're not saying, we're just saying, here's the next location. It doesn't tell me what's at that location. That requires layer 4. And here, I will be path integrating the upper layers, and I won't be, all I'll be able to say is, here's a new location that I've gotten to because of the way path integration works. I'm just going to say, here's an SDR. And, and the critical thing here is, a couple of critical things is that path integration works in the upper layers too. So in some sense, once I've defined a set of possible changes an object can, that the object can occur, that can occur on the object, I've got this basis set of, quote unquote movement vectors, but they're really just the ways, the different vectors of how objects can change. Now I should be able to get, I should be able to path integrate. And, and I don't even know what this means, but it will work because grid cells path integrate. So there's, we're going to have this property, and the important thing is we want the inverse property, which is to say, if I'm given a state, a desired state, how should I move to get to that state? How should I move in state space first, and then I have to translate to physical movement later. What I'm doing here is essentially I'm following the consequences of, the assumption that there's grid cell modules in the upper layers. And the consequences of what those grid cell modules are fed as input, and I'm describing what those consequences would be before I even understand them intuitively. I'm just saying this is what would happen, and there's evidence that we need a way of understanding movement through state space because that's what behavioral, that's what behaviors are all about. We're trying to, we're trying to manipulate objects to get them in certain states. But I need to have a model of states to do that. I don't, I need to have a way of understanding how, objects behave. You can imagine, for example, in these upper layers, I might model a sequence of states which represent Like opening and closing the state line. but I can apply that same model to other objects. Like the upper layers are modeling state spaces. They're not modeling a particular object. The lower is, and so I could learn the certain behaviors in the upper layers and I can apply them to new objects in the lower layers. So I could say, oh, I'm trying to get this particular object into a particular state. How would I go about doing that? Because I know something about, how states transition. I realize this is already fuzzing a bit, but yeah. Can I try to Oh, sorry. Go ahead, Viviane. Oh, yeah, no, I really like this framing of saying that we need to be able to path integrate through state space as well. when we have the state for opening and closing, we know what path it needs to follow. And if we have something with more degrees of freedom, like a joystick or something, there are multiple ways I can move to get to a certain state. through state space. So I, really like this, way of framing it, that maybe the state spaces are also using grid cell mechanisms to, get this path integration property. I guess the, one thing I'm still having a hard time making the connection between how The state that we have in the upper layers then influences the location space in the lower layers. if I have the closed stapler, the object will exist at different locations. then if I have an open stapler? Like I would expect features to be at different locations in that case. It would be that would go back to this. There's a lot of things I don't understand, but I think I understand that one.

That goes back to this idea and this the green arrows here.

if I, got the, got rid of the upper green arrow. I just said the one coming from feature and the one coming from layer six A. That's our current models, right? We, take, we associate a feature with a location, but now I'm going to say, we have this other state And the location in the state. If I change the state, I may, I'm gonna pick a different set of good cells, a different set of layer four cells.

it's like I can have a completely different prediction, a completely different representation in layer four if the state changes, or I can, So it's, you can literally make predictions about the stapler, whether it's open or the stapler, whether it's closed or the stapler, whether it turns into jello. When you push the button, you can do anything, as long as you say, okay, the state has changed, what are the features now, at what location? Yeah, so basically for the stapler location space in layer six, we would have associations to features at all the possible locations the stapler could be in, whether it's open or closed. But then depending on the connections from layer three to layer four, like some of these might not be, a lot of these will not be, a lot of it won't be active. I didn't put this information in there, but try to imagine now a layer 4 cell, and it has dendrites, and it has thousands of synapses on its dendrites.

what do we know about those synapses? I'll tell you something. one of the surprising things that we know is that the bits from the retina, if we go to B1, The bits from the rep retina represent less than 10 of the synapses on that layer. Four cell, less than 10. Typical numbers like 7. Some people's 5, that kind of thing.

The, number of synapses coming from layer six is somewhere between 40, 45, much more than coming from the actual retina itself, which makes sense because the layer six is passing up an SDR where the, retina is not passing an SDR, but that leaves about another 50 of the synapses on layer four cells coming from someplace else.

Where do they come from? I don't remember the answer to that question. I don't know if we know the answer to that question. But they're not coming from the feature, and they're not coming from the retina, and they're not coming from LH6A. We also know that when these synaptic swarm on the retina, I mean on the, dendrite, They're not mixed together. this is a standard thing we see throughout the brain, at least at the cortex, is that the, if I imagine I have a dendritic branch and it goes out, a millimeter or something like that, the bits from the retina are clustered together in one section, The bits from the, from layer six are clustered again in another section, and the bits from elsewhere are clustered together in another section, and actually they know what those sections are, I don't remember, but I remember once being surprised to realize that the bits from the retina, are not proximal to the, cell. I used to think they'd be proximal to the cell, they're not, they're further out in the retina, or further out on the dendrite. there's another, theory, there's a lot of evidence for this that they've seen throughout the cor, throughout neurons in the cortex, is that the order in which those three zones become active matters. that is the evidence is that if you activate the furthest region zone of the dendrite first, then the mid region, and then the closest region. The cell is going to really, it's going to, it's going to really get excited or it's going to have, it's going to get depolarized. If you do it in the other order, it doesn't work as well. So these are just not randomly assigned to the, there's a, there's an assumption that the, that, that neurons make about, what should I see first, then this, and then this, and now I can make a prediction. It's, so it's completely understandable, for example, If, if, for example, if the state variable from layer three was at one point, further out on the dendrite, if that didn't occur, that cell is never going to, it's never, or it's just not going to go. it's just saying, it's until I get the right state, I'm not going to even consider being active or depolarized. So I'm pointing this out because I don't know what the right term here is, but the state and location have to be together. They have to be independently recognized on the same, cell, on the same dendrite. along with the feature for this cell to say, okay, we're, good. And so if you, change one of those, that cell won't get depolarized. It'll say, all right, not my turn. Someone else is going to do this. I was just, yeah, wondering with this influence of the state on the location, whether, depending on the kind of behavior, or like state that the, learning module is, modeling, whether something more like grid cell distortion would also be useful just because, yeah, if, for example, the stapler. I can see, something with the app, it's very simple to just learn, okay, I'm now going to predict in this different state that there's going to be a different feature at the same location. So you'd use the exact same reference frame, undistorted, but, with something like a stapler that can be continuously opened. the, it just feels like that could be quite inefficient to have to learn a bunch of independent, reference frames, whereas, grid cells can be distorted, at least in the enthorhinal cortex by a little bit. Certain factors, and so I'm just wondering if you warp the reference frame instead, then you can still path integrate. Maybe, I don't, and then still predict the same feature, but you're just gonna predict it, yeah, based on a different movement. I'm confused, Niels. I don't see why I need to do that. it sounds to me like you're talking about, we don't want to learn every point of an object either, right? And we can't do that. And so we've talked about this, that, the neurons have to do. if you're worrying about how many points I have to learn, do I have to learn all the stages of the stapler, it's just like I don't have to learn every point on the surface of an. it's partly that, but it's also partly the commonality of the locations. Like in some sense, when the staplers open, I understand that the top of the stapler is still the top of the stapler. and again, if, if, the reference pain is just distorted so that it's a different movement that I need to take. To go to the top, rather than, it's a totally different location that I've learned. I'd rather not go there, the distortion of grid cells is a bit controversial, it doesn't occur everywhere, and I don't think it would be a, I don't know, I don't understand it, but I don't want to go there yet, I don't think we need to.

it's, just, my intuition, strong intuition, that's the red herring. We don't go there yet. I think there's some very legitimate questions about how the state space and the location space interact. And I don't have the answer to those questions yet. but I'm confident we can figure them out. Because I have to go back and look at the anatomy, what we actually know about the anatomy between how these things connect to each other, there's a lot of information about this. and I have to think about it more, but I'd rather, I don't have a need to introduce that complexity yet, Niels, I just don't think it's, not, it's simpler to think about grid cells and spaces and locations before, distortions. I don't know if it's been shown that the distortions in the enthorhinal cortex grid cells really perform a very important computational role. I'm not aware of that. I think it's an observed behavior, which is not understood, but, from a theoretical point of view, I don't see a need for it. and it may be just an artifact that has to be dealt with. I don't know, but I don't want to go there yet. I don't need to go there. There's complexity that we don't want to introduce if we don't have to. I wanted to confirm something I thought I heard you say was when you were talking about the various patches on the dendritic tree where things are, segregated, synapsing, according to function, but you said there was a sequentiality that, that particular, one particular patch had to fire before the next one, before the next one, before it would depolarize the entire neuron. Could you explain that sequential mechanism? is that really, I don't know. Here's what's been observed. they've observed that If you take a neuron, and you activate its synapses, like photoactivation, and if you activate going one way along the dendrite sequence versus the other way along the dendrite sequence, you get very different responses. At the cell body and now these people aren't thinking when they do these experiments they're not thinking what we think. Like they're not saying, oh I have to have 15 active synapses to generated an NMDA spike in the dendrites. They're not thinking that. They're just saying if I scan from left one direction and I scan the other I get different responses but and then there's, I think there's, speculation on the mechanism of this and I can't remember if I'm going to tell you my speculation or other people's speculations. I don't remember. But, there is, there is a, dendrites have a, they get narrower as they get towards the end, and they have weird properties where they get to junctions with other dendrites, but you can easily imagine that as, a dendrite gets smaller, and it gets narrower as you go further along, that the, that what will happen is the skinnier it is, the easier it is to activate it, because there's less leakage and it's less capacitance, and, and so it's easier to activate a thin dendrite than it is a thick dendrite. So if you activate the thick end first, maybe nothing's going to happen, but if you activate the thin end, it depolarizes the thick end and makes it more receptive to, the next input. Okay. In some sense, it, they might have different thresholds at the two ends, and you want to, by activating along the way, you're, priming the one at the very end, the thickest one, to say, okay, I will now, I'm ready, I really am going to generate a spike. But if, I wasn't depolarized at this point, I may not. So that's one sort of theory and, idea how that could happen. There's others. There are different types of receptors and different, they also vary, the ionic receptors vary in terms of their density at different points along the dendrite. There's lots of ways this could happen. I, the reason why I was drawn to it is that provides a very compact, Mechanism for sequence memory that things have to be actuated in particular sequence for things to work. Whether, whatever the, manifestation of that is, the mechanism is. It's an i, to me it's a really important principle. so you're not the first person to suggest that Personally, I think it's wrong. Okay. I'll just be honest. it's, a simplistic way to think about sequences. But, the way we do sequences in the temple memory algorithm is a far more sophisticated and powerful way of doing sequence memory. And this is a very crude way of doing sequence memory. it would assume, it would say, since every dendrite has to learn a sequence, and that would be very limited. where if you do it with the temple memory algorithm, you have almost an infinite number of sequences you can learn. And, it's just a much more powerful mechanism. I think the right interpretation is that if this, as we say, if there is a preference for moving along the dendrite is, to do the operation I was just talking about, which is to make sure that the cell, You want to still make sure that it's at the right location and the right state, and then, to make the correct input prediction. and, so it's not just, hey, if I'm at the right state, I predict, or if I'm at the right location, I predict. I want to do an and of those two variables. and, You would need more than our tip, our dendrite model is too simplistic right now, so it doesn't matter. We're on the, if I had a, if I got depolarized from the state or I got depoed from the location, that's good enough. We don't need that. We need to make sure that they're both happening at the same time. So there has to be an additional mechanism in the dendrite. But the way you described it, it's, everything, in your SDR is operating at the same level. This, you're saying, different types of stimuli are, synapsing at different points along the dendrite. So it's almost like a nested if this, then the if this, It's more like that. It's more like that. But don't think of It's more it's it's not like the state. Then there's the next element, which is the. It's not like I'm following a set of SDRs coming from the same source, it's more like I have to have an SDR from one source, and an SDR from another source, and an SDR from another source to get this to work. and I'm just thinking, I'm just thinking that gives you the mechanism to combine these various qualitatively different signaling things, whether it's location, space, state, or whatever, you, it's saying, When I'm trying to combine things that are disparate, but are necessary for the activation, I have this mechanism for making sure that all those things are in place before I get that particular stimulus output. Anything from there, you can go into your SDR mechanism and say, okay, now we're on a level playing field in that sense, and we're combining things that are like to like. All right. this low level mechanism.

this came up because someone, maybe it was Viviane, I can't remember, someone asked a question like, it was confusing about how a different set of cells in layer 4 would be, active at the same location under different states. And, and we, got down this path by talking about mechanisms that would allow that.

it is a detailed dive into one little part of this presentation. I don't want to spend too much time on it. but I, it definitely will happen. I have another short follow up question on my question, so I understand how this works. How it works for one object that we have the state from layer three, Pick when, which features, should be, expected to come into layer four, but how does it work to use the same kind of state space or behavior and apply it to different objects that use completely different location spaces? I have no idea yet. Okay. Yeah. Can I like, also go through like an example to solidify my understanding and just point me if I'm wrong. Just so that I'm, like, making sure I'm understanding. So let's say, with our features at location, with Monty, we look at a patch. Sorry, I'm going to draw the, zoom, hopefully you can see it. Oh, here we go. Look at a patch, and then, it's yellow and I can see, let's say I moved enough to know that this location or this object is a banana. Oh, this is quite terrible. let me, okay, yeah, it's like automatically making into, okay, got it. yeah, there we go. It's banana. And, but let's say we're just on a patch for now. Yellow. And, over time, so there's a change in time, let's say delta t. And then this patch turns, let's say, brown, so we haven't, changed our location of where we're looking at, it's just that there is a change here, not movement, but in color, from yellow to brown, and now we know that, the state is, ripe banana, not just banana, but a, ripped banana. Oh, yeah, is, That's, a little confusing example because bananas don't change instantly from yellow to brown, right.

I, guess you can argue that's a, behavior of a banana, but it's not, it doesn't fit the way I'm thinking about behaviors right now. You could have a sunset though where the color is changing relatively quickly. Or take something simple, you're looking at a stoplight. And it, it gets into red, the green, the yellow, right? That's, the kind of thing we're talking about here, right? Those are changes to the object. And if I was focused on, if a column was looking at one, one, light bulb in the stoplight, and it changed, it went from green to dark. I'd say, oh, that's changed suddenly. That's a change of that light, I'm saying. But the same mechanism should apply to this banana or sunset example as well, right? It's just that we don't see anything between the states. I don't, know if it does apply, we're really bad at noticing changes that occur slowly. There's all these examples of things that completely change. If they change slowly enough, we don't even notice it. Yeah, I don't mean we see the movement between states, it's more like we can recognize a ripe banana versus a non ripe banana and we know it's the same object. I, in this case, I would say, yeah, those are two states of the banana, but there's no way I can move between them. there are states that change, objects change states that we can't interact with, we can't, make happen.

and, they just, they exist in a couple of different states and that's it. Awesome. Yeah, in this case, the, state cannot be reversed. we can't go from right to unright. And so maybe, like a criteria here is that it's not that it just has to be fast but it has to be something that we can change states between. I don't know. I think it's possible to have different states of an object that you can't transition between. Okay. Take our mug, the coffee mug, with and without the logo. Okay. One of the problems we have right now is that to learn the mug with the logo, we have to learn an entire new object. We have to, there's no way, we don't have a way right now saying, oh, preserve all the mugness and just add the logo to it. We have to learn as a new object. This case, I could argue that the mug with the logo is the same mug, it's just, it has a different state. In one state, it has the logo on it, and in the other state, it doesn't. now I can't transition between those states. But I see it as the same base object. I say, yeah, it's the mug. I know how to work with it, all the behaviors I know about the mug apply. But if I, if it's in this, if it's in the logo state, then I know that I should predict the logo at some location. And, and if it's not in the logo state, then I don't predict the logo at that location. And everything else is the same. Nothing else has changed. I don't re anchor the grid cells. I don't learn new features. I just, those are the, so the state allows us to, I'm not sure if this is happening, but the state does allow us to represent the two versions of the mug very quickly and simply, and there's no way of transitioning between the two. Okay. Now, here's an interesting example, a counterexample. Viviane recently sent me a little gift, and one of the gifts was, a coffee mug that had the Thousand Brains Project logo on it, and, but it also, it was a mug that was black until you put hot liquid in it. Then it becomes white and you see the logo. And she sent me this as saying, oh, I was trying to think of a behavior that a mug might have. it fits into this definition, right? Because there is a mug that, it's like, the mug with and without the logo, but this is one I can control. I can put hot liquid in the mug, and therefore I can make the mug change color, and make the logo appear. that's like saying, there's no hard and fast rules here, right? There's objects can change. It's still the same object. Sometimes there's a, there's, they transition between the two, and sometimes they don't. But it, it gives me an example saying, instead of saying the mug, I have two classes of mug, I can say this mug has different states and I can have the same mug of, 20 different logos and once I know which quote state I'm in, then it's, then I know what to predict, but, there is no, there's no transition through the states based on any kind of, behavioral, action that might have occurred. It's just, it changed, it's a different state. it would still be right.

Go ahead. I was just thinking another example of a state change that could happen to a mug is if you had a spray paint can and a decal cut out or something, you could add a logo if you wanted. That would be a, that would be, I'm not sure that would be the, I'm not sure what that would be. That's a complicated. Yeah, where that would be modeled. But am I creating something? But I guess it's just that, yeah, there's a lot of different states based points you could be in. And I guess in some sense, as long as you. Okay, this is a new state space. What are the things we need to learn associated with it? now there's a different logo or whatever, or a different image. The other example I've talked about is you have a bunch of mugs and one of them has a chip in the edge, right? I see it on the shelf, I don't see the chip. I just pick it off, and then as soon as I feel the chip, I go, oh yeah, that's this one. And now I'll be able to recognize, predict the chip, but I don't have to learn a new logo. Nothing else has changed. I don't have to learn a new bug. Everything else is the same. I operate it the same, but whenever I go to that location, I will know there's a chip, there's a chip there. we have to do this. The brain does this. And we can't, we certainly don't want to learn the NUMBUG every time, we've got to have some variation of it. we need to have some way of saying this is the same object, but in the, but now I have a way of identifying it uniquely because it has some difference from the, all the other ones. And sometimes those changes can occur gradually, and sometimes they don't occur at all. they're just different states of the same base object, even though there may be no transition between them that I can observe. they would still be represented, I'd still have to represent them at two different locations in the state space up above. I had some thoughts on that, but I don't want to go into it. I was thinking about filling the mug and maybe that gets at a bit of. Where you were going, Hojae, where that there's like a, as you add liquid to the mug, there's a constraint that it's only going to accumulate in the mug. So as you pour in more, the level in the liquid is only going to get higher. It's never going to go lower until you start pouring it out. So there's a, there's a, sequence of filling constraints. let's talk about that. Could be modeled as states of the mug. that's an interesting example, because if you think about pouring liquid into something, we do that to lots of things. I might say, hey, I need to pour some liquid in something, I don't see anything available except a shoe. I'll say, oh, I'll pick up the shoe, I can put liquid in it, it'll stay there. And, so what's going on there? what I think is going on there is that the idea of pouring liquid into something, is represented in the upper layers here. it is a, a, behavior I can, it's something I want to do that's independent of the object itself. It's like adding liquid to something, it's something I've learned, and I can try to apply that to different objects. I can look at different objects and say, can I do that for this object? Can I do that for this? I don't know how this is done yet, but I think this framework I'm laying out here allows us to do that, where you can say, okay, the goal is to store a liquid, And if you're adding liquid to the strainer, it's not going to accumulate. So I need to, I can look for different objects, even ones I've never actually put liquid in before. But I can ask, and I don't know how this is done yet, but this framework will allow it to be done. So if the goal of the column is to store liquid, it's, it can say, can I do that with the current object I've had? No, maybe try another object, and the goal stays the same until it finds an object that it can do it with, and then it executes. I don't know how that's happening yet, but it feels like it must be happening, because we can solve those problems. At least having these two different spaces gives me the framework in which to ask exactly how that's done. so the object is in some sense independent of the behavioral states that are learned, but I can associate behavioral states with a particular object, certainly, but I can also change, associate the behavioral states to novel objects that I haven't seen before. And behaviors to them.

I don't, I'm sure how much more slides I have here. can I ask you to erase your blue there? Thank you. Yeah.

the other thing that I talked about before, I said different types of changes to define spaces similar to movement vectors. So again, this is like the types of changes you observe become the movement vectors of the state space, but they're not really movements. They're just movements through the state space, but they're not physical movements.

I was asking myself the question, in the lower layers, anchoring the grid cell modules defines a unique space, and we associate that with the object. Okay, so we're on object A, we're on object B. What would anchoring be equivalent to in the upper layers?

I don't know the answer. It's really hard to think about this.

but I I can speculate. I could say, anchoring up, maybe that would be like, the, anchoring on a set of behaviors that are appliable, applicable to certain objects. So it could be like, it could be like, now I'm in the, now I'm in a space of things I can do with liquids or now, I don't know yet, but we could ask that question because if it is true that there are grid cells modules in, grid cell like modules in the upper layers, they should exhibit the same properties we see elsewhere, which is, an anchoring To represent a particular subset of all the things that can be out there and also path integration, what does that mean? And, so we can ask these questions about the space that isn't really physical space, but it's state space. And I'm just starting to get through this, these, ideas are only a couple days old. literally I worked on 'em over the weekend and, and so there, there's a lot of movement, there's a lot of changes in the, there's a, it'll be easy to make a lot of progress on this in the coming weeks and months. Let me see if that was it. That was the last slide I had for today.

it was a lot. Yeah, that's very interesting, though. Yeah, nice, and I would be interested to talk a bit more, too, about how states might, or how behaviors might help us categorize objects as well, because I thought that was an interesting idea. I know you said there were some problems with it, but I think it could be Classes of objects? Solving something. Let me tell you the classes of object problem. I literally thought about a cup with liquid, and I said, oh, okay. I said, no, here's what I thought. I said, oh, I said, what if, imagine I have a whole bunch of different mugs, and, they have different shapes and sizes, some are fancy with curly Qs, and some are straight, whatever. I said, could we anchor the lower layer grid cells as a class? it represents all mugs. These are all things of a certain type, even though they're physically very different, and they don't go between each other. And then the state would say, oh, I'm on a mug that looks like a little dainty cup, or I'm on a big one that looks like, made of wood or something, I don't know. and then I say, okay, I could do that. It's pretty cool. But then I said, but I also might have a state like it has liquid in it or it doesn't have liquid in it. And that's independent of what it's more, which one of those, I can't use the state to say this is the, wood mug and this is the little dainty mug, and also use the state to represent whether it's full of liquid or not full of liquid, because I can't do both of those. I need a third variable. Okay, with a union of states?

what does the state, I, don't see how it could work, Niels. I just don't see how it could work. What does the state represent? It's either gonna, I just, I couldn't get it to work. Maybe you can get it to work. Maybe not necessarily that the state itself is the class ID, but more that, the fact that you can apply the same type of behaviors to a group of objects means that those objects are similar in some way and therefore might be classified as the same type. I think that's the right answer. Yeah. Quit holding objects. Thanks. exactly. That is probably the right answer. We say, okay, there's a class of objects that can hold liquids that you might drink from or something like that. And, but, and I would, and then I could say, this new thing, does it fit in that class? it's interesting, often an object can have two roles at once, right? It can be, it can look like a mug, or it could look like something else. Or you could, a chair is a classic example. You can show me, I can show you a big shoe, and then you say, oh, that's a big shoe. And then you say, oh, but it's also a chair. And you go, oh, yeah, it's also a chair. and so what, really what's happening there is, you're applying a behavioral model, to an object and saying, would it work at that? And then I can say, I guess that's in the class of chairs. So I'm agreeing with you. I think that's the right way to think about it. A class, that's it. Maybe that's the answer here. Maybe a class of objects is the, what you get when you, when you anchor the grid cells in the upper layers. It's like a functional definition of the thing. the class is a functional definition, and, It's not like each object is just in one class, you can categorize objects by different aspects, and Right, so I can associate the class, I can have a shoe, and associate class of shoes, or associate class of chairs. and I, or I can associate it with a class of pop art, or something like that.

it's the same model of, it's the same physical model of the chair, but I can, buck it into different sort of classes of behaviors or affordances, something like that. Those two are definitely. Something as humans, fundamental to how we think of things. there's a lot of philosophy around those distinctions. Is that right? We've been thinking about them for a while. Like philosophers think about this? Yeah, Oh, interesting. I'd be curious to know what they thought. I don't usually find much useful stuff out of philosophers, but maybe. I think it's, but I like it. This answers my question, maybe it answers the question, what would anchoring in the upper layers be? Maybe anchoring in the upper layers is a behavioral class. Things that share behaviors.

that sounds good, just saying the words. I don't know if it works well or meaningful, but it sounds good. Yeah, at least that's what your write up initially made me think, and I really like that idea. But that's not what I said. I didn't say, I didn't say, it's not the state that says, it's not like the anchoring of the grid cells defines a class of objects. The anchoring of the grid cells defines a specific object, There may have different behavioral states, but the, but it's not, but the state is not, doesn't define, the state SDR does not define the class, but maybe the state anchoring defines the class. Yeah. that sounds good. I like that. The sort of, the state behavior, do you want to, my take on this, and maybe this is me not understanding the sort of minicolumns idea, but if, we have the class of objects that hold liquids, do we want to restrict that to a minicolumn that only then represents a certain set of objects? Would it not be more useful to have that more generic? Remember, the minicolumns don't represent a set of objects. The minicolumn represents A common change in an object and therefore it becomes one of the basis vectors for path integration in state space. But can I apply it to any object? Yeah, Anything that observes that change would invoke the same minicolumn. And I guess where I'm getting stuck then is that not Like in the ca case of liquid holding objects is, the set of liquid holding objects too big to confine to a single small structure in the brain? Does it not be, it wouldn't be, it wouldn't be a minicom capacity. Mini, the minicom wouldn't define that.

the minicom maybe to answer as in also Michael, I guess it would be lots of different, columns that would learn this behavior. So like it wouldn't be, if that helps. it wouldn't be just one, column that has that behavior represented? No, the minicolumns don't represent, they have very, they have no direct correlation to any particular object. They just represent a type of change that have been observed in lots of different objects. and also, Jeff, any given column will also, it might learn some objects, but there will be many other columns that will also learn the same objects. And so the same would apply to behaviors. probably, yes, In fact, we might vote, one of the questions is do we vote on behavioral spaces, right? We want to vote on the object space, and, so this is an interesting, this is an interesting thought.

in this current scheme I proposed here, we would want to, instead of voting on objects in layer three, We would vote, maybe the voting on the object would be the layer 5b projection, because it's voting on what is the base space here, this is, the stapler. It doesn't matter if it's open or closed, we're looking at the stapler. And then the voting from, the voting in layer 3 would be voting on the state. what state are we in? Are we all agreeing this is the same state? Whatever object, you're, trying to figure out what object it is, I'm trying to figure out what state it is. These two are obviously going to interact with each other. But, it changes what we think about the upper layers doing. It also asks the question, what does the layer 3 output to layer 4 going up the hierarchy represent?

it's, it may not, it's confusing. if I were to, if I were looking at a compositional object, the thing I'd want to pass as a feature to the next higher region would include the state of the object. So if, for example, if. If, if I'm arranging stuff on a desk, I would want to say, oh, there's an open stapler or a closed stapler at this location. I wouldn't, that would be the model of the desk if it's always open or if it's always closed. It's not just that it's a stapler. anyway, it does beg the question, what, how would we interpret these upper layers? They might, it's different than we thought.

I'm just throwing that out. These are all. Brainstorming idea.

What I'm going to do next is I think it would be nice to review what we know about the connections between like layer five and layer three and layer two.

there's a, it's very confusing, it's inconsistent, but there's a lot of literature on it. And I've forgotten a lot of it. and that might give us some clues as to how these things interact with each other. how does the state space interact with the location space? and the other big question in my mind is, if I want to make a change in state space, how does that translate into movement? which will, the movement will always be an object space, right? and it's really confusing because if we think about the tasks that we humans do all the time, we often are using different modalities. I'll be, Like, if I'm trying to put water in the mug, I'm going to determine the state of the water visually, but I'll be pouring the water with my hand holding a, a carafe. And so I'm judging the state change visually, but I'm affecting the state change with somatosensory outputs. I find that really hard to think about, but we have to figure that out too.

Anyone else want to bring some regard to some other stuff?

Yeah, this was great. we're already a bit out of time, but definitely up for talking more about this next week if you make any more progress. I'll try. I'll try to make, I got a lot of crazy stuff happening in my life right now. but I'll be motivated to work on this, I'd like to work on it, but anyone who wants to chat or send questions back and forth, let me know, I'm happy to enter a conversation about it. But I'm definitely going to continue to work on it, because I feel like this is, I've been through this process many times, this feels right to me, even though we don't understand a lot of it, but the basic idea seems quite promising, and, again, it's not the first time we've talked about these ideas, but it's the first time I'm putting some thought Some detail on them.

so I'm gonna keep working on it.

Yeah, maybe just a follow up thought. It would be interesting to see what connections, if any, because I feel like often when we've discussed behavior in the past, at least with the kind of, in terms of what's happening at a cellular level, the focus has been on kind of sequence learning. So you learn the, state where there's a sequence of, yeah, kind of states. But, But that would happen naturally, right? It would happen naturally. Imagine I'm taking these, these bits from the retina that are low, they're sending them to the upper layers, so it's limited, and, oh gosh, I just lost my train of thought. What was your question again? It was about, so I guess the standard sequence memory is unidirectional. But here, we're more doing something that path integrates. Okay.

but, as in back and forth through the state spaces through different directions and depending on the action. It's always, like in the temple memory, it's very easy to learn sequences if they occur. So I'd have to be more precise, but the general idea is if something does repeat, consistently, the neurons will naturally learn as a sequence. it's a series of states.

occur over and over again, it will just say, hey, that's the sequence, or even if it's just a couple times, that's the sequence, I'm going to learn it.

it can even learn it very quickly in one shot, if, you wanted it to.

and so I think learning sequences is a natural, outcome of this, the way the neural models we've come up with work. And yeah, because in the end, the first time you do something, you got to think about it and observe it and watch what's happening and so on, and if you practice and practice, then you get second nature and you don't have to think about it at all, you just follow the sequence precisely.

I started to think about the stapler as like, where the generic version of it is a hinge, and it's in a, or Even more generically, a sort of constraint under some kind of rotation. And that is maybe the behavior you learn, and then you can apply it to a lot of objects. All right, I think so. It's I used to think about it like a hinge, right? It's got, in fact, if I think something might have a hinge, I will look for the pin that actually is, that affects the hinge, I would personally do that. It's oh, if I was looking at an object and I didn't know what it was or how it worked, and I said, how, I wonder if this lifts up the first, I could try it, but I also would look and see if there's a physical evidence of it. Yeah, or like the part lines that it's going to segment on, or I think you're right. I think we do that. And yeah, there's a behavior of hinging that we've learned, and we can apply it to lots of different things. My laptop hinges, and my, paper hinges.

And then once you make that assumption, then you can, then you have expectations how it will behave, right? And you don't necessarily have to learn every point in the space of a stapler being open or closed. no. Because the constraint lets you predict the novel states. also I think there's an ongoing issue we always have to deal with is the fact, the system has to interpolate between points that are learned. and we've talked about that a number of times, so you don't have to learn, in fact, if you just learn a few points in the trajectory of the stapler lid, the way I think the neurons work, and I've talked about this, is that it would be as if you learned every point. There won't be any difference. the way the SDRs work, it would essentially cover every point along the way, so you can learn four points, A, B, C, D, but any other point in there would act as if you learned it as well.

neurons can do that. I can go over that again if you want, but I've talked about it a number of times. and I've talked to Niels and Viviane about the fact that in Monty we're doing, we're using discrete numbers for representing locations and that always leaves a problem of interpolation between points. And we've come up with mechanisms to deal with that in Monty, but They're very different than the mechanisms that neurons use, and I've always worried about that.

in our implementation, we might say, oh, what's the nearest point that we did learn?

or nearest two points, something like that. And whereas the neurons don't do that. The neurons, it's like, it can't tell what points it learns. It just, it forms a continuous representation along a trajectory of SDRs. Yeah, I think what we discussed last time was like basically how we implemented it, it has the exact same behavior, like it can do the same thing, but it does it probably less efficiently. It does it less efficiently, yeah.

Yeah, it's dependent on the hardware implementation. Is that, do we ever, will we ever have to move away from that? I don't know, who knows, but let's not worry about that now.