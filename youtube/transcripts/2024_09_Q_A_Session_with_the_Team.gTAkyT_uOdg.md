Okay, I guess I'll just open it up to the floor if anyone wants to start asking the first question.

Sure, I guess I'd ask a question about, comment on this a little bit in the tutorial, but, using the different learning modules for pre training versus, continual online and, is the displacement, is, should we only really be using the displacement graph learning module during. Pre training, or is that something, or is it, would it be okay to just go ahead and drop in like the evidence based learning module at any point in place of the displacement graph learning module?

I think you can use both for the supervised pre training. Because if we do the supervised pre training, there's no inference being done.

all the, like the displacement and evidence matching module are both equivalent.

I think the reason we used to use the displacement learning module is because it would store displacements between nodes in the graph as well. we're not using them right now to recognize objects. Like we only use the points in the graph to recognize them and not the displacements, but we just thought it would be nicer to just store everything possible in case we ever want to use them or try different things. But yeah, it does seem like it might be an argument for, because yeah, we can keep the displacement learning module code. So we could get displacements if we want. It doesn't take that long to train the models. But I guess yeah, if we're not using the displacements.

A, the models will take up less memory on disk because, that's probably a fair amount of the memory is, storing all those displacements. And then, yeah, it probably would be easier for people getting on boarded if it's okay, I just need to understand the evidence learning module. The config looks simpler.

I imagine that would be, yeah, easier. Yeah. Yeah, I agree. Okay. Totally.

Hojae, you have a question? Oh, sorry. I was going to say I'm going to go to another room because I can hear Scott twice. Oh, okay. Okay. Yeah. Sorry.

yeah, I hear it now. It's weird. Yeah. we were here, but we're basically because we're on zoom separately. It's basically like we're distant.

Remote in the office. Yeah, remote in the office.

I know you posted a question, Will, about like drawing out how the learning modules and sensor modules would map out onto the body, basically. Yeah. Yeah. It was just like, I'm going to have as part of my role, I'm going to have to describe this to all kinds of different people. So it'd be useful for me to be able to say. What's happening in the human body as far as we know what our best guesses are, like what probably what information is traveling from the sensor to the thalamus to the cortex. What the cortex is communicating between itself and how it communicates, the various bits of the brain that are involved that we think of, and, what we think is going on there or where there's areas where we have no idea what's going on. We know something important's happening here, but we dunno what it is yet. And this is an area of active research and we, want help. that would be great. But I think that's gonna be, that's gonna take a, while to generate that. and the other one was like, was around, again, as part of my role is like comparing our technology to other AI technologies out there. And so you know how you go to a, like a SAS website and has its pricing page and you can compare against competitors. I was thinking of having a checkbox, one of those, one for Monty where it has lots of checkboxes, then one for concurrent neural networks, one for reinforcement learning, and one for open AI, and one for da. and in that checklist, An idea of when we think the other technologies might be able to do that capability because everyone's trying to get to this same place. when we come to think of what demo we want to build, it'll be useful to say, let's build a demo that it's going to take them two years to get to so we can spend our time doing it right. So those are the two things that are on top of my head.

Yeah, nice. Yeah, I guess in terms of the checklist, I think maybe I mentioned this to you once when we were ahead of one on one or something, but, but yeah, I started last year working on like a blog post, comparing deep learning, to the thousand brains, theory and, but yeah, it got put on the back burner and then just stayed there for a while. But I think the plan was maybe in October or November, like once we open source it, it would be a nice time to revisit that because that, might be a way to, to, Help introduce people to the thousand brains, but but yeah, but maybe we could, both stuff that's already in that and then other things, yeah, come up with a checklist like you're suggesting, that really emphasizes the differences. Yeah, I think one of the problems. The argument that we faced for decades in doing this is that, the AI world comes up with benchmarks. They tend to be benchmarks of things humans can do and are difficult and exceed the human performance. And, they they basically say, it doesn't matter how we do it. As long as we beat the benchmark, that's all that matters. And the argument for the thousand brains theory is the opposite. It's saying, no, the mechanism matters. and. And this is a very hard argument to make. A lot of people just don't really don't accept it. so I've tried different things over the years. first, one thing I've tried is clearly we can show that this system, the brain is working on different principles than these neural networks. And we can talk about those principles and we can say, because they're clearly different. don't you think that's going to be important to know those differences? And for the most part, people say, no, they don't care. the other thing which I've tried to use more recently. It's just really focusing on sensorimotor aspects. this is a system that works by moving through space, and it also is inherently capable of moving. That is, it generates movement, and it also requires movement to do learning and inference. And that means moving its sensors through space. So almost all other learning systems use feed in data in some form, with a stream of words or tokens. You feed in label data and your assumption is feeding the information it's going to train on. But sensorimotor systems and brains learn by exploring and moving through space. And I think that, that argument might work better, for some people because it's easy to come up with a lot of things that we learn that are important that you can't learn unless you move. And we'll, I would never say, oh, we're going to beat, we're You know, a chatbot in high level language composing poems, it's that's not going to be our thing, or doing math, but we need to focus in on systems that, a sensorimotor, and they could lead to very sophisticated robotics, but they can also lead to these desirable learning attributes, where we don't have to have, a data set to train on, the system can explore the world on its own and, and learn, and it'll learn, and by doing so, it'll They can learn new things that no one else has ever learned before. So it's a tricky, I admit it's a tricky thing, but I've never ever had any doubts that studying how the brain is going to work is going to be the true AI. but lately I've been focused on the sensorimotor part. Yeah, and I like this idea of this comparison table. I think the only problem with it will be that a lot of what our system will be able to do is like a concatenation of ands. So basically, if you just take one category, like recognizing objects, you will find better expert systems that are good at this one particular task. But then if you concatenate it with, but also from very little training data and in a continual learning fashion and self supervised, And in a sensorimotor setup and under noisy conditions, then it becomes unique and you don't find anything else that can do it. So it will be tricky to show this in a comparison table in, in a fair way. Yeah. Unless we emphasize that by it'll almost be like a diagonal. Yeah. I'll have a, we'll have a fully green column and then there'll be like a diagonal of all the other ones that are like point solutions. Yeah, But I guess, yeah, there's a risk that people are just like, oh, it's trivial to combine them or something. And the deep learning people or the transformer people will say, we'll get there. We're going to, we'll figure out how to train these systems to do everything. And they believe that. And I never really want to tell them what they can't do. I don't want to sit there and go, oh, you won't be able to do this, you won't be able to do that. But deep down, I say this system works completely differently.

can I throw out another idea here? This is the idea I use at the little, very short Stanford talk I did recently, where I, and you can watch it someplace. There's only like eight, ten minutes or something. where I point out, the key thing about AI is not, what it's, what it does, but the key thing is how it represents knowledge about the world.

And, so the sensorimotor learning systems, as in brains, build a structured, three dimensional model of the actual things in the world by exploring them, whereas a chatbot really has no sense of the three dimensional structure of things in its models. Its models are not like that. They don't have reference frames. They can't move. And so you can really build an argument about how, the nature of the internal models really dictates, what it can do and the power of it, and, Monty and the sensorimotor link system have a very different internal model of the world that actually captures the structure of the world. That seems to work for some people.

Yeah, I feel like there, there could almost be two different comparisons. One is how is knowledge represented and what principles does the system work on? And there we would again have very few comparisons and it would be like, all right, we have sensorimotor learning and deep reinforcement learning has that too, but all the other ones, but that one doesn't have the structured models and all the other properties of the system. And then we could have a second table with like capabilities where it's like what Niels just said, that Monty would have the whole column green, and then we have a bunch of alternate systems where each of them have one cell green where it's particularly good, and I guess that ties in a bit With the paper that we want to write about Monty capabilities as well, like what do we compare to, and I feel like for most comparisons, like if we want to compare Monty to an alternative system, we need to compare on at least two dimensions, like number of samples plus accuracy or something like that.

yeah. One of the, we talked about doing some videos. to introduce the thousand brains project, and, I imagine one of the introductory videos that I at least considered doing would be along the lines of what I was just saying, this idea that, it works differently, this is how brains work, and then unfolding that and seeing what the attributes are you get from that, and that would help you in that education process as well.

I also want to throw in, if my marketing term, like you could try is also data less. It's, there's a certain context in which that is true. I'm just thinking of how serverless became this polarizing force that wasn't ignored and everybody had an opinion, whether for or against. And so if you, another, maybe data less would be a similar hook. What do you mean by that exactly? Oh, it's whatever you want it to mean. That's the beauty of the term. Okay. it's you don't need to gather all the data. The system will, if it's a sensory motor system, it'll gather its own data by interacting with the world versus forcing the thing. it's, training data lifts. It's, because, people could argue oh, but we send our kids to school and they read textbooks. that kind of stuff. So there's some of that's going on with humans. But mostly you're right, mostly no one takes a baby and says, here's your training curriculum, And it will make people argue, which is the point.

I flash images of different emotional states at my babies and different types of people, on, the TVs so they can learn. Cause we live in a place where there's only white people. And so I was like, all right, we have to, I have to expose my kids to lots of different faces. Otherwise they're just going to grow up weird.

Do you want to, do we want to take a stab at your second part of your question, about the sensor, about the sensors and how they go to the brain?

yeah, we could put up like a little whiteboard and try and draw a sketch of it.

I was, so excited to use my new iPad as a whiteboard, but I'm just not up to getting it going.

Yeah, I think I have mine downstairs too. Okay. we could talk about it.

I think I shared the whiteboard, but it just Do you guys see anything? Ah, now it's coming up. Yeah. Showing a whiteboard. All right. I'm not sure what you're going to put on there. So I'm just going to draw like a stick figure because that's. All I can do with my mouse pad. Oh, you're not using your iPad. Yeah, I would have to go and get it. I don't have it at my desk right now.

this is gonna start looking weird once I draw a face on it.

And then maybe we have a zoomed in view on the brain.

Pretty good on a mouse pad, Viviane.

Actually pretty good. This is not really zoomed in, but That's how I feel right now, that person.

All right, I don't know if I should start just talking through how I'm thinking of how it maps or, yeah. Here, I could do it. I don't care if you want to do it. All right, so let's say we have a hand which has the sensors.

So what I'm basically, so basically the sensor module would be a patch of skin on the finger, which converts like the actual information that goes to the sensor, like pressure and temperature into the cortical messaging protocol. So in this case, it would convert it into a, spike train that then travels to the brain, and we would go through the thalamus where we can, okay, this is going to get a bit crowded in this small view.

Maybe we can just talk about it from here. Yeah, We would have learning modules in the visual, sorry, that would be somatosensory cortex if we're doing touch. We would have learning modules everywhere and each of these learning modules would be connected to one patch of retina or skin or higher level learning modules and they would send some rotation hypothesis down to the thalamus and that might modify the signal before it then goes. into the learning module.

You, are you disagreeing? No, Okay. I'm tempted to throw my own words in. Yeah, go ahead.

the way to look at the body, every part of the body has sensors, every single part. So this, and so every part of the body has a patch on your back and the toes, and they all send up to a cortical column. The difference is that some parts of your body have high acuity, right? So The area that is, if cortex, if I take a column in the cortex, the learning module, the area of the fingertip is going to be quite small that it represents, or the area of the back of your back could be several centimeters. So it, they're all working the same principle. They're all taking sensory input from some patch of skin, and literally in the brain, there's this, there's a map of your body, right? They literally can, if your body's laid out up there in cortical columns, But it's highly distorted because the areas represented on each column can vary quite a bit, so you have, if you, they call this the homunculus, so it's like a little picture of an, of your body and you're on the surface of the cortex in the somatosensory cortex, yeah, but the hands and the lips are really big because they have a lot of high acuity, and you could say they have higher sensors, but it's really that the column is representing a smaller area of the skin. And so that's basically all parts of your body, then the same way, exactly the same thing, and you have a complete representation of your body, in the somatosensory cortex. The vision, the retinas are the same thing, there's a lot of, it's a patch of sensory, tissue, and it's also represented in the visual cortex, so a column in the visual cortex represents a patch in the visual, on the retina. But here, too, it can be highly distorted. The patches in the center of the retina are very small and have high acuity, and the patches that are, in a column in the visual cortex representing the perimeter of your visual system, or your retina, it can be much larger. if you saw people draw pictures of what it looks like to show an image on the retina, the image on the retina is not distorted, but when you get to the cortex, it's very distorted. It's like a crazy fish eye lens of all kinds of nonsense going on. no one looks at that. It's not like someone looks at that image anymore. This fooled a lot of researchers for a long time. And the same thing with the auditory cortex. It's a little bit harder to understand because you have an array of sensors along your cochlea and each patch of the cochlea gets a column in the cortex as well. and then the general rule is anything coming from the sensory organs has to go through the thalamus before it gets to the cortex, and, the visual system, the retina, can go right from the retina to the thalamus to the visual cortex, but like auditory and somatosensory generally stop along the way and get preprocessed, so information comes from the ear, it doesn't go directly to the cortex. It gets processed in another brain region, the inferior colliculus, then it goes to the thalamus, then it goes to the cortex. So the old, old parts of the brain do some pre processing on the inputs, but less so on vision than in, touch it does and in hearing it does. and that's the general rule that all information goes through the thalamus to the cortex, and that's why the thalamus is sometimes called the gateway to the cortex. But now we know that when information goes from one part of the cortex to another. Part of it also goes through the thalamus, so that, it's not just oh, this is how information gets into the cortex, it's also how information goes from one region of the cortex to another region of the cortex, so I don't know if that helped explain that. yeah, it does. Oh, there you go, that's a beautiful picture, that's the classic homunculus picture there. Yeah. literally, if you think about the vision, it's in the back of your head, right? It's the very back of your head, but the body representation shown in this picture, It goes across the top of the head so you can, those are the blue and the, orange or blue and color purple colors there. And, so you can see roughly how much each column, what's it representing. you can see like the hands are really big and the face is big, but the rest of the body's p.

It sounds like that's built in when you're born, but you change size as you, grow, as you get older. It is built in. It's, although if you have an early, trauma, or even later in life trauma, these, what these columns will represent, can change. for example, if someone were to, the damaged part of the cortex, representing the hand there, then other parts of the cortex would start taking over some of what the hand, what was handled in that part. So the map can shift around a bit, surprisingly. Yeah, where if someone's blind, their visual cortex can be taken over by, if they're born blind. that can be repurposed. So yes, it's genetically determined, but it's also very plastic, it's determined. So now this represents. there's good parts, the big parts of the cortex, these are the largest areas of the cortex, are for vision, primary vision and secondary vision, V1 and V2, and primary and secondary somatosensory cortex, and, the auditory cortex isn't so big, but there are, of course, many other parts of the cortex that aren't, directly associated with sensory organs, and so they're part of the hierarchy, and, the purpose and how they operate for most people is, Much more of a mystery because it can't say, Oh, I'm going to probe the skin, this cell lights up.

why is it that the vision doesn't have any pre processing? Why is it different than the other sensors? Oh, that's a great question. no one knows. We can try to figure that out. first it's an interesting clue that it doesn't require it. Now the retina does do a lot of processing. It's a very complex organ, so it's not like you're just getting these cell photo cells that are going right to the cortex. There's a lot of processing happens in the cortex, but you can think like in, in auditory cortex, for example, we have a sense of where sounds are coming from, right? We, when we hear a sound, we know where it is, in our 300, around our head and how far away it is and so on. the timing differences, the way that largely it's done is that there's very, small timing differences between the sounds arriving in your left and right ear. Those timing differences are well below what most neurons could process. And one idea is if you just sent those ears to the cortex, it wouldn't be able to do that. But there are these highly specialized neural centers that are designed or evolved to tease that apart, to get the location information. before it's sent to the cortex. So that's an example of why you might pre process, something. there's a lot of literature on all this, tons and tons of literature on this, but it's nice on the vision sense in that case that it makes it easier for us to, perhaps think about vision system. We don't have to necessarily do as much complicated pre processing.

Okay, so just to echo back what I had, the journey of the information from the sensor to the brain, so you have, your finger touches something and the nerve essentially sends back like feature data. There's no location, nothing yet. It's just this is rough or smooth or hot. there's location data in the sense that It goes to a part of the cortex representing that part of the skin, right? It doesn't get put in a big pile of data and someone has to sort through it. There's myolingo, that's like before it's part of the cortical messaging protocol, like it doesn't have that pose, at pose yet. Like your finger might be at a different location relative to your body and that information needs to be integrated somewhere. Also, if you're rotating your finger around a point of contact. Like on an edge, you'll get different sensations on your finger, right? At the point of the fingertip itself, it doesn't, know too much, right, it has to get, I'm not sure what I'm, The brain knows it's coming from the fingertip, and, and, But I guess. What Will is getting at is that at some point we need to integrate like proprioceptive information and information about how we moved our muscles to know like where the finger is in space like relative to your body right now and I guess the question would be where exactly that information would be integrated.

one of the, if you think about vision and audition, the information comes from the retina doesn't tell you anything from a patch in the retina. It tells you about nothing where it is in space, really, or where it is on a relative to an object. so the general principle is that the brain has to be fed movement vectors, not location data. I used to misunderstand this. the most important thing is, let's say the retina senses something, a patch of information, and then it has to know how the retina is moving through the world, and, and it's a combination of Moving, movement data updates, through path integration, it essentially eliminates hypotheses and Viviane and Niels could talk how we do this in Monty, but, it's not like the, it's come, the information comes in the retina, it doesn't say, oh, this is on the side of the coffee cup. It doesn't know that. internally, there's a model, there's different models, and it's going to take a sense and move and sense and move, and it says, oh, what are the only things that are consistent with this, these series of sensations and movements? maybe it's a coffee cup. And then, you're on the coffee cup, and so every movement from now on can be path integrated to say, here's the new location on the coffee cup. but there's no, direct information from the retina saying where it is in the world. Touch, is a little bit complicated because we do have this proprioceptive system, which provides some information where your fingers are relative to the body, right? but they tell you, it tells you nothing about where things are relative to objects you're touching. Okay. Again, that has to be inferred through a series of touches, and sensations. So is it true to say that Sorry, go ahead, Viviane. You can go ahead. Okay, so is it true to say that if you didn't have any models in your cortex yet, vision wouldn't know anything? Like it would be just, there'd be no way it could figure anything out until you have the models in your cortex already? If you didn't have any models in your brain, nothing would make any sense at all. Nothing. You'd have zero idea of anything. And so the only way to learn models then would be through some, one of your sensors that has location information about. Where it is, so it can build a model, then your vision system could use that to The most important thing is for it to, to know how the sensors are moving. You could, from scratch, take a fingertip And it has no models at all, but as long as you know how the finger is moving through space, as it's touching stuff, you could build up models.

it's, you can just, it's just like sticking your finger in a black box or some odd shaped thing in there, you've never touched it before, and as you start moving your finger around on it, you start building a model of it. I don't know if it's maybe a useful example to contrast with, like smell, at least in humans, which is quite like a simple, Almost like model less, way of representing things because there isn't much kind of movement information, at least for humans. maybe it's different for dogs, which can, sense direction of wind and sweep their noses lots and, do a lot more kind of movement through space. But basically, all you can really build without movement is similar to what you have with smell, which is just you kind of sense something, and then you can associate it with something else, but it's just like a vague cloud of information. It doesn't really have any structure to it. This is dangerous, and this is good. dogs, you mentioned this, Niels, dogs, they stick their nose on the ground. They stick their nose on objects. to them, the nose is like a really great finger, right? They know how it's moving through space, and they can build structured models of the world through smell. I think they also get a sense of almost like smell flow in that, I think their nose is wet because it, basically detects the direction of wind by which point is drying out. Interesting. Something like that. Yeah. I know they can tell the direction of footprints. They, know which way you're walking based on the smell of your footprint. So they know the shape of it. So this is an interesting example, right? It's a great example to bring up because for humans, smell is a pretty poor sense. And we don't really build models based on smell, very, we can associate several smells with different models and different states and models, but on its own, it's not a very good sense for us. But for other animals, it could be a very good sense because they have a much better sense of movement and flow of direction and movement and so on. And they have better sensory detectors, like the direction of the wind and so on. all these work on the same principles. The learning module does the same thing. that's, the mind boggling thing about all this. it just goes to the cortex like every other sensory organ. But you could still, going back to if you have no models and you only have vision, you could still learn as long as your eyeballs can move, right? You would learn something. You can learn as long as your sensors can move and the brain knows how the sensors are moving. Literally like they're moving in some direction at some velocity. That's all it takes to learn. And you have some sense of distance based on, stereoscopic vision and other cues. that's a challenge for vision, right? We can talk about that. but, even people with, one eye can learn two, right? so that, that issue about depth is something we've never really addressed in our work. I don't think we have, Viviane, some of you have, but, I don't think we ever do. But it seems like we use a lot of techniques to detect distance. Primary one of them is, I forget what it's called, when you move your head slightly left and right, even with one eye, you can tell the relative motions of things in the foreground and the distance. Parallax. Yeah, parallax, thank you. that seems to be a real, a really important one, because you can do that with one eye.

and there's a bunch of tricks that the brain uses for this. but we've never quite really dealt with that. It's much easier to just say, oh, we're sensing on the object, we know we're on the object, now as we move, we know how we're moving relative to the object.

Yeah, I think we said that maybe some of the extra layer for subtypes that are found in the visual cortex do some of this like depth inference processing. That's purely speculative. Yeah. I said it, so, that's another little trick that if you guys aren't aware of it, cortex is very, uniform. One of the biggest exceptions is the primary visual cortex. In some mammals, not all mammals, in some mammals they have what's called a striate V1, which means it has extra stripes, extra layers, and no one really understands what they're doing, and we've just wildly speculated that the thing that's really unique about vision, it has to do with these issues of sensing at a distance, and so maybe they're involved in sensing at a distance. Like they're doing comparisons left and right, doing the parallax thing. It's a reasonable hypothesis, but we haven't really explored it.

But by the way, some animals, don't have striate B1 and they seem to see pretty well. Like I think dogs don't have striate B1 and they still, they'll see, they don't see as well as we do perhaps, but they see.

I guess I have a question to throw out here too. So related to this, like transmitting movements versus locations. so right now in Monty, we are actually communicating locations in a common reference frame. So like location relative to the body, and then the learning module infers, like calculates the movement between two successive locations. And we've been saying so far, this is pretty similar. It just changes where we're calculating the relative movement. and when we communicate that. But there's one crucial difference. That's one place where we really need this location in a common reference frame, which is voting. So when we vote, we need to vote on locations in some common reference frame. And I don't see how that can work if we're only getting movement information.

yeah, we also have to vote on orientation and we haven't. You haven't said that either. Yeah, so there again, we would need the orientation in some common reference frame or orientation relative to I'll take your word on that. I guess in some ways, is it mostly the orientation that matters? Because with the vote, there'll always be some displacement, or normally there's some displacement between the two, like what the two learning models are seeing. So if you pass literally the displacement, as long as it's oriented correctly Doesn't that kind of, I have two fingers moving on an object if they want to Yeah, I guess how would you know? Yeah. Yeah. How would you know what the displacement is? If I have the location of each finger relative to my body, it's pretty easy to calculate the displacements between the fingers and use that for what, but if I only have how the fingers are moving in space over time, then I have no idea how they're located relative to each other.

this is the argument, perhaps, for the Why there's a, there's these two maps, object centric and ego centric. I can't think of the words. what's the word for, that pathway? Like the where and what pathway. Allocentric. The where and what pathway is a formal term for it. The where and what pathway, right? So there's these two separate processing streams for, apparently for all sensory organs, at least for touch and vision. And, one's rep the columns are representing space relative to the body, somehow relative to the body, and one's, Representing space relative to external objects, and so that would fit into what I think Viviane was saying. You'd need the second one, you'd need the egocentric space, because if you have that, then you can determine the relative position of the two sensors. Is that right? Yeah. Yeah. Have we done anything like that? No. right now we're just assuming that all learning modules have access to that information.

that's something that, that's something that's a reasonable, we probably could, you might be able to do that with just pure, brute force engineering as opposed to trying to model how the brain does that, right? Yeah, I think we've often argued with the allocentric or like the what pathway or whatever, it would. As long as it, even if it's temporary, as long as they agree on a particular allocentric reference frame to use when voting, then it doesn't matter too much. And I think there is evidence of that. It's almost like anchoring to like a, an object in the environment. And then you're gonna do calculations relative to that. So like, in here, but that might just be an extra complication for us. Yeah, basically, take example, Vivian had the two fingers, right? And you wanna know where the two fingers are relative to each other. In a biological system, that's really complicated. there's all these noise and these joints and all this proprioceptive information. but if you had a robot, It's a simple, it's, a basic engineering problem, so why bother to do it the way the brain does it, way we do it as a mathematician, as an engineer we're doing.

Yeah, I guess that's a, to me, it's a pretty important question for us to just figure out because it determines the cortical messaging protocol, whether we're sending locations or displacements. And that's the one place in the cortical messaging protocol where I think we might make a bigger change sometime in the future from locations to movements. In case it's not obvious to anyone, some of you, this coding has to occur. And it's not it's like, there's so many mysteries about the brain that were resolved by, by just understanding that each column is doing its own sensorimotor learning. And yet we have this ability to do this flash inference, right? You can flash an image in front of you. You don't have time to move your eyes, and you can do limited object recognition that way. I can flash an image, and you say, oh, that's a boat, that's a car, that's, that's Abraham Lincoln, or whatever. And there's no ability to move your eyes, so somehow, every, all these models are getting just one impression, and they have to reach a consensus, and, and so we know it has to occur. and so the idea of voting came up under that context. For a long time, vision researchers just ignore the fact that the movement was important because of this idea that you could do flash inference. They say, movement can't be important because I can, I don't need movement to recognize an image, a limited recognition of an image.

So we know it's happening, we just have to figure out how we want to model it. I have a question. Yeah, oh, go ahead. regarding this, because we're talking about a body oriented reference frame, and yeah, we can engineer physical things, but. Are we able to engineer the corresponding body centric reference frame in abstract reasoning in space? do we know all the consequences of using an engineering substitute for this? no, we don't, but we also don't know that happens in abstract spaces. I don't know what the equivalent of flash inference is for abstract spaces. there may be only something that happens in touch and, touch and vision.

That primarily happens there.

When it comes to the abstract reasoning stuff, all we know is that the cortical architecture is very, same. It must be working on the same principles. and we know that's sufficient. but it's really, lots of other parts of it are mysterious.

Nice picture, Niels.

yeah, sorry. This was just while we were talking about the, common communication protocol. So Yeah, Jeff, Viviane and I were discussing the other day about, some motor policy stuff and this kind of question came up about displacements versus, kind of location, because, so like one of the policies we've recently implemented is this kind of like moving back on an object policy, so you would be like sensing this wine glass and then moving along it, and then let's say you make a saccade to the mug, And this isn't in the straw world, but you you sense, okay, the evidence that I'm getting is now for a very different object. So I'm going to move back to, where I was until I recognize that first object I was on.

Are you learning the object, or are you? This is after you, objects, But if I've already learned the wine glass, wouldn't I know that moving towards the mug is going to be off the wine glass then? Okay. Yeah, so that'll, that's part of it, but it's also, I guess assume for this point that you're not certain it's a wine glass yet, because you've only gotten some evidence, like you, I don't know, you've traced out a little bit, but like suddenly you're just, yeah, getting, so then the question I guess, or yeah, and so there's various ways that we can communicate to the learning module, let's say it's a, whether it's at a scene level, or it's like at an object level, like it's monitoring, it's modeling the wine glass, It knows it's now very far off of, yeah, it's model of the wine glass, to your point, or like it's most likely hypothesis, it's moved very far away.

if it's going to propose to move back, it has to communicate somehow to the eye, to it needs to go back to this point in space, from like here, where the eye is currently looking. After the saccade, and it's just yeah, how we want to, I don't know, represent that. It seems a little, it's, I think the details here really matter. maybe a more general question is, we have some top down kind of goal from the learning module that's representing objects or scenes that's saying, I want you to move to this particular point in space from where you are right now. I guess this is one example of why that might happen, but. I'm still confused by this. Yeah, if I've model, if I have a model of the wine glass and I know I'm on the wine glass, then I should then, if I move off of it, I know moving off the wine glass, I shouldn't expect to see the wine glass anymore. but if I wanna go back to the wine glass, it's an interesting question of oh, I had to have some sort of temporary memory that I was, looking at the wine glass. If I could invoke that temporary memory, then I could just say, oh, I can go look at any part of the wine glass. let's go back to the wine glass and look at the stem. if I can just invoke the memory of that, I was just sensing a wine glass at this position in this orientation. it's, I could go anywhere, I don't have to go back to where I was. Yeah. It seems to me more of an issue of like temporary memory, like I, I was looking at one object and now I'm looking at another object, I have, I don't want to forget that I was looking at the first object. Yeah. Yeah, so I feel like that's where maybe like the scene representation helps. That, that would be presumably have, a representation that, okay, the wine glass is here in space, and that doesn't change when you then move on to a different object, where the low level learning module might be like, okay, I'm now on a new object, so I'm throwing, I'm forgetting about wine glasses, I'm now thinking about mugs. let's, not call it, for a moment, let me just not call it scene representation. How about I just call it Okay. I have temporary memory of the things I just looked at. Yeah. So this, is, this is something we haven't really dealt with much, but The vast majority of our lives, we're going around looking at things that we know, we recognize, but they're in new arrangements. so you're walking through, down the street, you, enter a shop, whatever, there's tons of things around you, and you, immediately, you just continually focus on different objects all around you, and you're building up, you could call it a scene, but you're building up a temporary composite object. You're basically saying, oh, right now there's this cup on the counter, and there's this bin over here on this desk, and I don't know what it is. and you have some temporary memory of this, so that if you left the shop and you come back right in again, you'll remember all this stuff, but a day later, you've forgotten all of it. so it's more it feels like some of these things could be addressed by having, and we, of course, we think a lot of this happens in the hippocampus, in the hippocampal regions.

Yes, having temporary memories of composite objects. I, if I'm recognizing the wine goblet here, and I jump over and I recognize the coffee cup, I, built, I'm building up a composite object of the wine glass and the coffee cup, that's only going to be good for a little bit of time, and therefore, that's how I know where to go, I can just go walk through this composite object saying, okay, let's go back to the wine glass. I know what it's, yeah. So I, I guess then, yeah, so we know the representation, whether it's in the hippocampus or whatever, the, temporary representation of this compositional, arrangement Yeah. Knows where it wants to go on the object.

now it needs to communicate that in a useful way to, the kind of motor system that's actually going to carry out that movement.

And so For example, one way we thought maybe it could work is if it's not going directly to a subcortical thing, let's say it's a fairly complex movement, or maybe this is happening subcortically, but basically if it's communicating a location in egocentric coordinates, then that, like the eye can basically, if there's something that's modeling where the eye currently is in kind of egocentric coordinates, like But the eyes at this point in space, so this, whether this is subcortical or cortical, it's like an eye model. It doesn't care about what it's looking at. It just cares that it's looking at something at this point in space at that distance, and then it's receiving sub signal.

it's basically that common, the cortical messaging protocol question in a different application. The learning module knows where to move on the object's reference frame, but how to communicate it to the actuator, to the finger, to move to that location? We would have to communicate the location in body centric reference frame or in the finger's reference frame. maybe not. these are hard things to think about, but again, I'm thinking like You have a wine glass and then, let's say you're just looking at the wine glass and now you're looking at the cup. In my mind, you've built a new temporary composite object of wine glass, cup, whatever else you looked at recently. And now the question is, okay, given this object, this composite object that has a bunch of features, I want to go from one location on the composite object to another location on the composite object. That's, that's no different than moving from the rim of the cup to the, handle of the cup.

it's the same thing. These are just features of the object. And, of the composite object.

I just, if we can solve how to If I can solve the problem of how do I go from a particular point on the rim of the cup to a point on the handle of the cup, I think you've solved the problem of how to go in from the wine glass. That's fair. Yeah. That's maybe a simpler way to frame it because it is the same issue, but I think it is still a question. It's just a question if we communicate a location and body centric reference frame that we want to move to, or if we communicate a displacement, like a movement that we want to make. So I think it's a displacement. I'm not sure. This is what I think it is. To me, I feel like it's like you don't have to go to body. The biocentric coordinates.

you've got a movement in the objects space, right? I know. I want to go from the rim of the, of the cup to the handle. That is a allocentric movement, right? It's go this direction from here to here. and then all I have to do is run that back. Do run it. Take the. Take the orientation of my finger, or the eye, or whatever, and, then calculate what that is in body centric space. It's it's like the idea of, like, when you're reading text, we talked about, let me tilt the text, I, you're, still reading the text, but it's moving in a different direction now, but you're still moving in the same direction on the text. that would be the way I'd approach it. I would just say, we do generate movements in the allocentric space, and convert them to the egocentric space based on the current orientation of the text. of the sensor to the object. I'm not sure if that would work in all cases, but it seems like the first place, it seems like Yeah, it does feel like displacement would be the most natural. I guess the only, and this isn't a complication Can we use the word displacement? What are you meaning exactly in this case? Yeah, communicating, or, to your point, we can simplify this, and just Is a direction in the allocentric space, is displacement, how to get it would originally be in allocentric coordinates, but because it's a displacement, we would want to, as long as it's oriented and scaled correctly, it could be interpreted in another space. So I, the way I look at it is we got two points on the cup, or we have two points, one on the cup and one on the wine glass, doesn't really matter, it's the same thing. and, we have to, calculate, at that point, maybe what you're calling displacement, is a direction and distance in the orientation of the object we're modeling. Yeah. And then, yeah, and I guess, so that, that would be, something like this. Then I guess the one kind of complication is like something like the eye. If it wants to move to this place, obviously the eye isn't going to make this exact movement. the eye is going to rotate in the socket in order to enact that displacement in kind of perceptual space. Why are we rotating the eye in the socket? I don't want to do that. Why would we do that? Because we want to now look at the new point. We don't rotate, oh, not, oh, yes, we want to move it. Sorry, yeah, that's what I mean. We move the eye. Yeah, alright, yeah, I guess it is really But I guess I'm trying to move away from the example from the finger, because the finger feels a little bit more like we're still moving in a similar space, because, the finger is going to do a similar movement through space, but the eye is going to be moving through perceptual space. But the movement of the eye itself is very different, so it feels like there needs to be a structure that, that knows how to do that mapping. I don't see, I'm missing the difference. With the finger, how I move my finger from one point to another depends on the orientation of my finger, right? Yeah, okay, fine, I was oversimplifying the finger, but I guess my point is, the eye is definitely, a very different type of movement. To me, the only reason the eye is different is because it's doing it at a distance. Yeah. I talked about that earlier. That's the thing we haven't really dealt with. it's I have to, there's an extra level of calculation or something has to happen there because, I have to know how far away my eye is from the object to know how far to move the eye. Exactly, yeah. Yeah, so I'm curious if you feel like that is something that you might almost have like a. Learning module that's modeling eye space that is doing that and is receiving, or if it's a subcortical structure that's doing that. I think the only thing I need to know is I have to have it, somebody has to keep track of how far away this object is from my face. I'm looking out right now, I'm looking at, boats, and the further boats, looking at a boat far away, I have to move my eye less, and if the boat's closer in, I have to move my eye more to scan from the left and right of the boat. So somebody has to, we have to keep, and so we have to, take account of how far away this thing is, which is actually the same as keeping track of the, which is a flip side of the scale, right? I associate distance. I say, oh, it's far away or close, but it's also the same as if it was a larger boat and a smaller boat. so those are intertwined. It's like there's a scale issue, which is tied into distance. But I don't, I'm not sure why we need to have a separate modeling of eye space or something like that. It's just. It just feels oh, I know which direction I have to move the eyes, the only question is how far do I have to go? And that is an issue of scale, and scale is tied to distance, so I don't know how to tie those together, I don't know how the brain does that, but it's pretty simple. You just, have to know the scale of the distance, you have to have a scale factor, you have to have a direction, a distance in allocentric space, like the distance on the boat or distance on the cup, and then, you scale it for how far away it is. or what size it is right now. Does that make sense? Yeah, no, I think that's an interesting point, the connection to scale. So yeah, basically as long as you have the orientation and the scale, then it is just a distance, or just like a normal displacement. But somehow we, this is an interesting question, I don't know if we've ever really talked about it, but scale and distance are obviously tied together, but not always, right? I can have a small cup and a big cup right next to each other, and so there's a different, and they're the same, and they're just different scales. Or I can have the same cup from a distance, and it's the same as the small cup, but I don't see it as small anymore. So somehow the brain, I don't know, maybe it's in those extra layers in V1, it's keeping track of the distance of the thing.

which we don't really have in touch. We do have in vision, auditory, we have a sense of distance and audition.

but I try not to think about audition too much, it's too complicated. yeah, maybe we also, I don't want this to turn into like a brainstorming meeting. Yeah, I'm sorry. that was probably longer than. Oh, it's always so fun to do that. A chance for the newer people to ask some more questions too. Sorry. Oh, no worries. this is also very useful since Yeah, thanks. That was really useful feedback, actually.

I have thousands more questions. I can take over the whole meeting if you want. Go ahead, because we were taking it over. Yeah, okay, I'll do, I have a hippocampus question. And from my understanding, the hippocampus is like this, building scenes temporarily, so I want to keep track of what's happening right now, and I'm going to use models I've built to quickly populate that scene. So if I go into a coffee shop, and I look around, I can say, okay. There's a human over here. I've seen humans before. There's coffee cups around. There's a machine over here. I have models for all this stuff in my neocortex, and now I'm going to use that to build a scene in my hippocampus very quickly, and I'm going to remember that for a day or two. But if I go into that coffee shop lots and lots of times, then slowly that whole coffee shop It becomes a model in my neocortex that I can then use and go back into it. So in, for the coffee shop I went to two years ago every day, I can still go back there in my mind and be in the coffee shop and then remember where all the objects were and all that stuff. what's, I guess my question is, what's happening there? Like, where's the information going back and forth? Like, how do you think? I'll take, I'm still thinking about this. It's all right if I take this one. Yeah, so the way to think of the hippocampus, and this is a simplification, but the way to think about it, it's like the rest of the cortex, but it's just super fast learning. Okay, so everywhere else you have to form synapses to learn. In the hippocampus, you don't. They have these things called silent synapses, which are all these extra synapses, tons and tons of extra synapses that aren't doing anything, so you can turn them on in an instant. So think of it that way. It's the same mechanism that's occurring in the rest of the cortex, very similar. This is a simplification, but very similar, and yet it's really fast, okay? So that's a useful thing to have, right? Because I can just quickly go, oh yeah, look, all these things, I build it up. People used to think that memories were transferred from the hippocampus to the neocortex. obviously, as you experience something over and over again, like your coffee cup you used a long time ago or the house I grew up in, which I haven't been in 30 years, I still have a map of that, right? somehow it gets transferred and it becomes more permanent. To the cortex, but they used to think it was actually transferred. But the mo, the I read a paper that said, no, it's not actually transferred. It's learned again in the cortex, meaning if you're exposed to it over and over again, it's learn. It's built up slowly learning in the cortex. So the cortex can't learn it very quickly, but if you expose it over and over again, it can learn slowly. And so you have this fast learning mechanism in the hippocampus and a slower learning, learning mechanisms elsewhere in the cortex. And, And no information is being transferred back and forth. It doesn't work like that. Does, replay during sleep still help to relearn it in the neocortex? there's so much literature on that, it's really confusing. I don't have an answer to that. people were looking for that. They were looking for this, oh, how is this information going to get transferred from the cortex, from the hippocampus to the cortex? That was the theory. And this whole replay during sleep was like, oh yeah, this is when it's transferred back. I don't know, it's really confusing, no one understands what sleep's about. but I do know this other paper somehow, I read it, don't ask me what it was, but I read this paper, it was pretty definitive saying, nope, it is not being transferred, it's being relearned on its own, independent of the hippocampus. Which makes a lot more sense to me, because I don't know how you would transfer a model from the hippocampus to the cortex unless you do this replay thing, but then what are you doing, you're training it backwards? I don't know, it makes no sense. So it makes a lot more sense is if you're exposed to something over and over again, it gets learned in the slower neocortex. And this, also fits into the thousand brains theory like, what models are learned where, because there's various resources that, you know, resource constraints. maybe I'll just leave it at this. I've speculated, there's evidence when you highly train something, like a musician, you train them over and over again, that the memories they use to perform are, move down, further down in the cortex.

that, instead of, if you're new to music and you're just trying to play, it has to be processed very slowly, it has to go up multiple levels of hierarchy, or you're going into the hippocampus, whatever. But a musician, it's all, it's been relearned, and it's in the lowest levels of the cortex, and no thinking is required, it just, happens, it's all in the lowest levels. And I've made this, I've made the argument, like when we, read letters or words, a lot of that, if you're a good reader, a lot of that occurs probably in V1. which is contrary to what most people believe, but it's such a highly trained thing. That we don't have to look at the letters and think about it anymore. We just recognize the word and we just go right through it, so where things, I guess I'm trying to say where things learn. The first time you learn anything, it's probably in the hippocampus, and then you practice, and practice, and then it's re learned elsewhere. And the things you practice the most, and are highly, learned, are in the lowest levels of the cortical hierarchy. I have a question following that is, because there's procedural knowledge in a lot of professions that people do it all their lives, but they will forget it as soon as they stop participating in that environment. this is why we have checklists and other things. How does that play into what you just said? I don't know. Did people forget it right away? yes, for example, pre flighting an aircraft or like procedures for flying instrument flights. If you're flying an aircraft, if you're doing it every day, you remember all the rules for the airspace and who to contact it. But if you step away for a month or two, then you start forgetting as to what the sequences are of like, how are you supposed to execute it? People, if you take somebody away from flying for two years, they will not, they might not be able to fly. Remembering how to, what procedures to execute in order to enter a controlled airspace, for example, that does just go away. I, think in general, it's, pretty obvious. We forget everything and, our memories decay about everything. So neurons are forming new synapses and we're getting old ones all the time. So I think the fact that you forget something is not really surprising at all. even something like my house, I made the example of the house I grew up in. My memory of that house now is far impoverished, from when I grew up there, I can only know the very basics and maybe even that I have wrong. Maybe that airspace example is also a bit easier to forget because it's such abstract knowledge. So you can't really learn a model of it in the, lower levels. Yeah. And also getting it right is so important. I feel like it's, similar with like surgical checklist, like often they're quite basic, yeah. Counting how many sponges went into the patient and how many came out. people can remember to do that, but it's really, damn important that you do it correctly. you make sure you have a checklist. Whereas, someone, who hasn't ridden a bike is a classic example for several years is not going to be as good at riding a bike as when they did it last, but they can still vaguely remember. I'm sure, Tristan, you could fly a helicopter. So the thing is, I do have, I have the motor skill, just like riding a bike. I have the model skill of, I could execute helicopter flight, but I don't know, the sequence of how to turn the engine on. that's gone. Alright, we can speculate on these things, but it may not be that fruitful. maybe that's a long sequential memory, versus, Our ability to remember things, we forget everything, but maybe some types of things like sequence memory we forget more rapidly. I don't really know why. It doesn't bother me. I think the bigger question for Monty and the Thousand Brains Project is, real brains are always forgetting, right? And there's reasons for that. We have biological constraints on how much material and the energy required, the volume of our head, and so on. With Monty, it may be that we don't have as many constraints. Maybe we can just keep adding neurons, or adding synapses, or adding dendrites, or changing the learning rules, and just keep, so the system doesn't forget, hardly forgets anything. and, that might be possible. so there's, there are things in biology we don't want to necessarily emulate. And, I don't know yet whether it's really helpful to get things or just necessary from a biological point of view. Yeah, I think there's definitely different speeds at which we forget different types of information. Like from what I read, at least that, for example, children, one of the last things they learn to store in models is where they learned specific information, like who told you this information. And that's also one of the first things you start forgetting as you get dementia or older, you forget like where information came from. and So it seems like there's at least, the type of information, has some different rates of forgetting. We also forget names, of things. I have a, theory about why that is, but yeah, there's different types of forgetting. one thing you asked earlier about the hippocampus, it might be helpful for everyone to have, a visual picture of what's going on in your head. it, if you remember the, inner cortex is a sheet, right? It's a sheet of tissue about three millimeters thick. And it goes in and out of the fold, and then it goes around the side of your head, and it folds up into the inside, inside underneath the other parts of the sheet. at the very edge of the sheet is where the hippocampus is, and the enthorhinal cortex is. It's a continuation of the sheet. The sheet starts, stops looking like prototypical neocortex, with all of its layers and cells. It starts degenerating to something, it's a little bit less, a little bit, it's different. It goes to a three layer structure. and, and like the enthorhinal cortex is sometimes called the three edge structure. But imagine, you got this cloth, this sheet, and it's getting, it's now not looking the same as it looked before, it's getting thinner, and it folds back onto itself. So the hippocampus is lying on top of the enthorhinal cortex. and one of the, one of the speculations about evolution was that these are older structures. And the enthorhinal cortex and the hippocampus, and literally what happened, you had the enthorhinal cortex and on top of it was folded back to the hippocampus, that became the six layer neocortex. And, and then from there it starts being prototypical for the rest of the sheet. So it's you might have started with this sort of wrapped over thing of two, three layer structures, that became the six layer structure, that becomes the cortex. So it's, a continuation of the same, this theory, it's not my theory, it's just someone else's theory, but this theory says that the neocortex is basically just a, better engineered version of the old stuff. it figured out, hey, let's just tune this up and then we'll make a lot of copies of it. and the fact that the two are aligned on top of each other, literally how's the cortex became, the six layers of the hive. I hope that's helpful, and I hope that, It makes the argument that these aren't really, they're not really different structures. It just, nature preserved them in a different form.

I have another question, but I'll Go ahead. Yeah, okay, so you're building a scene in the hippocampus, very fast, like instantaneously, but then you need to, there might be uncertainty about the scene that you're building here. we think this is a glass, we're pretty sure it's a glass when you put it over here, but Does the hippocampus control motor action then? Does it have a say in, like, how you're going to move, your sensors around?

I am not aware of the literature on that.

I would say, I would guess, that if the model of the scene, which is just a composite object, if that model exists in the hippocampal complex, then the hippocampal complex will have to direct the behaviors relative to that model. Who else is going to do it? you need the model to direct behaviors. So if I walk into a dining room and I quickly notice where all the, the dishes on the table are and I sit down and I want the, green beans, and I just, now I know which way to reach to get the green beans, right? I think that would have to be somehow directed from the model in the hippocampal composition. Who else is gonna know how that is? but I'm not, I actually don't know anything about the literature. Maybe Niels does or Viviane does. I don't know anything about the literature in terms of how the hippocampal complex directs behavior. Yeah. I just did a quick Google and it says that the hippocampus shows motor activity and another article says recent evidence has shown that the hippocampal complex is involved in the generation of ML behaviors and overnight consolidation of motor memories. Yeah, that's all very true. But I would expect, just in the rest of the cortex, these layer five cells direct the motor output of the cortex, I would expect, you would find the equivalent somewhere in the hippocampal complex. It's probably known. I bet there's something that Ali can, the work from the researcher Ali Pasha last week, would he be able to reach the hippocampus and do an experiment there? would that be interesting? I, think I asked him that question, didn't I? Yeah, I can't remember what you said, how many millimeters he can go. I think it was hard to get to. I think he said he has to be intrusive. Oh, that's right. Yeah, you had to put a thing in so you can get the light, But I can't imagine that it wouldn't have motor output, I feel like it, it has to. it has to. If that's where the model exists, that's where it has to be. Yeah. No one, you need a model to generate motor output. I seem to remember, when we were looking at one of the, Shuri and Gilliam papers or whatever, that, the, some of the L5 outputs from like prefrontal cortex was to the hippocampus. Kind of implying that, the motor output of prefrontal cortex could be, like, modulating memories, which I thought was interesting. or I could have, I don't remember that, but I would have, wouldn't that be, wouldn't there be an alternate interpretation? Remember, the motor output of every cortical column, those, layer 5 cells, goes two places. It goes subcortical to generate the motor behavior, and then it also goes to the next higher cortical region, the next higher cortical column. So wouldn't the prefrontal cortex, its motor output going to the hippocampal complex would just be the feedforward motor command, like everywhere else. It's, that might be the same as layer 5 cells, going to the So as in the hippocampus is the higher region than It's just the higher region. That's where I look at it. I just view the hippocampal complex as this It's a bit wonky, it's got, it's not as regular in structure, just as cleanly as the rest of the cortex, but it does basically the same functions. And, and so learning a scene is just learning a composite object, that's all it is. And anyway, but the motor output from prefrontal cortex would presumably go someplace subcortically, and also to upstream to the, hippocampus. So if we can explain why layer 5 cells project to the next higher region, then we can explain why it's projecting to the hippocampus. Yeah, that's a really good way to think about it. yeah, it's definitely true. We also need to have short term memories of more abstract things, Yeah, and the hippocampus is not just seen in analysis. They've shown it's yeah, short term memory of almost everything. Yeah, that's where you build programs, I imagine. That's where you're building software is in the hippocampus, like short term memory. You get interrupted and now you can't remember any of it. You fall off your bike and you can't remember anything. That's too soon. Oh dear. I had a real concussion this last Friday. I don't remember this, but I was sitting on the ground at the end and it's like yelling at me, Jeff, so I, you're there, I'm like, didn't even hear, I didn't know I had broken my arm.

Anyway, more questions? Yeah, in that paper that said there's no, that there isn't transfer from the hippocampus to the neocortex, that's not what REM is for, that, what does it say, what was its position on, if you, if the hippocampus is damaged, then you can't form any new long term memories. Did they have an opinion about that?

I don't remember. I read this paper a long time ago, Will. I don't, I can't cite it. I don't know the author.

alright, so there's this basic idea. If you remove the hippocampus, You can't form any long term memories, new long term memories, right? So there, that was evidence that there was, that, like this is the famous patient, where they took out both of his hippocampus and then he got stuck and couldn't remember anything there.

I, that seems contrary to the theory that I just told you that was, like, memories are not transferred, and I believe that paper had an explanation for this, but I can't remember it. Let's see if we can recreate it. Why, if you don't have a hippocampus, you can't form any kind of, temporary new, composite memories of anything. and why would that, I'll leave it at this. If you remove some part of the brain and just cut it out, willingly, all kinds of shit happens. Who knows what's going on, right? What the hell is REM sleep doing? We don't know. No one really knows what REM sleep is doing, if you interrupt it or if you cut these other lines, I don't know. there's a dozen reasons why, if you remove someone's hippocampus, they can't form long term memories, other than it doesn't, have to be that the members are transferred from hippocampus to the cortex. It could just be that there was cabling going through there that also, yeah. there's all kinds of, that's like the crudest thing you can do. Just chop out some tissue. And, it's, it's surprising that the thing works at all, I guess I, I don't, I wouldn't lose sleep on that one.

can I introduce like an abstraction to understand Monty a little bit better? This is more about sensor modules and maybe it will touch on the benchmarks a little bit. and maybe about the whole model of the eye, maybe why it's important. It is or is not needed. It might be related.

we can maybe think about humans as like hatches of sensors, right? Sensors are everywhere, receptors. but we have, for example, photoreceptors only in the eyeballs. as far as I know, I don't have it on my skin because I don't think my skin can see. And let's say, like proprioceptors in muscles. So we have like different distributions of receptors across our body. and, but, for Monty, and that's, and we're all constrained, like our eye, like the photoreceptors are, their movement is constrained because they're connected to muscles and joints and bones. So it cannot have a full, we can't rotate the eyeball backwards, whatever. like the, Bones and muscles basically gives us the constraints, but for Monty, at least the Monty that we have now is completely virtual, it's not a robot yet, which will also constrain movement, but technically the Monty can have it. Just a single, I'm not sure if I'm using the right terminology, let's say like a single patch of finger, but it doesn't need to just have, Merkel cells for pressure, it can technically have, photoreceptor, pressure, temperature, proprioceptor, da, in I think we have a Monty surface that is not just touching, but also seeing that Niels pointed out.

so I guess with this kind of, technically we can make like a super nose Monty that can move the nose everywhere. I don't know. And we'll make a, build a model. I guess with this kind of because we're not, I guess this goes into the benchmark. if we have those kinds of. Sensors everywhere, like how, I feel like a lot of the benchmarks in AI are how humans are doing, but technically, I find it a little bit hard to think about, benchmarks because I think technically we can say like we can put sensors, in and have them move around everywhere, in lightning speed. You mean like when we're comparing the agent versus the surface agent on the benchmarks? let's say, more generally, let's say, more generally, let's say we have a distant agent that's just the eye that only has the photoreceptors. I think that scenario will be closest to object recognition and kind of classification in AI ML. but, yeah, but, but yeah, go ahead. one of the key ideas here is that we're building a very general purpose sensorimotor modeling system. So we're not trying to build a human or simulate human sensors and movements, but it should be very general. we could imagine a learning module, learning how to navigate the web by clicking on links and stuff like that. So yeah. This is kind of part of the key idea that we could have superhuman or supernatural, sensors and movement, and it would still be possible to, learn that and model it with a learning module. But then, yeah, related to the benchmarks, I guess we don't have a benchmark for what a superhuman or kind of a super agent with all the sensors would be like. The problem with existing benchmarks is that they're picked to work for the technology that people already have and the tasks that they assign themselves to. And often those aren't the best benchmarks for us, we have all kinds of new capabilities, so I've learned to get very defensive about benchmarks, because I was like, oh, I don't want to do these benchmarks, we ought to design a system that's so damn cool, it works on these principles, and we'll figure out, we'll find, we'll define our own benchmarks that apply to sensorimotor learning systems as opposed to benchmarks that people have done in the past. Labeling images or doing whatever. I'm, I just, that's a general rant about benchmarks. Existing benchmarks aren't our friends. They are the friends of the people who created them on the system, for the systems that they designed. and I think we have to prove, Monty is, different and better, in different ways than most people think about them today. I hope I'm not too different. I think that, yeah, I think that, yeah. I agree, yeah, yeah. moving forward, so we have polar sensory modules, there's, just getting into the code, we have a, habitat distance sensor module, habitat surface, for each, distance, do we want to go down one level and kind of create generic classes for different receptors, like the photoreceptor class, temperature, thermoreceptor class, or appropriate receptor class, and Combine these kind of sensors into an agent that the classes that we could use that we could put into an agent. because I, yeah, I guess right now like the we have distant sensory modules, we have surface sensory modules and they seem to both possibly get touch and vision, but maybe we need to separate out the Distance, whether it's on or away from the actual sensors. So the distant one doesn't get touch. there's not really a sense of touch in any meaningful sense at this point. Yeah. I don't know if you saw there was like, in general, I think the view is like the distant agent could get anything that's like electromagnetic radiation, like any sort of wave propagated information. So electromagnetic radiation and sound. But the general idea is like once, especially once we open source this, anyone can take any kind of sensor. You can take ultrasound, you can take whatever sensor you have, you can take some abstract sensor on the web, whatever, and just write a custom sensor module for it that takes the raw sensory data, converts it into the cortical messaging protocol. So that, and, that way. Anyone can plug in any sensor into that system and they can plug in different sensors into the same modeling system. You can have a self driving car with LiDAR and vision and all that. All those sensors can have their custom sensor modules. They all convert the information, the cortical messaging protocol, go into generic learning modules, and all these learning models can boot with each other. So that's the idea behind it. you can build the sensorimotor, you can build the cortical learning modules in any kind of arrangement you want, in any kind of density, and hook them up to any kind of sensors. It'll work. yeah, we're actually at the end of this meeting and, yeah, Niels already wrote he has to go to the next meeting.