Okay. Yeah. I prepared a few slides.

but yeah, I'm trying to keep it short and yeah, hopefully have more time for just discussion and getting input from everyone. So the goal of today's meeting is to show the kind of computations that we're doing in Monty, especially the ones that are taking a lot of time and then seeing if. The mega team has any ideas of how they could be sped up if we can maybe leverage some of their work, if we can maybe leverage sparsity or any other techniques that you know of. And then maybe at the end of today, we can also think about if we can, if it would make sense to have some more Mega, Monty cross team meetings and if there are any synergies between the teams.

yeah, sounds good. Yep.

All right. Yeah, as I wrote. No one needs to have any prior knowledge about what is actually happening inside of Monty. but I'll just give a quick, yeah, outline of where we're at right now and what's going on in relation to runtimes and the kind of operations we're running. So at the moment, if we have a learning module that knows about 10 objects, it takes about 0. 1 to four seconds to perform a step. a step is basically getting one observation from the environment. And then using that observation to update the internal state of the learning module.

and in order to recognize an object, you usually need to perform many steps. Is it in serial or is that in parallel? Are you running on multiple CPUs? I'll get to that in a second. Okay, great. And will you also explain why there's such a big range from 0. 1 to 4 seconds? yeah, so it depends, on how many hypotheses are being initialized. for example, depending on how noisy the observation is and what part of the object the observation is on, it initializes different amount of hypotheses and then different amounts of hypotheses need to be updated at every step.

then, yeah, if we have, five learning module, that number gets multiplied approximately by five. Here, I don't have a range because I just took the average from one experiment right now. but, yeah. On average, they are about 2. 5 seconds per step. If we know about 77 objects, it gets a lot slower because now we have to update evidence for every of the 77 objects. So about 1 to 15 seconds per step. And for five learning modules, it, again, scales by about a factor of five.

now, given that we have to take multiple steps to recognize an object, when the learning module knows about ten objects, you usually have to take less steps because there are less objects to disambiguate. so it takes about 0. 3 to 3 minutes to recognize an object. If we have a set of 10 distinct objects, if we have 10 very similar objects, it can take up to 15 minutes to recognize an object, which is quite long. And then for 77 objects, really depending on which object we're looking at and how similar it is to other objects and how ambiguous the viewpoint is it can take between one to 86 minutes to recognize an object, which, yeah, I think we can do a lot better.

I feel like we also need to do a lot better if Monty should ever be applied in the real world. To recognize an object in less than 86 minutes. Like the length of a soccer game, right?

Yeah.

And, yeah, I'm sorry, just to say, are these times, I know you said you're going to talk about it later, but are these times single CPU, or is this is this Lambda using whatever CPUs you want? Yeah, these times are from running on Lambda with 16 CPUs. All right.

Okay, just, quickly about the terms. so we have steps, the, that was the first numbers I showed, so one step is getting an observation, updating the internal state, and then moving, and then you get the next step. we have a variable number of steps in an episode ends, for example, if we recognize the object, or if we recognize that we don't know this object, And then, we have a number of episodes within an epoch, which in our case is always going through the whole data set of objects once, and then we can run multiple epochs, for example, testing different orientations of each object.

steps in a typical experiment. First of all, We loop over episodes in an experiment, and this part, Ben actually nicely parallelized, so if we're evaluating, we can run every episode in parallel on a different CPU on the Lambda node, that doesn't work during training because When we are learning, it matters. The order matters, basically. But since right now we're mostly evaluating, this helps speed up that part. And then while we haven't reached a terminal state yet, we first go over all the sensors that the agent has and collect the observations from these sensors. And then we loop over the learning modules and perform a matching step for each learning module. this loop could also be parallelized, but it isn't right now because, mostly because no one has done it yet and we thought it's not the highest priority because we don't have as many CPUs as episodes anyways.

just wasn't the highest priority yet. And then for each matching step within one learning module, we have to loop over. The objects in memory, and for each object we have to update the evidence given the observation. The loop over objects, for this we use multi threading, and then the evidence update is vectorized, so it's mostly large matrix multiplications. Then the learning module sends outputs, that's, Yeah, an output of the object it thinks we're on, it's pose, and then a vote and a motor command. Then we look again over learning modules, receive votes, could again be parallelized. And then receiving votes again has to loop over objects and update the evidence using, The vote. So here again, for the loop of objects, we use multi threading and then the evidence update is factorized. And then finally we check whether we reached a terminal condition. getting the sensory observation takes about 0. 01 seconds, it's pretty fast compared to the rest. this is the slowest step, updating the evidence for each, learning module given an observation. that takes about 0. 01 to 0. 05 seconds per object. if we scale that by having to do it for 77 objects and five learning modules, it adds up to about five to two to five seconds, with multi threading, it takes about one second. and then, the update using votes is, a bit quicker because we don't use as many, we don't, we threshold the votes to make it a lot faster. overall it takes about 0. 05 seconds with multithreading. If we would, if we do use all votes and don't threshold them, it is a bit slower than, the first update.

and then overall, taking as many steps as we need to recognize the object, it takes about one to eight minutes, for episodes where we don't time out.

So how many steps is that in general, usually?

it's usually between 20 and 200 steps. we have a maximum of 500 steps, which is, when we reach time out.

yeah, so since we already have these outer loops paralleled or use multithreading for this presentation, I'll focus on the evidence updates, which are the slowest part of the code at the moment. is this clear so far? Yeah, very clear. I had one question. when you say multithreading, what mechanism are you using for multithreading?

we use Python multithreading, the map. fullmap or something it's called.

I'd have to check real quick.

yeah, we use, this. Threading the thread. Okay, so That is not preemptive.

Okay, so Is this using Python threads then, Luis or Kevin? Yeah, this is Python thread, that's not preemptive threading, that would be Yeah, so these are not real threads. No, there'll be a lock and a guilt. Yeah. Yeah, so that's why I was asking, because at deeper levels, you want to distribute things. You can use affinities and various things, because Basically, at any one CPU, you only have hardware, direct hardware support for two threads at a time. Anything more than that is basically put into queues. but if you're using Python threads like, Luiz just pointed out, it's not, it's not the, it's not the most efficient mechanism possible, for the hardware. It's only running on one CPU. I think it's not even real threads. Correct. It's, it's yielding. It's yielding the Okay, so when they say they're running on multiple CPUs, how is that being distributed then? the, episodes is running multiple processors.

Yeah, so this is the parallelization part here, each, yeah, each episode is its own process, and then within that process we do the multithreading. It seems to be doing something, the multithreading, right? Because it's up to five times faster than without it, right? Yeah, it's definitely faster. So it seems to be doing some goodness.

It can't be one CPU, though. it depends. I think we need to wait until we see what's going on under the hood, right? Because the difference in multithreading will be impacted by different things, right? So depending on what the operators actually are. The advantage of that thread is there's some I. O.

because it's, going to do yield to Gil and Gil is going to wait just for the I. O. There's nothing else to wait for. So let's look at the, I think you were going to talk about what's in the actual code, right? And then we can back up a level if, that makes sense. Yeah.

okay. So if we actually run a profiler on one episode, so in this episode we have five loading modules, and this is looking at the, this is not using multithreading just because the profiler doesn't deal well with that. so it's a bit slower than it usually would be. but anyways, it looks at the time taken by the top 15 functions. And, As you can see, the longest time is taken by the KD tree search, which is the first topic called, is that Python, KD tree? is it like, is that put in Python? Okay. No, it's it's using C, yeah, sty. Okay. All right.

Yeah. So basically today I have basically two, two topics of things that. I think could be sped up. One is, yeah, KD tree research. And then the second one are the Everything else, yeah. Everything else, exactly. So yeah, all the matrix multiplications, all the like evidence updates that are vectorized.

and then there are a few other ones that are mostly like rendering the observations in Habitat. and adding an object and things like that. and this one, I think, I can figure out on my own.

so yeah, let's jump in on the first one if there are no questions for now.

Yeah, I've used KDtrees a lot. mostly in C implementations, so I can take a look at that. I don't know how SciPy implements it. usually SciPy implements a lot of their stuff with C on the back end. And it's just a Python interface. I'd have to look to see how that's being done. the SciPy one, I checked on, and we upgraded SciPy, so we used the C back end. So there's a, we have to create a version of SciPy so we could use the C back end. I guess it's probably a pretty decent implementation. Yeah, so I guess one question would be if there's actually a, if KDTreeSearch is actually the best way to get what we want or if there's maybe a better What are you using it for? What are you using it for right now? That's the next slide. All right, cool, got it. is Einstein, like Einstein summation, or is that like an eigenvalue summation or something else? Yeah, I'll also show that on a slide. it's, yeah, basically doing a dot product with three dimensions.

So yeah, let me just, go ahead. so yeah, first of all, the shapes and sizes of the matrices that we're working with, just as a primer for the next slides. So we have the number of points in the model which is usually around 500 to 5000 depending on the size of the object. We have the number of hypotheses which is between two and eight times larger than the set number of points in the model. We have the number of nearest neighbors that we look at. usually it's either 5 or 10. We have the number of features that the sensor module is detecting. it's usually between 5 and 15. And then we have the number of incoming votes, which, if we only take the highest percentile of votes, we have about 1 to 200. if we use all votes, it can go up to 200, 000. Which was then, a lot slower. Wow, it's like a small metropolitan area. I don't understand how the incoming votes can be that high. what's the logic, how they get that big? it's basically, all the possible hypotheses. So if we have 4, 000 hypotheses from five learning modules, it's, yeah, 40, 000 times five, incoming votes for possible poses. And that's like in the worst possible scenario. The worst possible case where everything is equally probable or likely to be happening. And the hypotheses are, the hypotheses of the locations on objects, so it could be the same. there's a limited number of objects, there's a lot of places. Locations and pose, right? Yeah, so that's why hypothesis is like two to six times larger than the model, because for every location on the model, we have two to six possible poses. got it. Thank you.

Okay, so Finding the nearest neighbor. what we do right now is use KDTreeSearch. Basically, the problem we're trying to solve is, we have an array of search locations. so in this picture down here, it would be, given two hypotheses, the two gray arrows. We would have two search locations, the green dot up here and down here.

H would be number of hypothesis, in this case two, and they are in 3D space, so we have XYZ coordinates. And then we have model locations, which would be all the black dots in here. that would be N, and then again in 3D space, so XYZ coordinates. And we want the IDs of the k nearest neighbors of these search locations. so for example, if k would be three, we would want to have the ID of these three points in the search radius. so what we want to have returned is of shape H, K.

yeah, maybe I'll pause here for a moment. All right. So KV tree typically works by doing a spatial bifurcation, right? You take half of the elements and put them in one side, half on the other, and there's like a dividing plane. Anything that's X less than this, or X greater than that gets separated into binary tree. And then you do the next one on Y and the next one on Z, and then you repeat. Yeah. And it's like an arc tree decomposition, is that correct? Yeah, so we basically built this location tree at the start of an experiment. so we, already know the model of the object in this case, because we're just evaluating. So we built the KD tree once, and then at every step we just query the tree with the hypothesis locations. So you can stop that decomposition at any stage. Like you say, if I want to go until I have like only 10 leaf nodes left, right? And then I can say, I can search through, that, or I can continue to decompose until I have only one object on each leaf node. that's one of the optimizations you can do in the KD tree search is you can terminate it early. If you want like a whole bag of things that are local. they're nearby. Is that during building or querying? that's, that would be during building, That would be during the building phase. how many points? That's like how it's structural structured. Yeah, so the building is not that big of a deal because you only have to do it once. It's more the querying that's, querying, yeah. No, but it would be a parameter you set during building that would affect the speed of the query. Yeah, so we have leaf size set to 40. Did you try to vary that to see if it speeds things up? Yeah, so if I said lower, it was lower, but yeah. How many points do you have in that again?

So N is around 500 to 5, 000. Can you just, guys, I'm going to ask the stupid question then, right? Can you just use a lookup table?

We've got a bunch of memory on this thing, right? We've got two terabytes, right? Can I just use a lookup table? So the search locations are not identical to the locations in the model. So they, they're just like nearby. before we use KD2 search, we just did the difference and then linear norm and then the minimum. And that was definitely a lot slower. Yeah, you could do what Lawrence is saying actually.

yeah, basically just create like this huge you disco, discretized the space Yeah. into the mesh and for, into a small mesh. And for each voxel you, you have your neighbors have the list of Ks. Yeah. And 5,000 elements. Even having 40, keeping the list of 40 is a small tree, right? Yeah. And then you just, so you don't store any mesh at all, is, is that on there's no connectivity information between the adjacent points. Is this a list of points?

Building a mesh would be one way of speeding that up a lot because for each point that you register, or even if you're detecting like a point like in between, stored points. You can still figure that out pretty quickly by connectivity. Yeah.

So you have the graph. Yeah. I think Lawrence's idea is a good one actually. When, would you create that list? After training, at least instead of the tree, you just replace the tree With the tructure? Yeah. You replace the tree at the star, and that is basically a single, you do your training and then you build a tree and then all your inputs is based on Yes. Because it's static, so it won't be really simple. yeah. You've got a bunch of memory, right? Let's use it, right? You just have to figure out how to index it. Indexing is the tricky part, right? But as you say, if you know what your parameter space, your sort of spatial space is, you just put it up, chop it up into bits. Yeah, actually, the other thing that you can do is, there are actually images representing each voxel. You can just round it, and that's it. yeah. Yeah, I think it'll be hella fast then, right? The other thing you can do to accelerate it using this implementation you've got right now, a quick and easy thing you can do is to cache those, those nodes. So if you're only moving a short distance on the object for each step, you can reuse those lookups and go straight to the leaf node to see if your, nearest neighbors are still in the vicinity. Yeah, but Eric, I think Lawrence's idea would. Take out all of them, there'd be no tree to begin with. Exactly, but I'm saying like, if you just wanted to implement something just to accelerate what you've got right now, just to see if it makes any difference, it's just going to cache the tree nodes that you're finding in the KD tree search, and then just basically reusing those until they no longer are valid, and you pop back up one node in the tree and go back down to find the neighbors. I'm not sure that would help. That. That assumes you're gonna look at the same note over and over again in sequence. I'm not sure that'll help. and I'm not sure you even have access to that. it's a, look, it's a locality. It is it a locality based caching basically is what you're doing. Yeah. If you's a cod, then you're gonna see disparate stuff is the problem. It's not gonna be a continuous, it depends on the length of this cod. You're right, Yeah. But also another thing I thought of. Yeah. But also it implies changing the KD three implementation of site. And I think that's it. I don't think I want to get there. So just so I get it right, what you suggested learns, you said to discretize the space into voxels and then basically just use those as a lookup table whenever we get a new point. Yeah. See if that looks one at the floor. That was, that's what I said, I just said lookup table. That's what that was. You know the point you're on and you just look up all the adjacent points. Yeah, but you'd have to, within a, the would be, you just say, within this resolution, I don't care. Yeah. I don't care what, you lose that much resolution. Oh, you're not in, you're not in, I know which ones are close, which ones you wouldn't. Information too, whether it's 1.15 inches versus 1.16 inches. You might actually get different nearest neighbors, but here you would still get the same. So it would be a slightly lossy. But you could, that would be okay. How would that scale into the future? Do we want to worry about that now? I wouldn't worry about it, yeah. You can go, I'm going to look up terror by summary, right? Ha 5, 000 bodies. You can like, stay on the ladder. At the point it starts, at the point it starts to fail, you can start hashing techniques. Yeah, you can hash as Well, the good thing about, I was trying to imagine always, let's say we're scaling something to human brains, but, the learning modules themselves don't continue to scale. they don't go to maximum. Yeah. And to make bigger brains, you need more learning modules. This speeds up the operation of a single learning module, right? So, my question, I've answered my own question. If you wanted to be really, big brains, this wouldn't be a limiting factor. No, and I wouldn't be surprised if neurons do something like this. That's essentially what they do. I was trying to think that. How does this relate to neurons? I haven't gotten there yet, but you may be reaching a conclusion. Yeah, I think it's very similar to what neurons do.

That makes me happy that my neurons are doing lookup tables. That's basically what an SDR is. Within some resolution you match and then you do something. That's basically what we're talking about.

We're solving both your problems. We're solving two Monty problems. We're making it more neural.

Yeah, this is great. Has anyone seen an objection to this idea? No. I mean, there might be a couple of other things algorithmically I could suggest, but if you're happy with the lookup tables, that's probably It does. I think it's gonna be really hard to beat lookup tables. Yeah, it's one of the rules. Yeah, if you've got the bandwidth for it. It answers the interview question. Yeah. The only thing I would add to that is, if you have three dimensions, there are Several ways of creating the index. One way is just to, say, let's suppose it was, your indices were 256 and you, so you have a one byte for the X, one byte for the Y, one byte for the Z, and, three bytes, that, the way you want to sometimes think about this is what the implication of that is on the underlying cache memory structure. There's an alternative format where you Interleave the bytes. It's a, there's a name for it. I want to call it a Hamiltonian, path. And what it does is, it basically improves locality. If you're hitting around one particular area, you'll, there's a greater probability that you'll be on the same cache line looking for the next one. And in fact, that's what they do in texture maps. these days. So Kevin, one, this is all in Python. Yeah, like No, if you're gonna no, you misunderstand. The, it's, a simple bit twiddle. Yeah, on the index he used to look something up. But that I think, what we will have done with this is I think we will have knocked down this, the runtime here such that now it's in the Einstein and things like this, right? I think what would be cool is try and implement this, see where the timing is, and then we can do additional optimization. But my guess will be, this would be like the 10th hottest operation now, right? Yeah, I think the only concern I have is the resolution loss, because it's the Right now we have pixel. We're going to voxel, right? So There's a volume in there.

Yeah, but if you're using a uniform discretization of space, which is the other thing that you can do, is not, the points are being separated by how many there are in each set, but like the space itself. if you know the maximum dimensions, like a bounding box around the space, you can just subdivide it into say 32 by 32 or 256 by 256 or whatever, and that gets you the voxels automatically. And it's, the set volume to each one. But look at this picture, for example, of this mug, so that point that's marked in green will include the points that on the other side of the mug, inside the mug and outside of the mug, because of the volume. The voxel is encoded, the location of the voxel in memory is encoded in the actual XYZ coordinates or IJK coordinates of the point itself, right? You can actually read off Where in that space it's going to be based on, is it on, basically the way I think of it is, every time you divide the space in half, that's like a bit, either I'm on the left or the right side, it's either going to be a zero or a one, and so when I subdivide and I subdivide, basically you're adding another zero or another one, which you're saying, which side of the cut plane am I on, and effectively that drives you down to a single voxel, a single leaf node, and so the position of the thing in XYZ space can actually read off exactly where it's going to be at in memory.

So if you've got the memory space for it, it's a very, fast lookup. It's a very fast hashing algorithm. So in this case, would we basically not actually care anymore about storing less points because we just have the discretized voxel space and we just want to say for every voxel whether the object exists there or not? does the surface intersect the voxel? That's really what you want to store a bit, the one or zero. It does the, surface of the shape intersect the voxel at this location? Is that what we're saying? I thought it was just replacing the KD tree search. Yeah, that's what I thought. Yeah, we still want to know the idea of the point where we have stored the feature. I think it's just to take the XYZ, hash it to an index, and look it up. Yeah, I think what we're proposing is like an optimization of basically giving the tree a static. Let's just build a lookup table out of the tree, right? Exactly. Could you make it? Absolutely. If you wanted to do incremental learning with it, could you still modify without rebuilding the table every time? Oh yeah, Seems like you should. Yeah, I mean there's stuff that people have done, something that you've done. Subutai and I were also saying LHS to each other, right? So locality sensitive hashing is also a way that people do this. You can do periodic rebuilds and all sorts of stuff. But if you have to rebuild those, what can you just incrementally add? Incrementally build things. And you can also delay rebuilds and all sorts of things, right? You can do minus with all this stuff. Can you do actual true incremental work with this table? Could you, let's say I'm going along, I find new objects, new points, whatever. Can I just Can I dynamically modify the table without rebuilding it? Yes. Yeah, it would be very simple. So you're just modifying it very locally. It gives you a lot of flexibility. Yeah, to modify the table. Viviane was asking could I have more points, but it seems like the size of your box could be modified too. Yeah. It's actually faster than doing a td tree rebuild. Pretty much that's it. So do you have any, libraries for this that you would recommend? let's start with the dict, but you could do an LSH library or something. LSH is the other way to scale, like if you want to get to massive objects, massive points, right? So it's locality sensitive hashing, right? So basically, you There's again a lossy technique where basically, a lot of people use it in transformers now as well, so basically, rather than the hash avoiding collisions, you're hashing to points in space that are similar. That was Anshman. Yeah, Anshman was big, yeah. The big thing was LHF, right? LHF is also the solution to many problems. there are actually a bunch of open source implementations for that, right? Yeah, I was just looking up, I just did FAST. LSH, Ash, and Python, there's a bunch of them. Yeah, Okay. Yeah, I think that would be better than, just a crude partition of the space because you're trying to encapsulate a surface in, but you're paying the penalty of a volume whereas with LSH you could, basically winnow that down to only, only storing locations that make sense as opposed to all the empty space. Okay. Thanks. you wouldn't store the empty space. With a dink, you wouldn't store the empty space. You'd just have only where the points are. What is a dink? just joking. lookup table. Lookup table. Hash table. Yeah, when you first said lookup table, I was assuming you were, actually carving up space into volumes, and every one of those would have to be represented in memory. No, no, you don't want to do that. More like a hash table. Yeah, okay. Yeah, so I understood the same thing as Kevin, so that's why I was confused. Oh, I'm sorry. Yeah, no.

You take the, location, you hash that so that you get to some point, and then you just, there's the five points right there. Yeah. Viviane, quick question. the inputs to the KD tree search, are those like global XYZ coordinates, or are they like object specific coordinates?

Object specific. Alright, so you have a point on an object and then you're querying for nearby points based on distance from the initial search point. Yeah. Distance and direction, alright, okay, okay. yeah, thanks a lot, should I move on? To, actually the next slide is Let's not go downhill. Lawrence and I have an 11 day apart, Yeah, yeah, this is actually still related to Katie, the Katie tree search, but just to check in if this would change anything with, what you're recommending. So when we're, voting, we actually, we have a very similar problem, but the main difference is that we have to build the location tree at every step from the incoming votes. so basically we have a bunch of incoming votes and we want to. Query in that space of the votes, basically. So would it still be fast to, build up this discretized voxel space at every step? Or is that, would you say something else would be better in that case?

do you have to build a tree that's being created? Yeah. So we basically get, votes of shape V times three, votes, if we threshold them, yeah, around 20 to 200, and then in that space, we query these, locations from the hypotheses.

But this is, the part of the loop that was much faster already, right? Yeah. Okay. This is a different, is it's using a completely different KD tree here or is it the same KD tree you had in the previous slide? It's a different KD tree, so in the previous slide we could build the tree once at the start of an experiment from the model of the object, but now both the hypothesis locations that we used to query and the tree that we need to query in changes at every step. So here we would have to build the tree or the voxel space or the lookup table at every step, basically. So the first one was the evidence update from the sensory data, and this one is the evidence update from the module voting. Yeah, exactly. Okay, But didn't you say that loop was already pretty fast? Faster, but not as fast. Yeah, so that one was, yeah, so if we threshold the votes, it's pretty fast because V is only up to 200 elements, so it's fast to build the tree anyways, but if we do use all the, votes, which can be up to 200, 000, then it's slower.

Is there an advantage to using 200, 000? Would you like to use more than 200, 000? Or does it rapidly fall off, is it, it's a minimal advantage, but not enough to justify the time cost. neurons couldn't do that, right?

They couldn't, process 200, 000 quotes. Is the thresholding based on an evidence measure or a confidence measure or anything like that? Yeah, so basically only the most confident hypotheses get communicated to other learning modules. And on average, how many votes is that for per module that they're confident about? It's between one and 200 usually. Okay. All right. So you actually do have a lot. Okay. Depending on how symmetrical. If this is being done by unions and an SDR, you can't get that high a number. You can't form a unit of 200 hypothesis. You might be able to form a unit of 50. Yeah.

I'm just saying it doesn't mean you couldn't do better than the neurons, but I'm saying the neurons are not that they're going to plateau at some lower number. Yes, yeah, the main problem is like if, like Eric says, if we have a very symmetric object, then all of the poses are about equally likely. So if we then threshold the evidence, they will all be.

Would there be a way of compressing those, yeah, would there be a way of consolidating those for which the evidence is all the same for a set of votes? Like having a list of, like for the given evidence, all these possibilities are there that way you don't have to, I hate you have to expand it out anyway when you do the, compression. The voting with other modules, so there's no way to distinguish two votes from two hypotheses from each other based on the evidence, then they're functionally equivalent. There should be a way of compressing that down somehow. Yeah, so we do have a mechanism to detect symmetry if we, if if two poses have consistently the same predictions, they're classified as symmetric after a certain number of steps, but still until they. Took the, this number of steps that it was to, yeah, communicate a lot of thoughts. Okay.

I'm almost tempted to say the right solution here is to not have so many hypotheses. It's like Laura and Albert Beck. Yeah, no. So yeah, right now with the thresholding, it's actually pretty fast, so it might not be the highest priority to optimize this one. Yeah. Yeah, I agree.

Okay. yeah, then, Move on. I guess just one last note on the k tree search radiuses. We only want points in a given radius, so specified by the maximum match distance, but we also want to get a matrix. And if we just query the radius, we get like a ragged list since each point's radius contains a variable amount of neighbors. So what we do right now is to create a query for k nearest neighbors to, to get an array of shape. H times K, and then we mask all the entries that are too far away.

I guess there's really a better solution, but yeah. Is this still relevant if we do the lookup table?

Yeah, I mean in the lookup table it can also happen that There, it will always be that within a given radius, there can be a variable, amount of neighbors. Is that a problem? Why is that a problem? It's not obvious. Because then you can't use the matrix operations directly, because they expect a fixed You can't just put zeros there or something like that, or?

Yeah, but, yeah, so yeah, I guess the two options would be either to fill up by, with zeros, but then if only one point has a hundred neighbors, then the matrix gets huge, for all the ones that just have two neighbors, and then I guess we do the opposite and, cut it off at a certain number of neighbors, and then Mask the ones that, are not what we said to do earlier when we were talking about the lookup table, pick some number of points. Yeah. Yeah. So that problem applies to the lookup table as well. It's, yeah, I guess just a universal thing. But you're saying if we did the query radius in principle, we'd have much fewer points.

in some cases. I actually didn't look at in how many cases there are more points than k in the radius. It depends on how dense our models are.

but yeah, maybe I'll move on, this one is that much for speed. So now just a quick question. So right now you're assuming that you can't assign any importance to the points, they're all Equal in your eyes as contributing evidence, right?

yeah. What if you relax that assumption?

How, would you The points get eliminated based on the, the evidence not fitting the hypothesis, right?

I guess they, they contribute to additional hypothesis, but I'm just thinking that in large flat areas, some of those points, It's if you eliminate them, then you have gaps in your model, but if you had something that was higher order than just a point, then you could allow for, something to substitute for a large flat area as opposed to a myriad of small points.

This is the isolation problem I was talking about earlier. Yeah, so I guess that's something that will come in more when we add hierarchy, that points. Yeah. And then also when we build the models, we take into account how fast features change in an area. So if we're just on a flat surface, we store less points in the model than if we are in like a part of the object where it curves a lot and features change a lot. Yeah, I wouldn't necessarily postpone it to where you think of hierarchy because then you're getting into assemblies of parts and stuff like that. What I would say is that you, if, your fundamental primitive is, starting to become burdensome because it's just too low information content, and you need a lot of them before you, you get something out of it, my indi, It would almost I would say that you would need to replace it with something that has more structure to it, so you don't need so many representations of it to, to, yield a hypothesis.

it's something, that's not a point, but, just for argument's sake, a, rectangle, that basically as long as it's flat enough, you, it's the rectangle, and then the descriptor for the rectangle could basically substitute for a myriad of points if things are flat enough, but, because if you're, going to go down to pixel points, the problem is you're going to run into these combinatorics, so unless you do some kind of abstraction, which I don't, think it's the same thing as hierarchy, it's a, it's going to basically be a problem until, you can, efficiently represent, a area of a portion of a surface by a few parameters as opposed to, hundreds of points.

That's exactly what I was talking about in my research meeting last week, was how do you discretize a space using as few points as possible. You can use fewer points, the more high order information you're given, say, in addition to getting the point, you're also given the gradient or the curvature. you can then extrapolate away from that point and represent a very large area near there if the, if those features remain constant, but if the features start to change, if the curvature changes or the gradient changes, then you might need to drop in another point there to capture that change, and then basically you could interpolate between those two points to get a smooth transition from one to the other. Yeah, maybe that's a good topic for another research meeting to, to get into more.

yeah, because just because we only have about 10 minutes left. I just want to get to the second topic real quick and see if there are any other thoughts on that, if that's okay.

so yeah, now the matrix operations.

the one that takes the longest is the Einsam operation, which we use to get angles between the hypotheses and the observation. So basically. since we have, since we use unit vectors to calculate the angle, we basically just need to take the dot product between two vectors and then apply, our cost to it. And we want to do this for all hypotheses, poses, and then your neighbors in one step.

that means we need to apply the dot product between the hypothesis. nearest neighbors, which is shape H times K times three, and the transformed observations point normal. so three are the XYZ coordinates of the point normal, which is a unit factor. and then, yeah, as an output, we want to have a matrix of size H times K. and Einsam is basically just an efficient version of running a for loop over all hypotheses and then applying the dot product between. the hypothesis point normal and the transformed observation point normal.

then we also apply normal dot products, for example, to rotate displacements by the hypothesized poses. So in that case, we have the possible rotations, which are three by three rotation matrices, and we have, as many of them as we have hypotheses. And then we apply the dot product, between that and the sense displacement, which is Again, unit vector and X, Y, Z coordinates.

No, it's not a unit vector in this case. It's just a displacement. And we got a matrix of size H times 3. We also apply matrix multiplications. for example, to get the rotation matrices from our pose hypotheses. So in this case, we have the poses stored in all the nodes. So that's, number of nodes in the graph, times three And then we have the sensed pose, which are also three unit vectors, they are orthonormal and yeah, we apply a matrix multiplication between them to get, N, three by three rotation matrices. Then we have the standard operations, like addition, abstraction, arc costs, and multiply to do a range of different things. here the dimensions always stay the same. And then we have, yeah, some reduced operations, like applying the maximum to get the highest evidence in the, nearest neighbor Radi.

yeah. So if we boil this down to just the size of the matrices, two things that I think could be places where we can maybe optimize something. So one thing is, we don't need to test all hypotheses at every time step. So right now, since it's just matrix multiplications, we just, We update the evidence for every hypothesis at every step, but in theory we could mask a lot of the rows in the first axis, so H. And then the second one is some entries in K are also masked, like I mentioned. less than k neighbors are in the radius, then we mask these entries. So yeah, I don't know if you have any, thoughts on that.

What are you using to do the, the matrix math? NumPy. NumPy. All right. So with the first one, are you saying Like how many rows are you trying to ignore on average, like what percent, because that seems like it's just brute force doing a lot of stuff that it doesn't need to do. Yeah. So theoretically we could ignore a lot of these. So for example, we could only look at the N highest evidence hypothesis. So that could be 10, it could be a hundred, it could be a thousand highest hypothesis, whatever. But it would definitely be still lower than age, which is usually between. Like a thousand and forty thousand, especially towards later steps in the episode, this would probably just be like five or so high up, high evidence hypotheses. So you really just want to, you only care about a very sparse subset of the rows. Yeah. So I don't know if there's an efficient way to do that.

the, advantage of updating all of them at every time step is that you can easily recover a hypothesis, even if you get several inconsistent observations.

but yeah, I'm not sure how relevant that actually is and if it's worth the computation.

I'd have to take a look at the math to see if there's anything that pops to mind. so it's hard telling from just the description. Yeah, yeah, I would try to minimize the number of times you're calling arccos and sine and so forth, because those are inherently a bit slower than the built in addition, multiply sort of things. Oh, okay. It's just because the way they're implemented, they're usually lookup tables or something like that, because they're, not like, they're not something that you can just implement in transistors directly, like addition and subtraction and multiplication and so forth. Yeah. there are a variety of, when they're really computing the ARC codes and you've got floating point numbers, they start with a lookup and then there's probably a polynomial iteration on it. So they're expensive operations. Newton iteration. It's not a single op or anything like that. There's a variety of optimizations that people go to, to try to compute that. The other thing is, if you take, the easiest thing is, you take the dot, the dot product, which you say is normalized, and then just go to a lookup table to get you in the ballpark.

The other thing is, if you're comparing distances, like if you're computing two distances and you're trying to compare which one's smaller, don't compare the distances like by, like when you have to, sum all the squared, components and you take the square root, just compare the distances squared. Okay. That way you don't, that square root is also an expensive operation, and you can compare directly the, distances squared, the, one's going to be bigger than the other, obviously. so that's another way you can save yourself some computations if you don't, if you don't actually need the distance, if you're just comparing two distances, you can get by with that. Yeah, that's a good, point, yeah, because, that will tend to have a coordinate bias though to it.

How When you, if you're just taking the distance, okay, I'm sorry, nevermind. Distance squared. No, I understand. I thought you were talking about the square of the components. Nevermind. You're right. Yeah. Compare the distance to squared is, easy enough. So from the profiler, actually the slowest operations are, or yeah. Multiplied by how often they are being performed in the algorithm is the Einsam, and actually I was surprised also the Maximum operation.