I wanted to go over basically what we talked about over the last month. It was a good review for me and, just kind of give a quick recap and, and then I've got like a whiteboarding, sort of example problem that I can show you and we can kind of So, starting November 6th, so before this, Niels was talking about our various motor policies and then we kind of moved into this talk about going goal oriented behavior. Jeff says how the cortex creates goal oriented behaviors, a major missing component of TBT, and introduces this, model or, this scheme, I guess, where we've got upper layers of the cortex that are, representing the state of an object, and the, lower layers, representing the space of an object. And then, the idea is that, like, once we have states defined as part of these models, then we can start moving on into talking about goals and goal states. And, a goal is defined as the desired state, the state we would like that column to be in.

And, my arrow keys aren't working.

and then from this document and conversation we had on the 6th, there were a few open questions. Like, we've got a state space, some kind of state space, presumably more abstract than what's going on in the physical, like, 3D location space. So we've got these questions about what movement looks like, and path integration in state space. there's going to be some connection between the states and what the actual representation and the location slash space layers are. We don't really know how that works.

on sort of a different level. Thank you. Different theme here is like, is there voting and temporal pooling of behavioral states and, how columns work together to generate goal oriented behavior, and this is the one that we kind of follow up on the goal oriented, or like sort of a hierarchical, arrangement of these columns to figure out how goals propagate and actions propagate. And then in Slack, Jeff puts, posted some follow up thoughts. So, We're processing for goal oriented behaviors, we'll need to include like a where pathway essentially, like, meaning we're just looking at the what pathway, we're looking at objects, we're looking at objects in their own reference frames. And an example of this is like, you need to reach for something in the physical world. It's in body space, it's in world coordinates, and, and there are some back and forth about this. Viviane and Niels, you brought up the idea that we're estimating pose, and that also, is represented in Monty. So, you know, the degree to which we may need to include this, I guess, has been, there's been some back and forth on that.

The need for episodic memory, so you still have to remember what your top level goal is, your second level goal is, in order to execute all these sub goals. And I think this is kind of an interesting problem, because I've been thinking about, like, the persistence of object models and states as we move along through tasks. Let's say we've got a cortical column that's foveal, and then you start moving around and looking at different things, like, how do we maintain some kind of persistence? And our representations as we, continue sensing throughout the world, moving through it. And then, and then, we kind of started talking a little more explicitly about hierarchy. And, this is also an interesting problem. Like, some, you can kind of formulate the hierarchy in such a way that it seems like you need n levels to have n nested compositions. Or there may be an alternate way to do that with fewer, with Jeff talking next about, how we might be able to do this with, just a few layers.

When I say layers, I mean, like, tiered tiers in the hierarchy.

And then Niels also presented on that same day. And, brought up this example, which I think was kind of a review from, from discussions you guys have had before, and brought up this problem, which is pretty intriguing, I'm trying to move my Zoom.

So there's this, this goal, like, where we wake up, we're really tired, and some top level, LM is receiving a goal state. In this example it was, kind of doesn't really matter where it comes from, but you know, goal state in this example came from hypothalamus saying we want to be less tired. And so this top level LM receives the goal state, get less tired, and then it's going to, Delegate subtasks as we move down the hierarchy. So through these L6 to L5 connections, we move down to the kitchen modeling LM, which receives a whole state, get coffee made, and, and then that can get broken down further to a coffee machine LM, assuming we found it and we're looking at it, and that's, you know, get that coffee machine on, or have that coffee machine Beyond, be in the on state. And of course that can get further broken down into, get the coffee button on. and then an interesting and important part of this model is that these, you can always sort of dump off to subcortical control at any point. So if, for example, the goal is to have the coffee machine made, or coffee machine on, you need to go find that coffee machine first, so you may yield control over to subcortical machinery in order to look around in the room, find the coffee machine, and so on, and then you kind of reestablish the, the process of moving down the cortical hierarchy to get these goals done.

So, some interesting conversations came out of this. This one in particular, how does the recruitment of lower level islands happen, is very interesting to me.

you know, there's, and this will come up, this came up in the typing, but if you want to move down the hierarchy, Scott, can I ask a question? Sure. Is the goal, I mean, I'm enjoying this, very good, the question is, is the goal just to review, or do we want to discuss these items and, and do you want to go through the whole, you know, what, what, where are we going, because I have a lot of thoughts already. So, I have like. Just a couple more slides, and then I've got a whiteboard with a new problem, so if you want to jump in and discuss. A new problem, like, are we gonna, a problem that we'd apply these questions to, or a new way of looking at all this stuff? That's the idea. Which is it? A new way of looking at all these things, or a new problem, too? A new problem that is sort of like a simplified, I, after talking about the typing, all the typing stuff, I kind of boiled down to what seems like a simpler problem to me.

Because I think some of the, you know, some of the things I said were very, very speculative. I want to make it clear. And then some of the things we just presented here, I think, are not correct. And so, you know, we can, we can debate these things, but I haven't said anything yet, because you've just been reviewing what we talked about, which was, of course, brainstorming. so I can, I'll let you finish your review, but I don't know, these shouldn't, these, all these points shouldn't stand as like, oh, this is where we are, they were just brainstorming ideas. Yeah, yeah, this is just the story of our conversations, basically, up until this point. Well, you know, most of what we've talked about so far, a good portion will be wrong, so just everyone bear in mind that, you know, that's the nature of this. I would say just jump in at any time, like if. I can go back to a slide if there's anything you want to Well, I didn't want to If you're just reviewing what we talked about, it's great. It's just, but just, I don't want to take it as, okay, here we are, let's build on this. That wouldn't be correct. and we can go, you know, I can, for the thing I said, oh, well, that what I presented was wrong, or this I don't think is right. Like, I don't think subcortical processes can go find the coffee cup. That's not subcortical, that's, that's cortex. you know, so things like that. Yeah. Yeah. We can maybe clarify that. But, you know, like for example, I, I wasn't suggesting that, that, that, yeah, something like that, that's kind of map based. I think it's more like the goal states, as long as they're primitive enough for a subcortical thing, like just going to a particular location or something like that. Right. I then's just like walking. Yeah, more like walking would be the Right. Right. And those are pretty limited. Those are, those are things that are mostly genetic. Not completely, but a lot of, but, but it's not like there's a map of the kitchen. Right. Right. I think, I think that's what you meant, Scott. But yeah, I get to your point. Subcortex doesn't know what the puzzle looks like and that's any, any visual recognition tech, almost 99. 9 percent of visual recognition tests are happening on the cortex. So there's some few, few, very small, few exceptions. And the whole process, the whole part of where I talked about goals, the goal, modeling goals in the upper cortex, upper layers was still very intriguing, but I consider it extremely speculative. So, I haven't, I haven't eliminated it, but I just certainly don't, I wouldn't like, oh, that's not gospel, no, that's not an idea. Yeah. Okay. Well, let's keep going. I just wanted to make sure. Okay. I'll say a few things, you know, as we go along. And I also think it's, I think it's very clear in my mind, at least, that you can't represent all the levels of a hierarchy at all, whether you're modeling hierarchical composition objects or compositional behaviors, you can't, you can't use the levels in the cortex to do that. I mean, you can't say, oh, I have five subtasks and there I'm going to use five levels or I have a nested structure of Six nested, you know, compositional structure, therefore we use six levels in the cortex. That, that's almost certainly not true. And this is where the idea that, and we also know that a lot of other animals who do sophisticated things don't have as many levels as we do. And we also know that the levels are not really so clear all the time. these all things suggest that much of, much of what we view is, Hierarchical is always using the same levels over and over again, it's just moving the window up and down the stack of composition. I'm pretty certain that's happening.

Okay, I said a few things. Keep going. Yeah, it makes sense to me. It doesn't make sense to me that we don't have five layers so we can do objects that have five levels of composition, you know? Right, right. There's got to be some other way to solve that. Right.

Yeah, and the only other way I can think of is to have the same set of layers, you know, moving up and down, over the conceptual stack. You can look at some portion of it at any point in time, and therefore that requires some way of remembering, where you are in some sense, because you have to shift those things around. Okay.

So if you have a complex, let's say, multi nested object. In the sense that, you've got one big object, it's got these, say, two or three subcomponents, and each of those subcomponents can be broken down into several other subcomponents. Would you expect that entire object to be represented in a column? No, no, no, it can represent, you'll have to have at least two levels in the hierarchy, right? Because you need two levels to represent the relationship between two objects. But let's, let's say I have four levels, A, B, C, D. Those two levels can represent A, B, they can represent B, C, they can represent C, D, right?

Maybe it would help. I was just thinking that, like, the example we talked about, I don't know if you've heard this one, Scott, but, like, with the Numenta mug, at like, at one point you might be attending to, like, the logo on the coffee cup. And so those are the, your two levels, which are kind of controlled to some degree by attention. Thanks Are are representing those two levels. But then if you wanted to focus in on the Menta logo, you could kind of represent the, the letters that are composing the logo, and their relation to the logo itself. And then you're kind of at that point not as consciously aware of the mug or, or the room for that matter. Like, and then, and then there's all these different levels. You know, you could look at the mug on the table or the table in the room, but at any kind of point, you're, you tend to be attending to two of those. Right. And you don't, and you don't really, thinking about the other two, but it's, it's con, it's misleading because you have a memory of where you were. So it feels like you can say, oh yeah, but I'm thinking about the cup, because that's what you were thinking about. But at the moment, you're attending to the letters, you're actually not thinking about the cup. You're just, and it is, you can actually catch this pretty clearly. You, yourself, you're just. you know, there was a, there was a tool people were using for a while for doing presentations. I forget the name of it. it was sort of a faddish tool, but maybe people are still using it. Were you, you, was it, what's it called? We just, you can create, you can keep deeper and deeper and deeper nesting and things, right?

That sounds familiar. Yeah, yeah. Anyway, the point was, You can do that. You, you, and when you, I was, when I first saw that, I was like, well, this is interesting. You know, there's unlimited depth here, but you can't think of all the depths at once. You can, you can sort of mentally work your way back up and work your way back down again. But it's not like it's all there and you can deal with only any level of complexity nesting.

so, I think it's, it just tells us that you have to have at least two levels and you have to have memory to know where you are so you can bump up and down the stack as you need to. that's almost a certainty. Yeah, that's clarifying. I don't think I really caught that the first time through in these meetings.

okay. One thing also I just thought maybe was worth quickly mentioning, because yeah, you mentioned about the, hippocampus and, or like some sort of memory. at the higher level. I'm pretty sure we've talked about this before, but I guess it's like, you know, another thing you can think about is like prefrontal cortex, which seems to be where like short term memory is, like, is maybe another type of like learning module with a very short, time frame of its representations. I'd love to hear some research on that, because my knowledge is probably just old. I always associate short term memory with the hippocampal complex, that's the only thing I associate it with.

If it's really happening in prefrontal cortex, I'd love to know that, and I'm not doubting it, I just, I haven't read many papers on prefrontal cortex because the only theories we have for short, real fast short term memory, meaning like you can remember something very quickly and then you can forget it again, is, is, is, the silent synapses, and I only, and I don't even know that they exist in the hippocampus, but they may also exist in prefrontal cortex. That would be a very interesting thing to know. Yeah, so my, my understanding is like, there's pretty good evidence that a lot of, short term memory involves the prefrontal cortex. I, like, my understanding was the bigger debate was, whether that is mediated somehow just through activations, like persistent activations, or through some sort of synaptic, like more permanent synaptic changes. People who talk about persistent activation, I've never, there's, there's, there's some very special cases for persistent activation, and this is like, like you're asked to memorize a set of digits, and you can, most people can do seven, and you have to keep practicing in your head, you know, you, or, you know, there's some very short term memory, which requires reactivation, but anything that where you can get distracted and come back to it later, I don't believe it's persistent activation. This is, I'd like to see the evidence for it, but every time people speculate about it, I haven't seen it. It almost has to, in my current point of view, it almost has to involve some sort of physical modification of something. And, Yeah, and I think some of the recent theories are, as often happens in neuroscience, when people propose two different opposing views, I think a lot of the recent theories are suggesting it's a mixture of both. So that there's some element of persistent activity and then there's some element of synaptic changes, which also maybe fits with the finding or like the fact that, you know, if you do get distracted while you have something in short term memory, often you do lose that information, but not always. Right, right. So it's probably, yeah, some in between. Right. Although, again, I read some theoretical analysis about persistent activity once, and it's really hard for neurons to maintain persistent activity. You know, they don't really want to do that. there's a, you know, they operate on the order of milliseconds and they want to go on and do something else with them. So that would be an interesting thing to do some research on, because I've kind of written off the persistent activity theory as something useful for us in our work, but I could be right about that, so that'd be really interesting to know. But I think we can generally say, I think, well, you know, it's interesting, because if persistent activity was a real way of storing short term memories, then you could do it theoretically in any learning module. Right? It could be, you could have some sort of persistence in any learning module. I have no evidence for that. All the evidence I've seen is not. But, I've always felt like, okay, if it's going to be synaptic changes, you can't grow synapses fast enough. It could take at least a minimum of a minute to do that. therefore you need some other way of, and these silent synapses fit the bill beautifully. So I said, okay, that's the solution here. You have these neurons in the hippocampus. You know, not 3, 000 synapses, but 30, 000 synapses, and most of them are silent, and then you can do a very quick enabling or disabling the silent synapse. That's just a chemical process.

So that's the way I've been thinking about it, but it could be wrong. It'd be really, really important to know if, if there are other learning areas of the brain, prefrontal cortex or other learning modules actually could do a short term memory, that would be beyond this sort of like memorizing seven digits type of thing. Right. I'll be right. Yeah. Anyways, sorry. I, I didn't mean to. To derail. I guess I was just No, I, I derail it. Trying to No, but yeah, just like that. because I know, you know, a lot of people ask like, oh, what about the hippo? And stuff like that and, and just in general, like how, how are we keeping track of like, variables in the environment that are quickly changing and stuff like that. But, so it's maybe just useful to think that like most of these things are still, are probably gonna be something like a learning module still. But just with like a shorter time scale, whether that's episodic or short term memory. And of course there's all these parallels between the hippocampus, with lower levels and upper levels of the cortex and people speculated they're this very similar. So, you know, the way I've reviewed it, the hippocampus is different, but it's also the, the predecessor of cortex and has the same basic features going on up there. and so I've just sort of written, okay, we can think about at the top of our hierarchy, we're going to have some learning modules that are, that are fast memory. they're going to, they're going to have grid cells, they're going to have, you know, place cells, you know, the same kind of stuff we have in learning modules. Right. Yeah. I mean, we want to simplify if we can, and that's a good simplification, I think. Anyway, if anyone sees any papers about prefrontal cortex doing short, short term memory, I'd love to be able to read those. Yeah, I'll try and find the two or three that I'm thinking of and share those. That'd be great.

Scott, yeah, go on. Yeah, okay, so, this is just brief, but, next couple weeks, we got Jeff first. And he's really emphasizing how we can use a couple of columns hierarchically connected and represent composition that way. And, and then we kind of turn to this problem of typing.

And in that discussion about typing, we talked about, do we have a keyboard model? Did we have a keyboard model and don't anymore? Because we're unable to just type. Not, we don't necessarily even know where the keys are, can't like pull that out, which is kind of a side point here, but, that'll derail things too much. I'll, I'll stick to this, but, then we talked about, you know, there's a lot of back and forth about model free and model based and, subcortical control and, it got fairly complicated. There's a lot of discussion, but there were a couple points that I got kind of mentally stuck on, and I wanted to work them out, in a more concrete way. So, one of, one of these problems is if you have a goal state, like, basically, how do you select the next LM down in the hierarchy?

part of that is learning causal relationships. When you say, how do you select the next? Element down the hierarchy. One of the things we were talking about here was like, oh, I could type these in any one of my fingers. Right, exactly. Or I could do it with, do it with my nose. So is that what you mean by this? This question? Yeah. Or like in the typing, a key you could do, you grab a chopstick and press the key, or you could press it with your, you know, there's all these options. And there's some complex process. Someone suggested earlier that, that, we just do the one that's most convenient for us. So, we would typically poke things with our index fingers, and we would typically poke it with the one that's nearest the thing we want to do. So that's a pretty, that's a pretty simple policy. and, and, and the task of such typing is learning not to use your index fingers. That's the flip. So, some of that might be easy, like, but then I think sometimes if you're presented with a really difficult problem, you don't even know how to solve it. you know, what is the next step you take? It's not as simple as like, how do I push this button? But I'm like, oh, you know, my hot water's not working in my house right now. So, you know, what would be the next step I would go to try to figure that out, you know? it's not working correctly, it's working.

so it's not, it's not, I, the finger one's pretty simple. Oh yeah, use your index finger, and if it's, you know, if you have a choice between your left, if the equal distance between your left and your right, pick the one that's your favorite hand, and otherwise pick the one that's closest, you know. That's a simple step, like, which, how you actuate this thing, but if it's, if it's a more difficult goal step, then it's not so clear how you choose. Yeah, yeah, I think we may be able Oh, I just had another thought when you talked about like, where's the keyboard model learned and whether it's forgotten. And I just thought of this now, so I'm not sure if it's right, but one possibility why the keyboard is such a strange example where we can't really remember where the letters are, yet we can type them very quickly, might be that the keyboard is actually an object that has too many details. To be remembered in one model, like it's too complex of an object to model as one. So, like, we, we don't actually store the whole model of the keyboard, maybe like an approximation of where the numbers are, where the letters are, where the escape key is, and the space. but we, we can, it's just too complex to model the whole keyboard. So, and secondly, we don't, when we type, we don't have time to do a model based policy. We want to type much quicker than we could with a model based policy, so we quickly learn these kind of quick motor sequences for, words as more of a model free kind of sequence. so maybe that's why we, we can't like reproduce the entire keyboard model or remember where each key is exactly. Well, I think, I think it's a combination of both of those things. Like, first of all, I mean, one for the whole point of getting to be an expert on something, whether you're typing or playing, playing a musical instrument, is that you figured out how to do this and you don't want to think about it. You're an athlete. You don't want to have to think about how you do these things. And you'll be faster and more accurate, you'll definitely be faster if you don't have to, you know, attend to all the stuff, you just want to play back some pre learned sequence. So there's definitely that, and once you've done that, you don't need the model anymore, right? It's like, like an athlete who's a skier, they don't want to think about what they learned how to ski, they just want to, Go with the flow and, you know, use like your muscle memory, you know, that's it. Yeah, I guess what I'm saying is that there maybe never was an entire model of the key. Like even when we do the hunt and peck strategy, it's not like we have a model of the keyboard and we're thinking about that model and where the letter is. It's like we're looking and searching for the letter and then It's a very, it's a very good observation. It's a partial model because we kind of know generally where the keys are, right? it's maybe just shows that the accuracy of placement of features in this case is too low. Yeah, exactly. That's, I feel like, more of what we're thinking that it's just like, yeah, there is a model of the keyboard, but it's just not that accurate. And more, like, more generally, like, all our models are probably that kind of accuracy. But we don't normally notice it because we're not trying to make such precise predictions. And so it's only when it's like, which letter is next to which other letter that it's like, oh, I'm not actually sure. This could be a really big clue because we've always assumed that we do these, you know, we do features at locations that there's some sort of, you know, ability to do any number of these things. And, but if you're right about this, it's a really interesting observation. It says, well, our models are really not that accurate. Somehow we learned to live with them anyway. I don't know what to make of it now, but it's a really interesting observation. I have one other little observation about this. The way we teach people how to use a keyboard, in fact, when I was trying to recall the keys on the keyboard, I went back to this method. We teach, we teach it, at least I was taught, that you learn the letters in sequence going across the row. Like you say, oh, the left hand is A, S, D, F. And then you have Cordy on the upper row. and so I remember being taught that, where you can learn sequences, obviously you can learn very long sequences, we have no trouble learning very long sequences. So we, we, we take the spatial keyboard model and we turn it into a series of sequences and then you can count your way across.

My mother, when she lost her eyesight, Okay. very poor eyesight, she would start on the left side of the keyboard and just count the number, she would recite the letters going across and then know which letter to press. And so that stuck with her until the end of her life.

I have a question, is there, is there a difference between like inference models and generative models? Because like If you speak a foreign language and you haven't talked to it for a long time, like you can recognize the language, but generating it becomes problematic. Or you can, we can easily recognize keyboard layout and it's like, yep, it's everywhere it needs to be. But generating is problematic. Like it seems like there's a generative pathway. Not pathway, sorry. Seems like gen, gen, generation, generating output versus recognizing it are different things and with different levels of accuracy. Yeah. I think we talked about this a couple of times before. Like just because we can't reproduce a model doesn't mean we don't have a model because the better test is like whether you could recognize if something is wrong with it. So like you're very, very bad at actually drawing an object. Like if you. You are really good at recognizing your dog, but if I would ask you to draw your dog, or even just ask you, like, how long is the nose of your dog? And where is it placed relative to the eyes, exactly? Like, you're kind of bad at that. so Yeah, I think that's a very interesting distinction to make between like inference and recognition and kind of prediction errors internally to the learning network. Well, I've always assumed that, you know, the, the what pathway is mostly a, an inference model, right? It's, it's, and, it doesn't, as Viviane just said, you can generate predictions from it, but it's not designed to do generative, generative, you know, to generate anything, people are terrible at like drawing anything practically as you see it. And I guess one of the main issues there is like, we're not communicating, like the learning module or cortical column has a model of the object, but we're not ever like sending that model outside of the column, we're only producing action outputs, from based on that model. So. It's not like we can communicate this detailed model anywhere else.

Go ahead, Niels. Oh, I was just gonna say, yeah, and in terms of like, the connection to kind of the current algorithms that we have, like, you can kind of imagine that if you are making a prediction Like when you're doing inference, you're essentially verifying that prediction. And so like, you can have kind of a higher tolerance for noise, because even if your prediction isn't very good, as long as it's in like the rough neighborhood, then you can still be kind of right when you, when you get that incoming information, but then if you're making a prediction and then trying to generate something from it, suddenly that uncertainty is going to be much more impactful because it's not grounded by some information coming in. So I feel like that's kind of. You know, if you're basically, yeah, when you're, when you're doing inference, then you're having this like sensory information verifying your hypotheses. But if you're trying to generate, you're essentially just doing this like dreaming type generation that's, quickly becomes kind of noisy and, and unconstrained. You know, just reiterating what you all just said, but I remember, some of you remember, I took a face drawing, ArtPlus. A while back. And it was, you know, it was for artists, and it was a pretty serious thing. And what struck me is that even really great artists cannot draw faces. It takes some years of practice, because even the smallest error in drawing a face is recognized as an error. Just the tiniest error in placement or orientation or something. And other people say that's not right. And yet when you're trying to draw a face, it's like, how do you know? You can't do it. So they teach you all these rules. There's all, tons and tons of rules about where different parts of the nose have to be relative to other parts of the nose, where it has to be relative to the ears, and where these lines are. I mean, so that's how people learn to draw faces, they don't look at faces, they have to be taught incredible level of detail, but for some reason we can't generate what we can see. Yeah, or like, isn't there like the simplest strategy is just to turn an image upside down if it's of a face, and then you kind of use these like low level detailed models essentially of local components. If you want to copy it, actually. Yeah, if you want to copy it. Yeah, that's of course easier if it's a photo. But yeah, you can do that. But I'm just, but that's still just copying because then you're basically saying don't be biased by the fact what you think you know about faces. Because people, whatever people think they know about faces is wrong. And, so don't, don't think you're drawing a face. Draw these little details. But that's not a generative model. That's just copying something, you know, to give you a blank sheet of paper and say draw a realistic face of someone looking, you know, sideways. It just. It's really, really hard. You just can't do it unless you've been taught and trained for years. It's fascinating. Anyway, so I guess back to your original question, the models we've been doing are not generative models, and, you know, interesting, just with a simple little typing example, I quickly felt like, oh, to solve this problem, we have to be, we have to rely on models. models that are in the motor, you know, in the motor, the wear pathway, to actually execute the right behavior. So, I'd be loved, I wouldn't go so far as to say those are generative models, but they're clearly separate places.

Another way to put it, I don't think there's generative models and inference models in a single learning module. That would be, seems unlikely. It seems like, you know, generative models are ones when you're actually trying to create behaviors to do something. Those, those might be mostly all in the what and sort of where modules. But I mean, aren't the models at least generative in a, in a basic sense? Like, they might not be very good at generating. Like, that's at least what I was trying to get at, was that, like, You can make predictions, it's just that those predictions aren't so accurate that you can make a hundred predictions and produce this really nice image. It's like, given some information, you can make a prediction to a next movement or whatever. Maybe, but think about the issue with the teeny changes to a face that you see is wrong. That would say that the predictions are pretty accurate. you know, you just, you know, if it's just the slightest bit off, and, so that would suggest that it's, it's not a fuzzy prediction, it's pretty precise, at least in some places. Yeah. So, but would you say we have any exact generative models? It seems like any column can output actions and generate something, it's just not going to reflect the detailed models that we might have inside. Well, I don't know. I mean, I don't want to generalize too much here, because I think we're not, I don't think we can come to any conclusions yet, but I would say clearly I have very precise generative models, but these are ones I've practiced, right? I don't know if I have any precise generative models that didn't require a lot of practice, but I'm just trying to think of, I'm trying to think of some. Yeah. You mean like in the sense that I can form very precise words and like have exactly the muscle movement so you can understand me, but it's not like, like that generation of words. It means that just because I can recognize words and understand what you're saying, doesn't mean I could speak. And, and, and it, it, it could, you know, yeah, I'm just, I don't view as, you know, we never really, really understood what V1 and V2 are trying to communicate to eye movements, you know, when they send the projections subcortical. I mean, yeah, you can say, oh, projections per echolocus, it's going to help move the eye somehow, but we don't really understand what the language is and what they're trying to do. And so I, I don't know, I've never thought of the learning models as we've been discussing as, as, Okay. It's generative. I always felt like the way you phrased it, that anything generative has been practiced a lot. And it's probably like a model free, more model free kind of policy. Yeah. Interesting. I, it seems a little severe to suggest that, but I'm trying to think of counterexamples.

I had a year training teacher who, his approach was, let's say you're learning different musical intervals or whatever the task is. you could have someone poke them out on the piano, and then you go, oh, hey, that's a major theory, you just learn to recognize it over time. But this whole thing was, if you can learn to sing it, if you can generate it, you'll be able to recognize it every time. Like, once you've developed the generative capacity, inference is 100. Like, learning intervals, like in ear training. Yeah, ear training. Yeah, yeah, I took a couple semesters at it.

and it was true, it was a really hard way to go about it, but if you can generate the intervals. You can, like, you'll never have any problem recognizing after that. Yeah, it is hard to generate intervals like that. Just, you know, someone says, you know, sing a minor seventh. It's really hard to do. even major thirds are hard to do. What I, what I will do is I'll, I'll, that's what you said, I'll, I'll sing a song that I know begins on a major third and then I can sing a major third. That's how you do it. well, all right. Well, I think we're, we're poking around a big area here, which is, about You know, when it comes to doing highly skilled practice behaviors, which even making the coffee cup is like that in many ways, I think it's very likely we can't just rely on the kind of learning modules that we've been learning so far. We'll have to have some sort of practice behaviors to do so, and, I was just thinking about my, my two, two year old grandson who's in the house right now. He understands many, many words. But he can't say them. So many of them sound the same, but, you know, so it's a really hard trick, but he understands the words and he thinks he's saying them. the rest of us are scratching our head. interesting. I know it's, it's related somehow. He can't generate it, but he recognizes them and knows them. Just one last, I guess this kind of relates, I think, to what you're saying, Viviane, about like not sharing models. The learning modules, but it also feels like when, yeah, when you're like drawing a face that like, so much of your model at that point is based on compositional representations, in terms of like facial components and stuff. So even if you go to like a point in that generative model and you're like, okay, now I'm going to like predict what was here. It's like, well, what you're predicting is like a compositional, like some subcomponent. And then it's hard for your, maybe the motor system to be like, okay, well, now I need to produce an eye or whatever, which is why maybe like just going as low down the system as you can, like, okay, now I'm literally drawing an eyebrow. So you can do that well, but then the problem is you're so focused on that small model that you've lost the, the global model of like, where's this eyebrow meant to be in this broader face. And then it always ends up being kind of distorted. So it's like, we're either kind of engaging like a very detailed model, or we're engaging a coarser one. and, and neither one of those is really sufficient to draw like a, a good, face.

yeah, I'm not, I read it all. I be, but I'm not sure where it's going. may maybe. Yes. Sorry. We should have time for your whiteboarding. Oh yeah, that's all right. so I dropped the link to, into the chat. Oh, wow. That white keep, sharing my screen. But I'll also, you know, you can go in there and. I'm not sure. What are we looking at now? So this is the example problem that I sketched out, because there are a few things I wanted to try to get clear on. One is What is I'm sorry. So what tool are you using here? So this is called a ScalaDraw. I dropped it. If you go to the Zoom chat, you should be able to open it up. Okay. Okay. All right. So here's a task you're suggesting. What's that? Now you're switching it to a new task, which you're going to suggest we think about. Yeah. Okay. It's the one I thought about, and I was thinking, you know, I'll just, Bring it to the group and see what you think. Great, great, great. if you hit K, I think it's K, you can, laser point.

okay, so here's the idea that I wanted to start with.

it's a hierarchical goal. We've got a lamp, it's got a light switch on it, and we want to turn the light on, so it's already a compositional object.

so there are several like sub-goals to complete here.

first off, we've got the current state of the lamp, which is off as indicated by the grad background. Before we go on, can I, can I ask a question? Are you imagining we already have a modelless lamp or are you imagining that we don't have a Modelless lamp at this point? I'm imagining everything is learned. Oh, so I know where the light switch is. because like if this lamp was on my nightstand, I would know where to reach to hit the switch. I wouldn't have to look at the lamp at all. but if it was a lamp I hadn't seen before, I would say, oh, how am I going to turn it on? You know, and sometimes you get, you can't find where the switch is. This one's obvious, but, And then that's a different problem, like that's a goal oriented problem where you don't, you know, you don't know the solution and you have to find a solution, which is like maybe there's a switch, and where is it, versus the one that I've learned already, which is then it's, it's a little bit less, then it feels a little bit more rote, like, like the learned task as opposed to I have a goal, how do I achieve the goal? But anyway, you're imagining that it's, it's something Well, that's a good point. I actually hadn't thought about that. I originally started with a light switch on the wall. And then I, which doesn't really change things too much, but then after talking with Niels, put it on the LAMP to make it a compositional object, work more easily, more clearly, and there's no real exploration at this point. There is kind of like, there's some learning stuff in the next column over, but, I guess if I already know this LAMP, I have a model, but then I'm really not, I'm not discovering how to solve the problem. I just, I know the solution and I just need to get my finger in the right position relative to the LAMP.

somehow, like, maybe the model of this lamp, I would know that, somehow in the model of the lamp, I know that there's a switch that I can rotate, and, that's one of the features of the lamp, and I know that if I press it, the light goes on, which is a very different problem than saying, oh, here's a new lamp, I don't know how to turn it on.

I'm not saying either one's right, I'm just, I wanted clarification of what you're thinking when you're going through this problem. That's a distinction I hadn't really thought of. All right. I'm suggesting it's important. Yeah.

Okay. So, I just named these visual system part one. I didn't want to give them, any concrete names here to say that there's any tiers or hierarchy or what the hierarchy is involved. But, but essentially there's a, these two things are not equal. And because this is so goal state of LAMPON is, transmitted.

And then there's this like magic that happens, which is that you need to have memorized. Or learn somehow that there's an association between the, the switch and the, and the light bulb.

and then down here, sorry? Oh, I was just going to say, yeah, I mean, I can wait, but I guess like, yeah, just like here, for example, I think, It would be more like the goal state that it's transmitting is the switch on, if like the lower level learning module is a model switches.

so this is the important part of the, that I was, yeah, but maybe this is what you're going to get to. Yeah. Well, part of, part of the reason I wanted to do this is I wanted to clarify what the goal states were, like, do they reflect what's happening in the upper level or do they reflect what's happening in the lower level? What's the domain? I think it's more like, yeah. So like the upper one, it's the lamp, like is the lamp object. So it can have a goal state. It's like kind of driving goal state that it's potentially received from somewhere else, is the lamp is on. So it's like the state of the whole object that it models. And then, potentially, it has, or like, ideally, at some point, you were exploring this lamp, the switch went on into the on state. So that's, that's like a low level object at a particular state. The lamp suddenly went into the on state. So then would that look, when you were at that like location, both on the lamp in like physical space, but maybe you could argue in some sort of like state space, like being on, then, the, the switch was also on. And so there's like a learned association there. Right. In terms of what this goal state is, is it, as it moves down to the hierarchy, is that goal state going to be lamp at this point, or is it going to be switch at this point? Right. I would say it's going to be switched because then, yeah, because then it goes straight to the top box would be lamp on and that top box has the model of the lamp and it selects. Okay, for the lamp to be on, we need the switch to be on, and then it sends go state of switch on to the, to whoever models the switch.

Okay. It's interesting, I wonder if the, a couple observations here. This is a nice switch, because we're all familiar with this type of switch, and it's not clear which is the on position, which is the off position, because it feels the same, you know what I'm saying? It's like, depending on which way it's rotated. So that's an interesting one, to turn the switch on is a matter of finding the ramp and pushing down on the ramp, as opposed to knowing which end is on, and that's an interesting observation about the switch. It seems like the model though, the model, one model, we have a model of a lamp. And one of the features of the model lamp is this switch, and this is, and we have a model of these switches. This is one type of switch. I know this type of switch. We all do. I know what it feels like. I know what it sounds like. so I just assigned that, that is one feature of, of the LAMP model, and, and the LAMP model knows that, somehow, you know, the LAMP model knows the different states of the LAMP, all in the same learning module. The LAMP is either on or off, those are two states of the LAMP, the switch is in one position or the other, that's two states, that's also a state of the LAMP, I believe. but now, when you actually want to, when you want to turn the light on, again, imagine I was doing this, it's on my nightstand, it's at night, I may, I say, oh, I know there's a switch on this lamp, I know where it is, I, I can find it, but which way I push it is going to involve the switch model, it's like, at that point, I'm, I think like, oh, I'm feeling the switch, Yes, I need to push against the ramp that direction. And that, that's not, that feels like that's the switch model, not the lamp model. I mean, you know, especially if I'm not super familiar with, maybe the switch can be, maybe the switch is like a three way switch, and so you can't say which way it's going to be when the lamp is on. so it feels like there's two clear models here. One is the lamp, which includes the switch, and, and it knows where the switch is, but then when you actually rotate the switch, you, you, Especially if it's like a three way switch, if you know what that means, then, then you would, you'd have to invoke the model of the switch and say, oh yes, I'm invoking the model of the switch. I feel the ramp. I know which way to push. So those, those, I'm just speaking, I'm thinking out loud what are the two states I would know.

One observation I also want to add is like I'm looking at here and there's like this model of switches and off switches and on. now that Jeff pointed out that he just. Hitting a ramp on a switch, that model could be simpler that it's togglable. Like, and not explicitly actually modeling any on or off state is just, I just need to switch lamp. I need to invoke togglable action without that's, that's a better words how I just saying it. This is not an, this is not a switch where you know the on and off. The general model of these switches is you toggle it, right, and so you have to orient your finger to the switch, then you know what to do. Obviously, if it's on a particular lamp on my nightstand, and it's always in the same position, then I can remember, I don't have to do that fielding, I can just remember I need to push it this way. but in the general case, you would have to invoke the model of the switch and, and toggle it. That's a good way to put it. There is no on and off. It's just like, okay, just change it.

That kind of highlights, it's not modeling specific state. It's modeling more like an affordance, which is. Feels like a different state space. Maybe, I don't know. Yeah, I was trying to pick a switch. Like, it originally was a switch on the wall, but I was trying to pick a switch that had two really identifiable states. Interesting. You pick one that isn't. Typically the wall switch is up, is on, and down, is off. Again, except if you have a three way switch. Where you have multiple switches in the same circuit and then depends on what the other one is. That's where the three way switch is. That's where the three way switch is. It's a confusing term. It's not, it doesn't have three positions. It means that, for whatever reason, that's what they call it. So you don't know what's on and basically you just have to toggle it to change the state and you can't say what it is.

yeah, that's when you have like three, three switches or two switches controlling the same light.

Anyway, it's interesting. I like this little switch. But I think it's fine, Scott, this example, even if we don't, if we can't identify which set is on or off from the looks of it, we'll still be able to tell whether we changed the state of the switch after applying an action. So, yeah. And I agree. I really like this example. It really nice and simplifies everything. So we can really focus on changing the state of the object. Let me, let me add a slight complexity to it. Sometimes these switches are on the back side of the base of the lamp, right? And so if you don't see a switch on the front, you run your hands around the lamp trying to figure out where the switch is, right? And, it could be up by the, by the light, it could be down on the back of the base. But let's say you're running your finger behind the back of the base. And then you feel the switch. Well, you don't know what kind of switch you're going to feel. You haven't seen it yet. And there could be other switches. There could be a button in the back that you push in and push out. There could be a toggle switch with like a little lever on it. So even that is like, this is a little bit more complex goal oriented behavior. You have a lamp, you see the lamp, you recognize it as a lamp, but you don't know where the switch is. and how to operate it. and so now you have to find this, you can have to find the switch and you can either visually or tactually, recognize, oh, this, this is a, this type of switch. I now know how to behave, generate the behavior with that particular switch. Or you can imagine, just finish, I'm sorry Viviane, just let me finish the thought. Sometimes the switch is on the cord, so if you don't see it on the lamp, then you run your hand down the cord. Until you find the switch in the chord, and sometimes those are toggles like this, and sometimes they're little rotating wheels, which have a click on and then a dimmer. So, we know all these things. And so it's interesting, you're discovering where the switch is, what type of switch it is, and then you can know how to operate it. I'm sorry, Viviane, I just wanted to get that out of my head. Oh, no, I was just going to add to that, that's given that you already have models of switches and types of switches. You just don't have a model of that specific lamp. Right, well, but you recognize it as a lamp. Yeah. And you say, well, there has to be a switch someplace. Right. Yeah, and for that to work, it feels like there needs to be a learned association between, so like for a lamp, you know, there's a learned association, between like the change in the high level object states becoming on and the low level object state, the switch on or switch toggled. But then there's also, you know, for a familiar object, a location associated with like a particular switch that you can go to immediately. But then with like a novel one, you can somehow rely on the association just between those states without having to rely on a specific location. So you're like, okay, I know I need to get, I need to find a switch basically to, to then be able to toggle it, even though I don't know in the case of this lamp exactly where the switch is.

I'm just trying to think in terms of L6 would, for a familiar lamp, would store like an association between like a particular location where a, the switch is, but you, we can't rely on that, entirely because, yeah, we can see new lamps and still understand that, we need the, the state to change. So it's kind of a, It's like a different learned association or like a complementary. It's a great problem scope because I can keep getting more and more complicated. I can think like, so it seems like when we, somehow the fact that there is a, we recognize a lamp, there is sort of like, there seems to be a behavioral model for lamps that might be independent of the lamp itself or, or not. I mean, it begs back to the question, is there like a, a behavioral state model, that can apply to different objects? So I could say like, well, Different objects all have, there's many different types of objects that need to be turned on and off, and and I have, and so there's a sort of general state of, you know, you turn it on and off, there must be a switch someplace, and that applies to different objects. I don't have to learn it on an object by object basis. Somehow I can apply it to different objects. and I can apply similar strategies, different objects. So I can have a very different type of lamp. and I went through the same sort of search, you know, pattern like, Oh, maybe it's on the back, maybe it's on the cord, maybe it's behind the bulb. I'm just thinking again, speaking out loud, there might be, it's a good problem because it might involve the need for state models, and behavioral models, independent of particular objects, I guess. Yeah. I think it's also, it's a nice problem because we can start. Very simple with knowing a specific lamp and just focus on the, motor decomposition part, and then make it gradually more complex by learning how to turn on any kind of lamp and modeling any kind of, Or maybe we do it, maybe we do it the opposite way, Viviane. If we do it the opposite way, doing the hard part first, you might, you might. Actually, we have to be careful that we saw solve one version. We, yeah, we came up with a trivial solution, which doesn't apply. Yeah. It depends. I agree with you. It's, it's simple and complex at the same time. And maybe we could agree to that. Yeah, it depends. It depends on what we wanna focus on first. Do we wanna focus on the moderate decomposition and goal state part, or do we wanna focus on the lamp recognition and behavior modeling part? Yeah. I don't know, but it's a great problem. It's very isolated. Lots of, you know, lots of different variations, compositional structure of, with behavior components like, you know, with the behavior of the switch, we have the different types of switches, you know, as we're searching for the switch, We know we're looking for a switch, but we don't know which type, so how do we do that? How do we search the space of switches only, you know?

you know, we're not just moving our finger around recognizing anything, we're looking for our switch. Yet, we don't even know what the switch is going to look like. Yeah, I've been kind of thinking about like half complete columns. Like if you take state, but you strip it of, can you make basically a representation of an abstraction by stripping away some of the details in the location space or in the state space? Well, I think that was, that was the suggestion. Maybe you're talking about something different, but that was the suggestion. You might have a state model and an object model. You, you can surely, you're, you're separating the behavioral states of an object from its physical state at any point in time. I'm not sure if that's what you're just saying or not. Maybe you're saying Right. So, but it would still be in the same column. It would just be that there's like well defined activity in the superficial layers and I don't know, that was purely speculative on my part, you know, I don't know if that's true, it was a nice idea that you could have, you know, object models, you know, you could have a set of state models in the upper layers and a set of, object models in the lower layers, In a single learning module, and then you could mix, then you could mix and match beha, you know, state models with different object models. That seemed kind of cool. we could do all that in one learning module. It doesn't mean it's right. it it, one of the, one of the reasons I suggested that was that. If you don't have, if you separate them, you say, okay, this learning module learns objects and another learning module learns states, well, we're trying, I'm trying to preserve Mountcastle's principle of a common cortical algorithm, so that would mean I'd have to have columns that are, have the same architecture as, but some are, some are doing state modeling and some are doing object modeling, Okay. It's possible. It's certainly possible. And then I have to somehow, I'd have to have to have some way of mixing the state models with the object models, in some circuit way that was efficient. And I, you know, I don't know. It just, I always try to do as much as possible in a single cortical column, so I threw them together. That could be wrong. Doesn't have to be right at all. It sure feels good.

This is the last step of this is that, again, we've got two states. We've got our current state, which doesn't match the goal state for the switch. You know, I can revise these. But essentially the association at some point is made, and we've got this delta between finger here and, and here, and the job is to just minimize that delta, bring it to zero. I think in a way that might be analogous to the delta between switch off and switch on in state space is, The delta we want to collapse to zero. I'm not sure if it maps so nicely into abstract state spaces, but. Well, again, bringing up the idea of it to toggle, that makes it really interesting to think about because, Yeah, it seems like we wouldn't, so as the goal state, We wouldn't transmit the actual model of how we want the switch to look like, but more that we want to put a certain amount of pressure to that ramp. Then the motor system can bring the finger there and put the pressure there, but it could also use any other part of your body or even like a pen or something to flip the switch. Really the goal is not to have one end the switch. The goal is to toggle the switch. Right? And so unless the switch is already in the on state, well then you might see I need to go change the bulb or something. Well, how would I know? First thing I would do is I would know to the on state, I, I would, I would, if I saw a lamp like this and the bulb wasn't on and I saw a switch, I would flip it. And then if it didn't turn on, I'd flip it back and forth a few times. Like, what, what the hell is going on here? Yeah, then I go look and see if the lamp is plugged in, right? That's the next thing I do, and if the lamp is plugged in, I make sure there's power in the core, and now that it's actually working, I say, maybe it's the bulb, right? So, I guess the point I want to point out is that to switch the switch is not a get it in a particular position, it's the, it is a behavioral, Action to change its position or change its state. because it could go, we don't know which way it's supposed to go. It, it's, it's, one of the thing I know specific to this switch and I know which side I have to press and the goal is to have that side down, whereas I say, I didn't know this particular switch. I have to figure out which is the upside and toggle it. Or if it was a little lever, I have to flip the lever over. If it's a button, I should push the button. So all those are just sort of like, state changes as opposed to physical changes. you I mean, and it does also feel like, to a certain degree, we, we do kind of model if the switch is on or off, but it's just independent of the morphology, like, and it's, it's inferred from more than that. So like, if, yeah, if you saw the, the lamp was off, you would infer initially that the switch is off. But if you turn the switch on and the lamp is still not on, and now you're in this state of kind of uncertainty. Those are all correct words, but I'm not sure how it feels to me. I feel like if I was doing this, I wouldn't think on or off. I was just find the switch and toggle it. Yeah. Again, when you, when you're familiar with it, I'm, I guess I'm more like if, if I was really trying to understand like, okay, someone's like, oh, this lamp isn't working or something, can you figure it out? Like, I'm going to be thinking much more about the state of the switch and whether there's power going to the sockets, if I'm changing the light bulb or whatever. So like, you would have like an explicit. I'm not saying you always need that. I'm saying the opposite. I feel it's the opposite. Imagine I got this switch, and I just, my inclination is just change it. It's not like I know which is on and which is off. I just change it. And if it didn't change, if the light didn't come on, then I'd go back and forth. I literally would go click, click, click, click, click, click, click. Like, why isn't this coming on? You know? Not even thinking about which position should be on. It's just that by changing the state of the switch, the light should come on someplace. So I'm, I'm just ing it subtle detail, Neil, but I'm, I'm thinking like at this point I'm not, I don't even know which is on and which is off. Unless I was saw and I saw one and a zero, then I would know. But it's a subtle difference. It iss not worth arguing about. yeah, it seems like the, the switch can't tell what stated is in like only the lamp model can tell whether it's on or off. And then the switch model can only tell you what side is tilted and what side is not, which tells you where you need to put the pressure to change the state of the switch, which then the lamp model can recognize, change the state of the lamp. So I came up with three different switch types. There's a toggle, there's this little rampy thing, and then there's a push button. And all of them, there's not an obvious on or off state. It's just, I just need to change it, and the expectation is that if I change it, the light will come on. and in a particular lamp, if I learn that particular lamp, I might learn the specific direction it is on and which is off. But in terms of an unknown, like a goal, like, oh, I'm turning on this lamp, I haven't learned it yet. It's just, it's, it's more this concept of toggle, Tristan or whoever suggested that, but I liked it.

I think the state change thing is going to be important, and it's something that I had to think about a lot when I was working through this problem, and like, state changes do seem to be super, like, informative, and whether or not, like, does your goal, can a goal state be a transition? Can a goal state be a state change? Because the way we've been thinking about it is the goal state is an end state that you want the world to be in, but sometimes it seems like your goal is the transition. I don't think it's the world, it's the state of a particular object. Okay, it's the state of an object, but I mean, the object could be the table setting, the object could be coffee pot's got coffee, or it could be the lamp is on.

Like in the case of a toggle, after you've completed the goal of pressing it, the state of that button is the same. Physically and everything. What do you mean?

If I flip a switch up, now it's a different physical characteristics than if I switch it down. So, like, it's easy to think about. But it is only if you know the orientation. If you don't know the orientation, it's the same in either position, right? I don't know if it's on the wall or something. Well, if I just, this little switch is symmetrical, it, it, the, the one down, side down, one the other side down looks exactly the same outside of the context of a particular, orientation on an object. You, you can't look at the switch and know which is, if it's on or off. The same with the toggle switch. Imagine it was an elevator button. It would be identical. I was just going to say that. Like a button that doesn't change. Right. Well, it's an elevator button. It glows afterwards, I guess. You must have like a remote and you push a button. Afterwards, it's, it's back to how it looked before. but the goal state of like the button might just be to go through the sequence of having the button indented and then coming back out again. It doesn't need to look different afterwards. Here's one way to think about it. We clearly have models of switches. I have a model of this type of switch because I know all about it. I have a model of toggle switches. I have a model of push button switches. So when we're searching the lamp, we don't know where the switch is, whether we're visually searching it or tactually searching it. We know we're looking for a switch. Somehow we bias the search algorithm for a switch, and my finger goes around the back side, all of a sudden I feel something, then I know which type of switch, and then once I know that type of switch, I know its particular behavioral model and what I'm supposed to do with it. until I find the switch, I have no idea what I'm supposed to flip a toggle, push a button, or push the ramp. So, we can, we just can decompose the problem into we're searching for a switch of, somehow your brain has to be able to say, I am searching for a switch. Then it, and it doesn't know which switch, then it finds something and says, yep, this is a switch, and then it says, given this switch, I know the behavioral model of this switch and how it behaves and how I should interact with it.

And maybe that's why so many LAMP switches are just an action you have to perform. Like, if it's a chain, you gotta pull it, or a button, you just gotta press it. Like, it eliminates the need to actually check the state of the switch. Right? Though it's interesting, as you pointed out earlier, most switches on a wall, I don't know if this is true worldwide, but in the United States at least, most switches that are on a wall, you have an up movement which turns it on, and a down movement which turns it off, unless it's a three way switch.

so that, you know, so if I was approaching, I wanted to turn a light on in a room, and I was approaching two switches, and one was down and one was up, I would first try the down one and flip it up, you know, so. There's an exception for switches that are on a wall, we have a different behavioral model. So we have to, we have to account for all that stuff somehow, right? Another one, my parents have this lamp that doesn't have a switch, and you can just touch the lamp in any place, it will turn it on. Oh, cruel. Yeah, I was very confused the first time I saw that lamp. Right, right. And they have those lamps in hotel rooms where they have like a turny switch and sometimes it takes two turns to turn the lamp off. Like the first one clicks and doesn't do anything. You have to do it again and it turns off. You're like, all right. Those are the worst. They should eliminate those switches. They're just so unreliable.

You're a clapping one. You can do it anywhere in the world. As long as you're like, I'm just wondering if it's. Like it's, if everything's happening actually in the affordance space and it's sort of just based on environment feedback, we then say, Oh, it's a switch, et cetera. It's like the lamp is on off and I want to change it. And then I'm just seeking for, to, to affect associated change that I learned from changing and everything else is incidental. Well, maybe, but that's certainly true, because I could come across a switch I haven't seen before, but, but it's also true that we can very quickly recognize any one of the pre learned switches, right? so we're definitely, what we're searching, maybe you're saying we're searching for an affordance.

I mean, mentally I'm thinking like, no, I'm searching for a switch. And maybe, maybe I'm wrong, maybe I'm searching for affordance, but I, I even have certain expectations like where the switch would be, right? I already said, well, if I don't see it, it might be on the back of the base, it might be on the back of the lamp part, or it might be on the cord, What makes me think about affordances first and like a switch being already like a learned more concrete behavior is. If it's just a completely unknown object, you're just going to mess with it. You're just going to affect change and see what changes you observe. Well, right. But we're not here. We're not talking about a completely unknown object, but we're talking about an object that we know is a lamp and we know has these two states of being lit or not lit. I think that's true. If I'd never seen a lamp before, I had no idea what it was, and didn't know they'd produce light or anything, then you're right. We would just pick it up and play around with it and stick your finger in the socket and get a shock, you know? So, that's true, but, we could think about that problem, but the problem is it's posed so far, it's a lamp, we know it's a lamp, so, you know, maybe we don't want to start there, but that's.

But I think, I think it's a good point. Maybe you've never seen a lamp before, so how do you learn what lamps are? You know, you, you might play around with something and see that the light comes on. There's, there's a lot of kid toys like that, that give, you know, a lot of kids they can go on right now. But you know, they have these toys where you just, there's a bunch of brightly colored buttons. If you press any of these buttons, you know, a light, they light up and some sound or music plays. So the kid learns it, pushing a button. pushing something, you know, they don't know this in advance, but they just learn, they back around those things happen. Yeah, yeah, we have this lamp where you just have to hit the top of the lamp and it will go through four levels of brightness and Link loves this lamp and he like Figured out really quickly just to, like, hit the lamp on top. I mean, he's not, not dexterous enough yet to actually flip switches. So this lamp is perfect. Just without fine water control, you can go through different states. And yeah, he learned it very quickly. Right. We did, we did the same thing with the two year old grandchild. There's a, there's a light over our kitchen counter and there's a switch on the side of the counter. So in the beginning we told them it was magic. And we point to the light and say, Jamie, say, say on. You say on, and we flip the switch. And he says, say off, and we flip the switch. And then you go, oh, I'm off, I don't know, it didn't work. And then, then we showed him the physical switch, and he, then he got it right away. It's pretty funny. But, you know, it's interesting, you don't really know this stuff until someone teaches you or you discover it accidentally.

it would be helpful for me if I could leave soon, because I have all these things going on. can I, I think this is, this is, I think it's a good problem. Scott, Niels, whoever worked on this, I don't know, Scott. I can drop this thing in the, TMP research channel also, if you want to take a look at it later, or if it undergoes any changes. But this is the last bit, was me trying to take a crack at learning the association between switch states and lamp states, in the model in which columns have state and location. But that's the last bit here.

I still think the idea, I'd love to have, and maybe you have it here, that we have models of switches.

Imagine we did a toy problem. We have a model of three different types of switches. Each one has its own behavioral model. And then we create, you know, somehow we have a model of lamps. Somehow, we have to know that somehow, when we see a lightbulb, there has to be a switch. I mean, I don't know how we do that, in that we're looking for the switch, and when we find the switch, we decide what type of switch it is, then we know what behavior to do. This, this, this deals with a lot of uncertainty. It's not all learned, right? We don't know what the switch is, we don't know what type of switch it is. it's a very, very simple version of the coffee pot problem, perhaps, you're trying to achieve something and you're searching for the solution.

yeah. Sure, yeah, so just like real, real quick, like, I wanted to take this abstract diagram we had on the left and map it onto columns where we've got state and location and part of that was learning the, basically a causal relationship between the switch and the Light, and what that might look like. And so I just started simple here, like there's no motor system.

a T0, lamp off, switch off. At the next time point, the switch goes to the on state here, and there's basically just a discharge because of the state change that propagates up to the lamp, and let's just say the lamp also goes on, like instantaneously, which is, you know, for all intents and purposes the case. and we get our typical spreading up into superficial layers, so there's some kind of connection forged between the state here and the state here, as well as some kind of information about, I mean, the fact that this is coming from the switch module or column, I would think would have some connection to the representation of the switch down here, perhaps. I would, I mean, I think so, because when I want to invert the problem and say, how do I use my memory to decide how to turn off the lamp, I know where to go to on the lamp to do it. And then I say in the next time step, so this state has also changed, and I didn't want to draw everything all like simultaneously on the same diagram, but since this state has changed, there will be like some emission out of here, out of here, just going nowhere, but also leaving the deeper layers arriving down to the lower part of the hierarchy. And because of this connection we have here, we actually do end up getting a full, perhaps, a full association between the state changes and the two modules. We start here, and we end up here.

So in a very vague, hand wavy way, I thought, you know, maybe there is some way that this loop of connections, is, is the mechanism, somewhere in there, that can be made more precise, is the mechanism that allows us to create this process. Relationship between the states in the lower and the states in the upper columns. That's the extent of it.

Yeah, I think that's a pretty good start. I guess the only error I'm a bit confused by is the dotted error in T1 between L23 and L56. What's this one again? This guy? Yeah.

That represents the fact that, like, I think we can assume a certain amount of columnar communication, right, that goes across the column, so that there can be some intermixing, there's got to be some intermixing between what's going on here and what's going on here. The reason I wrote the arrow is because If we take this path here, so here to here, layer, you know, these deeper layers, and then back to here and here. If we don't end up here at some point, then we don't have the top state of the module, or we don't have the state of the lamp. So we kind of need to have some kind of communication along this axis in order for these projections to contain state about the lamp.

Yeah, yeah, I mean, I think there's definitely I mean, although I think the major input to L3 is from L4, I think in general there is reciprocal connections. Sorry, hang on. So yeah, I'm not sure. Oh, did I change something?

There's a cursor visible. Yeah, I think you've moved, like you're moving an arrow. I don't have anything in my history of changes, but let's see. maybe I'll just draw it and then reverse everything. But At least how we talked about so far was that there's like a bi directional association between the features and locations. And then the actual state and ID of the object is just inferred like this, but there's no backward connection. I think before the changes, you had both of these errors bi directional. I'm not sure why, why there would be a connection down again to, a five, a six. Or also, what exactly the difference was between the dashed and solid errors.

So I just left the solid ones as the ones that are in the diagrams that we all know and love, the way, you know, input comes into L4 and then it spreads out from there. Those are like the solid ones and the dashed one I added because if we want state information to exit, lamp state information to propagate to here, then we need the dashed one. So it's dashed as in speculative or required for. In some way for lamp state to make it down, to make the association with switch state.

what, why would that require the connection from So I'm thinking about this circuit, I'm thinking about the circuit, the leaves two, three here, goes up here, and then eventually it's going to leave this layer and come back down here, right? Yeah.

Leaves the deeper layer, lamp. And there's a deeper layer switch, and then it does have this ascending connection that sort of synapses out here.

So, in order for there to be an association of some sort between the switch and the lamp, I kind of need to get information that's up here to exit from here.

I don't think we need that, because Yeah, sorry, Niels, you. Well, I was just going to say, yeah, I mean, I think there is, like, evidence of connectivity from L2, L3 to L6.

yeah, and I mean, I think, like, I agree that we want to condition, like, L6 and probably L5 on the state of the object. and even the But the location space is already unique to the object, so the location already tells you something about the object ID, essentially. We don't need an explicit connection from L2 3. But if the state, like Jeff was kind of saying, is like a separate representation, and If that can influence those things, then, yeah, then maybe that's how that kind of You mean the state is in the same location space, but like, wouldn't the state then also be different locations?

Yeah, I mean, I guess it depends on how we, we implement it, but like one, one thing I was thinking at least with that, like, The way you path integrate over the, the reference frame is kind of conditioned on the, the, the state.

But I mean, I, I think, I guess here the, the point is that, like, so yeah, somehow, so we, we have the goal state in the higher level, L2, L3, depending on that goal state, what is, cause like the, the only way that we get the feedback from the higher level column to the lower level one is the projection from L6, so depending on the high level goal state, The information that's being projected from L6 is going to be different.

didn't we talk about the goal state coming from L5?

it depends on whether it's hierarchical. If it's hierarchical, it would be from L6. If it's, if it's direct subcortical one, it would be from L5. Yeah, but I wouldn't, you said from L2 3, but that would be the actual state. But that's what's in the higher level one. And that's what's influencing what goal state, like, let's say the goal state is, I don't know, unscrew light bulb or something like that. Like, we're going to want to predict a different thing, like L6 is going to want to project a different thing to a lower level, learning module.

So it's like conditional on. Within the learning module, whatever the output goal state is, is going to be conditioned on its own internal goal state. And if that's represented up in L2, L3, then, yeah, I mean, yeah, I guess the other thing, I don't know if this helps, but like, It could be that the goal state is more kind of, like the actual state is L2, L3, the goal state is more in L1. And then it's just whatever is sending apical dendrites up there. That's how it's being kind of communicated down.

L1 doesn't have the descending, action outputs to lower No, but again, L6, could, I believe, have projections up there. Maybe it's Kira. Definitely L5 does. I'm getting a bit confused by the different L. Numbers. Definitely L5 has, projections up to, yeah, I think the way I was thinking, at least was, although L6 is the only way to send, like, hierarchical connections down from like one column to another. If L5 is the, is the kind of motor source, it has connections to L6. So maybe it's the one that's saying, like, like, instead of predicting, like, oh, you should be. Seeing this, it's like somehow changing those down projections, that's like, you should be, what you call it, enacting this, like this is now a goal state, but, I guess just to rewind it, like, I think that's what you're getting at, Scott, right? That like, somehow we need to bias the output goal state by the target, the module.

yes, eventually. Yeah, it's a lot of States and L's. Yeah, no, I mean, I think everything you explained on this figure, Scott. I agree with and it's a really nice drawing. I was just a bit, I'm a, I'm a bit skeptical of this, like, dashed arrow between, like, especially going down from L2 3 to L5 6 within that column and why we need that right now. But, Yeah, we may or may not.

I was just trying to think about a way to make sure that whatever state information we have about the land, Is accessible when information leaves layer six.

That may be a just Yeah, I think we can definitely Oh, sorry. Go ahead. Sorry. Well, yeah, I guess in the case of a lamp, there's no morphological change between the two different states. So it seems like it couldn't just be, entirely encoded by whatever is in, in the space of the lamp. It also needs to get some, well, I guess, features.

I think there's probably like a variety of ways like anatomically could happen, but like maybe the most reasonable one is L5, which has like a lot of apical dendrites up in L2, L3. Like it gets a lot of input from there.

and so, yeah. And if L5 is generally where we think a lot of motor stuff's happening, like that could, it has a lot of reciprocal connections to L6. So basically that information gets transferred down to L5 from L2, L3. And then L5 is somehow biasing the output, of the hierarchical connection from L6. And it seems like it kind of must exist anyway, because what the state is has implications for what the location representation is. Like this, you know, these locations are different when this is an off state than when it's the on state. So there's got to be some communication between these layers.

there's a connection right between the state and the location representation, just to really know how it happens. So, it seemed okay. Could that correlation be looser if the state doesn't model state but just, but a point in that state space is just a dimension which you can change? Togglable, smaller, larger, that sort of thing, versus an actual, this is large, this would be sizable, this would be togglable. Then that correlation wouldn't need to be as strong.

So like a contained state space versus a discrete state space and how that relates to the location representation? Something like that?

I think so. I'm not very, I don't have very refined thoughts on that yet.

Yeah, I'm not sure. Maybe I didn't understand, but I think in general It's probably fair to say that we're very uncertain how we're, how we are going to model states, like, you know, what Jeff was saying earlier, like, maybe it's this whole, like, two separate things, or, you know, maybe there's even a whole separate reference frame for state space, but I think one thing's for sure, and, like, this is already the case with the goal state generator, is, like, it has an output goal state, and it is, that is conditioned on its own driving goal state, so. And like, that seems essential to like somehow communicate that information. so I think that's a fair thing to show and then yeah, we can debate how it's implemented. I guess my problem with that connections from L2 3 downwards is that L2 3 wouldn't know about the state of the lamp if it didn't get the upward projection. So, if the lower layers don't already, haven't already inferred the state, then L2 3 has no information whether the lamp, if they don't already have that information, then L2 3 has no way of knowing whether the lamp is on or off. So, there's no point in sending it back down, because it just got that information from the features and locations.

I see that.

Yeah. Maybe it's clear if, cause we probably want the, like, this probably needs to be separated in the neurons as well, like to separate out what's like the actual state and what's the goal state.

yeah, that, that is presumably going to be different. Alpha 3 represents the actual state, not the goal state. Right.

Well, we're over. I'll like let everybody go, but, if you feel like messing around with any of this or making your own diagrams, I'd just like. Here's all the assets, basically, you can just copy and paste, come play in the canvas. Nice. Yeah, these are really nice figures. Thanks for putting this together. That's awesome. Yeah, and I really like that example you came up with. Thank you. It was good for me to like sit there and try to get some things down more concretely, think about it more carefully. Yeah. Yeah, it's great. Thank you. Thank you.