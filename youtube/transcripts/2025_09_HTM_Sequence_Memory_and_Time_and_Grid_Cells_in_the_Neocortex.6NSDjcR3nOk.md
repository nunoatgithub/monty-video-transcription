JHawkins: early on in our research, many, years ago, I came and I was thinking about melodies. This might have even before we started the Menta. I came to this realization that the brain has this really big problem it has to solve over and over again. It has to be able to represent things that are commonly observed in the world, and it has to be able to represent them uniquely in different contexts. So you had to have two different representations. You had to have one representation, which was the thing you're observing, and another is an equivalent representation, but it's unique. And they had to be tied together. So you think about it in a melody. the notes are coming in, or the intervals are coming in, and they repeat, over and over again. Different notes, each, at different points in the melody, but you have to have a unique representation at each point in the melody. And this is a general thing that applies to, sensory motor as well. So this... this was a big problem, and this was the genesis of the sequence memory. So the basic idea was. It was suggested by anatomy and Physiology, was that, if you take a layer of cells, and this is gonna be... by the way, this is going to be a neuroscience presentation, so again, you have to interpret this, how we're going to do this in money. Many columns represent the features. And they do so in a non-unique way. Meaning that they're just, the input comes into the brain from a sensor, ears or whatever, and it forms a representation of what it's sensing, and that representation is a set of mini-columns, and if I sense the same thing again, the same minicolumns will become active.

And, so this, these particular features can appear on multiple objects in multiple locations on an object. So it could be edges, like the visual edges we see in V1, they can appear at different locations on an object, different objects. They can also be intervals in a melody. and if you look at the biology, it suggests that a column of cells can only represent a limited number of these features. So if we look at, in V1, you might see there's oriented edges, we might call point normals, or, edges of an object in some sense. And, They go through maybe every 10 degrees, so there's maybe, 18 or 9, depending on how you look at it, or excuse me, 36 or 18. And then you'll see, some of these mini columns will be active, and it's because they're clustered. here I show For these little mini columns active. Because there... several of them will be active around a particular orientation. If we think of this like a line orientation, there might be one or three or four mini columns that are active to a lesser or more degree, depending on where they are. And, so there's a general belief there's a sort of a Mexican hat inhibition. Between many columns, and so if you look at this as a two-dimensional sheet of tissue, I'm only showing a picture of one dimension. You'd have a little bump of activity that's centered around the current feature, and the adjacent mini-columns would be related, slightly different features. And... and that, as you observe things, this bump would appear in different locations on the sheet of tissue. You have to have multiple bumps if this is gonna work. And I'll come back to it in a moment. So I show this picture showing, these four little minicoms active. What it would be, there'd be lots of minicoms, and there'd be actually multiple bumps of activity, but all the bumps would be representing the same thing. It's redundancy in some sense, That's a lot to say, I didn't draw a picture of this.

Niels Leadholm: Just to clarify, sorry, multiple bumps in each kind of hypercolumn? each column.

JHawkins: Yeah, I'm gonna... I'm gonna skip right ahead to... to the... just... just show you this next... this next slide here. Remember, this is from the tank paper about mini columns? I've got, grid cells, and if you recall, we've discussed this picture before. This shows one grid cell module. And in that grid cell module, there are 6 repetitive units. 6 units are basically mini grid cell modules. each of these 6 units, is a complete tiling of the space. And in this case, what they observed This... they observed there'd be 6 bumps of activity in this grid cell module. So there'd be one in each of these six quadrants here, and they all represent the same thing. And as a bump moves, in this case, these are grid cells, so if the location changed, a bump might come off to the right of this object, of this column, and more bumps would appear on the left. So you have to have multiple... to make the system work, you can't just have one bump of activity, because that bump of activity could be on the edge, and there's not enough cells involved. So I... this picture here is a grid cell module, but I think the exact same thing is going on in columns in the cortex. Both in Layer 6 and Layer 4, that you end up with multiple bumps of activity, because you have to have... you have to have it for the numbers to work, and you also have to have it because at the edge of these columns. You have, bumps of activity disappear, and you have to have other ones reappearing on the other side. So what I'm... what I'm referring to, I realize there's a lot at once here, is that we are... what I'm showing a picture of in the previous slide. I'm showing just one of these little areas here, equivalent, and I'm only showing as a 1D picture. I'm showing as a bump of activity at a 1D set of grids of many columns, but in reality, this picture here of grid cell modules is very equivalent to what I think is going on in the cortex, where you have many... in some sense, every one of these cells is a mini column, there's more than shown here, and there's... there's multiple sets that are doing the same thing.

Is that the visual cortex?

Niels Leadholm: In visual cortex, yeah, as you move through the column, along the... Parallel to the surface of the cortex. You get the gradual change in the orientation. And

JHawkins: have...

Niels Leadholm: He... but then you would have repa... let's say this is the preferred orientation here. Eventually, you have a repetition of that over here.

JHawkins: What... I'm not seeing anything you're drawing, where are you drawing?

Niels Leadholm: Oh, I'm, using my hands.

JHawkins: Oh, I'm sorry.

Niels Leadholm: Okay, so let's... so this is the column.

JHawkins: You have the preferred orientation here.

Niels Leadholm: And then as you move along the surface... oh, do you want me to...

JHawkins: I lost... I lost... I lost my Zoom screen here, where the hell is it? I no longer see anything about Zoom.

Niels Leadholm: If you stop there, you'll go either way.

JHawkins: That's gonna cause problems. There we go.

Niels Leadholm: Okay, you see me?

JHawkins: Yep.

Niels Leadholm: Yeah, so if this is the column, and so you have a preferred orientation here. And as you move along the surface, that kind of gradually shifts. And... and eventually repeats. And so what you're saying is, yeah, firstly. obviously, because this orientation is similar to this one, and this orientation is similar to that one, that's the kind of Mexican hat in a certain area, multiple columns. But then, later on in a column. the other hat is where that same orientation appears again, assuming the only input was... This bar, kind of thing.

JHawkins: And that's... that's general, I didn't make that up, that's... that's... a lot of people speculate that's how it works, and But no one's really articulated in the way we talk about it. But the idea that there's a Mexican hat inhibition between the many columns, that explains why they're bunched together like that. It would also explain how many different representations a column can represent, because it's within that spectrum of a Mexican hat inhibition before you get the next bump.

those things all play together, but I think you've got it right. So you can imagine looking down on the surface here of a cortical column, just like that picture I was showing of the grid cell, and you'd have several bumps of activity, all kind of representing the same thing, and a new input, you'd have another set of bumps in different locations, but they're also representing the same thing. Anybody else have further questions?

Niels Leadholm: But...

Hojae Lee: Yeah. Huh. Is it that... All the bumps together. Create a unique representation for that edge.

JHawkins: No, we'll get there in a moment, okay?

Hojae Lee: Okay,

Niels Leadholm: But yeah, I guess the sparse... the TLDR would be the sparsity in the mini column will be critical for that.

But.

JHawkins: Are you... are you seeing my picture again now?

Hojae Lee: Yeah, your slide 1.

JHawkins: Yeah.

Hojae Lee: Now it's black. Okay, now, we're good, alright.

JHawkins: It's funny, because I switch, and now it changes my sound. All of a sudden, my sound's my computer again, what the hell? I don't know why I'm doing that.

Alright, you guys can hear me still.

Hojae Lee: Nope.

JHawkins: Oh, it's doing...

Ramy Mounir: We can hear you.

JHawkins: Now it's coming back in, it's like it's randomly switching my speaker in and out. Okay. alright, so do we have... do we have an understanding of this or more questions.

Niels Leadholm: Yeah, just a quick question, it was still on the shape of the bumps. just as a sanity check, from a lot of Numenta's previous work and publications and stuff, didn't necessarily have this bump, Right. The idea was there, but they would have been one feature was, like, a random selection of mini columns. Another random selection.

JHawkins: We were... we were testing out the mechanism Which I'm about to describe. And, we had... we weren't really applying it in any particular instance, and,

Niels Leadholm: Yeah, I just wanted to check my understanding. memory.

JHawkins: if I could go back with a magic wand, I could make all the things we presented in the past equal to what we have today, but, it's not. so we didn't talk about that. In fact, we had... we would take... we tested the system sometimes with... with, You could have, thousands of objects, or thousands of features, or, thousands of mini columns. Each one could be a sort of a... random distribution of many columns. It wasn't like that. Bumps and all this kind of stuff.

Niels Leadholm: Okay.

JHawkins: Thanks.

Alright, so this is... this is how you basically represent a feature in a non-unique way, and going back to your question, Jose, the different bumps do not add any new representational capacity. At this point, they just add some redundancy.

Hojae Lee: But the next thing is gonna make a big difference.

JHawkins: The next thing is that the key insight was that, is that if you... if you assume that not all the cells in this... in this minicon, which, again, we're talking about one layer of cells, so I'm labeling it's layer 4 here. There may be anywhere you can take your guess anywhere, say between 10 and 20 neurons in a mini-column in that layer. If you would just activate only one of those. Cells, like the blue dots here, and not all 10 of them, or all 20 of them, whatever number you pick. then, you're able to create a unique representation of that feature. So you're still activating the same mini-columns. But by picking 1 of 10 cells. or one of 20 cells, you get this very, high representational content. So imagine you had 6 bumps of activity, each one had, let's say, 5 active mini columns. You pick 1 out of 10, so that's 10 to the 30th is the number of things you can represent by selecting one of these cells. in each mini column. And we worked through a lot of the math about this. You could pick them randomly, and it would still all work, and they wouldn't interfere, and all that kind of stuff. But this was the basic idea, is you just... if you picked one cell per mini column, you'd end up... this is a unique representation of the same thing. So these two representations would be co-located in the cortex. At the columnar level, at the minicolumn level, or at the cellular level would be two different representations of the same thing. And so you could have many, different ways of representing this feature in different contexts in the world, and each one of those would be very unique. So that's, that was the basic idea. And we did a lot of work to make sure the numbers worked out, and they did. It seemed like the brain picked just the right set of Things to make it all work.

Just so I can...

Hojae Lee: concrete. By representing the same thing, let's say it's You can select different cells, within... A mini colon.

JHawkins: In the active mini columns.

Hojae Lee: in the activation column, and let's say that represents color red, it will still be, even if the different cells are activated, it'll still be color red, it's just that.

JHawkins: if you looked at it in the... red's not a great example, we could use it. Okay. If you looked at it, it said. Okay, Let's think of it... let's think of it as an edge of an object. Okay, can we do that first? There's an edge of an object, so it's... the active mini column just represents some sort of edge at 30 degrees.

Hojae Lee: Okay.

JHawkins: that edge could be on different places on the same object, the 30-degree edge. It could be on many other objects. In fact, there's only maybe a dozen different edge orientations you can observe. it's going to be one of those dozen, but what you want is that representation that is unique for a particular object on a particular location, and unique in all the objects in all the locations. So this is what... this is very much what we see in border ownership cells. This is exactly what border ownership cells are. They look at that, one of those blue dots, and they say, hey, this cell's always active at the tail end of the tiger. at this location on a tiger. How could that cell only, in V1, be active on this location, on the tail of a tiger? It's because it's a unique location on a unique object. And, Now, these cells, even the blue dots, are not unique themselves, right? They're more unique than the mini columns. But it's a combination that's unique, right? It's really the combination. But in the case of border ownership cells, one of these blue dots would always appear on the... on this particular point on the tiger, and it wouldn't appear most other places. But it would appear occasionally in other places, too.

Hojae Lee: Okay, fine.

JHawkins: Does that make sense?

Hojae Lee: Yeah, so even if we have selected different cells, it will still represent the 30-degree edge. It's just that it'll be... Active in a different context or a different location.

JHawkins: the gray dots are your common feature edge.

Hojae Lee: you.

JHawkins: And the blue dots are the unique representation of that feature.

Hojae Lee: Okay.

Niels Leadholm: And for what it's worth, yeah. And that fits really well with, neurophysiology studies, like the Hubble and Wiesel experiments. I can't remember if you've come across those, Hoji, but when they

Hojae Lee: Good luck.

Niels Leadholm: Orientation preference in cats.

Hojae Lee: Yet.

Niels Leadholm: And stuff like that, where if you go... if you penetrate the probe down, perpendicular to the surface of the cortex. All of those neurons just seem to respond to the same orientation. Which seemed really weird at the time, because it was that's a waste of information. And then if you go perpendicular, you get that gradual shift in sort of preference of orientation.

JHawkins: I always... I always thought that was a very weird thing, too, Neil. I looked at it and go, that can't be right. The brain... why would the brain make all these cells do the same thing? And, the pat answer people sometimes gave was like, oh, they're all just, it's redundancy, it's a... that kind of thing. I said, that doesn't make sense. And of course, what other... other things happen when they... when they get all those cells in the minicon to respond, the blue dots to respond, they have to give them, sinusoidal gratings, which are non-objects, those are things that you don't really... there's nothing unique about them. But they noticed that when the animal would observe a realistic image. That the... the classic receptive fields, which were defined by looking at sinusoidal gratings, those classic receptive fields went out the window. Cells didn't respond as they expected them to. So they said, oh, this cell responds... this blue cell responds to an edge at 30 degrees. Because we proved that with the sinusoidal degrading, but then they show the animal real-world images, and that cell doesn't... often doesn't respond at all when it's got a 30-degree edge. And they're like, what the hell's going on? that kind of thing. This explains all that. Okay. By the way, it doesn't... it doesn't even have to be one cell per mini column. It could be 2 out of 20. so there's nothing magic about that one. It just has to be sparse.

Hojae Lee: so this allows us to, represent 30-degree edge in many, different contexts and.

JHawkins: Yes, gazillion subconscious.

Hojae Lee: yeah.

JHawkins: it doesn't mean you can remember them all, but the representational capacity is there.

Hojae Lee: Okay.

JHawkins: It's very large. Now it's... we need that. If you think about, if this is an interval in a melody, there are thousands of melodies, and each have many notes, and that particular interval appears many, times in many different places, in different contexts. I need to be able to represent... maybe there's only, 12 melodies... 12, or let's say 25 intervals, or... 12 notes, whatever you want to call it. There's a limited number of things that go in a melody, yet you have to... each time a new note comes in, you have to have a unique representation for that input at that location in that melody. So that was, the genesis for this idea. So then, to make a sequence memory, or a model of sequences, it's pretty simple. You just have to associate the active neurons at one moment in time with the active neurons at another moment in time. So I'm not showing that here, I'm just showing the one set of active neurons. But if... imagine if you go through a sequence, you would have a series of these activations, these blue cells and the mini columns, and each time each... each note, or each element in the sequence would be different. And then if you just associate, one pattern to the next through horizontal connections. You would learn to associate pattern A with pattern B, and then pattern B with pattern C, and then pattern C with pattern D, and so on, in a high-order sequence.

Viviane Clay: when we talk about the Layer 1 input for the specific timing between elements in the sequence, are you thinking of that as being, like, an additional conditioning,

JHawkins: Yes.

Viviane Clay: The current element biases, predicts the next element, but that only works if the correct timing signal also comes in Layer 1?

JHawkins: But I... I was thinking of showing that in this figure, and I said, oh, it's so complicated, I didn't have time to do it. And then I said, oh, I bet you Vivian or someone's gonna ask me about it. My prediction is correct. Yeah, I'm not showing that here. What I'm showing here is just, it shows the transition from node A to note B, or element A to element B to element C, and element D, and so on. But, as in melodies, and often in other things, there's usually timing between those, and so you don't want to just run through these sequences as fast as you can. You need some way of gating the transition between one element and the next. So that would be the layer one input, which I'm not showing here.

And of course, it's complicated, I won't get into the complication, but anyway, the idea basically would be that You... when you're at an element in the sequence, it predicts the next element. But you don't want to... you don't... you also want to know when it's going to occur, and so the... the next set of cells would be depolarized, but you wouldn't really expect a note to come in until the... until the correct timing signal was presented on the apical dendrite. And, so I didn't show that here. Because we were just talking about locations.

Is that... is there questions about that?

Viviane Clay: Yeah, And then, just to double-check that... we're not misunderstanding on the, like, how location in the sequence or element in the sequence can be represented. So you say, ordinal location cannot be decoded, but the SDR is unique to that element in the sequence, right? we know where in the sequence we are, but there isn't, a common representation for the first element in the. It is decodable.

JHawkins: Yeah. If I had my magic wand, I could look at the SDR that is the activation of these cells, and I could say, oh, that is the 23rd note in Beethoven's 5th Symphony. Yeah. But... but it's... I can't... I'd have to... I'd have to have a magic wand, or I'd have to have learned an association with that, which we're gonna see in a second.

Viviane Clay: Yeah.

It's like how the locations are unique to the object as well, a specific location representation doesn't necessarily translate to other objects, because it's, a unique location to them.

Niels Leadholm: there's no such thing as a zero location.

JHawkins: That's... that's the... that was, like, this big... another big thing. There's no sense of zero location sequence, there's no sense of origin in... in... in our, in the grid cells. so it is true that... so this is where Vivian and I got a little confusion here, because this is encoding the location on the object, but it's... it's not like anyone reads it. Anyone can... you can't read it out, and you can't say, let's go to the fourth location. You can't do that. you... you can just say, I have an SDR, and I can... and you and I know that it's the third location, and it predicts the fourth one, but I can't say, look, what's at the sixth location in the sequence. You just can't go there.

Viviane Clay: But two columns could vote on it, they could form associations between their specific unique SDRs at that point in time.

JHawkins: But that would... but... but... I don't think they would, but you could, but that still wouldn't tell you the location. I'm just saying, there's no way of reading out, this is the nth element in the sequence. You can't read it in the SDR. You can... you can associate... if I had a separate column that was just keeping track of ordinal numbers, then yes, I could associate with that ordinal number.

Viviane Clay: Yeah, I agree with that, that there isn't, a 1, 2, 3, 4, 5 representation anywhere, but why would you not think that they could vote on us?

JHawkins: you said another column. In my next reveal here, the SDR is voting on location, so can I just jump ahead to that? yeah. so this is the basic mechanism we had for the model of sequences. And, we tested this a lot, just to make sure it worked, and the numbers worked out, and so on. We didn't do anything realistic with it.

So then, when it comes to modeling objects, sensory motor learning, it's the exact same thing But, we're using the same cells, in fact, let's say layer 4 here. So we have the feature, we form a unique representation of it, so we have the non-unique representation at the mini columns, feature not unique, then we feature unique object location.

So the... so in the... in the previous one, the model sequences, you can just, as long as A follows B, and then B follows, C follows B, and so on, you can learn that up here in a high-order sequence. But now we want to do sensory motor learning. you, the order in which the patterns appear are not a high-order sequence, and therefore, in layer 4 itself, it cannot predict what's the next input on it. It doesn't know that. So our basic model for modeling objects, which we're all familiar with, is you have a second layer of cells that represent the location. And we're pairing the location with the pattern in layer 4. And in this sectional cells, labeled 6A, were representing location, and... and this is how I viewed it, and I've just walked through it. I've always felt like the grid cells themselves are not unique, remember? They... they're... they... they do not represent a unique location object. They're like one of 15 or 16 possible locations, and they repeat over and over again. That's very much like the orientation columns in Layer 4. very similar to that. It's a... it's a non-unique representation of location, and so I've always viewed it this way, that the minicoms down in lower layers, some of them are going to represent grid cells themselves. And then if you had a separate set of cells underneath them, the blue dots. So then you could make a location that's unique to both the object and the location on the object. I said this is speculated because this is not generally... observed. And I'll come back to that in a moment. But this is what's required, theoretically, to make this whole system work. you have to... you basically have to say, okay, I have a non-unique location, which is grid cells. Those are updated by movement, and then I'm going to pick a unique version of each one of those things in the same bumpy way as we did in the pump of activity, which is what we see in grid cells. So now I can pair the unique location with the unique feature up above. And since the blue dots in the layer 4 and layer 6A are both representing... are both unique to the object and to, to the location on the object. They are both... one is updated by sensory input, and one is updated by movement of the sensor. But they're equivalent. in some sense, they... they both are unique to the location and the object. And therefore, if one, you can predict the other, and if you know the other, you can predict the previous one. So this is the genesis of our basic idea about, how, centrimotor modeling works. it... you do... I've convinced myself, and maybe correctly or incorrectly, that you have to have this unique location to make... to make this whole system work. It wouldn't be sufficient just to use grid cells, which are not unique. You have to have another representation Which is unique. And if you assume that's the case, then you can imagine a parallel construction between the upper layers and the lower layers. Just like this, and now this will work. And so inference is a matter of sensing and moving and sensing and moving. You're updating both of these two things simultaneously, and as you go. You narrow down the choices that, things that are available that match both, the sensors and the movements of the sensors. this is... yes, go ahead.

Viviane Clay: And how does the different re-anchoring, different anchoring of the grid cells play into that? does that create an extra amount of unique representations?

JHawkins: It only does if you have multiple grid cell modules, right?

At one point, we said, oh, the way we're going to get this unique location is to use multiple grid cell modules. And that's in our paper... the first paper we wrote about this, that's what we assumed. But it never really worked. There weren't enough grid cell modules, first of all, and it didn't look like you could access all of them, and they had different scales, substantially different scales, which was also a problem. we kept looking for more grid cell modules, they could exist in more places. And that would solve this problem, too. You could have multiple grid cell modules in a column, and each one would anchor differently. So that was one idea. It never really worked out, because there weren't enough... it wasn't enough cells, there weren't enough good cell modules to do that.

And... but that's how we started. This is an alternate flavor. This says there's another way to get to a unique representation of location, and that's to think about grid cells as more like.

activation of a mini column, and that, then there would be unique cells That fire, that fire uniquely for each of those grid cells, as shown here. If I had to guess today, I think this is the more likely scenario, the one I'm showing here, and not the one with the multiple grid cell modules. And I can give some reasons why I think that's the case. dumb.

Ramy Mounir: Cool.

JHawkins: The important thing is you have to have a unique location.

Ramy Mounir: And a quick question about the model of objects. We wouldn't need to learn associations or transitions between multiple consecutive layer 4 representations in this case, right? Because we're making association with layer 6.

JHawkins: Which is a great point, Rami. You don't need that. If I'm... if I'm moving my fingertip randomly over an object, or different directions, then layer 4 can't learn any transitions. However, the Layer 4 cells are still the same layer 4 cells, and those... those axons still go horizontally, so in this picture, they're still there. And the point about this is. That this system can... can... it can flow between sensory motor... inference and learning and sequence inference and learning. For example, if I... if I move my finger the same way every time when I picked up an object, I just... I learn to say, oh, I'm gonna run my... I play with this object the same way all the time. I can pick up my pen and I play with it in the same way. then the sequence in layer 4 would be a high-order sequence, because I would be going through the same motion every time, and therefore I could learn it as a high-order sequence. It would be both a sensory motor sequence and a high-order sequence. And I see evidence of this all the time, that when we practice something, we go from being totally reliant on, thinking about how we're going to move our fingers and doing... to basically being able to just play out a sequence naturally. As a sequence. Imagine this is now, a layer 5, column... a layer 5, which is the motor output of a column. if I... if I... if I practice the same motor output over and over again, it'll naturally learn to play that as a sequence, as opposed to, individual sensory movements, you know what I'm saying? You can go back and forth between these two representations. Yeah, so they're not, mutually exclusive.

Niels Leadholm: That's another example.

Ramy Mounir: Sorry, I was just gonna say, I'm wondering if we could generalize, basically the same system that we're using for models of objects, but we use it for models of sequences by just having a 1D grid cell in layer 6A, And learning the transitions between, basically, going from 1 to 2 to 3 to 4. And if we're building associations with the features in Layer 4, then that could drive the sequenced learning in Layer 4.

JHawkins: I didn't follow that in the beginning.

Ramy Mounir: So if we just have... if we assume that we have, a 1D grid cell in layer 6A, and we're just learning a sequence, we're not learning a model of objects, and we're building associations between layer 4 and layer 6A, I'm wondering if we could learn the sequence by... By just having those associations, and we just learn the transitions in the grid cells, between location 1 to location 2 to location 3, and that drives the activations in layer 4.

JHawkins: I think... I don't think you can learn... Can you learn the sequences in Layer 6?

Hojae Lee: Basically, you want to add.

Niels Leadholm: Yeah, I guess that would be advantageous if there was some... yeah, something about the movement through the space. I think one example I think we talked about is how with songs. like you were saying, Jeff, L4, you would just learn note-to-note, in Layer 4. But... maybe... displacements through... tonal space or something is movement in L6. But I guess, yeah, what you're saying, Robbie, is more like. I don't know, just the movement forward in the sequence, whether that's time or... Action. That is

JHawkins: You don't... you don't know, in the... in the Layer 6, you don't always know what's gonna go... in the same direction,

Ramy Mounir: Unless it's a sequence of time,

JHawkins: But the sequence... any kind of sequence we learn down there has to be of the... of the sparse representations. It has to be of the... of the population.

Ramy Mounir: But, I assume with Hebion Learning, we're going to, just basically two transition... two representations at layer 6A that always happen,

JHawkins: it would be the blue dots we're talking about over here, right? Yeah.

Ramy Mounir: It'll be similar to Layer 4. They would still form associations because they... we still go from one representation to the next. Even in locations, it would be just, location 1 to 2 to 3. It's unique to the sequence. It would still form those transitions.

JHawkins: maybe it confuses me by talking about 1D grid cells. if I... if I just look at the picture I have in front of me. And I... I follow a particular sequence. of patterns, I move my finger in the same way over and over again, then that... you could argue, in theory, then you could load this sequence in layer 6A. In the same way we learned in Layer 4, is that what you're saying?

Ramy Mounir: Yeah. Yeah.

JHawkins: that could be... The one slight hesitation I have on that It goes back to some of those Thompson papers. And I don't want to rely too much on her work, but it's all I got.

they... she just, when you talk about the connections in Layer 4, people... everybody say, oh yeah, all the cells send horizontal axons to all the other cells, so this is... the horizontal green thing is there. It's very, clear. But when you look at layer 6A cells, she often describes these cells as very, The axon branches are very vertical. they don't seem to... they just draw them as, they're sending these horizontal axons throughout Layer 6A.

I don't know if that's true or not. But that's the only reason I didn't draw that arrow there. I haven't seen someone say, yes, in layer 6A, we see all the cells send horizontal actions across all the other cells. If I... if I'd seen those words, I would have drawn the horizontal ax... the horizontal green hour in layer 6A2. It doesn't mean it's not like that, I just don't know.

Ramy Mounir: That's interesting, yeah.

JHawkins: Yeah. Yeah, I... it's just unknown. To me. There's a lot that's not shown here. it's not shown how grid cells come about here. And for that, there seems... there's this need, as I've talked about in the past, there seems to be a need for these one-dimensional, sets of cells that fire at different phases, remember that? The different frequency of the speed. So there's a lot more stuff that has to come... has to exist for these grid cells to exist. And that other thing is a bit like a... if you recall, we debated about... I drew it as a set of cells in the minicom, each one firing at the same rate, but at different phases. And then there was a separate argument made in the literature that it could be different Different dendrite branches could be flowering at different phases. a lot of mystery here. that, I don't know the answer to them.

Niels Leadholm: I was recently reading, or watching some talks on how, insects, like bees, perform path integration. And I've been meaning to share my notes on that, but... I think that can be interesting as well, because they have a fairly primitive form of path integration. But it still does the job. And it's also, I think, neurally simpler to implement than full-on grid cells.

JHawkins: You want to make sure it's as powerful.

Niels Leadholm: Yeah, but whether it's powerful enough, is the question.

But yeah, I'll share that soon. But it was interesting how they find a solution to, if I found some interesting flowers here or whatever, how can I follow a path?

JHawkins: From an arbitrary starting location and things back to it. One of the, one of the general themes that we've talked about, and I've observed, is that you think some of these mechanisms start out for very specific problems that animals have to solve. And they're not very general solutions. And then, over evolutionary time, what happened, especially in mammals, especially in humans, you've taken these sort of specific solutions, navigating in environments, or going from flower to flower, or whatever it is, and we made a very generic version of the same mechanisms, which are now much more powerful, we can use to model anything. I'm always gravitating towards, mechanisms that are very generic. And you have to be careful when sending something like an insect. You might have a solution that works really well for certain things the insect does. But it wouldn't work for modeling, coffee cups with logos on them and things like that.

just, that's just a... I always... I always, when I look at simpler non-mammals, I always think... keep the. my head. okay, could work for great for this ant, but, what about, or this bee?

Niels Leadholm: Yeah, I think it's always just appealed to me, because that works in these absolutely tiny brains. And so it... it, whereas, entorhinal cortex is a decent chunk of brain, And Just getting something that will fit inside of a column.

JHawkins: I think.

Niels Leadholm: as an interesting problem, but...

JHawkins: but,

Niels Leadholm: but... I agree with you.

JHawkins: Yeah, we just don't want to get fooled into thinking, oh, our brains do it the same way that bees do it, or something like that. Anyway, this picture I've shown here is a very... From an abstract theoretical point of view, it's very, broad. It can be applied to anything, really. This is just about forming, forming representations and forming unique representations, and this solves... this mechanism solves sequences and sensory motor, high-order sequences and sensory motor sequences, and it can go back and forth between them. So that's... that's some of its appeal to me.

Viviane Clay: Yeah. Yeah, I like the elegance and simplicity of it, and how it brings back the sense... the HCM mechanism into the picture. I guess, yeah, I'm just trying to think through... concretely using it for object behaviors now, and... I think the main reason I drew... in the diagrams I made, I drew the, location and sequence as a separate representation versus, stored in the associations in Layer 4. was because... I don't know, I just started thinking about it right now, so maybe I'm... if I think about it for a few more minutes, I'll figure it out, but... the issue I see with this is, like, how does the sequence work in combination with the location representation?

I have a unique representation in layer 4, For that location, at that point in the sequence. But then, I move my sensor to somewhere else on the object, so layer 6A representation changes, but whatever feature I predict in layer 4 at that location depends on which state in the behavior I'm in. And if I don't have a separate representation of the behavioral state, that's... currently, I don't see how that... how that works.

JHawkins: Alright, so let me just see if I can rephrase your statement. This is... if we wanted to bring in... Behaviors, which are another type of sequence. Behavioral models, right? And there's a... there's a... How does that lay on top of this? And...

Viviane Clay: Yeah, like a sequence of states of an object, basically.

On the surface, it's not clear to me why it wouldn't fit onto this, but I haven't walked through it yet.

Niels Leadholm: Yeah, it feels like it requires hierarchy in this setup, because... I think the object-level state... I don't know if this works, but yeah, the object-level state would be the input to Layer 4.

JHawkins: can I... can I make the...

Niels Leadholm: Something like that.

JHawkins: If Layer 6A is a direct implementation of location. now we're talking about location and spacetime, right? Four dimensions. And if I... if Layer 6A could represent Could... could include time. then Layer 4 doesn't have to change.

Viviane Clay: So the...

JHawkins: You could add it to layer 4, but it seems to me like... I don't know, it seems to me like, given I'm calling Layer 6A location, then it's a location in spacetime,

Viviane Clay: so... is that what you're suggesting, that the time is actually encoded in Layer 6A and not...

JHawkins: I wasn't... I wasn't... first of all, time in terms of In terms of just, like, how much time between elements in a melody?

We're talking about something different here. We're talking about where we are in a behavioral sequence, right?

Viviane Clay: Yeah.

JHawkins: I hadn't thought about that, or I can't remember what I thought about it right at the moment. it's oh, what did we think about that? I'm trying to recall. so I don't have, I didn't walk in with, a preconceived notion right now, saying, oh yeah, that's how we're gonna handle that. I didn't... I'm just thinking right now, in this mechanism. where would it be? How would I know where I am in this sequence?.

Niels Leadholm: When, is there a reason we'd want to treat songs and behaviors differently?

it would be nice if it's the same...

Viviane Clay: Yeah, so that's why I drew the sequence representation as a separate thing, it wouldn't be, like, a 4D space in layer 6A, it would be... I think I put it in layer 5B, or... but it could also be, like, a separate representation, layer 4, or even layer 3, since that connects... since that sends up the apical dendrites to layer 1. But it would even be a very simple representation. It would just be this kind of one-dimensional next-step kind of representation that's always the same if we're at the same location in the sequence of a particular behavior. And then, if I'm at that location in the sequence of that behavior, that tells me, for all of the locations in that object's reference frame, which features to expect.

JHawkins: if I'm following you. There's... there's two approaches we can... to do this. One is, you start with this diagram at the bottom of this figure. and you say, oh, I'm going to, I'm going to make sure Layer 6A is actually representing Space-time. And therefore. the representation in Layer 6A is unique to both the location, and the object, and where I am in the sequence of the behavior. The other approach is to have an external signal, which is A sequence signal of the entire behavior.

And, then that... that was an additional... Factor to incorporate to make a prediction. Is that... does that make sense? Those two basic ideas?

Viviane Clay: Except for the external part.

Do you mean external to the column, or to outside?

JHawkins: I'm just saying it's not here in this picture.

Viviane Clay: Oh, yeah.

JHawkins: I didn't.

Viviane Clay: So basically, just someplace that disentangles the current state from the current feature on the object, or the current location on the object.

Niels Leadholm: Yeah, it feels a little like how... in the same way that we have, a local feature, and then we have the object ID, it feels like there's a similar thing for, the input in L4, like the sequence that might be playing out there. Versus the state of the, what the column is representing. which is.

Viviane Clay: the more Yeah.

Niels Leadholm: A global one.

Viviane Clay: Yeah, state is almost like a sub-ID of the object, or something like that.

JHawkins: The state would have to go... state would have to go through a sequence.

Viviane Clay: Yeah.

Niels Leadholm: But that sequence could... be, semi-independent, of... what's happening in L4?

Viviane Clay: Oh, actually, do you remember when we talked about, behavior models and morphology models being in the same column?

JHawkins: Yeah.

Viviane Clay: We put the behavior models in layer 3, because Layer 3 forms these apical dendrites to Layer 1. maybe Layer 3 could be... Could be learning The sequence of states, and each state is a pooling over the activations in layer 4. the object ID, and then we might have a pooling over states for the global object ID.

JHawkins: That's interesting.

Yeah.

That's an interesting idea.

Viviane Clay: And then Layer 3 could still be using... The kind of associative mechanism that you drew on Layer 4 right now for predicting the next element in the sequence.

the next state representing SDR.

JHawkins: it would...

Niels Leadholm: Yeah, or layer 5, because, that also has apical dendrites in L1.

JHawkins: The theorist in me would like the following to occur. I would like the representations in these two layers, the Layer 4 and Layer 6A, to be unique. To both the object, the location, and the... In the... where you are in the state... the state of the behavior. That would be the cleanest way,

Viviane Clay: Yeah, I think it would be in that example.

JHawkins: So the question is, if it was Layer 3... if it was Layer 3 was representing the... This... the behavioral sequence is what you're... what you're suggesting, right?

Viviane Clay: Yeah, so basically you could think of it as, the SDRs in layer 4 are unique to the location object and state, but then we pool over those in layer 3. We pool over all the SDRs in layer 4 that correspond to the same state of the same object. And that's the state... object state representation. And then we can learn a sequence of object state representations that is conditioned on time coming from.

JHawkins: I got that. The question then I had was, how does I... how do I get Layer 6A to be unique? to... to state, as well.

Viviane Clay: Yeah, it would have to connect to the Layer 3 state representation.

JHawkins: Yeah, I don't know if it does, it might, I don't know. I don't know if it does.

Niels Leadholm: it connects to Layer 5 a lot. I'm still unclear why it needs to be Layer 3, but I don't think that matters too much.

JHawkins: You're saying maybe... you're saying maybe Layer 5 is that state?

Viviane Clay: Yeah, so in the original diagrams I made, I put it into layer 5B and used those apical dendrites. I just thought back at, because I just read the document we wrote back then where we put the behavior model into Layer 3, so I thought that could be an option too, but... yeah. But... either one, I think.

JHawkins: Yeah, that's... I like this idea, if... I'm trying to summarize it in a way that makes sense to me.

It's like you have... you have... alright, we just... let me see if I rephrase your proposal. There's another set of cells that represents... The behavioral state. I imagine it would be unique to the object. And the point in the state space, where you are in that, right? And, Then I assume it's unique to the object, or it's unique to the behavior, maybe, I don't know, think about it. But anyway, and so now you have... Isn't the idea with behaviors that they are objects?

Niels Leadholm: Because we have columns that are dedicated to behaviors.

JHawkins: Was Vivian's... I thought she just abandoned that.

Viviane Clay: No, I didn't. It would be, like, the state... SDR would be unique to the object-slash-behavior model, depending on What the column is getting as input.

JHawkins: Okay, so we have a column that's just modeling behaviors.

Viviane Clay: Yeah.

JHawkins: We're talking about now.

Viviane Clay: Yeah.

JHawkins: And so Layer 4 is representing changes Changing features.

Viviane Clay: Yeah.

JHawkins: Layer 6A is representing where in space those changing speeches are occurring, and the other cell is saying, where are we in the sequence of These changing....

Viviane Clay: yeah.

JHawkins: that's... it's an interesting idea. I guess it feels like it needs a little flushing out. it's like one of these ideas, you think about it, and you go, and then you think about it some more, and you go, oh, wait a second, some problems with it, or something like that. It maybe just feels early, it's just a little bit too simple, too early, not too.

Niels Leadholm: I think it would be helpful to think through, a sequence that's being represented in Layer 4 versus a sequence that's happening in... whatever, Layer 5 behavioral state, whatever.

Cause... With that separation out, it's... it's not clear why you need any sequence memory in Layer 4.

Viviane Clay: this is how we... in my mind, this was how we thought about it before, and that's how I showed it.

Niels Leadholm: yeah, no, I agree.

Viviane Clay: everything.

Niels Leadholm: I think it is how we've been talking about it, but so now we're trying to bring back in sequence memory in layer 4, And so the question is, how do they fit together? Do we need both? that's what's not clear to me.

JHawkins: I'm not sure why you're saying you bring back in Sequence Memory Layer 4. Oh, because these are behavioral.

Niels Leadholm: Because we've not really been talking about that. Because, yeah, I agree with you, Vivian. When we've been talking about behavioral sequences and stuff, it's been at the object level, and so we've been saying, okay, this is the state we're in, and so if we go to this location in our reference frame, we're going to predict a particular moving feature, or whatever.

Viviane Clay: Yeah.

Niels Leadholm: Different from the HTM sequence memory.

Viviane Clay: But, like, why couldn't we have the same mechanism in a different layer, like layer 3 or layer 5? Especially since, as far as I remember, layer 4 doesn't really have apical dendrites.

JHawkins: there's a complication of that. That's been the classic view, because stellate cells do not have them, but now more and more people find pyramidal cells in layer 4, so there are some cells in layer 4 that get input from layer 1.

Viviane Clay: Okay.

JHawkins: Not most of them.

For what it's worth.

I didn't follow... I didn't follow your guys' conversation a second ago. I'd like to. Oh, Maybe you can try just saying it again, and let me... let me see if I understand it. This is about why... you're arguing about why we might or might not need sequence memory in Layer 4?

Niels Leadholm: I guess all the... for the past few months, we've been talking about behaviors. And, time and all this stuff. you have, a different, kind of state of the object or the behavior. stapler is opening, and so now it's in this kind of more open state, and so that's where we learned In this behavioral model, kind of some moving features at a particular location, and that's where, like you say, it's a space-time model, because it's Location in the reference frame and the point in the sequence determines what kind of features we predict.

And that was always at a... Global, object level, okay, this... this model is now in this particular state, and that's what's conditioning our prediction. Whereas sequences in, classical HTM are more, low-level sequences. It's low note A, then B, then D, or whatever. And That's why it feels to me like if we're gonna have both. Maybe it's something about hierarchy, where it's like.

The output of a lower-level learning module is... would be low-level... In the receiving one, but... Or, and I think this is what you're arguing, Vivian, maybe we don't need sequence memory in layer 4, But I think, and that's why I think it would be helpful, what is an example of a behavior Where we want... Both, where we want the low-level sequence, and where we want it at the global level.

JHawkins: I'm not sure what you mean by low-level sequence, that's confusing me a bit.

Hojae Lee: Neil, sorry.

Viviane Clay: I think it's, whether we learn a sequence of input features that come into Layer 4, or whether we learn a sequence of states of the entire object. That's, the object ID is represented in layer 2, 3, and that object goes, or that behavior goes through a sequence of states, and those states influence all of the features and locations on that object.

JHawkins: Yeah. Can... can I... let me just throw out an idea, maybe it'll help, because it... The way I would prefer this to look. Again, elegance often is worth something, but sometimes it's wrong. that if I have 3 layers of cells, for whatever reason, maybe the third layer is representing this behavioral sequence. I would love them to all work exactly the same way. They all have many columns, they all form these sparse representations. They all... they can associate With each other via the vertical blue-green arrow, but they would also all have the ability to learn patterns within their own layer, as in the horizontal green arrow. It's it's available if it's useful. It's not... if it's not useful, it's not learned.

to me, I never see a harm in having these horizontal connections, which are pretty much a prevalent feature for most layers. The one exception being, I was just talking about maybe in layer 6A, but... it's a pretty prevalent feature in Layer 3 and Layer 4 and everywhere. You get these horizontal connections, so that... that to me, if you... if every layer of cells has this basic architecture, then every layer of cells has the ability to learn high-order sequences of its... whatever it's representing, and if it can't, and if it can't, it's useful.

that's not addressing specifically the question you were talking about, which I'm still confused about, but it... But I think...

Viviane Clay: Yeah, I think that means, yeah, there isn't really an issue with... With doing both, with allowing Layer 4 to learn sequences, but also being able to learn sequences of object states in a different layer, disentangled from it.

JHawkins: I think, again, I'm not really following your Neil's argument, but The general idea is there's no harm in having it there, and then you can ask, under what conditions would these horizontal connections be useful?

Niels Leadholm: Yeah, and then... I guess then it's also a question of. In general, we've been talking about how time is obviously helpful, or maybe critical, for A lot of these sequences, and so then it's also does that mean both layer 4 and whatever layer is representing kind of object-level state is getting time input?

JHawkins: Yeah, I don't know. I don't know. Interesting question.

Niels Leadholm: maybe... I don't know if it'd be a helpful exercise to try and think about how would we represent a song in... If state is at the global level.

Because.

JHawkins: Is... is state... a global level thing, It's in the...

Niels Leadholm: again, that's how we've often talked about in that,

JHawkins: Couldn't it... couldn't it just be a voted-on? Layer?

Viviane Clay: Yeah, if... If we have a song, we could just learn it in layer 4, and maybe it is how Jeff says, that every layer has the ability to learn sequences, and if it's sufficient to learn a sequence in layer 4, then maybe those neurons form more dendrites to layer 1 to get the timing. But then, if it is a... A sequence of object or behavioral states that require more pooling over multiple locations. Then we would learn it in a different layer. As a sequence there.

JHawkins: Yeah, that's.

Niels Leadholm: It's great if we're just not... if we're not understanding it clearly enough or something, because it feels messy that it's Both of these are learning, and it's... It's not super clear to me. Why it would happen sometimes in one layer and not sometimes in others.

JHawkins: I'm still... I'm not following everything you guys are thinking about, but... again, it may be helpful, just continuing on with the thought I had earlier, imagine you have multiple layers 2, 3, whatever. They all work on the same mechanisms. The differences between them, so they all have these horizontal connections, they all can form... they potentially can form the associative connections between them, like the vertical green arrow. And some may or may not have time.

but... but they're... they're updated by different things. in this diagram I have here, one's updated by sensory features, and the other's updated by movement vectors. But, so they're all the same, they have... different things update them, and maybe some of them can use time, and some can't use time. That doesn't violate the basic principle that I'm trying to achieve here. And then... then you could say how... then you could look and say, what if I had 3 layers of cells. And layer 3 gets time, and Layer 4 doesn't. And layer 6, I don't think it does, but... You don't know, but anyway, my point is. The idea that you have these multiple representations, each layer has a representation, each one It's trying to, They predict the next input, and each one is driven by some different driving force. And then they all settle together. That's... that's a very powerful idea.

I just don't understand what you guys are... so I'm just saying time does not have to be necessary... for all these things to work under the same mechanisms, it doesn't necessarily require that all the layers have time. It requires some of them have time. But not all of them.

Hojae Lee: I think this is going back to, one of the earliest ways that we thought about behavior, which was that Layer 4 gets the features, Layer 6 gets movement, and, Layer 3 gets the changes. Those being with the driving.

Yeah, I think I copied a slide to the Zoom chat, or posted a screenshot to Zoom chat where If... all the layers working as a mechanism. There's no reason why the behavior model cannot be in like a child LM, there... and then, so basically, we thought of...

JHawkins: Should we go look at this diagram? Are you showing.

Hojae Lee: sure, yeah, it was...

JHawkins: Did Bob stop sharing then?

Hojae Lee: okay.

JHawkins: You said you posted an image... I didn't know if you were talking about that.

Hojae Lee: yeah. as a... on a Zoom chat. everybody opens? It's like a slide.

JHawkins: How do I get it to be big here? Tristan Slominski: Could you just share your screen, Hoji?

Hojae Lee: Yeah, sure. That's a good idea.

Yeah. There we go.

JHawkins: Excellent. Yeah, that's good.

Hojae Lee: Yeah, so I think that idea of, okay, feature going to layer 4, and then changes going to Layer 3 was...

JHawkins: I'm sorry, what are we looking at on this image now, right now, Jay? What do I...

Hojae Lee: Oh, this compositional plus behavior model.

JHawkins: Okay.

Hojae Lee: where... At some point, Oop. There was an argument for the behavior model cannot be, like, in the... Child, LM. But must be on the parallel line.

Viviane Clay: I think that was just about when we apply a behavior to... A morphology model that...

Hojae Lee: Okay.

Viviane Clay: We could use that connection, but the chat LLM could still have a... have learned...

Hojae Lee: shuttle.

Viviane Clay: Yeah, but then, I think that's when we put both models in the same column, still.

Hojae Lee: Okay.

JHawkins: but I'm not sure. That might still, we... that's where our current leanings, we're having them in different columns, but... If you want to make an argument why... But I'm open to discussion about... Alternate. Flavors.

Hojae Lee: I personally okay, things going, the features going to Layer 4, and change is going to Layer 3, which means that, the behavior model Morphology model can exist in the same column. I think the reason why we separated it out was because we didn't want to keep track of two different reference frames for the behavior model and the morphology model.

That would.

JHawkins: That was one of the big... that was one of the big reasons.

Hojae Lee: Yep.

Viviane Clay: And because there wasn't really a reason to keep it in the same reference frame, in the same column.

even if we send the changes to Layer 3, that still doesn't solve the problem, that we have right now, which is that yeah, even, just because I am detecting a change at one location right now, and I know which change to expect next at that location, if I don't have a separate state variable, then I don't know what change to expect after I move my sensor somewhere else.

Hojae Lee: Yeah.

JHawkins: I'm sorry, but Vivian... I... my recollection was part of what Jose said, is that one of the real things that's pushed me over the... to the dark side of having two different, a behavioral column and a was because... because it required two separate reference frames. And that seems almost impossible. Hoji, are you suggesting there's a way of solving this with one reference frame?

Hojae Lee: I'm just wondering, like, where we're, like, where we're converging to, because I feel like we would... started with one, and then went to another, and then coming back to the first one.

Niels Leadholm: I think we're still saying one, because we're still talking about behavior models. Yeah. We're not talking about morphology, right now.

Viviane Clay: Yeah, we're just talking about... Who... having different states in a behavior model.

Hojae Lee: Okay. Independent of the morphology model. So we still have two different columns. one that contains a morphology model, and the other that contains a behavioral model. And we still don't know how to exactly, Use a behavior model to predict The features to the next step in the process.

JHawkins: Were you saying we don't know how to com... how to... how those two models interact, the behavioral model and the morphology model?

Hojae Lee: It sounds like it's an open question, but I'm getting wrong.

JHawkins: Yeah, I don't know.

Viviane Clay: we've talked...

Niels Leadholm: We talked about that, but that's, a different topic from what we're.

Viviane Clay: Yeah.

Niels Leadholm: today.

Hojae Lee: I'll happen.

JHawkins: although I've embraced Vivian's suggestion of having separate behavioral columns.

Viviane Clay: Yeah, I don't see why it's the dark side. I feel like it's much more elegant.

JHawkins: I know.

Viviane Clay: Same mechanism.

JHawkins: But now, we've basically divided columns, the whole cortex has got two types of columns.

Viviane Clay: And, many times... columns.

JHawkins: I know,

Niels Leadholm: They're all columns still.

Viviane Clay: Yeah, they're all columns, they just get different inputs.

JHawkins: Alright, I'll just state it... I've accepted your logic, but there's something that still slightly bothers me about it. I'm... I'm not like, oh, yeah, that's it. It's okay, it seems to work, yeah, it's pretty good. There's something irking anybody about it. it's okay to think about I don't want to revisit it completely, but if we stumble upon something which makes us question that, then I don't mind pursuing that a little bit. Yeah.

Niels Leadholm: Can I, what do you call it? Yeah, just on the... what we were discussing earlier, I don't have a great drawing or whatever, I've got some things to point at, but can I try and Just get a little bit what maybe the open question feels like.

JHawkins: Sure.

Niels Leadholm: If I share my screen.

JHawkins: I'm confused by that. Oh, great. Thank you. This clears it up right away.

Niels Leadholm: basically, we've been talking about different kinds of, sequences, behaviors. We've been talking about things, as you describe often, Jeff, as, a higher-order sequence, it could be a song, a sequence of letters, whatever. And we also talk about things like staplers opening and closing. And if for the moment you... let me get rid of this, For the moment, yeah, you just see this one on the... on the right. In general, so this is a kind of behavior or sequence modeling column. if we say it's learning songs, and if we say it's learning through HTM, then we would say that it's getting, A, and then B, and these are coming into layer 4, and then it's, What do you call it? Yeah, then learning this kind of transition structure, which may or may not be conditioned on time. And then if it's more like a... something like a stapler, then that's where we talked about, okay, the input feature is actually a changing thing. Learned at a location, and so that's where, we would learn a bunch of, Local flow patterns here, and then a bunch of local flow patterns here, and those local flow patterns over here are associated with a different global state, or kind of state for the entire object. And with the conditioning.

JHawkins: The entire behavioral object.

Niels Leadholm: Yeah, the behavioral object. Yeah, sorry, not stapler, so this is a behavioral object. This is the lever, kind of behavior. Or the hinge behavior, sorry.

That's going to tell us, based on where we are in the hinge behavior sequence, which is some kind of more global thing, and where we are on... in the reference frame, we can know what we should predict, like some flow patterns here, or in a different location. In this diagram, where's the information that... where we are in the sequence?

I guess we've discussed different locations of that in the past. I think, yeah, both Layer 3 or 2 and 5 have come up as candidates. I guess the advantage of 3 is it maybe makes it easier to communicate to higher levels.

JHawkins: they have 5 pitches as well. Okay.

Niels Leadholm: But, it's always felt to me like Layer 5 would be nice, just because that's the kind of motor output, and The connection between, Behavior and action is so close that... yeah, but... Anyway, so I think that's, the starting point for, I think, what Vivian and I were talking about, which was just, or at least what I was trying to say was, okay, today. we were now talking about HTM, and it's... so it's a different kind of sequence, almost like a different kind of behavior. It's still time-conditioned and all this stuff. And so then the question is it really something different? Is it, You're saying, maybe... Kind of sequence learning is happening in multiple layers, and it's just... It's a case of whichever ones have associative connections and maybe have access to time, they can do it.

But I guess what I was just thinking... what I was trying to say was that If you imagine there is a global state. But we don't actually represent the transitions between states here. The, instead, you feed the, hypothesized state up. then that becomes a bit more like an input, like A or B or whatever. If this higher level column is then actually keeping track of the sequence. then it could condition the lower level one and say, oh, you are in this state, or now you're in this state. And so then this behavioral model.

JHawkins: But why did you dismiss the idea that it could be learned locally in the first column? the sequence.

Niels Leadholm: Just... It just feels It feels a little bit like everything... having multiple reference frames, the idea that we... or I don't know, it just feels weird to me that it's okay, songs are learned in a totally different way from...

Viviane Clay: staplers opening and closing, and maybe they are? it would be the exact same mechanism, it would be the mechanism, just applied to a different layer.

Niels Leadholm: But that's what I mean by that.

Viviane Clay: They're different.

Niels Leadholm: Weird than...

Viviane Clay: It's.

Niels Leadholm: Learn in a different layer.

Viviane Clay: And it wouldn't be a different reference frame, it would just be a sequence.

JHawkins: again, going back to what I made... the argument I made earlier, I think every layer, in theory, could learn sequences. if it made sense for them. So it's not like I'm representing it... it's this... it's this inherent property of a bunch of cells, so it's not like I'm doing extra ones, or at second place, or something like that. Whatever it's representing, if it can learn sequences, it learns sequences.

I don't know, I guess I'm not ready to jump into the hierarchy yet until I understand why it doesn't work in R1.

Niels Leadholm: just for example, okay, so now we need multiple layers to have access to time. But we've also talked about how, for time to make sense, it needs to receive feedback from the columns to basically tell the matrix cells whether to speed up or slow down. So now we also need multiple layers projecting to the matrix cells to tell them whether to speed up or slow down. It just feels like it brings complications by having every layer Be able to do this.

JHawkins: Maybe not, It's not clear that's true, all these layers are connected together, It would be sufficient as an ensemble of layers that they tell the matrix cells what to do. Not every... as long as there can be somebody who can represent the group, correctly, then...

Niels Leadholm: Yeah.

JHawkins: And by the way, learning sequences doesn't always mean you have to learn the timing of the elements.

Niels Leadholm: Sure.

But at least we know there are, things like this that we want to have time for, and there's things like this that we want to have time for.

JHawkins: it's funny, because when you show those two pictures, I... my first reaction is want to make them the same. I want to see why ABDFA is the same as sapler opening and closing. They're like...

Niels Leadholm: That's what I'm trying to get at with the hierarchy, is it just feels like if this state is passed to this one, and that's what's conditioning Or that's the one that's keeping track.

then the actual kind of sequence is just being learned here.

JHawkins: I don't know, to me, the hierarchy... we've found one reason for hierarchy so far. One good reason, and that's compositional structure.

And... If... if we want to... if we want to view this... sequence memory is somehow compositional structure, I'd be more... Happy to talk about it that way. But, and it's not clear to me, we're talking about that, I'm still confused, but...

Niels Leadholm: maybe that is one way to... because in the sense that... This is a... a song is a higher order sequence of very simple objects, it's just... it's direct sensory inputs, it's notes, whereas.

JHawkins: it is...

Niels Leadholm: In closing, is a sequence of a morphological object, so it seems like it makes sense that the input In sum, has to be an entire object, and cannot be... Or in this case, hinge behavior, sorry, it has to be a representation of many different changes going on at once.

And so it... in order to represent a behavior as complex as a... as a stapler.

JHawkins: That could be... It has to be.

Niels Leadholm: Compositional.

JHawkins: You could argue that's a difference in quantity, or you could argue, as you're saying, it's a difference in quality. I'd rather shoot for the difference in quantity, okay, one's one-dimensional, one's two-dimensional, or something like that, and what's the...

Niels Leadholm: Yeah.

JHawkins: Grid cells represent one-dimensional objects, and they represent two-dimensional objects, the same grid cells. why couldn't I do the same thing in a single column? I have, it could be representing one-dimensional sequences and two-dimensional... sequences. I guess I'm... I'm always hesitant to jump the hierarchy until I've absolutely proven.

Niels Leadholm: Yeah, no, I know it also has...

JHawkins: The nice thing about hierarchy is I could say, I need two columns because the columns are representing two different objects. That's what they're doing. They're representing two different objects, and I'm trying to find the relationship between them. That, I'm willing to hang my hat on. Old expression. I don't know what it means.

Ramy Mounir: Thank you. Do we lose the location-by-location basis of changes if we just pull over object states like this? Because what you're describing sounds like second-order pooling, which is... so we pull from features into an object state, and then from object state into object behavior. And I think we discussed this in the first retreat, but we ended up saying that we want those changes to be at a location-by-location basis.

Niels Leadholm: Yeah, I'm not sure. I need to think about it, but the location-by-location basis, that's captured more in the top-down connections.

Viviane Clay: Yeah, I think it would work. proposal that, if R2 would learn the sequence of states, it could inform R1 which state to expect, and R1 still has a model of which locations to expect, which feature set for each of the states.

But it's a... it's a lot of... it's a lot of machinery required for a very simple task of learning a sequence.

Niels Leadholm: and I agree with you, Jeff, if this is the hinged behavior. then... then what is this? Are they... Yeah, I'm not proposing this because I think it's perfect, it's just... When we were talking about it being in... this is basically me thinking out loud about, okay, it feels weird. that songs and staplers are different, I want to try them to understand them in a Dude, this is true. What are my intuitions for how to do that?

JHawkins: I just didn't... I didn't follow how you got to that. in my head, I'm going to reserve hierarchy for when two learning modules are representing two different objects, and we're trying to... that could be two different behavioral objects, I'm fine with that. It could be two different physical morphology objects, I'm fine with that. But they've got to be two different objects. I can't use the hierarchy as the convenience of, oh, we need to put something and stick it over there.

That doesn't work for me. It's that's... that's the lure of the death of hierarchical... solutions. So I don't like that. It's not clear to me Hey, mate, you're ahead of me on this news. I'm not keeping up with you, But I'm not yet... it's not yet clear to me what the problems are we have doing this with one learning module. I just... You've jumped ahead of me, and

Niels Leadholm: yeah, it's... I agree, it's possible. It's just... I think, as you often say, sometimes there's... you just have a feeling that something feels off.

JHawkins: Okay, but I'm not sure what it is. I look at those... I look at sequences in the stapro, and I go, oh, those are really the same thing. And you're telling me they're different things. I'm like, I'm not sure why they're different things. They seem like they could be the same thing, or they... I certainly would like them to be the same thing. why would I, make a distinction between them?

I don't know.

That's a tough one.

Niels Leadholm: yeah, if we maybe... for the time being, embrace the idea then that, okay, Layer 4 and Layer 3 or 5, wherever the state is represented. And maybe any layer can just learn Sequences, if things... if we have repeated... repeated transitions. Then they will learn that through associative connections, just...

JHawkins: It's natural, they have to... nothing has to be done.

Niels Leadholm: And then it can also be time-conditioned.

Then.

JHawkins: Where did we end up on the idea that the low... I'm sorry, my memory is really bad. We were talking about Layer 6A as potentially being 4-dimensional space-time, or just three-dimensional space? Did we come to a...

Viviane Clay: thing... I would argue for it just being three-dimensional space, because that's what we need to path integrate through, and then having separate representation for a sequence, because there we don't need to be able to move through it arbitrarily.

JHawkins: I... there's... the last sentence didn't make sense to me, but...

Viviane Clay: in the sequence, we only move from the current element to the next element, to the next, In 3D space, we have to be able to move through it arbitrarily.

JHawkins: we do that with the movement vector, right? Yeah. Grid cells, but why couldn't... in the grid cell literature. There's a lot of evidence that grid cells represent episodic time. Sequences of time, there's... there's people... your episodic memories are sequences of things. And my first reaction would say, can I get grid cells to represent space and time? And why is that hard? Yes, I path integrate... I need to path integrate through space-time, I can path integrate using movement connectors. That's the hard part, but people... we know it works, Now what I have to do is add another, quote, movement vector, which is just time, and I can't... and that's... that's something I can't control, it just moves ahead on its own. But it's another type of movement, so I can have... is it pos... is it possible... it's not clear to me that grid cells couldn't path integrate through spacetime. That they.

Viviane Clay: Yeah, I guess you could represent it as a fourth dimension, but movement along the fourth dimension would be very different from movement.

JHawkins: It would be.

Viviane Clay: for you.

JHawkins: It would be.

it's just, the thing that drives it would be different, right? But I don't... I don't know. Again, I'll go back to the literature on grid cells suggest that they represent time sequences as well as other things. They're just to be very, flexible about what they represent in our episodic memories. episodic memories are oftentimes sequences of things, right? Almost always there.

I would think the evidence suggests that at least good cells in the entorhinal cortex or space-time.

And then, at least then, for me, from my perspective, it warrants thinking about it a little bit, oh, how would I... what would the mechanism be if I had to add time as a... another path integrating factor?

Niels Leadholm: Yeah, maybe... Just to carry this discussion forward, nothing concrete, but, we often talk about, examples of things where it is easy to path integrate through a sequence or kind of a behavior, and instances where it's not. things like songs or rhymes or whatever, the alphabet, A, B, C, D, it's very easy to go in one direction, but it's very hard to go in the other direction.

JHawkins: It's... that would be, like, the time movement is only in one direction, where... where physical movement of the sensor can be backwards and forwards. And,

Viviane Clay: Yeah. Yeah, we could. presented, we could have it learned in the.

JHawkins: Just think of time as a...

Niels Leadholm: But then... sorry, but so if time... if we can't path integrate or go back through time in the representation, what is the advantage of having that as part of the... The sort of path.

JHawkins: we need... we need to... we need a way of rep... we need... if I'm trying to get to Layer 6A representing Space-time. That would be the simplest solution for me, because then, when it projects to Layer 4, it says, here's what I predict at this point in spacetime, as opposed to this point in space, and then layer 4 has to integrate time from someplace else. very simply would say, if Layer 6 can incorporate spacetime, four dimensions, then layer 4 works just as it does today. I don't have to do anything differently.

Niels Leadholm: I can't.

JHawkins: It just feels

Niels Leadholm: we're already struggling to fit grid cell... or, a path-integrating cell into a column, and now it also has to represent time, whereas it feels like we have a straightforward neurological... basis for the... if state is just conditioning apical dendrites.

another input.

JHawkins: After our last research meeting. I tried to come up with mechanism... I spent a day and a half on this. Trying to come up with the actual neural mechanisms that would allow neurons to make different predictions under different times and different contexts. And I couldn't... I couldn't solve the problem. it was... just got really... I just couldn't come up with an answer. It doesn't mean there isn't an answer, I just... I couldn't come up with it at that time.

Niels Leadholm: It's was this, like...

JHawkins: Apical conditioning? It does sound... I know, it sounds simple, and then I tried to work out the details, and I couldn't get it to work. So maybe I just was having a bad day, or,

Niels Leadholm: What was the issue that came up?

JHawkins: I was afraid you were going to ask me that. I don't remember. It was... I was deep into it, right? I'm taking notes, I'm just thinking and scribbling notes and stuff. I don't know, I have to go back and look at it again, but it just didn't pop out at me. It was like, I don't know, it sounds like it ought to be simple, but... I was trying to figure out, oh, I have these... I have these calcium spikes, and I have these back calcium spikes, and I have these, these... what could the neuron... how could the neuron behave differently under these different conditions? And, how could it make, how could this... the apical input enable one set of basal dendrite... behaviors, and another April, I enable a different one. I just couldn't figure out how to make that work.

Ramy Mounir: Anyway...

JHawkins: I...

Viviane Clay: Yeah, I feel like it's def- it should definitely be possible to... do it with, 4D grid cells, but it seems a bit... too much for such a simple thing. we could just have, unique representations for each state, and then use the sequence memory algorithm to associate the current state with the next state.

JHawkins: I got it. But... but I'm going to go back to the idea that grid cell... grid cells in the enterhinal project seem to do this. They... we could go back and look at the literature about them in coding sequences and so on. They just seem to be extremely flexible. They can... they can represent one-dimensional space, they can represent two-dimensional space. We don't know if they can represent three-dimensional space, the verdict is still out on that one. But we do know they represent, time, and, or sequences, and episodic memories, which are sequences of events. So I would say... and by the way. It might be possible, it might turn out, that when you try to understand how grid cells solve space-time, not just space. It may make them simpler. That this... the addition of this additional requirement may suggest an answer, which is oh, it's much simpler than we thought. it's always a possibility that happens sometimes. Gotcha.

Viviane Clay: Yeah, I guess one thing that could be nice about it is that we might have sequences for behaviors that aren't just transitioned by time, but are... instead, they're action conditions. opening the stapler is usually not something that just happens on its own, but you apply an action and that makes it change states. if we had a general grid cell mechanism, it could learn how actions transition you through state space.

JHawkins: Yeah.

Ramy Mounir: So that means Layer 3 is now going to be pulling... the most general thing would be a behavior ID, but if... if it's not really moving, then it would be just an object ID. So basically, with 4D grid cells, Layer 3 has the ability to pull, and just represent the API. if I have 4D grid cells, it's possible that Layer 4 and Layer 3 have no idea about this.

JHawkins: They don't know, they don't know, they're just associated with some SDR coming from Layer 6. And they have no idea what that means. So anything beyond that is going to work the same. So the pooling will be the same. It'll be pulling over things that are occurring in one dimension, two dimensions, four dimensions. They don't know.

Ramy Mounir: So it could be a behavior ID or an object ID, if it'.

JHawkins: It would be an ID representing all these patterns, whether they're three-dimensional space or... Very simple. or space-time.

Niels Leadholm: Maybe one... Oh, yeah.

Just one thought, tying back to the conversation we were having the other day about... Sort of a generic reference frame versus...

JHawkins: Or,

Niels Leadholm: A reusable reference frame versus, specific predictions. Because it...

JHawkins: I'm sorry, I don't remember that. What's the reusable.

Niels Leadholm: That was the conversation about, mugs, how you learn, many different mugs, and then you might get a...

JHawkins: that, so the idea that, classes of objects could be All have the same reference frame.

Niels Leadholm: Yeah. But then... but then you'd have, the state would... Help you predict deviations from that. And it... that feels relevant to this discussion, because it's the same thing Where it feels like... if, at least a naive implementation of a 40 grid cell is literally adding an additional dimension, and so you have this explosion in how much you need to represent and learn, whereas with kind of something more like what we discussed then. over time, in different states, a lot of the information you've learned about an object is redundant. You can reuse that. And the state conditioning just needs to tell you what is different. And that's clear, at least in my head, how that would work with the state being separate, rather than adding this fourth dimension to,

JHawkins: Yeah, it's funny, I almost jumped to the opposite conclusion.

Niels Leadholm: Okay.

JHawkins: My... my first thinking is, oh, how would I add that to Layer 6? What... how do I think about Layer 6 In a way that... in a way that encompasses the, the different classes of cups. you might be right, Neil, I don't know, but there's... I was like, oh, what is, what, what is it... what does it mean to have, this, path integrable space thing. I don't know, I just felt oh, time is one other dimension we could add, and... and, I don't know. I see your point. I see your point.

Viviane Clay: Yeah, it's a good point, I'd forgotten about that. I think that is a pretty strong argument for it being separate, so we don't have to relearn all the things that stay constant.

JHawkins: why do... why would it force us to relearn something? If I didn't do that.

Viviane Clay: Yeah, not in the behavior models, because there we're only storing changes, but I guess if we... Have a morphology model that has different states. But those different states might only differ in, small variations. Like the opened and the closed stapler, if you have a morphology model of both. We don't really need to relearn how the bottom of the stapler looks like.

JHawkins: I know that, right?

Niels Leadholm: partly it depends on how the, the spacetime grid cells are...

JHawkins: Yeah.

Niels Leadholm: work. But I guess if we at least agree that in Monty, with, 3D Cartesian space, and then we add the temporal dimension. And so we're gonna replicate We have to replicate all the information that's learned at every new, slice of time. Then we have a significant increase in the amount of Kind of information we're trying to represent and.

JHawkins: 16.

Niels Leadholm: learn.

JHawkins: We know we haven't...

Niels Leadholm: maybe there's some way that... I don't know, maybe it's only the grid cells that change over time that differ, or... yeah.

JHawkins: it's possible that you could still have 40... grid cells and... and, then a separate sort of class representation, I don't know. We still haven't resolved how to Learn these behavioral models. Because a single column doesn't seem to be able to do that, and it's just not enough time to learn everything. it could represent it, but it can't... that was another issue we haven't resolved, is like, it clearly takes, a bunch of columns working together. To learn this behavioral sequences. And, we don't know how that's happening either. And that's a hole in the theory, no. implementation. That may not be a problem for Monty. But Monty couldn't do anything.

Viviane Clay: Yeah, it's.

Ramy Mounir: Something that, complements what Niels was saying.

Viviane Clay: Good.

Ramy Mounir: back then when we talked about this, I also suggested, so what we're seeing here is, basically the location being, a generic location, and then for the different instances of this object, we have, a specific SDR for these locations. But what I also suggested was, that... This location, that basically represents time the same way, where the generic location is not changing with, this location on that object, but the specific would be... as it moves, it's just changing which cells in the mini column. So it's representing time in that way. And then, if we assume.

JHawkins: I'm not feeling that's representing time. Wouldn't the pose of... wouldn't the pose representation or the orientation representation be changing as well as when that lid goes up?

so wouldn't the... wouldn't the square rotate with the... With.

Ramy Mounir: That would be in the features. I'm talking about the location in 6A.

and I'm just saying that maybe... I know that we... you said, Jeff, that Layer 6A doesn't have these, connections that, associations that may represent transitions, but I'm saying.

JHawkins: I said only that.

Ramy Mounir: get tough.

JHawkins: The Thompson paper didn't suggest them, so that doesn't mean anything. It's just one little data point, so I'm not... I'm not hanging my head around.

Ramy Mounir: But I'm suggesting that maybe if they exist, then we could learn also connections between states the same way.

JHawkins: I think as a high-level comment, I would agree with that. I'm not sure how that relates to the picture of the stapler here, but...

Ramy Mounir: It's just that, if we were going to say, this is basically what Niels was also suggesting, that if we're going to represent 4D grid cells. We could just represent the time axis as a distortion in the location, or, a variability from some generic location. Did I get what you were saying?

JHawkins: Yeah, that.

Niels Leadholm: Yeah, I think maybe one thing I would... I just need to understand better is because I think in my head, that was, like, the bottom of the stapler Would be the same.

And then you would need to represent the parts that were changing But I think when you were describing it, you mentioned that the... The location on that tip of the stapler that's moving, that location is constant.

I'm not saying that's wrong, but I just feel like that's different, and so I just wanna...

JHawkins: I thought we broke off the top of the stapler, and we're using the existing model for the stapler, just... Restricting at the top, in which case there's nothing... it's just the same object, it's just rotating.

Ramy Mounir: this doesn't assume hierarchy, We're assuming it's just... it's one object, so this is why I also had this, the balloon thing for the distortion. Basically, the... because we can't break this into multiple objects. But yet still, these locations are... we can represent how they change or distort over time.

Niels Leadholm: And maybe it helps also just to... Try and use the language of, a behavioral model, so it's Actually, all we're representing here is flow... local flow patterns. There's no concept of a stapler head. At least not in the. hinged behavior model.

JHawkins: This, to me, would be like, an object like this balloon, in some sense, you could argue that it's not changing shape at all. It's just, you're looking at it different scales. And you're... and you're seeing a flow between those scales. I... I can see a balloon that's close to me and further away, and then further away, and of course, it changes in its size, just like this image does. But I don't view them as... I assign the exact same reference frame to them. I just scale it. I just am able to scale the reference frame.

Viviane Clay: But there are other distorting behaviors.

JHawkins: I know. I know that. But I'm saying this particular one. Sometimes when we use examples that that have a simpler explanation, it's not a good idea. It's better to focus on the ones that are more complex, because.

Niels Leadholm: all...

Hojae Lee: all roads, all discussions lead back to the can of worms. I was just gonna say that. So we talk about worms?

JHawkins: But yeah, of course...

Viviane Clay: Briefly... oh, god.

JHawkins: Can of worms is equivalent to a crumpled t-shirt in the behavioral model and the morphology model.

Sorry.

Viviane Clay: I was just gonna say, on the, 4D, Topic... I think, at least in my mind, in terms of Monty.

no matter how it would be done in the brain, whether the fourth dimension is part of the grid cell representation, or if it's a separate sequence representation. I think we'll end up with, conceptually, that the models become four-dimensional, so we have three-dimensional locations in space, and then an orthogonal, totally orthogonal, fourth dimension of states or time, in which we learn these things. And Yeah, basically, in how I was describing it in Monty would be that We have the usual models, but then... all of these, XYZ locations for each feature are conditioned on the state, so it can be different states, depending on the state or the... element in the sequence, you expect different features at locations, or different changes at locations. And then similarly for inference, when we try to infer Where we are. Those hypotheses would extend in the fourth dimension, so we would have to infer, which state we're in, and what location. In that... Stay, on the object in that state. Yeah, just to highlight that, I think, conceptually, it's definitely valid to think of it as 4D space. And then the question would be, how would it be implemented in the brain?

JHawkins: I'll go back to something that I think Neil said earlier, something like that, but somebody did. The Layer 6 is highly interconnected with Layer 5. And if layer 5 is representing, we think about it as movement, but movement is also behavioral states as well, right?

In some sense. I'm just saying. I'm going back to trying to get layer 6A to represent 40, locations, and so you'd have the locally there, you'd have the... you'd have a... your signal there at Layer 5 would be the thing that might help Layer 6 represent 40 space. I'm just throwing that out.

I think it's worth trying to make... to me, it's worth trying to get layer 6 to represent 4D space. And that could be separate from... states of objects, classes of objects.

a tall cup doesn't become a squat cup. A short cup. That's not a behavior. That's just two different flavors of an object. That could be separate from Things that change and morph into each other.

Niels Leadholm: Yeah, or.

JHawkins: Did we make any progress today?

Niels Leadholm: Yeah, maybe I misunderstood what you said just now, but, it feels like... If we did... to be devil's advocate for, spacetime grid cells. Then, yeah, a class would just be the grid cells that are active for, the reference frame that's active for the different objects that it could morph into and things like that, because... They would share the same space-time space.

JHawkins: Yeah, I... A devil's advocate, you're advocating for it, or you're not advocating?

Niels Leadholm: Yeah, I'm advocating for... because it begins, early.

Viviane Clay: That's the advocate.

Niels Leadholm: That's right. That's... okay, fine.

JHawkins: You can...

Niels Leadholm: heavenly advocate, for, space-time grid cells. There's that. And then I guess the other thing is just, For behaviors that aren't just sequences, for things like the joystick, or, just the complexity of objects that can go through Time... or go through their behavioral sequences in different directions. Go through behavioral space in different directions.

JHawkins: But they're not sequences. That's a... that's a very different thing. That's, to my mind. That is not a behavior. That is... that is just... This thing can move in the world. And if...

Niels Leadholm: But somehow we can understand and we can model it.

JHawkins: that's right, but, that... to me, that's... that could be equivalent, oh, I can pick up my cup and move it to a different part of the table. I can, I can... there's no... it's not... it's not an inherent property of the cup, and if I have a joystick and I can move the top in different directions, yeah, it's limited, but it's... it's certainly... it doesn't seem like there's some set behavior for it. It's I have a limited area in places I can put my cup on the table, I have a limited area so I can move this joystick top.

Niels Leadholm: isn't a stapler just, a one-dimensional version of that? a more limited one?

JHawkins: because... no, but... the fact that it is limited is the difference. The fact that there's.

Viviane Clay: Thank you.

JHawkins: influence. The fact that it is a high-order sequence. is the difference. It can be learned as a sequence, whereas the the movement of the joystick, or the movement of the cup on the table is not... it can't be learned as a sequence. You can form a sequence from, joystick movement if you just do left, left, that's... You could... you could learn it that way. You might learn while doing a... playing a game that you do a certain patterns really quickly. I used to have it when I was a kid.

Viviane Clay: Not good.

JHawkins: As a kid, I had this... Plexiglass Cube, which is a three-dimensional maze. And you could see into it. And it was divided into maybe, 10 by 10, so it'd be, like, a thousand cubes inside, and it had all these... and basically it was a passage. You'd put a ball on one end, and you'd have to try to rotate this cube in all these different dimensions to get the ball out the other end. It was very hard to do, because you couldn't really see the passages too well, right? Because it was all plexiglass. And so I learned how to do this. blindly. It was like, I just learned how to rotate this thing over and over again. It always comes out, and then the fun of it was over. So why do I bring that up? I don't know. It's something which started out as a very complex, hard-to-solve behavioral problem, where it became a high-order sequence. I guess that's why I'm bringing it up. It became a complex, high-order sequence. It took, 30 seconds to do.

Niels Leadholm: That's interesting, I think that ties in nicely with some things we've talked about before, where... Even things like a t-shirt. You... the first time you're folding a t-shirt, you Firstly, you need to learn, a temporary reference frame or representation of the t-shirt in its current state. And then it's a very kind of deliberate, model-based thinking of, okay, how does this thing need to move? to a particular location, but then over time, you develop a pattern for, oh, folding a t-shirt, I grab it by these points, and then I follow this sequence. And I guess what I'm just trying to rephrase is, Maybe there was never really a learned model for the former. you leveraged simpler models through a slow process to be able to behave intelligently, but it's... But it's very slow, and there's not, a behavioral sequence or something that you can just...

JHawkins: I would agree with that. In the beginning, you just...

Niels Leadholm: And and so maybe the joystick is the same, it takes a while, and I guess eventually you just learn, a mapping between, oh, okay, if I push up on the joystick, my character moves forward, or...

JHawkins: that's a step, that way... I don't like the joystick for these reasons. It doesn't, to me, represent a behavioral sequence, and yet it is a way of using behaviors to affect changes in the world elsewhere. sort of goal-oriented behavior, which I think is a good example for that, but we haven't really tackled that one yet. I'd rather put the joystick into that category. It's a going behavior, like, how do we learn how the joystick controls the world? To achieve gold. Which is not the behavior of the joystick, it's... it's how the joysticks interact with other objects.

Hojae Lee: just one more... sorry, I think we're adding one more, one more thing. But, one more that would simplify, Jeff, the 4D grid cells. model even further is if we represent layer 6A with XYZT, and also layer 4 with all the features, and also the changes, because, for example, one change that we're trying to extract from sensor modules is, optical flow. instead of going... instead of the changes going into Layer 3, they're also going... changes is just a type... another type of feature, considering. then we could just keep the existing layer 4, 6, and whatever model comes out is, a morpho-behavioral model. it's not a separate morphology or a separate behavior model, but, it just... has all of it. And I think that would be, like, an even further... I'm not saying that's right, or, we should do it that way, but I'm just saying,

JHawkins: so I think.

Hojae Lee: simplifying.

JHawkins: OJ, I want to make sure... I think you were... I think you were arguing for the, the nicety of using 4D grid cells, because everything else flows out Whatever happens, and it works.

Hojae Lee: Yeah, I'm

JHawkins: Is that not what you were saying?

Hojae Lee: That's what I want to say. I guess I went back and forth on whether, separating out space and time, at first, I liked that idea because it seemed like it would give us more... flexibility in terms of using, one independently from the other, while, but now, it now switched back, but maybe, representing as XYZT, I guess I'm moving more towards this, that, that idea after this meeting, but, And... Thank you.

JHawkins: You're putting a... you're putting a warmest vote on four-dimensional grid cell. Is that what you're doing?

Hojae Lee: Yeah, I'm just saying that if we have four dimensions, and also include, change as a feature, then, I'm just, taking that even further in terms of, okay, let's group, everything together, which might not be the right idea. No.

Viviane Clay: Yeah, in my mind.

Hojae Lee: I like it, I like that.

Viviane Clay: It's really whether we have XYZT, I feel like that's... we need that, but the... whether T is represented in the grid cells or somewhere else...

Hojae Lee: Yeah, the tuple XYZ all together, in the crystals.

Viviane Clay: Yeah.

Hojae Lee: Yeah. anyways,

Niels Leadholm: And then, just to clear, what was your reason for preferring that, or...

Hojae Lee: so let's say that if we are representing grid cells, if the grid cells represents, say, space-time. And I've taken that as a... given, and also in Layer 4, we're representing features and changes, then... with the existing mechanism that we have with, the green arrows in layer 4, and the vertical arrows between layer 4 and 6, before, with just features and just space, we were creating a morphological model, but with a If layer 4 can receive features and changes, and Layer 6 can receive space and time. Then those two combinations can create a sort of a combined morpho behavior model.

JHawkins: Okay, so you're going back to the idea that we don't have a separate behavioral column. We have behaviors and.

Hojae Lee: Yeah, maybe, yeah, maybe we don't have a separate morphology and behavioral model, maybe it's just one thing.

Viviane Clay: But then we can't apply different behaviors to different objects if we... if they're just in the same model.

Hojae Lee: I haven't thought about that, obviously.

JHawkins: it'll...

Hojae Lee: I was just... I know that somebody was gonna...

JHawkins: Going back to the figure that Rami showed earlier, at one point, we decided the only way to... when we were still working on the idea of a unified morphology and behavioral models, we came... we reached the conclusion that Behaviors can be applied to other models, but it requires hierarchy.

Hojae Lee: You can't do it within the column.

JHawkins: So the behavioral model in one column could direct Morphology's in another column. Now, I'm not saying that's right, but that's... that's... that's how we got to that picture that Rami showed.

Hojae Lee: Yeah.

Niels Leadholm: Yeah, for what's worth it, it doesn't feel to me like we need time in the grid cells to have behavior morphological models. that's... that's not the blocker. Because I guess it's the blocker if we want to fit it into L4 and L6. I think to paraphrase what you're saying, I feel like... An argument for having... time grid cells is that it prevents having to have time represented in another layer. If that's a disadvantage.

JHawkins: you're throwing time... you're throwing time into the L6 bucket, and in theory, no one else should know about it. But this is... this is time... this is... This is actually... it's not time as in the matrix cells, let's be really clear about that. This is...

Hojae Lee: Yeah.

JHawkins: Sequences. This is time sequences. That's what it represents. It's not timing.

Viviane Clay: Yeah, it's more like state space, maybe.

Hojae Lee: Yeah.

Viviane Clay: discrete states in a sequence.

JHawkins: it's a sequence of states that are learned in one dimension.

that... that would be a lovely outcome if somehow, if we added time... if we added... if we made layer 6 cells, we figure out how they represent space and time. And then... Everybody else doesn't have to change, and somehow we can get the whole... single learning module learns both behavioral models and morphology models. I would love that, but, I'm not... I'm not dissing on Vivin's idea.

Niels Leadholm: I still feel like that's a separate question, whether... A single column can learn both behavior and...

JHawkins: But if it came out...

Niels Leadholm: even if we represent time in layer 5, or whatever, layer 3, We can still learn both in one column. Or, in principle, we can learn both in one column. The issue was...

JHawkins: If the problems that... there were problems that made us think about moving to two separate.

Niels Leadholm: I agree, but that's the thing, I feel like those are separate from having somewhere to represent time.

JHawkins: Yeah, maybe I'm going to stop using the word time. It's really representing I don't know, I want to distinguish it between matrix cells, which represent, timing.

Viviane Clay: I've been calling it state, because I feel like that applies more also to morphology models, that you can have different morphological states, and different behavioral states.

JHawkins: But in this case, it's states that flow through time, and the time... the states that change through time.

Viviane Clay: Yeah, you traverse state space in one direction.

Niels Leadholm: Yeah, but it may also be action condition, as we sometimes talk about, it might not be necessarily time that pushes you through it.

Viviane Clay: No, it's an order sequence.

JHawkins: Yeah.

it's funny, because you... thinking very theoretically, you might say, oh, objects have different states. there were even different classes. And if they transition in a uniform way, we would learn it as a sequence. it's... there's some sort of potential hope for unifying Classes and behaviors, classes are the steps along a behavioral sequence, but if you don't have a behavior... you could still... basically, you can have different... you can have different morphology of an object representing different classes, and if they do transition through time, then you'd learn it. And if they don't, then you wouldn't learn it.

It's an interesting idea.

I'm gonna have to write notes after this meeting. There's too many things we talked about.