Okay, everybody see this? All right, hi, we're team. Everything is awesome, Rami and myself.

there we go. Explain the team's origin please. so in the beginning, we got some Legos. we bought a bunch of stuff, started assembling it into a platform, and then this is the platform actually running. So this is our, robot assembly, so that's one of our motors and our part of our motor system. And then we were calibrating it very scientifically of seeing how fast the thing can go unfor you. So we calibrated it well. then we're also, so the other part of the robot is a sensorimotor module. This is the elevator that goes up and down. And in the background there, you can see the, this is the sensorimotor with the camera and the depth with the RGB camera on it and the depth camera on it. and that's what the robot looks like. Combined.

We did a bunch of debugging. This is an example of, debugging the depth sensorimotor. there's interest, like you can see different depth resolution versus RGB resolution. So every dark point is an RGB point, and the depth sensors were like more pixelated, worse. Some of the lessons learned, early on we were just trying to remind each other not to eat the data set. This is since some of it was edible and it was in the kitchen, so we just finally separated out of the rest of the food that we planned on eating. the other, another lesson learned was it was really great, I think everybody was mentioning this, building something with Monty end to end, so that we had to write and understand every single part of the Monty system. Like I know Ramy and I are way more familiar with how the whole system works now. of course, IANS we, I understood, TER for a little bit. so that was great. And I think I still remember them. There's a resource, that really helped, with the intuition. some of the Monty framework thing, things, there's some ideas for action refactor that I wanna do after we're done. noticing too many resets happening before an episode. if you're running a simulation, you never notice it, but if you have a robot that's like, why is it doing the resets so many times before it starts? So that was a neat finding. Yeah, we ran into the same problem, spent way too much time on it, and then decided that we'll just always have to press the button twice before we start the experiment. We had to do the same, like we had to take two pictures that, because the first picture would always be black for whatever reason. Nice. I removed the extra reset, so that was my, because our, coding approach was to, we just forked the entire Monty repository and we just hacked whatever we needed to. So that, that was convenient. That was also a convenient approach to the hackathon for us. Yeah. picking coordinates that make the problem easy. As you saw, our robot has a central platform that rotates and a thing that goes up and down. So we're like, our units are the robot radius and we are on the unit circle, and that simplified a lot of things immediately. so that was super helpful. Next on debugging and visualizations. Yeah. I think to me, I feel like having a, very good, this is what I figured out early on, that if you have very good visualizations and a bugging that will go, a long way in, in something like this. so yeah, we started working on some, nice visualizations. You'll see that in the next, few slides, but, basically we are able to see the sensors positions live as it's basically making movements and basically also what like the current MLH is or like what Monty thinks is happening and what the rotations are and all of that stuff. we had some, trouble with, the coordinate systems. So we, we started using, so for the get state of the agent so that we know where the agent is. We're using a different coordinate system that wasn't Monty and we figured, it will still work fine. but then the, it does not plug well with the depth to 3D. So we ended up changing all of my, our computations for the, get state of the agent. And that's where, learning the co also that's where we started looking into cos too. the, and then we tried to do model scaling. basically we're using different, like different, a different scale, than what is in the dataset that was provided to us before. So that did not work very well, and I'm not really, sure why, but, so we, have, we added some code into the grid object model that will basically, you can just say, scale this model by this scale factor and we'll just scale the mesh and recalculate the KT three and everything. but that still did not help very well. what we ended up doing is that we basically trained our models, but then the models that we trained are, very sparse. they may not work very well, but the idea is that if we train a dense model, it'll work, better. but you'll see that we still have some performance, A reasonable performance. and early on we, like in the first day, we had some, a lot of networking issues. And, it basically, the reason is they have multiple sources of, like they have multiple routers, and if you have the different clients are connected to different routers, they'll not be able to talk to each other. So what we ended up doing is that we needed a router, but then, we could not make that router connect to the internet except through a hacky solution where we plug it into a wifi extender and that wifi extender gives the internet to the router, and then we can connect to the router.

but the, the cool thing about what we built is that we could actually like decouple the simulator from, Monty itself. So we, what we tried is, running two different mons on the same hardware. So basically, Tristan, and I, we were running experiments at the same time and we were fighting for the hardware, which was very cool to see. Basically like the, sensors moving, like I, I give it a command to move somewhere and then Tristan just moves it somewhere else, and then we start arguing about, and need the resources and, all of that. yeah, and a lot more stuff that we, the death camera was like giving us a lot of trouble. yeah, too much troubleshooting and debugging that went there. And yeah, like I said, debugging setup goes a long way. yeah. Okay, we did some, so with troubles, with changing the scale of the existing dataset, we ended up training on the dataset ourselves. So if you don't recognize it, but that is a cup. All right. And so we, this is training sped up 40 times. So on the left you see the visualization being built and on the right you see the robot, using a, training policy was essentially orbit around the object from the bottom up and just scan it. And so that was the, that's the idea behind that.

this is what, the final scan for the cup look like. So it does end up looking like a cup. It looks like a cup, but the handle is, yeah, I think we, we have some trouble with the depth information and maybe the focal point of the, like the, field of view and all of that stuff. So it, looked, it's a perfect circle and everything, but the, the handle is a little, detached from, the cup. and this is why this, oh, I see. Our evaluation works fine with the trained models. but then maybe this is why scaling the meshes didn't work very well because, they're different, during inference we can see that, so this is, a different kind of visualization where it's basically, I'm plotting the current MLH. in, in real time. it's basically if you can figure out the rotation, the rotation, this little.is a sensorimotor, basically, this is where thinks the sensorimotor is compared to where the mug is. and it was able to figure out that the cup was, right side up, but it could not figure out the rotation, like basically the rotation on the y axis or like where the handle is.

but it could separate it from all the other objects in the dataset. that's also good. and we were using, and essentially we use based policy, just random actions. we had to rewrite it a little bit to, you use our actions, but it was, but as you can see in the video, those actions are just purely random, the up, down and ro and rotating around the object. But the sensor's always guaranteed to be on the object in these cases. No, How do you move back? It is random. It's built in. Yeah. Yeah. So if you're off object, how do, how does it know that it's off object, I guess is my question? there's a, the, transform, right? So based on the depth, it knows that the depth see, yeah. So the depth is like too far, then you're not an object. Okay. So we go through the building, the semantic, labels based on the depth. Okay. I have more questions. Okay. We can get into, is this the time for live demo or do they come after all the presentations?

maybe we wait until after the presentations and we pick one object that we all demo on. Okay. Sounds good. that's it. Nice. Great job. Looks really cool. Yeah. So can I ask those same two questions with regards to Yeah, could you ask them again? Yeah, I gotta bring 'em up. Okay. how did your project, demonstrate Monty's capabilities and, how does, what, how do you see Monty, what the application you're doing, for the future? Yeah, demonstrating Monty's capabilities, I think this would count as, the, probably, I don't know, I'm not sure, but this is probably the first system that has, actual movements in the world, basically instead of, like circadian over an image. So it is, not like we're taking an image and then we're moving a patch over the image. it's actually the patch itself is moving through the world, which is in my opinion, is very different from, what we had before. so it's demonstrating a different side of Monty's capabilities with an actual, motor basically. Is that, that this thing works. As a sensorimotor. Okay. Yeah. Can I throw in there, so just one point about the patch, like in the previous presentations you saw like a full image of every, being taken. Literally, Monty only sees those dots. there's nothing else, there's no image of the cup, there's nothing. Monty learns just those dots on the left. And based on that, it learns to recognize objects. There's no full picture given to Monty at any point. Go ahead. Yeah.

and then in terms of, future, capabilities, I think that, so this is a first step, in my opinion. There's, just like with the drone team as well, there would be, opportunities there to explore boating, which I think is very important. it's easy. Now, we're building this with Lego, so it's easy. Like we could, add a different sensorimotor and add that through a different angle, and then we can just move it, we can move it independently or we can move them together as, part of one agent, so they can move, like two sensorimotor patches on a finger together. Or they can move as like different hands and then they like look at different parts of the, of the mug. And so basically it's very, extendable, the way that we set it up. so yeah, if that answers your question, I'm not sure. Yeah, I think it's this, it's a great point. It's the very first time that Monty is working with a motor in the real world. It's a, huge thing. And then I guess just for us, it'll be a great setup to have to test Monty in the real world. Right now, we basically, everything that's a bit more complex, anything that involves movement, we have to test in simulation. So now we have a test bed in the real world, and we could give it any kind of object. We don't have to create like a simulated object data set or anything. We can pick something from the kitchen and put it on the platform. See what Mon Monty does. Yeah. Yeah. it, yeah, it also scans very nicely. it, it scans very fast, but then, the, it's still sparse, compared to what, I would expect. Like I looked at the measures that, you gave us, and they're very nice, but like very dense. So this is sparse, but the fact that it still works even with those sparse models is, mind blowing to me. So does it work, at any speed? Does, if the, if you slow down the motor or speed it up, does it still have the same results? slowing it down is always easier. speeding it up. We are, we're basically, we're running at a, at a high speed actually. And, but we're limited by how, like how far, how, how much time it takes to transfer the image from like wirelessly, from the sensors to the computer. And process them. I, this is actually the bottleneck with one of the challenges was I was sending a bigger, so that to be able to stitch the, depth camera with the RGBI was sending both like bigger images from the Raspberry Pi, from the, from Monty, from the, robot to the computer. and then stitching those. But then, we found out that if we just send the patches and do the competitions for which patch we want, which is basically sends smaller images across the network, it goes, much faster. so Monty doesn't have a problem. Actually, Monty is much faster than, the bottleneck of sending, okay. Images. So yeah, it works fine with high speeds, Short answer. And yet to Monty it doesn't matter how fast you move or in what pattern you move over the object, it's not like you need to have seen that pattern before during training. You can generalize to any speed of movement as long as what Rami says, the transfers fast enough. Yeah. And you can increase.

You're muted. Muted. And we can have, and you can speed up the inference, by having Monty have multiple sensors. So I have thousand sensors looking at something. You can recognize it instantaneously rather than having to move or you can move it once. so yeah, you can, once we put this in hardware, this could go very, fast. And another thing about the future demonstrating future capabilities is we have zero precise movement. So when the elevator goes up, we ask it to go a certain distance, but it goes whatever distance it manages. there's minimal distance when it rotates, we're like, rotate by seven degrees, it's gonna do whatever it wants. but we, but as a result, we, but we are able to, work on proprioception and make sure that Monty was still processing where the thing actually was. So there's no, so what we also demonstrating is you don't need any sort of precise movement as long as you figure out where you end up. And the policies still seem to work Okay. And still infer and train objects, even with imprecise motor control, which is a new demonstration outside of simulation. Yeah. Actually there were some times where the, the ro the, rotation robot, that one that rotates the platform, it was not even moving. Like it, it tries to move, but it looks like it doesn't have enough power or we're just doing small steps. but the, basically because we're using pro price entry, set, we're getting the, position of the agent, it doesn't matter. So basically it would just add. Points at that same location or depending on how the, object is built doesn't need to add these points. So the lesson learned is that, just being able to get an accurate, position of the agent is more important than moving the agents, moving the, agent somewhere or, or trying to move it to somewhere accurately. It doesn't matter as long you're, as you're able to get the position accurately.

that was helpful.

Yeah. Awesome. Looks great.

That's how they got their name. Yeah, everything's awesome. That was Tristan's idea.

what you're observing now is, it's end, it's resetting into that starting position.

Cool.

Oh, wow. Already hot sauce, most likely hypothesis things that might be Monty's heart. So I don't know. Oh, it can I, oh, it's neck and neck, yeah. And back, neck, and neck with what? I can't see the. No. Now hot Sauce is a clear lead, but there was Monty's heart as well there for a few steps. Monty's what? Oh, detective Hot Sauce. Oh wow. Good job. That's pretty awesome. Nice. Nice. Wow. let me share my entire screen actually. 'cause that will be, better.

so pick an, pick another one. Can you do the mug? Yeah, they already did the mug. Let's do something different. Okay. Spam. We can do something. Yeah, do the, you can do the spam. Ours are pretty quick. hold on. Let me just turn off the, I wanted to share because I wanted also e See, there we go. All right. What's on the, okay. Yeah. Yeah. It's still, all Here we go again. It's resetting.

oh. Terry, by the way, the spam can here is called potted meat. Can Oh, really? Full name in the dataset. Oh, okay. No product placement.

And by the way, you can see the models in the upper right corner of the visualization. that's, all that Monty knows. That's cool. interesting. That sparse.

Okay. it's between the two visual visualizer, it keeps saying global matching Step three three. Oh, we didn't debug those logs. Maybe it's, what's happening is the camera is above the object, so it wasn't getting any sensorimotor input, so it was just bypassing the learning module off object. Oh, nice. There you go. Whoa. Boom. Wow. What else you wanna identify? how about the Numenta brain? All right. This one is a little different setup. You have the platform. I don't think this one works very well. Do you have the platform?

Okay. Yeah. It's, it is the top platform, right? I don't remember on, yeah, just put it on top platform. Okay. So we, we have an extra platform for this one because our sensorimotor didn't go far enough to see it on the platform that we built.

and we also, presumably the platform is kind of part of the model. yeah. Although, we're not sure if this is the platform we use or if we use the different ones. So we'll find. Okay. Cool.

testing robustness. Yeah. So here you go. This subject is also very small, but the patch is sometimes just off the object a lot and that, some, way though it doesn't work for that reason. And hopefully it will not fling the brain off of the platform either. it's pretty light. We don't have many tolerances built in here. A flying brain. Yeah.

Oh, don't see Monty's brain in hypothesis space. Yeah. This one might not work. We'll see. Yeah. Right now the sensorimotor is above. So when you're seeing stuff like this, it's just not getting any data. 'cause the, again, our policy is just purely random movements.

Also, the time flight Sensorimotor works best with shiny objects and, Objects that are not like metallic or whatever, they don't really give good data.

Oh, yeah. Sorry. Visualization.

Yeah, I thinks it's a tuna soup. Can, this is why it's tuna soup. Can wait tomato soup or tuna can tomato soup. Sorry. Tomato soup probably. I guess that probably. Okay. you got an example of a failure mode That's not bad for three. Yeah's. Awesome. Do we get a second try on the path? Yeah. What if we get the brain? No, it's actually pretty good about the brain. We just put it on the bottom, right? Yeah. Just normally. Yeah. the heart. The heart, sorry. Yeah. So it's pretty good recognizing Monty's heart and you can see the image of that's its Model of the Heart. Yeah. Wow. That's pretty cool. Yeah. Do you wanna try the spam can again? Yeah. Should we try one more just to you, you change, hopefully have us, yeah. Okay. The checklist. Yeah. I would say the price for our highest, actually definitely it goes to you guys, but, wanna have, a little bit of success. Sounds good. Okay. I'll stop sharing as soon as this completes. So are you, like in monthly when you rotate, is it moving the agent around the circle? Yes. it's, we are faking it. Orbiting an object.

There we go. Monty's heart, Monty heart.