Viviane Clay: Alright, thanks. Yeah, welcome everyone. I'm Vivian Clay, Director of the Thousand Brains Project. And today, we hope to give you a high-level overview of the Thousand Brains Project and the new type of AI that we're building. And so we set up a series of 9 short talks. So everyone from our team will present a different aspect of the project. Not all talks have the same length, so don't worry if by the halfway mark we're not through half of the talks. Overall, it will take, about 2 hours, but, in case you have to drop off at any point, we're also recording this meeting. If you have any questions, feel free to post them throughout in the QA section, and then we will answer them in the end. And you can also upvote other people's questions if you find them interesting. So yeah, we hope that you'll walk away from today's talk series with interest and excitement about what we're doing. And there are many ways in which you can help us. You can contribute code, build applications, or tell others about the Thousand Brains Project. We think that, yeah, what we're creating is really exciting and important, and we hope you'll feel the same way at the end of this.

Just a brief intro of what the Thousand Brains Project is, in case you haven't heard much about us yet. The Thousand Brains Project is a non-profit company that was created at the beginning of this year. And we believe that the future of AI and robotics will be based on sensorimotor learning. And so we're building a sensorimotor AI platform. We call it Monty, so if you hear the term Monty throughout today's talks, that's what we call our AI platform. And this is a new type of machine intelligence based on the principles of the neocortex, as laid out in the Thousand Brains theory. It's not deep learning, it's not LLMs, it's really a different type of AI that's specifically designed for sensorimotor learning, super efficient, both in terms of energy use and data use. Can do continual learning and a bunch of other exciting stuff that our brain can do, and you'll hear a lot about that later. And we also want to make it accessible to anyone, so it's, everything is open source, we upload all our meetings on YouTube, associated patents are under a non-assert pledge, and we're trying to build a really vibrant community around this project, and really build this together with all of you, if you're interested.

And, yeah, a lot of talks later will also touch about this part of the project. But to start, let me tell you the story of this special project, and what we're doing, and how we got to where we are today. And to tell this story, I'll use a fun visual analogy. we often talk about how the brain uses reference frames to efficiently represent structured knowledge. And you can think of reference frames as coordinate systems, or maps, that represent how different things in the world are related to each other. And here I will use a map as my reference frame to lay out the structure of the project, and I hope it helps you follow along, or is at least an entertaining way to hear the story of the project. So without further ado, here's the legend of Monty.

On a foggy October morning, the crew at Numenta, which is where this project started, arrived at the shores of Monty Island. We didn't know much about this island, but the captain of the ship, Jeff Hawkins, had a clear vision. One, that we can reverse engineer the brain. And two, that machine intelligence, based on the brain's principles, will be the most important technology of the 21st century. And with that vision we started out to explore this island. We also had a really useful compass, which is the Thousand Brains Theory. And through almost 20 years of studying the brain, this theory was developed, and we used the principles as a guide. Scott will talk more about those principles later, but just at a very high level to give you an idea of what those are, one is sensorimotor learning, so everything is about learning through movement. Our eyes move about 3 times a second, our whole bodies move. There's really no intelligence on this planet that doesn't move around. Everything about the system is about learning through interacting with the world. Second, having a general repeatable processing unit, modeled after cortical columns in the neocortex. Third, local information and learning. So each of these general processing units only receives very local information about a small patch on the retina or a small patch of skin, and there's no big picture anywhere. And also, learning isn't the kind of global process, but very local. And fourth, reference frames, so what I mentioned earlier, a way to represent how knowledge relates to each other.

And these principles have served us really so far as we were exploring this island, and we've been using it as a good compass on any kind of decision point throughout. Based on those principles, we build a basic framework for a new AI system, and we call it Monty, after Vernon Mountcastle, neuroscientist that first proposed the idea of the general repeatable processing unit in the brain.

Next, we set out to build the components of Monty. We set up a first settlement to construct learning modules. And we constructed them, after what's known about cortical columns and their architecture and connectivity. Another settlement specialized in the construction of sensor modules. Sensor modules are basically the interface between Monty and the world. They take in raw sensory data from any kind of modality and translate it into a message, into a language that learning modules can understand.

And we also had to define the connectivity between learning modules and sensor modules. And so we defined a set of traffic rules for messages sent between sensor modules and learning modules within the Monty framework, and we call that the Cortical messaging protocol. We iterated on the learning module design. Several times. Until an individual learning module could achieve robust object and pose recognition. So we could learn an object taking one path along the object, and then do inference on the object in a totally new orientation, taking a never-before-taken path on the object, under noisy conditions, which is pretty cool. We call this the Evidence Learning Module and still use it today.

Over time, we improved sensor module capabilities. And built a new settlement specialized in the design of intelligent policies. thus far, Monty moved largely random, but by now, we have several more sophisticated model-free and model-based policies that Monty can use to efficiently explore the world using either the sensory input or its learned models and hypotheses. A new monastery in the forest started investigating scaling the system to more learning modules. They implemented and tested the first versions of voting between learning modules to reach rapid consensus during inference.

Settlement with craftsmen specializing on optimizing the implementation, led to significant speedups of the system.

Another scouting party ventured off to explore the possibility of stacking learning modules on top of each other to learn more complex compositional models. Through hierarchy.

We also ventured out to the lake and tested the innovations so far in the real world. This was a week-long hackathon, we termed the project Monty Meets World, and it was really exciting to see Monty work outside of the simulator for the first time. We could put an object in front of an iPad camera and recognize it, and that felt really special at the time.

After a lot of the initial infrastructure was put down, we had convinced ourselves that this actually works. But we have to return to the mainland to help on some larger projects there and find more resources to return with. And after about a year passed, in mid-2024, a small team returned back to Monty's Island, and the Thousand Brains project was officially announced. With funding from the Gates Foundation, as well as the Korean Electronics Technology Institute.

We grew the team from 2 full-time employees to 8 full-time employees. All of you will hear from all of them today. And additionally, we decided to broadcast the existence of this wonderful island. And all the work we're doing here. And so for that, we set up a small harbor village in the south of the island, where ships from the mainland could easily talk in the hopes to build a welcoming community around the project.

Next to the small community, we set up a lighthouse to send out messages to the boat nearby. And this way, we could publish all of our past year's advances, and interested people could stop by in the Harbor Village to learn more. We took our private framework code, cleaned it up, and published it under the extremely permissive MIT license. And also put all related patents under a non-assert pledge, so allowing anyone to build on this new approach, commercial or otherwise. And with the announcement of the project, first boats started to arrive on the shores of the southern community. In December 2024, so about this time last year, we hosted our first community event around the lighthouse, and today we hope to continue this tradition.

We also published a white paper describing the goals of the project and the principles of the implementation and the algorithm.

Additionally, we put together a range of other resources for people who newly arrives on Monty's Island, a lot of documentation. YouTube channel, where we published almost 150 videos by now, as well as a discourse channel, where people can exchange ideas and ask questions on our forum, and our team will, answer you guys.

On January 1st, the Thousand Brains Project officially started operating as an independent nonprofit, separate from Numenta. And, we have a 501c status, meaning that donations to the projects are tax-deductible, and if you're interested in contributing to this exciting research that way, you can find more infos on our website.

As part of our communication efforts, we set up a large watchtower. And sent out a team of surveyors to map the settlements on the island so far. Surveying, and reporting all the innovations on Monty's Island, was a huge effort this year. But we're really happy that, ultimately we were able to send an impressive report of Monty's capabilities to the mainland. This preprint was just accepted at Neuralcomputation, a really great journal, so you'll be able to see it there soon, or you can check out the preprint, and Niels will cover a lot of the exciting results in there in his talk.

We also added more checkpoints along the roads to measure the traffic on them. For instance, we added the ability to measure the amount of floating-point operations that Monty uses during learning and inference to compare it to transformers. Later, we also added an observatory on top of the watchtower to make it easier to watch what is happening inside the learning modules and Monty's brain. And Ramy will tell you a lot more about these cool interactive visualizations of Monty's brain in his talk. There's a whole repository, if you want to test these yourself. All the while, we documented more and more of the project, and broadcast new tutorials and explanations in our documentation to the Lighthouse. And you can find a variety of information on all the topics around the project here, as well as tutorials. You can interactively go through to get started.

Over the past years of discussing and testing how to model compositional models, the theory solidified. And so in our recent preprint, we describe the long-range connections in the neocortex and the functions that they could serve. And it contains a lot of detailed descriptions of our theoretical advances over the past years, like the need for reference frames, the role of the thalamus in this process, as well as, how the brain could be modeling compositional objects using hierarchical connections.

The theory around modeling compositional objects was also turned into actual code and tested, so Monty now has the ability to stack learning modules hierarchically. And we set up new benchmarks and metrics to measure this ability, such as this dataset of different logos on different shaped objects. And the team of architects is now working on improving this ability through various adjustments to sensor modules, learning modules, and the policy.

Throughout the year, we also improved the usability and accessibility of our implementation. Monty is now versioned and distributed on Anaconda. And we steadily improved our infrastructure around the project, so the roads here. That included fixing bugs and issues, larger refactors, introducing rough formatting, switching to different configuration management, and many other improvements.

We also mapped out a plan for how we can turn our current code into an easy-to-use platform, and Tristan will talk more about that path through a platform later in his talk.

In May, we held a little robot hackathon and built some more piers into the lake of reality. We tested Monty on several real-world tasks. such as a drone, a mobile ultrasound device, and here's a Raspberry Pi with some motors and camera on a LEGO setup. And yeah, it was really exciting seeing Monty actually control a motor in the real world for the first time, and learn and infer objects this way.

If you are interested to build something similar, we also documented all of this in our documentation.

We ventured a bit further into the lake with our application of Monty to ultrasound, and created a small dataset to continue evaluating Monty's performance on this application. So we basically put a bunch of household items into a bag of fluid and let Monty learn these objects through one scan on each of them. And, yeah, we'll publish that dataset soon, if you want to check it out and test it yourself.

We set up a little camp below the magical tree of simulation. And did several things with the simulator. One is the ongoing work on integrating Mojoku as a second simulator. As well as simplifying Monty's simulator protocol, and prototyping separating Monty and the simulator code completely. Throughout the year, Learning Module Town has been working on improvements on how Monty can sample and delete hypotheses dynamically, and Ramy will give a nice intuition of how Monty's hypotheses evolve over time in his talk. But at a high level, this led to improved noise robustness, speed, as well as the ability to quickly change hypotheses and deal with multi-object environments, so notice when the sensor has moved from one object to another object, and quickly update hypotheses. We also improved the connectivity between sensor modules and the motor system to allow for better model-free policies. And to coordinate between goals coming from Learning Module Town and goals coming from the sensor module market, we added a little checkpoint before the mortar policy city, and that's called the goal State Selector in Monty. Together with a new sensor module on the market that can detect saliency, Monty can now move much more efficiently to interesting parts in the world. And that makes both learning and inference more robust and efficient, and this is a feature we're currently extensively testing. We also began constructing another sensor module stand on the market, specialized in the extraction of 2D features like edges and the texture on an object, like here on the Thousand Brains Project logo. We also made several theoretical advances on particularly on how the brain models object behaviors. We published a write-up of those on our documentation earlier this year, and continually refined it throughout the year. We also published and developed a concrete plan of how we could implement this ability in Monty, and basically let Monty deal with a dynamic world where objects can move and have behaviors instead of just static objects that never move.

We also have been making some progress on Figuring out how we could learn causality and use models to effect change in the world. So, caual interventions. And as part of that, we also started exploring hierarchical decomposition of goals into subgoals and more complex action planning. And that tied together some of our theory on hierarchy, as well as behavior models and goals. Our theory discussions also led us to explore the concept of attention more, and we got more clarity on how attention could tie into all the other aspects of the theory. In the progress of, thinking about all this, we also explored many related topics, like fast learning, the role of the hippocampus, categories, and time. And, in the past year, we've mapped out main new areas of the island, and think we have a much better understanding of the big picture now, even though many open questions still remain towards the east of the island. And if you're curious to how we develop this theory, we spend a lot of our time in Zoom meetings, in deep thought and intense discussions. We meet every week for up to 4 hours. Every once in a while, we meet in person and do this all day on big whiteboards, and every couple of months, we have focused brainstorming weeks where we virtually brainstorm, for 4 to 5 hours, and, think the rest of the day. a lot of time goes into developing, this theory, and, over time, the simplest ideas stick, and, yeah, key insights evolve. And if you're interested in this process, we also record all of our meetings and upload them on YouTube so you can be a fly on the wall of how these ideas crystallize.

And so as these ideas crystallize, we make actions plans of how to implement them in Monty, we prototype them, and then integrate them as new features into Monty, and publish results.

Throughout the year, more boats from interested sailors have been arriving at our little island, and it warms our hearts to see the many exciting ways in which people decide to contribute their time and skills to this project. Will is going to talk more about that in his talk. But a big thank you already to anyone who has forked or started our project on GitHub, and especially to those who have contributed to the code. And also, we've had almost 2,000 posts on discourse, our forum, which have been a pleasure to read and interact with.

To make it easier to see different aspects we could use help on, we turned our project roadmap into an interactive feature work table that's filterable by different aspects, and Will is going to talk more about that. As well. looking back, it's been a really exciting year. We started the Thousand Brains Project as an independent nonprofit entity. We published many of our exciting advances. We're especially excited about the growth of the community, with great conversations happening on our forum, and amazing contributions coming into our codebase. We've made a lot of progress on the theory, all these purple parts. We prototype new ideas and integrate new Monty features. As well as moved Monty closer to a platform that's easy to use. And we tested Monty in several real-world applications. overall, we're proud of our settlements on Monty's Island, and really excited for what awaits us in the next year. And... With that, I hand it over to Niels, who will talk about how our approach compares to deep learning, and what Monty can do today, and its many advantages.