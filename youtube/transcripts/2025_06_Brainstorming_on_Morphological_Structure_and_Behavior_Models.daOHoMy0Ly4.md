okay. Hi everybody. Welcome to Wednesday research meeting. today we're gonna be talking about morphological structure and in the context of everything else we've been working on. So don't have much in the way of slides, but I guess I'll just lay out the basic bullet points and PowerPoint.

So it got, so did you present, you put something together? independent. What I was writing, I basically just took your bullet points and put it on slides, but that's it. so there's nothing, we don't need to do that. I might as well just open the Word document. that's what I was think about doing. Yeah. Oh, do you wanna open the Word document next? No. you can open the Word document. or maybe, I don't know, think I should, because then I can scroll through it easier. yeah. But I don't mind start, I don't mind starting off your bullet point slides, see what you thought. Okay. Okay. we'll see how easy this is to, yeah.

at least I get to, shut this off again. Hey. Oh. Oh, come on. oh. that's too bad. stay tuned for more, thousand Brains Mugs and Monty.

so these are the main, line items, which we'll, get into. I started trying to break them down a bit, but, I think there's, we're just automatically sort to get into them, but, more logical models themselves, just objects in the world have a shape. Deformations is, it can feel a little bit overloaded sometimes talking about some deformations as variations of objects or deformations as behavioral, like in time deformations. But, we can work out how to not confuse ourselves, permanence, meaning like that's, I'm thinking in two ways. It's things in the world can be temporarily or permanently arranged they the way they are, or you can, they can be temporarily stored in memory versus permanently stored in memory. two related but slightly different things. objects that are composed or nested in some way. And then of course, as we've been talking about a lot.

objects that change, in their own, within their own reference frame, and, spatial and temporal structure. Okay, so I don't need to take any more time bullet points. I'll, throw this over to Jeff. Yeah. So as I hinted in my, or suggested in my, slack post, I was, I just felt like our, world is getting very complex in terms of our models and sometimes it wasn't really clear what was going on. And, and then I, to forget things go backing out, oh, no, we talked about this, blah, blah. So then I said, that's sometimes a good idea when things really get complex, just go back as we usually often do and go try to review everything. and so I started thinking like, okay, we're asking this, a learning module, AK cortical column to learn models of the world. And we've got a lot of different types of flavors of those models and the different types of, and so I started thinking about, there's all this kind of structure in the world and, and I'll just try to enumerate or, or spell out the different types of structure that we know exists, that we know we can learn. And, I thought that would be an exercise. And in the process of doing that, I was hoping that some clarity would come out of it. oh yeah, these things are really the same sort of a unified theory of, of structure and modeling. like that's, science often does that. You start with a whole bunch of confusing things and then it solidifies and gets less confusing. So I can't say the exercise was successful in that. a, I didn't get through everything and BI was still confused as I was writing it up. but I do think it was useful and I think it's useful to go through it as a team. I. To discuss it and to, people can propose, if I've missed something or, you just already, Scott, you just pointed out like, oh, permanence can apply to both the permanence in the world or the permanence of the model. And I hadn't made that distinction. That was interesting, useful. I started out thinking along the lines of just thinking about the world. What are the structures in the world? So it wasn't a taxonomy, a model, it was a taxonomy, a structure. but it was hard to keep the two separate, because, I don't know, it just was, I just kept sliding back and forth.

maybe we can just go through it. Item just like you started to do, Scott, and then, we, can discuss them and, I'm sure it just asking questions or proposing alternate ways of thinking about it would be a useful, thing. so I don't, I think this is useful for me. I hope it would be for everyone. I don't wanna take everyone's time, but I think, I thought it would, even though I didn't complete the exercise, I thought it would still be worthwhile going through today.

so is that okay? Does that make sense? Any questions? I think what you wrote already sparked some discussion in at least, or stuff to say in Exel. let's do this. Okay, good. Okay. should I, just share my screen and then, Sounds good. Okay. I'll do that. I'm looking for the share down here and I'm gonna share, I'll just share that screen right there.

I'm sharing a screen, it says, but it's showing me the wrong thing. we're sharing. Oh, we can see your screen. Oh, you can, oh, I see. So I'm not, seeing what you're seeing. Why is that? It's just a bit, zoomed out.

might be worth, we're in full screen somewhere. It looks like it's, I, I'm using two. I'm, got a split screen here, so let me just bring this back over to here. Maybe that'll help.

I won't see what you're seeing.

okay. That looks more like what I expect. So you're seeing my word file here, right? Yeah. And now I'm, scroll up. Okay. Yeah. And now it's more zoomed in. It looks good. It's, it is. I'm okay. it's looks good on my screen. It is a good size for everyone. Should I, yep. Should I zoom in? Okay. the beginning of this document was this preamble of stuff we already know, and I don't know why I wrote it down again just for completing this. but, Just setting the stage here that, the whole idea of a brain or int intelligence system is it's taking advantage of some kind of structured universe. And the whole goal is to, to, there are really two things, is to learn the structure of the universe and or the world that it inhabits, and then, build an internal model and then use that model to, act, to, Hey, I didn't spell it, but you recognize where you are, recognize your situation. You have models of how things are supposed, how things typically behave or what, and it to guide you into acting. So I didn't even talk about the acting at all here. I just said, okay, how do we build models of the world? that's the most important thing. when I think about like deep learning, I always think oh, what kind of model does it build? Or like the large language model, what kind of structure can it learn? I. it's a different type of structure. It's not the kinda structure we learn. it's a linear structure of token in some sense. but we of course learned the structure of the physical world by movement. So I talked a bit about sensorimotor learning. We all know that now. and then, then I came up with this list of ideas, when I started writing, I started writing the easy ones. I started talking about composition because we've already talked about that a lot. And they said, no, let me go back and start with the hard ones. The ones that I, that were most difficult, the ones we don't really understand very well. but, that was just a rough thing. So I moved morphological and defamations up to the top.

and so the morphological model, this is, this was one that was actually really tricky for us to figure out, and I don't remember if we did it as this team or earlier. I can't remember. I think it was this team, but I don't remember. but We were really confused in the beginning. We had this idea that models are just like ob, you build like a model something, it's just composed of other things at certain locations or poses to a reference frame. And yet we were, we can't kept bumping into issues like a line drawing or, how does that work? and there was, I don't, I didn't enumerate all the issues here, but we ended up going to something which I would've never expected is that the base model in, that we learn or that thousand brains projects should learn are, morphological. They don't really define what the actual features are. It's just that the relative, the position and orientation of the features, is the first level of description of a, of an object. and so that would mean if I'm looking at a visual object, it would be the edges. Of it. And, but it wouldn't include things that are more qualitative like color. and it wouldn't even be able to do something like, not very easily. Something with, loading the logo on the coffee cup. it would be more like, oh, there's an object on the coffee cup at some orientation. But, the model itself doesn't really say exactly what it is. Now, of course, we do have models of details with these qualitative features. and but we have a good model for how this works in the brain. We have the minicolumns, which represent the orientation of the feature. and then the individual picking individual cells in those minicolumns gives us almost an infinite number of, or almost unlimited number of particular instances of, of a morphological model. So I could, I could have a morphological model that describes this, the, edges and boundaries of an object. and, and then I could have many different flavors of it, using the same morphological model because it's the same set of minicolumns that are active. but we can pick individual cells within those minicolumns and say, this is a particular cup or this is a particular item. also it allows us to recognize objects when we don't have less qualitative features. So it explains why we can recognize an object when it's black and white, or we can recognize an object.

that's in some sense missing a lot of the details we typically associate with it. It also explains, of course, how we recognize line drawings. And we often talk about, a face made a fruit well, or vegetables. It's like we still see it as a face because the morphological model is the same, even though the individual, features, they have the right orientation, but they're not the right features. They're not, it's not a nose, it's a, a pair, something like that. So that was the first thing we wrote about morphological models. I guess on that, I stumbled upon that topic again too when I was thinking about, defamations earlier. And I think one thing we talked about, maybe one or two years ago, a lot, but I'm not sure if we ever incorporated into, the theory is like whether the morphological models are separate from the feature models and we can, mix and match them easily. So basically that the morphological model, no matter which features are on them, will always have the same object id. like the cup, doesn't matter which logo is on the cup or what patterns on the cup. If it's shaped like a cup, it'll have the cup. Object id, but then you can apply a bunch of different feature maps onto it. so just how that is, how that would be disentangled.

did you come up with an answer? I have some thoughts on it. I came to the conclusion that if we can disentangle it, it will solve some problems, but, it didn't come up with an answer of how we would disentangle it. No, this also gets, go ahead. I was just wondering some of the problems, like some of the recent ones or some of the problems around, I think it would solve some of the object distortion problems. it does play into object distortions because when I was asking myself or deformation, I use the word defamation. Is that okay? that same thing, right? Yeah. Yeah. Same thing I said. I said, what are we deforming? We're deforming, I believe my first intuition is we're deforming the morphological model. and so you can apply defamations to a morphological model and then particular ones would carry over their features.

it also gets to something that Niels has brought up many times, which is, at least in the past, the issue of class classes, of objects, and, it felt like, this is the sort of class I could have, a class of objects that share morphology. and, that's, what class is. And then you, have very different features, but I also, that class has to include deformation. So there's something like, there's a morphology model. You have a defamations you can apply to it, and you can also apply individual feature sets to it. And, and that, that seems like it might cover the universe of these things. it's interesting in the biology, I. The way we always, when we first started this, the temporal memory algorithm, which is the algorithm we think is being used for this, even though it's center motor, we assumed the minicolumns themselves were the actual detailed features, but never met the biology. The biology always said, oh, the minicolumns look like orientation. They're like, there's just not that many of 'em. In our first paper on the, one that, the thousand, why neurons of thousand synapses? We would, we mapped out, features onto many comms as an SDR type of thing. But that's not right. That's not what it looks like in the brain. So this fit all of a sudden, oh, we have this, if the minicolumns are just orientation, that's great. And then the individual cells would be, because of sparse properties, you can have many individual representations of those minicolumns. But then we have the question, I think with, I'm getting to the point you brought up, which is what I want to infer the object. What am I inferring? What am I labeling? what am I not inferring? If I wanted to, create a class, an id, we assumed that there was a, temporal pooling going on in the upper layers, and so we were temporal pooling over the individual cells so that the algorithm we outlined would say, oh, I'm, I have a label for this specific instance of this object. But, we also wanna be able to say, oh, it's, a generic object. It's a cup, not just the Memento or the Thousand Brains cup. yeah. at least for what I was thinking about, it would already sufficient if just all the cup shaped objects would be in the same reference frame. if we can use the same unique location space, for any features that might be on the cup, then I think it would be okay to then pull it into different object IDs. But. As long as they would be in the same reference frame, it would solve some of the issues of object distortions or, yeah. okay, so, this is, I wish I could, I'm not drawing, I could try to draw on a scale draw, but it might take me too long. I think this is very similar issue. So the issue I was just about to bring up is that at one level you'd wanna be like, you'd wanna be doing this, temporal pooling over mini column activations. 'cause that would be the idea of the, maybe the idea of, I don't even know how that works. I don't, it's like you you wanna be able to do, you wanna be able to do temporal pooling over the cells and you wanna be, you wanna have a unique representation of the minicolumns. You wanna have a unique representation of the cells and you're just suggesting something. Same for the reference frame. You wanna have a unique representation, for the class of objects of cups. You also wanna have a unique rep, you wanna be able to somehow tie that into a, reference frame that was specific to specific cups. So there's this duality we're talking about between, the class of the object, which is just, independent individual, it's shared among many objects. And the reference frame, we'd be shared among many objects, but then we wanna be able to specify specific versions of it. I have images in my head as I'm talking about this, so I think I, I think it's a good point. If I were to say, your point is we should be able, it would be great if we could treat the, morphological object as if it has its own reference frame and that we can path integrate to it and we can temple pool over it, but we also wanna be able to do the same for specific instances.

does that make sense? Is that what you were saying? Yeah. Basically having the ability to one, apply a bunch of different, I'll call it feature maps onto a morphology model. but then also having no matter which logos on the cup, it'll activate the same, minicolumns and same locations in the same reference frame on the morphology of the cup model. The, problem we have there, if you imagine if we had a reference frame that was just for the morphology model, it's like the class reference frame. if I know where I am on that reference frame, how do I make a specific prediction for a particular instance?

guess you wanna, yeah, I guess what, how I was like, how I would propose it could be is that we have one reference frame for all instances of mark shaped objects, no matter what the features are on there. That reference frame is for anything that has those particular orientations at those particular locations. And then whatever cells we activate within the minicolumns can be like additionally conditioned by the specific object ID or by a feature map id. that tells you, all right, if I'm on this feature map and on this morphology, then at this, at that location, I should expect this particular feature.

Yeah. But I'm, thinking about neurons. I don't, have the neurons do that. Maybe you can do that easily. Software, wouldn't just like apical kind of, yeah. biasing, yeah. Do that. Yeah. Yeah. But I, guess it's also with the top down feedback is that would also potentially be unique.

which I don't know. Yeah. In general with the feature maps. 'cause I feel like. We avoided the need for feature maps by having the point by point compositionally, which, so with the logo on, on a mug, I'm not sure what you mean by feature maps. This just be maybe I'm what, at least my interpretation of what you were just describing, Viviane, of having some sort of large kind of representation that you wrap around a morphology model or kind of bind to it. that's what our, the hierarchy is meant to be doing. It's in, and I thought that in hierarchy, the object idea of the child object is also just specific activation within the minicom. So that would be treated as features, which I was thinking of as like basically the feature map defines which specific cells you would activate in a mini column.

I guess I'm just confused. Where we need an additional feature map, representation. what's insufficient about the compositional? like the main one where I, it makes sense to me right now is if we want to say okay, the color of this object is, like blue, so we wanna, or it's covered in checkerboard patterns or something. So we wanna predict that everywhere without having to do a bunch of binding. which I feel like yeah, could be something like relatively simple, happening in L four or something just about I don't know if it's necessarily a feature map, but it's just keep predicting the same thing unless I tell you to learn something different. Maybe after we go through the document, I can just draw some of the drawings I made on Xca draw and then, it might be clear. Okay, cool. When I'm talking, because it's a bit hard to describe with words. I think, just before we go on, I'm. You know that it is tempting to think like this can be solved in the hierarchy, right? Because that's the way you can relate to reference frames to each other.

and, that mechanism feels really good. So I, think this, I think that's what you were suggesting, maybe Niels. anyway, so I just wrote down some ideas here that we've got these two, so I'll just call 'em object spaces, the morphology and the detailed, I call it something else. And then how can they share a reference frame? And it seems like we need to be able to fluidly go back and forth between these two. we need to be able to pattern engage one, but predict in the other. But maybe we can't predict we can do, if we can't predict a specific one, maybe we predict just a morphology. There's a lot of, a lot of things like that happening.

I think it's also worth, I'm just gonna say here, can hierarchy, Solve these problems.

one, one thing I, before I go on again, one thing I, realize is that, when we, can learn an object of vision and then infer it with touch or learn with touch, infer vision, we can only do so with the morphology object. 'cause the qualitative features of touch and the qualitative features of vision can't be shared. So it's okay, so I end up somewhere of this when I learned this thing, I end up with a, a detail, a model of an object that is shared across modalities. But that one's is really just a morphology object. Or at least I think it, it certainly can't have color or texture or temperature and things like that. so that's another way of thinking about it. And if I think about hierarchy, I would imagine this pure morphology shared model would be higher up in the hierarchy. 'cause it's across modalities. I. Or it could be voted on cosmetology. But anyway. Okay. So I'm just spouting ideas here. So can hierarchy solve, so we, this, is a good idea here.

anyway, so it, it, felt like then we've been talking a lot about deformation. Did we cover everything I wrote about morphology here? Yeah. Let's see here.

Yeah. And deformation. This is the one that, you know, whether it's on the spectrum of t-shirts to bent logos to Vivian's balloon, these are all confusing.

so I said the examples are flattened ball, the bent mug, a crumpled t-shirt. I made the observation, I dunno if it's true, I said, in all cases, the relative positions of features is maintained.

but the overall morphology. It has changed. the relative position of two things on, a t-shirt or on a Ben logo or something is they, it hasn't changed. It's just been, it's just the mythology has been messed with. you mean like the neighborhood relationships office? Yes. Between features the neighborhood, like the distances might have changed, but Right. We're next to each other are still next to each other. Some, there's some, concept of, it's as you suggested, you could stretch the reference frame and if you stretch it, as long as it just keeps stretching it and don't break it, then the, and the features go wherever the stretching takes them. That's what I'm talking about. if it's three features, A, B, and C in a row, they can't become ac and b, they, can get stretched out or they can bend, but the somehow they're still, if flowing through some space, they're still in the right orders. And of course, on a t-shirt they could fold on top of one, but if you follow the surface of the T-shirt, they wouldn't be out of order.

so it struck me. Now again, this is just, I may be wrong about this, but it seems like we had two possible explanations. one was to, it was like the compositional idea where, where I said, Hey, the logo could, you can take an object, like a logo and apply it to any morphology model. And so you could take the logo and put on a, wrap it on a mug. You could take the logo and wrap it across the top of a car. You could put it on a sphere. And so the idea there is you have a, you have to have a hierarchy and you have, a morphology model in one region, and you have the specific object you're modeling in the, like the logo on the other region. And then through a hierarchical connections, you, you are basically using the morphology of one region to do the path integration.

and then, go back and pick up the assignment in the other one, which is the specific thing, like the logo. so that was one idea. I still think that's valid. it has a lot going on. You proposed Viviane a different one, I think, which is more of a defamation of the reference frame. it wasn't like the reference frame was deformed, but the reference frame, the, how you update your location. Orientation and, direction through path integration would be modified on a point by point basis, which is pretty much the same as the formula, the reference frame. yeah. yeah.

yeah, it wouldn't actually involve the thalamus itself to, to do that, but, and yeah, like you said, it wouldn't, it would just update the location you expect to be in instead of actually deforming the reference frame. But yeah, it's, I thought originally you spec, you suggest it was done through the thalamic projection.

that's why I liked it. I didn't say where it would actually, how it, I'm not sure what the best connection for it would be, but I don't think it would require the thalamus, it would just require sending a movement command to the child objects reference frame.

alright. But what the Thomo is already doing. Scale, and we think it's doing scale and orientation changes. and I thought originally when you proposed this, you said, oh, we could just do it on a more, point by point basis in the thalamus. And that's one of the reasons I liked that theory. That's one you could, you put it out there, you said, this may be crazy. You probably don't gonna like it. I said, oh, I do like it. And that was because you were, because of the thalamus. If you hadn't brought the thalamus, then I probably wouldn't have liked it as much anyway. Yeah. It's, very, it actually, it's, I, actually didn't see it as an alternative to the compositional solution. It builds on the compositional solution. Oh, really? but, I tried to make a visualization of it, which I can show later to show kind of the similarities and differences. but yeah, what you wrote is pretty much, fitting. Yeah. I still, I still, Like the, compositional thing. yeah, because it's a mechanism we're pretty certain exists and we're using it already, and it'd be nice to use it again. And just like we were talking about, just like we were talking about the relationship between morphology models and specific models, and maybe that could be solved with hierarchy. This is very similar to that. yeah, so the mechanism I was proposing basically takes the compositional solution and then adds an additional mechanism for updating location instead of predicting, orientation changes only. Yeah. I guess I'm not sure how that would work if you didn't go through the thalamus, but, I, I thought when we last talked about it, yeah, it felt like it would be easier if we didn't go through the thalamus because it is like a translation rather than a rotation or a, yeah. Scaling. so that actually feels like something that fits better in L six or, okay. Something like that. I, within a column rather than. yeah, so my preference, it's just me and I don't, my, I get one vote, maybe one and a half votes. my preference is oh, I don't wanna introduce a new mechanism, if at all possible. I'd rather just camp on to existing one or even a new concept of so I'm just, maybe I don't understand it well enough, but it felt like, oh, this is a whole nother thing that we have to somehow get these cells to do. And it's already, we're asking a lot, but that's the thing, that's what L six cells are already doing, is integrating translation, integrating movement, whereas we've never talked about the thalamus doing translation. Okay. fine. I still, it just, it didn't feel right to me. yeah, it doesn't, feel right to me now, but, I think we can make the case and maybe there's an in-between, like you said, maybe it would work. I just feel like I wanna start with the idea that can composition solve this problem? And, and if it can't and you want to add some additional details to composition, that, that could, if I go for that, if I'm convinced that the com that the existing mechanism somehow doesn't work or insufficient, yeah. I think I didn't do a very good job on explaining how it, relates to the composition, mechanism and works. I missed, so maybe I can clarify it a bit later. Anyway, I think this is, an area we're still, we don't really understand. it reminds me a lot of the, confusion we had before we understood there was this, these, morphological model that was, as I've said before, that was, there's not an easy thing to come up with. It was not an easy thing to think about or accept at first. but then it made all sense. The biology supports it and looks right, so this reminds me of that. It's like I. There's something weird going on here, and maybe it's simpler than we think and it's just like different way of thinking about it or something, I don't know. But right at the moment, it's confusing to me. yeah, one thing I've been trying to think about, it's too early to present, but just, yeah, whether kind of something more just like associative connections, between like certain cells and L six or something could do something similar. and that, I don't know, might be a like, yeah, might have slightly different properties and also help with some of the biological plausibility, but I'm hoping to try and yeah, write some of that up. one thing that if we, can still brainstorm here, when I was thinking of examples of deformation and you had that one where the cup was bending, remember? the whole cup was bending. And I, it occurred to me like, it feels like I try to, I have a already have a. Some pre lorned memory of that shape. like I've seen that shape before, so now it's familiar to me. I can infer that bent shape with Oh yeah. It's like a bent pipe or bent, straw. I, know that shape. and then I was trying to imagine defamations, which I, I didn't know I've never seen before, and I felt like I'd have to spend some time learning it. I, it wasn't like, oh, I can just, it was almost like I've already learned somehow that image of that particular bent thing. and I might even know it as part of a behavior, oh, that's the intermediate state in some behavior. But it wasn't like I could just take some arbitrary, it wasn't seem like defamations were arbitrary. It seemed like I'd already had, I was trying to fit into a, some previously learned shape and say, okay, I'm applying it to that shape. That's the, bent. Straw shape or the ben pipe shape, I can now apply this to that. and then I was thinking about shapes, which I couldn't, dely had never seen before and became much, much more difficult to do the, inference and prediction tasks. I'm sorry, I'm just pointing out that I think even those deformation may be, recognizable, morphology models. Yeah. That's how I can get back to the, morphology versus feature model idea because I feel like if we can separate the two then pretty much all problems could be solved by just recognizing a different morphology and projecting a certain feature model onto them. with the bent cup, you might just recognize that bent morphology and then you project the logo onto that, which will automatically bend with it and you project whatever other colors are on the mug. So the, big point. Go ahead. I think it, again, I guess I want to know a little bit more being just territory, because I, thought now I think that morphology models might be, anyways, so really to what Viviane said, that we might need to also do backwards what we project morphology into some features, like I was thinking about mirrors, like mirrors can have so many, like different mythologies, like bathroom or handheld, but the share, the same feature of something is like reflecting. so it's not that we're like map. I don't know, like I don't know how to handle that, where okay, if I don't, know if I have a morphological, I guess morphological models of mirrors, but I guess they, they share. So you got me confused data mirror. so let's step back. Say this again. I got confused. Okay. so the. Morphological model of a mirror is so I can imagine us grouping like bug shaped things. yeah, a morphological model of a mirror is weird. I don't know what that means. you're saying a mirror image isn't that more kind like accordance almost like the fact that something illuminates? Yeah, like like literal or the fact that something is a chair and you can sit on it bathroom mirrors or body mirrors, like okay, morphological model of a hand mirror. Okay, I got that. Yeah. But there are so many different morphologies of mirrors. There's like the handheld versions, there's the bathroom mirrors, there's like the body and I think that it can be like weirdly bent or something if you go to like mirror houses or something like that. So in that case, the morphologies that a mirror can take is wildly varying. Again, I think, you're mixing two things up, aren't you? One is just saying the shape of a handheld mirrors. I can imagine handheld mirrors. And now you're saying the shape that the MER produces if I'm in a fun house, those are two things. Aren't they just two separate things? Are they two different IDs? No, I think it's, more the fact that you're saying that all these mirrors have the same property of reflecting things. Yes. And that's why I was saying that I feel like that gets more to the thing we discussed before, which is like you can also classify objects more on something like avoidance, like the fact that it can, a vessel can hold fluid, like vessels can have feelings of different, of morphologies. The fact that you can eat something. and like at least the briefly when we've talked about in the past, I think our feelings like, yeah, that's probably represented as an ID as well, but it's something different. Like whether that's in a different layer in L two, L three or what it is. But it's kinda learned in a different way. like, context and whether you can use something for certain tasks and things like that. but I feel like, yeah, what you were talking about Viviane with a feature map is something different that's. I, think if I understand it it's actually a good example of applying features to different morphologies. Like you have all these different shapes But you're applying the same features to it, which is the kind of reflective surface. So you can apply reflective surface to all kinds of shapes without problem. and the reflective surfaces in this case of feature and then that is on, can be on a bunch of different morphologies. Yeah. Yeah. yeah. maybe I'm on the side for disentangling more. I'm finding the mirror difficult. I'm, I'm, not sure how to that's fine. We can drop it for now. Think the mirror is maybe adding a lot more complexities because it will have to model like what's in front of it and what is it reflecting. Yeah. And stuff like that. I'm fine. Yeah. To me it's it's not working for me 'cause it's, yeah. But it just briefly, dimension on the logo with that being like a feature map. I feel like doesn't the logo have a morphology as well? that's how we recognize the logo. It does, but once it is just an object id on the parent object, then it is a feature map. Like where does the logo exist? that, that's why I'm confused how that's different from just the hierarchy we have at the moment. But yeah, again, maybe the drawings you said will help.

Yeah. And maybe we can just go to drawings now to just, if you want to, I can stop sharing if you want, but, I think you only had one more. Okay. I can just, I can try to finish up here if you want. I thought, yeah, the permanence one was interesting, right?

permanence is one. it's just a real practical thing we have to deal with in, in our systems.

and that is, The world, as I say here, has varying levels of permanence. Things are always changing. In fact, I couldn't think of anything that was permanent, forever. So it's just a, it's just a scale of relative permanence and the meaning, the structure can change over time for various reasons. It could be objects of moving, it could be a behavior. It doesn't, it just, anyway, there's, I was thinking mostly objects are moving. So I gave some examples. one that, I think about all the time is I'm about to cross the street at, on my bicycle, near the coffee shop where I work. And I have to look around and see, are there other people on the other side of the street waiting to cross with me? Are there cars in the turn lanes? And I quickly look around and then I build up this model of the intersection.

and it's unique every day because, there's different cars and d maybe someone's got a stroller. Whatever. and I built this thing up very quickly and I use it to act appropriately in the intersection, and then I never, it goes away. It never occurs again. and this is a really important part of living in the world, right? It's, and so our systems have to be able to do this. And so it does bring up the question that, was mentioned earlier. Scott mentioned it, which is I'm just thinking okay, the system has to be able to deal with various levels of permanence, and yet not get confused and has to somehow know that some things should be the same, because they always are the same and some things are always changing. So I don't expect 'em to be the same. if I went back to the same intersection 20 minutes later and all the same cars and people were there, I'd be like, oh my God, I'm, taking drugs or something.

so I just thought it was an interesting problem of the world.

and I think about, there's different ways you could solve it, but in the brain it seems to be that there are some parts of the brain that are just dedicated for quickly learning models and, forgetting 'em quickly. and that's generally hippocampal complex. there's some evidence other parts of Cortex can do quick learning, but mostly we associate with the hippocampus. and we coach, we think the hippocampus works basically on the same principles as the cortex. Not, exactly, but basically the same bi sticker ideas are going on there. And in biology you have to break it out. At least you have to break it out because it requires a different biological mechanism to learn the, these things very quickly. but that may not be true for Monty in the Thousand Brains Project. and so really the point of this is to say we have to, as we think broadly about thousand brains and building robots and intelligent machines, this is a very important practical issue. We have to deal with, quickly learning models. and then, and some may, some can have to be permanent or semi-permanent. and then occurred to me like, if in a computer, I suppose we could, we don't have to get rid of previously temporarily learned models, we could just store them in some data store someplace so that if I wanted to, I could go back and say, oh, on, on, June 17th at nine in the morning, what did the intersection look like? And I could recall that where human can't do that, but I suppose our, Monty systems could. so I just thought it was worth bringing up. So we all are thinking about it a bit, as a, as an issue we have to do. And, and the brain provides some insights to how we might go about it. But I'm not sure the brain's particular mechanisms for handling us are essential, for a thousand brains.

So that's it. I just didn't want people, I want people to think about it. I was thinking about something in along similar lines when working on these, mug stuff in Blender recently and making new objects and saw that you can make random surfaces. I thought, Monty could learn all of these random surfaces perfectly. what, I know what you mean by random surface. So it's a mesh, just a 3D mesh, and you can click a button and it just randomly generates one. but what it just, it's like a folded t-shirt just does some weird thing. Yeah. Just imagine like a big spiky surface. Okay. So it's its like take, a, rec, a reference frame that represents a T-shirt and just crumple it up. That idea. Yeah. Okay. And I was thinking, Monty could learn all these perfectly as it is now. I could. It had enough time. If it saw them individually, it basically would memorize what, that's basically like memorizing noise in a sense. I wouldn't be able to do it very Well, I think the, combinatorial problem of remembering all the way the t-shirt crumple are impractical, right. You, it's, more than number of atos in the universe type of thing, right? Yeah. you could learn one or two, some number, but certainly not all of 'em.

yeah, But I was thinking that, there might be features about the object to help inform, especially I like this idea of a shared reference frame, if there are features about the object that help put it into a shared reference frame with other things, and that shared reference frame is a space where we can operate temporarily in and not necessarily need to be adding a. New features to it, like that particular baby stroller in the intersection or whatever. it's a vague thought, can you say that again? Sorry, I led to a vague, I have a vague understanding. Took magic. Yeah.

Yeah. So I started thinking about the morphology, the sort of zoomed out course versions of the morphologies, right? especially in an unsupervised kind of setting where I'm looking at a new mug and if I want to be able this, if I want this mug to be learned in a, and recognized in a way that's aligned with my general mug models reference frame, I can use the course features of the reference frame in sort of a model free way to initialize. The axes of the grid cells and, the scale and all things like that. And help try to put something into a common reference frame and maybe having a common reference frame is a way to, I, I'm still, off Scott. maybe I'll try to write this up. This is, you can write better off put maybe, right? Okay. Yeah.

It might be helpful, but yeah. Yeah. We, maybe it's worth briefly just mentioning I guess one, the like common reference frame, A nice peel of that sometimes has come up is, this issue of kind of the search space when you first start recognizing something that like you have all these objects you're trying to search over and that maybe that could help constrain that. And also it doesn't seem biologically plausible that we can search in parallel over all, these objects and, but but yeah, that's maybe just another reason to think a bit more about that idea. Hey, can I just throw an idea? I thought of it and I just so I don't forget it. Stomach can, may remember it for me. there's a classic example of a problem, like you're looking for a place to sit, but there are no chairs. But then you find things that, what can I sit on? Or you might see something, so I can use that as a chair, or, you might say, that's a different chair than I've ever seen before, but I know I could sit on it or something like that. and that remind that. What that made me think about is what you're doing is you're, that's almost you, you're, let's say you have a chair morphology model, your classic chair, and now you're willing to, you can morph it, deform it into different ways. And maybe if I'm looking at another morphology model and I could tend to a subset of it, I could then see that subset is similar to a, to the chair and the same way that. that behavior models are a subset of the whole overall model. anyway, it's an interesting problem. Say how do I see something say, oh, that's good enough to use as a chair. And I think it's the morphology models have to, you have to be able to look at, I think I do the opposite. I take a model of myself and try to, imagine it sitting down on the object and then instead of trying to take a model of a chair and matching it to the object, I didn't say, I didn't say it would take a model of a chair, it'd be more like, I'm looking for a place to sit. I think that's the starting point, right? I'm looking for a place to sit. I guess I get, I guess I am looking for a a sable thing. We can call that a chair. I don't know. it's gotta have a horizontal surface at a certain height and maybe a back, maybe not. Maybe I'm not sure. I just, I feel to determine whether it's suitable, like that's where you do the mental simulation. yeah, you might bias your search for chair like objects to help you be more efficient. But almost anything could be, a chair under, or, or so many things. Or, so many things could, yeah, hold some quit or whatever. All right. Maybe it is, maybe it's not a good, let's erase this example. But it's fun because that, that it has an exam, extreme example. 'cause I recently sat on those, like medicine ball, not like those huge like yoga balls and gym, like where you can do things like, and that ball is not, doesn't have any morphology of a, doesn't have any flatness. I guess the only thing is that if put your weight on it, you will. You can't sit on it. 'cause I guess I did a pretty bad chair. I think it would, you'd roll right off, wouldn't you? yeah. So it trains your course. That's the whole point. I guess it's supposed to be actually pretty good for you to sit on those as an office chair. Yeah. Yeah. Anyways. I think it's a different, it's a different thing than sitting. It's a, I think I realize I was conflating two different things. One thing is a classic example of chairs of different shapes and morphologies and how do you all know that they're all chairs? they could be baroque, they could be this and that. and then that's really a separate question. Then finding a place to sit. It's not the same thing. I was conflicting through things. Alright, I'll take that back. I did. But I do think the idea that on an object we might see a subset of it, the morphology of a subset of it is recognizable as, something we know already. In the same way as that, behavior can be a subset of a bigger object. Again, composition would play plays. Okay. I'm, rambling. Sorry about that. I guess I have one more thought on what Scott said, about the kind of random objects generated in Blend and the crumpled t-shirt and stuff. And it reminded me of the solution we, suggested for the practicality of learning object behaviors, where at first you learn very few points of the behavior and you try to, I interpolate between them to make predictions. And it seems like that would also apply with learning apology models and also learning distortions of morphology models where if the object is sufficiently simple, let's say you have a random shape, but it only has six edges, they might just be in different locations than on a normal cube. that, that would be relatively easy to learn, I would say. quick to learn. 'cause you could just store six points or a couple more and then interpolate between them to make rough predictions. But then if once they is more and more complexity, like a hundred edges on that weirdly shaped object, you need to store more and more points to be able to make accurate predictions. Similarly, with the T-shirt, when it's crumbled, there are so many little local changes. You need to learn to make good predictions that it's difficult if you learn, but you can learn a course model of the overall shape of the crumpled t-shirt, like whether it's like L-shaped crumpled or in a pile or something like that, and make those kind of rough predictions. so yeah, I guess just remind me of that. And in terms of Monty, that maybe like gradually moving more and more towards learning like I. Models with very few points and inter interpolating between them to start with, and then only if it's really necessary learning, adding more points or higher re resolution in certain areas. Yeah, I think that's a, good point. That's a theme that we see everywhere. that's I, the example often I use is recognizing trees when you're a child, you just, if you don't know anything about trees, they all look the same to you. And then you, as you get better, you learn to know what details to look for. and then you fill in those details later. It doesn't throw away the original model, just, you just can add more stuff. It takes some time and memory. But anyway, I think that's what you're saying.

Yeah. I have a thought on the topic of permanence. I see this clear distinction between permanent models and temporary models, and it makes me think that, perhaps it's, more of a, on a spectrum of, how things are permanent or temporary. I, didn't I say that wrong? I said it's a spectrum. There are no permanent. Oh, you didn't? Yeah. Yeah. I couldn't think of anything that's completely permanent and so it's just a spectrum. Okay. Yeah. I'm not sure. No. and I'm wondering if it relates to the problem of, key frames in, in, in a behavior where, you know, so some, if we think of, of one key frame that we always see, like maybe very frequent as, like a permanent, arrangement of these orientations. And then as it moves, maybe we have another key frame and that's another semi-permanent arrangement, but all of the others are like temporary. And I'm wondering if the two problems are related, to each other.

I didn't think of it that way. I thought in the behavior, you don't learn the morphology as the thing is moving. There is no, There is no memory of the in-between states unless it stops or it's going very slow. And if it stops temporarily, then you can say, oh yeah, I get it. I can learn that as a morphology model.

but I don't think, I don't think we're learning and forgetting the in-between one. that doesn't feel right to me. It doesn't, maybe you think that's right. I don't know. It doesn't feel like that way to me.

It's like, when people used to, there was an old, one, one of the very first, uses of scientific uses of a, of a video, a motion picture. There was a lot of debate about what workman horses ran exactly what their feet were doing. No one could figure it out. 'cause as a horse is running by you, you just can't see it. It just, it's only when they actually took video, or. Motion picture of a horse, and he looked at each frame. Then it resolved this big debate about what the horse's feet were doing and its legs were doing. Yeah. And so that's there's no way you can see what the morphology of the legs are as long as the horse is running. That's, that was great example. Yeah. And I'm just wondering if it's more of a statistical thing where, you know, if you see something 90 of the time, it, it has this permanent, it's, it becomes more of a permanent model that you can store. And, if you, if it happens to change a lot, you basically don't store it. that's the basic biology, that, having been learning is that, with repetition, it, actually, this was in the, neuron paper we wrote. It was an observation that some of the scientists made once, I don't remember who it was, but it was really insightful. They said heavy learning is not really about increasing the strength of a synapse. It's in, it's about increasing the permanence of a synapse. So it may release the same number of vesicles, but the synapse itself gets, becomes a, thicker structure and it gets this really, permanent physicalness to it. And, so the idea is that, if you've repeated something over and over again, or if it's done in some sort of emotionally salient context, the synapse has become permanent and they just don't, they don't outgrow very rapidly. whereas if you just re do see something a few times, you get this little filament that, that can go away. But it's not like strength have been, it's all about, about permanence. So there's a basic understanding of the mechanism, although many neuroscientists don't really understand this still, that, that, yeah, under you have is a complicated formula about what leads to the permanence of a synapse. Again, mostly sounding things are remembered very strongly, even with just few exposures, but generally you have to expose something over and over again. and if you do it enough times and if properly, then it becomes, it'll be much, takes much longer to go away. This is, yeah, that makes sense. You got memories from your childhood that sort of stick around, even though you haven't experienced them in, in my case, a lot more years than yours.

but, decades. yeah. And, it's interesting to think about Monty and how we would implement all that stuff. I don't really know because we're, not doing synapses right now. And we just, we modeled it. We modeled it in the, and they're on paper, just a variable called permanence. as to, yeah, I guess the closest thing we have right now in Monty is the grid object model, which basically, only adds points to a model permanently, in the top K locations where they were observed. So only the most consistent features on an object will be, will become part of the model that's used to make predictions.

again, it may be required to have some sort of forgetting going on. Yeah. And then if other features are more consistently observed, then the ones that are less consistent will be dropped again. yeah. And maybe, I don't know, do you ever have, do things, do you have things that become permanent? Permanent or is everything is on a spectrum like said. right now in the mechanism, it's like a top cake kind of mechanism. So if something else becomes stronger, then the lower ones fall up. Okay. So it's not, there's no permanent, one. Yeah. It's not like an object would at any point be completely forgotten. It's more like the locations that we used to represent that object.

Yeah. We don't have a lot of forgetting in Monte right now. another thing related to this is if you think about the hippocampus, and it forms these episodic memories very quickly. it's, I'm sure that's where I'm storing my memory of the intersection while I'm in the intersection and and I can recall that even an hour later I'll recall details of the intersection. It's not like it's lost immediately. But anyway, if it's in the hippocampus, Basically I'm already, the memory in the hippocampus is already a, it's a composition of other high level objects, right? Cars, people, strollers, even people like you look at someone and say, oh, what's their intention? it's a child or that's a parent with a child. You have different, it's very high level objects. And so the whole rapid memory is built on the fact I have a very, already a very, good model of all these things. And so if I were to, if my models are to change in the brain, it's okay to change 'em at the top, at this sort of hippocampal level type of thing. But I don't wanna change 'em in V one or V two very often because if I do, then, everything above it gets blown away, right?

if I forget what, bicycles are and all of a sudden, I, or I have a new memory of what idea, what bicycle is, all my old memories that are based on bicycles are gonna get really messed up. So anyway, there is this sort of hierarchy of permanence. Seems to be, required to keep your sanity, Okay.

All right. We can, if you want, I can, stop sharing my file here. it's funny, I wrote a whole bit, a whole bunch of stuff on composition, and I said, we already know this. Why am I writing it? I'm just repeating what we already notes. I deleted. I don't think we have everything. We have more things to say about behaviors, and I have more things to say about spatial and temple structure, but I didn't get to it.

anyway, if someone else wants to, bring up a c draw, yeah, I could go over what I drew, but I saw Hojae, you also drew some stuff. I'm not sure if, you wanted to go over that first or, not really. I think it makes sense to, before I forget, like what we were just talking about, my memory, my short term memory is very short and my long term memory is also short. So just to keep it in context of what we were talking about. Yeah. Okay.

yeah, so I have two things I drew on here. One is after reading your document, Jeff, I realized that I didn't really make it very clear what the kind of proposed solution was and how it related to the compositional model solution. And I went back to the, this document and I realized I never really showed it, how it relates. I, did write that it is as in our standard hierarchy framework, but that was like in a bracketed comment. So I tried to make that a bit clear. And then the second thing is that I just tried to think about, I. Circles and ovals and, if we can solve this with pure morphology models. And that's where I had the thought about separating morphology and features a bit more. So actually after thinking through the second one, I'm not a hundred percent convinced that we need this mechanism anymore.

I think I have to think more through it would way would be easier, but, yeah. Yeah. Interesting. It's my bias then maybe it would be more in especially interesting to start with the second stuff, but I don't know what you think.

yeah. So I guess, the second one kind of, did you feel it supersedes the, other stuff is, the second one, the circle thing? Yeah. And feature map. It turns like the circle seems to be the hardest thing in the world to understand.

it was really more like an afterthought after I applied the like proposed solution to circles and ovals. So it's just this orange text, basically saying that, okay, so the whole point of, doing these modeling, these object distortions in a separate model from the morphology model, the whole point of that is that, so you can apply the same distortion to different models. so basically, I can learn this kind of circle to oval distortion. And then I can apply that to like a ball that squi, or, and that can have the logo on it, it can be striped, whatever that distortion applies to, anything that might be on there. But then as I was trying to put some images of examples on here, I realized that all the examples I could think of were just like changes in features on that morphology. all of them would still have the same oval morphology. So the thought was just that, if we could basically just learn, separate morphology models for the circle and the oval, instead of learning the circle morphology model and the oval distortion model, and then apply any kind of feature map onto it, we wouldn't isn't, is that what I was saying earlier? It was like we just learned these as separate objects. Yeah, exactly. Yeah. So we would basically, so here blue represents a morphology model that's basically defined by orientations at locations. So we have the circle and the model of the circle is, defined by these specific orientations at those specific locations. we have an oval and the model of the ovals morphology is defined by those orientations at those different locations. So that's kind of option one. We just learn, two separate models for these two shapes, two different morphologies. Option two is we learn just the morphology of the circle and then we learn how to distort it. and I just went through an example for that. But then, basically I. what does this help us with? wouldn't we have to learn an equal amount of distortion models as we would have to learn morphology models in option one? yes. We would have to, if we just have ovals and circles and they're all just morphology models. but it helps a lot once we start applying the same distortion to different models.

that was my reasoning, but, can I interrupt right now? Yeah. Why couldn't we, the view, instead of calling it a distortion model, what, isn't it just a behavior? I, have two morphologies. The circle in the oval. We got these two things, I could have just learned them separately. I see circles and I see ovals, or I could have said, oh, this circle, becomes an oval. Which I say would then it would be behavioral model. yeah, but I'd still. I still would just have the, I still have the two morphology models because I, I observed the, circle on the oval after it's been squished. I'm, just saying it just feels like the transition is really just a behavioral model. It's not a, it is not a distortion model. It, the behavioral model could say, oh yes, I can go. Sometimes these go between these two, but they're still two separate morphology models. what's the difference in behavior model and your, distortion model. Yeah. So I didn't get around to that. That was the third item on my list to think about today, the relationship between the two. But I didn't get around to that. But, I think they're actually, they are very related. I guess the main problem I see with this just being a behavior model is that I don't really know yet how we would use that to then make predictions about the morphology. I'm not sure I would, I could look, imagine I have two morphology models, circle and oval, and they just are separate things. And then I say, oh, a behavior model. if I see a behavior, then I can then predict the morphology model, the resulting morphology model. It's just a way of tying 'em together. It's, to predict the next morphology model.

and so it, it is like saying, oh yeah, these can, these are tied together in time, sometimes through a behavior, but they're still separate models, still separate morphology models. Yeah. So you were saying you would have both of those models, a model of the circle and a model of the oval, and then you would have the behavior model that lets you recognize when the circle is transitioning into an oval and then Yeah. And the behavior model's optional, right? Yeah. I may, or may not have it. It's only if I observe it. Yeah. So you're once on kind of option one to learn two separate morphology models, and then optionally you have the behavior model if you often see them transition between each other. And the behavior model would predict, I could say, oh, I know it's, think of the stapler, maybe I have a picture of an image of the stapler in the closed position and in the open position. And those are two separate morphology models. And, I, if I never saw them transition between 'em, I might think oh, there's two different types of similar objects, different morphologies. But if I see the behavior between 'em, then I can say, oh, yes, this should be, if once the behavior started, I should end up at the second morphology model. Yeah. And so I guess one thing just that I had with that so far was that. Alright, you learned this morphology model and this morphology model on those two objects, but then now you're seeing this circle with the thousand brains logo on it and it's distorting. How do you make sure to apply that? Like how do you then make predictions about how the features on that object change? And that's where I always got stuck. Like, how do you then have an object with a similar morphology but different features on it and apply that different morphology to it or that distortion? So I guess if we can think about this path a bit more about having separate feature maps to be applied to the same morphology, that's what I meant with that could solve all of the issues I was seeing with Option one. and I would be very happy with it.

Did you wanna say something? I have a question, but Niels, did you want to ask a question first? Yeah, I guess I was just curious. because I would say like originally our kind of naive approach would be to have just learned a different morphology model for the circle and oval, but I remember you had concerns about that, Jeff, like just that felt wrong somehow, that those would be separate models.

so I'm just curious if, if that issue, I don't remember issue has disappeared or, I don't remember making that comment, but it's certainly capable. Okay. Certainly capable making a comment like that.

I wear my thoughts on my sleeves, so they do, I think it was maybe like when does a circle turn into an oval? Like at what point is it a new model kind of question? yeah, because, and because you could have tons of different ovals and it's do we wanna have Yeah. different morphology models for all different ovals. I don't necessarily have a concern with that. I think we would probably just learn, if it's a slightly different oval, we'd probably just learn it on the fly or something like that. it's funny. Be close enough to previous ones, but I was just curious, it is interesting to think okay, ovals exist on a continuum, right?

but I'm not sure I have, I don't think I have a lot, I don't think I have an infinite number of models of Volvos, but it is possible that I have quite a few, I can have a model of a circle, I can have a model of the oval maybe, like they've been showing it and I don't, maybe a couple other flatter ones or something. I don't know.

I, certainly don't have a lot of 'em. I can't have a lot of 'em. But the question is, if you asked me to draw an oval. I would probably, I might draw the same ones over and over again. draw a bunch, draw four different ovals. I might always draw the same ones. Those might be my canonical ovals. I don't know. it's an interesting question. I don't know.

okay. Yeah, no, I, you know what, I never thought about the, but the distortion here, if I see an oval and I see a logo on it, I don't expect it to be distorted because, ovals are their own thing, and I can put a, it's like a little badge and, that's, a lot of logos look like that, like the one you have down there. I don't think that, that one's squished. I did push, but I guess I was thinking of you're actually observing the distortion like you have so my point is, I have two different expectations. if I see a logo on an oval, I don't expect it to be distorted. However, if I see the logo on, on a, circle, and I know that the circle is now behaving, and it's, I, actually know that the circle, there's a behavior, I'm observing the behavior. then I guess I would expect a logo to be distorted probably. I would recognize if it wasn't, but it was, if I actually playing with a ball, my grandchild and we squished the ball, I expect the things on the ball to it would be distorted. so I don't think there's a single answer to this question.

I it's just an observation that there, there isn't a single answer to it. if with the behavior, I expect distortion without the behavior, I wouldn't expect distortion. yeah, I guess in a more general case, if we have behaving object or distort distorting objects. We'd ideally wanna be able to generalize two, two different versions of those objects. if I learned the stapler once in its open state and now I see a different stapler, that opens, I would expect the features on that different stapler to move in the same coherent way as features. But didn't, but I thought we, I thought we solved that problem. Yeah. We saw. So yeah, I was just using that as an example, but we didn't solve that problem for like actual distorting objects like the balloon or the ball squishing, or the bending or something like that.

it's interesting if I saw, a balloon balloons are not just ovals or OIDs or whatever the three dimensional thing is, that's an object that I recognize and I know it has a behavior. I know how it got in its current state. There's no question in my mind that thing was, inflated. if I just saw a ball, I wouldn't think that, I would say, oh, that's ball made like a ball.

and the case of the ball, I guess my first expectation is anything printed on the ball wouldn't be distorted.

my expectation of the balloon is that something printed on the balloon would've been in a different size or state earlier, and now it's, whether it's distorted or just got bigger, I don't know. But, but I do know that I, because I know there's a behavior to the balloon, I would expect the feature of the logo on the balloon or the image on the balloon to change as the balloon changes.

this is an observation. It's not an answer to how it's happens. It just, so it does seem to be tied to this knowledge about the behavior and object. But yeah, maybe an example, like a pear or something. Like a pear doesn't have a behavior, but you, if we are projecting a sticker or a face or whatever onto a pair, I guess a sticker. I don't think we can predict it particularly well, but it would definitely distort. I have a sticker. it, the sticker on a pair is the same as the logo on the coffee cup. it's not distorted in its own surface. It's only wrapped around another surface. So the sticker is not distorted. it is just mapped onto a different morphology. Whereas the balloon really is distorting the logo or distorting the image. the, it's, not, I think these are the way to think about, I think about this. These are clues, right? All these things are clues as to the solution here. It feels good to, yeah. It's another, it feels like we're making some progress. I. Yeah, I think one, one other random observation, and this is pure introspection, so I'm not sure how accurate or useful it is, but it seems like when we see an a distorted object, like the spent coffee mug, you usually imagine how it got there. at least in my mind, it seems like I'm imagining the cup being original and bending into that state. any kind of distortion that I'm thinking of, I can imagine how it got there. So I don't know if that, it's funny about, yeah, we're digging into a little minutiae here, but, if I saw a pipe in head bent, I would say, oh, that pipe got bent. That's what happens. Pipes are built straight. If I saw that mug, I would say mugs, ceramic mug, ceramic mugs can't bend. They break. So it had to be formed that way.

I'm just, pointing out that's there's subtle Good point. Yeah. It's unless it was a rubber mug, I guess it could be a rubber mug. yeah, I know. but yeah. Does it make sense how option one would work? like basically learning separate morphology models and then using them, like generalizing them to different instances of that object? If we can separate out morphology from features more? I think the approach is a great one. Niels did bring up the issue that I suppose that I brought up with there's a lot, there's an infinite number of ovals, so how do we handle that one? and I was trying to dance around that, because we don't wanna have to infinite number of morphology models. but, perhaps, a few would do. And yeah, I guess for what it's worth, option two would have the same issue. 'cause you would have to learn an infinite number of distortion models as well. Alright. Good. Shared burden. So yeah, so maybe it's a combination where, we don't have an infinite number of ols, we have some small subset and similarly the distortions, they can't be anything. They can just be relatively limited small local changes. Yeah. So they interpolate in the short range with distortions. And you have to do some sort of interpolation in the short range. Yeah. you don't do infinitely accurate predictions. probably on those, yeah, With the distortion Viviane wasn't because one of the advantages of that, is that you can reuse that a bit more than you can use the separate reference frames in this example. like that doesn't just need to be used for. Ovals that could, I don't know, be destroyed. Yeah. I guess egg shapes. Yeah. One of stuff like that.

that's right. One of the other things that I was thinking we could use this, these distortion models for our more kind of fancy stuff where you have a blob or whatever, or you, or like you have, I don't know, let's say you have, what do you call it, A clay mug. You're just forming it and then you press your finger here and it's indenting like that. we wouldn't be, I wouldn't imagine that you have learned a separate morphology model for this indented specific cup, but you can just, you apply a local distortion here. so not sure how that ties in, but Yeah. Yeah, that's interesting. It relates morphology changes I keep talk about with like separate, oh, sorry. You're, what's the word you use for the distortion model? Or def what's the word you're using for this? Yeah. Like a distortion model or distortion model to me is, I think I'm gonna just make that into the behavior model. It's it's a way of tying two mythologies to a behavior. And the, properties, I'm not, I'm just stating and not knowing how it's gonna work, but the properties, this is what I like it to be. If I, if, I could, if properties that you're desirable to have of being able to apply distortion models to new objects. Should be, if it's the same as we wanna apply behavior models to new object. and they really, yeah. Could it be exactly the same thing? Yeah. it's very similar. I guess the main distinction I made, why I made it a separate thing, is that behavior models are like a step wise thing. So if you wanna apply a behavior model to a morphology model, you have to step through the whole sequence of changes, right? But, we've, but we've, thrown away that idea by saying, we're gonna remember these different morphology models, right? So we don't, have to do this stepwise, it's just saying, I, I need to know, I'm just, bridging two morphology models. I'm not predicting the second morphology model, like by path integration or by, by these trajectories. I'm just saying this morphology model leads to this morphology model through this behavior. so I only have to be able to predict the next morphology model. I don't have to be able to regenerate it on the fly.

Yeah. And then, so with a local distortion like this on the morphology of the cup, would you just apply like a local morphology model of an indent or so let's, be specific. What, so we got this mug. I now have a mug that looks like this is what you're saying? yeah. Like I I have a mug made of clay that hasn't been dried yet, and I Take my finger and I poke it into the mug and, it indents and I know how to, I know how to predict how it will indent and how it will look after I remove my finger. but now did you fire it? And I have it and I'm using it every day. I'm not going that far. I'm just, I know how to, it feels similar to the t-shirt that it's like you. Probably learn like a temporary model, or it might become permanent, but like you could immediately learn that Okay. At that location. but yeah, it is, in terms of predicting the distortion locally, it's, I guess you have to apply some sort of like physics type behavior model of clay. it feels like this is a, it's, just like a key frame in the behavior model of Yeah. Stapler. So just like you open a stapler and you just left it there, it becomes just like poking the, cup and just leaving that indent and in the cup.

yeah, sorry. So just like the, this distortion is just becoming a stepwise, key frame in the behavior model. which just, so if we have a set of distortion maps, that becomes a behavior because every one of them is the changes that you would do at this. Key frame. Yeah. Okay. Yeah, I'm okay with that. You have a good example there. I have two examples. I thought this is more for grins as opposed to really educational. Here's a mug with some indentations on it. And and I know this mug, this mug is probably probably 50 years old or so. It was made by my neighbor. And, anyway, you can see here we have a bunch of indentations, but I can, remember this because of the face. And, here's another mug made by the same potter many years later, and this one's really distorted, or some sort of, vessel. but I can't really remember this one because the distortions here are on, I'd have to really study this carefully to, to learn all these. But yeah, I guess you pick up on the feet quite quick and at least maybe the. But the rest of it, you ask me like, oh, Jeff, what would, what's the top of the shape, the top, this top lip look like? I would've no idea remembering, but if you asked me, what does the face on this guy look like? I have a very good memory of that, and, she made this just by putting her fingers in that I just thought I'd show it to you, just for gr Yeah, no, that's a, good point that we might just learn, a new key frame for the distortion, and if it's too complex, we wouldn't actually learn it.

Or it would take a lot of time to learn it. Yeah. I guess if I, I still I'm still not, I'm still puzzled, and this might be off the scope of the problems we're talking about, but as the potter is making these cups. He knows how to get the cup into the shape he wants. He knows how to apply these local distortions and how they will change the shape of the cup and how the, how that, those predictions would be made. but yeah, that might be a bit out of the scope. Yeah, that feels very much like the T-shirt, I think, like how to, model clay. All right. anyway, I think, let me see if I can come back and just for my benefit, just re recap what we're saying here. Yeah, we try and write it down too. We have multiple, we're gonna assume multiple ology models. we're able to map, using the compositional hierarchy, we can map, objects onto those different morphologies and they will automatically get to just, if, it's quote a, just if it's like a curved surface, I can just map the logo onto the curved surface. It just happens automatically.

And, then morphology models can be tied together through behavior models. That is, if the two morphology models were, key frames or stopping points in a behavior, then that can make a difference. It means I can predict a new morphology model and I also, I convicted neuro morphology model, but there is no, we don't path integrate deformation. That was the thing you were trying to solve with the, all those, yeah. Displacement factors. so let's not do that. so yeah, just, coming back to the balloon example, if we have the balloon that's inflating, we would basically just learn a couple of morpho, different morphologies of the balloon, like uninflated, slightly inflated, more inflated, fully inflated. And then we would learn a behavior model that tells us how we transitioned through those morphological states. And then if we have certain features on the morphology model, I guess that's the point I'm least clear on, but, it sounds like you're suggesting we could, we can potentially solve that completely with the hierarchy solution the same way we would map the, logo. without, I'm saying that's, a goal, right? That's a goal. I'm not sure I, walked through all the issues, but I, think that's a desirable, and likely outcome that we can use this mechanism. yeah. Okay. Yeah, because I'm a little confused 'cause I, I feel like what we've just described is almost like where we started and this is, but then the reason we went down this path of having these, like what you suggested Viviane and stuff was to enable this prediction with the balloon and stuff like. I don't think we, there was a, we could get it to work with, even if we learned separate key frames, it didn't enable us to generalize, with kind of a known behavior and then mapping like a new logo onto the balloon and stuff like that. Yeah, I think, that's actually, yeah, maybe it now it makes sense. If I can just briefly walk through the, diagram here, 'cause that gets exactly at that, issue, that you mentioned, about the hierarchy solution being very specific to a certain parent child combination.

I guess if you just glance at the diagram difference between this one and this one, the main difference is that the feedback connection is missing. so basically the compositional solution that we have works already with any kind of object distortions to recognize them, like static ones, not the behavior. I'm talking about static distortions, like the, logo on the cup being distorted.

the only issue I was trying to get around is that this is specific to a parent child combination and requires us to learn location by location associations in the backward connections, to make predictions about the child object. 'cause this is like a unique reference frame to the child and a unique effort reference frame to the parent. And so in the forward direction, the morphology model is already quite general. we, can recognize that kind of morphology of the distorted logo, independent of the logo. 'cause we just have the relative orientations in the minicolumns at relative locations. So in the forward direction it is already independent of what the child object ID is, at least to a certain degree. But then. In the backward direction when we make predictions about what to sense on the child object, it is very specific to that child object. so then, okay, but but the way we got around that was to send back movement commands, right?

That, that was for in the context of behaviors. Here I'm just talking about static object distortions, like basically. Okay, so this is just showing the general like hierarchy for compositional object solution where we have orientation of features on the lower level object. So the logo, we have the orientation of features on the logo at certain locations. We have specific features like color at those locations. And then we have the orientation of the logo and the orientation of the cup being calculated together in the thalamus and combined into. The orientation of the logo relative to the cup that activates the minicolumns and the parent object, and then associates those relative orientations at relative locations. And the logo ID is basically just a feature again here. but then didn't we have two backwards projections, right? The purple one you show here is very specific. Yeah. And, that that says on this particular object, there's this particular feature. but what about, but the point of sending back the movement commands was that I could predict a new orientation of a generic feature. I could say there's a different image on the cup and it's, and, I'm gonna predict it's gonna undergo the same orientation changes.

Okay. I'm not sure I ever thought of sending a movement command in the static case. no, there's no behavior model. There's nothing moving. Oh, it's, it is this purely static case. Yeah, just the static case. So actually we have a second backward projection, which is the orientation of the look. but okay. It's, let's take out the movement idea. Don't we tell the child object what orientation it should be at any point in time? Any point in any location we have to, To make the correct prediction. Yeah. In a com, in a compositional object, we, not the way we've thought of it. We tell both the ID on a location basis, but here's the ID and here's the orientation at that location. So I could ignore the id, but I still would have the orientation, right? Yeah. So yeah, that's exactly what I wrote here. So it is also general in applying the child orientation. So we are telling it what the orientation to app, expect for a child object and that's very independent of the child object id. it's just not independent of the location. The location is in the specific objects reference frame. So I was just wondering Yeah. Earlier in the beginning of today's meeting, we already brought up a very big problem or an issue that we have to think about, which is, are, is the reference frame location space, is it a morphology reference frame or is it a specific object reference frame? Yeah. and this would work fine if it was a morphology reference frame. Yeah. And that's why I like that idea so much, like I, or that path to go down that path a bit further of like thinking of morphology objects as very separate from the features themselves. In fact, I think, it was a week ago or a couple week ago, I, blurted out one point in time like, we need to just think about all objects being morphology and that's like the base item. So in this case, I, we don't have, we don't, I don't know how to do this yet, right? I gotta change our mindset 'cause we've been thinking about specific locations and specific objects, but we need to, how can we get this to work? But if we had specific locations on a morphology object to a specific location on a morphology object, that would work, I think that would, at least, that's at least an area to explore and see how, see what happens. yeah. Yeah. And I, really like that idea and I only thought of it at the very end of drawing all of that, like when I wrote the orange. but yeah, I think that's an alternative solution. So basically this solution is very general, like not specific to parent-child combination in all of these mechanisms except for the purple connection, which is specific to a reference frame. And the idea I had to circumvent that was to apply movement to the child reference frame. 'cause a movement command is independent of that specific. Specific reference frame. but the idea of just using like a general morphology reference frame should also work. but by the way, I think we have to have both, you have to have both the specific and the morphology predictions going on. Because a particular mug may only, I know it's gonna have a thousand brains logo on it. that's what I would expect unless I see something else. So it's, and the same thing here. the feed forwards from layer three to layer four says, oh, here's a specific object perhaps on this thing. We, we wanna be able to learn specific associations between specific parents and specific children, but we also wanna be able to learn the morphology, we need to do both. So there, there's still a need for somehow region two, to tell region one. You should be, I'm, on a specific cup, therefore, you, I'm expecting you to have a specific object in R one. I mean it, we don't wanna get rid of that purple line altogether. Maybe it doesn't project to the reference frame. Maybe the problem is we're tying the two reference frames together. and maybe that's not right.

or if it is, it's two morphology reference frames, but then I need some way of actually saying, this is the specific object you should be referring to.

Yeah. Yeah. I guess we all use the same reference frame, but then maybe for which cells are activated in a mini column, that could be conditioned on some kind of specific object ID in layer, I dunno, some of the upper layers. Yeah. I have, this requires some, we don't know how this works yet. I don't know how this works yet, because we want, we don't want to, we, even for a morphology model. I want the point on the morphology model to be unique to the morphology model. It can't be just a mini column. Minicolumns are not unique. There has to be cells. But now, so now I'm gonna use the cells to represent a particular point on a morphology model. How do I then create a, feature specific point on a morphology model? It's I don't have a mechanism for that yet. wouldn't it be enough to have it be a unique location down here and then the minicolumns themselves? the problem is if we don't want a unique location, we want locations that are unique to the morphology model. Yeah. So we just said that's, if we go all, in on morphology. So our cells represent unique need points on the morphology model and the, the unique, orientations on a morpho at different points. We have a unique reference frame for morphology models.

Now how do I add in the, specific features on top of that? Yeah, so I guess what I would propose just off the top of my head would be to use the same reference frame for a specific morphology. Like all of the objects that have that morphology, like all of the circles. Okay. Use the same reference frame down here. They store orientations at locations and you activate the same location representations. But then, depending on what's the print or pattern or colors or whatever on the circles, let's say we have circle of stickers and they have different company logos on them.

depending on which company sticker you have, you might have some kind of specific object ID up here that conditions what cells in the mini column will be activated. but those cells also use same location space. those cells, are those cells representing a unique point in the morphology. If they are, then I can't change them to represent, a red thing. You know what I'm saying? It's like they can't do both. They can't be unique to the morphology and unique to the specific object.

I can think of ways of solving this problem, but I'm not sure if they're biologically realistic. Yeah. Maybe one thought, I dunno if it's helpful, but, just in terms of this kind of unique versus shared morphology model, so it relates to, yeah, something I've been thinking about recently, which is like if you had, let's say two subpopulations in L six, a. And one of them is the actual reference frame, and so it does path integration and all that kind of stuff. And another one is a bit like L four. It's just like a unique pattern for a particular location, but it can be reused. Maybe it's a bit more like L three actually, but it can be reused across different instances. If that's what's sending the back projection to the column below, then that could be reused by multiple objects. And so you could have like maybe unique reference frames for like different mugs and stuff, but it means that on all these different mugs, if you're on the handle, they will always activate the kind of common location fingerprint for being on the handle. And then that's what is I responsible for setting a projection backwards. It's it's almost like a form of location pooling across different morphologies. Or sending the back projection or Justin knows that we need a separate set of cells that represent the specific piece. Is that, this is I guess more getting towards Yeah. Enabling it to generalize, but I agree with your earlier comment that like, we don't want to give up making specific predictions.

so can I, yeah. Let me, bump up a level here, The mechanism that the space, the temple memory of this is, it's really interesting and I'm just stated again, you have a representation in many columns. Just, it doesn't matter what it is, it's some representation by the activation of minicolumns associated with each minicom you have a set of cells, and so by then you can have two of these two representations active at the same time. You can have the mini column representation and then you can have the individual cellular representation and they're tied together. Like ones that you know, because basically I have, you can think of the minicolumns, like a cell, and for that cell I have a number of cells that represent that cell, and I pick one of those. So now I have this level of detail, but I haven't thrown away the model. I just have a no detail. But if we wanna have a morphology model, it almost feels like you have to have another level of this. It's okay, I have a mini column within that mini column of cells, and for each of those cells, I wanna have multiple cells that I could activate individually. Therefore, I have a unique representation of the morphology point, which is a unique representation of, the grid cells.

it's a great mechanism. I just never thought, I haven't thought yet. How could this be extended one more level down? and maybe there's an easy way of doing it. I don't know yet. the minicolumns is an obvious choice. Minicolumns, all the cells are representing the same thing. You pick one done. Now I need like a mini column equivalent for a cell that says, oh, for this cell I have a set of this other cells all represent that cell. And I pick one of them and I, would be done. That's not actually outta the question, that's not a ridiculous suggestion. I'm not saying there's extra minicolumns or something like that, but it's not out of the question that there could be multiple cells, that fire together under a morphology model and then become specific, under a individual detail model.

I can think of some ways that might happen, but I, it's a little, I don't wanna, I just had another idea.

it doesn't completely work all the way through because, I'm, not sure like how realistic it is to work because we already said that we can't rec, represent object ideas in minicolumns. But, I was just thinking that. When we, you even mentioned that earlier when we were talking about feature maps or the t-shirt distorting the neighborhood relationships are preserved, so if we have a logo that wraps around a cup or a logo that's on a t-shirt that crumples up the neighborhood relationships of all the points on the logo are still preserved. So there still seems to be some local structure to the features themselves as well. so basically when we apply different features to a morphology, if we just encode the features in the mini column of the object, I see that there might be the problem that we would have to learn the same kind of, I call it feature map for different morphologies. Like you have to relearn the logo on different morphologies basically, but. Alternative could be that we have some minicolumns that, represent orientations and we store orientations at locations and that those are the morphology models. But then we have other models, and other minicolumns that represent features like color, like blobs, would be that, and those learn relative relationships of features. So it says, all right, in the checkup board pattern, there are the black ones and then there are the white one and black and white and black and white. And it just learns this kind of neighborhood relationship in a probably pretty simplistic 2D, reference frame. And then that's like technically a separate model in a separate reference frame, and you can apply them to each other using hierarchy. I thought I was following what you're saying. Then I got lost a bit. I thought, basically in a nutshell, it would be that. For learning, like learning the, what I call feature models now or learning the features wouldn't happen in the specific cells that are activated in the minicolumns, but they would also be minicolumns. It would be exactly the same mechanism as we use for orientations at locations. so we would have minicolumns presenting specific colors and we learn colors relative to, okay, that could be, I was going, I was thinking a slightly different variation of that, which is, in, if you look at V one in the Tex, you have minicolumns represent orientations. And as you go in some directions, the orientation regularly changes. That's how we think about it. But then you go in another direction, the orientations don't change. The minicolumns seem to all have the same orientation, and so there's actually multiple minicolumns that represent the same orientation. So you could, I'm not, advocating for this, but I'm saying, the first thing that occurred to me is says, maybe on a, on the morphology model, all those sideways minicolumns are active, but then on a specific, feature, location, the subset of something like that, it's just another level of in it, it's another branching out mini You have two, two dimensions of minicom. Another thought that occurred to me that might work, and I haven't thought through it yet, is I wonder if you could use phase of activation to somehow separate out, morphology models, some specific models. I haven't worked that out yet, but it's clear to me that. Phases of cell firings is a big thing in the brain and we don't really take much of advantage of it. and we, it's in the grid cell theories, but, and nowhere else are we thinking about it. And I don't know if there's some way you could say, oh, these cells are in phase and then they become out of phase or something like that. to specify, which specific object. And you could even go through multiple different hypothesis by going through phase, through time. Anyway, those are just two basic ideas. I don't, none of 'em strike me as very fun to think about, but I don't like 'em that much. but it is a little bit like what you were saying, but you're saying something different. You're saying I think have minicolumns that represent the actual features like the color blob, which they are there, the color blobs are there. they're not really, pathology. they could be if it's the color blobs activate. I think when there's an edge of color It's like a college transition. So yeah, I feel like it would be nice if those color blobs and, like similar cells was like what we were talking about a couple weeks ago with like almost like a model free representation, of things like things where we don't necessarily have a way of describing how to break something down into its subparts, but we just say oh yeah, I can see that architecture or I can see that's, that's just like glossy white or that is, all these things that we can have. Yeah. don't, I have the impression that all the retinal ganglion cells, the ones that are color, are not color, are all, center surround, edge detector type of things. so you would never have just red, you a cell would have to representing a transition from red to something. They're like edge detectors. Like I we're gonna almost outta time here, but I wanna leave on a sort of a positive thinking thought just to get it. It feels good to say, oh, let's think everything is about morphology models. And then, and composition is about morphology. Models and behaviors are about morphology models and deformations are about the morphology models. And then we have to ask, okay, how do I make these specific predictions? How do I get a specific, particular, thing?

yeah, I think it a good suggestion to think about the egg and the oval and without any features on it because yeah, I think we can actually solve. All of our open issues with behaviors if we just think about morphology. But now we have to think about how to separate the features out from the morphology model. And how to bring them back in for the predictions. The iron is, we started the other way around and we had to figure out how to add morphology and it's oh crap, this is so hard. And then now we say, oh, let's start with mythology. And you have to figure out add back features, which maybe won't be so hard.

What you describe is like a three streams in, cortical column. And it's one morphology, one feature, and one behavioral. remember, like you won't have a presentation about behavior, but in through two kind of parallel streams, but now it'll be three. We have features separate. Yeah. And that's actually what we talked about. I think at a, a retreat two, two years ago, we were like the three models, morphology, features and behavior. And then I guess, wow, I don't remember all, of my memory is temporary. Nothing's permanent anymore, so I can't remember the conversation at all.

okay, cool. I think it's a, liberating idea. Whether it's right or not, I don't know. But it's always fun to have ideas that at first feel liberating.

because it does feel like morphology is the basis of behaviors and morphology is the basis of defamations and morphology is the basis of recognition in general.

almost everything we recognize, we start with morphology and, and then we say, okay, what are the, how the, how do we add back in these specific features to make unique versions of the morphology?