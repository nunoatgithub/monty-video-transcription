Okay, I'm just gonna get started then. So this, I guess is like a kind of progress update sorts, for the stuff I've been doing over the last four or five months now. and it started off working on temporal memory, getting acquainted with the algorithm, reproducing it, potentially trying to make it faster. and this was like a sub project of sorts, to be done with temporal. it was sample in varying object recognition. So I'll try to go through this quickly. There's quite a bit to go through. I'll go over the problem that I was trying to address, the kind of data that I was working with, the algorithm I was using and the results, and then potentially where to move from there. Okay. So the main question I guess we want to ask is, can I use a single sheet of temporal memory? So exactly one column, one level of the hierarchy, and just one layer for some kind of sample in varying object recognition. And if I phrase that differently, I can ask the question. if I revisit part of an object I've never seen before, can I identify what object it is? No matter where I. And here. When you say one column, you mean one cortical column? Yes, one column. When you say one layer, are you using any concept of, locations in there? No. it's, there's no layer dedicated for locations, but there's a location signal. Yeah. Okay. Okay. Similar. We don't have a grid cell layer. Got it. like we did columns. Exactly. Yeah. Layer means there's no input and output there. One layer means there is an input and there's no real, there's no output going to another layer. There's no fool either because there's no object id layer of any sort. And, this is really just to focus on just answering this little question for a while, but before we put it into a fuller, our system, how does it differ than the paper? before. So this idea, like this continuous sheet? Yeah, And we're not changing orientation? No, not yet. Okay, great. So basically, can I disambiguate between multiple objects that I've trained on while observing some kind of features at locations. Along the object surface. and the, question is, why is this important? and at the risk of grossly oversimplifying what deep learning is doing in this field, I'd state that they need to ingest a lot of information about any kind of 3D object in order to potentially disambiguate between multiple objects, or cognize an object despite, occlusions. and we saw example the vector neurons. Okay, so the kind of data that I'm working with, is the YCB mesh dataset. and basically you can think of it as every object is its own individual point cloud. and the point clouds kind of range between anywhere from 7,000 to 12,000 points describing that object's morphology. so it's, quite a reasonable number of points. So the feature that we're looking at, is basically the Gaussian curvature value for every single one of these points. you can think of this as like the proximal input to temporal memory and the context, which we can think of as like the basal input is a location in a fixed object centric reference frame. this reference frame doesn't change for any of the objects. So just wanna quickly address this, like why are we using point clouds? Why not anything a little bit more complicated? and the reason for that is it's quite a simple data format for your quick and easy prototyping. but there are plenty of caveats. the first is that pre-processing is quite expensive just 'cause there's, thousands of data points. and in my case, since I'm using, temporal memory, converting between point clouds to SDRs is quite tricky. you are trying to create this kind of continuous set of SDRs, where there's a continuous overlap between points, from inherently discontinuous point cloud points. Why do you have to process all the, data points? The whole point is to do a sub sampling of the data points and see if you can different sub sample, right? But in order to, in order. You're right. I only, I'm only gonna pick a part of it, but I, still need to pre-process everything. You did it. Alright. But that wouldn't be an inherent part of the problem. That's just the way you did it. Yeah. for testing for instance, I wouldn't need to pre-process everything because I'm, randomly picking Yeah. You randomly picking ones that learn on you randomly picking ones to, to infer. So you wouldn't need to all 7,000 points, you just not at any given one time, but yeah. Yeah. the other challenge is re yielding some kind of reasonable overlap between neighboring points, whether that's in the coordinates, whether that's in the curvatures and this kind of process is really done through some kind of trial and error. and there's also the problem of precision. The point clouds a real value, but SDRs are quite discreet, so everything needs to basically be casted as integers, before I even proceed. So there's a lot of, bit of in, there's a little bit of information that's being lost. Okay. I'm gonna do a very high level overview of this, simply because I don't want to spend too much time on this, but this is like the strategy for, encoding, any of the points, the, coordinates into SDRs. So the first step would basically be to scale all the coordinates for a given object. I, I think this is like a pair of binoculars or a Lego piece. I can't even tell. It's not bin binoculars. Yeah. Okay. I dunno what it's, but, Either way, for all the objects that you're using, you're gonna scale the coordinates to some kind of, similar range. and you're gonna convert all the floating point values to, and if you guys can see the, purple point right over here, this is like a sample point that I want to convert into an SDR. And how I would do so is define some kind of fixed sphere around that point. and then basically try to compute all the neighbors, in that sphere. and when I'm computing SDR, I'm randomly but deterministically picking some of those pink neighbors that you see and I'm hashing their values to some buckets or, indices in some SDR. and I set those buckets is on and I set the rest is off. Similar to the coordinator. This is coordinator. Yeah, just the visual representation of the coordinator. This is a good explanation. but yeah, you, would think of, doing this for basically all the points that I have to get individual year as well. Does that then, so, just the, for the kind of, metric of nearness then of nearby points. Is it then a discreet effect as you get within that cloud? Or do you then, almo have more of a tin continuous effect if you were to sample a random point? Depending on its nearness to another point. Is there a gradual, increase in the overlap of their SDRs? Yeah, so nearness is defined by whatever's in the sphere or not. so, it's more of a binary effect. Yeah. But as you move that point, like I guess if you move that sphere down by, by picking different points at the center of that sphere, you do still get that gradual, overlap between neighbors that you're moving between, if that makes sense. So even though it is binary, like whether I'm in the sphere or I'm not in the sphere, you do still get that gradual overlap if you move that sphere down through Yeah, through the effect of multiple spheres, essentially. Yeah. Yeah. Okay. So that was for coordinates. You can do the exact same thing for Curvas. you would just scale all your curvature values to some same range, and you would follow the same neighborhood calculation and hashing procedure. Sorry. So for the location, does that mean you're only storing like local location? So you wouldn't tell for the small cloud of points that you're hashing. You get some similar similarity for those, but not for slightly more distant points. Yes. You wouldn't get similarity for more distant points. Yeah. But the coordinate encoder of bays is very simple principle. And that is, if I have this sphere right here and I pick an even larger sphere, all the points contained in the larger sphere should be in this smaller sphere when I've hashed into an SDR, there should be overlap. That's just a, simple principle that you can test supply. yeah, you can do the same thing for curvatures. but there's an inherent problem with the way we're doing this, and that's just, there's too many points. a full point cloud, maybe 10,000 points. It takes about six minutes just to process with a single sheet of temporal memory. So the whole idea here is can we reduce the number of train points that we have? and one very quick and dirty solution is just group the coordinates and the curvatures into some kind of clusters. and I pick basically the proximate mean of every single cluster. in other words, yielding a finite set of the most commonly occurring features and locations. and I would only use those points to train. In other words, if I have a 10,000 point cloud, I'm re, I'm just reducing it to end points for training where n is the number of clusters that I've generated through my basically K means method. So this is just a simple visualization of what this clustering even looks like. if I cluster by locations, basically clustering groups, the clusters are grouped by similar coordinates, you would get this, this center image here. See if. and just by looking at it, you can tell like the purple points, which are the training points. It's uniformly distributed along the object surface, and that's exactly what we want, right? if I, was only clustering by locations, that is something I would see. and if I was only clustering by the curvatures, this is something I would see, where I'm only gonna pick the most commonly occurring curvatures in my point cloud. there's a problem with both of those methods, and the problem is that in, in the first case, there's no guarantee in the overlap in the curvature SDRs if I'm only looking at locations and vice versa. if I was only clustering by the curvatures, there's no guarantee in the overlap, for the location SDRs. So a reasonable middle ground, that I found worked well is to do both. And that's the image that you guys see here all the way on the left. and that's basically just, it's clustering by both. and there's some guarantee, but lot for both locations and curvatures. When you cluster by location, how do you make sure that they still end up on the surface of the object? The centers of the clusters. so when I'm picking the, when I'm picking the approximate mean of the, of each cluster, that object, that point exists in the point cloud. So you're picking basically the point in the point cloud that's closest to whatever computed mean of each cluster is. So there's always a guarantee that all the cluster points are, on the surface of the object. Okay. Jeff, you had a question? I was just trying to, you're training by now you've got the set of points that you're gonna train on and you're just gonna train them individually. Whereas in a, if I was touching this object, for example, with a finger, I would be moving continuously along the surface. I would be a much better learning strategy. and and it just strikes me as that would be an important point of keeping. you would have sort largely, it just would, it seems like that would make a difference. It's, I can trouble expressing it. Is that a concern here at all? Or, because you're just like, basically this should be like, I'm just touching my finger randomly different points around the object, as opposed to running my finger of the object. Which is, seems to me if we're trying to learn continuous representations using temple memory, you would wanna do it that way. Yeah. That's absolutely right. I could have done it that way. this was just easier, I guess to, to experiment with. That would probably be a next step. I don't know what kind of results you're gonna get, but if you're not gonna get good results, then the first thing I'd say, that's not how I would've learned. In some sense, you almost guarantee the continuity if you move your, if you move your sensorimotor con continuously on surface. Yeah. whether it's eyes or tuck fingers that, alright. Just that in get to the results in that thought experiment that you posed. I would technically the best learning strategy for, I don't know, learning what this bottle here is, not only do I need to move in a continuous fashion with my finger, but I also need to experience all the curvatures, most commonly occurring curvatures on the object. Such that if I, don't know if I touched a different point that I've never touched before. I know exactly what this object is. you would, if you, may not know exactly what it is because it depends on where you had touched it, right? you might have only touched the front side, and then now you're touching the backside. just careful. Okay. It is, it's gonna taint my, when I look at your results, I'm gonna have to keep that in mind. Hey, maybe this strategy of learning isn't the right one. If I think about how to create this sort of continuous, you want to, in some sense create a continuous representation of the locations and curvatures, and that seems inherently the way you're gonna do that is gonna be important to keep in mind that you're doing this continuously. I can give some examples why doing it, jumping around would not work. Okay. Okay, so this is just, just a bit of, I guess data about the data. so the, curvatures that I'm using the SDRs are about a thousand bits. and about 11 bits are on, the coordinates, which is my basal input and my AP input, which is the. Random but deterministic object. Object ID for every single object. They're both about 2000 bits, and 30 doing representations. yeah. Where I would imagine if I think of curvature as the feature in some sense, like that's the, the, output of the spatial pooler. It's not dense, it's not sparse like that. it's what's denser. just that, yeah. The reason it had to be this sparse was because, I was scaling the curvature values, the raw curvature values. Into the exact same range as my coordinates so that the clustering would work. Okay. and that kind of affected how, the, that's another thing I'm gonna put in the back of my mind, that's not quite right. Yeah. when we first did the temple Pooler. In fact, when we always did, we always had these fairly sparse, outputs of the spatial pooler. But we know biologically that's not right. If you about the minicolumns, it can work, but it's not that sparse. It's 20 to 30 synopsis, right? it wouldn't, I'm just saying we use, we'd like, we would say, oh, there's a, we have 2000 minicolumns of which, but in reality is 40, 40 on at a time. Yeah. Reality. it's more like we look at the, you look at the distribution of like view one orientation columns. It's much less, it's months less sparse, but just keep going. Okay. yeah, I mean there's about 1,024, minicolumns here and there's about five cells per column. and the activation threshold is. Jump into the algorithm. The algorithm is very straightforward. the, biggest challenge in this whole, sub project was, getting the data correct. but the, algorithm is that for every single object that I'm looking at. And for all the coordinates and the curvatures that I've considered in my clusters, I would pass in the curvatures as proximal input, the coordinates as basal input, and the apical input gets the deterministic, object id. and during training, I'm storing the winter cells, for each observation, as I'm iterating. I In terms of the numbers? Yeah. we didn't have the apical. Oh, AP did. I think it's maybe closer to the columns paper. the coordinates coming in and then the Yeah. But in terms of the numbers and, numbers are very similar. Yeah. Yeah. It's a little smaller because he was running into speed issues. It's close to college. My, I'm thinking like, oh, these are the kind of numbers we had. Yeah. In the, columns one, there's an explicit, object ID letter that feeds back the correct. This is like an approximation. There's no, which means I'm like, I'm manually picking that. Okay. Inference, is pretty much the exact same thing. I pass in the curvatures, as proximal input in the coordinate as to basal input. and the only thing is I'm holding object id. I withhold that during testing. and the testing set notably could be either one of two things. It could be a uniformly sample subset of the remaining point cloud, or it could be an occluded subset of the object and I'll showcases where, both work. but the goal here is that during testing, I want to store the predicted active cells. and after I, I iterate through all the coordinates and curvatures and I iterate through all the objects. I'd want to compute the overlap with the winter cells that I stored during training. and I, the next slide I'll show you guys, a visualization of how computing this overlap looks like, but are there any questions here? this is very likes, very naive way of, of doing inference, but it's more like a. Me check making sure that one sheet works in this fashion. your goal is to test the algorithm as we published it and then it seems pretty straightforward, sense unless missing, so this is where the bulk of the information is. Try to go through this one step at a time. So the reason this figure exists is because, assume that I've, I've trained on five different objects. that's object five here, object 6, 7, 8, 9, and 10. and can is it too small? No. Starting with five is interesting. You, yeah, I just, somewhere in five. but the, goal here is that. I'm inferring on object seven. So I'm, seeing object seven. I have no idea what it is. And the only way I can predict what object it could be is by comparing against previous trading data. So if you look at the X axis for each of these, black columns here, the X axis kind of store, shows you the winner cells collected during each training observations. So there are about 50 training observations here, and each of them I've stored a set of winter cells. and on the Y axis I'm basically looking at my, predicted active cells that I collected during each inference. Observation for this object seven. again, I have no idea what that object is, but the only way I can predict what that could be. Is by computing overlap between the winter cells and the predicted active cells. And that's what these, these dots are inside each of these block columns. It's just the amount of overlap if it's above some certain brush rate. So if you look at, let's say the training observations collected for object seven, and I'm comparing them against the, the predicted Actis cells for inference for object seven, I noticed that I have the most overlap compared to all these other objects. so you're basically doing, you're trying to pick the, the object that has the most, the, highest total overlap. When you're comparing the predictive selves to the winter cells. That's what you see here. the numbers at the top they just described. It's just a sum. It's just what the total overlap is. this is pretty sure that the algorithm thinks with the, with reasonable confidence that what I'm seeing right now is, so you're adding these up, but if I had a typical pooling layer on top of this, I don't have to add anything. It just does it automatically. Yeah, it does it automatically. So it's a complex step, but in reality, neuro would be simple. Yeah. You're, showing it. Okay. is this right at that object? Yes. Yes. yeah, unknown object seven, but it did predict seven, so that's good. it had a total overlap of about 74. I don't know. The next closest prediction was it could be object six, which is, I don't know, had a total, overlap of 26. So this is overall like. How many observations? So 50 training observations, and then 50, testing observations. Sorry, 50 training, 50 testing uhhuh. Okay. So you might have been able to determine that earlier, right? I couldn't, yeah. you just ran the whole limit. And then 29. So one issue we have here, which we didn't have in the problem figure, is many of the inference points won't match anything. because you're looking at different Yeah. Points you have in store. Many of them, despite all the stuff we did, many of them actually don't match at all, so enough match to distinguish it, but, so I think it's still an issue here. They should be, if you look at object seven, yeah. 1, 2, 3, 4, 5, 6, 7, 8 points actually matched. Is that right? Out of 50. Yeah. Yeah. So then, of 42 of them didn't match. I, it's saying, if I haven't touched, if I haven't learned the whole object and now I'm touching on it and I'm touching points, I have learned, I can still recognize the object. Yeah. But if we like, if we can recognize it with one grasp Yeah. we have to have a much higher grade, I think. I'm not sure we're really solving the continuous. Okay. That tested. yeah. inference is just predicting the object with the highest total overlap between what I collected during testing and what I collected during training. It's a very simplistic approach, unlike the columns paper, which has that explicit object ID layer. And the next steps would obviously be to use multiple layers, possibly multiple, cortical columns, or maybe as supervisor suggested, we might wanna really adjust continuous representation as well. I don't think those things. Touching in one. It's a little fuzzy in my mind right now, but the idea that you have very points and very curvatures point, and somehow we have to be able to interpret between, you're saying we're not doing that only radiation threshold in the sense addresses. If you say you need to sample like 50 points right. In training and then yes, it's shown. It's almost. And if you do that, as long as you pick enough in your tests, let's say 25, no matter which, 25 enough. I think it's okay that many of your sensations do not match anything. What's important is that you eliminate what you do sense eliminates the possibilities, narrows down the choices. Yeah. we'll probably have to change the voting a little bit because nothing matches. It's not necessarily a bad thing, right? Not yet. Not yet. We basically need to loosen up a few things. Yeah, I, it's funny, I have to think of it. Take some time to think. Okay, we're getting close to the results here. Yeah, I'm excited. Okay, so this is what the objects, oh, I didn't even know One slide away. One slide away. Yeah. Okay. So this is what the, is really to see. So this is what, the training, the training and testing objects could look like. so I've just shown three different objects here, and I've showed you the two ways I could generate the testing samples from it. In the top row you could, it's very hard to see the purple points, but the purple points are the training points and the green points are the testing points. So it's a uniform, random sub sample and then whatever remains outside of my training group, that is my testing group. In the bottom row, it's a lot easier to tell because I have these broken points and those are still my trading points. But the green ones are occlusions, so it's basically like car's example where you take a sheet, and where the green ones are occlusions or the green ones where you stand on the greens. The green ones are. Those are for, testing? Yeah. But is that where I'm sampling testing or is that like the part I can't see that is where you're testing. Okay. So that's the part. You can see that's not the inclusion, this is an included object from the green point to where you can test. Correct? Correct. Like it's 90. Yeah. So the same thing. You can only touch this part. This right here. any more pointing, the algorithm slows down, so I have to look. you could, no. You could have a bigger, you could have fewer green points, but distribute large. Yeah, you could say 25. And then, and I'm thinking about doing that. This is a pretty difficult pass, right? you're basically, you, as you said, including 90 of the object, We'll see how it works. That's a hard standard to meet, I think. Okay, so the first test is not the occlusions, it's the uniform, random subs from the testing set. And I'm still using the 13 training objects. sorry, I'm using 13 training objects, and the goal of this exercise before I even jump into this graph is there a sweet spot for the number of training points that I need? Such that I reach a really high testing accuracy with a limited number of testing samples. That's the question I want answered. so the graph here shows you, that test. So all these different lines, they're color coded, they basically describe the same experiment run with different, a different number of trading points or clusters. 20, 50, a hundred, 150, and 200. Why did it go down for hundred 50, 200? Why did it go down? Doesn't look worse. Oh, the, testing accuracy on here, on the side. I'm saying where, like, where's it, where's the, one 50 line is lower than the green line, the hundred line. Is that right? Am I reading that right? Yes, correct. And and, 200 lines is lower than the green, the hundred line. So why would it get worse? Okay. Yeah, so that's the, question here. Okay. I'm reading it correctly. Let's put that one. Alright, keep going. So the testing accuracy is on the Y axis and the number of test samples that I'm using is on the X axis. So if you notice something, if you look at the orange and the green lines, which describe 50 and a hundred training clusters respectively. The green line is up here and the orange line is right below there. you can see that after about 50 testing samples, it actually reaches a hundred percent accuracy. no matter how many testing samples I use after that. But if I used a little bit more training examples, like 150 or 200, it performs slightly worse. And the reason for that is exactly the, intuition behind asking this question, and that is there is indeed a sweet spot for the number of training clusters I need. Any more than 50 or a hundred in this case just introduces noise into the problem. it's not clear why it introduces noise. These are just additional training points. Why would make it worse? 'cause you're looking at, you're looking at new, are your radius is getting smaller. Yeah. Your radius is Oh. Oh, okay. that sure. Now if I add more training points, I'm Now you have to hit them. Yeah. That's interesting, but that, that, that radius is in the clustering algorithm. It's you're over clustering your training points. Yeah. But you could have overlapping clusters. Yeah. Why, would you? Yeah. you could still have a fixed radius. Just allow the clusters to overlap. Yeah. That's what I thought you would've done. That explains this result. The more you adding now these things getting harder to hit 'em. Yeah. Okay. I didn't visualize the clusters using like tSNE or something like that. I, could have done that, but it's okay. It's alright. That explains that result. Yeah. How many up Big 13. I would imagine if you kept the radius the same size, then you would've had similar results or better Possibly. Yeah. At least in this limited experiment, the conclusion that we could draw is if you had maybe about 50 to a hundred training points, and you use greater than 50 testing points greater than or equal to 50 testing points, you get a hundred percent accurate. so that is all 13 objects are correctly classified. yeah, In some sense, the number of training points you, if we wanna do a continuous representation. You, the more training you do, it shouldn't lead too. It, at some point it should stop adding memory. It should stop taking more memory to do it. and it's just intuitively you wouldn't get, now radius, this here, it's just a point. At some point you, you, since you clusters are overlapping, training a new point doesn't really change anything. You've already learned that space. So that would be like the first thing to do, it seems to me is like try getting away from that narrow radius here. I think if I had, if I had fixed the radiuses, we probably wouldn't see this. Plus in addition, you wouldn't see this. Yeah. Yeah. It would be good no matter how many trading points I used. Yeah. but the other thing is that the algorithm needs to have that addition where it doesn't grow segments beyond a certain number of. Maximum number of seconds. Because right now I'm just, I'm continually just adding more and more. And that's why you're not really doing continuous learning. Yeah. you're not doing a continuous representation because it needs to, if I'm learning the shape of something and I keep moving over, I'm not, at some point you just don't learn anything. Yeah. Yeah. And in my case, I am. Which is all good. Which is all good. Okay. Okay. we understand that. So this is an example, of where it works. and this is the same example as, before. I have 50 training points and 50 testing points. and this is the, object that I'm using, and this is like a, drill bit or a drill. And if you look, I'm just randomly sub sampling my, space for my testing points. And those are in green. And the other ones, the purple points are the, the training points. And here, over here you can see that it pretty confidently predicts that, object date, which is this object is indeed object date. there's really not much journal overlap anywhere else. yeah, that's the one highlighted in red. I appreciate this is more specific to this situation where it's the objects artificially, fixed in an external reference frame. But I'm just curious, do you have a sense for how diagnostic the location information itself, might be in this case? Just in the sense that, there's probably certain regions of the 3D volume that are only sampled by, certain objects and. That's a Yeah, that's a great question. and I didn't include them in the slides because it didn't work so well. and if you go all the way back, what, what didn't work? I'll show that in the, as in just using the location information. So you're only clustering by locations basically. I did try this. so in this as in, if your feature essentially is the location, I guess is what I mean. Yeah, as in, if, you were just to look at the overlap of the feature SDRs with, at testing and training and kind of some across those, if you would get like how many hits you would get and if that would be enough alone to dis invigorate the objects. Okay. So no basal memory without taking into account curvature. Yeah, exactly. Ba basically just proximal input. I imagine it wouldn't be very accurate. I'm just gonna in, yeah, I was just interested in this. yeah, it might be, there's still probably a pretty substantial intersection between different objects, so you'd have to, but you, might get some locations that are unique to an object and then, then you'd go run. I don't know if, I don't know if there'd be a lot of substantial overlap. it's like the, two planes of the objects have to be right at the same point, I'd say. Oh, because they're on the surface. It's not like they're occupied volumes. It's the surface points. It's the surface which be pretty low. How big is the radius compared the width of the object? it's 15 of the maximum range of the object. So the range here is zero to 100. The radius is about 15. Oh, that's good. Big. So you would a fair amount of all that? Yeah, I think, no's a good one. I think that would, you would be able to recognize object just by that. Yeah, that is a good point. If we think about this as an exercise, entirely learned continuous representations, then we can dismiss some of these issues. 'cause they're, 'cause it, we have, we're assuming we know the locations here. Of course you have to infer that, right? We don't know that. We assume we know the orientation, but we have to infer that. So all these things we've talked about extensively have to be done. So this is a subset exercise. and as you pointed out about continuous representations in the temple memory. So as long we focus on that and then, we can put aside things like Neil just said, yes, that's, we could be using it, but that's not the, we're not trying to create a system here that's a practical system. We're trying to exercise point a part of it now. I think that's correct. Yes. Okay, this is a case where it didn't work and that's if I used too little testing points. So I still have the same 50 training points, the same 13 objects, and I guess the object here is a cut. but I only use 20 testing points. And here there's quite a bit of ambiguity about which object it could be. it could very well the object 10. It doesn't look like that. It looks like the object 10 that jumps right out. Least ambiguous. So it's, it is, it has a higher threshold than all these other ones, but it's incorrect. Okay. So it's the sake, it's not just ambiguous. Yeah, it's, I guess it's both because it thinks, if you ignore object 10, it could very well be object 6, 7, 9, or 12. They all have about the same. Yeah. But 10 is wrong. 10 is wrong. Yeah. It should be nine. and what. What is object 10? That, that, I have no idea. I didn't check. Okay. I think it's, I think it's a dice. it's a deforms dice. It entirely depends on what testing samples that picked here though. it should be entirely dependen. It should also depend on what the shape of the object is. The, so the, it just so happens that the testing samples here most closely match with the training examples of what I collected during I, I understand, but it's not, it there is, it's not just. Random which ones you sampled. It's also there's the shape of the object really makes a difference, right? Yeah. Should, I mean you could, I don't know what the dice looks like. I, the dial looks are, I have no idea, but you could imagine if I had a cube in this space a lot. Those points would be, similar. It's both. Because what you're describing is parameters by whatever I trained on what recollecting during training, the winner cell is parameterize, what the morphology of the nonfiction look like. I think if we introduce continuously changing locations and, curvatures that you wouldn't, you shouldn't get that this error. Okay. this is the second, subset of testing. And this is not doing that uniform random sampling, but this is looking at random occlusions within the testing set. here, and I didn't vary the number of training points that I, needed to use. I used the same as before, so 50 training points and same 13 objects. And the question I want answered here is, how many testing points do I need to get a high enough testing accuracy? And over here on the, plot, you can see that, I've used quite a, reasonable number of test examples for these occlusions, anywhere from 200 to 800. And what I find is that after about, I dunno, 500 testing points, you get about a hundred percent accuracy. that's all the 500 in this small, yeah, in that small, I'm surprised you can't even get a hundred percent accuracy given how smaller, area you're, referring over. That's, I think it's because I know exactly what curvatures and where the locations are for those curvatures. if I gave you a, in that there's a cereal box in that data set and there's a di dye in that cereal box and you're just touching on so little surface of it, you wouldn't be able to tell matter how much you touch. I guess again, you, unless you're doing what Neil did set and it the exact location, that is why Yeah, that is why. Yeah. Yeah. So I guess this result seems I think it makes sense to me, but that's because I know that. The locations are, they're completely fixed. Yeah. it's a, global coordinate system for all the objects. and it's a direct, I can do a, one-to-one direct comparison between objects because it's in the exact same coordinate system. Yeah. So again, these, all the results have to be interpreted with those limitations. Yeah. Everything should definitely be taken with a grain of salt. I wouldn't say grain of salt with those interpretations of mind. it's not random. It's okay, given the way you set up the problem, these are the issues that are gonna happen. But in this case, this is like a, nice visualization of why it works, this is because it's a screwdriver, it's a weirdly deformed screwdriver. Yeah. Yeah. Real screw. but the green points are the ones that I can see. and the Purple Points are the ones that I've trained on. and if you can see here, this is object 11, and it is correctly predicted as object 11 with a huge margin. but you also did a lot of samples. I did. There is about, 600 samples here, 600 testing samples, but here it doesn't seem like you need 600. I probably don't in this case. that, that plot I showed earlier was for all 13. I mean at least 600 or at least 500, this is another case of where it works. This is back to that drill. yeah, seeing object data correctly knows its object data and it does. So with the large margin, in some sense, seeing so many testing points is similar to running your finger along that green portion of the object because you get lots and lots of samples. it is, but yeah. 'cause they're so close together. Yeah. It's not continuous, but they're so close together. Lifting finger, I guess that's here. But if you were to just slide it along, you'd get probably dozens to hundreds of points. Even if, I represented that continuous movement for the point cloud, I think it, it implicitly means I am picking it up and touching it. There's no like continuous. yeah. Okay, this is a case where it didn't work. I have, no idea what this object is. this might be a logo piece. I have no idea what a bag of marbles. Is this? I think that was something else. I don't think you know what Lego look like. Testing points. Six. So many things going on here. Very, small part your testing, but you are knowing these exact locations again. yeah. and here there is some ambiguity, whether this is Object two or Object 11. It, should be object 11, but it's predicting it as, or no, it should be object two, but it's predicting it as 11. Yeah. Or as not certain. Maybe you look at that, it's not. Yeah. If I had a threshold for my prediction, then yeah. That's a reason result given the small testing, but also all these things we've about means. Okay. Open questions, next steps. the first thing I want to address is, I guess like the downsides of point Cloud. So that's something I went over in the beginning and I want to reiterate it here. And that is, it's very expensive to process, and it's very expensive to distill information into saline patches. along the surface of, any object. and there's this data complexity problem. I don't need like thousands of points to describe exactly what it's, this is, you're talking about the data set here, right? Yeah. Yeah. This is right. This is not point. Cloud is not really part of the solution. It's just part of your dataset, right? Is Yes. Yeah. so this is like a knock against that particular kind of data set. and where we, I don't know where we, I don't know, might want to move forward, for a more realistic training structure, but it, in the real world. We don't start with data point clouds, right? In the real world, we have objects that we're sensing and there aren't point clouds. So in some sense, having a point cloud is not a, it's just a fact of the way we're, the data we're working and Getting rid of it is it's not an, it's not a, an important thing other than this practical problem for, in the end, if you have a real object, you're sensing it. You're not starting with a point. Yeah. You're just doing a point clause, because that's what the, YCB data set gives you. some data y icp I'm saying in real world, let's say I had some sensorimotor. Even on this, you could start with this point cloud, just put aside the pre-processing because that's irrelevant. You're, when you're sampling, you could be calculating those as you sample, right? Couldn't you, be calculating, not point cloud, but could you be calculating the SDRs, you could be doing it on the fly, but the reason I did it separately was because it is still very expensive to do a 100. It's still quite expensive to do it, but you're doing thousands of it upfront. I'm doing thousands front, but if I just picked one individually, I'm still saying that the process for generating an SDR is still great. Expensive. Yeah, but you're only doing 50 of 'em, but it's 50, like the perfect points, basically. 50 versus thousands. Wouldn't it be less expensive? Imagine like a more realistic training scenario that you pointed out, right? Yeah, where I'm, running my finger across that object. I don't know if I could realistically reproduce the same data pre processing for that more complicated experiment. It would be way too expensive to do that. I'm not sure I'll take the word for it. And what I'm trying to propose, is that we should probably move away from point clouds To what? To, to more realistic 3D models of objects. Oh. Closest. The, the what? the fastest the, what do you call trim mesh. Like the mesh. Either that or I guess like a, I don't know, proper 3D render object. And the features, I dunno what a proper 3D render object. Any other options? I know a couple weeks ago Ben was working on a 3D surface reconstruction, so Ben, what the, what was the representation you ended up with there? Was it like a surface three? yeah. It was an implicit function that returns a value greater than one, or, sorry, greater than zero when you're outside of the object, and then less than zero if you're inside the object and zero when you're on the object surface. Let's be clear, Ben, you've been argued for a parameterized models of objects. Internal to the system. And here were just talking about what is the model of the object external to the system that we're using to test it. So I just make sure that we're not mixing those two up. I guess it doesn't really matter. Good distinction. okay. In some sense, Monty has to work with all of these different data types. at some point, whether it's, point clouds are like lidar data you might get, or even if you were to touch an object multiple times like this, you would get, it's like point clouds with curvature features. It's not that far, but he's, processing all these 10,000 points up front. Yeah, no, that's a diff that's a pre-processing issue. But in terms of the data format, I think Monty has to work with point clouds. And it has or points. Points, or it has to work with 3D images. It has to work with audio data. Yeah. It has to work with all of these lidar data. At some level, but that's why. But if you're training on 50 samples or you're moving your finger, some, it's not, you shouldn't be doing 10,000 of these things to, alright. This is like a, an engineering problem, not a. Scientific problem. Someone has a question. Niels, was that Niels? Yeah. sorry, I was just gonna Yeah. Quickly say, yeah, it seems like the kind of hierarchy as well that Koran was talking about also last week was, would definitely be helpful in terms of just if you can quickly develop a representation that you are at least looking at a cylinder or a sphere in that kind of fairly, or like a reasonably broad region. And that would quickly narrow down the list of, objects you need to, and, would be a, less weird Yeah. Representation than even, though you start with point, a point cloud. but I appreciate, yeah. Is very nontrivial. you're right, and, I wasn't, I didn't hear what presented last week, but we don't wanna rely on that right up front. We wanna, understand the basic mechanisms here because, a cylinder is just another object, right? We have to limit the number of objects we can learn. we have to limit the number objects before use hierarchy, but a single level needs to solve problems and it needs to learn continuously. It needs to form continuous representations. we can introduce hierarchy later, but I think we have to get the basic mechanism working in a single column. Be my argument. Create continuous representation. Something I've been thinking about this whole meeting is we have discre, if we have binary representations. his, radius is, that binary? Not binary? No, But the rep mean, ultimately the representation that we have in memory is basically SDR. Yeah. But that's not binary. It's set of components. Each one is binary, but Right. But the SD is not binary. It's discre. you only have fin many, possible representation. there's, it's, almost infinite, right? The number of representation you have within SDR is astronomical, right? So it's not limited. I guess the trick is you want representations that are near each other to overlap significantly, right? The reputational capacity of an SDR is. So it's the, check, because you don't wanna have a lot of the speed. SDR, you wanna have overlap. S I'm, sampling as I move on surface, I don't want this point here. Be unique. Then you end up with a sense, understanding your point. I'm mistaken. I just as like with SDRs, you have two to the power of the dimensional. That's, the maximum number of unique SDRs. You, so how do you go to continuous representations? the things, there's just so many, right? The computer, right? There's just so many of these, Right. The actual number, the number of ways you can represent, Sparse is huge. If you have the total large, it's national. There's no representational problem. The problem is making sure that points that are near each other, are similar, have similar overlapping. That's the problem. That's the challenge. it's not a capacity problem. It's we don't. I getting back to earlier where both the sdr for the curvature and s for the location for highly sparse, which means in generally, that would be difficult to have. Yeah. I guess we briefly addressed hierarchy, but we dunno if it's needed or not. You said, it's gonna be needed, but I just don't think we should focus on it until we solve the problems with a single column. That would be my recommendation. Okay. We, we have to solve the problems of, hey, you don't know the location and B, There has to be an overlap for these SDRs and we don't wanna rely just on, we don't wanna rely on location to do these things. We wanna learn, in a way that if I move my, as if I'm sampling continuously, I think that really makes a difference than if you do random points. okay. I think we have to think about it more. Ben started this great building brainstorm doc, and I've been contributing to it. It too. there's a lot of unanswered questions there. I dunno if anyone wants to take a look. Whatever. yeah, that's, separate. The other, I guess next step is, can we move away from this fixed object centric coordinates that I've designed in my system, we should be able to. Plausibly using wind cell codes that would enable the sort of, actually, I don't know if it, I don't know if it enables scaling variance. I don't know why I wrote that, but it should definitely enable viewpoint in variance for sure. as soon as you there's two things. One is. You can, when you learn an object, you can assume, you know it's orientation and it's locational, right? But when you're infer you don't have you. And so I think that would be the, that would be, as you say, viewpoint, take right code. I think that would be the, to me, that would be the higher priority than the hierarchy because we haven't really gotten this one layer to work as we want work. Just your last I, thought great was a lot illustrations. Okay, thanks. Yeah.