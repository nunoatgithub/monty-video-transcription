I'm going to get started. This is a progress update for the work I've been doing over the last four or five months. I started off working on temporal memory, getting acquainted with the algorithm, reproducing it, and potentially trying to make it faster. This was a subproject related to temporal memory: sample-invariant object recognition. I'll go over the problem I was addressing, the data I was working with, the algorithm I used, the results, and where to go from here.

The main question is: can I use a single sheet of temporal memory—one cortical column, one level of the hierarchy, and just one layer—for sample-invariant object recognition? Put differently, if I revisit part of an object I've never seen before, can I identify what object it is, no matter where I am? When I say one column, I mean one cortical column. When I say one layer, I'm not using any concept of locations in there. There's no layer dedicated for locations, but there is a location signal. We don't have a grid cell layer like we did with columns. One layer means there's an input, but there's no output going to another layer, and there's no object ID layer. This is just to focus on answering this specific question before integrating it into a fuller system.

How does this differ from the previous paper? The idea is a continuous sheet, and we're not changing orientation yet. The question is: can I disambiguate between multiple objects I've trained on while observing features at locations along the object surface? Why is this important? Deep learning in this field often requires ingesting a lot of information about 3D objects to disambiguate between multiple objects or recognize an object despite occlusions. We saw this with vector neurons.

The data I'm working with is the YCB mesh dataset. Each object is its own point cloud, ranging from 7,000 to 12,000 points describing the object's morphology. The feature we're looking at is the Gaussian curvature value for each point. You can think of this as the proximal input to temporal memory, and the context, or basal input, is a location in a fixed object-centric reference frame. This reference frame doesn't change for any of the objects.

Why use point clouds instead of something more complicated? It's a simple data format for quick prototyping, but there are caveats. Pre-processing is expensive because there are thousands of data points. Since I'm using temporal memory, converting point clouds to SDRs is tricky. I'm trying to create a continuous set of SDRs with continuous overlap between points, even though point clouds are inherently discontinuous. 

Why process all the data points? The goal is to do a subsampling of the data points and see if different subsamples work, but I still need to preprocess everything. For testing, I wouldn't need to preprocess everything because I'm randomly picking points to learn and infer, so I don't need all 7,000 points at any given time.

Another challenge is achieving reasonable overlap between neighboring points, whether in coordinates or curvatures. This process is done through trial and error. There's also the problem of precision: point clouds are real-valued, but SDRs are discrete, so everything needs to be cast as integers before proceeding, which results in some information loss.

Here's a high-level overview of the strategy for encoding coordinates into SDRs. First, scale all the coordinates for a given object. For all the objects, scale the coordinates to a similar range and convert all floating point values. For a sample point, define a fixed sphere around it and compute all the neighbors in that sphere. When computing the SDR, I randomly but deterministically pick some of those neighbors and hash their values to buckets or indices in the SDR, setting those buckets as on and the rest as off. This is similar to the coordinator, just a visual representation. You would do this for all the points to get individual SDRs as well.

For the metric of nearness between points, is it a discrete effect within the cloud, or is there a more continuous effect if you sample a random point? Nearness is defined by what's in the sphere, so it's more of a binary effect. But as you move the sphere by picking different center points, you still get a gradual overlap between neighbors as you move between them.So even though it is binary—whether I'm in the sphere or not—you still get that gradual overlap if you move the sphere, through the effect of multiple spheres. That was for coordinates. You can do the exact same thing for curvatures: scale all your curvature values to the same range, and follow the same neighborhood calculation and hashing procedure.

For the location, you're only storing local location, so for the small cloud of points that you're hashing, you get some similarity for those, but not for slightly more distant points. You wouldn't get similarity for more distant points. The coordinate encoder is based on a simple principle: if I have this sphere and pick an even larger sphere, all the points contained in the larger sphere should be in the smaller sphere when hashed into an SDR—there should be overlap. That's a simple principle you can test. You can do the same thing for curvatures.

There's an inherent problem with the way we're doing this: there are too many points. A full point cloud—maybe 10,000 points—takes about six minutes to process with a single sheet of temporal memory. The idea is to reduce the number of training points. One quick and dirty solution is to group the coordinates and curvatures into clusters, and pick the proximate mean of each cluster, yielding a finite set of the most commonly occurring features and locations. I would only use those points to train. If I have a 10,000 point cloud, I'm just reducing it to n points for training, where n is the number of clusters generated through my K-means method.

This is a simple visualization of what this clustering looks like. If I cluster by locations, the clusters are grouped by similar coordinates, and you get this center image. Just by looking at it, you can tell the purple points—the training points—are uniformly distributed along the object surface, which is what we want. If I was only clustering by locations, that's what I would see. If I was only clustering by curvatures, I would pick the most commonly occurring curvatures in my point cloud. There's a problem with both methods: in the first case, there's no guarantee of overlap in the curvature SDRs if I'm only looking at locations, and vice versa. If I was only clustering by curvatures, there's no guarantee of overlap for the location SDRs.

A reasonable middle ground is to do both, and that's the image you see here on the left—clustering by both. There's some guarantee for both locations and curvatures.

When you cluster by location, how do you make sure they still end up on the surface of the object—the centers of the clusters? When I'm picking the approximate mean of each cluster, that point exists in the point cloud. You're picking the point in the point cloud that's closest to the computed mean of each cluster, so there's always a guarantee that all the cluster points are on the surface of the object.

Jeff, you had a question?

You're training by now—you've got the set of points to train on, and you're going to train them individually. If I was touching this object with a finger, I would be moving continuously along the surface. That would be a much better learning strategy. It seems like that would make a difference. Is that a concern here, or is this just like touching my finger to random points around the object, as opposed to running my finger over the object? If we're trying to learn continuous representations using temporal memory, you would want to do it that way.

That's absolutely right. I could have done it that way; this was just easier to experiment with. That would probably be a next step. I don't know what kind of results you'll get, but if you don't get good results, the first thing I'd say is that's not how I would have learned. In some sense, you almost guarantee continuity if you move your sensorimotor system continuously on the surface, whether it's eyes or fingers.

Just to get to the results in that thought experiment: technically, the best learning strategy for learning what this bottle is would be to move continuously and experience all the most commonly occurring curvatures on the object. If I touch a different point I've never touched before, I know what this object is. You may not know exactly what it is, because it depends on where you've touched—it might only be the front side, and now you're touching the back side. Just be careful. It's going to affect how I look at your results. Maybe this learning strategy isn't the right one. If I think about how to create a continuous representation of locations and curvatures, doing this continuously is important. I can give examples of why jumping around wouldn't work.

This is just a bit of data about the data. The curvatures I'm using—the SDRs are about a thousand bits, and about 11 bits are on. The coordinates, which is my basal input and my AP input, are the random but deterministic object ID for every object. They're both about 2,000 bits, and 30 doing representations. If I think of curvature as the feature, that's the output of the spatial pooler. It's not dense; it's not sparse like that. It's denser. The reason it had to be this sparse was because I was scaling the curvature values, the raw curvature valuesInto the exact same range as my coordinates so that the clustering would work. That kind of affected how things worked, and that's something to keep in mind—it's not quite right. When we first did the temporal pooler, and in fact, whenever we did it, we always had these fairly sparse outputs of the spatial pooler. But we know biologically that's not accurate. If you think about the minicolumns, it can work, but it's not that sparse. It's 20 to 30 synapses, right? We would say, "Oh, we have 2,000 minicolumns," but in reality, it's 40 on at a time. In reality, if you look at the distribution of V1 orientation columns, it's much less sparse. Just keep going.

There are about 1,024 minicolumns here and about five cells per column. The activation threshold is set. Jumping into the algorithm, it's very straightforward. The biggest challenge in this subproject was getting the data correct. The algorithm is that for every single object I'm looking at, and for all the coordinates and curvatures I've considered in my clusters, I pass in the curvatures as proximal input, the coordinates as basal input, and the apical input gets the deterministic object ID. During training, I'm storing the winner cells for each observation as I iterate.

In terms of the numbers, we didn't have the apical. AP did, and I think it's maybe closer to the columns paper. The coordinates come in, and in terms of the numbers, they're very similar. It's a little smaller because he was running into speed issues. It's close to the columns paper. I'm thinking these are the kind of numbers we had. In the columns one, there's an explicit object ID layer that feeds back the correct ID. This is like an approximation—I'm manually picking that.

Inference is pretty much the same. I pass in the curvatures as proximal input and the coordinates as basal input. The only difference is I withhold the object ID during testing. The testing set could be either a uniformly sampled subset of the remaining point cloud or an occluded subset of the object, and I'll showcase where both work. The goal during testing is to store the predicted active cells. After iterating through all the coordinates and curvatures and all the objects, I want to compute the overlap with the winner cells stored during training. The next slide will show a visualization of how this overlap is computed.

Are there any questions here? This is a very naive way of doing inference, but it's more like a check to make sure that one sheet works in this fashion. The goal is to test the algorithm as we published it, and it seems pretty straightforward unless I'm missing something. This is where the bulk of the information is. Try to go through this one step at a time.

The reason this figure exists is because, assume I've trained on five different objects—object 5, 6, 7, 8, 9, and 10. Starting with five is interesting. The goal here is that I'm inferring on object seven. I'm seeing object seven, and I have no idea what it is. The only way I can predict what object it could be is by comparing against previous training data. If you look at the X axis for each of these black columns, it shows the winner cells collected during each training observation. There are about 50 training observations, and for each, I've stored a set of winner cells. On the Y axis, I'm looking at my predicted active cells collected during each inference observation for object seven. Again, I have no idea what that object is, but the only way I can predict what it could be is by computing overlap between the winner cells and the predicted active cells. That's what these dots are inside each of these black columns—it's the amount of overlap if it's above a certain threshold.

If you look at the training observations collected for object seven and compare them against the predicted active cells for inference on object seven, I notice the most overlap compared to all these other objects. You're trying to pick the object that has the highest total overlap when comparing the predicted cells to the winner cells. The numbers at the top just describe the total overlap. The algorithm is reasonably confident that what I'm seeing right now is object seven. If I had a typical pooling layer on top of this, I wouldn't have to add anything—it would do it automatically. It's a complex step, but in reality, the neural implementation would be simple.

Is this right for that object? Yes, unknown object seven, but it did predict seven, so that's good. It had a total overlap of about 74. The next closest prediction was object six, which had a total overlap of 26. There were 50 training observations and 50 testing observations. You might have been able to determine that earlier, but you just ran the whole set. Then 29. One issue here, which we didn't have in the problem figure, is that many of the inference points won't match anything because you're looking at different points in store. Despite all the work, many of them don't match at all, though enough match to distinguish it. I think it's still an issue. If you look at object seven, eight points actually matched out of 50, so 42 didn't match. If I haven't learned the whole object and now I'm touching it at points I have learned, I can still recognize the object. But if we want to recognize it with one grasp, we need a much higher grade. I'm not sure we're really solving the continuous case. Inference is just predicting the object with the highest total overlap between what I collected during testing and what I collected during training.It's a very simplistic approach, unlike the columns paper, which has that explicit object ID layer. The next steps would be to use multiple layers, possibly multiple cortical columns, or, as the supervisor suggested, we might want to adjust continuous representation as well. It's a little fuzzy in my mind right now, but the idea is that you have various points and curvatures, and somehow we have to be able to interpolate between them. We're not doing that; only the radius threshold addresses it. If you need to sample 50 points in training, and then in testing, as long as you pick enough—say, 25—no matter which 25, it's okay if many of your sensations do not match anything. What's important is that what you do sense eliminates possibilities and narrows down the choices.

We'll probably have to change the voting a little because nothing matching is not necessarily a bad thing. We basically need to loosen up a few things. I have to take some time to think about it. We're getting close to the results here. I'm excited. This is what the objects look like. The training and testing objects could look like this. I've shown three different objects and the two ways I could generate the testing samples. In the top row, it's very hard to see the purple points, but those are the training points, and the green points are the testing points. It's a uniform random subsample, and whatever remains outside of my training group is my testing group.

In the bottom row, it's easier to tell because I have these broken points, which are still my training points, but the green ones are occlusions. It's like the car example, where you take a sheet, and the green ones are occlusions or where you stand on the greens. The green ones are for testing. Is that where I'm sampling testing, or is that the part I can't see? That is where you're testing. That's the part you can see; that's not the occlusion. This is an included object from the green point to where you can test. Correct. You can only touch this part. Any more pointing, and the algorithm slows down, so I have to look. You could have fewer green points but distribute them more widely. You could say 25, and I'm thinking about doing that. This is a pretty difficult task. As you said, including 90% of the object, we'll see how it works. That's a hard standard to meet.

The first test is not the occlusions; it's the uniform random subsample from the testing set. I'm using 13 training objects, and the goal of this exercise, before I even jump into this graph, is to find a sweet spot for the number of training points needed to reach high testing accuracy with a limited number of testing samples. That's the question I want answered.

The graph here shows that test. All these different lines are color-coded and describe the same experiment run with different numbers of training points or clusters: 20, 50, 100, 150, and 200. Why did it go down for 150 and 200? The testing accuracy is on the Y axis, and the number of test samples is on the X axis. If you look at the orange and green lines, which describe 50 and 100 training clusters respectively, the green line is higher, and the orange line is just below. After about 50 testing samples, it reaches 100% accuracy, no matter how many testing samples I use after that. But if I use more training examples, like 150 or 200, it performs slightly worse.

The reason is that there is a sweet spot for the number of training clusters. Any more than 50 or 100 in this case just introduces noise. It's not clear why it introduces noise—these are just additional training points. Why would it make it worse? Your radii are getting smaller. If I add more training points, now you have to hit them. That radius is in the clustering algorithm; you're over-clustering your training points. You could have overlapping clusters. You could still have a fixed radius and allow the clusters to overlap. That's what I thought you would've done. That explains this result. The more you add, the harder it gets to hit them.

I didn't visualize the clusters using tSNE or something like that, but that's okay. That explains the result. If you kept the radius the same size, you would've had similar or better results. At least in this limited experiment, the conclusion is that if you have about 50 to 100 training points and use greater than or equal to 50 testing points, you get 100% accuracy—all 13 objects are correctly classified.

In some sense, the number of training points, if we want to do a continuous representation, the more training you do, at some point it should stop taking more memory. Intuitively, you wouldn't get more benefit. At some point, since your clusters are overlapping, training a new point doesn't really change anything; you've already learned that space. The first thing to do seems to be to try getting away from that narrow radius.I think if I had fixed the radii, we probably wouldn't see this. In addition, you wouldn't see this effect. It would be good no matter how many training points I used. The algorithm needs to have an addition where it doesn't grow segments beyond a certain maximum number of segments. Right now, I'm continually adding more and more, and that's why you're not really doing continuous learning. You're not creating a continuous representation because if I'm learning the shape of something and I keep moving over it, at some point you just don't learn anything new. In my case, I am, which is good.

This is an example of where it works. This is the same example as before: I have 50 training points and 50 testing points. The object I'm using is like a drill bit or a drill. I'm randomly subsampling my space for the testing points, which are in green. The purple points are the training points. Here, you can see it pretty confidently predicts that this object is indeed object eight. There's really not much general overlap anywhere else. That's the one highlighted in red.

I appreciate this is more specific to the situation where the object is artificially fixed in an external reference frame. Do you have a sense for how diagnostic the location information itself might be in this case? There are probably certain regions of the 3D volume that are only sampled by certain objects.

That's a great question. I didn't include them in the slides because it didn't work so well. If you go all the way back—what didn't work? I'll show that, as in just using the location information. So you're only clustering by locations. I did try this. If your feature is essentially the location, that's what I mean. If you were just to look at the overlap of the feature SDRs at testing and training and sum across those, you might get how many hits you would get, and if that would be enough alone to disambiguate the objects.

So, no basal memory without taking into account curvature. Exactly—basically just proximal input. I imagine it wouldn't be very accurate. I was just interested in this. There might still be a substantial intersection between different objects, so you'd have to see, but you might get some locations that are unique to an object, and then you'd go run with that. I don't know if there'd be a lot of substantial overlap. The two planes of the objects have to be right at the same point, I'd say, because they're on the surface. It's not like they're occupied volumes; it's the surface points, which would be pretty low.

How big is the radius compared to the width of the object? It's 15% of the maximum range of the object. The range here is zero to 100, so the radius is about 15. That's big, so you would get a fair amount of overlap. I think that's a good point—you would be able to recognize the object just by that.

If we think about this as an exercise in entirely learned continuous representations, then we can dismiss some of these issues because we're assuming we know the locations here. Of course, you have to infer that. We assume we know the orientation, but we have to infer that as well. All these things we've talked about extensively have to be done. This is a subset exercise, and as you pointed out about continuous representations in the temporal memory, as long as we focus on that, we can put aside things like Neil just said. We could be using it, but we're not trying to create a practical system here; we're trying to exercise a part of it now. I think that's correct.

This is a case where it didn't work: if I used too few testing points. I still have the same 50 training points, the same 13 objects, and the object here is a cup, but I only used 20 testing points. Here, there's quite a bit of ambiguity about which object it could be. It could very well be object 10. It doesn't look like that, but object 10 jumps right out as the least ambiguous. It has a higher threshold than all the others, but it's incorrect. So it's not just ambiguous—it's both, because if you ignore object 10, it could very well be object 6, 7, 9, or 12; they all have about the same score. But 10 is wrong. It should be nine.

What is object 10? I have no idea; I didn't check. I think it's a die—a deformed die. It entirely depends on what testing samples were picked here. It should also depend on the shape of the object. It just so happens that the testing samples here most closely match the training examples I collected. I understand, but it's not just random which ones you sampled; the shape of the object really makes a difference. You could imagine if I had a cube in this space, a lot of those points would be similar. It's both, because what you're describing is parameterized by whatever I trained on and what I collected during training. The winner cell is parameterized by the morphology of the object. I think if we introduce continuously changing locations and curvatures, you shouldn't get this error.

This is the second subset of testing. This is not doing uniform random sampling, but looking at random occlusions within the testing set. I didn't vary the number of training points; I used the same as before—50 training points and the same 13 objects. The question I want answered here is: how many testing points do I need to get a high enough testing accuracy? Over here on the plot, you can see I've used quite a reasonable number of test examples for these occlusions, anywhere from 200 to 800. What I find is that after about 500 testing points, you get about 100% accuracy. That's all 500 in this small—I'm surprised you can even get 100% accuracy given how small an area you're referring over.That's because I know exactly what curvatures and where the locations are for those curvatures. In that dataset, there's a cereal box and a die in that cereal box, and you're just touching so little surface of it that you wouldn't be able to tell, no matter how much you touch. Unless you're doing what Neil did and setting the exact location, that is why. This result makes sense to me because I know the locations are completely fixed. It's a global coordinate system for all the objects, and I can do a one-to-one direct comparison between objects because it's in the exact same coordinate system. All the results have to be interpreted with those limitations. Everything should be taken with those interpretations in mind. It's not random; given the way you set up the problem, these are the issues that are going to happen. In this case, it's a nice visualization of why it works—because it's a screwdriver, a weirdly deformed screwdriver. The green points are the ones I can see, and the purple points are the ones I've trained on. Here, this is object 11, and it is correctly predicted as object 11 with a huge margin. I did a lot of samples—about 600 samples here, 600 testing samples—but here it doesn't seem like you need 600. I probably don't in this case. That plot I showed earlier was for all 13. At least 600 or at least 500; this is another case where it works. This is back to that drill. Seeing object data correctly, it knows it's object data and does so with a large margin. Seeing so many testing points is similar to running your finger along that green portion of the object because you get lots of samples. They're so close together. It's not continuous, but they're so close together. Lifting finger, I guess that's here. If you were to just slide it along, you'd get probably dozens to hundreds of points. Even if I represented that continuous movement for the point cloud, it implicitly means I am picking it up and touching it. There's no continuous movement.

This is a case where it didn't work. I have no idea what this object is. This might be a Lego piece. I have no idea—a bag of marbles? I think that was something else. I don't think you know what Lego looks like. Testing points: six. So many things going on here. Very small part you're testing, but you are knowing these exact locations again. Here there is some ambiguity whether this is object two or object 11. It should be object two, but it's predicting it as 11, or it's not certain. If I had a threshold for my prediction, then that's a reasonable result given the small testing, but also all these things we've talked about.

Open questions, next steps. The first thing I want to address is the downsides of point clouds. That's something I went over in the beginning and want to reiterate here. It's very expensive to process and to distill information into salient patches along the surface of any object. There's this data complexity problem. I don't need thousands of points to describe exactly what it is. You're talking about the dataset here, right? This is not point cloud as part of the solution; it's just part of your dataset, right? Yes. This is a knock against that particular kind of dataset. We might want to move forward to a more realistic training structure, but in the real world, we don't start with data point clouds. In the real world, we have objects that we're sensing, and there aren't point clouds. Having a point cloud is just a fact of the data we're working with. Getting rid of it is not an important thing other than this practical problem. In the end, if you have a real object, you're sensing it; you're not starting with a point cloud. You're just using a point cloud because that's what the YCB dataset gives you. Some data—I'm saying in the real world, let's say I had some sensorimotor data. Even on this, you could start with this point cloud, just put aside the pre-processing because that's irrelevant. When you're sampling, you could be calculating those as you sample, right? Couldn't you be calculating—not point cloud, but could you be calculating the SDRs? You could be doing it on the fly, but the reason I did it separately was because it is still very expensive to do a hundred. It's still quite expensive to do it, but you're doing thousands of them upfront. I'm doing thousands upfront, but if I just picked one individually, the process for generating an SDR is still expensive. You're only doing 50 of them, but it's 50 of the perfect points, basically—50 versus thousands. Wouldn't it be less expensive? Imagine a more realistic training scenario, like running my finger across that object. I don't know if I could realistically reproduce the same data pre-processing for that more complicated experiment. It would be way too expensive to do that. I'm not sure; I'll take your word for it.

What I'm trying to propose is that we should probably move away from point clouds to more realistic 3D models of objects. The fastest—what do you call it—tri-mesh, like the mesh. Either that or a proper 3D rendered object. I don't know what a proper 3D rendered object would be. Any other options? A couple of weeks ago, Ben was working on a 3D surface reconstruction. Ben, what was the representation you ended up with there? Was it like a surface? It was an implicit function that returns a value greater than zero when you're outside of the object, less than zero if you're inside the object, and zero when you're on the object surface. Ben, you've argued for parameterized models of objects internal to the system. Here we're just talking about the model of the object external to the system that we're using to test it. I just want to make sure we're not mixing those two up. Good distinction.

In some sense, Monty has to work with all of these different data types.At some point, whether it's point clouds like lidar data or touching an object multiple times, you get point clouds with curvature features. It's not that far off, but he's processing all these 10,000 points up front. That's a pre-processing issue. In terms of data format, Monty has to work with point clouds, points, 3D images, and audio data. It has to work with all of these lidar data at some level. If you're training on 50 samples or moving your finger, you shouldn't be doing 10,000 of these things. This is an engineering problem, not a scientific problem.

Someone has a question. Niels? Sorry, I was just going to quickly say it seems like the kind of hierarchy Koran was talking about last week would definitely be helpful. If you can quickly develop a representation that at least tells you you're looking at a cylinder or a sphere in a reasonably broad region, that would quickly narrow down the list of objects you need to consider. It would be a less strange representation, even though you start with a point cloud. I appreciate that it's very nontrivial. You're right. I didn't hear what was presented last week, but we don't want to rely on that up front. We want to understand the basic mechanisms because a cylinder is just another object. We have to limit the number of objects we can learn before using hierarchy, but a single level needs to solve problems and learn continuously. It needs to form continuous representations. We can introduce hierarchy later, but we have to get the basic mechanism working in a single column. That would be my argument: create continuous representation.

Something I've been thinking about this whole meeting is, if we have binary representations—his radius, is that binary? Not binary? No. Ultimately, the representation we have in memory is basically SDR, but that's not binary. It's a set of components, each one is binary, but the SDR is not binary. It's discrete; you only have finitely many possible representations. It's almost infinite—the number of representations you have within SDR is astronomical. So it's not limited. The trick is you want representations that are near each other to overlap significantly. The representational capacity of an SDR is huge. You don't want a lot of speed; you want overlap. As I'm sampling as I move on a surface, I don't want this point here to be unique. Then you end up with a sense of understanding your point. I'm mistaken. Just as with SDRs, you have two to the power of the dimensionality—that's the maximum number of unique SDRs. So how do you go to continuous representations? There are just so many. The actual number of ways you can represent sparse is huge. If you have a large total, it's astronomical. There's no representational problem. The problem is making sure that points near each other are similar and have significant overlap. That's the challenge. It's not a capacity problem.

Getting back to earlier, both the SDR for the curvature and for the location are highly sparse, which generally would be difficult. We briefly addressed hierarchy, but we don't know if it's needed or not. You said it's going to be needed, but I don't think we should focus on it until we solve the problems with a single column. That would be my recommendation. We have to solve the problems of not knowing the location and ensuring overlap for these SDRs. We don't want to rely just on location. We want to learn in a way that, if I'm sampling continuously, it makes a difference compared to random points. I think we have to think about it more.

Ben started this great building brainstorm doc, and I've been contributing to it. There are a lot of unanswered questions there. If anyone wants to take a look, that's separate. The next step is, can we move away from this fixed object-centric coordinate system I've designed? We should be able to, plausibly using grid cell codes that would enable viewpoint invariance for sure. As soon as you learn an object, you can assume you know its orientation and location, but when you're inferring, you don't have that. So, as you say, viewpoint invariance would be the higher priority than hierarchy because we haven't really gotten this one layer to work as we want. Your last thought was great, with a lot of illustrations. Okay, thanks.