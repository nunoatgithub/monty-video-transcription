I have three goals in this presentation: to talk about an upcoming challenge, discuss one possible way to tackle it, and share some ideas that this possible solution led me to. There are many ways to think about Monty, and I want to introduce another lens through which we can view it and discuss it.

Here's the problem. For our initial demo, we're assuming we know the boundaries of objects. We have an object in an empty void and get a label indicating whether we're sensing the object or just the void. That's fine for getting started, but it's a challenge we'll need to solve later. If we can solve it later, we can fall back on everything we're developing for the initial demo. The little green markers represent a label saying we're sensing the momentum mug, and the red observations are not on the mug. In reality, the observations will look more like this: small bits of sensory input without knowing what they belong to.

It's not as simple as that. If you think about having an image and processing it, it feels like that, but if you're thinking in three dimensions, whether dealing with touch or three-dimensional vision, the problem isn't quite as most people imagine. You'll see plenty of examples of that.

This is the problem I want to solve. Another way to phrase it is that, based on this image as an example, from just one static image, I can't tell where one object begins and another ends. I can't reasonably say these are two objects versus one. Using a depth sensorimotor or even a touch sensorimotor, I could build something that looks like this as a model of my sensory input. If we take away the real sensory data and look at it from the model's perspective, this is what we're left with. From just this sensory data, you can't know that these are two objects in that one.

How do we know in reality that they're separate? One way is if we run our finger along the object to start sensing it, we'll move the object a little and notice that one object moves while the other doesn't. When they move independently, we have data suggesting these are two objects, not one. If I'm moving my finger over this object, I can't move it over both objects at the same time. The confusion only arises when I jump from one to the other. Are you assuming I have two hands, one touching each object? If I'm just moving my finger along one object, I won't encounter the other or even know about it. I don't need to move it. If I'm just following the contours of an object with my finger, I wouldn't know about the other object unless I go down to the table and up on the other side.

That's what I'm assuming: when you sample the object, you sample both with your finger because they're touching in the image I presented, but this is just an image. If you're touching, that's not the case. Jeff is saying if you touch this and it moves, you don't know that this has—well, touching this, I'm just saying I don't have to have it move. When I'm touching this, as long as I'm doing this, I'm not even touching the other one, so I don't know it exists until I come down on the surface, go over here, and then up on the other surface. In this example, there's no space between; they're really right next to each other. They're next to each other, not glued together, just touching.

One could argue, like with a stapler, the top moves relative to the bottom, but I don't think they're separate objects. Even that's not sufficient here. Maybe that's the second point: you need to understand the...

That is the point I'm making. In the stapler case, and also in the case of two objects right next to each other, the behavior is similar. I'll challenge you on this because when I first started learning things, we isolate them as much as possible to avoid confusion. For example, when teaching letters of the alphabet, we show a single letter on a white space, not words or two letters touching. There's nothing else present; we don't show two letters butting up against each other.

Once we've learned those things, there's an inference problem, but that's not the same as learning from objects that are touching. Inference is different; some sensations stand out as recognizable, but they're not part of the other object. We need to be careful here—are we talking about inference or learning? I don't think we can learn the world well, especially when young, as a series of things all together. We need to isolate them.

For this presentation, let's assume you have no knowledge of how the world works and haven't learned anything yet. I'm thinking about how we're going to train Monty. Maybe one way to train it would be to isolate objects and not deal with this challenge. You can have multiple objects around, but the idea that these are physically touching makes it difficult. How do I know these are not one object? That's the challenge I'm raising. Maybe it's an unsolvable challenge in terms of learning, and you just don't want to try to learn the world like this because it won't work well.

My goal today is to present a way to learn despite this challenge, and I'll see what you think about it.

It seems that if you see two objects move independently, that signals they are separate objects, not one. With exceptions like staplers, I haven't yet integrated the idea of object state into this presentation. What I think is happening is that you have a graph model, which acts like a hypothesis about how the world works. Implicit in having that hypothesis are predictions your model makes, and you can use actions to test those predictions.

If I have a model, it means I've already learned something. Previously, I said we would assume we know nothing, but now I have to learn a model with these objects interacting. If you already have models of the world, that's a different problem. I don't mean models previously stored in memory; I mean building a model online as you sense things. At this stage, I have a set of observations, not a model. A model is something you've learned, like modeling a cup or classes. It's not just a set of observations. What would you call that collection of displacement? I have a set of observations, but they can't make predictions because I haven't learned anything from them. Without prior learning, I can't predict the future. You might have learned a short-term graph, which is still learning something. If learning is building a short-term graph, then the only thing you can predict is the observations you just made; you can't predict anything else if it's not stored. The best you can do is temporary memory of recent actions. If you want to call that a model, fine, but it can't make predictions about what's next because it's only what you've observed; it doesn't have a bigger model.

Typically, I have a model of something and then observe a subset, allowing me to infer the entire object. In this case, I just have a set of observations and can only make predictions about those. For example, if I saw A here and B there, I can predict they'll be back, but I can't predict what would go elsewhere. There's no model for that. Even with a new object, you can still form hypotheses about how it will interact with the world. If I encounter a new object and don't know what it is, all I have is a sensing observation. Without prior learning, I can't make predictions or hypotheses about it. I just have a collection of observations—relative displacement—and the prediction I could make is that if I pick it up and move it, the whole thing will move with my hand. It's a simple prediction, but it's still a prediction you can make with this model.

When you try that, you might see you have two objects instead of one. For example, if I pick up my phone with an eraser on top and move it, I can construct counterexamples. It's not a foolproof strategy, but it is possible to knock the eraser off the phone. Now I have different sensory input, and my sensorimotor system shows a gap between the two objects. The idea I'm trying to get to is this iterative loop: gathering a set of observations, predicting how they should behave, taking an action to test the prediction, and using the outcome to update the model or observations.

I don't think anyone is fully convinced by this idea yet, but compared to how we train deep learning models, it's an interesting alternative. Instead of just training with cross-entropy, the loop would look more like taking actions to sense the world, storing observations in a buffer, making predictions, selecting new actions to test those predictions, executing the action, and updating beliefs based on the test. Are there any other actions and tests you think would work besides trying to separate two objects and seeing if one moves independently? Are there other predictions you're considering, or is that the main one?

I have not thought of others, but I imagine this idea could extend to other things. I'm trying to understand because "select an action" and "make predictions" are very generic terms. I was objecting to it because you can't make predictions about a world you haven't learned anything about. Then you gave the example: if I'm trying to do separation, and I move one thing and the other moves, that works, but I don't know if there's a more general version of this. We should be careful because we're using words that suggest there's a model to make predictions, but it's not really a model. The hypothesis is that if I sense a bunch of things and they're separate objects, they'll be separated. Another example is squeezing something. If you squeeze it, everything that moves along with it could be part of the same object—they're connected. That's similar to the idea that it's the behaviors of objects that matter. It's not just what the object is, but how it behaves. You might not know the label "coffee cup" for something, but you can still learn how it works, assign a label in your own language, and understand its function. You need a model. In some sense, you're arguing for a temporary model by assuming a bunch of observations are together, and if one moves and the other doesn't, then they're not together. That's the gist of what you're saying, but it's not a general-purpose prediction. I can't predict what the other side of this coffee cup looks like or other things because I don't have prior knowledge. I just have current knowledge from one viewpoint and haven't built up anything else. I would think in terms of predicting behaviors of objects, not parts you haven't seen before. But object behaviors have to be learned too; that's part of the models. I'm not dismissing it, but one thing you pointed out is that moving the object isn't a behavior of the object itself. If I move something and other things don't move, I might assume that subset of the graph is separate. That's exactly what I have so far. You might be right—it could be a point solution, but the problem of occlusions and knowing where objects begin and end is still significant. Even a point solution to that is valuable. I see occlusion sliding back in when thinking about images. The alternative is starting by learning about the world by moving my sensors over the object, whether it's my eyes or fingers tracing it. As long as objects are relatively isolated, like picking something up in my hand, I won't get confused about its boundaries. I can learn this model well, and when I see it in a scene with something else, it's a different problem. Now I can say these displacements belong to a cup, and others look like a phone—they're not the same thing. I order blur models for these two things, and figure-ground separation typically assumes you don't have a model, just sensory data, and you're trying to figure out boundaries. If you do have a model, it's much easier. For example, you pick something up and touch it or move your eyes over it.

I don't try to teach you what the letters of the alphabet look like by showing you a paragraph of text. That fits into this because one of my points is that you can't just passively observe images and decisively say how objects will behave or where one object begins. By picking up, manipulating, and sensing it, you build the model. You start with the model, and then separation becomes easy. The way you started the process, you're saying, "I have this image and can't tell where one object is because I don't know anything—I haven't learned a model yet." So there's a big distinction: do I have models of these objects or not? If I don't have a model yet, am I still allowed to pick up and manipulate an object? You can grab something even if you don't know what it is, touch it, reach your hand into a black box and run your fingers over it, or look at it. That's how you learn—exploration. You pick it up, and in the process, it falls apart.

Automatically isolating a sufficient amount of exploration models, maybe you could say if you already have models of other objects, you could generalize behaviors to new objects you don't have a model of yet. If you don't know about any objects in the world, this might not work, but if you know there are solid objects you can pick up and manipulate, and then you see a completely new object, you can apply those behaviors to it. You don't have a model yet, but that's not a figure-ground separation problem; that's a generalization problem.

Ben, start over with the question: how do I tell where one thing begins and another ends? What you said is a different problem, I think. This is a more general framework than just figure-background separation; it's a generic framework you can use to solve many problems. That's the whole model building we've been discussing. I'm just reacting to Ben's point: we have no knowledge, we're presented with something about the world, and we need to determine where one thing begins and another ends. That was the question.

One of the other things is adding a different way of looking at Monty, the stuff you've already laid out. Some of this will seem obvious, like picking up objects and sensing them to build a model. That's not new. I'm proposing this loop as a different way of thinking about it: you sense data to generate hypotheses, take actions to test them. That, to me, is the whole model building exercise. If I open that up to the more general scenario, where I have models of things in the world and I pick things up and act on them, I make predictions. That's what the models are for. Every time you move, you're testing it. Not explicitly, but anytime you do anything, the model predicts what you'll get, and if you don't get it, you know something's wrong. That is the framework of learning in the Thousand Brains Theory or cortical column theory. I agree with that. I'm not sure if that's new; it seems like what we've been saying all along.

In the next slides, I'll take this material, which is not new, and add a little spin. Lucas, do you have something? Maybe that's not the right comparison.

Model-based enforcement plan would be the most direct comparison. It has nothing to do with it, but model-based systems are just data. That would be true for any model building system—anything with a structural, three-dimensional model that involves action selection and the idea that your sensors are moving relative to the object. That's how I think of it. If you are moving your sensors and actuators relative to the object, you'll make predictions, and the model will test them. It's just its nature.

Even on a higher level, we know from child psychology that children actively generate hypotheses and test them about new objects. They work like little scientists, experimenting and efficiently figuring out how objects work. Do you have a specific example in mind? For example, there were studies with a machine and little blocks of different colors, called blurbs or something. Children figured out which colored blocks activated the machine in what combinations. If they glued two blocks together, they had to combine them in certain ways to turn them around. I can send some papers on that. I'm just curious about the kind of examples. It's a machine that lights up and makes noise when you put different colored blocks on it. When shown a new version, they'd use previous learning to figure out if it works the same or differently. Exactly.

Briefly responding to your point, Lucas, my comparison here is not to criticize deep learning or say there's no example of deep learning that incorporates these ideas. It's more to say this feels like a cool idea—the idea of going around and building models of objects feels very different from couch potato image classification, where you just feed the model lots of examples. It's more like an embodied learning situation, more like feed-forward processing versus interactive sensing. That would be a better comparison. That's not really new, certainly not new for us or even for the community. The idea on the right is: what model do they have? Do those model-based systems have actions and sensors moving?

Okay. Now, thinking about actions, I wanted to discuss the advantages and disadvantages of choosing actions first. The disadvantages are that it's complicated, and we don't yet have a complete theory about how actions are generated and executed in the brain. If we focus on solving action selection, it could be a complex or expensive task. Jeff, a couple of weeks ago, you pointed out that we may be able to succeed in our first application without introducing that complexity. Actions exist on a spectrum. Simple actions include tracing the lines of something or following things. Another example is saliency, where your eyes move toward something different—these are basic actions. Then there are actions like, "If this really is a coffee cup, I should be able to unscrew the lid." That's a different scale, more like model-based selection. The first two examples aren't model-based. You were referring specifically to object manipulation when you said this. I was saying we could build a useful system that works on the first type of action selection, which isn't really model-based. The model tells you what to expect, but it isn't driving the actions or telling you what to do to achieve a goal. For example, if I think this is a coffee cup, I predict the top should come off. If it doesn't pop up, maybe it should screw off. Those are high-level, model-based actions. There's nothing to indicate there's a screw on this, but previous knowledge suggests it should either pop off or screw off. If not, then it's not a coffee cup. In contrast, if I just pick something up, move my finger around it, and learn a model of it, I can predict where it is. Those actions don't require a model. Earlier, I said we might be able to build systems that aren't capable of unscrewing the top but stick with movements, and movements are actions. You move, your hand is controlling.

Does that make sense? There's a spectrum of actions, and I'm thinking about what distinguishes Monty from other systems. One part is that it's a model-building, model-based reinforcement learning system built on moving sensors, which is rare. Most machine learning isn't like that. That's the main difference. How models learn and how sensors get integrated are other components. Ultimately, we have the idea of taking sophisticated actions, like seeing generality between objects' tops. There's a spectrum of actions. This presentation lacks some nuance regarding types of actions, but that's a general idea in neuroscience. There are movements of the body that are top-down generated and movements that are bottom-up generated, and both need to be handled properly.

Some possible advantages of including actions: going beyond just moving my sensorimotor system, like moving my eyes left and right, and thinking about interacting with objects. First, speed. In the example I gave earlier, if you're just receiving images of objects, you have to wait until you see two objects next to each other from an angle that lets you tell they're not the same. Until then, you don't know. If you can manipulate objects, you learn this immediately. That was your point, Jeff: if I can pick it up, I instantly learn what the object is. Speed of learning increases by interacting with objects. If we use a hypothesis testing loop, you can connect with the world faster by quickly testing hypotheses. It might also lead to better generalization; by interacting with things, you learn their behaviors. Judea Pearl has a levels of reasoning hierarchy and suggests you can't go beyond level one without interacting with objects. There are clear advantages to manipulating objects. What is level one associative reasoning?

Levels two and three are interventions, and level three is causation. To be clear, the first advantage is that everything in mind-built systems is based on actions for faster learning. That's the premise. I assume you mean more top-down, model-based interactions.

That's the way it works. I was thinking more about actions like manipulations and things like picking up objects. It still would seem difficult if all I had was a camera that I could move around the room, but I couldn't actually touch or move these objects. Obviously, you can touch them. Let's make a couple of distinctions here. One is, as we discussed, there's an object out there and I can touch it with my finger in different positions, or I can look at it in different positions. The next step is, I lift this thing up, hold it in my hand, and move it around. That is a subtle difference from the first scenario. It's not substantively different; you're still moving your sensors around an object, but instead of moving around the object, you're holding it and changing its orientation. It's complex, but still in the category of learning the morphology of the object and what it looks and feels like by sensing it. Whether it's stationary or picked up, it's not that much different.

There's also a difference, for example, picking up an egg versus picking up a level. You'll learn something by picking it up. My point is, if I'm restricted to learning by moving my finger around it or holding it and moving my finger while changing the angle, that's a more complex kinematics problem, but it's essentially the same. You're learning what sensations occur at different positions relative to the object, just translating to different orientations. That is a small step from just having a stationary object. Learning doesn't require having a model of the object; I can start building the model by moving my extensions over it and feeling what it is. Now, if I've already learned models of other objects and see a novel one, how do I generalize to what I've learned before? In some cases, how do I learn that this thing has a removable top? That's a different type of model. The model isn't a static object; it has behaviors. If I have behaviors, how do I apply them to novel things with similar behaviors? That's a different level of manipulation and action. Those are top-down cognitive, cortical action generation, whereas just touching and moving your finger around is much less so.

There's a big difference between unscrewing the lid or pulling off the lid and seeing commonality between a new object and an old object. That requires a model. It's almost like a separation between a semantic model and a model, but even the distinction of feeling around it when it's static and then lifting it up means you're learning more about it, like heft and volume. It's a modest difference, but enough to identify maybe. Let's not focus on the corner case of a heavy object being surprising. Imagine it's a cup with no surprise about its heft. I don't think we should focus on that corner case. I can pick it up and maybe it falls apart, and yes, I could learn something from that, but it aids in identification and separation. Let's assume there's nothing unusual, just an object sitting here. I move my fingers, pick it up, move my fingers around, or turn it, as opposed to walking around. Those are qualitatively the same.

I'm not trying to make a distinction or argue with the difference between the behavioral model, which requires previous experience and recognition of similarities. I don't know whether that fits cleanly into those three levels of reasoning. I don't think so. What I've looked at in the past, I didn't see how it worked, but maybe some can make that argument. Now that I've gotten feedback, I want to frame how the rest of this presentation should be interpreted. I anticipated hesitation about including all object manipulation types of actions right away, but I was in favor of including them. The way it's phrased here is open-ended and generic, fitting what we're doing now: actions, faster learning, active learning, hypothesis testing. That essentially describes the static object, moving your finger over it, as Monty does. If you're arguing for using previous models of objects to make hypotheses about the behaviors of new objects—cognitive action, model-based action selection, where models determine what to do based on goals or active hypothesis testing—then I'm interested in that. I just think the language is too loose; it describes exactly what we're already doing. I'm in favor of what we're doing, and I was trying to be balanced, but I'm really on the pro side here. Were you thinking about what influence at this point?

Still training, like Jeff said, you should be able to acquire your first model of an object by reaching out, touching, and manipulating it.

Publicis testing. Do you also apply previous? No, I don't think so. Can I go on? I want to get to this idea. I don't think this presentation has as much merit as I was hoping, which is fine, but there's still one kernel of an idea I want feedback on.

I was asking myself, what are the benefits of being able to manipulate objects, and can we get some of those benefits if we just have sensors and can change the orientation of the sensorimotor? That led me to think about the prediction and prediction error components of your model.

Here are four ways you might structure predicting your next sensory input. The first is a brute force approach.

This slide is supposed to be hidden.

Let me go to the next one. This was a draft slide, Miles. The first is the brute force approach. This is what you might do with deep learning: you have an observation and an action, and you use that to predict your next sensory input. It's just a pairing—I've learned this pairing. There's no underlying model. It's a very simplified model, the simplest model ever. There's no underlying three-dimensional structure, but this is commonly used in deep learning. I'm just adding layers of structure to it.

Next, I take my collection of observations and displacement, put them into a graph, then take the graph and an action, and predict my next sensory input. That's what we've been doing so far. Now, you could take this further and say, actually, I'm going to predict what my model will be at the next step. I take in a graph and an action; instead of predicting my sensory input, I predict in terms of the graph and the displacement of these observations in the environment. I predict this mesh object moved over, and from this, I could try to learn the mapping from the state of that object in the world back to the sensory input, and then compare the sensory inputs.

The even more extreme version would be: I predict my graph, get sensory input, and construct a graph from the new sensory input. I only do the comparison—the error—in terms of the models, not the raw sensory inputs. I'm lost on that one. Can we just leave that text up for a minute? I want to read it more carefully.

Even using a predicted graph model involves a whole bunch of assumptions. You have a graph, which means—

Can we say just a collection of displacement? That's a model, but it's a model without necessarily a label yet. A graph defines a structure of something. So, model then, but it's new—you haven't seen this object yet. That's what I'm thinking. I just have— I thought you said graph plus action equals new graph, but I assume the first graph is something you'd already learned. I was thinking of that graph as what you got from your sensory input from going around and sensing. This is our short-term graph. Short-term graph—good language. Then you say predictor number two.

It's just a graph.

How would you distinguish predict one and two? One is the same as—one, I'm thinking about it now, maybe this is wrong, is I have this cylinder and I'm constructing a graph of the cylinder, so I have some expectation of what to see. I know about cylinders already, so I have some expectation. Now I hit this handle—now it's not a cylinder anymore. Now I can predict the entire shape of this thing. So that's another graph in some sense.

No, I was thinking more about having a short-term memory graph of an object in the environment. I predict that if I apply a force to it sideways, my new graph will be basically the same—the graph doesn't change. Nothing changes unless that graph is in object-centric coordinates, so the graph doesn't change when you move it. That may be different. I was assuming you get to know something about where it is in relation to the body, so that would be a new pose relative to the body of that same graph. The graph itself doesn't change, but it has a pose relative to the body.

That's correct. I was running at the last minute to make these slides. You're right—it should just say "same graph, but new pose relative to the body." That's a really important distinction because we create a new graph for all, but we just moved an existing one. It's a new pose to the body, but it isn't a new graph. I guess you could say there might be a prediction about that. If I say everything right, I could predict where these things are, but it's almost trivial. You really haven't learned anything. I don't know if it's trivial or a subtle point, because what have I learned by moving some unknown thing to another position—a rigid body that just moved?

What I'm saying is that it's a different way of making predictions. Instead of making predictions by going from graph and action to sensory input, I'm now going from graph and action to new pose of the graph to body, and then from that to sensory input. In some sense, you haven't learned anything new. Kevin says you learned that it's a rigid object, but other than that, if I see a set of things in some arrangement and have no model for it, I have no expectations. There's no additional knowledge; it's just a temporary thing. Now I move it to another place, and now it's a different pose to my body. I haven't really learned anything except that it's rigid.

Actually, I want to make another distinction. You're not executing the action yet; you're predicting what's going to happen when you take the action. I make a prediction, and then the prediction becomes true. What have I learned? Wouldn't it also depend on the background? Now you can go back to the original premise, like figure-ground separation problems. I'm not sure if we're still trying to solve that or if you're making a broader point about models. I think Ben is trying to make a bigger point than just separating one object from another. This is an idea about different ways to structure prediction and error.

I'm going to argue that there's nothing gained in this particular model version—the top right version. How about the bottom right? Let's talk about the bottom right. I have a graph, which is a new graph—novel, never learned before. This is a new time I'm presented with this thing. It replaces the predicted graph with just "graph in new pose relative to my body." So let me pose it to the body. I move the object, but mentally—you haven't actually moved it yet. It's predicting what it will be after you move it. Then, what's my new graph? That function should also take the graph; you're updating the model, a graph, but there's no update to the graph because the graph hasn't changed. Just because you moved it doesn't mean the relative positions of things have changed. The graph itself hasn't changed at all; it's still the same set of components.

Are you moving the graph with the sensorimotor in that sense? You're moving the object, but you have a different perspective, different sensations. He didn't say he moved the object and flipped it over to see the backside. That's a different type of thing. You could say, "Here I have a model of this film, but I don't know what the backside looks like." I can rotate it, see that, and rotate to see more—those are new observations. No observations for me is sensation. But he didn't say that; he just didn't move the object. If I just take this and don't change its orientation to my body, if I just move it, now I'm observing it over here. I haven't learned anything new.

You were saying from a different angle, but have I changed the angle of the object to me, or have I just moved it? That's a big distinction. If I have this same object here and move it over there, I'm not seeing anything new about it. There's no new information to be had just because it moved, other than what Kevin points out. If I rotate it, I can just rotate it right here; I don't need to rotate it over there. Yes, I'll learn new things if I do this—absolutely, I'll learn new things when I do that.

Let me try to clear this up, because the slides are not the clearest. What I mean is, I have this short-term memory model of the object. I choose an action; I don't execute it yet, but I'm going to predict where the object will be relative to me. Now it's over here. Let's pretend while it's in this new location, I do a bunch of observations and sample what it looks like here and build a graph. But why would the graph be different when it's there than when it's over there? I don't know that that really has any merit, but the next slide has a visual of what I was talking about. It's possible it still actually will, because I still think this idea is kind of cool, even if—well, I'm trying to tease apart what you're actually going to learn from doing this.

If you're trying to do the figure-ground separation issue, I agree you could learn something from that. So we're going back to that.

Basically, the idea was to cast prediction error not in terms of actual sensory input, but in terms of two models. One model comes from observations while the object is in one pose. I predict that if I grab it by the handle and move it, the whole thing should move to a new coordinate. What I observe is that one part stays and another moves. I take observations of both and build separate short-term graphs for each. Then I compare my observed model after the action to the predicted model. I have to learn that the predicted part wasn't where I expected, and another part appeared where I didn't predict. I need to learn the correspondence between those two. I can also revisit my initial observation and, with the new model, explain the error and correct it.

But isn't the only thing you can learn here that these are two separate objects that don't move together? Is there anything else to learn? It seems that's the only insight: these components didn't move when I moved it, and others did. I haven't thought it all out yet. I wanted to get some ideas on the table, and I thought this might extend to notions of object state, like the state of an object. That's an interesting idea. To me, that would be the interesting thing to explore—how you learn models of objects with different states.

That's a more interesting problem. I'm having trouble imagining it, but maybe that's just me. How do you do that without first learning a model of not just a temporary observation, but an active learning model? Maybe for a state, like a bookcase, you have a bunch of observations around the object, do manipulations, and learn as a constraint that all those observations move because the stapler, for example, is just a hinge. How you do that in a graph model is hard. Thinking about it, it's tricky. I have an idea how to approach it, but it's really hard. If you're doing more than just moving the object—if you're applying forces and the stapler snaps open, for example—you toy with it to see what it can do. But representing that in the model is challenging.

That's the hook: you learn the difference between two models. If I snap the stapler open, I build a new model and then do a model diff. Instead of the red cylinder that was here and there, I learn that the part of the stapler that's wide open corresponds to the part that was closed a moment ago. But in this case, with two cylinders, they don't correspond. They're totally different things.

Example on the board and their project. Those are how you do it. The back cylinder is not part of the front cylinder. Correct. You don't want to learn that as the behavior of a two-cylinder object. That's not what it is. I'm going to push back a little bit. In a very general sense, there are two solutions to this. You could say it is one object with the behavior that these two parts move completely independently. Well, they can't be completely independent because then it wouldn't be one object, unless they're somehow connected. What I'm saying is, learning that it's one object with total independence is equivalent to learning that it's two objects. The way the objects behave in those two models will be the same. I don't get it. I'm missing something. There seems to be a fundamental difference. A stapler is one thing and has a constrained set of states. I learn it as one thing, and I have to learn what those states are and how they transition. That's all part of one model. Here, I have two separate objects that can move completely independently in the world. They're not one thing. Otherwise, everything in the world would be causally tied to everything else, which can't be. Here, it's a totally different thing. What I'm saying is, this isn't one thing. These are two separate things. I can't build a single model of them because they're two separate things.

Jeff, I agree with you, but I'm trying to use the same type of thinking to explain object states and boundaries between objects. By learning that there's no physical connection between these two things, and that they behave and move independently, that inference allows me to separate two objects. I can differentiate these observations into object A and object B. I agree with that, but I'm extending it beyond that. I don't know that we're going to make progress until I think about it some more. If what you're seeing is the sensorial impression, there could be two potential hypotheses: they're connected or not. If I care about it, do I want to take an action to distinguish that? Then move on from there. If the sensorimotor system is giving you ambiguous information and you want to tease it apart, you have to take an action to do that. Otherwise, it remains unresolved. It's part of explaining what is in front of me and how the composition of what I'm seeing is. That's the object separation problem. In the light of sensorimotor ambiguity, you have to take some action to resolve it. Once you've done that, you know how to classify it and can move on to learn more interesting things about it. If you're presented with something that's a conundrum from a sensor's point of view, you want to resolve that. Otherwise, it's ambiguous. You don't know if it's connected or not.

This is a solution problem. He's trying to present a simple example. It is a problem of object separation. Where I'm having trouble is how this extends to learning object behaviors in a model. I think that's a different thing. That's my point. That's why this is a point solution. Ben said he thinks it's a way of getting to the latter, but that's a hard problem. I'm making that distinction. I'm pushing back on the idea that this is a general way of learning object behaviors. If the set of actions you have is limited to moving an object, that's all you can learn. Even if I can push it or do anything, the point is that there are more complex things to try to tease out the behavior. With increased richness of actions, you can learn more. But with these premises, I'm arguing you can't. I do not have a model of the object yet. All I have is a temporary constellation of features. I haven't learned anything yet, and that's the fundamental issue I'm having. You have to learn a model of the object before you can make predictions about its behaviors. This is a trivial behavior: I move it from here to here. The object hasn't changed. There is no behavior; that's just moving something from one location to another. The hard part is building a model when it's actually performing actions.

I don't see how that can be done. You have to start with a structured model to do that. You can't learn behaviors for something you haven't learned. What I would like to do is riff on something you presented earlier, which I thought was interesting. Jeff, I've taken your points to heart. I still feel an intuition that I might be able to get somewhere with this. I'd like to go back to the drawing board and see if I can make this idea work. What do you think the core of the idea is?

There's one core thing you presented that I liked and wanted to riff on.

Do you want me to go back to the slide? No, that's okay. One thing I like is the idea that what we're doing is predicting graphs from which we predict sensations, rather than predicting sensations directly. I think there are some interesting directions to take that. I appreciate that. Maybe we've thought about that before, but it's a very clear way of stating it. It's the first step on a hierarchy of abstraction. It could be. Suppose you start sensing some shape, and we build up a set of displacements in a graph. In our experience, we have a whole bunch of different models we've learned in the past, and now we're sensing here and can immediately...

All of us here can start predicting that there's actually a completion here. Based on everything we've learned in the past, there's a way of filling out this graph that we can already predict, and then we can start sensing it. Maybe now we start sensing it going this way. Now it's not this graph anymore. Everyone can probably predict something. Is it a prediction of a graph or an illumination of possibilities? Maybe it could be a union of graphs. Then you're not really predicting a graph; you're basing it on something else. You're not necessarily predicting a sensation directly, which seems harder—predicting a union of graphs or a specific graph based on everything. No, it seems a little more tractable. It's a subtle language, but it seems more tractable algorithmically to me. It's more than just language. I just thought that was interesting. I don't know if this is language or something else, but I like that idea too. I thought of it completely differently. To me, this is just narrowing down possibilities. I have some data, some evidence. No, I think it's more than that because it may be that, let's say you've learned about cars, and now you're seeing a toy car. It's a little bit different, distorted a little bit. So it's not exactly predicting a union of things we've seen before, but it's specific to what you've seen today. Do I see it as a car? I see it as a car that I've inferred from a previous graph, whether it's different or not. I'm assuming you're using previous graphs to do this. It's specific; it's molded to the observation. So what is the new model I'm predicting?

Either this graph, or perhaps it's the fact that there's an object—it's more about the shape based on what you've seen before. You predict a different shape, but the scale and deformation must fit what you've already observed. That's the distinction. If it's a toy car, but I've recognized it as a car, I've already mapped it to a graph by definition. Now, I might make novel predictions based on the fact that it's a toy car versus a real car, and it's smaller. Am I predicting a graph, or am I listing attributes? I have a model of a car, and this one is different, so I'm able to extend those attributes—like scale. It's small, not a real car, so I have to scale my model. My view is that we're actually predicting a new graph. I see that it has smaller displacement now. To infer that it's a car, I had to use an existing graph and recognize it independently of scale. I already recognize it's a car that's small. In order to predict sensations, you're not necessarily using the previous car at a different scale. This is a new object, so you build a new model. It's not my Lamborghini in the driveway. It's interesting to ask: are you predicting a new graph, or are you saying, "I have an existing graph of cars, and now I realize this one is small and different in other ways"? I'm going to create a new graph with these different attributes so I can recognize it as a toy car. I'm predicting a graph—a new one that's similar to the old one, with a twist. These are predictions based on my previous model of cars. I'm not sure I'm predicting a graph; I'm still making sense of it. It's a graph because I can predict new sensations of new displacement. Am I predicting a graph or new sensations? Displacement. I think we predict a graph, and from that, you can predict sensations. It's a subtle distinction. You're instantiating the whole thing. Now you can tell me, "What would it be here? What would it be there?" You're predicting the whole structure. Predicting a graph is sort of instantiating a graph. I need to predict something, but I already have to know what it is. We haven't sensed it yet. I'm not predicting something I've never learned before; that's impossible.

Here's a flip side: I don't think it's just about predicting sensory input. You don't really notice blinks in your visual stream, or the small micro-movements your eyes are always making. You don't notice the pauses. There are many examples of how the brain filters sensory input. I don't know that it's really comparing predicted sensory input to actual input. It may be comparing what I think the world should look like to the new model I have of the world. In the end, the only way to test your predictions is by sensory input. There is no other way to do it. You're going to get sensory input, and it's going to run against a sensory prediction. I could predict a graph that's like a hallucination—I'm predicting a graph and not checking it with reality. I can imagine it in my head, but I'm not really testing it. Maybe "instantiating a graph" is a better term. It's not anything you've seen before, but it uses information you've seen before. You can impute a new structure, but some core components of the old graph have to be transformed into the new graph. That involves attributes of displacement, which could be something completely bizarre, like a horse with the head of a man. Maybe you've never seen it before, but you can still construct the graph in your head and make predictions about it. It's a new thing, not a subtle change on what you've seen before.

That's a good example. I wouldn't even call it predicting a graph. Predicting isn't the right word. You can instantiate a new graph by combining two previous ones. I could make a graph that has my cell phone attached to this coffee cup if it was really glued there and I pick them up. That's the new thing. I want to live with it. This idea of instantiating new graphs completely, or a union of graphs, is interesting. I could imagine writing code for that; it's very clear. I agree, that's one of the problems—our graph.

I've made that example before: you can see a coffee cup that's a different shape, but you still need a coffee cup, and now you can remember that one uniquely. Or I see a cat that's like other cats, but this is a unique one. I need to be able to create a graph of that unique one somehow.

Can I run with the hallucination idea for a second? If we think we have stored lots of models of objects, like I have a model of everyone in this room, and maybe somebody gets up to leave the room, I could start predicting—not predicting sensory input per se, but predicting the location of Jack, who's going to be close to the door in a minute, as an example. That is basically a hallucination, but I'm wondering if that's actually what's happening and you don't bother correcting it until you collide with corrective information. How could you correct it until there's no other way to do it, right?

Yes, we can hypothesize about future things that haven't happened. I'm wondering if what you're experiencing is basically the hallucination at all times until the prediction isn't correct, and then you have to adjust. I think that's right. How else could it be? You're predicting, but none of it's real until you've tested it. You can't know; it's a hypothesis. I'm imagining that I would sense this if it were actually true, but I haven't sensed it yet. It's like saying, I'm imagining that when I reach under this chair, I'm going to feel these levers. Until I reach down there, I won't know, but I imagine it. It doesn't teach me anything. I haven't learned anything until I actually reach down and say, "Oh, what does it feel like? Are these levers there or not?" That's what the models let you do—they let you run through scenarios and imagine what would happen if you did this or that. You're not learning from that; you're not validating your model. You're just running through hypotheses and saying, "This could happen, that could happen," but I haven't updated my model yet.

Now, the idea I'm proposing doesn't have to do with utility. It's just an interesting idea that what you actually experience is kind of in the model world, and it's only in the sensory world when you pay attention to your sensory input specifically, or when your sensory input is not consistent. I agree with that a hundred percent. It's no different than me imagining that there's a mental logo on my coffee cup, and I'm going to assume it's correct until I rotate the cup and don't see it—then I know it's wrong. I haven't learned anything; it's just a model making predictions until you sense something. They're just predictions. You live in a hypothesized world, and until you get otherwise, you didn't experiment.

With virtual reality tasks, I first took a 360 picture of one room so I could look around and experience it. In a different room, I looked at the picture. It feels like you're actually in that room, and when you take off the glasses, you're in a different room. In room A, I took the picture; I observe it in room B with the glasses, and when I take them off, I'm still confused. You're in another room but wearing the glasses, so are you seeing a different room or actually seeing the first room? I'm seeing the first room. If you're just wearing glasses, how would you know what room you're in? It feels like you're in the other room. It fits—you're suddenly transported to the other room. It's a weird sensation. You adjust your place, and suddenly you snap back. Whatever path integration you did is being fooled. Maybe that's it.

Back to the challenge: how do you represent the degrees of freedom, for example, the state? How do you represent its behaviors? That's a hard problem we're working on. We propose a partial solution, but it's not complete. I don't see how this addresses that problem.

I have to have a model. The model somehow includes the behaviors of the object and the actions which made those behaviors happen. I'm still going to try it, though. I'm still interested if a solution exists. You also have others not physically attached.

There's no attachment to that. There's a cable somewhere. We know that.

It feels like you don't perceive it as one object. I don't imagine these two things in some constellation to each other in some position and say, "Oh, there's this one thing here." I build the behavior. It's more like if I press this button, there's a causal relation to some other behavior elsewhere. It doesn't feel the same as a stakeholder opening and closing. It might be, but it doesn't feel that way. Maybe it's just more of an excuse to approach the relationship, which is generally...

Probably. There's a relationship that is outside their physical bounds, and they flip a switch and see something happen. They'll sit there and flip it for quite a while—it's like magic. The fact that I can have this, they do that. I'll tell you right now, I'll do that just for spinning something on a rod when they're really young. It's like magic—they go, "Look at this, it keeps spinning." Something happened. My granddaughter's doing it right now. It doesn't have to be switches and lights.

She's learning some sort of action policy about spinning it, and also the fact that if she does it again, the same behavior is repeated.

We only have a couple minutes, so I want to hear what Lucas' point was. Can I go back?

I'm just saying, the brute force approach—when you call predictor, that's actually a model. You have a model, you get observation and action, you have next action, and then you use the error to update your model, right? That's the prediction you're proposing. We do have an error, but that error cannot be used for the model because you already know how to build the new model. So what's the point of the error? It's only there to allow you to update your predictor. But why do you need the predictor? I was thinking the opposite—the point is that you no longer update the predictor. You do the updates purely in the model space. But why do you need the error? Is the model something you just mentioned? I thought the graph was the model there, but there is no graph in that one. I'm looking at the bottom right. Oh, I thought you were talking about the one where you already know how to build the new model. The error is not going to play a role.

That error would be there for the predictor alone, which predicts given model and action. Why is the new model needed? I hear you—it's the exact opposite of what I had in mind. How can you use that error to predict the model? You can't. It's exactly what's on this slide. You have to learn the things you didn't predict, which you now see, are the same as the things you predicted that you don't see. You have to learn that correspondence to understand how it works. Go back—see the little green thing on the far left, go back to one side, please. This is exactly the step you have to take. You already have the graph, so you know what your new graph is like. So why do I need this? I see your error here, but why? Let's explain. You have two observations: the first observation from which you made the prediction, and then the next observation. Now I use both observations and update so that I have one model for both observations. That's what this error is supposed to move toward, right? But what are you going to use it for?

Instead of predicted graph and observed graph, from this I derive a unified graph that explains both observations. Maybe error is not the right term, but you replace this line with a unified model of both observations. Would you accept that? You might be right—it might actually not be doing anything. We have to think about it more, but I don't see a way to formally figure it out. 

Thanks, Ben. Thank you very much for all the discussion, despite the acrimonious conversation. It's good to do these things and it's good to argue with each other. I really enjoy it—absolutely.