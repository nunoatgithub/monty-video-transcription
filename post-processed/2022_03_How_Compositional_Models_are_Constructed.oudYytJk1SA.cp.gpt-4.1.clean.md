This new idea is actually a small change from my previous thinking, but it's difficult to describe. I'll spend a lot of time explaining it because it's also challenging to understand its implications. I'm excited about it because it offers a new way to address the issues we've been working on—it's like a new tool in the toolbox, a new twist that seems likely to open up many possibilities. I haven't explored all of those yet, so I'll describe the basic idea, and there will be a lot of details I don't fully understand yet, but I'm still very excited about it.

That's just a warning about what I'm about to cover. I'll start with a review of some of the model-building concepts to provide a basis for understanding this new twist. In this picture, I show a column, which you can think of as a module in the system, but I'm drawing it more like a cortical column: layer three, layer four, layer six. This is basically the model we described in the Columns Plus paper. The basic idea about modeling that we introduced in the first columns paper, and then modified in Columns Plus, is as follows.

You have, and this may be a little different than you've been thinking about it, but this is how I see it: you have essentially two inputs to model a module. You have a sensation input and a movement input. The basic idea is that you sense, you move, you sense and move, and you keep track of where you're moving. You can build a three-dimensional model of something. That's the basic idea, and it seems pretty solid—almost has to be that way. So many things fit that, but exactly how it happens is still questionable.

In the Columns Plus paper, and in some frameworks, we modeled it with two layers of cells. One layer receives the sensory input, and the other represents location and receives movement information. These are the two basic inputs to the column. One represents the location of the object, and the other represents—well, actually, they both represent the same thing, because the way we did this, you ultimately end up with a very sparse distributed representation (SDR) in each of these layers. In the end, that SDR is unique to the object and to the location on the object, and that's true in both layers.

As an aside, if you think about sequence memory, you feed in a series of observations and use time as the scale instead of movement. The SDRs in sequence memory are unique to all things in the world—they're unique to the sequence, like the name of a melody, and also unique to the location in that sequence. It's a unique representation for this element in this location and this segment. That's what these representations are in the column paper: we have an input that represents what we sense, but that's not unique, so we make it unique and sparse using the temporal memory algorithm. We also have a location that is not unique, like grid cells, but we've discussed a couple of ways to make that unique as well.

The basic idea is that you're trying to narrow down your possibilities of where you are in the world, and you have two ways of doing it, which interact with each other. Your location predicts what you should see next, and what you see next narrows down what you might be looking at, which then tells you your location, and so on. I put these two dots here to represent this basic mechanism: you have two SDRs, each can be a union, but each is being driven or narrowed down by a different type of reality in the world. In this case, we have movement and sensation—two separate ground truths. By using these two elements, you can narrow down and get to a unique representation of the input in this object at this location. You could argue that this is a representational location that's unique to the object, but it's also unique since both represent unique locations in the object. That's the fundamental premise.

We did one more thing in those papers: we proposed a pooling layer, a temporal pooling layer, where you could take these unique SDRs—each one classifiable, meaning each SDR could be identified as a sequence from any of these SDRs—and classify it as that sequence, because they're unique to that sequence. You could say this is unique to some object. You do a pooling between all these very sparse SDRs onto some stable representation—a many-to-one mapping. That representation is of the object you're sensing in different locations.

We know this occurs because we have stable perceptions of things. There has to be a stable representation—even though your eyes or fingers are moving, there's a stable representation in addition to that, which is, "Hey, this thing is still there, it's not moving, the world seems stable." That means there are cells that are stable, and we said this would happen in every column. So, you have two representations: one is stable and independent of the sensed location, and the other changes with each movement. We're generally only aware of the stable one; we don't usually notice our eyes or fingers moving.

We added one more point in that paper: if other columns are doing the same thing but sensing different locations, they'll also have a stable representation in their column. We can associate that stable representation with the stable representation in another column. So, you have these two very sparse representations, and if we learn to associate them, then during inference, they too will narrow down. There are three narrowing-down processes here: one voting between these stable representations, and two others—well, I guess that's another narrowing down, but there are three SDRs being narrowed down in different ways. It's like saying, "I'm narrowing down my possibilities because my neighbors or other parts of the brain are telling me, and I'm narrowing down through sensing and movement."

Are there any questions about data yet? That should be all for review, but it takes a while to sink in for everybody.

Alright, there's another twist to this.

We've talked about it and wrote about it, though we didn't know about it at the time. There are other issues. These models have an orientation—you can think of it like vertical, north, as we discussed. They can be tilted; your head can be tilted, and you have to compensate for that. These models were learned at some scale, but now you might be recognizing the object at a different scale.

What we proposed, though never written up, is that the brain has to compensate for both scale and orientation. We're just talking about vertical orientation now. The thalamus seems to have the right architecture to do both. Movement information and sensorimotor information both go through the thalamus before reaching the cortex. There is a strong projection from the cortical column back to the thalamus. We have a theory that the thalamus acts like a multiplexer, rerouting inputs to different outputs. It doesn't seem to do other things; it appears to do this uniquely. We hypothesize that it can compensate for vertical orientation. For example, when you tilt your head, everything still works correctly, even though the input to your brain is skewed.

Movements and perception are adjusted automatically. If someone wants a picture, that's fine. I'm going into this hypothesis now. There's a lot of evidence suggesting the thalamus does both of these things. Scale is related to the oscillation frequency between these two elements, and changing that frequency would change the scale of movements and perception. I won't go into detail, but that seems to be what's happening. I mention this because it's important.

We have this model, and we're trying to narrow down what we do. For movement and sense, we narrow it down by alternating between two limiting possibilities. We can also do it by voting—not necessary, but it makes things quicker. I believe the system also has to figure out the scale and orientation of the current presentation versus the model as it was learned. I'm ignoring displacement for now; it can be added on top of this discussion without a problem. So I'll stick with the model we had in columns and columns plus.

Are we all on board with this?

Alright, thank you. Small questions, please. There's no arrow going back from the layer 2/3 neurons to layer 4 neurons. The classic anatomy does not show this projection going back. I don't remember if recent papers say otherwise. In the columns paper, we assumed there was a connection going back. I think we found slight support for it, but it's not well documented. We do know there are strong connections from layer 2/3 down to layer 5, so there is feedback, but it's more complex. I don't fully understand it. It's a good point, Ben. This information does flow back into lower layers of the cortex, just not to layer 4. There may be slight evidence for it, but there is a strong connection between layer 3 and layer 5, so feedback exists in a complex way.

Let me post these things to the slide quickly. I'm trying to stick to high-level concepts because the new idea is simple at a high level.

People need to understand this before they understand what you wrote in the document. That's right. Going over this again and again helps, even for me, but it really takes a while to sink in.

What's going on there? I have one more basic question and need a brief review. In the pooling step, can I think of that as a sum or a union?

I don't know what those words mean. For example, I have a bunch of SDRs in layer 4, and they're different at different points in time during the sequence. Can I think of layer 3 as the union of those? It's not the union. No, it's not the union of them. Go ahead. It's a single SDR that's stable, but the synapses on each cell form a union of the temporal memory representations. If you have all the different unique representations in layer 4, a single cell has to recognize all of those and stay stable during that. At least that's how we implemented it. So it's a union in the synapses, but not at the level of the layer 2/3 representation. I'll try to show that visually. You have an SDR here—it's very sparse, which is good. This does not change as you experience more parts of the object. You're taking this SDR and forming synapses on these cells to this pattern. Then you take the next SDR and form more synapses on these cells to that pattern, and so on. It's mapping many SDRs to one SDR. There is a limit to how much you can store this way, because in theory, you could try to store a lot of these patterns, leading to many synapses. But this does not become a union of these; it's literally...

If I put this input in, I should get this SDR. If I put this input in, I should get this SDR. If I had a backward projection, this SDR wouldn't associate with all of these, but that doesn't occur directly here. Many to one makes total sense now. It just doesn't change as you train more. I have a question about the thalamus function. Is the idea that the thalamus corrects all the rotation, and at the beginning, we don't know exactly what the rotation of the object is? That would be a mismatch until there's feedback to the thalamus to correct the rotation until it matches. We've never worked out the details of that.

But yes, it has to infer that information, just as it has to infer everything. There's some process to narrow down the right orientation and scale, but we've never worked out the details. It looks like the thalamus is capable of doing those things. There's a very large projection from the cortical column back to the thalamus, meaning many axons and many synapses. It's a classic associated memory type of thing. Somehow the cortex, that column, is going to work with the thalamus to figure out the right answer, just like it's working between other layers to figure out the right answer. That would be a good exercise to work out the details, but we haven't done that.

You'll see why I think it's important in a moment.

Any other questions about that? This is just the first layer of the hierarchy, the first level. This is just one region, one layer, one column in a primary sensory region. The other layers—let's go with that. Now, that's a good segue to number three. We're going to talk about two columns that are hierarchically arranged. I labeled those R1 and R2, region one and region two. I've had an assumption for a long time that I think is wrong. The assumption was that we know layer three in R1 projects to layer four in R2; that's a well-established fact. I didn't show it here, but R2 also gets input from the eyes or the skin or whatever. I can put that aside for the moment. It's very clear that layer three here projects to layer four. You'd say that's the input to this guy. You've learned something here, and now the object becomes the input to this model. Instead of passing in the raw sensory data, we're passing in some processed object that we've already recognized, and that's being sent here. I wrote here that I assumed R1 to R2 was a stable SDR, the object recognized. That just erased something—an object recognized by R1. The idea was, if I recognized a letter here, I'd be passing that letter into this guy, and this was stable over movements of R1. The anatomy suggests that's what it is because of this layer three to layer four projection. I always thought that was the way, that was the whole idea of hierarchical temporal memory: temporal pooling, and then more stability as you go up.

But I think that's wrong. There are a lot of problems with that. I was thinking about the logo on the coffee cup problem and other problems we've discussed about distortions of objects, how you can lay displacement on top of a morphology, and all these puzzling things. The basic problem was, if you just say this stable representation of whatever I've learned here is the input to this, it doesn't work. I wish I—how did this get erased? Did I do that? The eraser slid down. Did you take a picture before? The picture is with the eraser half obscuring it. Maybe I bumped into it or something. I need to—what to predict fails?

I'll say with multiple—oh, no, the picture is missing. I don't know what I wrote there. It might be cases: curved and arced, angled logo, curved and arc, many cases. Thank you. There we go. Puzzle picture.

I can't imagine how it did that.

Here's the thing. I was struggling with this. I imagined that R2 is getting this input: I've recognized this letter A, and now it's part of a word or something like that. But all the different R1 columns—there could be multiple R1 columns—each has its own little space absorbing this thing at different places. Each one's going to have the same stable output, but they're going to have different representations here. If I'm in this R2 region and get this input, and I want to send feedback back to R1 to tell them what to expect, I don't know how to send anything back. I've lost my location information. I can't say, "You're supposed to expect this, and your column's supposed to expect that." Every time I tried to work out how that would work, I got totally lost. I could never make it work.

It also doesn't solve other problems we talked about: the logo at an angle, the logo at different scales, the logo curved around the cup, the logo in an arc. There are all these examples where I had a model here and tried to apply it to a parent object, but none of it worked because everything was distorted. I can't just take this logo in its normal form and plop it on this—it just didn't work. This is where we had the idea of taking the displacement and wrapping it around the morphology or something else. The very simple solution to this problem is the following: just don't assume, no longer assume, that what you're passing here is a stable representation to the higher level. By the way, I said layer three projects to layer four, but remember there are a lot of different cell layers in layer two and layer three. Some people have identified at least four different types of cells in here.

Just because someone says layer three projects to layer four, it doesn't mean there aren't layer three cells or layer two cells that are not the stable representation. It's a simplification. The basic idea here is that R1 does not want to send that stable temporal pool of representation. It wants to send its unique SDR right now—here's what I'm sensing at this moment, the instantiation of what part of the model I'm seeing. If I'm looking at the logo, I would say, I'm looking at this letter in the logo, but it's really a unique SDR for this location in my model. When it's passed here, this region has no idea what it means—there are no letters or anything in the past, it's just an SDR, a very sparse pattern that comes in. This region is told that at this location in its model, there's this very sparse pattern representing what's being sensed by the other region.

I can then associate this unique location here with the unique location there. If you think about the unique location, it's like a node, and now I'm associating some pattern with that node. There happens to be a node on this model, but this region doesn't know that. It just says, okay, something unique is coming in—an SDR I'm going to associate with my location, and that association works both ways. When I get to this location, I can invoke that same unique SDR back over here through associative learning. When this region is moving around—let's say it's the coffee cup on the logo—it can tell the other region that at this location, you should be seeing this very specific pattern right now. It's both the location and the object: you should be at this point on this object and know what to expect from a sensory input. The other region might not know what that means, but it should be at that point in its model.

So, R2 is going to tell R1 what location to predict. R1 knows the details of that particular logo—it's the unique location, the state that's unique to the object and location. Given where I am right now, when we went through this—let's say it was a logo we trained on earlier—when I was at this location, you passed the pattern. I might now say, oh, that pattern's an "N," for example.

I said that was a pattern, and I'm going to associate that. When I get back to you, I'll tell you what that is again. That's what you said it was, that's what you were doing when I was here. This region can say, I'm here, and you can use that information to validate any models we have over here. I'm seeing this part of the logo right now, and you can use that to narrow down where you think you are on whatever model you're making. It's just another example of SDR-to-SDR mapping. Now we're going between two columns at different levels, assuming they're representing different spaces—one model of one object, another model of another object—but we're doing the same thing, passing information both ways and learning the association. Whatever I think I'm seeing here, I can vote for all the places you might have seen the logo over there. If I'm at any place that had the logo, like on the cup, and there was an end of allocation, I'll tell you that you should be expecting that over here. These could both be unions—if there's uncertainty, they can narrow each other down. It's the same mechanism of narrowing down between unions, but now it's hierarchical, narrowing down the state representation. The state representation is not being passed between these; it's narrowing down the point here.

Layer six is basically helping these regions resolve their state. The cells that project from the upper region to the lower region come from layer six and project up into layer one. Cells in layers five, three, and two all have their apical dendrites in layer one. In essence, the upper region could be enforcing or suggesting to all these cells simultaneously the state they should be in. It can be more complex—there might be multiple things going on—but basically, it could be narrowing down the stable representation. I didn't make that distinction; I'm just saying the implications are complex. I'm using a unique location here and pushing it back to help figure out where you are. It's not just "oh, it's a logo"—that's not sufficient. I need to invoke the specific state right here. This is the part of the logo you should be seeing, because when I'm at this point in the cup, that's the part I saw. That's what you told me you were seeing, so I'm going to tell you to see that again.

There's one more twist on this. R1 and R2 are not the stable SDR, but a reference to a mutation on the child—those are parent-child relationships. To solve the problems we know we have to solve, you also have to share the orientation and the scale, because the orientation and scale of the logo can be changed independently from the orientation and scale of the cup. I can have a small logo at different angles.

So that has to be part of what's recalled here, but we're doing it on a point-by-point basis. Imagine if the logo was in an arc, like in a quick curve. The model of the logo doesn't look like that, but the cup can say, at this point you told me you had an "N" at this angle, and then you have a "U" at this angle, and so on. That's what this system would be seeing. It may not even know that it's recognized the entire logo yet, but it's basically seeing this "U," and it may be part of the logo.

The point is, to get this to work, you have to share the orientation and the scale between these two systems as well. You have to say, I saw this pattern, I was looking at this pattern at this orientation and scale. The other system says, okay, I'll remember that. Then it says, at that point, I'm going to tell you, you should see that pattern at this orientation and scale. Maybe you could figure this out, maybe it doesn't have to be passed. I don't know. The clearest, cleanest implementation would be like that.

The question then is, is this possible? Are these kinds of patterns sent back and forth? There are multiple cellular layers that go between regions like this, and I have to go back and review the literature. We've read it many times, but I haven't looked at it in a few years. The Alex Thompson papers were the best a few years ago. I want to look at them again because I want to go very carefully through what kind of projections exist between these two systems. Maybe we don't have to send this, maybe it's inferred, but I think in the general rules you want to send this. You certainly have to send something about the orientation. I don't know if you do it, then I could try to figure it out. You could just say you should be looking at the end on the logo, and it has to say, okay, if I'm looking at the end on the logo, maybe I have to figure out the orientation and scale on my own. I'll put this as a question mark.

There's only a goal to the talent as well. No, not in this direction. No, backwards. Not in this direction. There are complications. Are these layer five cells, which are typically layer six? There are layer five cells here. These are the motor output cells. They also go into the next region, but they go over here. Those are the ones considered like motors. This is like the movement command. It's been going back and gets modified again. It goes up and back. It's complicated. As I said, it's really complex.

The idea is very simple. I'm not sending a stable representation between R1 and R2. I'm sending the unique representation between R1 and R2, and that looks like it will solve all the weird things we could do with child relationships on parents. It can solve curves, arcs, roundness, and any kind of weird thing I can think of. We're not trying to learn the whole logo at once. We're just trying to say, I saw this thing, which is part of the logo at this location. Later on, the logo looks like it's tilted at this point or whatever, and I'll associate that.

This means that R2 has to see every single location on the cup that the logo is on. But that's not Excel. I wrote about this and then wrote that. I don't think we can expect that.

There would be a solution to this.

For example, I don't know the solution, but I have some ideas.

Why can't R1 just send the location in its reference frame? The details of what's actually being sent—no, it is sending that. What I mean is that R2 just has to understand the relative location and orientation of the two reference frames. That's just one quantity. But that only works if the logo is not deformed in any way. No, it still works. Even a deformed logo has a reference frame, so you still have a location reference. The reference frame will change.

That's R one's job. R one's job is to represent that. R two just needs to say, where on this object are you at this point in time? That way, it doesn't need to learn all that intricate mapping for every single compositional object. I can't get that to work super tight. I think that's not going to work, but I could be wrong. Here's what I wrote: it'll work. Imagine instead of a cup, I just have this flat surface and I'm putting the logo at some location, at some scale, and some orientation. In that case, all I have to do is learn one point. If I associate one point, then everything else follows. Now, I can just tell this guy, here's a point where I'll give you your orientation, your scale, and your object. From now on, you can calculate everything on your own. I don't need to store all these points. Whether I store one or a few doesn't really matter, but I think when you start having the logo do weird things like the arc, I don't know how that works. Maybe you see it. It seems to me like there isn't a way to tell you the orientation of the logo because at any point in time, these weird things are going on with it. There isn't a way of assigning the entire logo to this new space. I can't do that. That space still has some relative orientation and scale with respect to the cup. It may be different from the original logo, but it has something. It's still a single association, but I'm not learning a new model of the logo here. If I'm not learning a new model of the logo, I'm using my old one. How is it that I'm able to— even here, it would have to learn it because R two has to learn all of those new locations that correspond to the current moment. In both cases, it has to learn it at a very detailed level. In one extreme, all we're doing is associating some part of the logo with something here, some part of the logo here, some part of the logo here. From this guy's perspective, he doesn't know what the logo is. He doesn't know that it has a current state; he doesn't know anything about it. It's just saying, these are things you told me when I was here. R one, this is what you told me you were seeing. I don't know how you can solve the basic thrust of what I'm thinking here. I don't see how you can assign the entire logo in these deformed states. It feels like you have to be able to deal with the fact that you're going to have to map parts of the logo to different orientations. I agree with that. Now you have even more learning to do because for every object that curved logo could be a part of, you have to learn all of these detailed associations. Whereas if you just have to learn about the curved logo once, you can apply it to any hierarchical composition. Now you have this n squared problem of every object it could be in. If I put the curved logo on a t-shirt, I have to learn all of those associations all over again.

I think there's a solution to this, and I tried to describe it in some detail in the writeup, which is the following. If you did learn every location—remember I was talking about the circle? Let me give the example there: circle.

How many points do you have to sample going around the circle? There are really no salient points.

What I was arguing is that these SDR properties, as you move in some direction on any object, the SDRs representing the locations change slowly, and at some point, they change completely. Imagine from here to here, there's a slow-changing SDR, and now all the bits have changed. Again, move slowly, and now all the bits have changed. Move slowly on, all the bits have changed. This is true for both the location and the sense features. It can change rapidly, but in general, they change slowly. My point was that if I want to learn all the points along here, I don't need to sample all the points. Any point between these two points will be correctly represented anyway. As long as I don't go too far—this is an SDR, this is an SDR, and in between them, there's some shared bits, so they haven't changed completely—then all the points in here, if I learn this SDR and this SDR, all the points in between are also learned simultaneously. If you point anywhere here, you'll get the correct SDR. You don't have to sample everything. If I trained myself at different points, at some point you have a similar amount of memory dedicated to that, and if you sample more points, you're not going to get any more memory. It doesn't need any more information; all the information is already captured. In fact, you can infer it from new locations you never learned, as long as you're relying on this sort of continuous SDR property.

To me, that's going to be the solution. If I take a temporal memory layer here, or any one of these layers, if I have a continuously changing location and a changing curvature—imagine the circle, with a gradually changing curvature—I can learn the curve without sampling every point, and yet every point will look like I learned it. Every point will be correct, even though I didn't sample it. That limits the amount of memory to store. You don't have to store a unique SDR for each location, because as you go around, you're only adding a few more bits to different neurons as you go. It's not like I have to store an SDR here, an SDR here, and an SDR here. It will naturally fill in the spaces in between. I'm working on the hypothesis that this is how it will get resolved. It's also interesting to point out that, as I said earlier, there are some situations where you can actually capture it all at once. If it's just a logo at some orientation and scale, one point will do it and then it'll be good. R one can infer in between. If all we have is one point, R one will guess in between and say, I'm going to use my model. But if previously it was learned in the context of the cup, then R two can say, no, at this point you really have a different orientation, different scale, or that's what it should be.

I realize there's a memory problem here, but I think there's a way around it. Another example I gave is the logo goes horizontally, which is easy to learn, and then it starts going vertically. Clearly, you have to learn something about the logo changing at this point; you can't just use the old model of the logo.

This angle could be anything. I could learn it, and it's simple to learn. There seem to be an infinite number of permutations on this idea that show I can't just rely on the learned model of the logo. That's not going to work. I need someone else indicating, "At this point, you should see this," and so on. The example is that you have to learn something new when you see something new like this. There's no way to do this without memory. The question is, how much memory will it take, and how will it be done? Do you have an alternate solution to this?

I didn't understand what you said. I was thinking you do need to learn these continuous representations of the circle and the angle one, but I thought R1 would learn it, not R2.

That would be like a new object or a variation of an object. For example, if we have a new curved logo—Christie decides our logo is going to be curved now—I can put it on our letterhead or a notepad and instantly imagine what that curved logo would look like. I don't need to learn all those detailed associations. I can put it on a t-shirt, different types of coffee cups, or our door, and all of those can be learned instantly. I can represent, predict, and reason about it instantly without learning all these detailed pairwise things. I agree, I would learn that. I would say, "Oh, there's a new version of the logo," just like there's a new version of the coffee house. But there are many examples where that doesn't make sense. If I put the logo on a flag, for example, I can't learn all the undulation models of the logo. I might have the morphology of the flag represented in R2, which at any point in time could be folded, but R1 can't learn, "Now the logo's folded this way, now the logo's folded that way." Another example is the curvature on the cup. When I learned the logo going around the cup, I don't learn a new logo. I don't think of it as a three-dimensional curved logo. I don't even perceive it that way; it's just like the regular logo on the cup. I would argue that's a case where you didn't learn a new logo curved on the cup.

If you don't perceive it that way, you think it's the same old logo; it just happens to be wrapping around the cup. Maybe there are two different examples: one is morphings of the logo that are not permanent, just fitting to the form of the secondary object, and then there are other things like the arc, where I would learn that, and I agree with you. I could learn that, but the other example, I don't think I would learn. I just have to deal with it. I'll have to think about that. As I said, the implications are complex.

That's it. This simple idea: don't send up the stable pattern, send up the changing unique SDRs. That's what's going on between level one and level two. There's another union narrowing down process.

In hindsight, it almost seems obvious that something like this has to happen, and I gave the reasons why I was mistaken earlier.

Does it also send up the stable pattern? I don't know. Does it need to? If R1 and R2 are both recognizing the same cup, then it would make sense. I don't know what R2 would do with the stable pattern.

I don't know.

It's a good question.

Maybe that's an orthogonal thing. If it needs to vote on it, then you would need it. If R1 and R2 are modeling two different things at once, then there's no voting. If R1 and R2 are both modeling the same thing for some reason, just looking at the coffee cup and its morphology, and they're both trying—Is it the same thing? Different layers of the hierarchy probably have different views of the same thing, not exactly the same. In this case, I'm arguing that R1 and R2 are invoking two different models. R2 is invoking the model of the cylinder, and R1 is invoking the model of the logo. They're not the same thing at this point. R1 doesn't know anything about the cup or the cylinder; it's just looking at the details of the logo. It sees that it's a logo, while R2 is looking at a larger picture. Now we're getting into a weird area. The question is, I think there are still two different representations. Remember, I said I'm going to ignore displacement for now. I think it's possible that each model has a morphology model, which is basically learned by the contour of the object, and then it can have specific features at points on that morphology. Where am I going with that? At this point, I was assuming R2 is just looking at the morphology, running the morphology of the cup, and now at some location on the cup, it can associate with this feature over here.

I'm assuming this guy is just modeling two different things.

In this case, it's hard. The question is, could a column—

In this regard, you might argue that a column just starts by modeling morphology and then assigns whatever pattern it gets from somebody else.

In the primary region, it's going to learn the morphology of something and then determine that it's not just about the shape or outline. Now, some sensory inputs are more associated with specific points in that morphology, such as the color or the shape of an edge. It's confusing, but my next step is to figure out how to bring back the idea of displacement and morphology and build the whole model.

Here's your graph, your morphology model, and at any point, you can assign an observed feature to that point.

I have a question regarding the back projection from R2 to R1. Is it the SDR of the part, or also the SDR of the part and its location? It's both. Those SDRs in layer four and layer six are unique to the location and the object. They're unique to the location, but does it require the actual location information itself, since these are two different reference frames?

You could say, "I make a prediction at this location. I expect this part, and in your language, this is the SDR for that part, so I send that information to R1." The part is synonymous with the location; you can't separate the two. I was arguing that in layer four and layer six, you might think one is the location and one is the feature, but when you get down to the SDR, it's unique to the location and the object.

It's the SDR for the location in the R1 reference frame. In this case, I'm just talking about R1. What I send back from R2 to R1 is actually encoding the location in the R1 reference frame. It's basically sending back this SDR, saying you should invoke whatever you need to based on this SDR.

It's the same kind of SDR sent to R2. Statistically, it's the same one. There's a unique SDR here and a unique SDR there, and this says, "I'm passing you this." I'm going to associate your unique SDR with my location, with my unique SDR, and you can associate your unique SDR with mine. I'm going to bias you; you should probably be biased to the same SDR you had before. It's just a simple associative link between two first large representations. It's not the exact same SDR; there's a one-to-one mapping between them. The arrow goes this way and that way. That's a great point because people get confused about the voting here. The voting between columns, the SDR in this column and the SDR in that column, are not the same. They're different SDRs, but they're associated. Just like here, this is a different SDR than that one, but you associate them. That's an important point. This mechanism never assumes there's the same SDR anywhere. There's no way for the cells to know that. It's just saying, "I have a unique SDR, you have a unique SDR. I'm going to get your pattern to help you narrow down your pattern, and I'm going to give you a pattern to help you narrow down yours." That's the same thing happening here.

When we were talking earlier, I think this is a lot more data than I originally assumed. Every location along that object in R1 has an SDR associated with it that's sent to R2. It's a very intricate mapping between these two. I was with you on this initially; I thought this can't be right. But the more I thought about it, I couldn't see a way out of it. It has to work this way. Then I started thinking about what mechanisms would let it work this way. I could be wrong, but my thought process led me to conclude that to handle all these cases of objects on objects and all the different complexities, I had to come to this idea of a very intricate mapping between these two regions.

I think that 3D curved logo was a good example. We haven't learned that 3D curved logo individually; it's just a flat logo to us, but it's wrapped around the cup. We have to associate that somehow. The way I think about it is, looking at this cup, I can only see part of the logo right now. Some columns are looking at part of the logo and recognize it, even though they can't see the rest. I can recognize it by these local displacements. If I'm looking at the obscured logo, I only see part of it, but I identify it uniquely. Then I rotate the cup and see a different part of the logo. The same column will say, "I recognize the logo; now I see these displacements." The rest is obscured, but I still recognize it, or at least have a good guess because of the mental model.

I don't recognize the entire logo at once on the curvature, and I don't have to. I just need to recognize a part of the model, and the rest I can't see. I still narrow it down and think, "I'm on the logo on this little section." That's what this column in R1 knows at this time. The challenge of wrapping around the cylinder is avoided. Right now, on the surface of the cup, I don't have to worry about where it is elsewhere. I'm just sensing this part, right there. Then I rotate it again, and it's right there.

As far as the R1 column is concerned, it's just looking at an included part of the logo. The curvature is irrelevant; I don't see it.

It solves the problem because you don't see the whole thing at once. The column just sees a little bit, and as soon as it's tangential to the cup, that's all it needs to know at that point. The entire curvature is encoded in R2. R1 doesn't know that, but I agree with you. I think we would learn logos if we saw them. That's a good distinction. There are some changes to the logo that I would learn very rapidly.

I've got this piece of the discussion. There's a request for my own understanding, but I'll let that go and continue with this flow. 

So, R2 represents the SDR for the logo part in its reference frame, like the curved surface of the cup. Be careful—R2 does not know anything about the logo itself. It just knows the SDRs; it says there's an SDR I'm associating with this location. It doesn't know the logo, but it has the corresponding SDRs of the logo. It just says, "I have some SDR; I don't know what it means, but I've got it." It's the corresponding SDR on the logo in its own reference frame. So, is R2 separate from another model that would learn the cup?

I was using the word "cylinder." If I assume R2 is learning the morphology of the cup, then the morphology could include a handle. I'm not sure what your question was suggesting—whether there's a cylinder in a cup. I'm going to keep on the path that there's a morphology model, and associated with the morphology model are specific SDRs that represent the output of something else. If the cup is defined by its morphology, then I would say R2 is handling that. There's no separate cylinder and then a cylinder with a handle. Maybe I could learn that, but I wasn't assuming that here; I was assuming a continuous morphology.

So, you're thinking that R2 could also learn the cup itself, recognize the cup? I'm assuming it's learning the morphology of the cup in this case.

Ben, I have a basic question that I think will help concretize my understanding. I want to go back to what you said about the circle and having SDRs at different points on the circle. There was something about interpolating: I have an SDR associated with this point on the circle, and another with a different point. Let's say I now land on a point between the two that I've never seen. Walk me through what happens.

Just imagine you've got a dendrite. I wrote this up. Here's a section, and I have some SDR I want to memorize, so I'll pull in 20 synapses, and that's going to represent some SDR—let's say it's a location SDR. Now it's coming out of this thing down here or something like that.

As I start moving, I'm at some location in the world, and as the thing starts moving, some of these bits are going to go away and some new ones are going to appear. That's what's going to happen. It's like a bump of activity, and you're moving the bump in some direction. Some new things are added to the bump and some things fall off. It's always a set of cells that are active—a bump of activity. As you move a little bit, many of them are still active, but a few new ones are added and a few go away. This bump can move in many different directions. If the bump moves in one direction, some bits fall off; if it moves in another direction, other bits fall off. If there are 20 synapses, sometimes there are 20 dimensions you can move in.

Just assume we're moving. At some point, I'm adding new ones and forgetting old ones. At some point, I've replaced the last of the old ones and have all new ones. Does that make sense so far? You're looking puzzled. These are dendritic segments—no, this is one dendritic segment. These are synapses? Yes. There's an SDR up here representing location, and let's say 200 bits are active. As I move, this segment is going to form 20 synapses, subsample this thing, and get 20 synapses. That's good enough to recognize that 200-bit pattern. This is the fundamental SDR method. This is a robust representation of that pattern. If I see these 20 bits active, I know I'm at that pattern. As the location changes, these bits come and go, so some of my synapses will become inactive, but I'll be able to form new ones because new cells become active.

As you move the location bump, if you move it smoothly, these bits will change smoothly; you don't change them all at once. They'll be changing one at a time or so, and which ones change first depends on which way you're moving. As I change the SDR up here, there's a cascade of things: my location in space, say of my sensorimotor, as I move it, the SDR representing that location is going to change. All those synapses have already formed under that dendritic segment, but as the set of active synapses changes smoothly with the SDR, I can form new synapses because new bits are active. Some synapses won't be getting any input, but there will be new bits that are active, so I have the opportunity to form new synapses. That's on a slower timescale.

My question is, at inference time, I know the morphology of some object, but now I'm sensing a point I've never seen. Now I have two sets of 20 synapses here and 20 synapses there. My argument is the following: imagine this represents a point in space and this represents another point in space. Anywhere in between here, between this point and this point...

I will recognize, I will infer, because it will be some of these bits and some of those bits. In fact, what I argue is if I just form these 20 and then form these 20, it will automatically infer between them, as long as these two points are not too far apart. It will recognize any pattern in between these two equally. It's actually inferring; it just says this represents the entire span from here to here. This dendrite will pick up anywhere along this span. In fact, this dendrite can't tell the difference between these points; it simply turns on. Remember, there are a lot of neurons here—this is just one neuron. There will be other neurons doing the same thing, and the population, imagine it's like the space in the temporal memory. Some minicolumns are active as you move. I have two inputs coming in: the sensory input and a location input as they move. The minicolumns will change too. A single neuron will not represent all points. At some point, this neuron turns off and another neuron comes on because this minicolumn is no longer valid. This neuron will learn some amount of distance across here. Now, imagine you're looking at the actual output of the temporal memory layer. Some cells are active. As these two inputs change—the location and the sensory input—if you go smoothly through space, the active cells will change smoothly too. In some sense, you can then represent an SDR that covers all points between these two points. I need to work it out on a visual example somehow. It's a little weird, I know. The only point is, if you don't sample too far apart, then all the cells will likely span a range of inputs, but collectively, they'll still be unique. I don't need to learn or train on all the in-between points. At any point in time, I can come back and have some pattern. If it's one of these in-between points, it will recognize it correctly. I don't need to train at every point.

Does this only work with spatial interpolation, where we know how movement corresponds to changes in the representation? You said there are essentially 20 dimensions in which we could be moving, and if you have two overlapping SDRs, there are many possible ways to interpret them. Does this only work if I know it's like a sliding window?

I'm a little confused by the comment. First of all, it only works as I've described if the SDRs are changing gradually. If they're changing suddenly, there's an overlap. If I sense from one location and jump to another, I can't interpolate in between. It requires smoothly changing inputs. If the inputs aren't changing smoothly, it takes more memory. You can still learn it, but it takes more memory. That's one thing. It doesn't just apply to spatial things or movement; it's just a property of SDRs. If the SDR is changing smoothly, like in the example where my SDR represents the curve at some point, as I move around, the curve will change gradually. If I get to a point where there's a sudden bump, like a coronavirus, the SDR would change suddenly and it would take more memory. I wouldn't be able to interpolate at that point. You have to recognize this change.

I think it doesn't have to do with locations or movement. But I found it interesting that, if I'm representing location, how the SDR changes between two points is determined by the direction you're moving.

Is this basically the brain's way of doing spline interpolation? How many points do I need to fully specify a function? If the function is smoothly varying or linear, I only need one or a few points. The function here is exactly the morphology of the object. If I'm trying to learn a really complicated function with lots of wiggles, I need a large number of points. I think that's right. Although I don't think you can represent, with a slide, I might be able to say, here's one function that represents the entire circle, or something like that. In this case, you still have to learn a bunch of points.

I think it's a good analogy. The next question would be, as I make small changes to where I'm sensing on this object, the SDR produced in, say, layer six will either be the same as it was if I'm slightly over, or different because a couple of neurons fall off and a couple come on. As you move, if you're able to detect the movement, if it's discernible, then something has to change. Some bits will fall off. If the movement is small enough that none of the bits change, you can't detect it. Now I want to walk through what happens if I make that small change and a new SDR appears in layer six of R2. How does that percolate back and affect R1? If R2 is moving smoothly along the surface of a cylinder, it's going to be changing continuously here.

Here's a conjecture. It's not proven. My conjecture is whenever you have two SDRs associated with each other, if they're changing smoothly, it's much more memory efficient than you think. If they're not changing smoothly, you have to store more. If I'm moving smoothly here and over here, it shouldn't take a ton of memory. It's a general property of associated SDRs: if they're both changing smoothly, memory storage is fairly efficient—not zero, but efficient enough. There are a lot of synapses between R2 and R1 in the brain, so there's a lot of memory there. In fact, people write that there's more storage up in these apical dendrites—it's huge. So there is a lot of memory, but we can't train on every point. That's just not possible.

If I train at every point, for example, if I trace my finger around here and sometimes train at every point, I can't create a huge amount of memory. My point is, I'll still store the same amount of memory if I go like this anyway. That's a general property—I'm arguing, it's a conjecture. There's a general property in T SDRs: if they're both smoothly changing, then it doesn't require a lot of memory. The conform of those two is also smoothly changing. 

If R1 is now representing, say, we're in layer four of R1 and I've got an SDR associated with a feature on that logo, like the "U" in "Menta," I make this as a simple exercise, but it's worth going over. If I change the position of my sensorimotor a little bit in R2, this changes the SDR in R2. Now I come back to R1 and I've got different inputs biasing my dendrites. I'll have the same effect you just described: some neurons will fall off, some will come online. Instead of the "U" at "eda," now I'm getting possibly the "M" or something smoothly interpolating between the two. I'm moving across from the "U" to the "M" and so on. 

Is there a question in that, or is it just walking through and understanding that loop? I think that's right. You have a continually changing representation of your location here, and you have a continually changing representation of your location here as long as you're moving smoothly.

By the way, this may not be smoothly changing if the sensed feature changes dramatically. Imagine I'm going from, for example, a curvature that moves smoothly. In some sense, imagine the feature is not smoothly changing anymore—suddenly, you go from one to another. If I'm detecting curvature or tangential features, and there's a 90-degree turn, it's sharp. That means the representation over here would change sharply at that point. It could be that the actual morphology changes smoothly, but the color on the object changes radically at one point. There are multiple dimensions. The more unique things that are happening, the more memory it's going to take to handle it if you want to learn it. 

When we learn a lot of objects, we don't learn them intimately well. Our concepts are often very fuzzy. Only when you study something very carefully do you actually learn exactly what it looks like. You saw that with the HighCo examples when he was trying to draw things. I do that with bicycles. I've thought a lot about bicycles, so I can draw a pretty accurate picture of a bicycle. Most people can't, but you have to really learn all the details—how does it turn, where does this come out, what's horizontal, what's vertical, that kind of stuff. We don't necessarily form all these synapses, even if we're scanning an object and moving around it. I may not form all the synapses.

We don't do it; we don't learn all the details. But if I really work at it, I can learn more details. I can learn the exact things—how they change, curve, and so on. That's another variable. We don't have to store every little thing, even if I scan over the whole thing. But if I really focus, train myself, and pay a lot of attention, I can learn all the details. Often, we don't. The point is, if things are smoothly changing, it doesn't take a lot of memory. But if things change sharply—like the feature or color changes sharply—then it's going to take more memory at that point to learn it. It's logical that you can't learn the detail without learning something. 

Can I go through one quick example to understand this? If we have smooth changing of the SDR, but we take away the assumption that it's ten adjacent neurons—let's say you have ten neurons active in the beginning, then you turn off one randomly and turn on another random neuron in a hundred possible neurons. At the next step on the circle, you turn off another random one and turn on another random one. Why would it be random? The random part is what I'm objecting to. It would always be the same ones, but just not adjacent neurons. Even adjacent neurons just mean it's an SDR. 

In the drawing you have there, you know which one will be dropped off and which one will be turned on. I was showing the bump activity, like in grid cells, but if I'm thinking about the location representation here, I'm working under the assumption that this SDR is not the set of bump activity. Here, it would be like the minicolumns that are active, and then there are individual cells in the minicolumns. At one level, I can say, "Oh yeah, these minicolumns have meaning, and one minicolumn will become active, then the next one, and so on—they're ordered." But when you look at the actual SDR, where you have individual cells in the minicolumns, there is no order. You can't see the order anymore. It's just a set of a hundred bits that happen to exist out in space, and you have no idea what their relationship to each other is.

If you don't see that relationship anymore, how can you do the interpolation? Let's say I have ten that are always on, and I have two points, and four of them are still overlapping but six changed. Now I want to have the middle point. How do I know which three from the beginning I need to take away and which three need to be added if I don't know how the changes are arranged? Some of them disappeared and some new ones appeared, right? So, how do I know which ones disappeared and which ones got added?

I don't know. I guess I'm confused by your question because if I have two SDRs and they overlap, it's obvious which ones disappeared and which ones became active—the ones added from the start to the end point. Those are the two points I'm given. But how do you know it for a point in the middle, in between?

Here's the thing that was a bit surprising and came up a number of years ago—maybe this is related to your question, Viviane. This is a valid point and this is a valid point, right? What about in between? Couldn't this be invalid? Couldn't this set of bits actually be something else? It's like I'm mixing and matching two things. We call that a mix and match error. Couldn't this be an incorrect assumption that these are in between? Theoretically that's true, but practically it doesn't happen. Mathematically, it doesn't happen.

Maybe I could explain that better, but that was the insight. Couldn't this be wrong? I'm taking half the bits from here and half the bits from there. Couldn't there be another point in space that's not between these two? Theoretically, yes, but practically no. Mathematically, it's extremely unlikely. Maybe that's getting to your question—how can I be certain that this set here is actually in between this point and this point? I guess it would be: how do I know which half of the points are from the first one and which half are from the other one? I don't need to know that. All I need to know is that these sets of synapses wrapped, and these sets of synapses wrapped. I don't need to know which is from which. It's just a bunch of synapses. The dendrite doesn't know anything about what these represent or which was which. It's just saying that if I do this exercise, then the dendrite will recognize every point in between these two points and not get confused.

The dendrite doesn't know which points come from where. There's no idea. Yes, they're subsampled from a bigger activation. It's not the whole activation.

I'm not sure if that's big. I think she was saying that's a smooth information point, but it's not—it's like a subsample from that. It's a subsample, but I'm not sure if that was the point. Maybe we're getting close to your question or not. So could you say that this only works for recognizing a new pattern in between, but it couldn't give you a definite pattern in between? I can't predict. Okay, then it makes sense. I cannot predict this. All I'm doing is recognizing this.

That makes a lot of sense. Thanks a lot. There's no prediction going on there. As far as the dendrite is concerned, these can all be mixed together. As long as they're within 40 microns of each other—the group I'm looking for—that's sufficient.

There's no ordering necessarily. It might be, but it doesn't have to be, and there's no ability for anybody to predict what this SDR should be.

Now I understand.

I guess you could predict if I was associating this point right here, which is in between point 0.1 and 0.2, with something else over here, some sense feature. Then this sense feature could predict that's what I'm going to be, right? This sense feature says, oh yeah, you should be at some sense. It wouldn't predict the synapses, but it could predict an SDR. A little confusing, but the point is nobody understands what these bits mean on the dendrite. There's no knowledge. You can't predict. It's just recognizing.