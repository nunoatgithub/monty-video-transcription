I started off with a very simple experiment, so please pay attention to the screen.

What did you notice? It looks identical, right? It obviously moved, but these are two images. You see quite a difference in size. If someone were to go into a cupboard at night and magically reduce the sizes of all cups by half an inch, you wouldn't even notice the next step.

I think I would notice because you have a physical memory of what your hands feel like. This isn't because you see objects at different scales all the time, so neither one of these is the correct scale—one is just bigger than the other. Now it's one sort of change. The point I want to make here is that, on one hand, it's very obvious to compare the two, and we can clearly represent the dimensions when comparing them. On the other hand, we can recognize the object independent of those specifics.

How does that come about? That's what I want to address. We have, in terms of recognizing the cup, quite some flexibility to recognize distortion, even if it's tilted in a certain way, but there are limits. The last one, the upper ring on the cup, looks more like a face.

A fingernail.

Needs a manicure. Suddenly, the recognition is different.

The point I want to make is that our recognition is flexible to some extent, but there are limits. Take into account the relative location—this is different from the scale component. Don't you think the scale difference is qualitatively different from this?

Distortion.

We seem to have two kinds of capabilities. We recognize objects invariant to position, pose, and scale information. On the other hand, we recognize the specific position and pose.

I'm going to object to the scale thing in the second case because it's not clear to me that we actually recognize scale. I can know where it is relative to my body, I know the pose relative to my body, and I can know if it's deformed. The example you gave of the two cup scales—I didn't see them as different. I think scale is like an overarching factor that's applied, especially in vision, because we see things at different angles and distances. It's not that I recognize the scale; it's that I recognize the object independent of scale. With touch, I would definitely say yes—I recognize the object's scale. If you gave me a small version of this cup, I would feel it and know it. But with vision, I'm challenging that. I don't think I recognize it. I think scale is fine. I feel like there are two different uses of scale here: one is the object itself being smaller or larger, and the second is the image on our retina being smaller or larger. Those are two very different things. If we're doing this completely in 3D, the size on the retina doesn't make a difference—we're invariant. But if it's physically smaller or larger, we do know. If I had a tiny version of this, I would immediately know it's smaller. But if it was further away, I would still think so. If there was context—if I see this cup in the context of other things around it—I can tell if it's smaller or larger. But if you just show me the cup independent of context, it depends on the context and what you mean by scale.

Compared to the same kind of cup at a smaller scale—yes, a smaller cup. But again, I don't think I can tell the difference between those two if I don't have context. Without context, I don't know if the far away cup or close cup, or two cups, are smaller or bigger. It's meaningless. We've shown pictures of the coffee cup on the screen a thousand times—it's never the size of a real coffee cup.

We did look at those two pictures and think, "Wow, that's really big." I think context is important. Again, I want to say that I think scale is a different problem than position and deformation.

Two weeks ago, as part of the exercise, we tried to reproduce three objects from memory without looking at them. Here, I tried something similar, but instead of just reproducing the whole object, I placed a smaller object on the other object. I found it seems fairly easy to reproduce the position on the object.

You simply have a good sense of the position of one object on the other.

Even though I didn't get the whole dimensions right, or the perspective is wrong, and the box with the elk on the bottom is a bit distorted, the location of the object is fairly accurate.

That's one observation. With this comes my proposal: there are two pathways. On one hand, we represent an object in a reference frame of another object—the supporting object. For example, the cup on a book on a table. We have a representation within the coordinate system of the supporting structure: where is the cup located, where are features on the cup, for example, shapes, and where they're located in the reference frame of the supporting structure. That's one thing. The second path is that we have this invariant representation at a local reference level of the object itself, where we don't take exact locations into account. It's more like, "I have this curvature feature here and, somewhere relative to it, another feature," and it's represented in a way that is invariant to rotation. But why do you say those are different? I'm confused by that. It seems to me the logo on the cup is just another example of an object on an object. Yes, and so the cup on the book—why do you say these are fundamentally different? It seems to me it's a very similar process, but what's different about it?

There are essentially two differences. One is that we use a different reference frame for the first case, which is from the supporting object. What is different about the first one? Why are you making that argument? Why do you say there are two different cases? I look at this and think I can take any object and show its position relative to another object. In some cases, that's a permanent thing, like the logo on the cup. In other cases, it might be temporary, like the cup on the book, but if that's how my book came with the cup on the surface, then I'd say that's the object. I'm not sure I understand the fundamental difference between these two. Structurally, what's different about it, apart from the reference frame? Is it the position? In the first case, we decode the position of the object in the reference frame. But isn't that what the logo is doing on the cup too? The logo on the cup is the same case as the platform.

So what is different about the sample here? The difference is Path two, the morphology argument. What is Path two? Path two is the argument related to the displacement discussed before, and the displacement relative to the local post is almost completely invariant to rotation. But couldn't you make the same argument on the left? I could just rotate the whole thing. There are two things that stand out: one is when you can't say object A and object B, like the circle or the curvature, the morphology of an object where there are no points to define a reference frame relative to another. Then, there are times when you can say specifically that this thing is located at this location for that thing. I'm certain about this morphology versus features displacement, but I'm trying to unite them. I'm not sure if you're saying the same thing—if you're talking about local shapes, that's the morphology argument. I want to make sure that's the same as the morphology argument. Everything is about morphology. The local descriptors are local shape descriptors, but here it's represented in a way that isn't a specific correlate. This feature is located relative to another feature, to the northwest or northeast of it.

But isn't that similar to the cup on the box? No. In this case, we have a precise location representation in this coordinate system, maybe an XY coordinate. We don't represent it in this content system. Where is the handle? In the book coordinate system? In the book system? I don't know. I'll keep pushing back. When I recognize the cup on the book, I don't recognize the features of the cup relative to the features of the book. I can say, here's this cup—I already know it. Here's a book—I already know it. I can rapidly associate the book and the cup together, and I don't do that by associating all the features of the cup relative to the book. That's the whole point of the compositional object: I don't need to study all the components of the cup relative to the book's reference frame. At least, I don't have to do that to learn it. I can do it in an instant. I can say, there's the cup relative to the book. Maybe I can calculate those positions, as you showed—where's the handle to the book? But I don't think I memorize that, or even have the opportunity to do that.

I wouldn't do it in a very detailed way. But why would I even do it at all? It seems like I don't even have to see all the parts of the cup. Somehow, I'm able to say if the cup is relative to the book, then that's it. It's very rapid. I don't have to look at the different parts and calculate the differences. If you store it this way, you get a compounding problem: an explosion of things to memorize, the relative items at all points. It can get very complicated. It seems like we do this very rapidly. We just say, okay, there's this existing object, like the Dement logo. I don't have to specify or memorize all the components of the logo relative to the reference frame of the cup. I can do the whole thing at once. I take the Dement logo and apply it to the cup. I see the advantage: this representation allows us to quickly recognize the object independent of the pose. This would allow us to represent what is actually the pose of this thing in the world—for example, the handle. I just need a few key features. Maybe the bottom of the cup is a dislocation, and this piece of the handle is a dislocation. Through this pathway, I recognize this is a cup, and this representation tells us there's something like a handle feature here and something like a button of the cover. I'm still computing.

We have digital languages. On the one hand, we talked about the what and where pathways in the back. The what/where pathway shows where things are relative to a body-centric reference, but you don't know what it is. It just says there are things out there, some disposition, and then you've got the what pathway, which says, here's this thing called the cup, but you don't know where it is anymore—it's just in its own reference frame. That's what you're talking about here. I always use those as basically the same mechanism, just using different reference frames. One is the reference of the cup, and one is the reference frame of your body. When you said the book, that puts it back in the what pathway. I have a book and a cup, and they're relative to each other. I could have that in any position, any pose. I still see the cup relative to the book.

I feel like we're mixing ideas here.

And you were also talking about reference frames and displacement, right? I think you said on the left you have a reference frame, like an XYZ reference frame, and on the right you have a displacement. But you also show a reference frame there. I'm confused by that.

Now, let's look at some other examples. You could think about recognizing the deformation within this reference frame, with different coordinates. This way, you could represent the deformation that way. Here, the object was stretched, so we have a different location as opposed to just the object on its own. But couldn't you represent the deformed cup on its own? Why are you tying it to the book? I'm confused by that, because we struggled to find a representation for the object on its own.

That was your path too, right? We could do it in a way that's completely invariant to the deformation. There's no optimization process involved—what would be the appropriate deformation, rotation, transformation to use? We have a problem recognizing a deformed cup, right?

We can do that. You can see the same structure in different shapes, sizes, and deformations.

That seems to me basically an object recognition or representation problem, which to me puts it in the "what" pathway. It doesn't matter if it's relative; it can be different shapes. So I'm confused—why does it seem like that's a fundamental problem with just cups, not cups and bowls? Does that make sense, or am I missing something?

I think there are two separate questions. How do I recognize the object completely independent of those factors? The other is, how would this deformation be directly represented in a straightforward way within the context of another structure? I can imagine it's independent. I can recognize it as a different shape or form. In principle, yes, but then you may have an explosion of possibilities. Obviously, we don't. I can recognize cups of different shapes independent of their context. My brain does it, so there must be a solution.

To me, it feels like I've got to solve the deformation problem one way or another, independent of its context. Again, I think the size or scale change you started with was a different example. That's why I'm trying to separate that out. If you didn't show me the book, I'd still see it as a deformed cup. I don't need the book to show me that. Or not deformed—it's just hard to present something without any context, and the context could be self-context. It's a different thing—an oval and a circle, right? It's self-context. I don't need a context to know that a circle is different from an oval; I can just see it. There's a representation of that, just like I can have a representation of a cup that's independent of context, that says it's shorter or taller, whatever.

Context can help, like when we were talking about the absolute size of something. The context would provide that information. If I see the cup and it looks bigger or smaller on my retina, how do we know it's a bigger or smaller cup? I have to look at the context.

Also, information like shadows and so on.

But still, I think this problem can be solved independent of context. It has to be solved.

My argument is that I see this problem as independent of any context. If you showed me a short cup and a tall cup, I see it and that's all there is. I know one's short and one's tall, or I know their relative sizes or morphology. It's independent of any context. If there's no other context out there, why can't it be?

It's self-consistent. I don't need anything else to tell me that this cup is wider than it is tall and the other one is taller than it is wide. It's obvious. It's a self-consistent reference, and you're trying to draw analogies with the "what" and "where" pathways. That's one thing I was thinking about. When looking at the literature, it seems quite messy. The experiments show that "where" is about things relative to the body and how the body might interact with the object, as opposed to the "what" type, which is just about object identity.

I think that's pretty clear.

Every neuroscience study, there's no definitive answer.

But I think that's pretty clear to an extent. People push around the edges, saying, "I found some neurons that respond this way or that way," but generally it shows some separate processing.

I think it's easily explained in some way. If you think about a reference frame relative to the body, then the "where" pathway basically says these are things relative to the body. When referencing relative to the body, I can't define the object. The object isn't defined in reference to the body; the body has to be defined. I can't find the object—it has to have its own reference. So the "what" pathway is like, "I'm going to use the same mechanisms, but now I'm using a reference frame that's attached to the object." I'll see the shape of the object, but I don't know what it is. Here, I see where something is, but I don't know what it is. That's very simple and consistent with the mechanisms and observations of the pathways. I see the advantage of having a reference frame that doesn't depend on other objects.

You can build up the whole world in this hierarchical way. That's just a compositional or hierarchical object. The coffee cup itself is that way—the coffee cup has a logo, and the logo has letters, so you can build that structured, compositional model in the "what" pathway.

I suppose I can do it in the "what" pathway, but I don't think I need a body- or world-centric reference frame to learn the structure of the object. To me, the book is just another composite object. I don't see how it's different from the logo on the cup versus the book in the cup. The book could be tilted or changed in direction, but I can still represent object A very well in the reference frame of object B. For example, the key on the keyboard has a specific location relative to the computer, and I can change my perspective. How is the logo on the cup different? It's the same. I'm missing what you introduced here; I'm not following your argument. The thing with the logo is that it's represented within a curved coordinate system on the cup. The cup is the supporting structure, and the logo is represented in this reference frame, with the locations of each letter represented in the reference frame of the cup. It's not the reference frame of the logo, but the reference frame of the cup.

That seems to be exactly what you're saying with the book, so I don't see them as two separate mechanisms. It's the same; what is separate is the ability to recognize a specific shape, maybe of the letters, independent of deformation. In this case, they wouldn't have a specific location-based coordinate representation. The way I've been doing this is that there are two possible ways of representing: you've got a coordinate reference frame, and you've got displacements. These are two possible representational schemes. The coordinate reference frame is very difficult to handle for certain types of deformation. It's difficult to infer what the object is in the coordinate reference because you don't know where you are until you assign it to the right reference frame. But when you deal with displacement, it automatically handles those things. Displacement says it doesn't matter where this thing is; I'm just looking at the displacement for the features. We talked about how the displacements themselves could be modified, and as long as the relative positions of features are preserved, the distances between them could be stretched and you would still recognize it.

Those are possible reference frames and displacement. If that's what you're talking about, I agree. There are these two possible representation schemes, and it seems like they're both being used in object representation. It's not like there's one pathway and another pathway; object representation has both these mechanisms, and they serve different purposes. It's not Path one and Path two; it's about how to represent an object, and you have to use some mix of displacements and reference frames. I agree, it might not be Path one and Path two, and everything could be in one column, but essentially, we talk about these two types of representations. One is a specific location-based representation, which needs some supporting structure, like the logo. On the other hand, there's the displacement type of representation, which allows for a certain amount of deformation—pose variation, for example.

So, this is what I was thinking. Maybe "path two" is not the right term, but it's another way, since we have the ability to recognize things based on rough relative displacement from each other. For example, if I hold this cup at the corner and blindly feel the rim, it would still feel like the cup.

It's this relative displacement—coarse-grained, relative to a part of the object. It doesn't matter how the whole object is oriented; it's strictly variant. This is a displacement representation.

Regarding the encoding, what I call path one: we didn't use the location as context, and for each location, we would expect a local shipment. This is like in the columns paper.

Maybe the only difference is that I call it a supporting structure, like the cup itself, whereas the other one uses displacement as context. I expect a certain displacement and a certain option. One of the possibilities Marcus proposed was that grid cells, which we think about in terms of reference, do path integration. They allow you to know where you are when you move, and they're also suited for determining the distance between how you might move, like planning actions. The idea is that grid cells might play a supplementary role; they're not really in the definition of the object itself. The object is defined by its displacement, which is a more compatible idea. But you might have this reference frame because you have to move relative to the object and calculate movements, so you can link to that and use it to define how to move, as opposed to it being an integral part of the object's representation. In the columns and columns-plus paper, we say the object is defined by displacement.

Maybe it's all about displacement, and the reference frame is there as a supplementary role when you want to move relative to the object—as your finger moves, where will it be relative to the object? That's the question: what is this encoding for? There are possible planning actions, but that's the hypothesis I'm working with right now. If we can define the structure of an object independently of these reference frames, it's all about displacement. That would probably be how it's done.

Every time I think about using reference frames to define the structure of an object, it doesn't work well.

For example, in this case, if I represent the shape—let's say the keyboard shape—in the reference frame of this computer, I could use this to guide my hand. You have to know when your hand is moving, how far it moved relative to the object. We know grid cells do that, right? Grid cells seem to be the way that movement is converted into displacement in some sense.

I've been discussing with Vivian other invariant representations. Vivian already showed she can have a pose-invariant, scale-invariant representation, and the code can be recognized independent of scale and pose. Then we were discussing other kinds of transformations, for example, shear, or generally, you could say an affine transformation. What's a shear transformation? Shear is, for example, this box is not flexible, not twisted. Imagine the top part. That's a very odd, not a normal type of transformation. It's interesting you show me that, and I would say that's fundamentally not a regular coffee cup. I think that distinguishes things. You can change the size and the relative dimensions, but I can see that as clearly a cup that's been, if you want to call it a shear transformation, I can see that. It's not like I would say, "Oh, that's a new type of coffee cup." Unless I saw a lot of those, that's an odd one. You also see the rotation and the scale. Just because you see it doesn't mean you can't recognize it as the same object anymore.

When the cup is rotated or scaled, you can see that as it was scaled. Under all the transformations, you still recognize it as the cup, but you're also able to determine that there was a transformation applied to it. When I see the rotated coffee cup, I don't say to myself, "Oh, that's the coffee cup rotated." Remember, I was making the argument last week about all views of the coffee cup being laid out in a sheet. I don't say, "Here's the coffee cup, and now I'm looking at it rotated." If you asked, "What is that?" I would say, "That's my rotated coffee cup." I don't say, "Oh, that's my coffee cup looking down from above." I see it as a coffee cup, but it's not like I had to do a transform to get it into some canonical view in my head. That's where I was coming from with that. The shear one is not like that. The shear one is wrong.

My coffee cup doesn't look like that. There's a fundamental difference between distortion—changing the morphology of the cup, distorting it in some way. The shear distortion might be just the same as making it squishy or short, but I see that as different. I don't see that as a coffee cup. I immediately know it's not my coffee cup. But when I look at different views of this topic, I don't see it as a different object. I don't see it as my object rotated or under some deformation. Rotation does not seem like a deformation.

But what if it's upside down? If I see my coffee cup upside down, I think, "It's in the closet, it's upside down." I don't have a problem with that. Even if it's upside down, I would say it's the same cup. The relative displacements haven't changed at all, so it's the same cup. As soon as you start changing the displacement, then it's like a circle that becomes an oval, like this cup becomes something else, but it's still related.

All rotations seem to me—they're not like I have my object and I see it under some deformation. The beauty of the idea I was presenting this week is that it gets rid of that idea. There's no concept of left and right to this coffee cup. I see them all as the same. If you do some real deformation, then I say, "Oh no, that's short or fat," or a different color. Obviously, it's different. But any of these pose changes do not strike me as a different color; I'd say it's the same, even upside down. This relates to the information we actually consider would be the same. That's why something becomes a different object.

There are two things: one is when does a specific instance become a different object—so this particular coffee cup, when do I see it's different? The other question is, what's a normal coffee cup generally?

That's an interesting question. I would argue under the deformation category, we just have to see what we observe in the world. If I never saw a cup that had a rounded bottom, like a teacup, and every cup I ever saw in my life was straight-sided, the first time you showed me a rounded-bottom one, I'd say, "That's not normal, that's odd." But since I've seen both, they're both called cups—straight-sided, curved cup side. It's experiment-based, but that's how we learn models of things, and it gets messy. How do we categorize? So, coming back to the transformations, I agree that the total space of those transformations goes beyond what we consider normal. But for a machine system, it might make sense too.

This is just a way of doing deformation, is that correct? So, a transformation is like what you see here. There's any kind of linear transformation, and one characteristic of those transformations is if you have two parallel lines, any transform keeps them parallel. Certain other things stay invariant, for example, volumes. If you take the volume of those three volumetric shapes here, the volume of each under the transformation stays the same. Relative ratio—the ratio, yeah. Are you saying it's a linear transformation? Is that basically what it is? It includes translation, scale, rotation, and shear—all included. The translation you get here, the rotation you get with a very specific matrix that has determinant one properties. Maybe this is how we want to implement this. I'm not going to say no, but I don't think this is what's going on. My example of the cup being rotated and not really being rotated counters this idea. There's no transformation between these two in my head. They're both valid; I just see them as what they are.

Where this would say, no, there's a normal cup and then there's a rotated cup under some transformation. That doesn't mean we don't want to implement it this way, because maybe we understand this better and go that way. I just want to point out that I don't think it's capturing what the neurons are doing. I agree. There's one way to get recognition that's completely invariant, so it wouldn't recognize the pose at all, but you could recover it. Wait, I thought the poses are not part of this transformation. But it is. If I take 3D rotation, 3D scale, 3D movements, and 3D shear, it would work in the sense that it would recognize it independent of the pose. The pose gets removed; the recognition system can't extract a pose after that. Once you recognize the object and its orientation, you can recover it. I used to think about that in the brain too. I say to myself, I'm able to say what this is—it's a cup, my cup—but I'm also aware of its pose. When I classify it, the pose doesn't seem to be part of that. So I said, there must be some neurons representing the object independent of its pose, and some others representing the pose of the object. So there must be two parts of these neural populations. My point, and what I'm working on now, is that may not be true. I used to think that, and it was always bothersome because it doesn't fit, but that's a lot of things—it doesn't fit within the brain. I was thinking, this idea that you've got this sort of displacement sheet, which is all of the object, and putting a part of the sheet, you're actually working in the state of the object, whether that's the pose or the state or something like that. It's saying, I've got this sheet that represents a set of displacements that represent the object. I'm only able to observe part of it right now, but I still classify the object, and the part I'm observing is the pose.

It's not like I have two separate representations. There would be one representation for the two. I think it's a very nice idea, and I don't know if it's true, but it's a very nice idea. I agree, it's a nice idea. I thought about it too, but my concern is that it's going to lead to an explosion of possibilities. No, I don't think so. I think it's actually quite efficient. At least the representation—I haven't worked all the angles out yet, but so far as I've thought about it, if you take the cup and unwrap it, you're storing the same number of feature displacements as you would have otherwise, just not in a 3D structure. You're storing them in a 2D structure and playing it out, but it's the same number. I'm still just figuring out the displacement on a local level. I'm thinking of these things, not trying to figure out the relationship between distant points.

I think it would work, but I haven't proven it. I'm going to keep working on it, and if the argument here is we want to be working on these transformations, I think that's fine. We might run into problems, but so what? Anything, I'm just pointing out that I'm not satisfied yet. I think there are some fundamental things—there are so many weird things going on in how we recognize things that I'm afraid.

It's one of these situations where I'm worried that we might really need to understand how neurons do this.

Just a side note on what they showed: those are what, at least in my field, are called barycentric coordinates. They can actually work in higher dimensions as long as you decompose the object into simplices. There's the two-dimensional form, the three-dimensional form. They have the properties you're talking about—being invariant. Was the term barycentric? Is that what you said? Barycentric? How do you spell it? B-A-R-Y... I think E-A-R-Y... S-B-A-O-Y. Centric coordinates. What does that mean? It means it's weighted by the area. If you think of it as a triangle where you have a point in the center and you form three triangles from that, the area of the triangle weights how much you want to contribute to the coordinate. In other words, the distance from the face. We use it in graphics for interpolation, for instance.

Instead of thinking about interpolating on a square, if you're trying to interpolate a triangle, barycentric coordinates are what you use to weight the contribution from, say, the color of the vertices to the interior color. It's something that works well when you pass it through coordinate systems and still produces good results. What they were showing there was, rather than breaking a triangle into three triangles centered at the point, they're breaking a tetrahedron into four tetrahedrons. So that's a four-tuple rather than a three-tuple, but that can go up to higher dimensions. It's a way of making something that's invariant to those kinds of affine transformations in higher dimensions.

Thanks for the link reference, Kevin. Let's move on to the next presentation.

Last week we talked about the sensor, but then Jeff pointed out that it's not really how it works because we move a finger to the side, so we hit something, or we have multiple sensations on different sides of the finger. The same group, the same researcher, together with a university, created this version that was quick. This is basically the same technology. The one we talked about yesterday is on the left. It has a camera on the bottom and gel on the top. The LEDs in this case put light because there is a coating on the top of the gel that doesn't let outside light come in, so they only see the deformation on the gel. They have different color LEDs to know where you are on the sheet, and they probably alternate them. It's similar to how you might make a touch sensor to figure out where your finger or pen is touching on the screen, using different sources and alternating. On the left side, that's the one we presented last week. It usually has this gel, these LEDs, a little casing, and a webcam at the bottom. On the right side is the new version, which uses five endoscope cameras because they need a small focal point, and there is no casing. They put the gel directly on top of the cameras. The distance is basically the size of the thing, the side of the pump. There's no air inside; it's completely gel. The thing on the right has a physical sensor wrapped in gel. The whole thing is gel, 360 degrees and 270 at the top. This is pretty cool.

In the five-camera setup, there are some blind spots, as you can see in the top right. These are the fields of view of all five cameras. B is the top view, showing different fields of view and blind spots. These are endoscope cameras, so they are medical grade.

They can use this for material exams instead of a doctor's finger, for example, going up the nose or other places. Because they put the gel, there's no room, so the camera is in the gel and needs a much smaller focal point. The reason for this is the same as before: when moving sideways, you can't see what's going on once you move the finger. They came to the same conclusion. The top camera sees just the top, and as you move the finger, the side and top cameras capture different views. Within the camera's field of view, they have resolution, so you can tell if something is on the left or right. Even the old one is more than just a single sensor. If you think about it like a sensory patch from a patch of skin, it's not resolution like this; you can't feel the entire area at once. To feel something like that, you have to sense multiple parts of your skin.

This is actually an array of sensors. You could break it up into five, or into 25, or take each of those and break them up further. That gives you a lot of flexibility. It's just a different configuration of the same technology. They have a simulator for this, a version of the one I presented before. The marble goes to the sides and the bottom, and the top is on the top camera. They didn't really simulate it physically before, but now they have a mesh for the gel and apply the noise from the camera, with different camera fields. It's all continuous, but there are five different cameras.

They apply the same noise they got from the real sensor, so they could compute the level of noise and apply that. I can show in the paper where they compare the real and simulator results. If you were to run the simulator, it would be slower because there is a physics component. The pressure of the marble on the gel is simulated by physics. In the old one, there was no physics, but they still had the ball on the gel. It's the same as the old action simulator, just with this new configuration. Now you have five sensors instead of one, and it's a bit more difficult because you have to figure out the angle. I'm trying to figure out the frame rate you can get from this. They provide the source code, and it's all open source. You could make a simplified version that doesn't correspond to the physical sensor. For our purposes, a lower resolution system would be okay. Right now, the resolution is 404 for each camera, which is a lot.

I don't know the exact number, but the data is probably out there. If I think about my fingertip, how many different columns are representing it? I don't know that number. With this sensor, one touch could read a paragraph.

This is not like a human sensor; it's superhuman, but for a human, it might be equivalent to 50 columns of fairly low resolution, where each one isn't capturing an image.

If someone reads braille, there's a very simple test: you poke the skin with two pins, and as they get closer together, at some point humans can't feel them as separate. At some point, they do feel them apart. That, in some sense, is probably the scale of a cortical column—maybe a good guess. On the tip of your finger, it's pretty close; on the back of your finger, it's much farther apart. On your back, it's surprising—you can have two pokes two inches apart and can't tell the difference.

I'm just trying to figure out what would be equivalent for a human finger here. It's pretty cool. I think we have flexibility; if it became slow, we might have options to reduce the resolution. It's open source.

This sensor would have higher resolution than a human finger. If you have 400 by 400, that's a little patch of your camera. That is far greater resolution than a human finger. We might want to focus on basic deformation rather than reading micro cracks in ceramics.

Look at that—the screwdriver there.

Is this the same team? It's the same researcher, but the university is not Facebook; this is Berkeley. It's a collaboration between the main person from the first one and Berkeley, but the main office is from Facebook. The main office is the person who built the physical system—he built the first one and the simulator. This is on Facebook. Berkeley took that and built this. So, Facebook did the first one, and then Berkeley did the second one, collaborating with the person who did the first one.

What are they doing with it? That's a good question. Maybe the metaverse. Tomorrow there's a Facebook talk about how they're using AI in the metaverse. If you want to see what they're doing, you can check that out. How would you use it in virtual reality? You'd need a remote hand that's sensing something physical, or it could be just a simulator sensing something through the simulation.

Would it be like my hand moving, and this thing would be in some remote world? If there was a representation of my hand in the metaverse and I touched something, it would need to know the exact sensation to transmit it back to me. Then you'd try to activate your hand—there would need to be something corresponding to activate your hand. All this high resolution wouldn't be advantageous in that sense, because your finger can't detect that level of detail. It also has applications in telepresence. Now you can pass the sensor back. Isn't that the same as virtual reality? The difference is you have a remote effector, like a robot with a hand that touches back. I was thinking about a physical robotic extension where you sense it, but in the metaverse, it wouldn't be like that. I can see applications like fixing the space telescope from Earth, manipulating it remotely, or for surgeons. There's a mismatch between the capabilities of this sensor and what your finger would communicate.

This is cool because if you were building a physical robot with a brain, that robot could have superhuman tactile senses. That would be exciting—maybe we could do manipulations otherwise impossible. Another cool superhuman capability would be to sense at a distance using your fingers. As you approach something, before you touch it, you'd be able to sense it. It would be like having a little camera's vision on each finger. In some sense, you could start seeing or feeling it before actually touching it. You'd be able to perceive or learn the object just by moving your fingers, like moving five little eyes. Then you could also grab it.

Sensing at a distance is like sensing on the surface with robotic manipulation. You'd have a super hand. It's surprising—there's no organism I can think of that has anything like that. Evolution didn't seem to create that. Maybe something like echolocation or sonar, or whiskers, which do touch but not in the same way. Some crabs have eyes extended on stalks, but combining that with physical manipulation is the interesting part. It's not just a bunch of eyes on stalks; you can also grab things. Combining sensitive distance sensing with surface and physical manipulation is unique.

You also have to consider the limits of human beings, especially trained ones. I remember seeing a claim from a Japanese machinist that he could sense things down to a tenth of a millimeter. Surface discontinuities require a huge amount of precision, and somehow we're able to do that. That was touch.

You can feel a human hair if it's on the surface. That's the level we have.

They can feel the presence of walls as they walk around. I forget what the research says about how they do it, whether it's through sound or radiated temperature. I'm not sure. I can just tell you, I can sense walls around me. My ears, the sound of it—it's because of sound engineering. Now I walk into a room and notice how different the sound is. That's an interesting observation. Animals that have a grasp also have sensing at a distance combined. I think evolution would go against it because a predator or dangerous circumstances could cause you to lose it. But why would it be worse than having it? It seems like an extra capability can only be advantageous. How would it get worse? You don't want to experiment with a part of yourself. I'm just saying you're getting an additional capability; you don't have to use it. It's not forcing you to do something dangerous. Imagine if I could really sense an object before I touch it. I don't see how that would make it more dangerous for me. It just seems like it would be better—almost like, "Don't touch that," because I know from a distance it's going to be something bad. I guess the tongue is maybe the closest, because you have different types of receptors. They're not cameras, but they are different types of receptors. But something has to be on the surface.

Very interesting. I like this. I'm glad this happened because you brought up the same date, and they also brought up the same issue.

Did you read their description of why they did this? Is there anything additional that we didn't think about? They say sometimes the task is exactly what we said—sometimes we don't have the actual sensation, and they have to move. Sometimes the object goes around. I didn't know if there's any additional stuff in there. This gets back to what we were talking about a while back, Vivian, about how when you move your finger along the edge of an object, you get to the edge. How do you know that? One reason you don't just go off the edge is because you can actually sense it. Some senses kick in, saying it curves around. The experiments didn't have the same budget as Facebook, so they used a 3D printer instead of a robot for experiments.

Oh, they're actually moving this finger on a 3D printer? They have a C machine. I see.

It's cheaper than a robot. They're very creative—lower budget, grad students instead of Facebook budget. This is encouraging. It's very fluky seeing those little lights glowing in their wells. If you saw someone's fingers glow like that and had a little bubble, you could see the little bubbles—they didn't get a perfect seal. That's what distorts the bubbles. Bubbles are visible.

You can see the bright spots. If this was learning from the beginning, it wouldn't matter. Just like you have deformation or problems with your retina, you don't notice that. Some people's skin is not the same.

Alright, very cool. Thanks. Alright, Ben. We don't have a ton of time, so I'll try to keep my presentation short.

There are just a couple of small things in our current diagrams for the Monty architecture that are a little ambiguous. I wanted to raise those and try to get some answers or at least start thinking about them, and also update you with some changes we made to the code to make sure the decisions we made are compatible with the overall picture.

Here's the diagram. The people to the right.

Oh, look at that.

This is from our documents. On the screen share, it still shows the first slide. It's not sharing that, but that's complicated. Can you, on the right screen here, just click on the second image? We'll do this manually. How about this video? That's perfect.

Just move both of these. When you move the mouse now, keep in mind they don't see your mouse. So just verbally, let's do it all on this. You won't see the animation. We see his mouse. The first question was, where do sensor modules live? Where do they begin? Is a sensor module the sensor in the body, something in the brain that processes the sensory input, or both? It would be like the retina as well as early processing.

Are you talking about biology or Monty? In Monty, I don't know where a sensor module actually begins and ends. In Monty right now, everything's code. We don't have any physical sensors yet. Conceptually, I can take a stab at this, but I'm not sure how others think about it. To me, the sensor includes, in the real world, the physical sensor plus any amount of subcortical processing that has to occur, presented in a format that can be fed to a learning module. In the brain, it would be the eye and retina. In the ears, it goes from cochlea to two subcortical structures, gets processed, and then passed to the cortex. It's the sensor plus some subcortical processing—anything that's not cortical processing, anything specific to the modality of the sensor that needs to be processed to get it in a form usable by the learning model.

Perfect. That works great for me.

The next thing I wanted to bring up is that there's no attention on this diagram, and there aren't enough dots and arrows. I guess there's the motor command, which could move the sensor. What I'm getting at is that I've been thinking of the motor command and attention as intermixed, but I want to separate them. We talked about the idea that you could give an attention signal as a location in space to attend to, but if that's the case, who decodes that into actual motor commands to move the sensor to that location? I guess it would be the subcortical motor area. That would be very modality specific. For example, the superior colliculus directs eye movements and works in coordinate systems like this. You would tell it to attend to a location, and it figures out how to move the eye muscles. On our Monty diagram, that would be part of the sensor. So now the sensor has to be able to do that.

As a rock climber, I've become familiar with the anatomy of the hand. There are no actual muscles in the fingers; they're controlled by tendons. You can see that by pressing on your forearm—the muscles there control the fingers, but they don't move the forearm itself. When you think about motor commands, action potentials come from the motor cortex to nerves in the cervical region of the spine, then out to the nerves in the forearm. The muscles in the forearm contract, moving the finger and changing the position of the sensor on the fingertip.

Here, you're using a different definition of sensor than Jeff used. In that definition, the sensor or sensor module includes everything needed, including subcortical processing, to handle somatosensory input. The subcortical areas would also translate signals to motor commands. For the eye, the superior colliculus is included in the sensor module. Logically, you have a signal that says, "move the sensor to this location," and how it happens is specific to the body or sensor. The physical location of the muscles doesn't matter; there's a signal to move your finger, and something figures out how to do that. That's not Monty's problem; it's the sensor's problem.

It still seems conceptually like the actuator is not part of the sensor.

The muscles can move multiple sensors at once, or they can be constrained. You can't move different parts of the same finger in different directions. How that happens depends on the particular sensor, body morphology, and system instantiation, which is outside of Monty's concern. That becomes a specific concern for the system builder. I feel like it complicates things to put sensors and actuators in the same category. Conceptually, it's simpler if those are two distinct systems.

I thought it would be simpler to make them one system. You think it's simpler to make two systems? Traditionally, the sensor is thought of as an input, not an output. But you're thinking of it as a modality or sensor system. For example, somatic sensory systems involve touch and movement of fingers, and the visual modality would be the retina plus the motor commands. Maybe it's better to think of it as a modality or a complex system that does these things. If I want to direct my gaze somewhere, it may involve moving my eyes, head, or body, but we do this automatically. If I hear something behind me and need to look, my brain says to look, and I move my body, neck, and everything else. I want to abstract that away and say someone took care of that.

Now, I think you bring up a separate issue: whether there's an advantage to having an actuator separate from the sensory system. Is there an advantage to being able to say, "I'm going to grab this object," or is it better to think about moving my sensors into position? Unifying them might make things simpler in some sense.

They are already unified because we have a sensorimotor system, so they already learn together. That's why it all works so well and is so easy and effortless. Still, I feel like conceptually those are two distinct modules: sensing and acting. They just learn together and coordinate, but why do you feel that way? What's wrong with the idea of considering them as one thing? Maybe it's not how you've thought about it in the past, but what's the advantage of separating them out? I feel like it complicates things because motor actions can have many different goals. You can move your hand to sense something, to do something in the world, or to move an object into your visual field to see it. You can do the same action for different reasons. Similarly, you can sense many different sensations while performing the same action.

I was hoping we could unify all those things into one. When I think about manipulating things with your hand or arm, it's hard to separate the fact that you're sensing while moving the object. It's hard to separate touching an icon on my screen from the fact that I feel it and see it—it's actually doing something at the same time. Separating sensation from action doesn't seem to reflect what we do. I can't separate them. Every time I do something physically, I have to sense it. I thought that was a very simplifying concept.

Maybe we don't have to resolve this right now. You can have them separate. The point is, if I want to attend to something physically by moving my finger, I have to give it a location, and someone has to figure out how to move the finger. If you want to call grabbing something by moving your finger to that location a separate operation, that's fine. But in some sense, it's outside the learning module—more into subcortical processes, and how we do that could be different. Does that make sense?

I'm not trying to have action independent of sensation. I'm on board with them being a completely combined system, and it's probably just a conceptual distinction. As long as that distinction is kept outside of Monty itself, that's the idea. In the brain, the parts that control movement and the parts that get input from the limb—how that's implemented subcortically, I don't really care. If it starts affecting how we think about the learning algorithms, then it makes a bigger difference. I'm indifferent to how we do it outside the learning models, but if you think it actually changes things, we're approaching the point where we may have to start thinking about it. This is what we have in software right now. The actions you can specify in software are like, "move this part of the body 30 degrees left." The software isn't going to handle motion planning for you. We do that outside. How you do that depends on the physical manifestation. If it's a body with tendons, that's one thing. If it's a robot with six joints and stepper motors, that's another. There would be no learning module—just a technique. The module would just say, "put your fingertip in this location," and another system would figure out how to get it there, regardless of the number of joints or the system. That's a solved problem—there's no learning, just location.

As a general rule, we want learning modules to be as generic as possible. We may not always achieve that, but we want to be as close as possible. When it comes to sensor modules and the ability to move sensors and manipulate things, that can be very specific to the implementation. There could be multiple ways of implementing it, depending on the system. For example, you could have a camera in the corner of your room that doesn't move but can attend to a location, or a camera that turns and looks, or a camera on a robot that moves to look. It doesn't really matter how you do that. But I can listen to the rest. Thank you.

Are you okay with that answer? Yes, I learned a couple of things and I'm okay with the answer. I just want to make the diagram clear. This diagram doesn't show anything besides sensor patches; it just shows them pointing directly to the learning module. It doesn't show any way of moving the sensors or the entire system. Actually, I didn't notice that until now. I grabbed this and thought it was a different image. This gets to the point we made earlier: a sensor modality has complexities that aren't reflected in this picture. This just looks like you're taking something from an image patch and putting it into the learning model.

So, in the way you're thinking about it, the sensor module includes the sensor and subcortical modality-specific processing, but it also includes access to a motor system of some kind. Think of it this way: the sensor has to tell you where in space it's sensing. That's one of the things we define. It's not just what's on the tip of your finger; it's where your finger is relative to the body, and that has to be provided. The eyes have to tell you where they are relative to the retina—what distance, and so on. We're already inputting location information, and from the sensor, that doesn't come from the retina itself. It comes from a whole bunch of other body parts that are somehow informing the cortex where the eye is pointing at the moment, and where the finger is relative to the body, which is a very complex calculation, but something is taking care of it.

All we do now is say, in reverse, I can tell the sensor, "I want you to attend to this location," and then it's going to do that. It can either say, "Here's the location I'm attending to," or, "Here's the location I want you to attend to." There has to be some processing that handles that. A cortical column, which gets to a learning module, doesn't do that. Each of those has its own motor output, though.

What I'm trying to understand is the map of those motor outputs—sensor module, motor outputs—and then who gets the final say. In the cortex, there are these layer five cells with a motor output. No one knows what they represent. They know the motor output because they project somewhere that makes things move, but they don't know what the encoding represents. I don't know of anyone who's ever studied it; maybe someone has, but I'm not aware of it.

If I have a bunch of cells active, what are they saying? Are they saying "contract a muscle"? They don't look like that, because they don't project to muscles. Nothing in the cortex projects to muscle. The cortex doesn't contract muscles; it sends a signal to someone else who figures out which muscles to contract. What occurred to me recently is that maybe these layer five cells are passing location information, not muscular information, because we know the cortex operates in location spaces. That's a possibility, but we do know it's not sending out muscle sequences.

We think the subcortical areas are doing that, because someone is. We absolutely know that. The pericalcarine has neurons that project to the muscles that move the eyes. They do.

Is there any interplay between those? Are the outputs of those layer five neurons in cortex going to the superior colliculus? Yes, they do. That's the point. You look at these layer five cells, and they all project somewhere that ultimately results in movement. Even the ones in what we used to call motor cortex, which is really just somatic cortex—if they're going to move your foot, they'll project down to some part of your spinal cord, where there's a plexus of neurons, and that group of neurons then somehow sends a signal to move muscles. Those are coordinated activities. Those sets of neurons in your spinal cord can do complete reflex reactions, like moving your leg away from something. In theory, what you're sending to that system is—we don't know, but lately I've been thinking the simplest thing is to pass location information and let it figure out what to do.

Maybe in this diagram, there are arrows—motor arrows—coming out of learning modules back to sensor modules. There might be location signals that generate behaviors. Good distinction. So, location arrows coming out of learning modules back to sensor modules, and then actual motor arrows from sensor modules down to whatever moves the actual body. Who knows how that happens? We want to let that be up to the specific system we design.

Right now, we have this one subcortical motor area responsible for all moving parts. Do we really have one of those, or do we have independent sensor modules that each move, say, my left foot or my right hand? Clearly, I need to be able to move my sensors independently. For each sensor, you have a processing block that computes the appropriate command. I can touch something with my left hand.

The question is, I'm consciously making that decision. There's a difference between "look at the mug" and "touch the mug" as far as movement. Even "touch the mug" has differences between using my left hand and my right hand. On the spectrum, you have eyes, ears, and different parts of your skin that can all attend to that spot. We're going to attend to that spot, but who actually does it? Is it my left hand, my right hand, or just looking at it, or both? There is some sort of cortical process—it's not just a signal that's thrown out there to attend to a spot. Somebody figures it out, because I can consciously decide to do it with my fingers or my eye. I have some control over this. In the columns in V1, they project to areas that control eye movements; they don't control your hand.

It's not like there's just one processing point that everything goes through.

We could implement it that way, and maybe that would be effective, but that's certainly not the way the brain does it. Maybe we can do it a different way. On a simple robot, you might have a single subcortical section that figures all that out for you.

I would still strongly advocate for separating the sensor patches from the motor movement because I feel that motor commands are not necessarily modality-specific or patch-specific. You can specify a location to attend to, and the motor area can determine how to move a sensor to that location. It should be separated from the individual sensor patches. Do you think vision and touch should be separated, or should they be part of the same system?

The motor area could decide the best way to sense a location. Neuroscience tells us that's not accurate. In neuroscience, every column we know of has its own motor output that projects subcortically. Visual regions project to the superior colliculus, and touch areas project elsewhere.

This division of labor is largely managed in the cortex. Sensors are not the same as actuators in the body. We've already discussed this today, and I don't think that's the right way to think about it from a neuroscience perspective, but I'm okay if you want to consider it that way. However, there's a problem: I can't decide to use my left hand versus my right hand for something if that decision isn't part of the system. That would be limiting. You can still have this as part of the Monty system; it just wouldn't be part of the sensor patch.

The learning module can still output information, but it would be processed by a motor area instead of a sensory area.

Alright, that's a distinction we can let stand for now. Just keep in mind it might lead to problems in the future, but I don't see an issue if commonality fuses it later. If you look at cortical columns, the tactile columns send information back to move the fingers, and the eye columns send information back to move the eyes.

If you want to say it goes through a separate channel for actuators, do you consider the eyes actuators? They're moved by muscles, so yes, the muscles would be the actuators. As long as you develop a system that works, that's fine. Anything outside of Monty that's modality-specific and implementation-specific can be tried, and we can see what works and what doesn't, and what the limitations are. One reason I feel separation is important is that motor commands are not just for sensing; they're also for changing the state of the world and for communication. You can do many things with motor commands outside of sensing. One could argue that when I move my hands for sign language, I'm moving my hands, not necessarily sensing anything. Whether you're moving to touch something or just to position them, it's the same action.

I'm fine with this for now. My prediction is that we may revisit this and find problems, but I'm not certain. A quick question: the proprioceptive system, where you get feedback about the actual position you achieved—where would you place that?

Good question.

I think we have an answer in the current Monty architecture. There's always prediction happening: you decide on an action, predict the outcome, take the action, and then update. In the elevator analogy, you're updating something other than the sensor you moved into position. There's something else that senses whether you've achieved the goal, and then you correct if needed. You can put your arm at a particular angle and have feedback that you've achieved that angle to some degree of fidelity, independent of whether you actually touch an object. There's something else sensing whether the goal of the muscular action was achieved. It's all in dynamic tension. I've been trying to isolate these things. One assumption is that we know where our sensors are relative to the body, and the proprioceptive system plays a big role.

If all we need to know is the sensor's pose relative to the body, we can offload that to another system. It's not part of what's going on here. But there are problems with that. If I want to move my fingers from one location to another, I need to know if my elbow will hit something along the way, and if so, I have to adjust my hand to get my finger there. There's a lot going on, so I'm not arguing that's sufficient. For now, it's a placeholder: let's assume we know where our sensors are relative to the body, we can move, and we can attend to a location via movement of the hands and eyes. We don't have to worry about the proprioceptive system per se, but there must be something equivalent providing this information. I wouldn't say that's the final answer because I see problems with it. Our bodies somehow know how to avoid obstacles when moving a hand. It's not just about finger position; it's also about the elbow, arm, and body. 

I agree, and that supports the idea of making the motor system a separate component. That would be the easiest approach for us—let someone else handle it. It depends on the specification.

Thanks everyone for attending some of the discussion.