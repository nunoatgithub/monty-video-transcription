So, as you all saw on Slack, I'll be talking about learning rules in the brain today. This is part one of what will likely be a two-part presentation. Unfortunately, Rami, the second part will probably be in September after our other researchers, Hoji and Scott, join, but I'm sure there will be a chance for you to see it as well.

The general aim is to cover the diversity of learning rules in the brain, but I'll focus on the computational details and their implications. The main motivation is to explore the significance of these rules for Monty and hopefully inspire ideas about how we might use them now or in the future. Please stop me if anyone has questions.

This presentation will not cover the full diversity of learning in the brain, and I won't be covering it at a particularly biological level. I discovered a presentation from Jeremy Forrest before I had put together too many slides, and it provides a nice summary of different learning rules from a biological perspective. I urge everyone to watch that if you haven't had a chance.

You could watch it after this as well. If you're wondering whether this is about learning in the brain but not focused on biology, it will become clear. That presentation was about the biological underpinnings and reasons that weights change, whereas this is more about what spike time dependent plasticity means in terms of what you can encode and how network dynamics change over time.

The reason I care about this is that, as we all know, Numenta believes there are unique advantages to biological learning rules, at least in certain cases. We already leverage some of these in Monty and in Numenta's prior work, but there's scope to expand this. I probably won't talk about it today, as it's better discussed after covering all the different types of learning rules, but these rules unlock various benefits we could realize in Monty.

Learning in Monty is, in some respects, brain-like. All the learning we do is local. In the context of brain learning rules, "local" means the information needed is available to both the pre-synaptic and post-synaptic neuron. That's all you need to determine the change; you don't need to transport information from another part of the network. Monty already does this. These changes are rapid, associative changes, strengthening or weakening connections between different parts of the network.

They're not necessarily represented as connections, but in terms of associating a certain feature with a location in space, that's a rapid change. These associations are also sparse; for example, when we learn an object, we only store the locations and features we care about, not every possible location on an object.

We want to keep these properties and avoid, for example, introducing backpropagation, which would break some of them.

The key thing that's not present, but could bring significant benefits, is high-dimensional feature representations and the learning rules associated with them. The classic example is SDRs. Using these representations unlocks things like fuzzy matching, with representations encoding features or objects. I won't discuss the specific benefits of SDRs today, but rather, if you're representing things with a more neural population, what biological learning rules might function in those high-dimensional populations. I won't just talk about SDRs, and I won't get to SDRs and Numenta's learning rules today. I'll start with more classical firing rates and STDP to provide context for Numenta's learning rules next time.

Any questions before I go on? No? Great, thanks.

When we talk about learning in the brain, it could be at many levels, including behavioral and biological processes like changing DNA expression, but I won't be talking about those. I'll focus on processes at the level of individual neurons.

Everyone is probably familiar with this, but just to clarify the structure of neurons: a pyramidal cell is one of the most common excitatory neurons in the cortex. It has a distinct structure with dendritic branches and a long axon that projects and sends spikes to other cells. The dendritic branches receive inputs, as does the soma, the body of the cell. These dendritic branches have synapses on their spines, which are tiny protrusions. For example, another axon may pass by and produce a small branch or directly form a synapse that connects to the spine.

The knowledge of this kind of structure is key to Numenta's representation of neurons. Rather than having just a weighted sum input to represent a neuron, you actually try to represent the structure in the way neurons learn, with apical dendritic segments, basal dendritic segments, and the feedforward input at the soma. These can combine at the cell, and it's worth mentioning that if you hear the term "post segment," it refers to a spatial cluster of synapses that are near each other in space.

Are you going to talk about the difference between apical and basal? I will eventually, but not today. That's pretty different, because the apical region has a different impact.

A couple of high-level points before we get into more details: you can broadly categorize synaptic plasticity, or learning changes at the neural level, into functional (weight changes) and structural (wiring changes). For example, if two neurons project their axons to each other, the weight between them can increase so that if the blue neuron spikes, the impact on the green neuron is more significant, making it more likely to spike. This is represented by an increase in weight, caused by various biological processes. Alternatively, you can have a wiring change where, through different mechanisms, a synapse forms where one did not exist before.

In general, this is more the kind of plasticity that Numenta uses. Jeremy talks about this, and there's evidence that the fidelity of these weights isn't very high. There is evidence of weight persistency, but it might be the equivalent of having very low, very high, and maybe two levels in between, like a form of quantization. In distal dendritic segments, the impact of an individual synapse is much lower, so it may be literally binary—either it impacts or it does not.

Another high-level point is that the connection properties and how they evolve over time fall on a large time scale. If you imagine connection properties becoming stronger or weaker, this can happen on the order of milliseconds (short term), all the way up to days, where you'd have structural plasticity and actually grow new synaptic spines. In between, you can modify aspects of the synapse for medium-term potentiation, increasing or decreasing the strength of the weight, and then more long-term, permanent potentiation. This would be a weight change, while structural plasticity would involve a dendritic spine that is inactive but can become active through rapid changes without growing a new one.

You can think of these two shorter-term changes as traces that might serve some purpose in the short term, versus more persistent changes that last much longer.

A final caution: if you have a background in neuroscience, this is often taken as a given, but you may not have come across it otherwise. There's something generally referred to as Dale's law: neurons of a particular neurotransmitter type—excitatory or inhibitory—will generally only release that type of transmitter when they connect to other neurons. This means neurons are either excitatory or inhibitory, and progressive changes to a weight will never flip it from positive to negative or vice versa.

It's worth pointing out that inhibitory synapses are quite different. They don't have spines and are generally not viewed as participating in learning, though there may be exceptions. Inhibitory synapses are involved in more background computational processes. The formation of new synapses and spines is generally not believed to occur with inhibitory synapses, and they lack spines. Another big difference is that, while excitatory synapses are largely similar, inhibitory synapses show a huge diversity of different types of inhibitory neurons, each with very different temporal dynamics and impacts on the postsynaptic cell. There are about five or ten completely different types of inhibitory neurons in the cortex. Algorithms that operate in the cortex are often highly dependent on inhibitory neurons for functions like sparsity and synchronization—these are the computational background, while excitatory neurons are where memories are formed and learning occurs. The excitatory synapses are like the memory of the system, and the inhibitory neurons are the control system, similar to CPU instructions. This is a very different view than in deep learning. At some point, it could be interesting to explore the different types of inhibitory neurons and their temporal dynamics, though the data on that is not as clear. For example, the soma of a cell generally does not have any excitatory synapses on it, but it does have inhibitory synapses. Excitatory synapses are limited to the dendrites, while inhibitory synapses are found on dendrites, the cell body, and the axon, resulting in a very different effect in the system.

To get into the broad category, I'll be talking about Hebbian learning. This is the classic associative rule: "cells that fire together, wire together." It was proposed by Donald Hebb in the 1940s, and one of the clearest demonstrations at the cellular level was by a Norwegian physiologist in the 1970s, who showed long-term potentiation when neurons spike together.

At face value, it's a simple learning rule, but immediate concerns arise: if two neurons are firing a lot and the weight between them keeps increasing, where does that end? In a naive implementation, weights can grow unbounded and the system becomes unstable.

A useful perspective is that any form of Hebbian learning needs some form of homeostasis to balance the system, and some form of competition, which connects to what was mentioned earlier about inhibition, to make the system interesting.

This leads to examining three forms of Hebbian learning: classic firing rate neurons, spike timing dependent plasticity, and Numenta's HTM neurons. The first two focus on weight plasticity, while HTM neurons are more structural.

A classic firing rate neuron has an output dependent on a weighted sum of inputs, where input neurons spike a certain number of times per second—their rate. Typically, there's some nonlinearity that determines the output firing rate of the neuron.

For a network to learn with Hebbian-style plasticity, you update the weight based on its current value plus a change—the Hebbian term. The magnitude of the input and output is always zero or positive, never negative, and the learning rate is a small factor that slows down learning.

If you naively implement this with a bunch of input neurons and an output neuron wired up this way, the weights would likely continue growing unbounded.

One popular way to address this is to introduce a homeostatic term, called Oja's learning rule, where there is a negative effect on the weight that is proportional to both the size of the weight and the square of the output neuron. The reason I'm discussing this is because it's interesting to understand what a firing rate can learn to encode before moving on to more complex neurons. I'm not familiar with this rule, but I've always thought of a synapse as a physical structure with limits—it can't get very big and can only release a certain number of synaptic vesicles at once, so it's physically limited. This rule implies it's tied to the activity of the neuron, which seems odd. I don't know why that term would be like that.

This equation may reflect what we see in biology, but I don't think it's actually what's going on. You can imagine a hard balance that's also controlled by the firing rate of the neuron, so the synapse can only get so big. You could simply say weights cannot go above a value of 1.0, for example, and that's a legitimate approach, but it doesn't solve all the problems. Regarding what this homeostatic term might represent, the squared term wasn't necessarily fit to biological data, but you could imagine that every time a postsynaptic neuron fires, it releases some factor that degrades synapses—a decaying term. From a biology point of view, I've never heard any evidence for that. It may be a useful rule, but I don't think it's reflecting biology as far as I can tell. Synapses are physical structures with limited area; they can only release so many synaptic vesicles and just hit that limit.

Most models, even the HTM neuron, have a decay. That's not the same as a limiting factor; it's a decay, and there are many ways to implement it. In HTM, it was bounded between zero and one, naturally, because it's binary, and then there's a decay. The synapses come and go, and we do forget. Part of forgetting is that if synapses aren't used, they decay, so you don't want to keep everything active. We forget things a lot, especially as we age. I agree this is probably not the most biologically plausible implementation, but it's interesting because Oja was the first to show this.

In this network, we have one output neuron that receives input from multiple neurons. If there's some statistical structure in the activity of the input neurons, the output neuron will pick up on that. If certain presynaptic neurons fire together (i.e., they're correlated), with random weight initialization (all weights positive), the output neuron will fire more when those neurons fire together, and those weights will increase up to a limit.

On the other hand, if you have another neuron that's poorly correlated with the others, the homeostatic term will dominate negatively. The other neurons drive the output neuron, so Y is high, but Xj in this case is low, so if it started with a nonzero weight, it will decrease. You don't want too many neurons firing and learning at the same time. This is before inhibition, which is really handled by inhibitory neurons. For now, imagine a single output neuron. In reality, we have fast inhibitory basket cells that enforce sparsity, but I don't want anyone to think this is exactly what's happening biologically—it's just a way to express some concepts.

To recap, neurons that are correlated when firing together will experience weight growth to the output neuron, while uncorrelated neurons will see their weights decrease. The result is that Y will learn to respond to the variation along the first principal component. If you're familiar with principal component analysis, in a cloud of points in higher-dimensional space, there's a lower-dimensional structure—the axis of greatest variation. That is the dimension to which the Y neuron will be most sensitive.

This is an interesting aspect of the network. I remember this was a big deal in the early nineties when backpropagation had just come out. This was referred to a lot and was considered significant. What makes it more interesting is the homeostatic term—all you need is that, and you can get this property. But to Jeff's point, inhibition is very important. Anti-Hebbian learning is a term you may hear; as the name implies, it's the opposite of Hebbian learning. For cells that fire together, there's actually a growth in a negative connection between them. If you imagine multiple output neurons and multiple input neurons, any given output neuron, if it's highly active, will inhibit the others. Per Dale's law, in the brain, it wouldn't do that directly; there would be an inhibitory neuron that connects first, which is why inhibitory neurons are called interneurons. It would connect to that neuron first, and when that inhibitory interneuron spikes, it would then inhibit the others. In this toy network, you get a winner-take-all scenario where one excitatory neuron in the output layer, when it wins, inhibits everything else.

Without this inhibition, if you had a bunch of neurons in the output layer, they would all learn the same thing—the first principal component. With this competition and inhibition, the first neuron, or one of the neurons, will learn the first principal component and inhibit the others when it's most active along that component. The other neurons then learn to represent the orthogonal principal components.

In this way, you can get an output where, if the output layer is of lower dimension than the input layer, you can naturally take a hundred-dimensional input and compress it down to, say, five dimensions in a PCA decomposition.

Any questions on that?

That roughly makes sense. The spatial pooler is a variation on this idea, but instead of each neuron being orthogonal to the others, you have an N-winner-take-all. It's not that one cell wins and all lose; a subset of cells win, then another subset wins, and so on. In the brain, it's not just one cell winning. It's the same basic idea, but more of an N-winner scenario. 

While we're on that topic, in terms of encoding more continuous properties—like how confident a neuron is that something is in its receptive field—is that something you ever thought about adding back in? Firing rates or firing probabilities? We didn't have firing rates, but for continuous variables, we used encoding that was distributed and changed slowly with the continuous variable up to some resolution. We didn't have firing rates as such, but we did have that coarse coding. You see that in the brain. We always felt firing rates are too slow to encode anything useful, because you need to integrate over hundreds of milliseconds to get a reasonable value from a single neuron's firing. But with population coding, in five milliseconds you can get a pretty accurate scale. It's clear in the brain that some parts are firing rate dependent, and some parts of the cortex are that way too. It's not that this doesn't exist, but when we talk about the representations we form in the cortex, there's much less evidence for that and much more for sparse and distributed representations. 

A neuron that drives a muscle fiber to contract is a rate-encoded neuron—the stronger the contraction, the faster it spikes, and you slow it down or speed it up. The time frames make sense for that, and some of those firing rates come from layer five cells. There are firing rate-dependent neurons, but when we talk about encoding information in models, the evidence suggests it's more sparse and not firing rate dependent. There are experiments—like in V4—where within a hundred milliseconds you can recognize an animal, or even less. If you think about information coming to the retina, processed through a couple of layers, through the thalamus, through V1, and eventually to V4, there just isn't time for any firing rate coding.

The only thing is, if you are sampling from a large population, even if each neuron only has time to give one spike, if those neurons are emitting spikes randomly with some statistics, as long as you sample from a large population, you could still approximate the firing rate input. That's still population coding, but yes, you could do that. I see what you're saying.

It's not like the same population is encoding completely different scalar values just by changing their probabilities; there's an actual shift. You definitely see that in the brain. It's confusing because in the literature, people slide between examples—someone might study a policy of the snail and show neurons that work a certain way, and the general assumption becomes that neurons throughout the cortex work the same way, but that's not true. You have to be careful; there's a lot of evidence for this kind of neuron firing, but the evidence in the cortex is quite different. You really need to know which neurons have been studied and in what scenario.

Is there a consensus on which side of the synapse learns? Do you learn by gaining more receptors, or by gaining more vesicles that can emit neurotransmitters, or both?

Honestly, I'm not sure. I think it's a mixture of both, but it probably varies. Primarily, it's the postsynaptic cell. A big part of it is the area of the synapse itself—how much area there is at the tip of the spine, where vesicles containing neurotransmitters pass across the gap. How many can pass at any moment depends on several factors, but the area is important. A well-learned synapse will have more area, even though these are tiny areas, compared to a skinny synapse. Another factor is that when vesicles release their neurotransmitter, they have to be replenished on the presynaptic side, and there's not an infinite supply. After a synapse fires and passes its neurotransmitter, there's a period where it can't do much because it hasn't regenerated new vesicles. How quickly it can replenish vesicles is important. It's a complex equation, but mostly, learning is viewed as happening on the postsynaptic side. Synapses get bigger mostly on the postsynaptic side; you can see the spines getting bigger. Most learning is generally viewed to be on that side.

With Rojas, did I get the rule name right? It assumes information is flowing back to the synapse, which is easier if it's postsynaptic or presynaptic. All of these rules, at least our rules, require in the HTM neuron that there is a back action potential. Once the cell fires, the spike goes down the axon, but another spike goes back up the dendrites to the point where the dendrite was active and receiving information. That is the learning signal—it says, "Yes, I did fire; this was useful; you should strengthen this." This allows for coincidence detection at the synapse. These synapses are not at the soma; most are far from the cell body, so they need to know if the cell spiked. There has to be this back action potential.

A final note: I brought up Oja's learning rule because it's a classic example of how to learn in these networks, but it's not the only way. You could have a hard bound on weight values, which might exist for biological reasons—this would be like clipping in machine learning terminology. Another classic learning rule worth mentioning is the BCM learning rule. Oja's learning rule learns the principal components, but if you look at the receptive field properties of neurons in V1, they don't correspond to the principal components of natural images. Instead, they correspond to local feature detectors like oriented bars. The BCM learning rule learns more like these; it's not that different from Oja's rule, just a bit more complex in how weight changes depend on the firing properties of the postsynaptic neuron.

Next, I was going to talk about spike timing dependent plasticity, because this is where you get into how weight changes actually happen in the brain. Before, this was a high-level approximation of what we seem to observe; this is much closer to how synapses actually change in the brain over time.

It gets its name from the relationship between the timing of spikes. If you have a presynaptic neuron sending a spike that arrives at a postsynaptic neuron, and the presynaptic spike precedes the postsynaptic spike, you get long-term potentiation—increase in the efficacy of that synaptic weight. If the ordering is reversed, you get a decrease, or depression, in the efficacy of that synapse. This only happens within a narrow window around the timing of the two spikes. The output neuron is looking not just at whether both are firing, but at whether there's evidence the presynaptic neuron was actually responsible for its spiking. It's a causal learning rule. It's impressive that evolution figured this out; it makes sense given the amount of noise and random spikes in the brain that you would want this kind of sensitivity.

This is another way that neuron activities get separated. We talked about inhibition. Basically, if your input couldn't have possibly been responsible for my output, then I'm going to tell you to forget that. This separates out neuron representations according to this rule as well. I mentioned that this is actually how weights are being changed. I thought it was worth showing some actual data from biological experiments where they show the percentage change in the efficacy of the synapse as a function of when the pre- and postsynaptic neurons spiked and the difference between those. You see it forms this curve that is often shown quite nicely.

This is essentially a coincidence detection learning rule, a temporal learning rule. It means you can have output neurons that are sensitive to the timing of input neurons, particularly neurons whose signals arrive together within a short period of time. This is a computational study from one of the groups most interested in this kind of computation in the brain.

What they're showing here is that along the y-axis, you have the neuron IDs—neuron 100, neuron 1. These are all in the input layer. The blue dots indicate when they spike over time. The spike times are approximately random, except for the patterns shown in red, where they fire within a short time window relative to one another.

If you look at the firing rate of the population, shown in the bottom box, the population's firing rate never changes significantly, even when these patterns are presented. Similarly, the firing rate of this input neuron across the simulation isn't different from neurons that aren't part of the input pattern. A neuron limited to a simple firing rate code and firing rate-based changes to its weights is not going to become sensitive to this input pattern.

But a neuron with spike time dependent plasticity does. It means that neurons can become sensitive to these temporal patterns. They identified the red neurons to show the pattern, but sometimes it's just one of them, like neuron 50, where there's just one red dot. The firing rate is taken across the entire period, about 600 milliseconds, and calculated by looking across the different neurons. The red or blue coloring is just to indicate the inserted patterns; they artificially increased the synchrony during those times. These neurons are firing with a distinct pattern, but the statistics of their firing, if you just look at the firing rate, are the same. It's synthetic data, questioning whether a neuron with STDP would learn to respond to these hidden temporal patterns. At the red points, they artificially changed the pattern, but the neuron doesn't know that—it's unsupervised learning.

Everything discussed here is unsupervised and local learning.

An output neuron with STDP learning rules does learn to respond to these patterns. This suggests that equipping neurons with more biologically plausible learning rules increases the capacity for encoding information. We're no longer limited to just firing rates; neurons can respond to particular patterns in time as well.

Are they saying the neuron learns to respond within the red pattern? If you scrambled the red patterns, it would not respond anymore, but they don't know if that claim was made. It's not really sensitive to the exact timing of the red pattern. You could set the time constant to be insensitive to the precise order, allowing for a lot of jitter, or set it so that neurons have to fire within a very tight window, like three milliseconds, of the prescribed pattern.

Suppose you did exactly what they did here. The claim is that it's sensitive to the timing of the input neurons, but after learning, if you scrambled the timing of a few of these red dots, it would probably still fire. It's not that sensitive. It depends on the time constant.

Especially regarding each synapse's time constant—does each synapse have its own? If the time constant is fairly wide, I agree. There's the STDP time constant, which is already fairly broad, but I'm thinking more about conductance and similar factors, where the time over which spikes can be integrated by the postsynaptic spiking neuron matters. I'm not sure if that's true here, because if you look at those diagonal patterns, if it went differently, I don't think it would make any difference to the output. Not in this experiment, but there could be a setting where it would, so we can discuss this next time. You can construct that, but I don't think STDP enforces it. The claim is that STDP causes neurons to become sensitive to timing, but that's not true. You'd need heterogeneous conduction delays. You can do it, but STDP alone is no longer sufficient.

Just for anyone curious, I'll talk about this next time. This is a form of synchrony. They have fixed the pattern, but there's another concept called polychrony. If they're firing with a specific pattern and you have asynchronous or heterogeneous conduction delays all landing on your postsynaptic neuron, depending on those delays, these might all arrive within a very narrow window—within a couple of milliseconds of each other. You can imagine an output neuron that's sensitive only to this pattern. If you reverse the order, and they all fire within this time window but in a different order, the output neuron would not respond.

But that's a big if, because you need those delays to be correct.

It's worth tying this back to general principles of Hebbian learning. Although things are now dependent on time, it's still fundamentally a Hebbian learning rule. You still need some form of homeostasis. Otherwise, weights will grow unbounded. One of the first papers to computationally study STDP included an inhibitory factor where inhibition slightly outweighed excitation. This determined how much the weight would change depending on the timing difference of the spikes. The weight could be increased (positive learning rate) or decreased, and this determined the sign and magnitude for these curves. They made the inhibitory learning factor larger to balance learning in the network and prevent runaway effects. It's not necessarily good biological evidence, but it's an example of the challenges when working with Hebbian networks.

For a real spiking neural network, these are generally very challenging to get to learn anything. In the study, it was still relatively simple stimuli. I'll show in a couple of slides, but they had a variety of biologically motivated, local factors. There were four different homeostatic factors working together to achieve stable learning, so the learned representations could be maintained over an unbounded amount of time, even with constant noise being fed into the network.

Similarly, competition is still important. This was a follow-up study from the Masquelier group. In the first study, they showed that a single spiking neuron could learn a single pattern, similar to a firing rate neuron learning a single principal component. In the follow-up, they showed that multiple output spiking neurons could learn multiple different input patterns and become sensitive to those.

LIF stands for leaky integrate-and-fire, a simple mathematical model for a spiking neuron.

These output neurons learn different patterns.

In general, this is pretty neat. There was a lot of excitement about temporal coding in the brain, especially between 1996 and 2006, with many papers in major journals. Around 2007, there was a shift in the zeitgeist, with concerns about plausibility due to noise in the brain. The success and adoption of artificial neural networks as models of the brain may have dampened enthusiasm for temporal coding. The temporal sensitivity of STDP is a biological fact. The existence of temporal codes in the brain is less clear, but it's probably somewhere in between. What's interesting about STDP is that it's a hybrid. The study I mentioned earlier had four different homeostatic factors. These were stimuli—imagine a 64 by 64 grid of neurons, with the white neurons firing at a higher rate, showing different inputs. The spike times of those neurons are governed by a Poisson distribution, so they're random in time, with no synchrony. The neuron receiving these inputs is a spiking neural network with STDP-based learning, and it will learn to encode these different patterns.

In general, this connects to the fact that at low firing rates, STDP is more temporally sensitive, whereas at high firing rates, it starts to act more like standard Hebbian learning. Part of the reason this emerges is that if you imagine a neuron spiking and then another neuron spiking, but now the presynaptic neuron is firing so frequently that it fires just before and just after the postsynaptic neuron, you get this overlap. This regime occurs at very high firing rates, where the process becomes more symmetric. In biology, when neurons are experimentally observed in this regime, they tend to act more like simple Hebbian weights.

but then, as mentioned, at lower firing rates, these two are going to be very different depending on whether the spike happens before or after. You will see more asymmetric, temporally sensitive changes to the weights. There are neurons that spike very rapidly, but that's not typical for pyramidal cells; those tend to be more inhibitory cells. It's not clear that this particular phenomenon actually occurs very often. Most neurons don't fire that fast for this rule to come into play. Some inhibitory neurons do, and in some places you see that, but it's pretty rare in the cortex. 

You do see a group of neurons that don't emit a single spike, but when they become active, they emit a train of spikes, anywhere between three and five, very close together. It just complicates things. I was going to talk about bursting next time. I know that's something you account for, because that can lead to what they call metabolic changes, which are changes in the chemistry or slower changes in the cell. 

I've always found that STDP was overemphasized. You said it had its decade and then declined. There's a very simple explanation, which you've already given: there are a lot of time delays and uncertainty in the brain, and many neurons have a background firing rate, which is pretty low, but they're firing all the time at some low rate. That's common. There are a lot of random spikes that don't mean anything. This rule basically says, let's only pay attention to the ones that are truly causal—if the presynaptic neuron is a good predictor of the postsynaptic neuron, we want to remember that, and it shouldn't be much of a delay. We're not trying to look back in time; we're just saying, right now, this neuron happened before this one. It cleans up the learning rule, and if the presynaptic neuron fires after the postsynaptic neuron, it couldn't have predicted it, so forget that one. 

There are simple mechanics here that explain why the brain works like this, as opposed to it being an important part of how information is encoded. It's more about how synapses learn, but I've never viewed it as an information-theoretic composite. As you point out, a lot of people wrote papers suggesting otherwise. 

Maybe it's not black and white in the brain. Some neurons are more amenable to encoding information in more temporally structured activity. For example, spikes that arrive from thalamic neurons, like LGN, onto the cortex, tend to be more driving inputs, where just a handful of spikes can be enough to cause a postsynaptic spike, whereas in many other cases, you might need hundreds to come together. In general, you can show that these kinds of small, sparser, high-magnitude spikes—if there was a temporal code—would use those kinds of synapses to encode and transfer information.

We proposed in our neuron paper an alternate explanation: the vast majority of synapses are far from the soma, on dendrite branches, and any individual spike out there has very little effect on the soma. Typically, a researcher will measure the voltage inside the soma and see what happens when a presynaptic neuron fires. They'll say, this synapse doesn't have much effect, so we need hundreds of them. Others will have a large effect, so we only need a few. Our alternate explanation is that for those far from the soma, you don't need hundreds to summate; you just need 15 or 20 near each other, which generate a dendritic spike, and that dendritic spike has a large effect on the soma. Most studies didn't account for that possibility; they just measured, "this synapse doesn't have much effect, so we need a lot," instead of considering what happens if 15 of them fire at the same time in the same dendrite segment. That would have a very different effect. That was a big part of what our neuron theory says—it explains that difference and gives a functional reason for it.

We shouldn't discount the possibility that binary temporal and more binary codes could be working together. One of the temporal codes we've discussed recently is the idea of groups of neurons firing in sequence during a phase of a background frequency, like within the theta frequency. Within the 100 milliseconds of that cycle, neurons go through a sequence of patterns. That's a type of temporal code, which is important in the brain, but it's not the kind of temporal code most people think about. Most people think of temporal code as the exact timing of individual synapses or cells, which is clearly important in some parts, but there's much less evidence for that in the cortex.

Maybe the way to put it is that everything you said here is correct, but it's also very confusing.

Hopefully not too confusing. You presented it clearly, but a lot of research hasn't really incorporated the insights we've had. I don't sit there and say, "Hey, why aren't you listening to us?" I don't feel that way at all, but I do feel like we really understand some of this pretty well, and the vast majority of researchers out there don't think about it that way. They just don't even consider these possibilities. There are a lot of detailed experiments on plasticity that show very specific types of plasticity rules that I found correlate really well with what we did in the neuron paper. I have a detailed list of things, like that P minus you pointed out, for example, that in that paper, there are experiments that validate that.

What happens when a dendritic segment fires and the postsynaptic cell does or does not end up firing? What is the impact on the dendritic branch? There are specific things that happen there that correlate really well with our learning rules, and there are some we haven't yet incorporated, but there are a lot of detailed things that are quite important at the end of the day. This level is interesting, but it's hard to know what to make of it until you actually put it in an algorithm, put it in a network, and have it do something real. A few years ago in the computational neuroscience community, they didn't do much of that at all. I don't know how you could combine all of these to recognize an image of a cat, for example. They show that oriented bars emerge, but that's about it.

The two Hebbian learning rules I talked about today—the firing rate-based ones—can learn input features, the PCA of the data, or more local features like oriented bars. Then STDP-based spiking can learn temporal spike patterns, but they can also learn what kind of firing rates neurons can. To your point, they're extremely complex to set up, maintain, and stabilize. In general, it's a great way to start pulling your hair out, trying to work with spiking neural networks. I don't think we should be tying Hebbian with firing rates. Hebb did not talk about the firing range; he just talked about firing together. I didn't mean to imply that we use Hebbian learning in our neuron role. It's interesting to read what he actually said. He did not say, "neurons that fire together wire together," as an example. He had a very specific sentence or two that's very telling. It's a lot more precise, I think. If you don't mind, I'd like to make a comment back to the question Michael asked earlier.

Think about a synapse: you have an axon and a dendrite, and they might be near each other and may not have a synapse at all, but they're sometimes referred to as a potential synapse. Even in that situation, where the axon and dendrite are near each other but not connected, they will have a Hebbian learning type of rule—they will form a synapse. The brain has the ability to do this. It says, "I know you're not touching, but you should be." There are other neurons or cells in the brain that facilitate this. You start off with a very skinny little synaptic spine, like a little thread, and it works. As the synapse trains and gets reinforced, the spine gets bigger. We talked earlier about how, when it gets bigger, maybe more neurotransmitters get passed through, but there's a lot of evidence—and some other people wrote about this and we adopted it—that once a neuron gets to a certain size, even if you still train it, it doesn't really pass any more neurotransmitters. Its weight value doesn't go up, but the spine gets bigger. The spine continues to grow even though it's not transmitting any more synapses. The idea is that the spine is a sort of permanence—how hard is it to forget this synapse? Once you've built it and it's structurally there, it's a lot harder to get rid of than if it's just some skinny little thing. We adopted that as permanence. It's a learning that is not about increasing efficiency or transmission, but more about permanence of memory. Things that have been reinforced a lot, even if you don't use them for a long time, you'll still remember them. Something you learned recently has just as much effect as the old synapse, but you might forget it quickly if you don't continue to reinforce it. I just wanted to point out, back to what Michael asked earlier, that learning is not just about increasing efficiency; it's also about permanence of these memories.

The presentation from Jeremy included a lot of discussion around the biology of that. In my next presentation, I'll go into the algorithm for how the permanence values change and what the significance of that is. You had the link in the presentation—I'll share it on Slack. It was two presentations from November 2021. I'll send it on Slack—Jeremy Forest. I could just look, but I wanted to mention that.

One thing to remember is that at its core, Hebbian plasticity is working in both these firing rate and spiking networks. Some homeostasis and competition is needed. When we talk about these algorithms, the same will still apply because, as you were saying, Jeff, it's fundamentally Hebbian plasticity as well.

That's what I'm going to focus on next time: structural plasticity, the role of actions, and the role of dendrites. There are a few other topics I'd like to touch on briefly. I mentioned polycritic and how it might connect to myelin plasticity. Dopamine and serotonin are also worth mentioning briefly. Other recent topics include contrastive learning, representational drift, objective coding, and possibly local-ish backpropagation in the brain. It would be interesting to touch on these.

Hopefully, this was a useful starting point. Thanks. That was great. Thanks, Niels. I have slides about potential benefits, but I think they'll make more sense after we talk about the other learning rules and those are all clear.