Thanks everyone for coming. Today I'm going to talk about transformer networks and how they relate to Monty, exploring similarities and key differences. Please feel free to ask questions as we go along. The aim is to identify possible cross-pollination between these systems—what we can do to improve transformer networks while continuing to develop the true Monty system, and whether ideas from transformers might be useful for Monty. In this discussion, I'll focus on the current implementation of Monty rather than the Thousand Brains theory in general, as it makes it easier to discuss concrete details and draw explicit parallels.

At the last By the Bay, we identified the need to improve communication between the Monty and Mega teams, to better understand what each group is working on and to influence each other. If Monty is a big, epic castle on a mountain, Mega is this transformer, and we want a kind of cyberpunk castle on a mountain. That's a stretch.

Did you use thermal diffusion to change that picture? It was a Dall-e 2.

It will also be interesting to contextualize some of the recent excitement around transformers. If, through their design, they have incorporated concepts from the Thousand Brains theory, that might explain some of their achievements, though they remain limited. In terms of limitations, there are many, including the sheer amount of data required. It might be possible to understand this better. Transformers being used as embodied systems is becoming an active area of research, and I'll discuss a couple of papers in that area.

Here's a quick preview of some similarities I'll touch upon. It's important to emphasize that similarity does not mean equivalence; these systems are not identical, but there are clear relationships between them. For example, the connectivity that exists, the concept of a common representational format, how voting operates and relates to self-attention (which I'll spend the most time on), reference frames and their relation to positional encodings in transformers, and embodiment, which is now a research topic with transformers. There are also many differences. The connectivity can be similar but is also different in many ways. Whether there are explicit object models is a major difference, as is how learning takes place. When I talk about embodiment, you'll see that some current approaches with transformers are quite curious compared to the Thousand Brains mentality.

Before I get into it, I hope everyone is fairly familiar with Monty and transformers, but I'll cover them briefly. If either is new to you, stop me and we can go through it in more detail. With the learning modules in Monty, we're getting features with a pose to update evidence for explicit, graph-like models. To process that, we have a buffer of where we were before, and given our new position and how we move through space, we get a displacement that tells us where we're moving. Each hypothesis along this space is based on our memory of these graphs. These hypotheses can also be used to vote with other learning modules. We can then output a pose of the object and potentially an action.

A transformer is composed of key blocks repeated throughout, and the main feature that defines transformers is the self-attention operation. I'll go into detail about how this works when I talk about voting. Generally, these were created for natural language processing, where you have a series of token representations—words in a sentence—processed in parallel by the self-attention mechanism, which compares all the words and how they relate to one another. This produces a new output for many different sets of weights, and the feedforward system combines that information. This process is repeated, creating a stream where each token passes through on its own, but during self-attention, it looks at and combines information from surrounding tokens. This is very different from the typical fan-in in CNNs and similar architectures.

And it's also important to note that with transformers, at the input you have positional encoding. Otherwise, there's no information about where items are relative to one another, since they're processed in parallel. You need to provide some sort of positional information. In the original paper, this used sinusoidal and cosine functions, which were literally added to the feature representation vector to capture position. 

How would encoding be done with a sinusoidal or cosine? Essentially, it's a bit like grid cells, where you have a set of sine functions of different frequencies. If you're the fourth element in a sentence, you plug in four into these different sinusoidal functions, which give different amplitudes depending on their frequency. Those values form a vector, and that's what's used. That vector represents an absolute position in a sequence of words. The sinusoidal line allows you to infer, similar to grid cell modules, where your position is. The reason for this choice is that it works for sequences of arbitrary length, and nearby positions have slightly similar encodings.

It would be interesting to understand that mechanism more explicitly. In grid cells, the sinusoids represent vectors of movement, while here you're updating a static representational location. I'm not sure how parallel that is. From my understanding, it's pretty similar in implementation, but the key thing is it's one-dimensional. This is a common theme in transformers, possibly because they originated in natural language processing. Even with 2D images, they tend to treat them as a raster and use one-dimensional embeddings rather than 2D.

This might be an important aspect of the idea today: it's one-dimensional. There are different ways, but it's not clear if they're representing just an ordinal order—first, second, third, fourth—or if there are temporal aspects. I don't think so, since they're not processing in time; it's just a position. Think of it as four different 1D modules with different frequencies, together representing a position in 1D. The frequencies are just parameters—hyperparameters. Who determines them? It's just a parameter, like the spacing in grid cells. Grid cells represent a position in space, and here it's updated similarly.

With sequences, you can have a one-dimensional order, but also a true one dimension where words are like spatial distances, like notes in a melody with gaps in between. It's not just a sequence; it's order plus time. Here, it's sentences, so it's ordinal in and ordinal out, but it's a continuous mapping. If you had a continuous input, you could use this approach. For 2D images, you could have a 2D sinusoidal encoding, which would be more like grid cells.

With grid cells, you can't separate them from movement, but here there's no movement. Maybe when I get to the embodied stuff, that'll make sense. I'll talk more about the different flavors of positional encodings, but the most common approach is to literally add the positional encoding. That's very different from how we think about it in Monty, where positional representation is explicit and separate from the feature representation. In transformers, they're entangled, not for any deep reason, but to keep computations low.

So, a bit of background. Moving on to the first topic, which is the architecture and connectivity. Here, I'm showing Monty and its general architecture. I'm emphasizing the representations being computed and handled in each unit, which is why I've separated pose and ID representation. This makes the similarities clearer. In Monty, you have multiple learning modules, potentially with a hierarchical relationship. Not all connections are shown, but as currently implemented, sensory modules come in, and it's important to highlight the voting and lateral connections. There is a one-to-one connection from a lower level to a layer above. Currently, we don't have any fan-in connections, though it's been discussed. The current view is that interactions across a layer of processing are primarily handled by voting.

One detail: multiple lower-level modules can fan into a high-level module if they're within that high-level module's receptive field. If a higher module has a larger receptive field and the lower ones cover small parts of it, they can all input features into the higher one. However, the higher module can't distinguish their locations from each other. If this was broken into three submodules, the location information would still be a one-to-one mapping. There's also a skip connection, where a learning module gets direct input from a sensory module, and another learning module gets direct input from a sensor module, not necessarily the same one, but from the same location space.

For transformers, if you ignore the feedforward layer, you can imagine each token as a column where tokens communicate laterally via the self-attention mechanism. When a token is processed, aside from self-attention, it's mainly in a single feedforward track, rather than a massive fan-in operation. Skip connections are present as well. The spatial representation is injected at the start, where the sensory input may be the positional encoding. In this case, "pose" just means the positional encoding. It's not the same as pose in Monty; there's no explicit graph. These embeddings learn what they learn, and there's evidence they can represent positions in space, but the only explicit spatial input is at the start. Even then, it's not very explicit, as it's usually just added on top of the feature representation.

The lateral connections in transformers are self-attention, not voting. The lateral connections decide where the attention heads are at that point in time. Voting can be considered a special case of self-attention if everything is set up appropriately. Both do an n-squared operation—every token interacts with every other token. Voting in Monty does the same, but the key difference is that there aren't learned weights; it's based on relative displacement. It's a question of how many tokens or columns around you to compare to. In transformers, the attention mechanism is learned, while in Monty, the graphs are learned internally within each column. Transformers don't have an explicit graph representation; it's implicit in the weights and how they transform between tokens.

Self-attention is unusual, and this will make more sense with later visualizations. To clarify, when you put object representation into these purple boxes and compare it to a learning module, does that mean it can represent entire objects, like models that can't be seen in one input, or does it just recognize the embedding input and learn everything through the connections?

One of the principles of Monty is that each column can learn complete models of objects, even though the input to a column never sees the entire object at once. It can still recognize the object by moving over it over time. Would a transformer architecture be able to do that, or can it only recognize the object through the weight matrices that connect it? No, it has to use the lateral connections. One key difference is that there's no self-recurrence in each token, so there's no way to update its representation except through lateral inputs. It's as if you decided not to have any sprawl-world type recognition, where you build up models over movement, and instead focus entirely on voting. This is changing a bit, though.

The naive transformer doesn't maintain any state. When the next input comes in, it's completely fresh, with no concept of successive inputs. Instead, everything is flattened—time is flattened into a single vector in a feedforward way. That's not the case with GPT architectures and autoregressive transformers, where you successively feed the input in and actually maintain the previous state by concatenating information.

There's a kind of self-recurrence, but it's unusual. If you imagine these as cortical columns, it's like dealing with sequences and time by adding or recruiting more cortical columns through time to handle new inputs, rather than having each token represent an object and maintain a stable representation. Niels, when dealing with long sequences, I don't think that's quite true, because you're concatenating what was previously evaluated with the current input, processing them together, and there is a form of recurrence in that sense. Do you mean like Transformer XL or Perceiver? I would need to look at that, but definitely Transformer XL does this by always concatenating. That's very different from, for example, the Onyx transformer, which can take many forms depending on the operations.

There was a place where it could add, but only up to a certain amount—maybe double what the normal one was. That would be saved and re-represented on the next cycle, so it's a finite window moving across. I'll check out Perceiver and double-check that, because Transformer XL passes in representations from the previous step using a fixed window. When it calculates self-attention, it concatenates this representation, but I'll look into Perceiver to see if they do something similar. It's possible someone has tried that, since it's essentially making each one of these an RNN. That's about the only way to get past the N squared limitation: Perceiver has a limited amount of N squared and then tracks across, allowing much larger token sequences and deriving something with more scope, but not all at once. It's not self-attention all the time; it's building up. I could be wrong about how much they're retaining in the cross-attention. Cross-attention usually comes in on the decoder side, but I think they use cross-attention at the input to handle large token sequences, like 10k tokens, and then process them accordingly.

Even if they don't specifically do it, you could see how it could be done, as long as you have some state built up that you feed in on the next cycle as part of the input.

Unless there are more questions on that, I'll briefly focus on some immediate differences. First, as I mentioned, the lateral operation in transformers is very dependent on learned transformations and weight sets. How would you think about attention heads here? Would each attention head be considered a model? If you have 10 attention heads, could each column have 10 models? I've wondered about this.

Given how neural networks operate in such unusual spaces, I'm sure they can do many more than 10 models, but there is likely a relationship between the number of transformer heads and the number of models that can be learned. I'm not sure it's a one-to-one mapping. In some cases, each head at various levels learns some aspect—noun, verb, syntax, or something similar. By whatever mechanism, other heads learn other parts. During training, each head seems to develop some functionality, but I'm not sure it's a complete model; it could be more like a feature. Those features are then combined in the fully connected layer, producing new inputs for the next level, which then tries to discern further structure.

The actual structure of what happens in the fully connected layer and how it consolidates information to be fed as input to the next one is something I haven't seen people discuss much. You can imagine that you're managing to create streams of capabilities that, at the very end, if you're classifying, there's no real model—it's a collection of features. I'm not sure whether attributing a model to the heads is accurate, at least in the current incarnation. The closest thing to a model, and I'll talk about this when discussing reference frames, is a bit like how capsule networks don't have an explicit reference frame for representing one feature relative to another. It's more about the weight transformations that are learned, such as learning, "I've got a nose, this is where I expect the face," or "I've got an ear, this is where I expect the face." It's more like that with the learned attention heads, or maybe also the feedforward layer. But there's certainly no explicit spatial map that things are being laid down to.

Along with that reference frame, there seems to be a missing aspect: the idea of moving your sensors under volitional control of the brain through space is a huge part of this. We're not just passively receiving information; we are actively exploring space, moving in different directions, processing motion, and keeping track of where we are, which requires reference frames. So it's more than just reference frames—there's a whole sensorimotor aspect that's missing here. That is very much missing.

Can these work across modalities too? I will get to that on the next slide. Anyway, there's no top-down influence, no explicit reference frame, no self-reference. Those are just some of the differences, highlighting at the architectural and connection level.

Common feature representations: to answer your question briefly, yes, they do. Common feature representations across modalities are important for enabling multimodal models. In recent work moving beyond just language into large multimodal models (LMMs), they do exactly this. The basic idea is straightforward: you take whatever you're trying to represent—pixel patch, word token, etc.—and do a learned projection of that representation into a fixed-size vector embedding. As long as those vector embeddings are all, say, 64 dimensions, you can treat them the same in your transformer and assume it will learn to handle the fact that one is an embedding of a pixel patch and another is an embedding of an English word.

This is just an example showing that in practice. PaLM-E is an embedded, embodied multimodal language model from Google that came out recently. This was one of the main papers discussed in the DLCT presentation a couple of weeks ago about scaling robotics using large language models. They take the query, turn it into tokens, take the image, turn it into tokens, and all of that can be passed into the model and used together.

You'll see that the embodied part of it is a bit odd, but I'll come back to that. I wanted to start with those two comparisons to ground the rest of the discussion. It seems like a fairly limited way of doing multimodal integration, but I can't say that for certain because I need to study it more. The way I think about the brain is much more flexible than this, but maybe I just don't understand this yet. Is it that different from taking a sound wave and a few rod-cone responses and encoding that as an SDR? We don't do that. We never do that.

We only—That would be a terrible way of doing it. The way the brain converges is purely through voting. They're trying to correlate objects with objects, and it's not a one-to-one correlation. A sound doesn't have to be correlated with a single object, and a single object can correlate with a single sound. It's a much more dynamic method. You mean with the spatial pooler? No, we've never done this. I'm just saying how we've described it and how we've written about it—it's all done through voting. There's nothing done in the spatial pooler. There's no common mechanism; it's just voting. One set of auditory models or a set of tactile models can inform a set of visual models, but it's not one-to-one, and it's not always appropriate. I'll just leave it at that. I look at this and say, this is cool, but to me, it's not the same as I envision multimodal integration frames. I want to point out that at some level you can say, yes, there are objects in these modules and there are poses, but they're really not the same. We can use the same words for them, but they're not the same. So I guess we can be agnostic about this whole embedding operation. We're not certain exactly what we would do, but the main thing is once these are input, then throughout this stream, at each level of processing, you started with a visual feature here, you started with a language feature here, whatever. You're going to continue going up those paths, and they're going to continue interacting laterally. I'm highlighting that because that is similar, but it seems like we've told them these things are one and the same, and that's not the way it is in the brain. They're not one and the same. There are different models and different modalities which can, at different times, help each other, but we're not trying to figure out what's in this picture and correspond the language with the picture. That's not what's going on in the brain. That sounds like what they're doing here. They're saying, can we use these two modalities to combine them in a single representation? I don't think that's happening in the brain. There's a real advantage to not doing it, but this could solve a certain set of problems if you want to describe images or take words and make them—it's fine, it does a good job of that. I just want to say we shouldn't accept these words as meaning the same thing. No, thanks.

Voting versus self-attention.

I'll briefly go through voting in Monty and self-attention in transformers, and then try to get into the concrete details of how they're potentially similar.

In Monty, let's say we have two fingers touching a mug. We're going to get two different features at poses, and these are going to be processed by different learning modules, which will develop different hypotheses for where we might be on that object.

We have information about the kind of displacement, including the rotation between these two sensor modules. This is key to the voting process, where we take the hypotheses coming out of a particular learning module and, depending on which module that information is going to, transform it by the appropriate displacement. This gives us a new set of points for where we would expect to be on that object based on where we are in this hypothesis space. That's what all these clouds exploding out are doing—there's only a small set of points, related to where we were before, that with this displacement move to this point on the handle. At that point on the handle, if you look at your nearest neighbor, you're going to add the evidence that's coming in, which may be negative, that's associated with your neighbor. You'll get these hotspots forming where these are consistent with one another.

Self-attention in transformers is a way to figure out representations, to determine what other representations are looking for them and are related, so it can bring that information together and condense it. When I first talk about this, it won't seem to have anything to do with voting, just to be clear. But when I talk about this as a special case, hopefully the parallel will be more obvious. When you do the self-attention operation, you have three important things: a query, a key, and a value. For every token—in this case, every word in a sentence is represented by a token embedding—you have a key, query, and value. If we focus on the word "it" for now, this is the word we're processing with self-attention, so it's going to project a query, basically asking, what other representations are around me that might be related or relevant? In answering that call, every other token produces a key. The reason for these arrows is these are vectors, so you can think of them as a point somewhere in space. To determine how much agreement or how appropriate the two are for each other, you do the dot product. The query dot with this key is going to be a larger value, and this is passed through a softmax, which gives the attention mask, the self-attention layer.

Here, the value—the probability associated with the animal—is higher. This fits with the idea that it knows it needs to look for something like a noun or an object it could be referring to, and "animal" in this sense has learned to look for words like "it" that might refer to it. But you could make the same argument for the word "street." How does it distinguish between those? This is a toy example, but in general, that's the trick: is "it" referring to "street" or "animal"?

Another thing is the voting operation. It's based on this embedding, which is determined by both the feature—whatever that is, the object type representation—and its position encoding. Maybe it has learned that immediately before it, or early before it, it's not going to find anything; it usually follows later in the sequence relative to the word it's referring to. That's interesting because you could say, "The animal didn't cross the street because it was too wide," and in that case, "it" would refer to "street." So it can't be that simple.

It's impressive. I just don't understand how it does it. I'm not accepting a simple explanation because it still leaves me wondering how it works. What were you going to say, Vivien?

Basically the same thing. If you substitute the last word for "too wide" or "too dangerous," it just depends on the last word what "it" refers to. That's a good point. This kind of one-to-one comparison is a bit odd, and how it integrates this may be a hierarchy thing. I think this is literally extracted from an actual network; I don't think someone made these values up.

It's possible the embedding is for the word "it" after it's been processed by five layers, and by that point, it's understood. You can look at the evolution from layers. I have that same library, and if you look from layer 1 to 12, it changes a lot. Probably on the first layer, it was referring to "street," but by the second one, it already encompasses the entire context. That information helps decide whether it refers to "frame" or "script." You can clearly see the evolution within layers. That helps.

What are we going to do with this? I want to make sure I'm clear. The token for the word "it"—there's nothing else besides just the token, right? There's no meaning to that thing other than its associations with other words. When I say the word "street," I have all kinds of associations. I can visualize it, think about it, imagine touching it or crawling across it. It's not just a word; it's not just a token. In these large language models, the tokens are in some sense meaningless. The only meaning comes from their associations with other tokens. It's a token referring to a token, but there's no grounding for what these tokens mean. I think there's a learned representation for each word, so "street" and "road" might be very similar. But it's all just correspondence between tokens. There's nothing else besides that. All the definition of what a token is comes from how it relates to other tokens, and nothing else. That's true for every aspect of this. Even in multimodal models, you might have an association of "three" and an image of "three," but even those are just tokens. In image processing, it would see what sorts of images are associated with what sorts of words in a stream. Would the brain have something similar, just with neural activity at a certain level?

I think the brain starts with a model of the world. Dogs, cats, birds, and we have it. Some animals like us have the ability to express that model through tokens, but it goes from a model of the world to a communication token. Here, we start with the communication tokens, and there's no model of the world other than the relationships between the tokens. There's no tie to specific movements or behaviors—it's just tokens to tokens. That's how it works, but I want to make sure I understood it correctly. I think that's fair to say. There's no explicit model, no sensorimotor understanding. What if you learn a model in a sensorimotor environment and then associate it with language? That's what we do, but you could do that here too. If you have that sensorimotor model, yes, but there is no sensorimotor model here. Some models do.

Let's get to that.

It's very hard to understand sensorimotor models without reference frames, so I need to understand that better.

Okay. Go ahead, Vivien.

Is it predetermined where a token starts and ends, or can it learn that on its own? For example, "tired" split into two tokens. When starting with natural language sentences, there are two steps: tokenization and token embeddings. Token embeddings transform the input into a fixed-length vector. Tokenization is how you parse sentences into components, and there are various ways to do it. You could do it at the character level, but then sequences get very long, which is computationally inefficient for transformers. At the word level, the vocabulary becomes enormous. In practice, most use subword tokens, splitting at almost syllable-like units, as in recent GPT models. Is that true—syllables? This example shows some that didn't work entirely, like verbs or concatenations. This is a simplified version. Do they really work at the level of syllables? Normally, they start with single characters, but there's a fixed vocabulary, like 50,000 tokens, so they aggregate the least common ones. Some tokens are long, like a full word that rarely appears, while others are just one or two characters.

They take a large corpus of text and find an efficient way to pick out 50,000 tokens that cover all the text, like a clustering operation. It's interesting and statistical, so the tokens themselves may surprise you. It's an interesting way of tokenizing language, but it moves you away from the meaning of the tokens. Some letter sequences have no meaning on their own but are useful for building models like this.

This approach is very statistical. The tokens are chosen that way. That's interesting—I didn't know that. I have a question from earlier: at layer five, whatever was the token "animal" is now a conglomeration of something else.

The fact that they're pointing backwards through the discrete beginning token is what's taking Jeff off track about what's possible. That's a fair point. It is a bit misleading. I didn't follow that, Kevin, but I think I understand this. It's important to understand that this is not a direct mapping but an inferential one. By layer five, you won't see the token "bee" or "animal"—it's gone through five layers of transformer processing and is now a more abstract representation. You can look back at the receptive field, but what you're seeing is more abstract; you've probably already parsed out whether it's a noun, verb, or other features. What they're trying to convey is a concretization of an abstraction, which is much richer. They're putting words on it so we can make sense of it, but it's not literal.

The final key element of the self-attention operation is that, after determining with the queries and keys how much a token should attend to others and integrate information, each token also has a third representation—a vector called its value. This is meant to capture more of its semantic meaning. When updating the token, you use this value representation to pass information onward. The key and query are about crosstalk—figuring out who to attend to—while the value encodes your own representation to pass to the next level. How does that work?

To get from the current token representation, you do a matrix multiplication with a learned set of weights to get the value. The important thing about self-attention is that it takes all the values and combines them based on the attention values. The larger the probability, the more influence that value has on the updated representation. Where did the value come from? The network learns what a good value representation is through backpropagation. The key and query transformations are also learned through backpropagation.

It's a parallel operation: the token's vector representation comes in, and each goes through separate matrices to produce these values. That allows you, at any stage, to produce a query, key, and value for any incoming token.

Okay, as soon as you tell me it's learned by backpropagation, I stop thinking of it in terms of anything I can conceptualize. It's an encoding scheme, if you wish. It's basically about how these things relate to each other statistically, but there's a huge amount of context associated with that. When you pull things apart into query, key, and value, they run through what are called multiple heads, which are representations of self-attention. They all get mixed together in another layer, and this repeats for multiple layers. There's a huge amount of abstraction in this process. Through backpropagation, it learns how things are related at multiple levels of representation. I agree that the terms query, key, and value are actually more confusing than helpful. If you just said this is a value, a vector determined by backpropagation, that's fine. The only thing that determines it is one of the mathematical operations used to produce an output. It's not really a query or key, and you can't really say it's a semantic value. Everything is semantic; it's just something determined by backpropagation. It will probably tend to cluster similar representations. We can look at it later and try to figure this out and maybe determine something interesting about it.

Let's say the animal or whatever has some value vector, and then you have the adjective "blue," which is another vector. Through self-attention, it decides to prioritize and combine those. You can imagine the new vector direction is a combination or interpolation between these two concepts. It's an open question how useful that actually is, but that's the basic idea of transformer self-attention.

Just to briefly show it again in a more vectorized, matrix-free format: we have our embeddings, which are the vectors coming in. As pointed out, this is layer five, so they would be more abstract than word embeddings at this point. Those are the ones coming in, and we can transform them by these learned weight matrices to get the query, key, and value vectors. There's one for each word or token coming in. This shows the same operation: we do the dot product between the query and the key, including for the token itself, essentially asking, "Should I attend to myself?" You do this for all the other tokens to compare. Then you do the softmax to get the associated probability. The output is the scaled sum of all these values based on their softmax values.

As Kevin mentioned, there are multiple heads involved. What does that mean? You can learn one of these weight operations, but it's reused at every point. For every token, you use the same operation to generate the query vectors and so forth. But you might want to do different types of transformations to represent different things or spaces. That's what the multiple heads are doing. Each input token is processed by all these different attention heads separately in parallel, producing a bunch of different outputs. The feedforward layer at the start is essentially a multi-layer perceptron that combines those.

Finally, moving on to some comparisons: the proposal is that self-attention can do something like voting as a special case, with significant caveats. The reason I'm covering this is not to say that transformers have already solved Monty, but to highlight what could make transformers more Monty-like. If we can first understand what similarities there might be—I'm having trouble parsing this sentence. Are you saying that transformers' self-attention is doing something like voting, or that it could if we modified it? It can, assuming the right weights are learned. Maybe that's not happening now, but it could be made to do something like voting. There's a reasonable chance it is, because the weights are basically the identity weights. Hopefully, that'll become clear.

This sounds conceptually different because in Monty, in voting, two models that have learned models of completely different modalities can still vote on object ID and pose. Here, it seems like the entire model would have to be multimodal—the whole weight matrix and everything. The entire weight matrix would have to be multimodal. They're not separate models that vote; it's one big model that is multimodal. Is that what you're saying? Yes.

As long as the representations look similar, there's no reason they can't communicate. Maybe I'm not understanding what you're saying. In Monty, in voting, we never communicate any model details to each other. Features are never communicated through voting; you don't know if the votes come from a touch model or a vision model. Here, it seems like it does explicitly tell you what it is sensing, like the embedding. Even in Monty, you could argue there's a certain degree of that. For example, a 3D model of a mug isn't going to vote with the abstract concept of a family tree. There's always going to be a degree of "can things vote?" But if we assume the tokens are both object-level representations—like in a multimodal transformer, one has learned mugs through vision, one through touch—then those can vote, at least at the object-level representation.

Definitely, to vote, you need to know the model of the same object. In Monty, what is communicated through votes seems more limited than what is communicated in self-attention. It might be more of a special case—maybe bring that up again when I get to the slide later. That's why I'm emphasizing the special case, because self-attention is doing a lot of things, which relates to what you're saying: it can send a lot of information. This brings back the columnar view of Monty, with voting. We're going to compute these online, using the sensor modules. For each graph, we'll go through and send the transformed poses using this relative displacement, find the nearest neighbors to the receiving points on the object, and then add this incoming evidence based on your nearest neighbor.

This is the transformer with the column view. Hearing this reminds me there are really two types of voting. We've never really teased them out before. For example, if multiple fingers are touching the mug at the same time, we need to know the relative position—they're in the same modality, which is different. That's not going to occur if I'm going across audition and touch, where you're not relying on knowing the relative poses of the sensors; you're doing purely object-level voting. I want to point out that in a particular modality, like touch or vision, the relative pose of the sensors is very important, but I don't think it's important across modalities. I've always argued that there are different levels of voting going on in a cortical column, with multiple layers of cells in layers two and three, doing slightly different things. Here, we're going to talk about voting that doesn't go across modalities. It's not exactly the same as what's happening in this image—this is a single modality, and we need to know the features at relative positions to return.

They're not exactly the same, but that's a good point. How could self-attention be like voting? Assume the weight matrices we normally learn are all the identity matrix, or we just don't use them. The key, query, and value embeddings are the same—they're just the original vector representation coming in. What this means is the more closely those embeddings (the X vectors) are to each other, the more they're going to attend to each other. The token here is going to attend to itself, and about equally to this other token; similarly, vice versa, they're both going to ignore a third token. The output is essentially just a blending of these two similar ones, which might correct for a bit of pose disagreement between them.

That's where the similarity starts coming in. You might wonder if this only works when they agree strongly with each other—what if there's legitimate disagreement about objects? We've talked about this before: if the pose is mutually consistent for two different columns—say, you have a mug and a phone next to each other—the phone isn't going to tell the column processing the mug, "You should be seeing a phone," because with that relative displacement, you'd expect to be on a different object. I'm not claiming transformers are doing this, because their positional embeddings are very fuzzy and not explicit. But in theory, the self-attention mechanism has no issue with this: if different columns disagree completely, then using this format where the embedding is the original embedding for key, query, and value, you're only going to attend to yourself when you differ from everyone else.

If the weight matrices are just the identity matrix, how can the mug be recognized in the first place? The input will not be "mug," it will be a feature on the mug. How does it get to that stage? There are a couple of answers. One is, you could have this as a different attention head—an attention head that is the identity matrix, doing voting like this, but other attention heads doing more interesting things. I'll get to that later. The other thing is, you're absolutely right: because we have no reference frames updated over time through movement, we're relying entirely on voting, or rather, on lateral connections for understanding anything. If you're only doing literal voting, you're not going to get anywhere.

But the most important question is, what if you have a source of noise? It's all well and good if you've already decided you're a mug—you just say you're a mug, and that's happening in both cases, so it's not that interesting. What's interesting about voting is, what if a column is unsure? Can the consensus that's emerged around it in other columns help it become more certain? In this example, assume this central token has equal evidence for car and mug, represented by a combination of two orthogonal vectors.

When you do self-attention, it attends to itself, but also to other objects around it if there is evidence it might be representing them. When you apply the softmax, you get nonzero values for these other columns. The new value for this token becomes a blend of these different columns. How much it ignores or integrates evidence from surrounding columns depends on how certain it is about being a mug. If it's 80-90% sure, it will be more influenced by others; if only 10% sure, it will mostly attend to itself and ignore the others. As the number of adjacent columns increases, their effect becomes stronger.

Hopefully, this shows that self-attention can, under certain circumstances, act like voting. I came across an interesting quote: cortical columns are like attention-weighted interactions between different word fragments in a multi-head transformer, but they are simpler because the query, key, and value are all identical. The role of inter-column interactions is to identify these identical embeddings. Geoffrey Hinton said this regarding the GLOM architecture. When he introduced GLOM, many people drew parallels to Numenta's work, though he doesn't mention it in the paper, even though he discusses columns and describes voting. He does compare his work to transformers, noting that the key, query, and value vectors are all identical to the embedding vector.

There are caveats. The positional encodings are less explicit and entangled, with no clear notion of space as in Monty. This assumes pose voting at the object level. In Monty, voting happens at the object level—do we agree it's a mug?—but also considers pose, such as where we are on the mug. Here, it's more about where the mug is in space, maybe in body-centered coordinates.

There is no recurrence within each token. If you rely on this to converge toward a certain representation using voting, and you're 50% sure, the softmax might only nudge it slightly toward certainty. To move closer, you need multiple layers of processing, requiring a very deep transformer.

How deep are these networks? Large language models can have many layers. The original had 6 or 8 encoder layers and a similar number for the decoder, or 12. I don't know how deep the latest ones are—some have over 100 layers. That's very different from 12. In convolutional networks for images, there were hundreds of layers. Here, 12 layers means 12 transformer blocks, each with multiple layers.

GPT has 96 transformer blocks. GPT-3, for example, and GPT-4 as well. Without recurrence or feedback, you rely on the entire stack to get your answer. There is work on fast inference by exiting early, so you don't need to go through all 96 layers to get your answer, but you still have to train the whole thing.

Without recurrence and feedback, the system is quite different. We spend a lot of time thinking about compositional objects—objects made of other objects and reusing models inside other models. Is there anything equivalent here?

That was one of the main questions I didn't answer satisfactorily, so I don't address it here. I will talk about recognizing an object composed of parts, but that doesn't address arbitrary fast binding between things.

Self-attention can do that in the sense that, for example, if the word is "blue" and the next is "Volkswagen," that's a novel combination. The key, query, and value vectors might have been learned so that "Volkswagen" looks for adjectives and "blue" looks for nouns, allowing a vector embedding that combines them. But this is a mushy, additive way of representing things, not compositional. There isn't an explicit mechanism for compositionality. If compositional representations are more efficient for some data, the model might learn them for those aspects, but it's not generically compositional. If I learned a model of something in a revision network and it appears as part of a larger image, it's not a separate thing anymore; it's part of a larger object.

Will I say, "Oh yes, this larger thing has the model I learned earlier," or should I say, "No, I have a new larger thing and I'm just going to learn the statistics of its parts"? For example, if I knew what a bicycle pedal crank looked like and I've learned what pedals look like, and now I see it in different machines—it might be a stationary bike, it might be a human-powered wine press—I might say, "Oh, that's a pedal." Would it know to recognize the pedal and its affordances, or would it just see a wine press and treat the pedal as just some statistical part of it? I think it can do some of that. That's why all these answers are going to be fuzzy, because in a lot of cases, since it's trained on so much data, it will realize it's more efficient to think about pedals and cranks and apply that to different things, and it'll do it. But if it's only seen very few pedals, it's not going to do that.

If I saw a machine I didn't recognize before, like a wine press powered by bicycle pedals, I might be able to figure out what's going on. I'd say, "Oh, a human is going to use this to provide power. Where does the chain go? Okay, it goes over here and looks like it's going to turn this tub or something." I would go through that process because it's built of components I've seen before, and I might figure out how they work. Is there a way we could test that with just language? We could just input it to ChatGPT. I don't know.

I think Yann LeCun put them together—Yann LeCun recently had this thought experiment: putting a bunch of cogs rotating either clockwise or anticlockwise in series, and then asking, "Okay, one cog is turning. What is the direction of the nth cog?" A cog is a gear, right? Is that the British term? Yeah, sorry, like a gear. Maybe French, English, I don't know. There were a bunch of tweets about whether it could solve it. Sometimes it solved it, but only if you told it—Yann LeCun was wondering whether it could solve it.

I think it can definitely do general computation, but it's not Turing complete. There are problems that require polynomial time or linear time to solve, and it's not going to be able to do those things. For example, sorting requires n log n time, and if you give it a list of a million numbers, it's not going to be able to solve it. I guess this was designed to point that out. It's not a general computer. I don't like using "computing" because we think about digital computers, but I do think of computing in the more abstract sense. Some problems are known to require polynomial time to answer, and it probably won't be able to do those, like the traveling salesman problem. But no one can solve that perfectly. Computers can solve it, but it just takes time—sometimes more time than we have in the universe. Up to a certain point, it can solve it, but I think transformers won't be able to do arbitrary Turing computation.

It's funny because we use the word "attention," but it's really quite different. If I see something I don't understand, a composition of components I haven't seen before, I narrow down my attention until I see things I do understand. Then I look at the relationships between those things and see if I can find parallels to other things I've seen. It's not a big feedforward process; it's an interactive process where I have to literally attend to different parts of an image or different parts of something happening in my life. Here, they use the word "attention," but it seems like a very feedforward process. There's no recurrence; I can't walk through hypotheses serially. With the autoregressive stuff, it's just running the same algorithm, taking an output and putting it back in. It's what we call active prediction, and it can diverge very easily. It's taking a prediction, sticking it back in, treating it as ground truth, predicting the next thing, and treating that as ground truth. That's interesting—it's not exactly what I was talking about, but it's interesting. That's the thread Niels and I have on Slack. That's one of Yann LeCun's main points—why he says these are fundamentally limited. It's essentially only doing active prediction.

It's not grounded in any way once it starts going. That leads to its hallucination, because there's no additional information. If you and I look at something we don't understand, we try to attend to it in parts and get our hypotheses serially, but each time we get more sensory input. We don't just confabulate our way through the results. I don't say, "I think it's a dog. If it's a dog, then it's going to bark, and if it didn't bark, then it's going to say this." Once you give it the query, the input is just going to hallucinate from that point on.

I'm really trying to get a sense for the fundamental component differences here. It feels like we're moving, sampling the world—not randomly, but constantly. Sampling the world isn't just moving our senses; it's attending to different parts of the world, and we're building these models dynamically in real time. That part, I think, is done in brute force. If we're given an image, we attend to different parts of it. When it's given an image, it attends every possible combination. In some ways, that's been the history of technology: we can build machines that are better than humans in many capabilities. In some sense, one could argue that calculators are superhuman—they're really great. Sometimes you can argue that computer vision should be better than human, because you can do what you just described: look at everything at once, train on a billion things, but humans can't do that. So it shouldn't be a surprise that things like these chatbots are really good at faking it in some sense. The question is, is it more than faking it? Is there really true knowledge about the world inside that can be taken advantage of? So far, I don't think so, but I'm not certain. I'm asking questions to get a better sense.

It's very brute force, and you can do really well.

We, as humans, look at language, which is already a very abstract way of communicating an internal model state that I have to you. It's abstract from the beginning—it's already post-understanding. We're coming up with a description, where the tokens represent my internal representations that I'm going to communicate to you, which could be completely fake.

Here, we're starting with that, and that's all you get. You don't have any of the ground truth or knowledge about how the world actually feels, looks, and sounds. You just have what the words correspond to. We're really good at assigning meaning—someone will say, "This language is really good, there must be a human on the other side, there must be something really smart on the other side." In the same way, we might look at the output of a calculator and think there's a great mathematician on the other side, but until you get used to it, you realize it's just doing its thing. I'm just rambling. What else do you have? I guess a few different things. Just one last thing to say on this question of compositional objects: it's worth highlighting that where these things excel is in this kind of fuzzy combination of vectors that might represent different things. That's why it's impressive that something like Dall-e or Stable Diffusion can take the fuzzy concept of lettuce and the fuzzy concept of hedgehog and create something that seems like a reasonable interpolation of the two. But on the whole compositional thing, it's relying on self-attention, which doesn't have a very explicit sense of space. That may be why they struggle with something as simple as, "I want a blue cube next to a red sphere, with a yellow pyramid on top of the blue cube." It will get the colors associated with each object wrong as often as it gets them right. Is that right? At least Dall-e. I don't know about Dall-e 2 or Mid Journey 5, but that was an interesting example: Dall-e 2 could do all these amazing things, but it can't do three colored geometric objects. So it does really well if there's one thing. This is related to the compositionality issue—a hedgehog made of lettuce is one thing, but three objects with three different colors and three different positions is another. It needs to keep track of where they are in space and what's associated with what, and that's where self-attention seems to struggle. If Mid Journey 5 has solved that, I imagine it's just through brute force, because they are famously better at hands now, though still not perfect. That was another thing we talked about recently, which I think is a similar problem. It's one thing to have a fuzzy representation of a hand, but to have a very clear sense of five fingers, you need a more explicit representation of space. I do have a few things—let's see if we can get through the rest of this. Let's get to the conclusion.

Reference frames: do transformers work with reference frames? They use positional encodings at input, which is a small step toward something like reference frames. This is what we did with the temporal pooler—it's the same thing. We said, let's solve the simpler problem of one-dimensional reference frames.

Then, before moving on to the more difficult problem of three-dimensional reference frames, which is significantly more complex.

There's some evidence they have better spatial awareness, which I'll get into. Some with a neuroscience or psychology background have compared their computations to grid cells, since they have these kinds of spatial encodings.

I don't think their success is due to positional encodings—new models don't have positional encodings, right? They use Alibi, replacing it with a learned bias instead. What does that mean? You get rid of positional encodings of words? You have a learned positional encoding instead; you don't have the version that Neils was showing. But it's still a learned positional encoding. It's still position encoding, just not the same. How does it learn? Aren't I feeding in text one word at a time? Or no? How's your thing going? You have a bias for where that would be in the sequence, but you're not giving it beforehand. You're just initializing and letting it learn through backpropagation. So it's not exactly a position encoding. It's a mapping from, say, the fourth token in the sequence—a learned encoding would map four onto a certain vector, and you learn the weight transformation that does that.

That vector presumably is still capturing information about fourthness. I'm not suggesting it's specifically cosine or anything, just that there is positional information. When these systems are trained, the only information you have on the tokens is their positions relative to each other; there's nothing else. There's no additional information about the world—just these tokens and where they are in a one-dimensional reference frame. That seems like a fundamental truth: that's all you've got for your training data. There can't be any other positional information except that this thing was in this position relative to that one.

Language is a sequence; I'm not sure what happens when you do this on images. You could do a variety of things. Often, they just flatten the image into a bunch of concatenated vectors, but there is also work where they use more 2D relative displacement in X and Y. Even that's not what you really want about the world, because what's next to each other on your retina has nothing to do with what's next to each other in the world.

There are different flavors: it can be absolute, it can be relative. Reference frames—there's some evidence, for example, that visual transformers have more of a shape bias than CNNs and are getting closer to humans, maybe. If you look at this graph, the difference between VGG or ResNet 50 in general is much smaller than the difference between these visual transformers and humans. In this figure, we have different shapes, and the more you go to the left, at presentation time, you take these different shapes and put a random texture on them. You might have an airplane with the texture of elephant skin, and you ask the system or the human to classify it. Humans will classify based on the shape most of the time, which is what you're seeing here. Other systems, particularly old CNNs, will focus on the texture and ignore the global shape. This relates to our morphology models. Maybe adding these positional encodings is doing something for transformers, but it doesn't seem to be enough, maybe because it's not explicit enough.

Are transformers on that chart too? Yes, they're the pyramids, these yellow and orange ones, so you can see they're the best of all the different networks. The others are CNNs. That's a pretty telling chart. One caveat: there have been a few different papers around this question, and some suggest it's also just that transformers tend to be trained on much more data, so it's not totally clear whether it's something special about transformers. I think this one controlled for that.

This leads to recognizing, for example, a mug based on this kind of self-attention operation. This assumes a simpler form of self-attention, a special case rather than the full version. Basically, assume that the set of key, query, and vector weights are the same. When we're doing a particular attention head, within that attention head, we see basically the same key, value, and query.

We have these input representations—say, rim and handle—with associated positional embeddings. If you have these different but reused weight operations, you could imagine them learning to transform, for example, from rim at a particular location to mug at a particular location. This connects to what we've explored in the past: a more capsule network-type operation, where everything is directly learning to predict, given a feature at a location, the relative position of the object to it.

Once that's done, once you have those operations or outputs, you could compare these, and they would attend to one another if they had good agreement.

Regarding positional encoding and embeddings, there are many varieties. As Lucas said, they can be learned or hardcoded, as in the example I described at the start. They can be added, concatenated, in relative or absolute coordinates. They can even be recurrent, which seems odd given the mass parallelization that transformers do, but you can have the position encoding at the nth position be recurrently dependent on the position encoding at the n minus one position.

A key point is that they're typically at the input only. One thing I was trying to research but didn't get around to was how much work exists using concatenated representations and ensuring there's a separate representation at each layer. It seems there's a risk that spatial representations get washed out as you go deeper, unless there's a strong objective function ensuring their importance. I don't think rotary invariants are input only; I didn't implement them, so I didn't read the paper fully, but I know they're not input only because you have to change every attention layer. There's something happening at every layer. 

Which one is that? Rotary. That's the one used before Alibi. Is that the one used in PaLM-E? I think so, where they rotate the matrix. I'm not sure it's really embedded, but I didn't want to get too into it. Take a look at that. There was definitely one from Google where they rotate the embedding instead of adding to it, which was interesting.

Versus Monty, at the input we're likely to use absolute, body-centric coordinates in six degrees of freedom. The real computing happens at object-level internal representations, and all of this is very explicit. We also have stronger skip connections for positional information. While skip connections might help preserve some of this for transformers, in Monty we're keeping the spatial pose and related information very explicit and separate, making it more likely to persist deep into the architecture.

These are the limitations I just discussed. One last point is that all these transformations and weights need to be learned for transformers to perform these operations, whereas Monty learns quickly based on sensorimotor movement and then uses those graphs to directly determine the expected position over the pose.

Now, embodiment is starting to become a thing with transformers. Two prominent examples are Gato from DeepMind and PaLM-E from Google. Gato is interesting because, as Subutai mentioned, these systems can be autoregressive if they're generative, and they took that into an embodied setting. For example, it might receive a fixed prompt, then sensory input like a game screen, and output an action. The next token it predicts is its action, then it receives the next sensory input, and so on. This can be done serially at inference time. It might get a batch at once with visual input, but it does have a sense of recurrence by concatenating these, though there's a limit before past information is forgotten. It's also computationally intense because all operations are performed at every step. To address this, they used an architecture designed for a longer horizon, but there's no real recurrence within the network—it's recurrence in the input.

PaLM-E is a more recent work, extending the large language model PaLM for robotics with a multimodal model. For example, it can get a robot to pick up a bag of chips based on a natural language prompt and an image. The model outputs more natural language, which conditions the robot's action sequences. The way they handle embodiment is by taking the original prompt, creating a series of language steps, generating these sentences, and mapping them to movements. The model, as part of end-to-end learning, has to learn what outputs will actually map onto actions the robot can take.

They don't do anything to directly constrain that. They're going back through language to do that again. I don't have to say to myself, "Open the drawer and reach for the green bag out of it." Once it generates the language instruction, does it then go off and do the entire sequence, or does it keep—at any given time point, it has access to the instruction, which is the start of the sequence. It has all of the actions it's taken itself, like all the actions it's generated in natural language, and then it has the current visual input. But what happens if you can't open the drawer, or if something requires a change in plan? They show that with this: the human knocks the rice chips back into the drawer. At that time point, at the next time point, it already said "take the rice chips out of the drawer," but then the rice chips are back. It sees the image, so they're still there, so it basically just outputs the same action. It's not like it understands why it's gone back or anything like that. A classic example might be trying to open the drawer and maybe there's a latch I didn't know about, or a button I have to press, so it says "open the drawer," but it doesn't work.

It's possible it would have learned that, "Okay, I tried this, and I see the image, and on the next one I might attend more to the lock or something." I don't think it's inconceivable that it could learn to do that. But it feels like the main kind of weird thing about this is there's just a bit of a missing world model and continuous representation, for example, of visual space. It literally just has the actions it's taken and its current visual input, which is very different from how we would interact. They're just concatenating all of this on top of each other and then doing the full transformer operation across all of it. It's not clear to me that they might be able to get this kind of system working really well under certain environments. It's hard to say how good they can get. This is just the beginning, or maybe they'll run into fundamental issues. 

Do they then generate sub-policies to actually translate these sentences into motor commands or joint movements?

No, I don't think so. I can't remember if there's also a step where it first chains together these things.

It was either this or another paper where "bring me the rice chips from the drawer," and then it's meant to first create a bullet point of all the actions it's going to take. Then it says, "Okay, first action, do this." I don't think they do anything with sub-policies. Just from my experience, it seems so much easier to take seven steps than to actually take the rice chips out of the drawer. That seems a lot more difficult than just writing out seven steps. And what if the robot arm was in a different position than it usually was, or things are rotated? These could be very simple robotic commands if everything's fixed. I don't know if they considered that, or maybe it's something they're doing now, but in general they were trying to really lean heavily on the language model. With the caveat that it's maybe not very good at describing these lower-level things.

Just the other really big difference to mention is, obviously, you have either rapid sensorimotor reference frame building, few-shot based on real-world inputs, or you backprop on a significant chunk of the internet and absorb all the kind of horrible things that are out there.

That's another big difference. The last thing I wanted to talk about is how we can maybe bring some of these ideas together. Two caveats to mention: I'm sure some of these things have been tried in the literature, but maybe not in combination, and maybe not particularly in the context of embodiment. That would definitely be a next step if we want to think about this. Firstly, just check, and maybe someone here already knows. I think turning these token representations into something a bit more like an RNN—within that token, it's still part of a transformer, but it also updates itself based on its past history. In particular, if there was some sort of separation of pose embeddings from feature embeddings throughout the architecture, then I think this would get a bit closer to something more Monty, and would make more sense for something like embodiment. Rather than having to constantly concatenate the full input at every step—the full past and the full future—when you are doing embodied learning, because this is what I meant by this approach: they just keep adding new columns onto the brain, or recruiting new columns if you imagine these are columns. Whereas if we assume we have a fixed number of columns, you can still process the visual input and whatnot in parallel, but you keep track of history through this kind of more self-occurrence within a given column. That would be more similar to what Monty would do.

Making this kind of voting a special case might already be happening, but it requires that the attention has learned the identity operation. I didn't get time to check whether that's an observation that ever happens; it would be interesting if it does. Maybe adding some additional attention heads to the ones that are typically there could provide extra computation that the system can access, and the feedforward layer could ignore it if it's not relevant. This might be similar to skip connections, which perform the identity operation by passing information through the system. It would also be parameter-free, since there are no additional weights involved, unlike the other weights I mentioned for more transformer-type or capsule object processing. Because the key, query, and value are shared for a particular attention head, that would be about one third the typical parameter count cost.

These could be interesting inductive biases to try. Some of these changes could be made to pre-trained networks and then further fine-tuned, but there are a lot of details to consider to determine whether it would actually work or what problems might arise.

Some of the more complex things to introduce would be top-down connections, which might cause issues for backpropagation, and explicit spatial models. I'm not sure if recurrence would help those emerge naturally. The representations would be similar to some in the literature where grid cells emerged just through learning. As you mentioned, Jeff, compositional objects like rapid binding between reference frames and features are not easily solved without explicit spatial models.