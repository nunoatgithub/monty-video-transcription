Thanks. All right. Some of you may have seen in Slack that I shared a document I’ve been working on, with feedback from Vivian and others, about how we can eventually bring neural elements into Monty—why and when we would want to do that. This was motivated by the feeling that we often talk about bringing in SDRs, grid cells, and similar elements, but there isn’t always clear agreement or disagreement about why or when to do so. I thought it would be worthwhile to articulate all those points so we can identify areas of agreement, disagreement, or uncertainty.

I'll share my screen. Once this project is open source, the HTM community might ask why we aren’t using Numenta’s previous work in this project. This document is meant to show that we’ve considered it, and for specific reasons, haven’t done it yet.

Today, I’ll focus on the overall aims of this effort, particularly the cortical messaging protocol and how we represent features and reference frame representations in Monty.

Ultimately, we want to be empowered by the fact that Monty is brain-inspired and use that to make it a better system, but we’re not trying to simulate brains. We want to avoid unnecessary complexity and also make rapid progress, which sometimes means using simpler systems in the short term. However, we need to be careful not to invest a lot of resources and algorithmic development into an approach we might later abandon, resulting in sunk costs.

The first category is what we’ve been using in Monty since its inception and still use for certain things. Here, we have some measure that can be represented with a single number or maybe two to three numbers—for example, hue, saturation, or the magnitude of principal curvature. The sensor modules in Monty pass that as feature information to the learning modules, which store and match against it during inference.

This approach is intuitive, easy to visualize and debug, and, as Tristan pointed out, easier pedagogically to conceptualize what features are coming in. However, it restricts what can be expressed and how features can overlap or relate to one another. We know this is not how neurons do it. You could imagine a firing rate code for some features, but that’s not realistic. There are places with rate coding, but features from sensors are always distributed. We do this because it’s easier for engineers and humans to understand, but we shouldn’t abandon the neuroscience approach until we know what we’re giving up. At the very least, we should be able to revisit it later, but we have to be careful not to go too far down an unfruitful path.

This is how we’ve agreed to do it up to now, and this presentation is about explicitly spelling out what we’re doing and why, as well as alternatives that are closer to biology but not yet implemented. We might start with a mixture of approaches, such as using a hashing algorithm to map hue to SDRs so that similar hues have more overlap in their bits. However, it’s not clear there’s much benefit to doing that at the sensor level. Once you start processing information within and between cortical columns, this distinction can make a huge difference.

We’re leaning toward using this approach to pass information into the learning modules, but for communication between learning modules, we’re moving toward SDRs, which started with Rami’s internship.

I think we should eventually use SDRs from the sensor module to the learning module as well, for consistency, since the sensor module also uses the CMP.

There’s a fundamental difference between features like color and curvature and features like object IDs. I think there will eventually be advantages, especially when top-down learning comes in, if everything is in SDR.

I've learned repeatedly that if you take shortcuts—doing things the way you would rather than the way the brain does, unless you deeply understand what you're giving up—you'll end up at a dead end and regret it later. I can't emphasize this enough. Every time I see us using real-valued numbers, it's okay for now, but you need to be sure before moving on. It's not that we know we have to use SDRs everywhere; it's more about whether we know we can't use SDRs everywhere. If you don't know, you can't say otherwise. I know the brain uses SDRs, and I'm not sure if we can get rid of them. Until I'm sure, I don't want to make that change. If I know exactly what I'm giving up and can solve the same algorithmic problem using real-valued numbers without losing anything important, then we can do that. The history of neurally inspired machine learning often starts with an idea from neuroscience, but people skip the rest because they don't understand it, and it never leads to the right answer. Our goal is to go the opposite direction: start with neuroscience and ask if we can use real numbers, but we don't know yet.

I have a question. If you have multiple coordinating pieces or subsystems and a representation like SDR, does it change at each step? Is the encoding of the SDR different in every place in the network, or is there a system that coordinates it? No, there is no coordination whatsoever. Stop thinking about SDRs as if they're numbers—they're not. There's no mechanism to coordinate them. Everything has to be learned. Using real values breaks that rule a bit. For example, you might encode hue the same way everywhere in every layer—blue is the same number everywhere. With SDRs, blue might be encoded with one SDR in one learning module and another SDR in a different module. Unlike biology, we could take a quarter of a column that's trained and replicate it, like in a convolutional neural network, and they might start off with the same representations.

But in biology, that's not the case. One part of V1 doesn't know what another part is doing. Neurons are dying and new ones are appearing, so representations are changing all the time. It's a very different system.

My thought is that this might hide a problem: the coordination piece or how you deal with different representations in each place. For Monty, it wouldn't be a big issue because everything is just learned. You store features in the graphs and match them when they come in—is this the same feature I have stored in this graph? I don't think it would be an issue at that level, but it does make it easier to visualize and debug the system, to see what it's actually learning in each layer.

Remember, these aren't numbers or vectors that reside somewhere. These are neurons firing or not firing, and the same neurons don't exist elsewhere in the cortex or column. They're just different neurons. There's no table that represents SDRs; it's a very organic system.

Would it be useful, even though we're using real numbers, to offset them so that every learning module has its own offset, ensuring there's no global correlation? That would still be a misconception of how it works. Nobody looks at these things outside their local context. There's no possibility of confusion in that regard.

It's not that a range of numbers is for this or that; we have to get the system to work without a table of values. For what it's worth, the system would work exactly the same for Monty if we offset the color values for every learning module, because they never get communicated elsewhere after that. We don't vote on features; we vote on the objects detected. The voting needs to learn mappings, but on the feature input side, it wouldn't be an issue.

Basically, it's fine for now. It's good to be mindful of it, but for whatever we're using real-valued activations for, we might as well take the benefit: we can easily visualize them and know exactly what they correspond to. Still, it's something to be aware of.

I've shown here, maybe controversially, that we could consider passing in features that might correspond to texture patches.

This could be a case where you don't necessarily learn a detailed model of how, for example, cheetah skin is distributed on the animal. You just know it's distributed over it, unless you attend to and specifically learn where a particular spot is relative to a landmark on the animal.

SDRs would naturally be able to represent textures like this, which we definitely can't do with real-valued or with the current setup.

and then, I feel it's worth explicitly addressing the elephant in the room about real value vectors. I appreciate you. To your point, Jeff, there's not good evidence that neurons can do super high fidelity activations like in deep neural networks. However, using these kinds of representations introduces some advantages. The question is whether we only gain benefits by deviating from biology, or, as you mentioned earlier, does this introduce risks if we go down this path?

It's not really possible. First, you can't have negative and positive values. Neurons have very limited possibilities for real values; they're spiking, and as we've discussed many times, you often don't even have time for a second spike. So you can't really do a spike value. The challenge, from the neuroscience point of view, is that if we rely on compartmentalized neurons, like the HTM neuron, which detects coincidences of features, then these real value activation values make that premise untenable. You might think a change like this gives higher fidelity or capacity, but when you look at how it's used in the system, it doesn't work as we might need. You can't look at these things in isolation; you have to consider who's receiving the information and how it will be processed. The power of a neuron to capture many different patterns is really important, and this might prevent that. So you have to be careful.

I have one question, backtracking for a second. You're showing that pattern of plaid there. I get that what Jeff is saying is these encodings, call them SDRs or whatever, don't have to be uniform across any particular layer. They're learned independently in the columns. But at some level, there's a recognition that if one cortical column sees a piece of plaid and another sees a piece of plaid, there's some notion of continuity. I'm wondering how you solve that unless you have some sort of alignment of what the representation is, or whether two codes are equivalent at some point, so that we can detect discontinuities easily. Unless you have a way of saying, "I'm seeing the same continuous pattern as it slides across the various receptive fields," how do you solve that problem of representation unless there is some alignment or commonality that says, "this is the same as this," or "this is different than this"?

I feel that's where hierarchy is really important. It depends on who's receiving the information. If two different systems are receiving two totally different inputs, it doesn't really matter whether the SDRs are the exact same or not; they're not going to know there's some commonality between them. Generally, you can imagine that at some level, something is, it feels like that, Kevin, but it isn't. Vivian mentioned the key to this earlier: what columns vote on are object ideas. They can learn an object using completely different sensory inputs and experiences, but as long as they're modeling the same thing, they agree on that thing. If I'm an expert in plaids, I would see different plaids and recognize them, and that becomes the object. You're voting on the idea of a particular type of plaid if you attend to that plaid pattern, but you don't need to share representations.

Imagine I'm touching the coffee cup. The distribution of sensors on my fingers is different; there are skin sensors, and they're not equally distributed over your body. One part of my skin will use one set of sensors to learn the coffee cup, and another will use a different one—maybe one is sensitive to texture, another to temperature and heat conductivity. You're not aware of any of this. It doesn't matter. As long as they're all saying, "yeah, we're all touching the coffee cup," that's fine. They don't need to share their details. You don't even know most of these things; you don't know what features you're detecting.

At the next level, when you're trying to agree upon these object IDs, remember the voting mechanism is that each column has some guess of what kind of objects it's sensing, but it's not an object ID. What they've learned in the past is that when sensor 1 is detecting some part of an object and sensor 2 is detecting another part, it doesn't matter. The only thing that matters is that both are touching and sensing the same thing, and they can agree that, based on their evidence, it's X. Then they can learn that's an association. They don't have to agree about anything else—just that, at the moment, they're both sensing the same object, and they can learn to associate one SDR with another. There is no passing of the SDRs or agreement upon them. Each has a representation and learns to associate it with whatever representation the other has. There's no other knowledge beyond that—just, "here's a pattern you've sent to me, and I have my own pattern I'm sending to you, and let's agree this is the same thing."

That's the question: how do you agree it's the same thing? Because they happen at the same time. They're basically sensing the same object. They just learn, "okay, I have some SDR in layer two, three that represents the coffee cup, and whenever that SDR is active in my layer two, three, your layer two, three usually has this set of neurons active, which in that SDR corresponds in this column to the coffee cup ID." So it's just associating whatever SDR is active at the same time when one object is being sensed.

Okay, I perceive. Thank you.

I had that exact same question. I think I asked Vivian on day one. You don't pass the SDRs around. These concepts are very hard for people to initially get. I want to point that out, and we have to be constantly vigilant because almost anyone coming into this project will misunderstand these things. There are epiphenomena, like the ability to fill in when you have a defect in vision. That is a misconception that's been passed around for decades. We don't fill in, and people have written about this. What we do is not perceive anything, and if you attend to some spot, you can predict it will be there. The classic example is the blind spot in your eye. People say we fill it in—not true. The blind spot is no different than touching a coffee cup with a couple of fingertips. I'm not touching most of the coffee cup at all, only a few spots. I don't fill it in. We have a model of a cup, and if I move my finger someplace, I know what I will touch. Nobody's filled it in. Everyone just says, "I know what model we're talking about here," even though I'm only sensing a little part of it. I'm not trying to be critical. These are things that took a long time to figure out—some of these common notions are not correct. We don't fill things in. We are able to predict what's missing if we go to that spot.

I'm trying to look at what the mechanism potentially is. If all I'm doing is saying there's a wallpaper pattern and I'm predicting that if there's missing information, I will still perceive it as continuous. I don't perceive it as a continuous pattern. I have a model of this thing. The model is that it's continuing, but at any point in time, I'm only sensing some small parts of it. Nobody else tries to fill in the rest. The model says it is there. If I go anywhere and attend to any spot, I will see it if we validate our prediction or model by sensing different parts of it, but nobody's filling in. Nobody's representing what's not being sensed. The model says, "I've got the pattern on the wallpaper," or "I have the coffee cup." I can perceive the coffee cup, even though I'm only touching it with one finger. I've invoked the correct model, and I fill it in only if I attend to other parts. It seems like it's there because I can go there and it'll be there, but I actually didn't. There are no neurons representing where you weren't sensing until you move the sensor to that spot. Then it says it's there. Other people have written about this; it's not purely our theories. I suggest we keep going because we could spend a lot of time on this.

This is just the last one on the kind of representation, which is probably not something we will visit, but I think it's worth explicitly bringing up: some sort of temporal code in the neural representation. This is one example of a temporal code you could have. There are lots of ways that time could come in. This shows neuron ID on this axis, with three presynaptic neurons and one postsynaptic neuron, and the same three presynaptic neurons and another postsynaptic neuron. In this instance, they fire with a particular temporal offset, and in another, they fire with a different temporal offset. Their signals, due to conduction delays, arrive at the different neurons depending on which sequence they spike in. There are many different temporal codes you could use. This was just a simple one to put in this figure.

The key point with the temporal code is that it allows you to encode an orthogonal dimension, which you could imagine might be relevant in encoding something like feature ID and its pose, or feature ID and its scale. The temporal code gives you flexibility to adjust that dimension of the code without distorting it. The feature ID representation idea has been around for a long time. My recollection is that this has always been derived from people trying to figure out how you could use neurons to form more interesting representations, not from people asking what the neurons are actually doing. There's a lot of literature saying, "If neurons did it this way, we get all this extra information," but in reality, they're just making it up. There are conduction delays, but the idea that you can individually adjust the very precise spike timing within milliseconds of a cell, and that is the encoding itself, is not something I have evidence for. That's part of what the neurons are representing. Everyone agrees that when a neuron fires matters—they have to fire together to do anything. But this is much more specific and requires a lot of coordination between these neurons to know when. This is in the category of a machine learning person or engineer saying, "Hey, this would be really efficient, let's do this." That doesn't mean it's biological at all.

You can tell my biases here. I'm trying to capture, with the temporal code, spikes relative to a phase, which I think is something you are more amenable to. We know that exists. From biology, we know that type of temporal code actually exists. If we were going to show an example, I'd rather show an example of that. Because now we know that exists, we think maybe that's something useful. Let's understand it. There's an empirical observation, but people don't really understand the information aspect of it as well. Here's an information aspect that doesn't correspond to biology.

I'll update this. Whether it's this kind or the phase-based one, I think it's still similar in that it's enabling another dimension of expressiveness. I appreciate there are still lots of differences between them.

As you can see from the amount of green here, the current favorite is SDRs, and that's what we're largely going to focus on going forward, at least according to the current plan. This is a living document, but we've discussed keeping, at least in the short term, some of these real value numerical features. Tristan suggested a parallel version of the system that maintains these for people who are first getting familiar with Monty and learning how to use it, but that would be a difficult fork to maintain.

For many engineers or deep learning people, features represented as vectors are intuitive, so they might find it counterintuitive to use SDRs. They may immediately jump to point neurons, high value, high fidelity, positive and negative values, none of which is correct. They'll take that leap and then go off track with it.

Have we reached the end of what we can do with real value features and numerical representations? I would say we're reaching a stretching point. With the work Rami did on encoding object similarity, we could have passed information about evidence matrices and adjacency matrices of evidence values as numerical features, but at that point, it was really starting to stretch what that could reasonably do in an elegant way. As we get into the hierarchy of the system, that's where we expect to see some of the benefits that SDRs bring, so it was a natural point to start exploring that.

Does using SDRs upset the way information is stored on a graph in Monty, or is it a natural transition? It's quite a natural transition—each point in the graph can store whatever features it's given. Right now, we have points in the graph with both real value numerical features and SDR features. One point to note is that we're only using SDRs for the features, not for the locations on the graph; the locations are still real valued numbers. That may be a problem, and that's the next discussion.

If we encounter problems we can't solve, I wouldn't move away from sparse representation. I'd focus on them because we know the brain has to solve these problems. We should focus on how the brain solves them before deciding it's too hard to use SDRs and switching to something the brain doesn't do. I guarantee that would be a mistake. First, understand how the brain does it. When we talk about locations, it's important to understand how the brain represents locations, and then we can decide if we have to do it that way, rather than saying we have a problem with representations and solving it with time codes. That would lead the project down the wrong path. You might later decide to use another coding mechanism for software or hardware reasons, but not because you're stuck.

With this, it's largely consistent with what Numenta has always had in mind, so maybe this is the less controversial one.

Over the next six months, as we focus on hierarchy and compositional objects, we'll keep working with SDRs where useful and have some real valued features. I'm more concerned with the language around using real valued vectors.

The more controversial topic is reference frame representations. Both Vivian and I feel there are many advantages to the explicit coordinate system, beyond simplicity and visualization. It's harder to see the advantages of the grid cell approach, so we may move more slowly toward the more biological ones or even stay with the explicit system.

In the document, I go through the pros and cons. We're explicitly representing points in the graph in some kind of XYZ coordinate system, and each point has information associated with it. This is easy to visualize, and these are generally unconstrained—we can keep adding points, and they can represent arbitrarily large or small objects. There's no limit to how big an object a network can learn.

Vivian implemented a constrained version where you have a voxel grid of 3D space. That space has two limitations: a maximum dimension—so a given learning module will learn objects no bigger than, say, 10 by 10 centimeters—and a maximum number of voxels, which determines the resolution of the space being modeled.

There is also sparsity: only K voxels can be active for that object, so not every voxel is used.

Essentially, there are three constraints. There is an algorithm for taking similar observations and mapping them into a 3D graph, but the graph is subject to these constraints. The observations are received in a similar way, and the output is still a 3D graph, but with the described limitations.

One of the things we believe neurons are doing—though we don't have proof—is forming unique representations of location. You can take an SDR, which has an astronomical representational space, and represent the location and object uniquely, not just to that location on the object, but to all objects in the universe. These kinds of representations have that ability.

In the sense that they're stored completely independent—not stored independent, but if I use an XYZ coordinate system, I can say you're at location 1,1,1, but the location itself doesn't specify the object. That is specified separately. So, it's location 1,1,1 on object X versus location 1,1,1 on another object. Theoretically, you could offset the x coordinate by a thousand for one object and the y by a thousand for another, and they would be very far apart in three-dimensional space, but object recognition would still work. It doesn't matter where in the space the object is; what matters is that the relative locations within that object are consistent. We're still working in Euclidean space, so the locations aren't represented as SDRs, but you could put them in arbitrary locations, even thousands of kilometers apart. That's good. I still see potential problems, but that's better than otherwise.

The potential problems are things I don't have enough data about in neuroscience. I suspect I don't fully understand them yet, but I know there are things neurons might be doing that we don't understand, and it could make a difference. I can talk about what those things are, but I don't have evidence either way. This relates to the overlapping of SDRs and how the brain resolves unions. Unions are a perfect example: you can simultaneously represent multiple hypotheses with the same set of neurons. That's part of how the brain resolves things. Here, you'd have to make lists of things, but you can't have a single representation of location that forms a union. You could say it could be one of these 20 objects and have a list, as opposed to that being inherent in the representation itself.

That's an interesting point. In general, when we talk about a union of locations, I feel like that's a form of interference. There are situations where we want a union, but when we're representing multiple hypotheses simultaneously, cramming all of that into a fixed neural capacity feels more like a source of noise.

I don't see it that way. I see it as beautiful—an elegant solution to the problem. It's noise only if you have too high a density of representation. With sparse representations, the system processes these things in parallel with no interference. We've shown that mathematically. It can read them out, but with things like path integration, there's a separate question about how that comes about. We know how a single neuron does that, but creating grid cells or representations is a little tricky.

Maybe what you're saying is that with this union, we don't fully understand the neural mechanism—exactly how grid cells work—but we also don't understand how they work without a union. I don't see it as noise. We do know that if you have an SDR and you're passing it to another set of cells, unions work well; there's no problem with them.

One of the things that comes up with grid cells is that we're not entirely clear how they work for certain things, like updating multiple hypotheses in parallel, especially in terms of path integration. Another topic we've discussed, but for which there's not good neuroscience, is how 3D space is represented by grid cells. Even if we wanted to use grid cells, it's not entirely obvious how we would do that. I've never recommended using grid cells per se—just as a placeholder for sparse representations of location that have the property of path integration and potential unions. It's an area where we don't really understand how the neuroscience works. We have some attributes that are amazing—what a clever way the brain came up with to solve not having numerical values. The brain doesn't have numerical values; it has to solve everything without them. This is a clever way of doing it. That's how I felt, and I think that's how Subutai felt when we first came across this—biology, or evolution, is smarter than we are. But we don't really understand all the details, so this is an interesting challenge.

Since we don't know how the neuroscience works in this case, let's keep using real value coordinate systems. I don't think that's better than the neuroscience, but I would do that with the idea that it's probably not good enough. We'll keep going until it doesn't work, and maybe we'll learn more about the neuroscience along the way. What I bristle at is saying the neurons don't do a very good job at this and that we can do a better job in some other way. No, we just don't understand the neurons yet, and I bet the way they're doing it is better than anything we're going to come up with. We just don't know it yet, so we have to do something else in the meantime. That's my attitude about it.

There is a coordination issue here: do grid cells solve it? How we represent location needs to be communicated between modules. It's no different than what we talked about before—there's no communication of locations, and it doesn't need to be.

If it helps, every learning module or column is dealing with object-centric coordinates, but if they pass information between them, that's using a shared coordinate system, like body-centric or egocentric. For voting to work, they need to know their relative location and orientation to each other, and the sensors to each other. I don't think we've ever worked out the details of that. That's something we still have to figure out, but the object location spaces themselves are independent. Still, to interpret the voting signals correctly, they need to know not just "I think I'm here on the object," but also, "If you're here, then I should be over here," because the sensors are not at the same location.

Early on, I was fooled by this. I thought these values had to be communicated elsewhere—location A here had to be turned into location B over there. But then I realized that when information comes into a column, we're not passing locations; we're passing movement vectors, and that's relative to a common reference frame, so that works. As Vivian and Neil both said, the brain does use other reference frames. If I want to move my hand, it would go through a common body-centered reference frame and then back up again. The original question is, do we need to communicate these things as SDRs? The answer is no, they're not. That doesn't happen.

We've discussed recently that it's not necessarily grid cells—some form of path-integrating neurons is really the key, but they can represent unique locations. Just to mention it, although I don't think it's a particularly good approach, is SDRs for location, where we're basically taking X, Y, Z coordinates and using some hashing algorithm to convert that into an SDR. Is this what we did? You're talking about what we did in anomaly detection and GPS, or—I think it was used then, but it was also used in Avi's work, which was actually the first attempt to use HTM for Monty.

It's a bit of a funky one, but in that previous slide where you're updating things, you asked why you marked hashing algorithms in red—this one, on the right-hand side.

So, which bit is red? Do you mean the "no path integration," or that the hashing algorithm is computationally intense? That's a fair point. Maybe this is wrong, but what I remember from Avi's project is that an extremely time-consuming process was just converting the locations into these SDRs.

I've been reading about locality-sensitive hashing, and it strikes me that there are aspects that line up with what SDRs do—the notion that if two things hash, they're basically checking for similarity, and two things hashing to a similar location takes you into a localized area of "memory," offering up candidates for further processing. In other words, it's a way of narrowing things down based on some hashing algorithm. A normal hashing algorithm is just a compression technique; there's nothing particularly clever about it. But if you replace that kind of random hashing algorithm with something learned, rather than just a random smashing of bits together, maybe you're getting closer to something more neurological.

Are you questioning whether it's computationally intensive, Kevin, or are you questioning whether it's intensive while also pushing the idea that location could be an emergent property of a process like this? So, you're really questioning the "redness" of that comment. I might just put a question mark there. I added a caret.

It sounds to me, Kevin, like you're saying, "Hey, I can think of a clever way of doing this, and it would have information-theoretic advantages, but perhaps it has nothing to do with how neurons work—or it could." Everything could; I can make neurons do anything, but real neurons are pretty restricted. Neurons might be using SDRs for locations or might even likely be using SDRs for locations. I think this is something we should consider—if there's a better solution than what Avi initially implemented.

This was a long time ago, but I remember the same thing you wrote: Neil said it was very slow and didn't path integrate. Maybe there's a better solution. I look at it with a much deeper dictionary of neurological principles, and I use that as a constraint. Sometimes I'll say, "That's not really possible," or "We have no evidence that could work," or something like that. I don't have any opinion in this case, Kevin.

I don't know enough about the hashing algorithms you're talking about to know whether neurons could do this. I'm just warning everyone: we should take approaches like that carefully because we don't know what we're giving up, as I said earlier. But I don't mind doing it in the short term.

Let me turn it around. What I'm trying to do is say that some of the power of what an SDR does could be viewed through the lens of a hashing algorithm, and then you have a whole bunch of math that lets you analyze what's going on. That's a useful activity to help us understand possibilities for how SDRs could work. That's what Rami did during his internship here. He worked on things that I don't think will actually do it, but it's helpful to think about possibilities and new ways of approaching them.

It is interesting to think about. Maybe, in that sense, there are a lot of advantages to explicit code, but let's say we feel that specifically representing each location as an SDR has some inherent value—maybe it's something about unions, as you were saying.

Ultimately, we can do path integration and similar operations in this location space, but as an intermediary step, it may be helpful to convert the XYZ coordinate into an SDR before further computations are done. If that's a learned mapping, rather than something random, it could be quite fast. The main point is to move away from the idea that either 2D or 3D is an explicit coordinate system, and instead focus on the notion that "this is close to this," which is what LSH does. However, I think there's more to it than just similarity; there may be similarity in multiple different spaces. Often, with LSH, multiple hashes are used to achieve greater fidelity in determining how close things are to each other. That's one way to bring together multiple projections to achieve higher-dimensional properties.

I'm trying to relate this back to the neuron, where, for example, 40 synapses in a group, 20 of which are firing, suggests a deep connection to emergent properties—something being similar or coincident with something else. I'm using this as a lens to think about the problem, rather than going off in another direction. The summary right now is that we're using real values and will continue to use them for the foreseeable future. We don't have any issues with the real value representation, so unless we see a real benefit to using a different representation, we'll stay on this path. Until we encounter a problem or independently discover the benefits, we'll continue as we are.

However, there's also the consideration of sunk cost. If we know we're going to abandon something, we should be careful about how much time we spend on it. We shouldn't abandon it just because we think we might; we should only do so when we know there's a problem and have a solution. If we run into a real problem using real value numbers, that can help us understand how the brain solves it, because we can look to the literature for answers. Niels, I think your recommendation was to stick with this approach for now, and I agree. We shouldn't abandon anything until we know where to go and have a problem to solve; otherwise, we could waste a lot of time.

One thing I did want to mention is that the overview recommends moving toward constrained versions of the graphs, such as not allowing a learning module to learn a huge object at super high detail. That seems like a biological constraint we want to have, and it will help with compositional objects and representations. The lower-level learning module shouldn't be able to learn the large compositional scene at high detail on its own; it should learn parts of the object, and the higher-level module should learn the compositional object. These kinds of graphs will also help solve other issues, like separating an object from the background and learning generic objects.

That's a good point. I have that noted here.

This may not be directly applicable given our current knowledge, but it could become relevant. After our discussion, I'm more open to it being relevant in the future.

Regarding SDRs for location (non-grid), that's the kind of hashing that might be a hybrid solution. We could still use something like this, but ultimately represent the locations as SDRs. We would do path integration in this space, but represent the locations accordingly. Can we agree not to pursue these ideas until we need to? Yes. These are all waiting in the wings.

If we encounter an issue, especially as we start looking more at object behaviors and abstract spaces over the next 12 months or so, we may revisit reference frames and find new needs. The problem with a slide like this is that someone might say, "It's been 12 months, it's time to work on this," but really, we could work on it tomorrow or three years from now, depending on what we learn. It's not a roadmap for implementation; it's a "go until it's broken, then fix it" approach. So, "roadmap" isn't the right term here. That is the plan, but I can write that more explicitly.