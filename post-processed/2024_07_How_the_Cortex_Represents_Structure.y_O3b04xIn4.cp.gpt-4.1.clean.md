I often find that rephrasing a problem or being explicit about what you're trying to solve is very helpful. When I get stuck, I usually go back and ask, "What is the thing we're trying to solve here?" Maybe I need to look at it differently and step back to see the bigger problem. I don't have this written down to show you—it's on my ePad, but not something I can easily share. The way I'm approaching this, and what you're going to present, your HAP idea, should fit into this scheme. Presumably, we're dealing with how the cortex represents structure in the world. We have a model for that in Thousand Brains theory, but the model is incomplete. It doesn't explain everything. We have these unique locations and features that are both represented uniquely in context, which is great, but it doesn't address two major classes of problems.

First, it doesn't deal with the fact that we have a sliding scale of objects. For example, we have a generic cylinder that becomes a specific cylinder, and somehow we need to share knowledge about the generic object with the specific object. That was one of the problems we were dealing with. Some objects have no specific features, like an egg shape, an oval, or a circle. We have a solution for very specific objects with very specific features, but we don't have a solution for moving from less specific to more specific. That's one issue. We also call that class of objects—like the cylinder and the specific cylinder—maybe one's a class of the other.

The other thing we haven't been able to address is object behaviors, which is a significant gap. Maybe we won't solve the first problem until we figure out how object behaviors work. I think object behaviors have a similar range: there are generic behaviors and specific behaviors. I can take a generic behavior and apply it to a new object, like a procedure I do that I can apply to a new object, which might work the same way. Somehow, we're able to transfer knowledge from one object to another, similar to transferring knowledge of morphology from one object to another. It would be nice if those solutions were the same, but I don't know if they are. So, we have this general problem of representation, for which we only have a partial solution—sensorimotor representation—and we've identified two big gaps: classes of objects and object behaviors. I'll stop there for a moment to see if anyone has thoughts about that, as it's the high-level phrasing of the problem.

I don't know if I've missed anything.

With the classes of objects, I feel like we had two ways of talking about it before. One is the overall category of objects and then specific instances, versus having a morphological model and a feature model, where you can mix and match different features onto the same morphology model. Are those fundamentally different in your mind? One seems hierarchical, with the overall class and subcategories, and the other is more mix-and-match, where you can put, for example, the Menta logo model onto a baseball cap, a coffee mug, or a plate. It's not a strictly hierarchical relationship. But does it become a specific instance once you start adding unique features? I think that was our assumption all along.

You have these unique features, and then you say, "Oh, this is the specific coffee cup," but then we lose the generality. The uncertainty for me is that morphology can also be quite unique and can define a unique object. How I thought of it so far is that somewhere in the layer four, layer six connections, and maybe layer five as well, we're learning a feature model, a morphology model, and a behavior model. The combinations of those three can make a specific instance ID. Maybe in layer two and maybe in layer three, we only have the ID of the morphology or the behavior. The models are separate in the lower layers, and in the upper layer, the combinations of which feature and which models are detected define a specific instance. For example, you're saying the banana that works like a person. Let me review what you just said: three types of model—a feature model, a morphology model, and a behavior model. In a hierarchy, the lower column would only be activating one of those at a time, and then that becomes— is that what you said?

This is just speculative, but how I was thinking of it before was that there would be three model IDs: feature, morphology, and behavior. Maybe in layer three, and then in layer two, which might be the object category, and then whatever is the specific object instance ID would be a combination of those three models that define the specific object instance. If I were to guess, I'll take it back. I don't know. I was going to say, "Oh, behavior model—where's that?" It's confusing because at one point I would say that's going to be layer five, those layer 5A cells—the non-intrinsically bursting ones and the ones that connect horizontally—because that's where behavior goes. But that would be like a behavior of my body, like signing my name. If we're talking about the behavior of an object, that would be in the upper layers, like layer three. So I was confused by that.

I'm not sure how—it's an interesting idea that there are three models, but if there are three different models, are there three different representations of location?

That's a tricky one, right?

Could you get away with one representational location, and would it be unique? I think they have to solve the location and feature problem at the same time. If we assume that the locations are unique to each object, then there would have to be separate location representations, but it would have to be the same space. Movement moves you through those. We're probably not going to have two different path integration mechanisms, so the path integration mechanism has to work on all small models of space. These are good ideas, but I'm not sure they work out yet. How do you really get this to work?

Maybe if you have that in your head, you can present it. I'm still struggling with which of these would be hierarchical between columns and which would be local to a particular column.

I'll throw in one more thing to think about. I was considering the different ways we've talked about representing space. We want a sort of generic space, and then somehow it becomes more specific. For example, I have to predict the generic shape of the cylinder, but once I know it has a particular ridge on it, I have to predict that at that location. I don't want to have to relearn the whole model. I thought of three different ways to form multiple representations with a single path integration. The goal is to have a single path integration and yet somehow form two or three different representations of space.

One way is to have two different sets of cells. I don't believe this is right, but I mentioned it recently: what if the double bouquet cells do the generic representation of space and the parallel cells do the specific representation? So, two different sets of cells, both doing path integration, since path integration should work on both. We could form a generic model with the double bouquet cells, but I don't think the anatomy supports that very well. It seems a little awkward. The double bouquet cell represents many columns, so their path integration would work somehow like that.

The second way we've talked about is having a single set of cells representing location, but under various contexts, it becomes more sparse. You don't go to a completely different set of cells; you just become more sparse. I could take one representation, say with 10% sparsity, and then form multiple unique representations from that, each with 2% sparsity, as subsets of the larger one. You're taking a broader thing and narrowing it down by dropping out bits, using the same set of cells at different levels of sparsity.

The third idea, which we haven't really discussed much, is that you could have the same set of cells but use the phase of those cells to form unique representations. For example, you could have a class of objects, and then a unique one would be a subset of the cells put in phase. You're not changing which cells are active, but changing which are in phase. You could represent up to some number of objects that way at the same time, cycling through more generic or less generic representations. For many years, we had no theories about how phase would play into our models. Grid cells and place cells showed us that phase is important and plays a role in coding, so at least we have a model to start thinking about that.

Those are the three ideas I could think of to deal with this. This isn't countering what you said, Vivian, just adding one more thing to consider. Do you have two sets of cells, one set of cells for location at different sparsity levels, or one set of cells using phase to make things unique? I couldn't think of anything else.

The methods for doing this might work, so that's where I stopped.

One thought on that is, I remember we discussed similar things during By the Bay. One recent thought was that with two populations, a dense one and a more sparse one, it feels harder to do path integration over a large set of potential objects. Let's assume we want to path integrate over the top five most likely objects. Unless those are all the same one, like all cylinders, and that's the one being sparsed, it feels easier to imagine how that would work with phase, where they're separated in time. With two populations, you don't only do two hypotheses. You wouldn't have two hypotheses unless they have the same parent or base model, so they would all have to be cylindrical objects and could path integrate together, then be pruned down to which cylindrical object it is.

With phase, it feels more limiting compared to what we're currently doing in Monty, where we explore a wider hypothesis space. At the start, we don't really know what the morphology is, so we explore many options, which takes time. If you fix a mystery object and try to figure out what it is, your ideas about it evolve slowly. Maybe that's because, through phase, you're cycling through different hypotheses in serial, rather than exploring all of them in parallel.

It's serial, of course, but you can't...

Two, you have an update. You can't start eliminating hypotheses. So, what's the state before you start eliminating anything? I'm struggling with that distinction between massively parallel and serial; it seems unclear. I think you opened the discussion at By the Bay with the idea that we want to have multiple grid cell codes simultaneously active, as described in the 2018 papers, and those all path integrate independently. But it's not obvious how you would do that. I'm worried that issue still exists. I think it does. In fact, I'm surprised if I said something other than this. We've never figured out how that path integration could work with multiple hypotheses. It just didn't seem possible, or at least we didn't know the mechanism for how it could be done. I suppose that even using different phases is also somewhat limiting.

If you think about different phases, you usually have a very short period of time to cycle through them. For grid cells, there's the theta phase, and you can go through maybe eight different representations in half the theta phase. But it might work out. We don't know. The other real thing to think about is object behaviors.

What is going on in representation in object behaviors? It's a little bit like a cylinder, and now the cylinder has a logo on it. It's like saying, here's an object, and now I know it's in a different state, so I have to make different predictions.

It's the staple example: it's the same object. I don't confuse that. It's not a new object—open staple blue versus closed staple blue. I see them as the same.

Even though the features have changed, how do I represent that? I have to represent somewhere that this is still a stapler, but also that this is a state of the stapler that leads to different predictions. Therefore, I have to have some different representation of location if the location is going to predict something. The location might not have to change. You could imagine I have a location space, and that's projecting to some feature, like layer six to layer four. But then we could also add some context, another projection to layer four, which says this is the state of the object. The state of the object is like a different feature and can lead to a different prediction. The location space could be the same for stapler, open or closed, but you'd predict different features based on context and the behavioral state of the object. You would still predict the same features, just at different locations, right? But I bet there are lots of examples where that isn't the case. If I look at my cell phone screen, I don't see the state of the cell phone, but I hold different features on the screen. Sometimes we see features moving or features in different locations, but sometimes the features change. If you open up the stapler, now you'll see components you couldn't see before, or maybe they change. The cell phone is a better example.

The thing that always worries me when thinking about how we do the stapler and similar objects is coming up with a way that we don't have to store a million points or a million different models to have a continuous or near-continuous representation. The stapler is very simple—almost like a 1D behavior. That's where separating behaviors from morphology models would come in, because we could learn this hinge behavior once, in as much detail as we want, and then apply it to any object that has a hinge, like the stapler or a door. Then we need to wait. I've talked about this before, but if you remember, there's a trick about neurons in our synapse model. If you're learning a trajectory of something moving through space—this applies even to your finger moving over an object—if you divide that into 20 different points along the trajectory or on the object, each time you have to store a set of synapses to represent that point. It's just too much memory. What I was able to show is that if you take an SDR—imagine it's your finger moving across some object from point A to point B—you don't have to store individual points along the way. You can store incremental synapses along the way very efficiently. You might think this mechanism is prone to errors, but it's one of those SDR tricks where the errors are very small compared to the potential size of the space, so you will very rarely get any errors. There's a way of efficiently storing a whole sequence. Anytime you have a sequence, whether you're moving your finger along the object or the stapler is opening and closing, there's a very efficient way of storing that sequence with very few synapses, and you don't have to have a lot of learning points. We've talked about this in the past, and you don't have to visit every point either. You could pick 10 points between A and B, and the next time pick a different 10 points, and it will still work and do the right thing. So I think there's a solution to the efficiency problem when it comes to neurons, which is interesting in Monty if we're doing real number of things. I don't think we had a solution for that efficiency problem before; we had to store all these points along the way and then interpolate between them, if I recall.

We would have to interpolate, but we wouldn't need to store every possible point. I think neurons do this very efficiently. If you want, I could walk through that algorithm again sometime, but wouldn't the neurons also be interpolating in some sense? There's a natural overlap between all the points as you move along a sequence.

If I store 10 points between A and B, I didn't really store 10 points. In some sense, through the synapses, I would learn and store all the points between A and B, with no distinction between the 10 points I stopped at and the ones in between. It's a continuous addition of synapses as you move through space, so there aren't unique points. You would extend the synapses on a dendrite, and that dendrite would represent all points equally between the two points.

As you do this, imagine I'm doing it with my finger and touching an object—the feature will change too, and different columns become active. An individual neuron only has to learn during the time its minicolumn is active. I don't have to learn these numbers of synapses, but another neuron will have a different span. It's very efficient; it could just add one synapse at a time. I think it all works out. I did the math one time. I mention that because I don't think we have to worry about storing all these points of the stapler moving and closing. I think I have a solution for that. It's really a matter of what would be the representation of object behavior.

It's easier when it's a 1D sequence, but as the space grows in terms of where we could be, it becomes a lot harder. I think it works for that too; it doesn't require this to be one-dimensional. It just requires that you're following a trajectory, whether it's multidimensional or not.

I just shared a whiteboard because I was trying to think through what you were saying and wanted to make sure I understand it correctly. If we have the computational approach or whatever we do in Monty, we would just lay down a series of points. Can you see what I'm drawing? There's a lot of other stuff on my screen too, like "Navigate the canvas, collaborate with others." These are clues about what I'm supposed to do. How do I get rid of those?

The screen is cluttered with helpful hints.

I'm not sure. I think if I click anywhere, they all go away. Okay.

So, if we're observing a line, we would just be laying down a couple of points on that line. Let me change the color. If we observe a point here, for example, we would just interpolate. You're talking about the way Monty does it today? We would just interpolate between these points, or if we observe a point here, we would look at this point and maybe a little at this point, but eventually it would represent a space like this around it. We would explicitly store these points, and the way I understand it, with neurons, we just represent this space. We don't lay down explicit points; we can recognize or deal with any point in this space.

Then it seems like it's functionally equivalent. If we don't consider the fact that here we store explicit points and just look at the behavior of what it would return if we observe another point, I think you're right. In theory, unless you get into it and find some holes, the basic idea is you would end up with the same result. When you do it with SDRs and neurons, there might be other benefits we're not anticipating right now, but as you've described the problem, I think they would be equivalent. I think the neuron's way is simpler—there's no decision about how close a point is to a previous point or whether you need to store it. There's no discreteness to these points.

If I think of 15 synapses on a dendrite representing a point, I can just extend the number and add new synapses along the same dendritic branch as the SDR morphs between one location and another, and it will automatically represent the entire space. That extended branch on the dendrite, which may now have 30 or 40 synapses, is bigger than the integration zone, but it would represent all the points in between reliably. Technically, there could be errors—mix and match errors and false positives—but it's unlikely. Functionally, in the sense that they can make predictions about any point in this space, they would be equivalent, but in terms of what they store and how efficient it is to retrieve that information, the neural representation might be more efficient. There might be other benefits too. For example, what happens when you give it a novel object or one that's slightly similar, where the two would behave differently in situations where the points aren't exactly in the same line? SDRs have properties that are surprising, but on the surface, yes, they would produce the same result.

Even though I didn't like the way we did it before because it seemed complex, I didn't see any problem with it. As long as you don't mind using CPU power to do that, it could work. When we talk about object behaviors, we might have similar problems. Maybe we can separate ourselves from that issue. I was reacting to what Neil said earlier about not wanting to store all the points between the rotating arm of the stapler. I'm not worried about that.

There are ways of handling it.

I don't really have a good answer. Go ahead. Sorry, I have a question. When we talk about object behaviors, are we also considering the temporal hierarchy of these behaviors, or do we believe that exists?

How would you define temporal hierarchy? For example, when performing the stapling action, that would involve pressing and not pressing, and you could do the pressing on the stapler. Maybe for something else, but that's probably not a good example. No, that's a good example. I think it's exactly the same; it's just the inverse of inference. In inference, you have a common set of sensations or movements that are represented uniquely in context. Now, when we want to play back a sequence, we have a unique sequence, but in the end, you have to perform a series of common behaviors. If you think about language, I have to be able to understand the words in context, and then I have to be able to say and predict what the next word should be. That's going to be a unique representation of that word, but I have to turn it into me speaking that one sound. So you go from a set of generic to unique in context, back to generic again. In that sense, that's what you meant.

That's not necessarily a hierarchy; that's just out of context, in context. But there could be a hierarchy too, in the sense that we have a hierarchy of object features, like we wrote in the paper. I imagine behaviors would have a similar sort of hierarchy.

When you're trying to do some complex behavior, as you go along, things happen and you have to adjust, maybe try a different solution. It feels like there is a hierarchy—even when you do action policies, there's a hierarchy of these action policies. But my general rule is that even if a hierarchy is required, you can't do everything in a hierarchy. The individual columns still have to be able to do some of this, but maybe not. That's the general way I've thought about it.

So, the hierarchy of behaviors—we still have to consider that. I don't know if that makes it easier or harder to acknowledge. When we do positionality in space, we're always thinking that everything is stationary. It might be that we need to think of everything as behaviors, and stationary or space is just a subproblem of that. It could be; I'm not saying it is, but it could be. For example, last time we discussed that we have some objects, but maybe the fact that we have behaviors that can be applied to different objects is important.

Some objects don't have behaviors. The coffee cup doesn't really have any behaviors. I would say that's a subproblem; it could be a subset. That would be a nice solution—if all behavior is right up there, equal to object morphology or class of objects. In my notes, there are really two basic problems that may be the same, and that's what you're suggesting: object behaviors and object classes. If we really solve the object behavior problem, it will inform us about the object classes. That may be the case. So it would be ideal if you're right, Robbie, that these are really the same problem and we've just been thinking about them as separate problems.

Last time, in the brainstorming session, you presented a paper where there's a bar in front of a monkey or a rat—I don't remember—but it was a bar that was moving, and there were some neurons representing just this movement. When the bar was not moving, there was no representation. That makes sense. There are two places you see those cells: one is in the lower layers of the cortex, which makes sense because this is representing the animal's movement through space.

Then the upper ones in layer three—remember I presented that paper distinguishing between layer two and layer three? They were showing that layer three cells—no, they were saying the layer C cells are motion insensitive or have directional sensitivity. You have to be really careful, and I mentioned it at the time, because those are the layer three cells they were looking for. There are a lot of other layer three cells that don't behave that way at all. So you have to be really careful about that. But there clearly are layer three cells that respond to motion. Some of them had small receptive fields, and some were end-stopped, meaning that if the line got bigger, they started responding less. That would imply those cells are representing behaviors of the object—they're movement, local, individual field, not global movement. The layer six ones seem to be more global. So those are clearly different. The odd thing was that in that paper, layer three is classically viewed as the input to the next level in the cortex, the next region. If we take that at face value, what we're sending to the next region of the cortex is not purely an object ID, but a motion-related object ID. If those cells don't respond without motion, it would imply that nothing is being passed to the next region of the cortex if it wasn't moving, which seems difficult to accept.

I think it has learned the object behavior because the training data they're giving to the monkey is that the bar is always moving. It's trying to attend to the movement, but the animal is specifically fixated on a dot in the center of the screen. If the animal does not attend to that dot, it doesn't get any reward. Literally, the animal is being told, "Don't move your eyes. Just pay attention to this dot." In the periphery, there are things flying around—they may be aware of it, but they're not attending to it. These are out of context; those movements don't relate to any objects in the real world. It's a weird experiment. That's the best I can do.

Unfortunately, Vivian and I have a meeting in 15 minutes. I was just thinking, would it be all right if I quickly mentioned something related to the last meeting? Is that all right, Viv?

It was just that I thought the conversation about how you represent generic spaces was really interesting, like the example of the song. Then we were talking about what is L4.

If the song is represented as a kind of spatial reference frame, and you're moving through that as the frequency changes, that gives you key invariance. Then, what are the features coming in? It reminded me of what we looked at before, where we used displacement almost as features. We talked about how, in certain contexts—like recognizing a face—there might be specific placements that are more salient or common, and we key off those. It doesn't solve all the problems, but if a particular frequency displacement is more of a relative difference, that could maybe be stored in L4 and would reignite all the songs that have that relative displacement in their spatial reference frames. You would start path integrating through those, but wouldn't relative displacement—Vivian brought up this idea—mean that the input to layer six would be a movement? It would be a relative movement, so you would have the relative movement into L6, which is what you're path integrating. Then, the displacement could be a feature, because you don't know which kind of graphs you should even be moving through to begin with.

The reason I thought it might work particularly well for songs and certain reference frames is that music is lower dimensional; there are fewer dimensions we can move through, so there's a more narrow space over which you need to save displacement. Also, I think humans are only sensitive to a semitone change in pitch, so we don't have the same issue as with the continuous space of vision, where you might do a partial displacement and fall somewhere in between. Displacement in music is almost more discrete. There are other kinds of reference frames, so it's not just a music thing, but my question is: if my assumption is that the actual melody, which is a series of displacements, would logically be learned in the lower layers—five and six—because that's independent of the actual features coming in, then when you said you thought you'd learn that sequence of displacement in layer four, I was puzzled. I'm not disagreeing; that would still be there. This is more about the relative movements that go through the whole song, but maybe there are certain displacements we've actually memorized. Rather than relying on path integration, it's the actual frequency change we've memorized, and that's the feature—recovering the specific, exact memory of, say, a famous artist's first note in a song, the key they're in, even if you don't think you know it, but you've memorized it. That's the general rule.

There must be a memory of what the actual key was—where it was on the cortex when that occurred. Somehow, that has to be stored. My thinking is that somehow that has to be in layer four, because that's a layer four-specific thing.

What Neils is getting at, if I understand, is that in music we don't really have path integration. The same melody isn't the same forwards and backwards. We can't just play a song backwards in our head or skip five notes ahead without significant effort. Even with the alphabet, we have to sing it in our heads to say it backwards. It seems like a different process. It fits with the temporal memory algorithm: I'll learn the sequence, but I can't go backwards with the sequence unless I hear it backwards. I could learn to say the alphabet backwards, just like I learned to say it forward, but it doesn't come automatically. I think that's your point—you can't just do path integration and go backwards. The displacement brings the kind of pitch, or key, variance.

I'm not sure if it's inconsistent with what we've said so far. Imagine layer six is representing the displacement, and now you're not really doing path integration; you're just learning a sequence of those things. You say, "I'm going to learn this interval, and here it is uniquely." Path integration doesn't work with a melody because we never go backwards. If I learned it in reverse, it depends on what you mean by backwards. We go up and down in frequency, but it's an ordered thing. I'm not suggesting we don't have an L6 representation for the song. Maybe it helps if I go back to the problem: it's nice that we have a more morphological model of the song, but if a note comes in and we're key invariant, how do we know which of all the songs we know we should start hypothesis testing and path integrating over? You can't. You have to have at least two notes. Those two notes give a displacement, and that displacement is what you've learned in L4 and are detecting.

I never could figure out how L4 could learn displacement and even represent it, but last time we said maybe the inferior colliculus did that. Then Vivian suggested just calculate it externally, not in the cortex. Calculate externally, which is convenient—lazy, perhaps, but convenient.

Layer six is getting a displacement, but layer four would not be getting a displacement. Presumably, layer four has to get something that's known. For example, there are first notes of some songs that are so unique that you know what song it is. It's very rare, but there are examples where the specific first note, orchestration, or voicing is so unique that you immediately recognize the song. You don't have to get the interval. My point is that it seems like layer four would get the moment-to-moment details, but layer six, under this theory, would only get the displacement.

I think I didn't understand your point the first time, Neil. Sorry about that. I hope I get it now, because if I do, I really like the idea. Are you saying the displacement could actually go to layer four, and there we learn a displacement model? The displacements are essentially the features over time, so we wouldn't even need layer six. We would just learn, basically like the HTM (Hierarchical Temporal Memory) algorithm, a sequence of displacements.

Personally, I still like that we have layer six, because I think you can path integrate a sense into a song. I know "Bohemian Rhapsody" well enough that I can skip ahead to a certain part. I'm just thinking about how this relates to whether you actually have to go through all the notes in your head. It's long enough of a song that maybe that's why it feels like you can break it up into parts. Thinking about another abstract example, like a family tree, seeing the movement—like parent—is a displacement. That tells you you're on a family tree graph, and that's a movement you can associate with a part of that graph. But it's still useful to have the overall graph and to be able to move flexibly through it, rather than just following a one-dimensional sequence from beginning to end without deviation.

In some spaces, a displacement is going to be more salient than in, for example, vision, where we do a million displacements with our eyes. We saccade all over the place, except for some things like faces. There's probably not much of a pattern to it.

Whether or not we want to have layer six for this problem, I like the idea that in layer four there's also a displacement model. Instead of just having a model of relative locations in frequency space in layer six and specific features in layer four—like the person singing or the instrument playing—you can recognize it in a different instrument. I don't think we actually said why, but having this displacement model in layer six would solve that in some sense. I was also thinking, could we have the same idea for object models, like general morphology models? That is, a displacement model in layer six in addition to the feature models. The problem with three-dimensional general space is you can go between any two nodes via any direction, so there's a combinatorial number of displacements to deal with. It's a combinatorial problem, but if we just want quick detection—like we talked about before, having some displacement model somewhere for faster object recognition, in addition to the actual path-integrated model—maybe one way to think about this is that last week we talked about the idea that you could input a displacement into layer six for music. What I hear you saying is you could also input a displacement into layer four. In my mind, that would probably go to the mini-column representation as opposed to being a feature. You didn't say that, but that's what I'm thinking. Now the idea is we're just extending the idea: what if displacement were passed into both layer four mini-columns and layer six? They might serve different purposes. Is that a proper summary? I think this is an interesting idea. So, we just go one step further from layer six and now add layer four.

It would be interesting to look at the literature on audition. It's always been much poorer than vision, but by now there's probably a lot more research. Every time in the past—about 10 or 15 years ago and earlier—when I read about representations in auditory cortex, it was very confusing and the data didn't seem reliable. Maybe that's changed. We might be able to find something now that seems better, but I like that idea. As long as you're not discarding it from layer six, I think that's important. I also don't want to get rid of layer six, and I think it was a nice idea that there's morphology involved.

I had a similar observation before we leave. When we do path integration in physical space, if you end up back at the same point, it's the same place. But in music, that's not the case. I could traverse a series of intervals and come back to the same interval, but it won't be the same place in the song. It's because, once you know the key to the song, you can sing the right note even if it's a new key you've never heard or sung in. That's like path integrating through the frequency domain to figure out how to sing a particular note. In physical space, if I come back to the same location, I expect to see the same physical things and have the same representation. In music, if the song starts on a fifth and I come back to a fifth, or go up and down, I don't end up at the same spot again. You're moving through time and never return to the same point. You can do that in small sections of songs where you repeat them over and over, but otherwise, you never end up at the same spot.

Maybe since you have to leave and it's 11 o'clock, that's a good idea to end on. We can think about placements. After last week's meeting, when we were talking about the music stuff, I thought it could be interesting. One possibility is to use it as a first abstract space to try and concretely solve, or rather, not 3D, as a way of showing that what we're doing in Monty can generalize to other types of spaces. If we can figure it out. In audition, it's very critically tied to time, and grid cells represent 1D space very well. But unlike a melody, if you come around to the same point again, it's represented the same, and in a melody, you can never come back to the beginning unless it's a repeated little riff that goes over and over, which we can also learn.

A lot of things to think about. Maybe we'll have more thoughts. Wednesday we're not going to talk about this, so maybe the next time will be Monday.