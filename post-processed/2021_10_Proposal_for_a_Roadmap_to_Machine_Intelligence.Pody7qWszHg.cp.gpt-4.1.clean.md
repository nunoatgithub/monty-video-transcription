Welcome to the Thousand Brains Project YouTube channel. This is the first video in the core video series, and it's a special one—the inception of the Thousand Brains Project. We recorded this video at the end of 2021. In it, Jeff discusses the main principles and ideas that this project is based on. We've been following the vision he presents here ever since, so it's a key video to watch. There's one concept called the AI bus. If you've read some of our other documentation, you might have noticed that we're not using that term anymore. We now call it the cortical messaging protocol or CMP, but that's just a change in terminology. Besides that, we've stayed very close to this initial vision. There were a few small things that we noticed had problems we hadn't anticipated, so we made small adjustments, but overall, this is what the project is based on. Without further ado, focus all your 150,000 cortical columns on this video.

I'm going to present some surprising things here, so prepare yourself. This is a proposal to be reviewed by people to get input.

Let me dive into it. We're all familiar with this roadmap. Hopefully my sound is good for those online. Let me know, and I'm going to walk through the whole roadmap again. We've been working on these components for a while. We started work on sparsity maybe two and a half years ago. We've been working on active dendrites for about a year. I think the progress we've made has been very good. We've made a lot of progress in sparsity. We still have work to do as a large team. We're going to do some amazing things, getting everything working on hardware. It's not clear we're getting toward the end of our first phase, but with dendrites, we're not sure what to do next. We're thinking about that. Reference frames in the cortical columns are in the future, so we've always had it like: reference frames, planning, and then maybe a more complete cortical picture, more research. We're at a phase where we've got something to work on, but we want to start the next thing. What is it? What's next? That's been a hard question to answer. It's not obvious, and it's been a challenge to figure out our plan. To clarify, we talked about reference frames. We're going to add reference frames, but what exactly is a reference frame? Why are we adding it? What's the point? What's the goal? I want to clarify that and be more precise about what we mean, so I'm going to explore that idea first and then expand beyond it. If you have any urgent questions, you can ask us. I'm going to use this as a taxonomy of models—machine learning models or brain models. I'm using this to talk about referencing, the big picture of what reference ends up being. This is how I think about it. I'm not proposing this as the definitive answer. I view the world as having unstructured models and structured models. You can think of AI or machine learning. Unstructured models are ones that make no or very few assumptions about the underlying data or its causes. A classic artificial neural network has some input pattern and a label for that. Beyond that, you make very few assumptions about what it means or the underlying cause, and you're learning a function between inputs and outputs. Then there are structured models. Structured models assume something; they make a limiting assumption. They say, "We're going to assume our data looks a certain way, and that's how the model will work." An example is temporal order. You have inputs coming in a series, and the order matters. That's what the model will learn. An example would be HTM sequence memory, a system that learns temporal order. It's really good at that, but it doesn't work for anything else. If you give it data that's not temporally ordered, it won't work. It says, "I can learn this structure. I'm going to assume there's temporal order in the data." The brain can do that too—we can learn melodies and so on. In this case, you don't need labels. The system can learn without labeling; you can learn a melody without naming it, just by learning the order. I put transformers with a question mark because I think they're a class of this, but I'm not 100 percent certain. For transformers, the order of the words matters. You can train transformers without a lot of labeled data; you just use the text of the world, and they'll learn, at least for language networks.

The second type of structured model is spatial. This is the idea that you're trying to learn the structure in the world where things have spatial relationships to each other—their relative positions and locations. In this case, you have an input, but you also need to know how you're moving it through the world, so there's a second input. Now we can assume there's some spatial structure in the world. Examples include the brain—the Thousand Brains Theory reflects this. Maybe SLAM is an example. For those who don't know, SLAM is simultaneous location and mapping for robots.

When we talk about reference frames, I'll come back to it in a moment. There are a couple of interesting observations about these things. If you look at the pros and cons of unstructured models, they can learn practically anything because they make no assumptions about the data. Subutai always says they can learn anything. I don't know that, but he says that, so I assume he's right. The point is that since you're not making any assumptions, if you have enough training data, you can get it to work. The downside is it requires lots of training data, and much of it has to be labeled. Because you have no underlying assumptions about what the model or the world is supposed to look like, there's no inherent ability to generalize. There's no built-in mechanism for generalization because there are no built-in mechanisms.

There are some limitations to what you can do with them. The pros of structured models are very fast learning. Our temporal memory can learn in one pass. We feed a whole bunch of different melodies in one pass, or you can make it a little slower if you want. There's the ability to generalize between learned objects because there is structure to those objects, and you can compare the relative structure. If I'm learning the spatial relationships in things, I can see if something else has a similar spatial relationship—oh, those are similar. Even in melodies, I can say, these are certain typical patterns I see in Baroque music versus modern jazz, and I can infer things like that. You can also generalize behavior if you have a spatial relationship model. We'll talk more about that. When you have structured models, if you have a bunch of them, you have the ability for those models to collaborate with each other better. This is part of the Thousand Brains Theory: we have this idea of voting between models. One of the reasons you can do that is because we can have multiple models that all have the same internal structure, so you can make some assumptions about how they can communicate with each other.

The downside is that it only works if the world fits the assumed structure. If you try to get the system to learn something that doesn't fit that structure, then you're in trouble. One of the observations people have made about AI is that today's AI, mostly unstructured models, are good at things humans find hard to do and really bad at things humans find easy. One way to interpret that is to take something like how to play the game Go, which is hard to do, and you're good at it. It probably doesn't fit in this spatial relationship and temporal order type of model. Therefore, we shouldn't be surprised if you take a neural network and train it with a huge amount of data generated by a computer—then it's going to be really good at it, better than us, because it doesn't fit these kinds of models. On the other hand, if you ask a classic neural network to open a Coke can with a robotic arm, it's going to be really bad at it. It just doesn't know how to do that because there's not enough training data to teach the world how to do all these things.

That's the basic idea. Now, to do the spatial relationship, we're going to assume that the structured model is capturing spatial relationships in the world.

You need a reference frame. You need some way of representing the relationship between objects, or relations between an observer and an object, or a sensory organ and an object. This is why we have reference frames: to build structured models that capture spatial relationships. Not many people work in this area of structured models compared to unstructured ones, but there are people who do. The brain has both temporal order and spatial relationships. That's why we did HTM sequence memory, and in the broader concept of the Thousand Brains Theory, we try to learn the behaviors of objects through sequence memory and so on. We really want to capture both of these.

This is the way I think about models and why we need reference frames: we want to build models that capture the spatial relationships in the world and the temporal order of those things and how things behave, but we are not going to do as well on many problems that classic ANNs can do really well. That's not the point. This is how I personally internalize thinking about models in the world. I don't know if other machine learning people think about it this way or not. If there are any questions about it, let me know. Make sense? Okay. Now, we spent a lot of time figuring out how the brain does this. I'll talk a little bit about what we've learned about how the brain does this, but I won't go into too much detail.

As far as we know, the brain uses a combination of techniques to represent spatial structure. At least two of those are grid cells, which you can consider as a matrix. A grid cell represents a regular structure, like an XYZ coordinate system. The brain also uses vector cells, which are more like polar coordinates. It seems to use both of these methods for representing the spatial relationships of things in the world. Here's a cartoon drawing of three objects in the world—A, B, and C. You can think of these as three things in this room or as parts of a bicycle, and you're trying to learn the structure of a bicycle. My brain can learn the relative positions of these things to each other. You can do that with vector cells, and these little purple arrows are like displacements. You can say B is at a certain direction and distance from A relative to some global direction, and C is at a different displacement from A, and so on. Marcus and I—Marcus came up with this.

And so we can represent the structure of things by learning the position and orientation from each other. We think the brain builds models of where things are relative to each other. These models have to be learned. Here, the little circle on the bottom is an observer. You can think of that as a person in a room, your finger, or a patch of your retina—something making an observation. The way we learn these models, the way the brain does it, is by making a sequence of observations, represented here by P1 and P2, which are pose one and pose two. An observer looks at something and notes, "There's an A off in the distance at this angle, this distance," then, "There's a B over in this angle, this distance," and can calculate the displacement between A and B. We think neurons are doing this. We've now said there's some relationship between A and B that exists in the world. We do this over a series of observations as we go around the world, constantly. Every time you look at something, you're determining where it is relative to you, and the brain can calculate where it is relative to the last thing you looked at or sensed.

The model should be independent of the location of the sensor. If I move the observer, the sensor moves to a different location relative to these things and makes two different observations, P3 and P4—meaning pose LA is off in a different direction, distance, and orientation—B, we should calculate the same displacement. It should calculate the same displacement between A and B, independent of where the observer is. We have an observation and learning method that's very dependent on where you are, but in the end, you build a model that's independent of your position. Mostly, our models are not completely independent of the observer's position. For example, if you take a face and turn it upside down, you still recognize it's a face, but you get worse at recognizing the emotion. We're not completely invariant to location and orientation, but largely we are. If you walk into a room, look around, and note where things are, the next time you walk in from a different direction, you'd still recognize them and be able to make predictions.

Why grid cells? So far, this little triangle picture doesn't require any grid cells. We think they're required for at least one thing: path integration. As the sensor moves through the world, it has a location relative to A and B, and we have to figure out its new location when it moves to make predictions. Grid cells are required to do this path integration of the sensor or observer. Lately, I've been talking about how grid cells may also be involved in coding the shape of space or the shape of an object. That's a new idea we've just started thinking about, and it's complex—another complication. One thing we can say for certain is we understand a lot of this, but not completely. There is nothing here we understand completely; there are many mysteries, and we keep digging deeper. It's pretty miraculous what we've figured out so far, and I say "we"—the royal we, not just us here, but neuroscientists and everyone else. When I got into this field 30 or 35 years ago, none of this was understood. Absolutely none of it. Now we know a lot, and we've made great contributions ourselves.

It's really exciting, but we have to be honest. We don't understand this completely. I don't know how to build this yet. We don't know how to do that. Now, I'm going to take that last picture and present it as a conceptual block diagram. It's pretty much the same thing, just presented differently. This is not a neuroscience diagram; it's a concept diagram. You could roughly think of this as a cortical column, but I'm not stating that exactly at the moment. It is a model, so I'm showing another modeling thing. Now you can look at this picture and see I've broken it into two parts. One part is basically moving the sensor—inferring the objects and their pose relative to you. It repeats that: your finger moving around, your eyes moving, your body moving, whatever it is. It's going to output some sort of object representation and a pose. I wrote here "pose RTS relative to the sensor." This is the pose of the thing I'm touching relative to my finger. That's what I need to build this model. Then we pass it into the second block, which is building that graph—figuring out what to do with it, even though we don't understand it completely. That's what builds that graph, that structure. That's the new model. I recognize something down here, and now I'm going to say we're building a new model by putting things relative to each other. That's pretty much what I just described. This is pretty much what a cortical column does. Lately, I've started to think that to do this in cortical columns, you actually need two cortical columns: a "what" column and a "where" column. It's not really a single cortical column; it's more complex than that. But these are the basic operations.

Now we've proposed—and we know in the brain—there are many models. That's a fact. There's no question about that. You can think of one per cortical column, all working on basically the same process. Not exactly the same, but pretty close. You can think of these as three different fingers: a finger column, an ear column, a touch column, a vision column—however you want. There are three different modeling systems. They may or may not have similar modalities to their sensors. They may be moving together or not, like my fingers can move independently, but my eyes—the columns of my eyes—all move at the same time. It doesn't really matter. So, we have multiple models here. A big part of the Thousand Brains Theory is that we're doing this sensorimotor learning in the brain just like this. But then we also had this voting layer.

Before I go into this, I've now come to believe that the mechanism we described as voting in our columns paper is much more than we thought. It's much more powerful and a bigger part of the picture. It's not just voting on object ID. We understood some of this a long time ago, but now more of it is becoming clear to me. I've shown it here with this green arrow, and you can think of this green arrow as a bus—a communications layer going between parts of the cortex, in this case between different models. I've come to affectionately call this the AI bus now, because I think it's actually turned out to be maybe the core or key idea to unravel all this. We had a brainstorming meeting a couple weeks ago, talking about what to do next, and I think it was maybe Ben or someone else who said, "Hey, I was really intrigued by the voting thing. I think we should focus on the voting thing." So, yeah, this is the voting thing, but it's more than that. I'm going to talk about it.

It's going to pass around a bunch of things, more than we talked about in the columns paper. One thing—I'm just going to loosely say—is object ID. What are these things agreeing on that they're observing? I don't want to get into the details here. I could have put that green object up at the top; it doesn't really matter. We're passing on object ID, and these things have to learn about it. It's also passing around pose. We didn't mind about that in the columns paper, but Markus, I think, was the first to point this out: when you touch something with your fingers, it's not that each finger is just voting on what they're sensing—they're also communicating their relative position to each other. That's part of the voting. There are multiple sensors on multiple fingers, and so on—where are they relative to each other? We also have to do that between vision, hearing, touch, and so on. We think the way this happens is that there's a different type of pose that's calculated. I wrote it over here as RTB—relative to a shared point, using "relative to the body."

Think about it this way: when you reach out and pick up something like this can, you have a sense of where it is. Your brain has to calculate where it is relative to each finger to make predictions. But your personal perception is not the fingers—it's relative to your body, someplace relative to your body. If I look at something, I do the same thing. When I want to reach for something, I have to have some sort of common language between what I'm seeing and where my hand is. I say, "Oh, my hand needs to be in that location." We believe what's being passed around is pose that's relative to the body or a shared point reference frame. That's not what we started with here; we had a pose relative to the sensor. I put a little green circle there to indicate a conversion has to happen. We have to have two different types of poses: one used locally for the column or model, and another that can be communicated. By communicating elsewhere, the different models can all agree and calculate where they are. If I know where my left finger is relative to the can, and I calculate on the green arrow where the can is relative to my body, then the finger can say where it is relative to the can. This is like a messaging communication going on between them.

This may seem really complicated, but we're pretty certain this is going on. This is the way it works, roughly—the details we don't understand. Now, what can you do with this? One of the things you can do is what we wrote about in the columns paper. You can have three different columns—or many columns—all modeling the coffee cup. Each one is getting a slightly different sensory input, each with a different location relative to that. We talked about it as three fingers, but it could have been a finger, an eye, and an ear. It doesn't really matter. They're all getting something, and now you can do this voting, and they can all come to agreement very quickly. What you gain is inference with fewer movements. That's what you gain from this—flash inference. You can look at something and immediately know what it is, as opposed to looking through a straw or moving your finger around and grabbing it by hand. We've already written about that, and we showed that works really well. We didn't do that with the pose; we only did that in that paper with the object, but we didn't do it with the pose.

There are other things you can do here. Here's an example that relates to something we've discussed recently and for a long time: not all columns can learn everything in the world, so how do we transfer knowledge between different parts of the cortex? One way is to have, for example, only the left model know what a coffee cup is. It's learned what a coffee cup is, but now I'm going to sense the coffee cup with a different model, meaning a different part of my sensory system. That could be learning what the coffee cup feels like with my left finger, and now sensing the object with my right finger, or maybe I learned what it feels like and now I'm going to try to look at it. 

What can happen is, because we go through this common information, as long as all the different modalities or models can project the same sort of sub-object—even something as simple as an edge at some location to the body—then it can be passed around to someone who can recognize it. For example, you could stick your hand in a black box and learn the shape of five different new objects by moving one finger. Then you could reach in with your left hand and recognize which of those objects you can infer from them. Or you could see those objects sitting on a table without touching them, but still be able to identify them visually. You might never look at it, just touch it, but that has to occur. This bus gives you the ability to route information around.

Another thing you could do is learn with one finger and train a bunch of models at the same time, because you're passing around the same substrate that needs to be passed into each of these models to learn. There's good biological evidence that this happens locally. There are papers showing that when you get sensory input on a very small part of your sensory system, whether visual or touch, it spreads to neighboring columns in a way that suggests this is happening. When I touch something with my left finger, it may actually be training other parts of my sensory apparatus near my hand on that same model. It wouldn't train the whole brain that way, but it could. The idea is you could get input from one and build multiple models of the same thing for different sensory modalities or models at the same time. There's a lot of flexibility that comes with this.

One thing that's become apparent to me is that this bus, if you will—the green thing—has a lot more power. For example, we have what we call episodic memory, which is in the hippocampus. It's not part of the cortex, but it communicates with the cortex and is located right next to it. The hippocampus is known for very fast memory and recall. This is how you can answer questions like, "What did you do earlier? What did you just eat? What did you have for breakfast? Where did you park your car?" Someone has to record that, and that's done in the hippocampus. It's a very fast memory, quick to recall, but not very long-lived; you generally forget it pretty quickly.

If you think about what you remember from your episodic memory, it's very specific poses: I was here, the can was there, this was over here, and I saw Christy across the way. I'm not remembering these models; I'm remembering the actual data that passed around these poses—where things were relative to my body and the sequence in which they occurred. If you put a block like this—call it the hippocampus—on this bus, you suddenly have the ability to play back what happened and ask questions: What was going on in this network a little while ago? How did we get here? What was the process? What things did you do earlier? It's a very interesting idea, but it's the same communications protocol. It's not different; it's the same information these models need, and episodic memory needs. It just has a different function, but you can put it on the bus.

You can also do the same with language. In the brain, there are small regions in the cortex responsible for language. In recent research, I pointed out in a paper that they're very small—I didn't know how small, but they're quite small and really important. If they're damaged, they're quite small. I put a question mark there because they look like other cortical columns, but maybe they're slightly different. I wrote about this in the book, in "A Thousand Brains"—maybe they're a little different, but they seem very similar. So I just put a question mark there; I don't have to deal with exactly how it happens. 

There are some things we know about language that are interesting. When we talk about things in language, much of what we talk about is in the same sort of language as the green bar. If I wanted to describe what my bicycle looked like to someone, I would say, "I've got these two wheels attached to a frame, one in the front, one in the back, a seat on top, and a chain that goes around." I'm painting a picture very much in the language of the green stuff here. You could be in his brain, imagining this thing at some point relative to his body, all looking at the front. If I say, "Imagine if I turn aside," you imagine it. In personal language, we have multiple ways of dealing with it. We can input and output language from hearing, touch, and sight. We have written language, braille, typing, and spoken language. Clearly, there's a need to get input into and output from the language model in different modalities, and the bus provides that mechanism. It tells us there has to be a routing mechanism. Instead of speaking this talk, I could type it up or do it in sign language, yet it's the same language.So I put a little question mark there because the language model should be very cortical-like; it should be almost like every other model, but we don't really know. The point is that this bus structure gives you the opportunity to add a language capability to this. Imagine we had a machine learning system that worked like this and had the ability to do episodic memory and play it back. You could add a language model to it that puts what's happening into words: at this point in time it did this, at this point in time it did that, and you could speak it, write it, or output it somehow. I don't really put a question mark there, but it's an interesting idea that seems to be built on the same substrates in some sense. In the cortex, these are all connected together with long-range connections. There might be multiple green buses in the brain; I can't guarantee there's only one. There could be different parts of the cortex that are connected together and other parts that are connected together, but the general idea looks like this.

These examples suggest a need for some type of routing or attentional mechanism for the bus. We have the ability to take language and direct it to different places. For example, I can say, "Pick up the coffee cup with my left hand," or "Pick it up with my elbows." I have to be able to route information to different parts of the body to achieve these results, so it has that component as well.

What I thought about is that this is what the architecture of the brain looks like at a higher level, and this is what we ultimately want to build. If I ask myself what machine learning and robotics will look like in the future, that's really the trick of this system: to try to imagine what the future will look like. When we build these things, it seems like the biggest industry in the world, but it's very hard to answer that question. It's very hard for pioneers of computers to understand what computers are going to look like. It's difficult to do this, but I think this is a good capture of it. Not many people are thinking along these lines—maybe a few, but not many. This is not mainstream research, yet I think this is the future in some sense. This is really what it's about.

The next question is, what do RTS and RTB stand for? Relative to sensor and relative to body. There are different poses. I didn't have that in there yesterday, and when I ran this by Subutai and Marcus, one of them said that's confusing, so I put those letters there to differentiate them. It's still confusing, but now you know they're not the same.

What's confusing to me is that the sensor is your body. The sensor is like a finger, but your finger is on your body. It's moving, so if I say, "Where is the can relative to my finger?" that's different from "Where is the can relative to my eye?" or "Where is the can relative to my ear or toe?" But we can agree that, in a common language, there's some centroid point, a relative position to a single point, and then I can calculate where individual positions are. What is that common language? That's the part I'm missing.

Regarding the green line, by the way, relative to body is just one example. Earlier in this slide, it was mentioned that the reference point for the hand might be relative to the palm or something else. If you're watching a video screen, it might be something else. However, from an episodic memory point of view and from a language episodic memory point of view, there seems to be a body-centric representation. If you ask me, "Where did you put this coffee can the last time you put it down on the table?" I can say I know where it is relative to my body. Can I tell you which hand I was using? No. Can I tell you where my finger was on the can? No, but I can tell you where the can is relative to my body. I have this sense that it's over there, this thing is over here in my mouse, over here. So yes, it is relative to a central, common point. There's a lot of evidence the brain does this. There are parts of the brain that seem to perform these kinds of reference frame transforms, so this is not all made up—there's a lot of biological evidence for it.

Going back to the question of how to proceed, we don't fully understand any of the components. There's not a single thing on this picture that I completely understand, and yet we want to go forward. How do you do that? I have a proposal for how to go forward. It's classic engineering—a classic large engineering problem solution. It's not a science solution, but an engineering solution. The solution is to define common interfaces and then divide the problem.

I say we start by defining what this bus does. We don't have to define it in neural terms; just list what it needs to do. Then, specify how someone would interface with it—how to input and retrieve information. The bus is smart; it needs routing, conflict resolution, and other functions. We try to define it so the bus itself has its own behavior, but to interface with it, you must provide information in a specific form, and the bus can accept it. You also have to accept information out of the bus. These arrows are all bidirectional. For example, in the model on the left, you could provide a pose, and it should tell you what you'll see of the object. You could ask it to find an object, and it would tell you where it will be relative to the finger. There's a protocol going back and forth. Someone can say, "I have just observed something at this position, put it on the bus," and someone else can say, "Now you need to move to this pose," so the system figures out how to move there. You start by defining and implementing this communications layer.

I'm not saying this will be easy. It will be easy to get started and easy to make mistakes or leave things out, but you start by defining a set of engineering specs for this thing, literally like an engineering spec. Then you can develop the other blocks independently, as long as everyone agrees on the protocol and what needs to be passed in. You could have different teams building a vision model or a sensor, each taking different approaches, but as long as they can all plug into the same system and communicate in the same process, you can develop these semi-independently. I say "semi-independently" because the bus itself will evolve and we'll find problems with it.

Notice I wrote, "develop these other blocks semi-independently using best available methods." This might be surprising—the best available methods may have nothing to do with neuroscience. That's a possibility. The point is to solve the problem of that box in any way we know how. Let me talk about that a bit before I go further. By the way, with this approach, you could start building these other modules right away. You could build a crude language model or a crude episodic memory model, but as long as it fits the bus, it's okay. As long as you have a protocol, you can implement it any way you want.

What do I mean by best available methods? At Numenta, we have two goals. The first is neocortical theory. For those who have been here a while, you know that when we think about biologically constrained theories, I've always argued we have to be at the extreme left end—not that our theories have to explain everything in the brain, but they can't violate anything we know about it. That's the requirement for a scientific theory of how the neocortex works, and I've been adamant about that. I've discarded many ideas because they don't align with how the brain works. But when it comes to the second goal—applying neocortical principles to create true AI—what principles do we care about? If the principles are in that previous block diagram, then the actual details can vary widely. We may want to build some blocks using traditional artificial neural networks, as long as the block fits into the bus structure. We might design the reference frame system using Cartesian coordinates if that works. One concern I've had at Numenta, which I haven't expressed much because it hasn't come up, is that we've spent so much time on goal number one that it's hard to break free from it. That could be our Achilles heel. We may think everything has to be like the brain—neurons, grid cells, and so on—but others may solve these problems differently. We don't want to be bound by the fact that we got here by studying biology. We don't want that to constrain us going forward.

This is the part that might surprise you. It's like airplanes with wings that don't fly. You pick and choose. You don't have feathers, muscles, or bones, but you do have changing wing shape, which was important. The Wright Brothers discovered controlled flight by twisting the wings. They also borrowed the wing shape, but not propulsion. So, it's pick and choose. Now, we don't twist the whole wing; the original Wright Brothers planes twisted the entire wing, but now we just move the flap at the end. Same idea.

When we come back to this, the point is that if we put teams on defining what this does, we all have to agree on that, but how to build it may be very non-biological. We could go about it any way we want. I don't want to speculate now on how we'll do it. We could implement a subset of what it ultimately needs to do—maybe focus on spatial aspects instead of temporal ones, use CNNs and artificial neural networks, and some glue to build these lower modules.

This is my proposal for going forward: we start thinking about the problem this way. We begin by defining the bus architecture—that's the first thing we define—and then we start implementing the components as best we can. There's no guarantee we can be successful at this; we may fail. I don't think this architecture is wrong, but it's a really hard engineering problem. We're smart people; maybe we can do it. We should be able to make good progress, but there are no guarantees. We won't know until we start working on it. We really don't know how this is going to go.

To me, this captures the essence. I always try to imagine what the future will look like and ignore where we are today. If that's the future, then we need to move toward it. After all these years of studying this, this idea of a distributed modeling system—able to vote and communicate with each other in sensorimotor learning of models of the world—emerges. None of this was understood when I started, not 35 years ago, not even 20 years ago. But now, I can't imagine what else it could be. This seems like the basics. There are a lot of basics in machine learning, but it's also the basis of robotics. You put in these modules, each with a sort of motor output, and you could start building incrementally more complicated robotics. That, too, has to be a distributed problem in the same way—moving all your joints together. Maybe that makes the bus much more complicated; I don't know yet.

This is a pretty significant departure from what we've been thinking about, but I'm proposing it as a way to get started—literally, starting tomorrow. We could begin discussing what the bus structure looks like, what would be contained in it, and start down that path. This is my last slide and there are no more materials.