Great, thanks everyone. I'm going to be talking today about ideas around 3D object recognition and its relation to grid cells and temporal memory, with a specific focus on rotation invariance.

In general, these are high-level ideas. There's no model implemented yet, but hopefully this direction will help solve a couple of outstanding problems. The general motivation is to address rotation invariance, particularly with ideas from temporal memory. It's unclear what the appropriate space for grid cells should be, or even what can be modeled biologically. The approach I'll discuss is one of several possible options. It's complementary to other views or approaches, but I think it has some nice computational properties and would be a relatively simple starting point to get something working. It also makes connections between what I'm discussing in biology, both at the neural and psychological levels, which will hopefully motivate this approach. The idea is that whatever approach is taken should maintain desirable computational properties like path integration. My view on rotation invariance is that it should work as well as humans, because, as I'll discuss later, humans aren't perfectly rotation invariant, and that may represent a trade-off that is actually beneficial. Invariance can come at a loss of sensitivity, but I'll get into that later.

The concepts to discuss are what can be called view spheres and how this relates to 3D object recognition and representation. I'll talk about what they are, why they're interesting, and possible connections to biology. Then I'll relate this to other complementary views on how to represent 3D objects, space, and object recognition with grid cells. I'll look more specifically at how this approach can be used for rotation invariance. Some of the ideas I'll discuss under rotation invariance can be applicable to other approaches in the Monty project and may be generally useful, even if this isn't the ultimate approach we take. I'll finish by talking about getting robust, patchwise representations that can generalize well, which could be useful more generally for the Monty project. Feel free to interrupt me at any point if something isn't clear. I'm hoping to get through everything; there is quite a lot, but we'll see how it goes.

The general idea with these spheres, for anyone who saw my journal club presentation the other week, will be familiar. You represent objects with a general sphere that describes, if you imagine your eye and the object rotating, the eye is the surface of this sphere, and you're encoding the relations that hit your eye as rotation takes place. For example, a teapot would be encoded with the lid at the top, the spout, the handle, and a flat surface at the bottom. Here are some other examples: a toaster with the lever and dial, or a car. The basic idea is that you rotate the object as you study it, learn where the features are relative to each other on this sphere, and then, as with standard grid cell path integration and temporal memory, at inference you look at the features you're currently viewing, predict where the other features would be, and based on the experienced rotation, predict a feature and hopefully achieve inference.

Niels, one quick question: Is everything on the surface of the sphere, or can it be anywhere inside the sphere? It's represented on the 2D surface of the sphere. I'll get into how depth and how features can be closer to the center and how that can be represented, but in terms of the grid cell representation, it's two-dimensional because it's on the surface.

That was an approach used in the Baratsky paper I talked about the other week. Similar ideas have been used elsewhere in machine learning, such as spherical 3D CNNs from a 2018 paper.

As I'll get into later, I think this has some possible connections to biology. It's important to emphasize that it doesn't imply the local features have to be 2D. We're mapping and remembering them on a 2D surface, but the features themselves may have some depth representation, which I'll discuss more. One nice aspect of this approach is that it may have benefits for invariance, which I'll give examples of. This representation doesn't need to be exclusive to other ones. For example, 3D lattice mesh is one we've considered, as well as wireframe part-based graphs. Another discussed option is more 2D planar views. I think this is also important and would be complementary and exist alongside view spheres. Having both brings respective benefits, and in particular, view spheres are useful for rotation invariance.

To give a flavor of the motivation, imagine an infant growing up. Vision is limited to about 8 to 12 inches for the first several months of life in terms of focused vision, and babies tend to interact with objects at arm's length. Any given object is rotated many times and fixated within that distance, which might be one way to build up three-dimensional representations of objects.

What's an NB? I'm sorry. NB stands for "nota bene," which is Latin for "note well." It just means "bear in mind." It's not commonly used here.

It's just me. I think it might be a British thing. At a high level, it's hard to know exactly how well this will hold until it's implemented in practice. My thinking is this could potentially help with distortions to the geometry of the object. One thing we've discussed is what to do when the mesh of an object starts distorting slightly, but because many features are essentially just being projected to the sides of the sphere, you can make strong changes to the shape of the features as long as the local feature detection is still operating. As long as the local feature detector for a handle is invariant to these changes, the actual representations—particularly the relations of these features relative to one another—can be relatively robust in this view. This wouldn't necessarily hold if you're doing a representation in profile.

In a hierarchy, the lid itself might be recognized in a similar way at lower levels, so you could have many distortions of lids and still recognize the lid. At this level of the hierarchy, maybe you're recognizing the full object. Within the sphere, you can have these changes to the object, but because everything is being projected outwards, many general structural relations, like one feature being in front of another, will hold. This leads to the next point, which is scale invariance. I'm not suggesting this will be perfectly scale invariant, but in a similar way, because you're projecting features onto the sphere, the general relations will hold. If you had a 3D mesh, for example, the distance between the handle and the spout could be very different, but in the case of the view sphere, you'll still have this 180-degree relation between the spout and the handle. I've heard before that scale invariance from a neuroscience point of view seems easy to do, so I don't think a new sphere is needed for that. I'm not convinced or fully understand the comments about distortions and how the view sphere helps with distortions.

The way I've always viewed this is that as long as the relative features are in the relative position to one another, you'll be able to manage it. You can have local scale distortion and similar changes. I'm not sure why the view sphere helps with that specifically. It seems like any kind of...

any representation where you're storing the relative position of features locally would handle distortions, as opposed to a global approach. My point wasn't clear about how we actually help specifically in the distortion case. It seems that anytime you have a grid cell-like representation where you're representing the relative position of features, it would work. My concern with relative arrangement is that as you distort and change things, if you've learned with one object and are used to recognizing the spout by seeing its tip, the distance relative to the handle will be different when you encounter another object. I'm not saying this is the only way to solve scale invariance. If you're able to appropriately scale the input relative to your learned arrangement of features, then that's fine. However, if you are seeing a novel object for the first time, it could be challenging to know how much you need to scale the movement. Maybe that's just something I'm naive about, but with a view sphere, the idea is that the relative coordinates are based on degrees on the sphere, so all that matters is rotating it 180 degrees. It doesn't matter how far apart the spout and the handle are. If you have a spout and distort it along an axis, the relative positions change, but the position on the view sphere doesn't change as much.

I'm somewhat skeptical about the benefits of the view sphere for distortions, but I still like the idea. The thing I like about it, and as we've communicated in emails, is that we have to figure out how to map three-dimensional objects using two-dimensional metrics like grid cells. This approach gets you in that direction. The object is three-dimensional with the view sphere; you don't see the entire thing at once. You can only see part of it, so you rotate the part in front, and the part that's orthogonal to it becomes somewhat two-dimensional at that point. I like the idea, but I'm not necessarily agreeing with all the benefits you're seeing right now, and I don't think it matters. Until I see this actually working, I'm not sure how much this will hold. That's just one of the motivations for taking this approach.

There was a brief mention of how to represent the fact that objects are 3D, beyond just rotation. If you imagine a boat performing seismic or sonographic detection of the topography of an ocean floor, you can eventually build up a three-dimensional map of Earth. Earth is huge, so that's going to be approximately smooth. If you imagine something with more distortion, the general idea is that at each location on the sphere, as well as learning a feature associated with that location (such as color or texture), you could also learn a depth representation. This could be relative to either the center of the object or the estimated center, or it could be relative to the eye. I'm not sure which would be best. The hope is that this would give at least a coarse representation, for example, of a mug. You know it's really far away when you look down into the mug, but when you look at the bottom, it's quite close. There's a depth offset, and potentially that's enough to build a coarse mesh and understand that a small object like a dice will fit into that. If you look at something like an orange, which has uniform depth all around, you could understand there's no cavity and you can't put anything into it. It seems we've discussed that the brain has to represent depth. At one point, it would have represented depth as well as the eye, and then you represent depth relative to the object itself. You have to have both representations, and if you do, you can calculate and go back and forth, which helps with scale.

It's not either-or. That's an interesting point I hadn't thought about. Maybe just apply both.

So far, I've talked about more intuitive examples like a teapot or a car, but the view sphere can get confusing depending on the object you're trying to represent. With the mug, you have a distance feature that you understand is flat on one side, and then a representation for the rim. As you transition from a bowl to a plate and it becomes more two-dimensional, the view sphere representation might still work, but it gets stranger because most of what you're representing is that the object is flat. At certain angles, you suddenly have a change as you're viewing the rim.

and then this open question about how to represent the depth. It might still work, and related to this, the even more extreme example is almost one-dimensional objects like a pencil. For representing these kinds of objects, you don't really need a view sphere because they don't have much of a 3D structure; they are largely described by one-dimensional or two-dimensional structure. The planar views, which I'll discuss later, might be more relevant. I'm not disagreeing with the general concept, but even a pencil has writing on it. If you want to see the writing, you have to rotate the pencil. It's similar to the logo on a coffee cup. It's still a 3D object, so if you need to make this distinction, some objects are long and skinny, some are round and fat, and they have all different shapes. At any point in time, every object—almost every object, with some exceptions—any physical thing would have a view sphere, and you would see different parts of it when you rotate it. So it seems that the principle would apply to almost anything. Am I getting that wrong?

No, that's not wrong. Sorry, I'm not trying to say that it can't be applied. It's more about how I picture it in my head and how the features change. I feel like it works nicely with more three-dimensional, slightly rounder objects. When it's something long, it just has certain views where, if you're looking at a pencil, your eye might follow the pencil, but if you're always focusing on the center—and this is an assumption and a limitation I'll get to—then you're essentially just going to see the same feature with any orientation, except when you look at it straight on.

That's where it gets a bit odd, compared to following the object with your eyes.

You get a better sense of the structure of the pencil. One question: I think I didn't get this in the beginning. What coordinate system is this view sphere in? Is it in the sensory frame or is it an object-centric representation? Some of the problems you're talking about wouldn't occur if it was an object-centric view sphere, right? Regardless of how you rotate the object relative to you, the features are still in an object-centric view sphere and remain in the same place.

So, with the pencil, you're rotating it. If you have a pencil and you rotate it in object-centric coordinates, if you project the features into a sphere that's in object-centric coordinates, nothing would change as you rotate it. It's just relative to the object. What you're talking about is when your sensor is at a specific point relative to the object's reference frame. The view sphere is not an object representational; it's a way of thinking about vision. Isn't that right?

I guess that's my question. I don't see anything here that says it couldn't be object-centric. It's not anchored to the object, right? The spheres are anchored to the object. That's a different problem. It is anchored to the object, but you're basically saying, "Where is my sensor around this object?" In particular, my sensor is looking down at the approximate center of the object. So that's not a problem with the sphere; it just happens to be that your particular viewpoint is not giving you much information about where. Exactly.

That's what I'm trying to say. In your proposal, the sphere is anchored to the object, and the representation of the sphere itself and the features on the sphere do not change as you rotate the object. It's just what you can perceive at any point in time, and basically your estimated location on the sphere. The sphere is a way of thinking about path integration in 2D in vision. That's how I was thinking about it—a way of saying, "I've got a 2D path integration. As I rotate the object, I'm moving in 2D dimensions, always locally in 2D." Ideally, that's just the thing about path integration. The actual thing you're seeing may not be a sphere at all; it can be anything. What you really want to do is bring up the most orthogonal flat views of that object so the 2D representation is least distorted. I don't understand it completely, but that's the way I was thinking about it. It's really a way to think about path integration and vision.

At any point in time, you have some estimated point on the sphere that your direction of gaze is at. As you move around, you can pass in a great, as long as you're on the sphere. I think I explained some of this in the journal club, and I've forgotten what, or I know several of you weren't there, but I just forgot that I needed to explain some of these ideas again a bit more clearly. That's the idea: you are estimating where you are on this view sphere, which describes your sensor relative to the center of the object. You can then path integrate over the sphere and predict features. The features projecting onto the sphere are intrinsic to the object.

Clarifying question: what if I move my gaze so it's now partially off the sphere? In general, eye movements are an issue. At the moment, I'm making the strong assumption that you are, say, an infant holding an object and just rotating it, not saccading around the object, but obviously eye movements are important, and I will touch on that in a bit.

That's the general concept. Now I'll make some connections to biology and psychology and discuss why these kinds of spheres might be interesting. Why not just use a 3D mesh or lattice? As mentioned, 2D surfaces have some appealing properties, which I'll talk more about, and it may align better with how we represent and understand the world, at least in vision. A 3D lattice is very costly, especially if you were to represent it at a cellular level in all three dimensions. As the Clucas and Marcus Lewis paper explores, there are other ways of doing higher dimensions more efficiently.

This view sphere approach is a way of getting something potentially working. The image here is not showing experimentally measured grid cells; it's a theoretical paper from 2013 discussing the possibility of spherical grid cell representations in the brain. Interestingly, a research group around the same time did a study on rats raised in spherical cages. They only published an abstract at a conference, but I contacted the author and they're potentially going to publish the actual study or a follow-up very soon. I wasn't able to get a preprint in time for today. In the abstract, they claimed they found something like that. It wasn't a strong claim; I don't want to misrepresent what they said, but it was basically ambiguous. It definitely wasn't like, "Oh yeah, they look like this." It was more, "We found some interesting grid cell properties when we raised them in 3D environments," or something like that.

Niels, I thought it was only us old people who couldn't remember what happened last week. It's good that you also can't. So the sphere where the rats were running around—was it a complete sphere or just a half sphere? Unfortunately, I don't know. I'm guessing it was a spherical cage; I don't know if it was a ball that moves under them or a spherical cage with a mesh they can climb on any surface. They cannot walk if it's upside down. My hamster could. They're very good at climbing.

My hamster was very good at that kind of stuff. Grid cells can have different frequencies, which can map naturally to different hierarchies of feature detail. Not to go into this too much, but as has been discussed several times recently, the evidence for an elegant 3D lattice of grid cell representations in animals navigating in 3D is not there. There seem to be clusters with some structure, but it's definitely not a lattice-like structure.

There was a study that Heiko mentioned, where they had a sloped surface. Hypothesis two was most consistent with their measurements. If you extrapolate this, you could imagine warping the plane around so the grid cells mapped regularly on the sloped plane. Is that right? Now you're showing these grid cell fields below the plane. Is the animal able to go below that? No, that's just a hypothetical illustration of what the grid cell representation might look like. Is it a lattice? If so, the plane should be intersecting in different ways, but he basically says it's a two-dimensional sheet on an angle. That was what they found.

As mentioned, there's a preprint and possibly some experiments on animals and pyramids happening now. Even if grid cells aren't representing this view sphere, I wonder if there could be a role for 3D head direction cells, which are established in bats. They could potentially do something similar. Is it true that head direction cells in bats have been shown to be 3D and perform 3D path integration and orientation? As far as I understand, yes. I don't know if there's been any follow-up paper debunking this, but this is what I came across. They found it was a toroidal coordinate, not spherical, because the toroidal has smoother properties. I think there's been a bunch of work since then, the details of which I can't remember now. I know I co-signed a couple of years ago, and there were some presentations on this. I can look it up. I'm trying to see the difference between these two.

I can go into it, but it takes some description. It has to do with how the animal pitches and becomes upside down. On the spherical coordinate, this causes a rapid, sudden change in the azimuth, whereas in the toroidal, it's always smooth and consistent with what they see in the neurons—how the population geometry changes as the bat's orientation changes. My main point is that we could justify this sort of modeling in terms of connections to the brain, based on head direction cells as well. Actually, to me, this is pretty interesting, but I won't be able to remember the reference. Could you send it to me later? Yes, definitely.

I was interested in this topic because I remember attending a presentation a few years ago by one of the researchers behind this study. They mentioned that bats tend to fly with a 2D bias, and that food had to be placed at different elevations in the room to encourage true 3D behavior. I couldn't find this in writing or any convincing study about a similar bias in rats, but I did find a study on dolphins. Dolphins spend a lot of time at a 2D plane and then shift to another plane, which could reflect a bias in their environment, such as concentrations of prey. These observations are based on tracking their depth over time, and the study didn't make claims about their 2D depth preferences, but rather how their behavior changes throughout the day.

There are also popular science articles suggesting dolphins prefer 2D movement, but I struggled to find a convincing paper on this. Generally, it seems they don't weave up and down over tens of meters, possibly due to prey concentration or because it's simply easier. It's difficult to extrapolate from this data because it's unclear whether dolphins know their exact location at those depths. For example, if I were flying an airplane above the clouds without visual cues, my path integration would be unreliable until I descended again. Dolphins may experience something similar, lacking landmarks for navigation and possibly needing to surface to reorient themselves. As a human, I know my depth changes from having to equalize pressure in my ears, so dolphins may have a similar mechanism, but at certain depths, they may not know their exact location.

If I imagine flying at different heights in an office, I would always map myself to the 2D floor plan first, identifying my room and then considering my height. This relates to the concept of the view sphere, where you have a 2D surface and then depth. I don't see locations at different heights in the same room as fundamentally different; they're the same location, just at different heights.

Looking at animals and moving away from grid cells and navigation, considering perception in humans, there seems to be reasonable evidence for a preference for 2D-like representations. Later, I'll discuss how learning certain views affects generalization to arbitrary views.

One interesting finding is that both children and adults show a bias toward studying side-on, orthogonal views of objects, rather than angled views showing multiple faces. In a 2014 study on children, the effect size was small, but it was more convincing in adults in the Harmon et al. 1999 study, where people spent more time on these views. It may make sense that, if it's expensive to store views, it is computationally efficient to save a few summary views and interpolate between them to recognize arbitrary viewpoints, rather than memorizing every possible viewpoint.

Is it just based on time spent looking at those figures? There were a couple of different measures. The Harmon et al. study, which I read in more depth, mainly measured time. I find it harder to work with planar views, so I need more time to understand them, whereas the 3D one on the right is easier for me to grasp. When learning, I struggle with those projections just to understand the object as a whole, whereas with the 3D object, I spend much less time. The left one is more difficult. One paper showed that it's actually harder to recognize the one on the right and it takes longer. I'll get to that paper in a moment. This is just looking at preference in how people study an object.

What's interesting about the picture on the right is that, perspectively, it's totally wrong. If it were like this, it would be expanded. It is not as bad as your firetruck, but if you look at the rear picture, it's distorted. The big evidence for me has always been several things. One is that if you want to describe your understanding of an object, whether writing, verbally describing, or drawing it, you tend to use the view on the left. You don't tend to use the one on the right. You see the one on the right all the time and can recognize it rapidly, but it seems harder to record. I have all these images of what cars look like from the side, top, and bottom, and I can draw those readily. That was one piece of evidence that always worked for me.

We're also dealing with the fact that grid cells seem to be 2D. If we don't have three representations, then we're going to have 2D representations, and we seem to pick these. It's odd that you can recognize the one on the right very rapidly, even if your internal model is the one on the left.

I'll talk more about the evidence around that. In answer to Scuba Test's question, James and I'll study also looked at how well children recognized and named objects. They found some evidence that children who studied planar views more were also better at recognizing objects, but that's possibly tenuous. A more interesting study I'll mention in a second. Interestingly, adults also tend to draw objects in planar views. What was particularly remarkable was the way these coffee cups were represented. It's a coffee cup viewed from above, and the saucer is viewed from above as well.

A more interesting study is the one from Langlois et al. They looked at memory biases in humans for particular views. They showed 3D objects, like a teapot, for a while at a random orientation, then showed another random orientation. The task was for the participant to use a keyboard to recover the first orientation as accurately as possible. Whatever they arrived at was then given to a different Amazon Turk participant, who did the same thing, and this went through about 20 iterations. Over 20 iterations, this would converge to particular planar views. There was a bias, even though they were trying to recover the original view, it slowly drifted toward these particular views. These are the clusters in the points they drifted toward, and different objects have different clusters depending on their three-dimensional complexity. The alarm clock only had two clusters, a front and a back, whereas more complex objects had more clusters. These clusters also had an orientation associated with them, so people tended to shift the teapot to be upright.

Importantly, there was not just a bias; people were also faster and better at recognizing objects given these views. For example, this is a biased view of a stiletto shoe, and this is the least biased view. Even though I know that's a shoe, I can't really see it looking at it there.

This was more convincing to me. It's interesting to see that it's not just about the representations people have. The earlier studies looked at how people study and learn objects, but they converge on a similar argument.

It's worth briefly mentioning the term in psychology of canonical or three-quarter views, which refers to this offset view, like the car before or a horse viewed slightly from the side as opposed to in profile. The word canonical essentially means the most typical or commonly used view, and there's evidence that people prefer canonical views over planar ones. People are faster and better at recognizing objects in canonical views, and some studies specifically identify the three-quarter view as canonical. 

A classic result from 1981 shows that when adults are asked to draw coffee cups, they tend to use a semi-three-dimensional structure, which is easier for adults than for children. Drawing a horse from a canonical view is extremely difficult for most people, but drawing a coffee cup is simpler because it's easier to add features that signify three-dimensional structure. This tendency may result from canonical views providing more unique descriptors of an object. Planar views are potentially more useful for learning because they offer a parsimonious description, but canonical views are likely better for recognition since they present more features at once. This may explain both drawing and recognition tendencies. Some believe these views are more typical because, in real environments, seeing an object dead-on from a particular angle is less likely, and the combination of views is considered equivalent. There's a large literature discussing this debate.

For example, consider a box like a die or a child's block with pictures. You learn what's on each face, but people generally don't learn the relative positions of the faces unless there's an obvious connection. It's interesting because you clearly learn the planar view of each face, but you almost never see it that way. If you only see the face view of a cube, all you see is a square and can't tell it's a three-dimensional object. When you see a slightly askew or three-quarter view, you immediately recognize it's three-dimensional, and each plane can be recognized independently as object features. However, most people can't reproduce the relative positions of features like the pips on a die. Seeing multiple two-dimensional views at once provides more features and reveals the object's depth, but that's not necessarily how memory works for these objects.

It's almost like seeing two or three views at once, each recognized as a slanted view of a two-dimensional surface, providing more data and depth cues. This seems consistent with the idea that we learn planar views but prefer three-quarter views.

I'm wondering about the preferred view directions. This doesn't contradict the sphere model; rather, they could be separate. I'll explain why I think they're complementary. If you think of the sphere as a way of doing path integration and not as the object itself, they're not contrary. The projection onto the sphere can be confusing, but the sphere serves as a way to track movement. Projecting a two-dimensional surface onto the sphere at any point is unclear, but the sphere is important for relating different views to one another. You need a spherical representation because the angles are inconsistent for different objects, and you need to know where each planar view is relative to the others. A sphere is useful for mapping and saving that information.

It's also generally useful for studying an object from different angles and developing a curved representation, in addition to a more saccade-based two-dimensional representation. You could have both types of representations interacting with each other. When you rotate and path integrate to a new view, you'd have the learned representations on the view sphere, which could also recall the representations associated with a planar view as you saccade around with your eyes to study the object.

There seems to be more evidence that grid cells can learn to represent different spaces as long as they are useful, whether it's a family tree or Cartesian coordinates. Both view spheres and planar views, along with other representations like wireframe and part-based models, may be examples of useful structures.

If you ascribe to the view that grid cells emerge through learning structure in the world, two examples of structure that arise are rotating an object and how its features relate to each other as you rotate it. The view sphere and holding an object at a particular view and looking around it—a more 2D planar view—are both cases where grid cells could potentially emerge to handle these tasks.

There are multiple examples of conceptual maps onto grid cells, and as far as I know, they are always two-dimensional. For instance, experiments with birds focus on two-dimensional spaces.

In these abstract spaces, it's unclear if there is an equivalent to rotation, or if rotation is a unique challenge for vision that grid cells don't generally address. I don't know what the equivalent of rotating a family tree would be; perhaps moving down a generation or breaking up generations, but it wouldn't be three-dimensional—just rotating around two dimensions.

In vision, the primary visual cortex in mammals, especially in humans, is unique because it has extra layers that process information from both eyes in specific ways. This extra neural machinery only appears in V1 in animals with advanced vision, so some mechanisms discussed here may be vision-specific and not present in other sensory modalities like touch or sound. Touch is not at a distance; your fingers are on or near the object, and it's unclear what the equivalent of a 3D representation would be in hearing or abstract domains. Grid cells may be as two-dimensional as possible, and the brain may develop adaptations for vision. V1 could be seen as specialized preprocessing, since V2 and V4 don't have these features. At the very beginning, you deal with view spheres, rotation, and distortions in three dimensions. For example, you can imagine a two-dimensional object, like a printed word, and rotate it in-plane without confusion, even if it's distorted. The brain compensates for this, and there may not be an equivalent in touch or other modalities. This may only exist because eyes are not on the object and things move away when rotated, which doesn't happen in touch.

When viewing an object from a preferred view, you can have a more two-dimensional, saccade-based code, mapping features to locations on a grid. This could support path integration along certain axes or planes, helping you understand spatial relationships like headlights in front of the tire, which is in front of the car door. The fact that these are stored in two views could explain why we are worse at recognizing objects from arbitrary angles, consistent with studies on grid cells in primates studying scenes.

These representations are not contradictory; they work well together. The view sphere helps you understand how features relate as you rotate an object, while the planar view is better for understanding structure along major axes of variation. When you move along the view sphere, cells associated with a particular planar representation reactivate, even though you don't store a 2D planar view at every orientation.

Having view spheres as the underlying structure is important because these planes are not necessarily perfectly orthogonal, so you need a way to understand their relationships to interpolate and predict views from novel orientations and achieve rotation invariance.

This is a high-level summary of these concepts.

There are some major questions outstanding. How do you account for eye movements when you look at the view sphere? I'll talk about this in a bit. I already discussed the other two aspects of representing narrower or flatter objects, and how useful the depth representation is compared to a full mesh-like representation. Also, how do we establish the preferred optimal plane of views in an unsupervised manner? Maybe some natural policies will result in these emerging, or perhaps you need a summary statistic to find the most orthogonal representations.

Heiko, do you have an 11:30? We do 11:45. Ten minutes. I won't be able to get through everything, but we can maybe find a time. I'm happy to jump on in the next couple days and finish the presentation if that works. We have the research meeting tomorrow, but you're off tomorrow, right? I'm happy to join just to give the presentation if that works. If you like, you can continue there. We don't have a plan yet for the research meeting. If you're willing, I'd be happy to do that. I'd like to see the rest of it. It would be a shame to wait until I come back, since I'm hoping some of these ideas will be useful for Abby, whatever he's doing when he gets back.

Regarding eye movement: so far, you hold the object and look toward the approximate center. But what happens when you start moving your eye? One idea is that there may be a transformation so you can approximate, when you perform an eye movement, where a feature would be projected onto the view sphere. You could estimate, for example, if you were holding this in your hand and used the card to the right, if you were interested in something on the right, you would naturally rotate the sphere so that point is in front of you.

If I'm looking at a coffee cup and trying to see the logo, I'll rotate it until the logo is facing me. We have to be able to see the code over there, but it seems we don't have a bias to rotate it into view unless we're interested in that location. It may not be much of an issue, but it's worth mentioning that the view sphere doesn't handle this elegantly, though there may be ways of dealing with it. Estimating where a feature is projected onto the sphere could use geometric principles or a trained neural network as a function approximator.

Predicting where you'll be on the sphere is straightforward when you make an eye movement; it's just that the feature itself may be somewhat distorted as if you had looked in that direction. The feature distortion is another problem, but I feel it would be less of an issue, though it depends on the object. Both of these would need addressing to enable the system to do both rotations and eye movements.

I thought it would be worth including a slide on implementing something like this, whether by grid cells or particle filters. In the journal club presentation, I talked more about particle filters. There's a video linked in this presentation on YouTube that describes the intuition. Both have pros and cons. For me, the most important are grid cells, because we reuse grid cells across objects, allowing more graceful scaling with the number of learned objects, though it eventually degrades. With particle filters, as they're normally implemented, you would need a new particle cloud for each object you learn, and as that grows, you may run into computational issues. There may be ways of overloading particles.

I haven't thought about that too much yet. The nice thing about particle filters is they have a more natural probabilistic interpretation, which will be relevant to the next topic: rotation invariance.

Five minutes left. Is this a good stopping point? Maybe this is a good stopping point.