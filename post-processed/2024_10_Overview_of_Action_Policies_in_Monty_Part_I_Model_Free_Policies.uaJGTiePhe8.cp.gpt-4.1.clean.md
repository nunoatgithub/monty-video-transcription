Today I'm going to be talking about action policies and how they exist in Monty, covering mostly their current state but also looking a bit toward the future. It's too much to cover in just this meeting, so I plan to discuss the different types of agents we have right now, as well as two entry-level policies: utility policies, which I'll explain, and input-driven, sensory-driven policies. In the next meeting, I can talk about the other types. Throughout this, I'll try to touch on where relevant the concept of abstract space and how that might fit in.

The core aim behind our work on policies in the short term is to achieve robust and efficient object recognition and pose recognition. We want Monty to act in a principled way, not randomly, and to handle things like similar objects that might be ambiguous, resolving that ambiguity as quickly as possible. Ideally, those decisions should be made quickly and without huge computational costs.

As alluded to with the abstract spaces, Monty is not just about 3D object recognition, and the policies are not just about enabling 3D object recognition. While we're working on this, we're also keeping in mind the longer-term aims of Monty and how those will fit in. You had it grayed out in your previous slide, but beyond this, it's really about using action policies to affect change in the world, to bring about goals. Most people would think that's why we move, as opposed to moving just to learn, so let's not forget that. It's a good way of summarizing that the existing policies are really focused on the perceptual element, but in the future, it's going to be important how you affect change in the world. This is the background we have to get right before we move on to having the system act in the world.

First, just briefly, a word on Monty agents. I'm going to assume some background awareness of Monty, but whether you're reading the documentation or have recently joined the team, you're probably aware of the surface agent and the distant agent.

We have this separation, but you might start wondering what this really means, since Monty is an artificial system. If it's implemented in the real world, there's nothing to stop us from putting a camera on a robotic arm. The way the surface agent—the "finger," if you will—is implemented at the moment is actually with a depth camera that maintains a very close distance.

It can also get color information.

It's useful to clarify that the differences are fundamental. The way I'd break it down is you can think of the distant agent as an eye and how it moves in space and samples information. It can sample very rapidly through saccades, moving from one side of an object to the other almost instantaneously. With peripheral vision, you get a broad overview that could, at a coarser level, give you some information about what's out there, and then you can choose to sample that information with higher fidelity, either by focusing the distant agent sensor—the highest resolution sensors, like the fovea in the human eye—or by moving your hand or another sensor to that location to sample it in more detail. With the distant agent, you get one view of the object, so there's a lot of self-occlusion compared to the surface agent.

If I'm looking at an object, I just have one angle. I can't just move my eye to the other side of the object, though I can saccade. I could move the entire agent to the other side, but the eye can't just change position. If it's a finger, I can move it around the other side, but if it's my eye, I have to move the object or my body. Moving my body is a slower, more cumbersome way of moving your hand, but it's the same basic idea. A Monty agent system could have cameras located around a room, for example. It's more common for self-occlusion to be a problem for a distant agent. The term "self-occlusion" just means the object is occluding parts of itself, so you can't see the backside. The object, in some sense, occludes itself to all sensors; if you don't move them, they can only sense one spot, and movement is required to sense more spots. Some sensors are harder to move and take more time than others, but they're all basically the same. Your finger can't touch the front and back at the same time; it has to move, and neither can your eyes. So it's not a strong distinction in my mind.

That separating space is important. If an agent is in a potentially hazardous environment, even a simple hazard like moving around an object, you risk knocking into it or disturbing it. The distant agent has a much simpler actuation space because it just moves as it chooses and doesn't really interfere with the environment or put itself at risk. But that also has its limitations. You're limited to air-propagating modalities of information and sensing, whether that's electromagnetic radiation or sound, and if you're separated by that space, you also can't directly manipulate the object.

I'll continue through this material, but feel free to ask questions or comment at any time. With the surface agent, it's intuitive to think of it as something like a finger, or maybe a robotic finger where you could add a camera. You still have all the modalities available to the distant agent, but you also unlock texture, which requires touch. You can imagine having more sensitive ways of measuring depth and temperature due to the proximity. By being next to the object, you can manipulate it. However, this leads to a more constrained and complex movement space. You can't just move arbitrarily; you might collide with objects, so movement needs to be more deliberate and thought out. The limb or system supporting the surface agent tends to be more complex than a simple ball-and-socket system, like for an eye.

As mentioned, these agents should eventually work together. For example, a distant agent can quickly get an overview of a scene, parse the information, and then use the surface agent to gather more detailed information, reduce occlusion, and potentially effect changes in the world.

From a practical point of view, you can think of these agents differently, but theoretically, they're just points on a spectrum. Touch is interesting because sometimes you can touch something with a stick in your hand—touch at a distance. Manipulating something with a tool, like a screwdriver or pen, is also touch at a distance. There seems to be a spectrum of proximity and distance, difficulty of movement, what you can sense, and what you can't, but they're all on a spectrum. These are just two possible action spaces; others might have a car, where the action space is acceleration and steering. These aren't the only options. Ultimately, we want an understanding of action policies that recognizes a spectrum—cortical columns all work the same way, and with different parameters, it looks like a surface agent or a distance agent. There must be a common theory, since humans and animals are extremely flexible about sensors and movement. It's not a hard differentiation, and that's worth emphasizing.

We also have types of policies we've implemented, and we're trying to classify them. There are maybe three broad types. Utility policies can be thought of as experimental tools; they're not what the model is doing in any intelligent sense, but something we enforce for experimental purposes. Then there are input-driven, low-level sensory information-driven policies, like reflexes. Finally, there are learning module-driven policies, which are more like model-based reinforcement learning, where a model of the world is used to drive action. The main categories seem to be learning module-driven and non-learning module-driven. The reason I'm focusing on this one is because I don't know if we've discussed it much in meetings, and it can cause confusion. It's not something that exists in the brain.

The input-driven policy is interesting because it's anything that's not cortical. Subcortical structures also initiate movement, and reflexes are one form. Reflexes have a sensory input and a neural system that moves the body. Some things are controlled by the cortex, and some aren't—the cortex just finds out about them. For example, hitting your knee and having a reflex reaction, or the cerebellum deciding to do something. I was torn about using the word "reflexes" because I don't mean it in a strict neuroscience sense, but more colloquially. It's basically learning model-driven and non-learning model-driven: either the cortex initiates it, or something else does, and the cortex finds out and has to decide what to do.

In that latter case, it's almost just part of the environment to the cortex.

There's research showing that when a behavior occurs that the cortex didn't initiate, the cortex will still think it did. Mentally, as a human, you'll say, "I must have meant to do this," even if you had no conscious knowledge of the action. It might have been imposed on the body or initiated by a subcortical structure, but people will make up a story about why they did it. The cortex observes something and says, "This happened, maybe I did it." It's just observation.

In reinforcement learning terminology, this would be model-based versus model-free policies. With input-driven ones, you don't need a model of an object to decide how to act. For example, one of the input-driven policies we have is to follow principal curvature: we look at the sensory input, assess the curvature, and follow that.

For example, the learning module might be aware that we're following a certain policy and decide, "I don't have a good hypothesis right now, so let's just follow the curvature." In this case, we're not using the models inside the learning module. Alternatively, this could be something handled by the cerebellum, which might determine, "I'm on a surface, I move along the edge," so we don't need the cortical columns or learning modules for it. It's not necessarily only reflexes; it might still be a separate movement. That's what I was trying to get at. I also tried to avoid the model-free versus model-based terminology because, based on the models we develop and have access to in the learning modules, we'll probably use some model-free elements eventually, such as learned shortcuts. You could imagine thinking through a learned graph or using heuristics that leverage and combine both approaches.

It's hard to draw strict boundaries; everything is on a spectrum. For non-learning, pedagogically driven motions, we have free rein to do whatever works. We're not constrained to biology or specific types—whatever works is good enough, since these are biological hacks the body uses to figure out reflexes. For example, when you're walking, how you place your feet and balance isn't handled by the cortex. We shouldn't burden the cortex with things it's not suited for. We can do anything we want for these input-driven, non-learning model policies.

Input-driven means the decision is based essentially on external information alone. The learning modules might still inhibit or disinhibit these policies, but the necessary information for deciding what to do comes from internal or external inputs. These are generally quick, simple rules. If it's learning module driven, then it's based on internal learned models and is generally a more complex, methodical deliberation determining the action.

External relative to what? Are we talking about the cortex? It could be internal machine status, which is internal to the machine but not external to the environment. Here, I mean external to the meta-agent, since many of these are driven by subcortical structures. External could mean subcortical structures—other parts of the brain can be involved; it doesn't have to be outside the body or homeostatic mechanisms outside the cortex.

Walking, for example, is a complex process. There are motion generators and action policies for walking and balancing that aren't in the cortex, but you don't walk just because your knee was poked. So, I interpret "external" as external to the cortex.

It could be either. I'm emphasizing external to subcortical and cortex because if most of these policies are implemented in subcortical structures, the information they receive is external to them, but it could also come from other parts of the body.

The policy's aim can be inference-focused, which is most of our current policies. It can also focus on learning, exploration, or changing the state of the world. Most of the ones I'll discuss fall into the inference category, and I'll note if they are more about learning or exploration.

With that, I'll move on to utility policies.

The best way to think of a utility policy is as something independent of the Monty system's intelligence. It's present in our codebase as a tool for scientists and researchers to study Monty's capabilities and debug it. They're usually called in the pre-episode function, before any information is sent to the learning modules, to get the environment into the desired state. 

I thought utility policies would include things like moving your finger around the object, but it's even more meta than that. It's like, in an experiment with a mouse, the scientist first moves the mouse into the correct cage before the experiment starts. We can't leave it to the mouse to jump around the cages. The mouse is a physical entity in space, and similarly, as soon as Monty is instantiated in the simulated world, it needs to move to a particular location, which we enforce. It's not something Monty is thinking about.

It's funny to call it a policy. Maybe that's a remnant of how it's implemented in the code, but ideally, we would disentangle it more. I take "policy" to mean how our system behaves, not the researcher putting the mouse on the wheel. We call it that because we use the agent's action space to move to the correct location to start. We shouldn't confuse this with the actual policies the agent uses during an experiment. There's room for improvement here. Currently, we get the agent to shuffle forward until it satisfies some criteria, but we could use a more external action space and just set the agent's position. Just be careful with the word "policy" and use it as most people understand it.

As Viviane pointed out, these are generally called in the pre-episode, before information is sent to the system. They typically make use of the viewfinder, which I'll show on the next slide. That's sensory information that isn't sent to the learning modules. They also use the semantic sensor, a feature in Habitat that tells you the ground truth of what you're looking at—whether it's a specific object, a blank wall, or empty space. This is privileged information that we're generally trying to avoid giving to the Monty system.

There are two main ones: GetGoodView, used by the distant agent, and TouchObject, used by the surface agent.

The viewfinder, familiar to most, is a zoomed-out view of the scene and what's visible. There's also a zoomed-in, small straw world view of the objects and sensory information coming in. This is what's actually sent to the learning modules and used to build up models of the world. The viewfinder is a tool for the experimenter working in Monty, useful for debugging or understanding system performance. We can also use this viewfinder for utility functions.

GetGoodView, used by the distant agent, tries to move forward while facing the target object until a majority of pixels in the viewfinder are on the object, as opposed to empty space. This is conditioned on not getting too close to the object. It uses the viewfinder, the semantic sensor, and the resulting effect. For example, at the start of an episode, we may be far from an object, but by moving forward until a significant portion of the input is filled by the object, we ensure a good starting point for making small saccades over the object to explore its surface. This is simpler to implement than trying to predetermine exactly where the agent can be, which risks starting inside the object.

TouchObject has a similar core purpose: getting the agent to sense the object of interest at the start of the episode. For the surface agent, it's about getting onto the surface, or more specifically, within a short desired distance using a depth camera. The image in profile shows a mug, representing the distant agent. It begins the episode, moves forward until it's touching the object, and then, once TouchObject is satisfied, hands over to the more internal input-driven policies to follow the surface. The blue line and red dot are meant to give a sense of orientation—imagine the base of the finger and the finger itself.

The red dot might represent where it's touching the object, and the blue shows orientation. You could think of it as a joint, or as the spot hitting something.

TouchObject is also used if the agent has fallen off the object, which can happen. It can reestablish contact with the object if that's lost.

There are subtle differences: only GetGoodView uses the semantic sensor, but both use the viewfinder. TouchObject can search for an object even if it's not visible in the viewfinder, looking around until it sees something and then moving toward it. GetGoodView assumes the experiment is set up so the target object is within the viewfinder field and then orients accordingly. In practice, TouchObject tries to get closer, defined by a parameter, while the distant agent tries to maintain a minimum distance from the object.

The distinction between these two, or the fact that they're implemented separately, is probably a historical quirk of when they were implemented. A simple improvement would be to consolidate these into a more unified set of utility functions. Scott shared some findings from a policy he's been experimenting with as an alternative to GetGoodView, aiming to fill the input image with the desired object without clipping it at the edge, which sometimes happens now.

In general, regarding utility functions and utility policies, it would be beneficial to clearly separate what is truly utility from what a model can use. For example, the touch agent currently uses the model or the touch object to get back on the object if it loses it, which only happens after the experiment has started. Similarly, for the learning module-driven policies—which I'll discuss next week—the agent is generally teleported to a location and then refinds the object using these, but we want to limit the use of things like the semantic sensor once an experiment has started. 

Just a note on the semantic sensor: as far as I remember, the GetGoodView policy also has an option to operate without the semantic sensor, where we estimate the semantic information from the depth information. That's a good point. In that case, the only place we'd need to use it is in the multi-object setup. It's fair to use it during the pre-episode, but we should ensure that we separate that out and only use the depth estimate once the experiment has started.

Were there any other questions? Tristan, you mentioned wanting to clarify some things about these policies. I hope this helps. Distinctions are useful for everyone when looking at the code. I've written up some notes that I can make sure we document somewhere, since I don't think anything was written up to clarify these differences. It might be good to put those in the glossary as well.

That's the overview of utility functions and utility policies. Now I'll talk about the input-driven ones. These are defined by using sensory information coming in to make a decision, but without learned models of the world. We have the base distant policy. The spiral scan actually doesn't use any input, I think. Does it not do moving back? Isn't that a separate policy, the move back? Maybe I'm forgetting. I was wondering about this. It's definitely not a model-based policy, so we can put it in this bucket. It might be worthwhile to revisit all the language here, because even the learning module policies are input-driven in some sense—they have to know where they are and how objects are moving. We used to call them bottom-up and top-down policies. I still like the model-based and model-free terminology because it emphasizes that the learning module is based on learned models. I would think of it as cortical and non-cortical, or cortical and subcortical. In the brain, all policies are driven by neurons, even reflexes. So, it's either cortex or non-cortex, but that's neuroscience. We should think about the language, since we'll be using it for years. Model-free and model-based are more familiar terms. As long as we acknowledge that model-free can also exist within the cortex, I think that's fine. Is that true? I think so.

What would be a policy? For example, riding a bike—much of that is not cortex; it exists within the cortex, doesn't it? No, I think it's mostly basal ganglia and spinal cord. 

This is why learning how to ride a bike is something someone can't tell you how to do. Most people don't even know how they balance a bike; it's intuitively weird. I would argue that's an example of something definitely non-cortical—it's basal ganglia or cerebellum.

I'll push back on that. You might be right, Niels, but I would challenge that. The way I view the cortex is that it doesn't really have any non-model-based policies. All the other stuff—walking, riding bikes, balancing, reflexes—these are things the body and other parts of the brain do. If I'm thinking of anything, it's just supportive functions. It's probably not really relevant. It's a clean break if we think about the learning models as pure model-based systems, and they don't have anything else to do with the world. All this other stuff, like riding a bike—it's strange: to turn left, you first have to turn right, and people don't even know this. It's just stuff that happens outside, and the cortex doesn't really understand it. We should try to keep the learning modules as pure model-based, and then there's a lot of other stuff, which you do a good job pointing out—utility things, reflex things, and other things that happen—but I would think all those are non-cortical. That's how I would think about it. I like making that distinction, because it will make it easier later. As a rule of thumb, if you have a policy that doesn't require a model, try moving it outside of the learning module. Almost all those policies will be either modality-specific or environment-specific, not general purpose.

I'm not super familiar with the terms model-free and model-based. If there's a problem with them, they sound good to me, but I'm not advocating for them. That basically does capture what I'm trying to say. I would only change it if we have a concrete example where we need a model-free policy inside a learning module or cortical column. In the brain, the one potential exception I can think of—and I've mentioned this many times—is the extra layers in V1. We don't know what they're doing.

I've always speculated that they're trying to deal with determining depth from two eyes that are at a distance, but that's the only place in the cortex where I can think of some extra machinery for vision. Everywhere else, it looks the same. It seems there wouldn't be any non-model policies in the cortex.

Wouldn't "model" conflict with deep learning models, like the actual weights of something? In terms of language, is it confusing? Maybe calling it a learning module policy would help. We use the word "models" in Monty—learning models, learned models—that's inherent to our system. In some ways, "model-based" is good because it's an established term. When people use it in reinforcement learning, they generally mean something similar to what we mean: you have some explicit model of the world, for example, simulating the results of Go moves on a modeled board, not just an implicit function. The way it's used in reinforcement learning is pretty much what we mean here as well. Every learning system has models—everyone. They're just different types of models. We have a sensorimotor model with locations and orientations, whereas deep learning doesn't have those attributes; it has different types of models.

That's how I've been using it generally. It would be correct to say "model." We just have to make sure people understand that our models are different.

It's not your model. This is only the third or maybe the fourth time we've changed these names, but each time it's gotten more accurate and better. Maybe it's finally clear.

If we spend some time now clarifying it, the benefit will be great. Probably tens of thousands of people will be trying to learn this stuff. I was thinking about how to unify the distinction we have for surface and descent, and also these different policies like utility space, reflexes, and learning module. This was just going off from the utility space, where we use the viewfinder to orient ourselves toward the object. Maybe we shouldn't call that a policy, but maybe we can even apply it to flesh out this idea. We have an agent at different states: searching mode, navigating mode, or exploring mode. Exploring would really depend on the kind of sensors or actuators you have. The current way we use the viewfinder would be when the agent is in searching mode, planning a path to get to the object. Maybe there's some policy—maybe this is indeed a policy—it's just a policy at searching mode. Maybe we can think of agents having finite states, like finite state machines.

Searching is a good way of describing it. We could have searching policies that fall under model-free, where the agent is just using the information coming in, like sensor patches, to look for something. There's still a risk, or we should ultimately separate those out and make sure that if we are using privileged information, we don't want the viewfinder anywhere in Monty in the long term. We don't want to use this privileged information during an experiment. We could say we have a sensor in the periphery that gets a larger receptive field at low resolution, and we might have that connected to learning as well. But right now, they are getting information that isn't given to them during the experiment, and we don't want them to get it during the experiment. That's the distinction in my mind.

We do have these kinds of states, like the matching phase and the exploration phase. In the future, we want the agent to actively switch between different policies. The learning module could say, "I recognize the object, now I want to use my exploration policy and learn more about the object." That could be model-based—"my model doesn't know much about this part of the object, so let's move over here"—or model-free—"I just want to scan over the whole object with a spiral," for example. There's a lot of literature about this, both in biology and machine learning, but in biology too. We have these overall modes: you only want to explore when you're comfortable, in a safe environment, not at risk. There are emotional states we can go through, like being willing to explore, which is inherently dangerous because you might encounter something bad.

The idea is that the whole system could be in an exploration phase, trying to infer or deal with something unexpected. That would be reasonable, because biology seems to do that.

In a deployed system, you might have systems that do no exploration. They're just trained, and now you want them all to work the same and not go off and try something on their own.

If you have multiple sensors and actuators, does the whole system have to be in explorer mode or Hungry mode? I'm a perpetual fiddler, so I'm sitting here fiddling and listening. I can say I'm exploring this object, and I don't know what listening is—that's interesting. I've asked myself about these behaviors, like fiddling, where people flip a pen in their hand. I will put the top of a pen off and on. I wonder, is that really cortex? Often you're not even aware that you're doing it; it's just happening on the side—tapping your foot, for example. Even scratching an itch is usually not the cortex, right? I've always thought fiddling, scratching an itch, or tapping your foot is some subcortical thing. The cortex is not involved in that at all. The cortex can observe it and say, "Oh, look, I'm tapping my foot or scratching my back," but the policy to implement the scratch is probably not cortical. Maybe in general, you can have one subcortical and one cortical process going at a time. You can be thinking about something while brushing your teeth, but it's hard to do both deeply. You can switch between them in short windows. My fiddling is constructive—I build little things, so I think I'm giving it some attention.

Here's another example: you're driving a car, and while driving, you're talking to someone. That conversation can be very exploratory; you might be trying to understand what they're talking about, visualizing things, or probing them.

Driving the car is clearly, for the most part, cortical—model-based, controlled, and goal-oriented—while another part of the cortex is exploring. So you could have both happening.

When you're driving, your eyes are saccading all the time, recognizing objects, their locations, and movements. You're steering the car, and you can still listen to a podcast and have abstract thoughts about a completely different scene in your head. Unless something visually unexpected happens, or something changes in the car's environment, then you immediately stop thinking about the podcast and focus entirely on the car situation.

One way I've thought about this is that we have many levels of hierarchy in the cortex, and things that are really practiced can be learned at lower levels. They don't require the upper levels of the cortex to pay attention. When you're driving, you don't really form episodic memories of the cars next to you at different moments; it all just gets washed away. A lot can be happening at these lower levels. Imagine a big hierarchy: some lower part is driving the car, while the part thinking about the podcast goes all the way up to the hippocampus, where you're forming episodic memories. That's where your mental focus is, while the other stuff is on autopilot until something unexpected happens, and then it jumps to the top of the stack to be dealt with. These are just personal observations that might have some value.

In terms of Monty, to wrap up, we've implemented it so there's always just one agent, which might have multiple sensors attached. In the future, we want to have multiple agents that move independently, like multiple fingers or two hands moving. Each agent could have its own policy and might change its policy at different times. We want that kind of modular flexibility. We can start simple, but ultimately, we want agents to be like humans—able to do things and think about other things at the same time. We'll get there. For now, it can be confusing because we just have to ask, can this finger recognize this coffee cup while thinking about podcasts at the same time?

Talking through the model-free policies we have, the simplest one is for the distant agent, where it's essentially just making small saccades as a random walk. This is entirely internally driven in a model-free way. The only sensory information that can influence this is if the system goes off the object; it then reverses the previous section.

Moving on, the object is determined from the depth image. We have heuristics for a sudden change in the distribution in depth images. This is shown again in a different view, where you see the mesh and the points that are sensed on the object. It almost looks like a random walk. Is it a random walk? It is. Is there a reason for that, or do we just want to make sure we cover it? Why wouldn't it just follow the line for a while? This is the simplest policy. It builds up from here. What I'm trying to show is that this is very inefficient. You can add some momentum to the random walk to make it go more in one direction for a while, but it's still a random walk.

Then there's the spiral scans policy, which is designed entirely for learning. This ensures we sample points on an object densely. It literally does a square spiral, going outwards across the surface of objects. Again, this is the distant agent. The base surface policy is one that touches the object and then moves along it, always remaining oriented to the point normal. I'll show what that means in a moment—how the finger follows the curvature of the object, and as it gets to the lip of the object, it curves around and follows the top.

That's really nice. Did you make those videos new for this presentation? I don't think I've ever seen them. This is an old one, but I haven't shown it in a long time. It was way back when I was debugging some issue with the surface policy.

So, just trying to understand this a bit better. Imagine you have this surface of the object in blue, zoomed in, and then you have this finger touching it. We start out oriented with this point normal. It's a recursive or iterative algorithm, but just for the purpose of this, we start there, and then to follow the surface, you basically take a tangential step perpendicular to this point normal, then move forward until you're touching the object and then orient to be parallel with the point normal. Then, repeat. Of course, I've shown these steps very large to make it easy to visualize. You can imagine if you make these steps quite small, then that's essentially what your finger is doing as it follows the surface. I wouldn't actually remove my finger from the surface, but I think I would sense that if the pressure is reduced, I have to move in a certain direction to keep a constant pressure, which is essentially the integration of what you're showing here. Exactly. As the distance becomes extremely small, only every fourth observation is sent to the learning module, so only the ones after the reorientation, right? With the whole subcortical process, there are a few filters before it gets to the cortex, but one of them is that a lot of this is just the finger orienting itself, and that's not information that needs to be sent. The idea is that the finger is following the surface and automatically adjusting its depth to stay on the surface. I can imagine that's all subcortical. The cortex could say, "Move in this direction," but what actually happens in the movements would be reported back to the cortex, as opposed to the cortex saying, "I'm following a curve." It would be more like, "I've moved in this direction and it looks like it's curved," as opposed to knowing what's going on. This micro adjustment that's happening here is almost certainly not cortex. In our system, it's not; it is a model-free, sub-LM system. 

The surface point normal is estimated by the sensor? Yes. The sensor module gets the depth dimension and estimates the point normals.

Thank you. Back to your question earlier, Viviane, about only sending every fourth point or something like that. That's not today's talk, but that is a major topic for us. In the cortex, it feels like the cortex can learn all those points and it doesn't really take any more memory, whereas where we're doing it now with discrete points, it does take more memory. That's a sort of elephant in the room I'll have to deal with sometime. We have another thing, which I'm not talking about because it's not really policy, but it's like the feature change sensor module, and that's more of a buffer that determines how far you have to go before. It's trying to be efficient, both in physical space and in perceptual space. I think cortex does it miraculously; you could train on different points. It's potentially a looming problem. I don't know what we'll do; maybe you've already dealt with it. Sorry, keep quiet. As it stands, this will follow the surface of the object, but it's still a random walk in terms of the directions that we are moving tangentially. Either it's an entirely random walk, or you add some momentum factor, and then you get these kinds of curves. At this point, there's nothing to follow the significant curves of an object itself, which is what this follow-up policy implemented, called the curvature-informed surface policy. This was designed to follow the minimal and maximal principal curvatures on objects, which is what you see here in yellow, where it's first following the maximal, then the minimal, then still the minimal curvature, or you can imagine the same if it was moving along the handle. This was an attempt to get a policy that would intuitively capture the idea that our fingers will tend to move over these kinds of curves when sensing an object, rather than just moving randomly all over the surface.

As a reminder, the principal curvature at any point on a 2D surface are the orthogonal points of maximal and minimal curvature, either flat or negative, and this provides us with two directions we could go: one with a minimum amount of change and one with a maximum amount of change.

It also just occurred to me that this could be related to how we learn the orientation of the object. In some sense, those two things—maximum curvature and maximum flatness—could almost always align with how we orient the object overall. I don't learn the cup at a 30-degree angle; I tend to learn it where the vertical surfaces are flat and going straight up and down, and the curvy ones are, well, it's somehow related to the orientation. In general, it's almost like the principal axes of the object. That's the way to put it. I think Biedermann had these theories around how we represent those, but that's how you can derive them from sensory information.

This is just showing what this can generally give you: you follow the side of a cup and then go around the rim when you reach that. This is showing the finger as a kind of stick moving along it, and as you see, we're still orienting to be perpendicular to the point normal, which is more noisy or rapidly changing in somewhere like the rim of a cup, so that's why it's going around like that.

and then, going into more detail, this policy tries to alternate between following the minimum curvature and the maximum curvature, and sometimes not following either. If we stick to one forever, we risk looping endlessly on the object. Similarly, if these directions are undefined, this is a bit of a hack—it's fully model-free, fully input-driven, and was a way to prevent endless repetition. The natural way to resolve this would be to keep the same principle, but add a model-based knowledge: if I'm returning to a point I've already visited, I should explore in another direction, which would be a top-down signal. That sounds right.

This was to get it working in practice. For example, there are many places where, to the human eye, it looks like the principal curvature direction should be defined—there's minimal curvature around the edge or side of the bowl, and maximum curvature here. But due to noise in the point clouds and how it's estimated, it's often not actually present to our sensors. Still, you see here it takes a kind of random path with some momentum, gets to a small edge on the bowl, follows that for a while, then goes in another direction for a while.

In terms of future Monty work or plans, this is a great example of where we would ideally want these to be three separate policies, and have the learning module be able to switch between them. Right now, we just have hard-coded random switches or triggers. I was thinking, for example, I would probably follow the curvature all the way around, and then, once I'm back to where I started, go in a different direction—changing direction based on the model I've just learned, or may already know.

If I'm learning, and I have previously learned this and am just doing inference, I might follow the curvature until I have a hypothesis to test. Now the best way is to head down the side to find out, but during learning, I might want to follow the entire rim of the coffee cup if I've never learned this object before. Then, once I'm back to where I started, I go in another direction. What you've done is great. I was also thinking about developing some kind of internal model of the general bounding space of an object. If I have an object like this, I might imagine a sphere enclosing it, and try to go around the object to capture all the angles. In this way, I can develop a sense of the overall boundary of the object. That's what we're doing, isn't it? That's what the model is, mostly, right, Scott? It's defining the morphology of the object, and that's the goal here. I'm suggesting not actually following the curvature, but following, for example, if I had a sphere projecting a line—basically, trying to find the exterior boundaries of the object. This would do that in two sweeps. With most objects, at least manmade ones, there are two directions. For example, with the tin you were holding, if you found yourself on the edge, you'd go around that way—that would be minimal curvature on that side. Then, once you're back where you started, the model could say, "Switch to maximum curvature," and you'd keep going, eventually doing another circumference. That's what you mean. I'm trying to picture why I might or might not need curvatures to do that. I imagine I could do this without following curvature.

You could just do a random walk with momentum. You could sample anywhere, and you'd end up learning the same morphological model either way. We're just trying to do it efficiently. I think this was inspired by our own observation that when we touch an object and try to infer it, we usually follow an edge or the rim of a cup, which are usually the tasks of minimum or maximum curvature. We want to do this intelligently, because a random walk is very inefficient. In general, imagine you're suddenly on the handle of a mug—if you move in a random direction, you could end up spiraling around the handle endlessly. It makes more sense to follow the minimal curvature, so you immediately get the general shape of the handle. In general, for many objects, if it's an ellipsoid or a cylinder, following the minimal curvature will quickly get you from one pole to the next, giving you the overall shape of the object.

I don't think Scott was suggesting a random policy. From how I understood it, it sounds more like a spiral scan policy for the touch agent, scanning around the object in a sphere. That could be an efficient way to cover the entire object surface, but it doesn't seem very natural for what we would be doing. It's like walking into a 3D scanner, moving the scanning element around the sphere and measuring everything. We could have all kinds of weird sensors that do things like that, and they would all fit within Monty, but if we're trying to build embodied agents that actually move in the world, they're going to be constrained by physical movement issues. Some limitations of sensorimotor learning allow us to do that sort of 3D scanning, but others require action policies where sensors move along surfaces.

If I pick up a brand new object, one of the first things I want to find out is its spatial extents. Once I have some idea of the overall spatial extents, I have more information about where to explore next to fill in the particulars. Let's break that down. The most extreme case, which is what we're doing here, is when you haven't picked up the object—you're touching it with one finger. You have to move everywhere to learn what the object is, and you can't get the whole picture at once. If I pick up an object and have hundreds of sensor patches on my hand touching it simultaneously, and they know their relative positions to each other—something the brain does, though we don't know how yet—even a single grasp tells you a huge amount about the overall extent of the object. If we had five learning modules touching all around a coffee mug, after just one sensation, they would vote with each other and immediately narrow down the hypotheses to objects of that size and shape. You'd immediately have only a few hypotheses and complete models of them. You could use those to inform the next movements; it wouldn't have to be a separate process to estimate a bounding box—it would just automatically come out of having these modules. Imagine grabbing a cup or any object with both hands; you have a really good idea of the object's boundary at that point. There could be something sticking out between two fingers that you didn't touch, but the challenge is that we have to develop Monty with this single learning module focus, which is primitive and seems wasteful. We have to come up with all these things Niels is talking about, but that's not the ultimate system. The ultimate system will have different learning modules working together, but we have to start here because this is the foundation.

This is not where we want to be; it's just where we're starting.

When you look at something with your retina, you're sensing different parts of the object instantly—different parts of the retina detect the edges, and you immediately sense its boundaries with one glance. If you're looking through a straw, you have to move it around, just like here. The same thing applies. Another potentially useful aspect of this heuristic is in terms of sub-objects or sub-features. In general, you're more likely to efficiently find those. For example, something people often sense to distinguish a mug from a can is the rim of an object. If you're moving along and encounter the rim, you'll follow it for a period and get a sense of it. If you're just doing a general sweep across the object, you bump into each feature but don't really get a chance to explore each one as you go. What you're bumping up against here is that this is also a single learning module—there's no hierarchy, so there's no object composition at this point. It's just a bunch of features at locations. In the coffee shop, it's always been a difficult question: what are the components? Is it just a single-level object, or is there a compositional component? Clearly, at some point, handles and rims become compositional components, as do vessels and cup cylinders. But we're not dealing with that right now. We're just dealing with a morphological shape that has no subcomponents yet, trying to learn its morphology and what features exist at different points. 

If a cup wasn't a single object but compositional, then you'd have multiple surfaces and could have a model for each surface or distinct cluster of things, like a rim as its own model. It's not clear. Could you drive exploration policies based on predictions on those submodules? If I see a piece of a rim, I could instantly predict what I'll see around it. Let's jump to something really clear: if I draw a picture of a face with two eyes, a nose, and a mouth, each is a clearly identifiable sub-object. If I see an eye and a nose, I should predict where the other eye and the mouth should be, and I can go check for them. Those are really clear. The coffee cup is unclear in terms of subcomponents, and it's always been a problematic example for us, so at the moment we're avoiding it in Niels' presentation. Ultimately, most things in the world—almost everything we do when learning new things—involve recognizing objects we already know and doing compositional structure. That's 99 percent of what we do, day to day, moment to moment. This is more like a child just starting to touch the world and learn shapes. So we're avoiding that issue with the coffee cup right now. It's too hard to think about.

Is it too big a thing to avoid? No, it seems pretty important. We're not avoiding it. In fact, the paper I'm writing with Niels and Viviane is all about compositional structure and how it works, and I think it's extremely important. On the other hand, a single learning module, or a group of learning modules all at the same level in a single region, cannot learn compositional structure, and we have to do both. We're not avoiding it in the sense of hiding it. It's more about getting started—let's get these policies on a single learning module, because we have to understand that. Then we can talk about how we do the face and other truly compositional structures. In my opinion, what will happen is we'll figure out, ultimately, what we do in the in-between spaces—is a rim a separate object, or is a handle a separate object? Once we get time, Viviane and I had some discussions at the last By the Bay about how, at least in the current implementation of Monty, we could start adding in learning of sub-objects like this. That should probably work pretty well.

I can't remember if we ever discussed that as a broader group. My intuition is if you started at the lower level, like cylinders and rims, then your problem is simpler—predicting a flat surface and having a policy that can explore a flat surface. Or is it easier to start from the bottom up? My intuition is almost the opposite. What we're doing right here is really hard, and that's where I'm having a lot of data problems. Maybe it depends on what you mean by simpler. It's more efficient if you have hierarchy, like how voting makes inference more efficient, but I guess Jeff's point is he just wants to get it working first, independent of subcomponents. First, we have to learn the subcomponents and then put those together. So it's about getting the system working before there are any subcomponents. If it's a handle system, like the communication, or if the handle is the lowest object, then it's a handle. But how you get that system that understands the handle working—that's the question. Mugs don't form other objects easily, but if you started with planes and circles, they form other simple objects. I could argue that mugs do form; if you look at a table setting, you have a plate, a mug, a fork, a knife—that's a compositional structure where a mug is one of the components. In my goals, it's a fair criticism to say it's easy—once we can say how we're going to learn the morphology of an object that's not compositional, that's what we're doing right here—then you can say it's easy to understand how we learn the composition of an object where the components are all pre-learned objects. The hard part is the in-between ground, like with this mug—is a handle a separate object or not? It's clear to me that when you start out in life, you don't know that handles are separate objects. If a child is first presented with a mug with a handle, it wouldn't say the handle is a separate object; it would learn it all as one thing, and then later realize that handles appear on other objects and separate them out. We don't really understand that yet, but there's this interplay between the levels and the hierarchy. I'll let that happen.

This is the approach we've taken: break it into two parts. Start with a single learning module, then add multiple learning modules at the same level, and then do compositional structure at different levels. We're definitely planning to look into hierarchy and modeling compositional objects in the next months, and definitely a lot more. That seems much more exciting. But if you jump to that now, you're just going to get it wrong, in my opinion. We have to understand the basics first.

One thing to add, because part of the intent of this presentation is to inform people who are using these policies: I wanted to make sure I covered this point, even though I hope to change it in the future. With the curvature-informed policy, it also has an evasive maneuver step where it keeps a trace of where in space it's been recently. If it feels like it's going back there, it takes a step to bring it away from that area. This would be much better implemented as a model-based policy, where the learning module understands it's starting to return to where it was before and guides itself away, rather than a subcortical structure keeping track of all that information. But this was a hack to get it working well in the current setup.

That's shown here in green. The system goes down, follows along, and at this point starts pulling the maximal curvature. In black, it's hard to see, but it's going up, then it's about to return to somewhere it's already been, and it can tell that from a quick check. So it turns around.

In other applications, the opposite policy might be useful. If we have very noisy path integration, we might want to revisit the same feature again to adjust our path integration. That's a great observation. That's the only way you can really ground yourself in the right location—by going back someplace you've been or observing something you've seen already.

It's also like any heuristic—there will be some object that causes it to fail completely. You can design a menacing object that traps you in a pattern of your heuristic. The learning part is needed; the model-based part helps you avoid that. When we say model-based, there's another topic we haven't explored much: the idea that the hippocampal complex is learning models, but it's more of an episodic memory. If I were literally touching something for the first time, exploring it, thinking about it, and concentrating, I imagine the hippocampal complex is keeping a trace of where you've been. If you're wandering in an environment, it keeps a trace of your path, whereas the models formed in V1 or S1 don't do that. There's some complexity here. You could say episodic memory is a temporary model—remembering the actual path you just took, but it's not really part of the model you ultimately want to learn. The model learned in the episodic memory of the hippocampus is a trace-based model: exactly how did I get here, where did I go, how did I turn? The model we want to learn in V1 or S1 is not like that at all. We don't want any memory of exactly how we learned the object or how we traced over it. That's another complexity happening in the brain. We could think of this as evasive, avoiding revisits. You could say there's a part of the cortex or brain doing this. I was thinking of it more as short-term memory.

That's a lot of prefrontal cortex. It could be prefrontal cortex. Whether it's prefrontal cortex or hippocampal, my point is it's not V1. It might be hippocampus or prefrontal cortex or something else stabilizing some representations in V1. Maybe. I could be wrong. I view it more like episodic memories. If I was, for the first time, touching an object I've never touched before and trying to learn what it is, I would have the ability to mentally recall, "Oh, I was already over here and need to go someplace else." I'd have that, and therefore some sort of short-term memory trace of where I've been. Whether that's prefrontal cortex or entorhinal cortex, I don't know, but I don't think that's what's going on in the module. I don't think there's any evidence of short-term memory in these things. Maybe there is, I don't know. This is in the gray area. I think it's fine to have a policy like this, but I don't think this is what all learning modules do. It may be implemented in a different type of learning module or a different part of the brain.

In terms of how to improve this, some simple things: as mentioned, I prefer to have this more as model-based, because at the moment I'm storing locations and related information in the code, associated with what's essentially subcortical motor systems.

As we discussed before, model-based switching of the curvature-guided policy is another area. If you look at the algorithm for this policy, it's really complicated, and I don't think it needs to be. This was how we tried to hack it together to avoid a lot of adversarial conditions. But to your point, Michael, there will always be adversarial conditions that aren't satisfied. Having more principled, model-based decision making would help with most of that.

A general input-driven improvement we've discussed implementing is saccading to salient features. This relates to the search policy mentioned earlier, where the distant agent could use learning modules to perceive the periphery at low resolution and, based on salient features, orient toward something interesting. This has been on our wish list to implement for a while.

The curvature-following policies are for the A surface agent, while the distant agent currently uses a random walk. With our eyes, we typically saccade from point to point rather than follow a smooth curvature, so it would be beneficial to find efficient policies for the distant agent as well. At their current scale, the random walk resembles micro saccades, with the eye drifting across an object, which is not very efficient.

If you were trying to learn something unfamiliar, like Chinese characters, they might all look similar if you don't read them. To recognize one, you would use small eye movements, focusing on individual bars or strokes. You would attend to each stroke separately, as that's the smallest recognizable object. As things become less familiar, you get closer to following edges, jumping to the lowest recognizable element, such as a line segment at a specific orientation and location.

Only after learning to recognize the entire character with multiple retinal points can you saccade from character to character. In vision, it's usually jumping from location to location rather than a smooth walk. Smooth pursuit is only possible when an object moves across the visual field; it's automatic and not under mental control, likely governed by the superior colliculus rather than the cortex.

The minimal recognizable element is always some edge or line section; without that, learning is impossible. Using the Chinese character as an example, it's composed of little edges, and you jump between those because that's what you recognize. You're building a compositional object from small edges and lines, making smaller and smaller movements until you recognize something.

For example, looking at windows across the street, once you recognize them, you can glance at any one and know it's a window. If you've never seen a window before, you'd focus on the vertical and horizontal lines and the blue panes. Once you know what a window is, you can just look and recognize it, but if you don't, you'll analyze it in more detail.

Is this a good time to take a break? We're already over time a bit. Maybe we'll stop here.