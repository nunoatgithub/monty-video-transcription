Alright, let me go through this. This idea is simple, though I haven't worked it all the way through yet, so there might be some errors.

The details are complicated, and I don't want to go through all of them, but I'll do my best. I realized this could be an issue with the scale problem: when we observe an object at a different scale than when we learned it, we seem to handle that very rapidly. It doesn't take any time; it just happens immediately and isn't effortful. This is a potential solution to that problem. It involves grid cells and how they work in the brain, so it's a very neuroscience-based solution.

Before I go through the neuroscience, I'll describe the basic idea. In our basic models, we have some location in a reference frame, and we're pairing it with some observed feature, which could be a curvature or something else. When we're correctly on an object and update our locations, it points to the correct new feature, and a feature can then narrow down the locations.

The idea here is that as you're moving your sensor or your body, and you're updating your location, it sweeps through a series of locations over and over again in a linear line. If I'm moving in some direction, or my finger is moving in some direction, it'll sweep through a series of locations along that line of movement about ten times a second. You would be able to match the observed feature early or late, and it would know that. It would recognize it was in the wrong location—either a little too far away or too close—and then it could adjust its scale. In the brain, this occurs on a theta cycle, which is about 4 to 10 hertz. Let's say 10 hertz, so ten times a second. The location signal will sweep through a series of locations along the line of movement, so you could match an input earlier or later and know that your scale needs to be adjusted.

That's a high-level description, and I can explain how it works if there are questions. Are you assuming the animal is moving along that arrow? I wasn't talking about the picture yet; I was just describing the high-level view. We think about moving our finger or the animal—it doesn't matter—and we're updating the grid cells, the location, as we move. Path integration is happening, so as we move, it updates, but what actually happens is it sweeps through a series of locations along that line, closer and further away, and you could match the feature early or late.

If you match it early, then you know the scale is smaller than you observed. If you match it late, the scale is larger than you observed.

That was just the overall top, and then I could describe what's going on here in this picture. Is that okay, or do you want to ask me a question? I do have one question: is that the only way you can get phase precession, from a difference in scale? That's the only way people have proposed, that I'm aware of. No, there are other ways. Not difference in scale. I haven't talked about phase precession yet. I'll get to that in a second, Kevin. I was just describing a very high-level view. The actual details of how the neurons do this can be complicated. I've struggled with understanding all the different theories about it, and I'm assuming not everyone here knows all that. I'm just trying to give a high-level view.

What is the solution? It ends up basically saying we're going to sweep the location along this over and over again, and we can match earlier or late. Let me go through some of the details here. In the upper left-hand corner, you see a typical image from a paper about grid cells. Ignore the arrow for the moment. You can see these little things—I'll call them fried eggs. These are areas where a grid cell might respond in an environment. This could be an animal moving in an environment, but it could also be our finger moving over an object. It's the same thing. Imagine the animal moving in an environment, and as it goes along, this represents the response of an individual cell. One cell would respond in each of these highlighted circle areas. They're quite large, not very specific, and they're color-coded by the frequency of firing in this case. They fire more actively in the center of the little fried eggs as opposed to the periphery.

There are two things going on here. There are a lot of theories about this, but all the theories about how grid cells come about are basically based on the same idea, and that's in the drawing below it. The drawing below shows two sine waves. The first one is labeled LFP, which stands for local field potential. This is a theta hertz frequency, between 4 and 10 hertz. This is the frequency you observe, and many parts of the brain have different interactions. It's a frequency that's shared for different modalities, for example. This would be shared in the hippocampus or the entorhinal cortex. That's why it's called a field potential—it's a field that all the cells are being subjected to.

The idea people hypothesize is: how do grid cells do this path integration, which they do in the dark, so the path integration isn't relying on any sensory input? The basic idea is that there is a second frequency. Where that frequency originates is a little controversial—some say it's in the dendrite, some say it's in the second cell, but it doesn't really matter. I'll label that one—the red one—as "cell," and that frequency is theta plus a small delta. It's a little bit faster than theta, but only if the cell is active when the animal is moving in a particular direction. This frequency says, if you're moving in a particular direction, if you're moving slowly, I'll increase the theta a little bit; if you're moving faster, I'll increase the theta even more. Now the cell is getting two inputs: the LFP theta, which is constant, and this slightly faster frequency, which is related to the movement speed along a particular direction.

As you see, these two sine waves, or frequencies, go in and out of phase, and that's the black line—the third line down. The black line shows the sum of those two frequencies. As an animal moves along the gray arrow above, going through multiple points where the cell becomes active, the cell is just sitting there, getting these two frequencies, and it slowly goes in and out of phase as the animal moves. The red sine wave indicates the animal is moving in some direction at some speed. If the animal is moving faster, these two cycles go in and out of phase quicker, as if the animal is moving quicker across the environment. If the animal is slower, they go in and out of phase slower. This is the general belief about how grid cells become active based on movement and not on sensory input—how path identification occurs, with these frequencies going in and out of phase.

There are two things going on here. One is, at first people say there's a maximum firing rate of the cell, and that occurs at the peak of the black line, when the two cycles are in phase. In reality, that's not a very reliable indicator of where the cell is. They make it look like there are more tick marks in the center and it gets sparser on the side, but in reality, it doesn't look that clean. The second thing, which is very interesting, is phase precession. When the cell spikes—when it emits the spike—it's at different points in the phase of the theta frequency. You can see the little dotted lines coming down from the troughs of the blue sine wave at the top, and you can see where those line up with the black line at the bottom. Where they line up changes over time. Where the cells actually spike moves from one part of the theta phase to another. If you look at when the spikes occur, you can tell where the animal is relative to that grid cell's ideal response pattern. As the animal moves towards a point, the grid cell fires early first, then right on the trough of the LFP—the theta cycle—and then it moves beyond that. There's a shifting of when the spike actually occurs as the animal moves towards the center of its field and then beyond it. This phase precession is seen in many cells in the brain—grid cells, place cells, and others. This seems to be a very common phenomenon. Most people say that's a good way for the brain to be more precise about where you are.So the grid cell responds over those large, egg-like blobs, but if you look at the phase of one, it's much more accurate—you can state exactly where the thing is. 

Any questions about that before I go on? Kevin, do you have a question?

No. I can look at the fried egg, where the center red piece corresponds to the middle—where my arrow is on the lower deck, right here. This entire thing you're showing corresponds to one fried egg, not the entire trajectory. What you're saying is, if you look here, this firing is near the beginning of the phase, and when it leaves the fried egg on the other end, it's on the right-hand side.

Just in case that wasn't clear. One question for me: in the middle, it shows multiple spikes, but my understanding is within a single cycle, you're not going to see multiple spikes. If you see one spike, it's just across multiple cycles you'll see more spikes on average. I didn't know that. I had the impression there would be several spikes at those peaks. There would be several spikes, but my question is, during a single cycle, are you actually going to get multiple ones? I think so, but then the phase wouldn't be very precise, because it takes 5–10 milliseconds to do a spike.

If we're at 10 hertz, that's the maximum, then you've got 100 milliseconds. If you're going to do multiple spikes, that's like a 30 percent error. I think it's two or three spikes. They make it look in that diagram like there are clearly more spikes in the middle, but in reality, it's not like that. I think it's noisy. There are a few spikes. If you look at the drawing on the right, they're showing the same phenomenon, but I don't know. I didn't show a real diagram, like a real recording from a cell, but if you look at a real recording, it's not so clean—it's messy. But the point is that the phase procession seems to be pretty accurate, even though the spike count is not. The idea is that the accuracy comes from the phase procession. No one has any speculation, as far as I know, about how the brain would use that—how it would take advantage of being early or late on the phase to know something more precise. I'm going to propose something in a second.

As the animal moves along that gray arrow in the upper left diagram, the phase keeps shifting back and forth as it gets to its preferred locations. I'll just go—oops, I lost my image trying to scroll. Let's see if I can get it back.

I'm doing this on my phone, it's not so easy.

I lost the image. You might be able to wipe the whole screen. Oh, yeah, there we go. On the right, it just occurred to me, there's something else happening. On the right, I show our classic model, which is a location layer and a feature layer. This is from columns and columns plus. We're essentially saying the model consists of a series of locations and the features associated with them. If your finger, eye, or body was moving through space, you would see a continuous series of locations and a continuous series of features as you move your finger along the edge of an object. This goes back to my argument that you could do this type of model in a continuous fashion.

What occurs to me is that as you're moving through space, on each data cycle, you'd be cycling through a series of locations—further away from where you actually are and closer in—because the phase procession thing, remember, we just looked at one cell before, but all these cells are doing this. There will be cells that represent all these locations, so at any point in time, you'd cycle through a series of locations: the ones that are spot on, where you're aligned with the peak of the phase, and ones where you're shifted early and shifted late. As you're moving along, on every theta cycle, you'd be cycling through a series of points along the line of movement.

In some sense, now you've got this feature up in the upper layer and the feature's in there. We're trying to predict this feature. The feature's either there or it's coming in. We're trying to match it up with the location. If the feature is not in the right location but it's within that sweep, it'll be found. It'll say, "Oh yeah, I found it. There it is." I matched the feature I'm expecting at a location, but my location was either shifted to the left or shifted earlier or later than I expected. Then you would say, "Yeah, I found it. I found what I was expecting, but it wasn't in the right place. It was shifted." That's the information you need to handle scale.

The way scale is handled in situations like this, at least it's speculated to be—and I think this is right—is the brain just changes the base theta frequency. If you increase or decrease it, everything scales up or down. All the cells based on this methodology will scale up and down, so you could scale up and down movement, path integration, motor behaviors, and so on. Now, you're expecting to find this feature at some peak activity on some grid cells, but you found it on an early or late phase activity of some grid cells. I didn't try to work it out, but that's all the information you need to adjust the theta cycle to match what you actually found. If I found that I was early on the cycle, I would have to speed up the theta cycle a little bit, and now I'd be matching it. The end goal is to adjust theta, which essentially adjusts the scale, and by noticing that I was slightly off, it gives you the information to move the theta to match what you actually observed. That's the entire idea.

I can think of some potential problems with it, but it felt like this is going to happen whether we want it to or not. This didn't require any speculation on my part; it's just what grid cells would do. That's why I said I stumbled upon it, because I wasn't trying to solve this particular problem.

Again, I don't think we have to do it this way necessarily. The idea is that you sweep through a series of locations along your line of movement, see which one matches better, and when you find the one that matches better, then you have the knowledge on how to scale the system.

Does it matter how fast you're moving? The faster you move, the greater the frequency of the red oscillation.

It does, because if I was a cell, imagine I'm a cell, I'm just going in and out of phase, and every time I'm going in and out of phase here—well, not phase—I'm either, my red oscillation is matching the blue one or not. Every time it matches the blue one, I fire. If my red one is going faster, then I will cycle through these on and off patterns faster. If I'm moving faster, then I will go through my locations faster. So it does matter how fast you're moving. That has been shown to be the case too—the red oscillation is velocity dependent.

So, the LFP theta—is that global across any set of grid cells, or is it, are the adjustments to theta, which you're proposing as an accommodation mechanism, can you have more than one theta going on in the brain? Yeah, more than one theta in the brain for certain. I don't know how global it is. Clearly, what's going on in your visual system is different than what's going on in your tactile system, or maybe not. My understanding is that it's global to some region, but not the whole brain. That's my understanding about it. We could look that up. I haven't read about that in a long time, so I'm not certain, but that's what I distilled.

The other question I had when you mentioned cell 1, cell 2, cell 3: If I were to superimpose the Friday diagrams of cell 1, cell 2, cell 3 on top of each other, what would that look like? Are they just displaced a little bit, or how do those "receptive fields" look? In that diagram, there's a little highlighted area in the middle. I didn't make this diagram; I took it from someplace. That's basically saying, at some point in movement—so this one second—that means the animal is moving, and this is what happened over the second of movement. At that point, the animal is at some location, and you see there's a red cell, then a green cell, then a blue cell. This is the idea that you're cycling through three locations very quickly as the animal moves, and those are basically happening very quickly, separated by some distance in the phase of the theta oscillation. Those are like overlapping eggs, if you will. When we looked at the egg diagram, that's just one cell. There are cells representing every other point on that blue background, so there are all these overlapping eggs. In some sense, lots of them. There's no point in that space that's not represented by grid cells, and they're overlapping quite a bit.

Okay, so what I'm seeing is that by tapping multiple of these cells and assuming they're in some kind of communication with each other, you potentially have a Vernier mechanism that refines the phase accuracy. The more cells you can harness for that, the phase differences become more accurate.

Maybe I didn't follow that. Is that necessary?

Is it necessary? I think, in the sense of right now, the bound of accuracy is on how finely these cells' receptive fields are placed. But if you allow them to, in some sense, interfere with each other, you can probably get a finer resolution out of that. I think something like that occurs separately from this in the retina, where you actually get a finer level of detail than the actual sample frequency would suggest from where the retinal receptors are. I'm proposing it as a mechanism that there might be a finer level of control if it was necessary by harnessing multiple of these things. Yeah, that's possible. I don't know. I'm just looking at it pretty simplistically. Imagine that I'm looking at the red, green, and blue cells firing in succession in that highlighted area. We're just basically—what the thing is. I don't know if I need more resolution than that. But basically, what's being sent to the upper layer cells—the feature layer in the upper diagram—it's saying, okay, does this location match your feature? Here's a location; is that matching what feature we're having? Then, here's another location; does that match what feature we're having? Here's another location; does that match the feature we're having? I don't know if I need finer granularity than that, but maybe it would be helpful. I don't know. I wasn't relying on that. I just said, yeah, it should work like this. I'm thinking in the case where if you want to harness this thing for something like curvature, and you want to be able to detect multiple curvatures in parallel, each little group of cells could—why would you want to detect multiple curvatures? What do you mean by detect multiple curvatures in parallel? I don't know what that means.

Also, I think curvature would be detected differently. You wouldn't use detailed locations to detect curvature; you'd get the curvature directly from the skin. You detect curvature in the feature layer. I'm assigning the curvature to the location, right? Okay. Kevin, you're saying if you wanted to get a final resolution of curvature—now, I wasn't even thinking about that here. I'm just assuming you have a representation of curvature in the upper layer, and it says, "I know what location I'm supposed to be at." You can imagine how this would get synced up pretty easily. Essentially, if the red cell matched the feature, then I'd say, "Now, if I just try to synchronize the theta with that new peak—if I just move the peak of the theta cycle, the LFP one, to where the red cell actually fired—that might be sufficient to set the new theta." Basically, every moment you can say, "I should just try to sync up the theta when I have a match." This is an additional point; I'm not reacting to what you said, Kevin. I think that's very simple. I'm just trying to extrapolate on the two, to say how—if Subutai is saying that curvature is actually something else—I'm trying to think of the notion of scale invariance as applied to the problem of curvature. I wasn't doing that. This is just scale and balance in terms of the reference frame made out of grid cells.

If I can get it to scale, the challenge we were facing is that I have a model I learned at some scale, and now I'm observing an object at a different scale. The model doesn't fit anymore. If I move a certain amount, I'm not going to see the right feature, and therefore I won't recognize it. Within a certain amount of variation, this system would be able to quickly detect, "Yeah, it's right, but it's just off a bit, and now let's adjust it so it's correct."

One question I have: in order for this to work, cell one, cell two, and cell three all have to be representing the same location in the object's reference frame, right? It's just at a slightly different scale, right? No, that's not right—they're at slightly different spacing. Cell one, cell two, and cell three are overlapping eggs. They're not at different scales; they're just different locations. They're different locations in the object's reference frame.

If you're looking for a feature and the object is smaller or larger, I'm not quite sure how this would actually work. Would this actually work? What we're trying to do in our models is basically finding or doing a correct prediction, or being synced on a model, is matching a location and feature at the right locations. As you're moving, what would naturally happen is you're, in some sense, trying expanded and contracted spaces ten times a second. You say, "Does any expanding space, contracting space, expanding space, contracting space—does any of this stuff match?" You're not actually expanding and contracting space; you're just trying different locations along the line of movement.

In our model, the feature is associated with a particular location in the object's reference frame, and we don't know what the scaling is between the sensor movement and the change in the object's reference frame. We do know, based on the path integration methods, if the scale didn't change, then it's all set. But we're trying to determine what the scale is. If you move your finger one inch, the actual change in the object's reference frame could vary depending on the actual scale of the object.

In our model, the object-centric location is associated with features. The location represented by cell one would be associated with that feature. Cell two and cell three would not have an association with that feature, because they're different points in the object reference frame. We're going to find out which of these series of locations matches up with the feature, but only cell one can, regardless of the scale. Cell two and cell three represent different points in the object reference frame; they're never going to match the feature.

The feature is at a specific location in the object reference frame. We're not certain of our location. Cycling up through these locations means we're not really certain where we are, so let's see who matches up. Only one will match up to a particular feature. Only one of those cells will match up to a particular feature; the others will match up to different features.

But you didn't see how it would work, so I'm confused by that.

I'm trying to think through step by step. Let's say the red cell is associated with the tip of my pen, but the pen happens to be larger or longer. As I move my finger, it's expecting the tip, but it's not going to get the tip.

Let's say, instead of the red cell, the green cell is the tip of your finger. If we were spot on—meaning the model and what I'm inferring now are the same scale—then when the green cell gets to its tip, when it's at the center of its egg, it's going to match up to that feature.

Now, if the pen is longer or shorter, the green cell won't be the one in the middle anymore. It will be shifted to the left or right because I'm not at the tip—I'm either before or after the tip. The green cell would still match the tip, but it wouldn't be the center cell; it might be the red or blue cell. It would indicate the tip, but with an offset, a phase shift. We need a mechanism to determine that offset to adjust the scale. Actually, it happens automatically.

At this point, imagine you are the red cell. You might match your feature, but you are offset on the phase. We know a shift or scaling is required because the red was not at the peak; it was on the edge of its egg. What's the mechanism for that? I don't know, but I suggested one a moment ago.

You've got the LFP, the black line at the bottom. We're trying to match up the trough of the LFP, not the peak. The trough is the one here, because there are a few polars in there. Ideally, your cell would match up with the trough of the LFP, but now it doesn't. On the left, if you're the red cell, you're matching up closer to the peak. Now, imagine I say, at this point, we're going to force the theta cycle—the theta rhythm—to bring its trough in line with the cell firing. I don't know how I would do this, but the idea is simple: we should have been lined up, so you line up with me right now. I'll reset your cycle or frequency to match up with me. The two peaks aren't aligned. The cell is—yes, I get that. I was just wondering about the actual mechanism. I don't know what it is, but it would be the arrow from the feature layer down to the location layer; there would be information there somehow. One can imagine that, obviously, it's just speculation. The information is there, locally and sufficiently, to say, "Just line these two things up, try to get them in sync." How that happens, I don't know. In this case, in the entorhinal cortex, I think the theta rhythm is established between the— is it between the reticular formation and the cortex? I know in the cortex, but I don't know enough about it.

We could look into it. From an engineering point of view, it's not hard to imagine how to do this. From a biochemistry and biophysics point of view, it might be trickier.

Jim: Jeff, I have a couple of basic questions. I don't know if I'm the only one who's about two times behind you, but I'm struggling to keep up. Can I ask some— I warned you, the biology in this area is really complicated. I have no problem working through it over a long time, but I'm behind right now. So, cell 1, cell 2, cell 3—first question: are these responsive to slightly different locations, but similar locations?

Jeff: They're overlapping eggs, yes.

Jim: Okay, so these are overlapping eggs. My next question: do they share this common LFP input? That's a shared input across all cells?

Jeff: Yes, that's right.

Jim: We're going to assume for now that those synaptic inputs from that LFP are from some other population of neurons, not neighboring grid cells. Is that right?

Jeff: That's right.

Jim: The LFP could be in that same population; it's just a global movement of all the variables. Is LFP a traveling wave, or is it static across all of them?

Jeff: I don't think LFP is coming from grid cells. I don't think people think LFP is coming from grid cells. LFP is whatever they measure, wherever they put the probe. The theta frequency is generally believed to be generated someplace else. We could look it up. I think there's a theta frequency in the cortex, and I believe it's generated between thalamus and cortex. That could be the source, but aren't they actually measuring all of the cells in that same region?

Jim: They are. So all of those cells are going up and down as a global signal. When they do this, the problem is they're measuring the voltage outside of a cell. That's why it's called local field potential—it's not inside a cell, so it's the accumulation of a lot of activity. It's hard to pinpoint exactly where it's coming from, so I didn't even try to think about that.

That was going to be another question: are these LFP inputs to a cell? Does it really look like this, or should a cell be receiving spikes? What is the signal to a single one of these cells? There are a couple of theories about this. If you look on the left side, where I showed just the red and green arrow, the green sine waves—

You know how I'm calling red and blue sine waves? Red and blue, yes.

There are two theories about where the red cycle comes from. One is that individual dendrites have their own oscillation—the red oscillation—and synapses from other cells provide the theta oscillation, which is closer to the soma. The blue signal drives the cell body, while the red signal comes from a dendrite. In this theory, the dendrite has its own oscillation, which interferes with synapses from elsewhere representing the LFP, or synapses near the soma. So, there are two synaptic inputs: two depolarizations, one from a dendrite and one from synapses from another source. That's one theory. Do you follow that one, Ben? Roughly, yes. It's confusing. I don't like that one. The other question is whether the LFP inputs alone are enough to make the cells fire. It looks like, in the diagram on the left, if it's not synchronized, it's not enough. Every cell has to detect a pattern of inputs to fire; it's not just the global input. Otherwise, every grid cell would be firing all the time. So, we have a subthreshold oscillation going on. Yes. One hypothesis is that inputs to the dendrites are responsible for the red oscillation. That's one hypothesis. I don't like that one, but I read a paper this morning that still proposed it. It has some real problems. 

Another possibility is that there's a set of cells firing like the red ones at different phases, and those cells provide synapses. In that case, there would be two sets of synapses on a cell: one from a population representing the red oscillation, and one from a population representing the blue oscillation. They're just synaptic inputs to the cell.

So, I have a question. The LFP could either be static, with everyone getting the same signal synchronously, or it could be a traveling wave moving across the grid cell region. There are different models. Huh? It's neither. Neither? No.

How can it be neither? It's just the global average of firing in the cells in that area. It's not that every cell is getting this exact input; every cell is detecting some other pattern.

This is the global average of the cells firing, going up and down.

But any individual cell is not getting this signal. I thought all the grid cells would be getting the blue signal.

Yeah, the blue signal. That's what I was saying about the LFP, the blue signal.

It's not a single signal; it's just a global average of a bunch of cells firing. If it's a global average and they're phase indiscriminate among themselves, then you don't have a mechanism at all. No, they're not phase indiscriminate. They are in phase. It's just that a smaller number of cells will fire in the middle, and a larger number will fire at the peak.

But it's not like there's a single signal coming in that's up and down like that. It's a pattern of cells. It's an SDR of cells. But isn't it true that the LFP is experienced by all the cells in this area?

I think all of the cells in the area—what you can say is that the mean of all the cells in that area exhibits that cycle. But that's not the same as every single cell getting it. The assumption is that all the grid cells in a grid cell module would be experiencing the same LFP.

I think they're exhibiting the same LFP. Maybe it doesn't make sense to think of all the cells sampling from that LFP. Two neurons are going to have slightly different versions of that blue curve, maybe distorted versions, but on average they're similar. I'm trying to get the right cartoon image for myself. My understanding of all the grid cell theories assumes that all the grid cells in a particular grid cell module, if you measured outside of them, would all have the same blue curve oscillation. It's an oscillation in voltage. A cell fires when there's a differential between the inside voltage and the outside voltage, which matters. It's not the absolute voltage, it's the differential. Even just the fact that you have some sort of potential voltage oscillating outside, you don't even need to have synapses—the cells are more likely to fire at the trough or the peak of that oscillation. I don't know a lot about this LFP, but it seems these aren't essential issues. I think the important thing is that all the grid cells in a module have to be experiencing the same blue oscillation. I don't think it's essential. All we care about is that the cells are in the space relative to this. The theories I've read about, which make sense to me, say you could speed up and slow down everybody simultaneously by just adjusting this global LFP. You could scale the entire module, which means everybody has to be experiencing the same LFP.

No. How not? Because it's just an ensemble of cells sending input into that region. Their firing rates would have to speed up or slow down, but it's not like there's a single scalar value that everyone's receiving. That has the LFP. You see what I mean? Oh, okay. I think I see what you mean. Is that a difference that makes a difference? No, I think it's purely an implementation detail. I don't think it makes any difference.

I'm just trying to react to what Kevin was saying. I don't think it's really important for today. Today the idea is really simple: we don't know the right scale, so as we're moving along and predicting the next location, we'll sweep through a series of locations in that direction and see if any of them match. If one of them matches early or late, then we say that's good enough, and we'll just adjust our scale and keep going. There are a lot of ways we could do this in software, and I certainly wouldn't do this the way they do it in neurons. For example, thinking about a temporal memory of a bunch of locations—can you see the dots, which are possible locations where I could be at this moment? Let's say I know the direction, and I go this way. For each of those dots, I would have a whole sequence of possible values where I could be depending on scale. I see which one matches my model, and one of them will match. If it's the scale version of a model I'm tracking, then one of them will match.

If I had an object with two features that were really close together, and another object where the same two features are further apart, I might get confused, but you have to continue inferring. As you're going along, just because you found one feature that's off by a scale, you're going to keep trying, and you might find the next feature is correct. You can't guarantee that when you find a single feature matching this way that you've discovered the scale of the object, because it could be another object that just has the same features closer together or further apart. In the end, it would all work out. The tricky thing is that there are a lot of possibilities, because from any of those start points, I need to check all the possible places where the next feature could be. But clearly the brain is doing this. It has to do it on a 10 hertz cycle, so it's got plenty of time to cycle through a series of locations.

The reason I was asking is that I was trying to hypothesize that there's a self-timing operation where the theta cycle originates from the interactions of cells themselves. Another way of doing it is as a traveling wave—you can imagine a traveling wave across the grid cells in the direction of motion. A lot of things become possible. I think it's unlikely to be the traveling wave, Kevin, but has anyone measured it? There are traveling waves in the brain, but in the cortex, it's believed that the theta cycle originates in the thalamus, and then the thalamus broadcasts broadly to all different areas of cortex. That wouldn't be conducive to a traveling wave; that's more like a broadcast signal.

It wouldn't make sense to have propagation delay doing that. Imagine the thalamus is centrally located, and the distance between the thalamus and all areas of V1, for example, is uniform. If I'm broadcasting from some central place, I can't really have a traveling wave at the place I'm broadcasting to. A traveling wave requires people to wait until the person before them acts. In this case, a traveling wave would mean, "I'm going to go when my neighbor goes," and then someone else says, "I'm going to go when you go." But here, it wouldn't be that way. It might look like a traveling wave, but it wouldn't be a traveling wave, because it's being broadcast from someplace else and might just be arriving at different times. That's why it wouldn't be a traveling wave. It doesn't have to happen everywhere at the same point, but it's not a propagating wave reliant on neighboring cells.

Okay. The argument is that from the geometry of the brain, it doesn't need to be a traveling wave; it's more like propagation delay. If the theta cycle is generated in the interaction between the non-reticular formation in the thalamus and the cortex, that's a single generator. Therefore, if you have a single generator, it's not a traveling wave. A traveling wave starts at one place and sweeps across. There are many of those in the brain, especially in the retina, but I don't think you need that here.

The simplest explanation is to assume we have a theta, or a scale we're targeting, and we sweep through different scales as we go. This also made me wonder if we could solve other problems, like orientation, by sweeping through different orientations. You have an alternate solution that's similar, but instead of sweeping, you try different cortical columns at each orientation, narrowing down and down-selecting. At some point, you just select. We can go through that tomorrow. We don't have a detailed mechanism yet, or a biophysical mechanism. I'm just curious about the general idea. Can you give me a general idea of what you're thinking? Yes, I can give you an overview in 10 minutes.

That's essentially the issue. Let's say I have a cup, and I move along the cup with a displacement (red arrow), and I don't know the orientation of the cup. A movement or displacement observed near the red arrow could be in the coordinate frame of the cup, near those directions. How could this be resolved? Thinking about temporal memory, like in the columns plus paper, we have associations between the sensory layer and the location layer, and at each step, we narrow down the union of possible locations and sensations. The difference now is that we have a set of modules or cortical columns, each representing a certain module with its own orientation of the curve.

When I do a displacement, are those multiple grid cell modules? Yes, each would have its own grid cell. If this is grid cells, it could be other location codes, but if it's a grid cell, it's the same as the columns plus paper. You have multiple blocks, like four separate path integration modules. That would be exactly the same, with the only difference being the way it's stacked from top to bottom. Each represents a certain orientation, and with the orientation, its own transformation of the displacement to a certain direction on the place code. If it's in that orientation, the direction would go one way; if it's another orientation, it would go another way. We have multiple guesses on what the orientation is. Each starts with a guess, and as a group, they narrow down the possibilities. Initially, it might be at all those red locations, then you check if the feature is at those locations. If not, only one column survives with activity.

The activity down-selects, and you end up with one, while the other columns could be reused with a new guess of location. They could inform each other. Once one matches, it's broadcast, and they all go into the same state. Grid cells in the entorhinal cortex anchor independently when entering a new environment, but once you've learned an object, they always anchor together, implying a learning process where they link to each other. They're not always linked, but in a learned environment, they are. When one module's orientation is correct, it can inform the others, allowing them to adjust accordingly. This way, you don't need thousands of columns to get an accurate direction; a small number, like eight, suffices. Once one identifies the direction, the others can use that information to refine their guesses, improving accuracy without requiring many columns for object orientation.

It sounds like you may be describing something different than what we discussed before, where we used multiple modules, one for each orientation desk. I think each row here would be a different orientation desk, and each row contains multiple modules. That's a lot of grid cell modules.

Why does it have to be that way, Subutai? Why couldn't each block in a row have its own orientation? I was assuming those are each a grid cell module.

They are—each one is its own grid cell module. What we're thinking is this row would have an orientation desk, another row would have a different orientation desk, and so on. Each layer would use multiple columns, not multiple modules. I meant modules in the sense of the Monty system. There's a language issue here—"modules" is an ambiguous term. What Jeff was asking is whether each of these are modules, and they are; each square is a grid cell module.

You assumed those four were in the same cortical column, just like our column plus. But what if that wasn't true? This is something that's puzzled me for a long time. The best solution we've come up with is having multiple grid cell modules, but it doesn't look like there's enough to have multiple ones inside a single cortical column. Could there be one grid cell module in each cortical column, and still have them voting across columns?

There's a question: how do you accurately represent location within a column? Do you use single modules or multiple modules? Regardless, this proposal involves having multiple guesses going on simultaneously. They could be multiple columns. We have to figure out the exact biophysical interpretation—are they multiple columns, or could it all be done within one cortical column? There's a lot to consider, and it could be related to that. Of course, it has to work with the rest of the system. I'll leave it at that.

Let's go through this in more detail tomorrow. I think you missed a step on the hypothesis that might help explain how the movement is translated into these shifts. That's the same as in the column specimen, but we didn't state it explicitly. For Jeff and others, it might be helpful to clarify that. That would be helpful. I probably have to run now. All right, this gives you something to think about. Thanks for doing that.