Today won't be a typical research meeting because the plan is to talk about the research roadmap and what's ahead in the next months, instead of an actual research topic. Niels and I have been discussing this for the past weeks and also met with Jeff in December, revising the roadmap after that meeting. Now we think it's a good time to share it with everyone, get feedback, and then finalize it. I have a general first slide to put everything into context, and then Niels will go over the research roadmap in more detail. I usually don't ask this, but please hold your questions until after Niels goes through his part, so we don't stop on the first slide and try to discuss everything that Niels will cover in the next slides. That question is addressed to me.

I'm not going to name any names. Okay, I already broke it.

If you can't follow without asking a question, feel free to ask, but I'll pull up the overview slide again after Niels' part, and then we can discuss everything in the big picture. I just want to make sure we get the whole overview out first before diving into the details.

Are you seeing my screen already? Yes. Okay.

I thought about our high-level goal, and this is a rough version we can refine together. We want to get our approach widely adopted, and I divided it into three pillars. One is to make our implementation easy to use and extend, to reach a wide range of people. We also want our implementation to do useful things so people actually want to use it, which means implementing everything we've figured out over the past years. There are also several larger open theoretical questions, and we need to make progress on those fundamental research questions as well, like object behaviors and manipulating the world. These build on each other: first, we try to make fundamental research progress by looking at neuroscience and figuring out how the brain does things, then move to the implementation stage, actually implement it in Monty, test these ideas, and see how well they work. If we forgot anything, we make it easier to use and try to get the community involved, encouraging people to build applications based on that. This is a new aspect we're just starting and will learn a lot about in the next month.

To go into more detail on these three pillars: making it easy to use and extend, and getting it adopted, involves documenting and educating people about the approach, fostering a community, finding researchers who want to use or extend it, and encouraging people to test our approach. Another part is making the code easier to use and extend, which involves some code refactors and two bigger items on the roadmap. We won't go into much detail on this part in this presentation since it's more focused on the research roadmap, but I have one slide at the end that covers the yellow part if we have time. At a high level, I won't go into too much detail here.

For implementing research progress, Niels and I identified two bigger objectives: better unsupervised learning, which is required for the next one—testing our heterarchy implementation and seeing how well we can model compositional objects. First, we need to remove the assumption that we show one object per episode and that the system is told when the next object is being shown and given labels. There are several things under this approach. Once we can do this—have compositional objects and move from one part of the object to another—the system can handle this, and we can test how well we can model compositional objects and utilize the hierarchy we've implemented.

On the research side, a big topic will be object behaviors, which we've discussed a lot in past research meetings. This seems to be an important prerequisite for implementing policies to manipulate the world, which is ultimately what we want the system to do—being able to manipulate the world is what the system will be used for in the future.

Another area to consider is compositional policies and how we can hierarchically decompose policies.

Once we make enough progress on these theoretical topics, they will move to the third column to be implemented and tested.

There are other theoretical questions that might come up in the next months but are not main topics, such as scale, object deformations, or dealing with abstract space. I'm mentioning them since they might relate to some of these areas.

In these research milestones, Niels and I divided both into two subparts: policies and learning module computations. Niels will go into more detail about these. For the code refactor, we plan to refactor data load and dataset. Tristan has already started some initial refactors and planned the follow-up work. After this is done, we can make the motor policies more modular, essentially having motor modules, which will be a prerequisite for part of the code and this research milestone. That's why we're focusing on these code refactors to support the research and implement the ideas.

I've marked the next or in-progress items in green: the refactors, unsupervised learning as the big next topic, and object behaviors as a major research topic. This view shows who will roughly work on which part, though there will be overlaps. For the next three months, we'll focus on unsupervised learning and object behaviors from the research side. In the next six months, we hope to transition toward compositional objects, hierarchy, and starting to think more about policies.

compositional policies and actually manipulating the world. The last thing is that to encourage people, especially the research team, not only to work on implementing tasks but also to make progress on fundamental research questions, we created the "What I Learned This Week" Slack channel. I want to highlight that this is a big part of the work, so we really encourage everyone to keep reading and thinking about these fundamental research questions, and we want to dedicate time to them as well. I'll hand over to Niels.

I'm going to continue from where we left off. As Viviane said, I'll go into more detail about these topics, but are there any high-level questions about this overview? You're welcome to ask questions while I'm talking. We thought it would be helpful to go through this first part in one go.

This slide is from June 2024, when we set out the two main themes for research: hierarchy and actions. Broadly, this hasn't changed; these are still the main areas we're introducing and focusing on. When we put together these short-term goals, we realized that actions would always follow hierarchy to a large degree. Before we could address hierarchy in an interesting way, we needed to improve unsupervised learning, which is why we think that's the next focus. This naturally leads into compositional objects, where hierarchy starts to manifest.

This overview lists some of the concrete things we're planning to address unsupervised learning and compositional objects. One thing to highlight is that Monty already has many features that make it well suited for unsupervised learning, and we've implemented some groundwork for compositional objects. When I discuss these concrete ideas, it might seem like a random arrangement of tasks, but most of the problems are already addressed in terms of what we need for unsupervised learning. There are just a few areas where we need to improve performance or ensure certain properties, and then much of this should fall into place.

For both unsupervised learning and compositional objects, you can broadly break down the projects into the overarching task of developing a dataset and evaluations for that particular goal. Within that, to solve the task and do interesting things, the two main areas for changes are at the level of learning modules, representations, and computations, or at the level of policies, including model-based policies, which is also the case for compositional objects.

Unless there are questions on that, my plan is to go through each of these and give a sense of what the task is and why it's relevant. Scott and Hojae, you've both mentioned projects that sounded interesting. It's worth thinking about what burning questions you find particularly interesting under unsupervised learning, especially regarding next projects we can work on later this month.

For the dataset and evaluations for unsupervised learning, the good news is we're not planning to start from scratch. We'll continue using YCB as before. As Viviane mentioned, we want to remove current assumptions about supervisory signals, both implicit and explicit. For example, we show an object for one episode, then reset the network and evidence values—an implicit signal that a new object is being learned. We also have more explicit information, such as passing labels to the system to indicate whether episodes are associated with the same object. Additionally, we provide 14 idealistic views of every object, which helps ensure good coverage and that all parts are learned. This is a solid initial setup to check system performance.

However, to move toward more complex and realistic situations, including compositional models, we need to address that few datasets have explicit and labeled parent and child objects. We could potentially construct such a dataset, but generally, with multiple levels of representation, some are implicit, and direct supervision like we're doing now wouldn't be possible.

So that's the dataset side. In terms of computations on the learning module side, we discussed possibilities for enabling learning modules to handle moving onto new objects and moving off and back onto previous objects, rather than maintaining an eternal memory across all time. We think the simplest solution is to have the evidence values we currently accumulate decay more rapidly. 

This plot shows the highest evidence score associated with a given object as a function of the episode. In this episode, there are multiple objects, and depending on where the sensory patch is, the colors at the bottom indicate which object is being visualized by the system—this is ground truth information. At the start, the agent is on the potted meat can (the blue line) and accumulates a lot of evidence for it. Around step 32 or 33, it moves to the bowl and stays there for a while. As expected, the evidence values for the potted meat can drift down, since it's no longer receiving consistent evidence, but even after 15 or 20 steps on the bowl, the evidence value for the potted meat can remains high due to prior accumulation. This accumulation is essentially unbounded and has little, if any, decay at the moment. It's a tunable hyperparameter, but we generally don't apply decay right now. We believe that shortening this memory will allow the system to handle moving onto new objects, forgetting the previous one, and building new evidence. For this to work well, the system should move efficiently when on an object, since it doesn't have much time to accumulate evidence. As the system is starting to do with the hypothesis testing policy, it should quickly move to interesting features. A major theme in unsupervised learning is adding mechanisms to encourage exploration of interesting features and faster convergence.

One approach is bringing voting into the unsupervised setting. In the single episode setting, we have supervisory signals, and we also have them in multi-LM experiments. Currently, the learning modules know they're all seeing the same object and establish connections based on that. However, if we remove supervisory signals and don't know if there are multiple objects or sub-objects visible, we need to naturally learn these associations. This is a deeper problem that we've had ideas for and is important for Monty's long-term capabilities, but it should also help in the short-term focus on unsupervised learning. For example, with five learning modules, each with its own sensory patch, all moving onto an object, evidence for or against different objects accumulates quickly, eliminating the need for piecemeal memorization. We still keep in mind that a single learning module can do most things we need, so voting isn't enabling something a single module can't do, but it helps make it more robust.

Those are the learning module computation changes we're considering. On the policy side, we want to move more efficiently. For the distant agent, which saccades across objects, it's currently just making tiny random walk movements. Larger movements are based on the hypothesis testing policy, but that tends to move it in space rather than to a different point. We have ideas for both model-free and model-based signals to help move to the most relevant parts of the object more quickly. This will be useful for unsupervised learning and, more generally, for long-term use of Monty.

A subtly different focus is on exploration policy. Currently, exploration policies aim to be exhaustive but are still basic, following patterns like spiral scans with the distant agent to cover as much as possible. This works well in the supervised setting, where we show each object from 14 different angles, but if we just present the object and, in one or a few episodes, the agent sees it from different angles, it needs to learn about the object and explore it efficiently. We think model-based exploration policies would be useful for this. Heuristics might help, but even a simple approach—where, after seeing part of a bowl, the agent identifies gaps, edges, or points of symmetry as interesting areas to move to—could improve exploration.

and then lastly, it's a subtle distinction, but there's an important ability to switch between a learning-focused and an inference-focused policy. Currently, we do this switching explicitly when transitioning from the training phase to the inference phase, and we can do this because we know when that transition happens in a supervised experiment. However, when we move to true unsupervised learning, the system can detect when that transition happens, but we also need to define this switch to happen naturally. This is more of an infrastructure change, so it probably fits more into what Viviane was showing on the left-hand side with the yellow boxes on the infrastructure side. The hope is that once some of those things are changed, it should be a relatively simple change. It's not implementing a whole new policy like the top two points, but it will still be an important step to get unsupervised learning working well.

That's everything on unsupervised learning.

Any questions?

I'm sorry, I didn't want to stop everyone from asking questions. It feels very wrong now that I said that. It's such a different atmosphere. I have a question. On the previous slide with the accumulation of the points that a hypothesis is getting.

This one? Yeah, that one.

So as you saccade around and move off the object, and you sense something potentially new—like it goes from a mug to a banana, and now you're sensing banana—rather than start decreasing the score for that object, shouldn't you just create a whole new thing to start scoring for, and then if you go back to the mug, you can start adding points to that again? So in the learning module, you'll have multiple hypotheses that you're keeping score for, or have I misunderstood everything?

No, I think it's a fair point, and there are two things about that. One is, we've also discussed that we might have a system where the learning module thinks, "Okay, I've moved off of this object. I was on it, but now I've moved off, so I'll reset my evidence values and start accumulating for whatever I'm on." We wanted to try first to see how it works with this kind of evidence decay, but that's another possibility. The other thing is that, yes, that's also important, and that's where the hierarchy will come in. We don't think it would be the lower-level learning module keeping track of both objects. The lower-level learning module would forget about the potted meat can after it moves off and is on the banana. But assuming the potted meat can and the banana both exist in the world and are both possible locations for those objects, as it moves off the potted meat can onto the banana, a higher-level learning module, which is modeling something more like a scene, would then understand that now it's moved on to the banana, as received from the lower-level learning module. When you move back to the potted meat can, that high-level learning module can then reinstate the potted meat can representation in the low-level one.

Once we're not doing multi-object or scene-like datasets—that was something we discussed with Jeff that we want to wait on—but with compositional objects, which are multi-object to a certain degree, that will definitely be important.

The way I was thinking, to Will's question, is that we can learn that we've moved off from the potted meat can and went to banana, but in an unsupervised setting where we don't have the labels, we don't know that we've moved off. It's possible that we're actually seeing a potted meat can fused with a banana and it's actually one object, for whatever reason. I don't think there's a straightforward way to say, "Okay, I'm in a different potential object that I can start accumulating for." Maybe if we see a trend where we're continuously losing potted meat can, and after some episodes, after we've lost, say, 10 points, then we can have a rule to start accumulating for a new one. But we have to assume in this case that we don't know if we're actually going to a new object. Is that how it works?

Yeah, definitely. One thing that comes into this as well, which we don't have a specific point on here but will be important, is how you forget things over time in unsupervised learning. You might initially have built, the first time you encountered a compositional object where there were multiple things together, you learn that as one object because they were together. As you say, you don't know when you're moving off one object onto another, but over time, you might see the logo in one instance at one time, and then associated with a different object at another time, so you'll start forgetting all the information that was fused to the logo and develop that as its own singular representation.

That will be really important. It's basically an unsupervised learning thing we need to do for compositional objects. We don't necessarily need to do it for the most basic unsupervised learning, where we remove one episode per object.

So we might need to assert, in terms of the episode transitions. But I don't have any questions about what you're all presenting. It's really great. I have a lot of ideas on how to implement this, but I don't think that's what we want to do right now. Is that correct?

I don't mind writing those up and posting them after this meeting. That sounds good. Maybe there's a chance we get time to discuss that at the end, otherwise we could discuss those next week or something. I'm taking notes and want to write them up so I don't forget. Okay, nice. I assume Viviane's comment earlier was about asking questions if you don't understand, but maybe not trying to solve all these problems right now. I think we should definitely talk about any additional ideas. I just wanted to make sure we get the big picture and the roadmap into everyone's head first, and then we can talk about the details afterwards. I definitely didn't want to say this is not up for discussion. I've got five specific ideas already on how we might go about this unsupervised learning, on top of what you already presented, but they could take an hour. I'll just leave them for now and write them up after we're done here because I probably have some other ideas too. It's all good. I have no questions. I like the way you organized it. That sounds great.

Any other questions on the unsupervised learning?

It seems like some of this, especially this whole picture, ties into something that's been on the research roadmap for a while, which is using negative information. There's a difference between a point that hasn't been explored and a point that has been explored and you know there's nothing there.

We talked a bit about whether to include that here. The first point is the negative information you might get where you have a model of an object, you move off, you don't see anything, and we felt that's most important for scene-like arrangements where you're moving through empty space a lot. We're going to start with simpler compositional objects where everything's in one place, so it's not as necessary. We might just wait with that for the sake of prioritization. As a separate point, one thing we don't want to do is store negative information. We don't want to explicitly say there's nothing here, because then it becomes an unbounded cost associated with that. Another big thing was that the idea we originally had was related to the fact that we can detect when we're not on the object, since we have an object floating in empty void, but once we move to more realistic scenarios, there will never be an empty void. So whenever you move off the object, you'll still sense something.

We still want to be able to use this information eventually. If I predicted to see a certain feature but then saw something different, we still want to be able to use that. We are using it to a certain degree, but we'll need to think more about how we would use it if it's not this explicit off-object signal.

That makes sense.

The other main theme that will follow up on the unsupervised objects and unsupervised learning is compositional objects. This starts with making sure we have a dataset and evaluation pipeline to actually explore this. This will probably be something we revisit, and it's not finalized what the compositional dataset would look like, but the most recent thinking is that it would be something like a series of four objects and four different logos. Those logos can be associated with each object, as well as the objects and logos in isolation. We would then learn in an unsupervised way on that data. This avoids some of the awkwardness we had with the dinner table scene, which had some advantages in terms of policy and other aspects, but this is a simpler compositional object and enables us to explore some interesting problems, which are on the next slide. This is a nice example of a research idea that we have a good idea of how to implement and what we expect it to do, but we just haven't implemented it yet. This involves having location-specific compositional representations where, if you have something like the Numenta logo bent at an angle, you might use two learned associations to represent that, but as the deformation between the logo and the object becomes more extreme, you need more neural hardware, or in this case, more associated connections to map that relation and understand it.

I think it would be really interesting to finally test this idea, because it's something we've discussed for a while and it's very much implementable. It's something we could explore with this kind of dataset.

I want to make the argument that these location-specific representations are not just for cases like the bent logo, but for all cases. Everything is representation. Just want to make that clear. That's a good point. It'll become most apparent in this situation, but it will be true for all compositional objects. The harder part is how we avoid having to learn every point, which is a general problem of the system—we need to extrapolate. You need to focus on where there's a change, and after that you don't have to remember everything.

Once we have a dataset, the main thing we need to add on the learning module computation side is the top-down connections. We have a clear idea of how these should work. As Jeff was saying, these are going to be location-specific in all instances. It's a case of implementing the code and testing it.

Another relevant topic we've discussed is the possibility of re-anchoring hypotheses. If you're familiar with SLAM or path integration, as you move, you integrate movement and receive sensory information that reinforces your perceived location. Currently, we rely heavily on path integration, assuming it's nearly noiseless and perfect. However, real robots, grid cells, the entorhinal cortex, and probably humans must, at certain times, use strong sensory information to recalibrate their position on the map. For example, a doorway might be a prominent landmark in your living room. If you touch your doorway, close your eyes, and walk in a circle, you may have a sense of where you are, but touching the doorway again with your eyes closed gives you certainty about your location.

This could operate at the level of single learning modules and also hierarchically, with the high-level learning module indicating where we should be and re-anchoring location. This will be interesting to explore and will likely help with noise and distortions, such as a bent logo or a logo distorted by the side of a mug.

Lastly, there are policies that will be particularly helpful for compositional objects. There are two themes: policies to recognize an object before moving on to a new one, and once you recognize an object, moving quickly to the next. When you look at the world, you get a sense of what you're seeing, then move to the next interesting thing, rather than randomly looking around and being confused. This brings in concepts like saccades, where you look at something, make a few movements to confirm what it is, and then move on to the next interesting thing.

All of this, as Viviane's earlier slideshow led to, connects to object behavior. For example, Scott suggested a lamp with a switch as a simple compositional object, similar to a mug with a logo. You can imagine complex behaviors and motor policies to learn, such as generalizing from understanding a switch and a lamp to a lamp with a different morphology, but still knowing to look for the switch to turn it on. That picture is impressive—he got the switch and everything. Just Google it.

Hopefully, the simplicity of the compositional datasets we're proposing doesn't suggest we're going into a narrow tunnel that's irrelevant for later, more complex tasks. As Jeff often remarks, a simple task is often a useful way to understand a much more complex problem.

Viviane, do you want to keep sharing, or should I? We can keep the screen up and discuss any comments, questions, or suggestions. Are you seeing the screen again? I have two monitors and want to make sure.

I was thinking it might be possible to reach out on a forum to say we're looking to work with new datasets. If any lab can scan these objects and do some labeling, we'd love to augment our datasets with compositional objects.

That's a good point, especially for object behaviors. Maybe we could implement some of these in a 3D renderer fairly easily, but objects that move or have interactive parts could be complex to implement. It would be great if anyone in the community wants to help. You could start with even simpler compositional objects. I remember being confused about how compositional objects work, so I spent time thinking about two rectangles—one representing a room and the other a rug or table—and trying to model where the table is and its angle. It's a two-dimensional problem with just two rectangles, but it was hard and I didn't have a solution. My point is we could start testing the code on very simple things to make sure it works and is bug-free, then move to slightly more complex objects. Ultimately, I wouldn't want to jump straight into understanding something as complex as a bicycle and its composition. There's a lot of work to be done just building the code and testing mechanisms before tackling something as complex as a mug.

I'm not saying we don't want to get more complex objects—we do. It's really simple. It also occurred to me that we are talking about moving simulators at some point, and we'll do a new simulation environment. We can hand-design some objects; I don't know how this would work with the particular environment that Tristan was reporting on. I looked at it briefly, but presumably you can design your own objects to put in there, though it wouldn't be real. For our summer hackathon, Rami implemented this dataset of 3D objects. This was when we were talking about doing the dinner set thing. We might still return to this example or this dataset later, but it was a nice example—Scott, with this 3D software, we have some flexibility. One interesting thing: we had a lot of problems importing this into Habitat, which turned out to be arguably much more difficult than developing the actual 3D model dataset. Maybe other simulators would be easier; I don't know.

It's worth thinking about the timing around the change in simulators. Another advantage of this first work with supervised learning is that we can just continue using YCB, given that we have a fair amount of infrastructure we also have to figure out.

It's in line with the simulator topic, and essentially game engines are simulators. Some of the game worlds are quite extensive and essentially infinite. I'm thinking of Minecraft—it's simple, but it's an infinite world, so it could be an interesting dataset already.

How soon could we actually jump to the real world? You did that for the hackathon last year. The real world is easy, right? We have lots of data; we don't need a simulator. At least from a vision point of view, I think the reason we've stayed in simulation mostly so far is that it's much easier to evaluate. We know the ground truth of what is currently being sensed, whereas in the real world, it's very hard to objectively evaluate and run many experiments. I wasn't thinking of sitting out on the street corner and watching things go by. I was thinking more about taking real objects and having them on a white background or on a white table.

You could have controlled environments where you say, "Oh yes, there are three objects here, and this one's compositional like this and that."

I'm not saying you should do this; it just seems like we've spent so much time on simulators. I'm just wondering if we could somehow shortcut that.

That's an interesting idea. One idea we had was that during the next TVP retreat, we do a two-day hackathon where each of us gets a little robot kit and we do mini projects with that, just to keep working with real-world data. Then everyone can take it home and do real-world experiments wherever they live. But it still requires more infrastructure to be set up.

It's harder to do repeated experiments that are comparable between everyone, wherever they are. The real world also runs in real time, which is quite slow. It doesn't have to be—if you're talking about static objects, you set up a white table with a white background so all you see are the objects, and you start. The reason I was thinking about this is because we're into the discussion section here, right? When you're talking about unsupervised learning, I was thinking the way we do that from vision and touch are quite different. They have different mechanisms involved. In vision, we have all these clues about what's going on: parallax works with one eye, we're constantly moving just slightly, and every time you move you can tell what object's in front of another, what object's behind, and that they're not the same object. Just the slightest movement of your head—not the eye, but the head—gives you a huge amount of data: these are separate objects. We can also take advantage of convergence of the eyes. That's not as important as parallax, but it's still important. There are also issues like policies for explanation; we know the brain uses saliency detection, so it jumps to places that are interesting. These are not model-based; they're just like, "Hey, there's more stuff going on over here." Especially the parallax one—it's something that would work in real life and vision would be very important. You can't really get that from an image; you need something that actually models depth so you can take advantage of parallax.

Last time we did the real world, we just used the iPad camera, which sends rays out and gets an actual depth image. That may work too, but it isn't as powerful. I think parallax tells you right away that two objects are not the same if they have a slight difference in depth. For example, if I take a dumbbell and put it at an angle, the depth difference from one end to the other isn't really important. What's important is that I could say, "Oh, that's further away, this is closer, this is separate," but from a parallax point of view, they're not going to be separate. There are some things—thinking about touch too—where it's really hard to build a touch sensor, but having real objects in the real world suggests solutions that make this problem much easier.

With Monty Meets World, we created a dataset where we took depth images of different objects, including some adversarial conditions like bright light, multiple objects, or a hand coming in. One thing with that is it limited the degree of sensorimotor interaction, because we have these depth images taken from one angle. Monty can saccade over that depth image, but it cannot actively explore the object. In fact, learning still took place with a 3D scanned version of the object.

It's a fairly complicated pipeline because we need to 3D scan the object, which is often imperfect. At inference time, we have this one image. It feels like we almost need to take the next step, which is to have a robotic arm that can actually move before we could do it at least to the same level as in simulation. If we have a 3D camera, we should think more about how to use it, since that's a different sensor than humans have.

If we're going to build systems that interact with the real world, it might be helpful to deal with real-world objects from the start, maybe not exclusively. We could look at some of the systems biology uses, like parallax, which seems very important for vision.

It really provides a huge step up for unsupervised learning.

Instead of us or someone else having to build a complex object behavior dataset in simulation, maybe once we look into object behaviors and motor policies, we just go into the real world. Then we'll probably notice a lot of things we didn't think about when we were just doing simulations. I don't think it will take us very long to figure out how these action policies and model behaviors work. I think we'll solve that pretty quickly.

That's my goal for our offsite or retreat. We'll see.

At touch, discontinuity of locations is really important. With vision, you can move your eye a little and slide from one object to another, and you don't really know that in a 2D image. Even depth information doesn't always tell you that. With touch, it's quite different. It's much rarer that objects physically touching each other are separate objects. It can happen, but it's much less likely. When you move your finger around an object, it's much less likely you'll accidentally slide into another object. There's always going to be some point of discontinuity—even if the two objects are touching, there will be an edge or an angle, and those are clues.

Those are definitely clues. When we touch things in the real world, we often move them slightly, and if we're looking at them at the same time, we get parallax, and then we know they're two separate objects. There's a lot going on in biology and the real world, and we don't want to exhaust ourselves trying to come up with alternate solutions if we can use some biological solution.

Just things to think about. Once you put a robot in and it's moving, you can only run in real time. The robot can go slow, but from an experimental point of view, real-time experiments become painful quickly.

For example, you might need to put a new thing in your robot arm and wait for 15 seconds. That process becomes painful quickly, whereas in a simulation environment with a physics engine, you can pick up objects and knock them over. Maybe the compromise is to keep in mind how you'll do this in the real world, and think about things that will help in the real world, like parallax, which we haven't really introduced. You think about that in the real world, then do your simulated world, but make sure the simulated world will work in the real world as best as possible. You can always drop back out and test a couple of real-world things. My fear in the simulated world is that we end up solving simulated problems, and then when you go to the real world, things are quite different.

There's a lot of research in that direction because of robotics and autonomous cars. There are sim2real and real2sim people working on bridging the gap between simulated and real worlds. I think we should always keep the real world in mind, and when the time comes, or hopefully when we understand kinematics, we can move forward. Keep in mind that thinking about doing it so it works in the real world and testing it in the real world is not the same as running all your tests there. As Michael was saying, that can be very slow. The real danger is saying, "We'll get to the real world eventually, let's stick to the simulation world."

A fun way to encourage this could be to have a real-world hackathon every six months, where we actually work with real-world data and try to solve different problems using our current algorithm. On a day-to-day basis, when we evaluate our experiments, we still run them in simulation and run our benchmarks in simulation. There could be a hybrid where you run them in simulation but always have at least one test, like in the hackathon Monty Meets World, where we do one real-world test to make sure we're not losing sense of reality. What I'm hearing is that ought to be possible. We currently have unit tests, integration tests, and end-to-end tests. This would be like a real-world test. It would probably be the slowest one and run the least often, but at least it would be the beginning of a regular feedback loop. You can think of the real-world test as a unit test for the real world. We don't have to do anything super sophisticated; we just have to make sure it works. Somebody runs it once a month or once a week, but it's the start of that feedback loop, and then we tighten it.

They automated the real-world part as well. In that test suite, HP also tested building one print cartridge in an automated function.

Maybe next year I can run the real-world test of washing my dishes with Monty once a day.

I wanted to ask that because Jeff keeps looking at parallax, and I just want to check my understanding. Parallax is something we can test with existing setups by adding an agent that moves the distance sensor's location stochastically. As long as we integrate that information into the agent's location, that will generate parallax, right? It doesn't control the head movement, but imagine it's an eye stuck on the head and the learning module. It's model-free; you could just move your head a little left and right.

It doesn't have to be stochastic; it could be continuous. That seems like a very nearby experiment we could add as we're figuring things out. Would you do that on real objects or in the simulator? No, it's in a simulator. It's an agent sitting in a space, just moving the agent location versus turning the distant agent, right? It's like first-person shooter movement.

That's exactly what I think about parallax, because it's something we know biology does, and it's really powerful. It may be something we want to include, even if we're doing it on simulated objects. I think, Tristan, that was exactly what I was arguing for—somehow keeping the real world in our thinking here.

The idea appeals to me that we're not too reliant on depth-based cameras, because depth isn't as important an indicator as parallax. We're not really aware that parallax is happening; we just sense that objects are at different depths, and we just know it. It works with one eye, so we're not relying on two eyes—it's parallax, that's the answer. There's a small amount of lens adjustment when you focus at different depths, but that's not nearly as important a signal. Sometimes it feels like a lot of the subcortical stuff is where, if we ever use deep learning in Monty, it could be useful. For example, deep learning is quite good at basic figure-ground segmentation, giving a core sense of what objects are near and their approximate depth. Some systems are specifically video-based.

Even with video images, I think we're going to create systems that work in the real world. It seems unwise to make a system that only works with two-dimensional representations, because our agents will be moving through the world, moving their bodies, rotating things. I've always felt that reliance on video is limiting. You'd want to make sure the agent or system is moving, and it's that kind of video—where the agent is active. The problem with video is that we think of it as what we see on a screen, but vision is an agent moving through the world, moving its eyes and objects. That's the problem we have to solve. We have to be careful not to try to solve just the image on the screen. I don't even know if it's possible to learn the world through a 2D presentation like that. It's something we can infer after we've learned the world, but I think it's likely that you really can't learn the world by looking at 2D representations of static or moving images.

We can probably do a pretty good job of inferring it later, but I don't think you can learn that way. It may be much harder.

I liked Tristan's idea—or Michael's—of just adding a little bit of movement to the eyes in our simulators. That's pretty cool. That was Tristan's idea, and it sounds like a quick thing to add and test.

Could be interesting.

If we have a couple more minutes, I could go over the last slide, which covers more of the rightmost column about the community aspect and making it easy to use. But I want to make sure we first discuss everything related to the research roadmap. If you have more comments on that.

I have an idea I'm writing up for object behaviors, but I don't have to do that now. I can write it up and post it, and we can talk about it another time.