I will share the Google Doc where many people have added comments, which was pretty exciting. Then we’ll go from there.

After the meeting last week, I wanted to walk through an example of my own, and I’ve done that with lots of people commenting. For today, I went through everyone’s comments and put together some questions that are still open.

Just as a refresher, the example we started thinking about last week as a group was this hunt-only keyboard, where we can sense what the key is. I tried to draw it. So, this is something where we can presumably sense that it’s an “N” without having vision. We were trying to achieve the goal of typing a phone number or the word “Numenta” in two hierarchies. That’s where we were.

One of the points with the most conversation or uncertainty was: where is the keyboard model? I think the settings are different. We can all agree that there is a keyboard model being learned in either of the learning modules, either at the lower or upper level. I call them R1 and R2. In the case where we’re so proficient that we’re not really thinking about typing each letter but are typing the word itself, do we still have a keyboard model? I went further and thought about how we learn to type. I’m not saying we don’t have a keyboard model—we do—but it’s not a very accurate one. When we learned to type, as humans, we often used vision to locate the object we want to touch, and then we would touch it. The model helped you visually find the key quicker. This isn’t necessarily the task we’re doing, but it’s how a human would solve it. The keyboard model wasn’t even essential for a human to do this. The hunt-and-peck idea is you look around until you find the key you want, then press it, and the keyboard model helps you find that key quicker, at least when you’re first learning to type. That’s key—it makes it quicker, because we’re trying to use models to make learning more efficient. Otherwise, it’s essentially a random walk until you find the key, which is what deep reinforcement learning does. I just want to make the point that when we think about the task here, it’s not clear that we have two learning modules, one using the keyboard model directly. I feel like I need a learning module that’s going to learn the actual movements to do the typing, and that learning module, like a motor, doesn’t actually have a model of the keyboard in it. It doesn’t need one; it just learns behaviors relative to body position. That was the insight I had: I’m not saying we get rid of the learning module, but I’m not sure it fits neatly into two learning modules. That was my conclusion from that little study. You might need a word model, and if you want a keyboard model, you also have to have the actuator model, which doesn’t have the keyboard. I don’t know if that was clear in my write-up, but that was a surprising realization for me.

Do you think we might not need a keyboard model? In the situation where you sit down to a keyboard that has a different spatial scale, like it’s miniature or larger, do you think a keyboard model would help you rapidly remap the scale? No, I don’t think that’s right. You don’t relearn to type; you just try to scale your movements. Assume you’re still touch typing with two hands. The more logical thing is to apply a slight scale factor to the whole thing, and your movements will be slightly less or more, but you don’t have to go back to the keyboard model. If I’m not typing right, I don’t look at or think about the keyboard; I just move less. That’s how you do it.

It almost seems like the typing part is a model-free policy at this point—we’re not really using the model anymore to do the typing. But if we use the hunt-and-peck strategy, then we switch back to a slightly model-based policy because we use a rough model of the keyboard, like where the letter keys are, to constrain where we look for them.

I think that's right, although I wouldn't say it's model-free. The motor behaviors are handled by a learning module that's playing back a sequence, similar to playing back a melody—sequences of physical movements learned through repetition. When you're typing a word, there's a model of how to type it, but it's not a keyboard model or a word model. The motor system is told to execute a sequence, and it does so. I wouldn't say there's no model; it's just more like a melody. Is that really a model? Depending on the linguistics, you could argue it's a model, but the point of a model is that you can reuse it in different ways, whereas a sequence is a fixed thing. Model-free systems could learn to do that.

I think this is happening in the cortex, in a learning module, just like playing back a melody. You can argue whether that's a model, but it's not the kind of dimensional model we typically think about in Monty. It's certainly happening in one of the layers in the cortex, which is just playing back a learned sequence of SDRs. I don't want anyone to think this is outside the realm of learning modules; I think it's inside. Learning modules do this. We started by studying melodies and made a lot of progress, but never completely figured out issues like pitch invariance. Anyway, it's a sequence. If I were to build the system now, I wouldn't say there are just two learning modules. If there's a keyboard model, that's one learning module. If there's a word model, that's another. Then a third learning module actually executes the sequence, whether it's replaying a melody or something else, but it's still a third learning module. It will do other things in other situations.

Couldn't the sequence be learned in the word? I was thinking a particular word would be a state or ID in the learning module that models words, and it would carry out actions until its state satisfies that word.

I didn't follow that. Your first question I got, but the rest I didn't. In the model that's actually moving the finger and making the motions, it has no concept of word. It just executes the sequence it's given; any other definition or meaning of the word isn't there. It doesn't know the letters. To type a word, it just moves the finger in the required ways, but doesn't know those movements constitute letters in a word. It's just playback—the concept of a word disappears as soon as it starts sending a letter at a time to someone else.

One way I was thinking is that higher up, you have a learning module that models words in an abstract space, like a sequence of letters on a 1D thing. It passes a goal state to the keyboard learning module, such as being in the state of a particular letter being pressed. It sends an abstract letter, and the keyboard model knows approximately where that key is—before the model-free version is learned—and then sends a motor command, either directly subcortically or to a finger learning module.

The keyboard model wouldn't send a letter; it would send a location, like a goal: move your finger to this location relative to this allocentric or egocentric reference. Maybe not even "move your finger," just "someone apply pressure at this location." If we only have one motor, it could be achieved in different ways.

Let's go down hierarchically. Someone knows what a word is in terms of its compositional letters, and sends a letter at a time to someone who has to actually do it. That letter could be spoken, typed, or written—there are many ways it can be instantiated. The word model doesn't know the details of how those instantiations occur; it just gets directed to the system responsible for typing on the keyboard, which then figures out where in space the finger has to press. We've discussed two ways of doing that: one is having a model of the keyboard to know where to put the finger, and the other, which humans often use, is using the visual system to locate something in space—find the letter A and tell the motor model to move there and press it. It could also have told someone else to move to that location and press it.

If you're following all this, I'd like to continue for a second. We've talked about doing one letter at a time—decomposing the word letter by letter—but clearly, learning moves down the hierarchy, and we don't want to do just one letter at a time. We discussed sending a letter at a time to another process, but ultimately, we want to send the whole word and let the system handle it. That's straightforward, but it means there are two messages going down the hierarchy: one is the letter, but you also know the context of the word. When you learn the sequence, I don't have to tell you the letters anymore. As soon as I give you the word, you'll know what sequence to play out. You need two signals going down the hierarchy.

With the way it was described—like with hunt and peck—that could still be supported by the keyboard model, but the goal state it's sending is not just "apply pressure here," but "move sensors here to confirm my hypothesis that this is approximately where the letter is." You would saccade there, see what's there, and then the keyboard model would be updated based on that. I was impressed by how poor our keyboard models are. I used to think I had a perfect model of the keyboard, but it's really not the case. The whole system could work without a keyboard model, but I'm not saying there isn't one. We could try to decompose this further. Someone says we need to type this word—let's do it one letter at a time.

Then we decide we're going to do it with a finger. The system says, "I have to find that thing." You could use a model, but we don't do this very well. We have to find where that thing is, locate it in the space of the hand, and tell the hand to move to that location and press. In the end, the hand and finger don't actually know they're typing an A; they're just being told to go someplace in the context of some word. They're given a context of a word they don't understand—just some SDR—and told, "Go to this location, now go to this location," and so on. If you do this enough, you'll learn how to do these things in sequence. If I just give you the word, I don't have to give you the letters anymore. That's how it feels to me.

It's odd that the motor model has no idea, other than it's getting a word SDR, what it's actually doing. It's just playing back a sequence. That was pretty bizarre. 

So, zooming out a little bit, given that it seems like a lot of typing is not using the keyboard model anymore, it's definitely a cool example to talk through. We've discussed a lot of issues and different aspects of it, but if we actually start to implement something, do you still think this is a good example to go with? I don't know.

Maybe not. All I wrote up was that it's an interesting observation that humans don't use the keyboard model very much. It doesn't mean we couldn't design the system to use the keyboard model, since that was our first explanation. We could design one that learns the keyboard model and uses it to figure out how to move the finger to different locations. That would be a good problem to analyze as a test problem.

I liked our dinner table example from a while ago because it had explicit compositional structure. We had different objects on the table—the cup, the plate, the cutlery—and their arrangement as the set dinner table. I think it's a great problem, but the issue is that we move the objects. I was trying to do an end-to-end analysis: how are we actually going to build something to move the plates and pick them up? There's a lot involved in doing that, and it's not really described in that problem.

In the first iteration, we said we just want to look at decomposing goal states, and the lowest learning module would output its goal state of moving that object to a location. The actual moving part would be just setting the environment to that state, like a subcortical or robotic system. So, the plate needs to be in a certain state.

I don't disagree with any of that, but I feel like we're just shoveling off some of the hardest stuff to subcortical—not subcortical, but the lowest level learning module would need a robotics approach to do path planning for moving the object. It could still happen in the learning module, but as a first step, we focus on decomposing the goal into subgoals instead of how to execute it.

I'm not going to object to that. I just find, maybe due to my personal needs and biases, that starting with what seems like a trivial, simple problem exposes difficult issues. If you start with a higher-level problem, you'll get bitten by it later because you didn't think through the whole stack of things that need to be done.

To me, the dinner table example is the simpler problem. The keyboard example seems to have extra things in it, like model-free policies, learning sequences, and associating. It's funny—I think the opposite. With the keyboard, I can actually imagine building something that moves and types, and I could make the whole thing work. With the table example, we can talk about the goals and hierarchical compositional goals, which is great, but there are a lot of unsolved, hard problems—like how does someone actually pick up the plate, hold it, and move it.

In the keyboard example, you wouldn't actually be manipulating the world, just moving the sensor. But you do need to press the key. I feel like both examples have the same hardest problem: how do you coordinate the finger or whatever? We have a good idea of how to generate goal states to satisfy a learned model or sequence, but the motor coordination part is the hard part. Maybe it's a bit easier with the keyboard, but even there, it's still challenging.

Maybe that's a good time to bring up that Hojae had written about learning modules focused on motor systems. I thought I could recap what we've discussed in the past on that, and then we could discuss it more.

Who's drawing right now? I can't tell who's drawing.

I'm drawing.

I have a quick comment on both of these approaches. I think about the step where you don't see a word on the screen, but you want there to be a word on the screen. It's this difference that drives something, right? I wouldn't call it a mismatch. What drives the action? It's the difference between the current state of the world and the goal state, which is to have that word typed.

I would use the word "mismatch" differently. To me, a mismatch means I expect something to happen and it's not there, which is not the same as wanting something to be there. That's a desired goal state, not an error. I don't type a word because something is wrong. I think it's a difference because the action brings those two things into alignment. Even if my goal is to have my hand in one place and it's currently somewhere else, I can compute the difference between the goal state and the current state and use that to guide the motion. I'm just minimizing that difference. Personally, I wouldn't use the word "mismatch."

I can reframe the vocabulary. It's a current state and a desired state, and the question is how to get from the current state to the desired state. We've always used "mismatch" to mean there's an error, that your model is incorrect or your expectation is not met. That's not the case here. Here, I have a goal state and I need to calculate how to get from the current state to the goal state. Maybe "discrepancy" or just "difference" is better. There's a desired representation and a current actual representation. It's not a discrepancy; if I'm in my kitchen and want to go to my living room, it's not a discrepancy, it's just a goal. I ask where I am now and how to execute to get to that goal. We can take a difference between these representations, and that's the calculation done to achieve the goal state. Yes, I have to determine what that delta is, but describing it as an error or discrepancy isn't quite right. It's more about how to get from one location to another, from my current location to a desired location. I don't think we should call that a discrepancy. It is a difference calculation, the reverse of path integration—the inverse of path integration. How do I get from point A to point B? I calculate that. It's a difference, but the next step is really critical: we've assumed that we just know the cause of letters appearing on the screen is key presses. At some point, you have to learn the causes of changes in the world. In this case, pressing keys or moving things on a table. I wonder if, when we actually train a system, we would need exploratory periods where hands just move objects around on the table, even without a goal, to create the association between hand movements and object movement. Now we have a cause—we know what type of motor action creates changes in state.

That part is interesting. Niels talked about this a lot. If I want to move something, like a fork from one location to another, that's a direct movement. When it comes to typing, it's a little weird because it's not clear up front that pressing a key will make a letter appear somewhere else.

That's not something you can easily learn by exploration. We don't just put children in front of a keyboard and tell them to play and see what happens. What children do is pick up objects, move them around, and learn direct cause and effect. Typing is different; you have to do one thing here to get something else to happen over there. If I press a key, the result is not tactile, it's visual.

When we learn to type, we don't just play around. We have an instructor or someone who says, "This is a keyboard. When you press a key here, the letter appears there." Someone has to tell you that.

That's a really interesting point, Scott, and I agree it's important. In terms of someone instructing you, that's how it usually happens, but in principle, a human who's never seen a typewriter or keyboard might interact with it and discover the effect of their actions. It doesn't take long for a kid to figure this out. There are a lot of advancements, and that's the same principle. Many cause and effect relationships are multimodal. It's a weird thing sometimes, and how you verify the effect and how that gets integrated is another complication. This multimodal aspect is going to be really important in our goals and behavior. It came up in the typing example: you visually locate the key, the visual system determines where to move in body space, and the finger moves there.

There are many things in the world where you take a physical action and the result is elsewhere or different, which highlights the importance of multimodal examples. When we're rearranging a table, do we have a body moving things, a physical interaction, and a visual system observing? Probably. That's also like typing. If I'm setting a table, I suppose I could learn to do it blind.

and blind people do, but it's much easier to do it visually. You're visually deciding if the plate is in the right position, and the vision system guides the hand to move things slightly this way or that way. When you're learning to type, you start with visually hunting and pecking for keys, and over time you develop a concrete model where you don't have to look at the keyboard.

The hunt and peck method involves finding the key, and the visual system locates it. It's worth walking through these steps, even if we don't use this example. The visual system identifies what needs to be actuated, and at that point, different body parts could perform the action. You could use any finger, your nose, or a pencil. When learning, you often use your dominant fingers initially, and training may require you to use other fingers, which are not usually as skilled.

This compositional breakdown involves having a concept of typing or expressing a word, which is then turned into expressing letters. In the learning phase, this is translated into the physical means of expression: speaking, typing, writing, or signing. Once you decide on typing, you determine which finger or body part will interact with the world—index finger, another finger, or your nose. When solving problems, we can approach them in many different ways, and capturing that flexibility is important in this task.

Regarding an earlier point, it's not that there isn't a model; it's that the model isn't singular—it's distributed across the hierarchy. When you say "a model of what?"—the model of the keyboard. It's not necessarily in one place; it's distributed across the system that translates words into letters typed on the screen. Most of us would say there is a model of the keyboard: a physical model of features at locations, where the features are keys. That model doesn't know words or letters; it just knows features at specific locations. It's a physical model of the keyboard. I'm not disagreeing with that, but the model may not exist in one learning module. It could, but I'm suggesting that all these different learning modules have different knowledge of the world, and the same thing can mean different things to different modules. We're already achieving abstract concepts, like a letter, which can be implemented in many different ways.

and the people implementing it don't know what they're doing. A letter, EOS, or layers—they're just doing something. They're being told to do something. I'm speaking off the cuff here. I would say you don't need a hierarchical model of a keyboard. You have a keyboard model, a model of words, and a visual model that can locate a particular object in space and say where it is. Then you have actuators, like a finger, that can go to that location. My point is that if you have all of that and zoom out to view it all, you have a model of a keyboard. These semantics, these words, are really important, Michael, so we should explore them. To me, the model of the keyboard is not the model of the entire process of typing—it's just the physical keyboard.

If you want to say we have a model of typing, you're saying we can have a system that lets us do the physical processes of typing without having that physical model of the keyboard. Instead, we have models of how to move fingers. I'm agreeing. At some level, you can view the system as having an understanding of a keyboard, even though it has no explicit model of a keyboard, which I think is the same thing. One of the things about the thousand brains theory, or the whole cortical column theory, is that every learning module builds its own models, and they're self-contained. The models that a cortical column has are self-contained. They don't have to be anything else. A model, by definition, is something that's inside a learning module. These models can be connected together in hierarchical ways, but even if I have a compositional structure, like the coffee cup with the logo, the model of the coffee cup doesn't know the logo. It doesn't know anything about logos. It's just a feature—some bits. It doesn't have any idea what that feature is or what it means. It just says at this location on the cup, there is something that someone's told me—it's this vector or this SDR.

It's maybe worth revisiting that point. One of the key points of the cortical messaging protocol is that we don't pass whole models as information at any point. We pass objects at poses, and that's key. One learning module can't say, "By the way, I have this model of a keyboard—do you want to take a look?" You only send objects at poses, but composed correctly, that's very flexible and actually helps constrain it. It's a somewhat custom vocabulary. I'm not saying you should change it, just that it's slightly counterintuitive to what the deep learning world would view as a model, where the whole system represents the model. You're right, Michael. It's exactly that—you hit the nail on the head. We could call it custom vocabulary, but in the language of brains, thousand brains theory, and Monty, models are things that learning modules do. As Niels just said, they communicate with each other, but we should avoid saying models span multiple levels and multiple columns. It's very misleading and would be confusing to people. It may be counterintuitive, but I think we should stick with that language and be very precise about it. Every time I reread the thousand brains thing, I come to a new perspective.

That seems like key stuff to say in a glossary. There are a lot of counterintuitive ideas here.

We all struggle with it. I struggled with this probably more than any of you. I went through all these same things.

That seems important to me. Think about these learning models as isolated units—they contain their own models, but they're self-contained. How they work together is the question.

Nobody else knows about a letter outside of the model of words. After that, everybody else is out, because those letters could be different things in different instantiations. All right, we lost track. We were starting to talk about the document that Hojae wrote.

Hojae, do you want to walk through the figure you were drawing?

Yeah, sure. Sorry, I lost my track of thought.

I was trying to go back to the hierarchy things again, but I mostly had a question about how we have these sensor modules, learning modules that we represent as long rectangles, but how the motor system comes in—that was the first one. I had a different or better thought just now, but I forgot. Let me try to walk through this one first, and then maybe I'll go for that one. I'll try to walk through what I wrote on the document as well, because that will also help me refresh my memory. I was thinking about the learning phase—when we're learning to type for the first time, and the goal is to type, let's say, "Numenta." This R2 will send R1, "Okay, type N," or—let's back up. Actually, I don't know if it would say "type"—it doesn't know about typing. It says we want the letter N. We want the—this learning module can't know about typing, sign language, and speech production. We can't expect it to know that. It just says, "My next state I want is an N."

So instead of "type," have "state of produce N"—make an N someplace. Then the subtool that will get passed to R1 will be "change, be in the present, have the state of N, of Numenta."

Going back further, when we're learning, there's no goal—just pure exploration. R1 will move around, sense, and the idea I wrote on the document was that it will have a model of a single key, like N, a model of a single key U somewhere out there, a single key N somewhere out there. 

So is this a keyboard model? Is R1 a model of the keyboard, or of a single key? A single key? Is this for a three-level hierarchy? Is this below the model of the keyboard? I'm still thinking about just two, where we're just exploring—there's no goal. I'm just moving around in space, sensing N, sensing U, sensing M, and putting it into my memory.

Are you storing these keys at locations, or just storing features of the keys?

I know you've made a comment about the locations. At first, I thought I was storing them in their locations as well, but it can't do that, right?

I think it comes together as a keyboard when we have the goal of wanting the state of Numenta, and when R2 tells N to create or sense N, the motor system somehow knows where to go to press this N. That requires a model of the keyboard to store where the key N is, not just how the key N feels. The keyboard might be anywhere in the world; you have to know where the key is relative to the keyboard to move there. You could do without the keyboard model, but it feels less efficient. To recap, for the N learning module or the key-specific learning module, its output is the letter at a position in body-centric coordinates. That position isn't fixed; it depends, as Viviane said, on where the keyboard is in the world. In theory, the keyboard could have a goal state, like pressing somewhere specifically on a key, also in body-centric coordinates. My issue is that this would be a lower level in the hierarchy, and you would still have the model of the keyboard to more efficiently find the keys.

When I type, I depend heavily on those little ribs on the keys—that's like an anchor coordinate. We had a lot of discussion about it. It anchors in body-centered coordinates where the keyboard is and where your finger needs to move. This is the clue that when you learn to do rote things like typing, you do it all in body-centric coordinates.

There's a model of typing, moving your fingers in body-centric coordinates, and it's not using an allocentric model. That's what those little divots are for: you get your hands in the correct position, and then you're done. To be devil's advocate, could it be a local coordinate system just for the hands? Of course. When I say body-centric, I don't always mean the torso; it could be the eye, a hand, or something else. There are different neurons. We haven't figured this all out yet, but the point is that when you've learned to type, you're not using a keyboard model. A keyboard model would be allocentric—the keyboards can be in different places and orientations. Once your fingers are registered on the keys, there are no allocentric calculations; it's all relative to the hands, just playing back motor sequences.

About the location—where to go—let's say when we're learning, we start the hand at zero, zero, whatever that is, and move around. For the most part, we don't sense anything, and then it gets to N. Can it keep track of the overall displacement so it knows the location of N relative to where it started? We started at zero, zero, moved around, and finally at N, it knows it started at zero, zero, and based on all the vector calculations of movement, maybe the location of N is stored in the motor system. We are introducing complexities, Hojae, which I was hoping we'd avoid. Your hand is moving around in space, randomly sampling, and I'm not sure that kind of question is fruitful right now. I was trying to isolate some components of this overall problem, and that's why having the grounded hand on the little nibs on the keys was helpful. I don't think we just randomly explore looking for keys.

Maybe for a child or anyone starting to type, the first time you see a keyboard, you make a visual model of it, not a tactile one. You look at the different features; it's difficult to learn the arrangement of the keys because there are many and they're close together. It's not easy to learn. You make a visual model of the keyboard, and the visual model instructs the fingers where to move: go to this location, go to that location. This is a common task in setting tables or doing anything—you use your visual system to locate where you need to go and implement things. The visual model of the keyboard locates the places we want to go based on the letter we're looking for, and then instructs different parts of the body to move to that location.

Fingers aren't just roaming around finding letters. We look at a keyboard, study it, and try to remember where the different letters are.

If we have the keyboard model and know the locations of individual keys, this would be a visual model.

Yes, I think it is. Then the question is, how does the hand know to go to a location? I'm confused about locations and how the visual system locates something in body-centric coordinates. It's somehow relative, and then it has to say, how do I move a part of my body to that thing?

The finger that's going to touch the key does not know it's touching an N key at that time. The visual system says, "Go here and press that," and the finger goes there and presses it.

there is, as Viviane mentioned earlier, a sense that this is trivial, but this is how it works. I had some thoughts about this translation step. There's a key in space that needs to be pressed, so the desired state is to have that key down. The motor system, as you pointed out in the doc, Jeff, could do this with chopsticks, your left hand, or your right hand. If you think about computing the distance between a current state and a goal state in space, the right hand—let's say I'm flipping a light switch on my right—has a smaller distance to the goal than my left hand. Many things in the motor system could be saying, "I can do that, and here's the length of the trajectory it would take." What actually ends up doing it is the system that chooses the smallest possible definition, which often seems to be the case. 

But if you're holding a coffee cup in your right hand, you might not want to put it down, so you can't reach across with your left hand. It seems obvious that the body can locate a point in space, though it's not always clear what it's relative to—some body-centric coordinate. Then you want to determine the trajectory from a part of your body to that point. It could be from your right hand, left hand, foot, or toe. We're able to do this very quickly just by thinking about one part of your body at a point in space, and then your body knows how to get there. It calculates this automatically.

You can reach with one hand, the other hand, use your knee, or use your elbow on a doorknob as a lever. Let's say you consciously choose to use your left hand, even though it's further away, maybe because your right hand is holding a coffee cup. In the typing case, the letter U is closer to my right index finger, which is what most people do when they hunt and peck—they use whichever index finger is closest. If we are trying to implement a system, this could be a potential mechanism: you're just computing deltas, like "I can get you in that state, but I'm X distance away," and another column says, "I can get you in that state, and I'm Z distance away." The question is, what is the policy for deciding this? I could tell you, "Scott, your job is to type everything with your left pinky." You could do that, but you'd be slow, looking at each key as you go, just like hunt and peck but with your left pinky. The question is, what do I naturally do? Maybe I pick the thing that's closest, or maybe I always pick my right hand, even if it's not closest, because it's more dexterous. I don't know the answer, but clearly we can do it with any body part.

It feels natural that the goal state—someone is pressing here or picking up here—is projected widely, and if there's a low delta, a learning module might answer that. In general, a learning module, when it receives a goal state, knows the models it knows and has some sense of whether it can achieve that goal state. If it's already holding something, it's just not going to answer that. That feels like a more unconscious reason for using one hand rather than another. There's probably some prefrontal cortex involvement that recruits the basal ganglia if you want to specifically avoid using something, which is more like inhibition rather than inability. 

This brings up an interesting mechanism. Imagine we're trying to express a letter, and there are all these modalities: you can speak it, sign it, type it, or type it on different keyboards. There are many ways to do it. I could even ask, "My hands are busy, Niels, would you touch the N key for me?" That's another way. You can imagine that this message must be transmitted to many learning modules in different modalities, and somehow the brain is able to direct it—pick one. Which one am I supposed to use right now? Am I supposed to speak the letter or type the letter? It's a mystery. We have these abilities to do all these things, but at any moment, we're only doing one of them. Even then, if there's an obstacle, it might force us to use another one.

That has to be part of the architecture. The overall architecture must include the idea that there are many different ways to implement a particular task.

and then there has to be some sort of policy about deciding which one we're using right now. I'm adding this to our list of things we might want to solve as we get into this problem. We might want to have at least two ways of implementing the same goal, such as typing letters. If we were to implement the keyboard example, it might be useful to say we have a left finger and a right finger. That's the simplest version: we only learn with the left finger or the right finger, but eventually, the left finger develops the model-free behavior. The finger is also able to respond, so if you inhibit the right finger or the right finger cannot respond, the left finger can be recruited. Let's get back to action policies to decide which one to use. Having the ability to switch between these would be a key component of any goal-oriented system. Whether we call them two fingers or two actuators doesn't really matter—they're independent means of doing this. I think it's important to include that as part of the solution.

It strikes me from an earlier comment that these seemingly basic things—moving limbs and sensors around in space, searching the space of possible actions for the one to execute, dealing with spatial constraints—are all significant problems in robotics. We're going to solve them, but getting these basics right is crucial. Even as a human, you probably don't learn to type before age five because you need a lot of experience learning how things in the world are connected and how to move your body. Solving these lower-level problems, like locating limbs in space, is essential. We have to be careful—there's a tangent here, Michael—but a large part of why it takes humans so long to develop is that our bodies are changing during that time. We're growing, and anything we learn changes the next month when we're bigger. You have to relearn these things repeatedly because your body is changing. It's not like we have a fixed body; it takes years to learn how to manipulate things because everything is changing—the brain and the body are growing.

I think we can assume, or at least shortchange that attempt somehow. We can create problems where we've endowed the system with abilities that aren't going to change, that are baked in—model-free, subcortical functions, whatever they may be. I would counter that with the example of a deer that learns to walk in an hour or two, and its body is still growing, so it solves that problem very early. That's true, although that's a non-cortical function. Walking doesn't really require the cortex. I'm just saying that part of the reason it takes humans so long to learn is that we come with so little innate knowledge. Partly that's a deliberate kind of tabula rasa from evolution, but also, we've developed as much as possible to fit through the birth canal. If we could, we'd probably be like two years old before being born, and that's what happens with some animals, but our brain can't—our skull wouldn't fit.

Two-year-olds run around really well, as I know. A lot of that development happens in utero for animals, but for humans, it has to happen afterward. My bigger point is that rather than worrying about keyboards—which is a good end goal and a good way to think about this—maybe we should focus on the simpler, more narrowly focused problem of just moving a finger to a point in space. My preference would be to say that is a subcortical function. The actual coordination of muscles to move anything is really complicated, and a lot of that is subcortical. I think it's okay for us to assume we don't have to learn that. It would be great to assume we have a system that can perform certain basic functions. I'm not sure whether that's acceptable. We talked about that earlier. To move a finger requires many different muscles contracting—the coordination of the limb. For the inverse kinematics part, we can say that's handled, but the constraint solving—like moving your finger to pick up a glass when there's a wine glass in the way—requires going around it. That's definitely cortical. We have to solve that problem, which would be at the scene level of understanding.

That doesn't easily solve it, because you have to understand where your arm is in space and what space it occupies. That's almost all done visually. So much of what we do is locate things visually and then move our limbs to those locations, with the visual system verifying that we're doing the tasks correctly. That has to be part of this, or almost certainly has to be part of the problem and solution we're looking for. Is a first-person shooter a good starting point or simple model? You have a target on the screen, you adjust your cannon to move the target and fire at what you're aiming for. I think we're looking for tasks that can be solved in different ways and have compositional structure. Niels' example of making coffee is a multi-step, multi-task, decompositional problem, which seems much better than just training a neural network to be a really good first-person shooter. That doesn't require a model at all; you can just use deep learning. But to achieve it within this model system, I think the coffee example is a good one.

Go ahead, Jeff. Another twist on the keyboard example I've been thinking about that might simplify a few problems is playing the piano, where you also have a keyboard. Whenever you press a key, you hear a different note, so you can immediately tell which key you pressed. It's clear we have a model of the keyboard because it has a repetitive structure, but if you need to play a new melody, it's a slow process at first, even with sheet music. With practice, you can play the melody very fast, similar to how we type words quickly if we know them. If it's a random sequence of letters, like Scott's example, we're much slower. The piano example is almost identical to typing on a keyboard; it has all the same properties, but we're less practiced at playing piano than typing, unless you're a piano player.

It always struck me as odd about playing piano, which I've done on and off. When you get good at it, it's like language: you know words, and on the piano, the structure you learn that's equivalent to a word is more flexible. The better you get, you can see an entire bar of music, various riffs, and you see the whole thing at once. An expert piano player can look at several bars of music, recognize it, and play it back without thinking about all the notes. They just see the whole structure, like a complicated word, but there are many of them and they're not well demarcated. It's interesting to think through because it makes it easier to imagine typing a new word or learning a new melody. It's a tedious process where you use the model of the piano to figure out where to press next. That's where we want to start with the learning module: use the model to go to the correct key and press it. The piano is identical to typing on the keyboard, just more difficult. That's how I view it. I'm thinking about it now, and it has all the same attributes. That's why I'm bringing it up.

Even if I gave you a new word you hadn't learned before, or a long word you haven't memorized, it's a single sequence, and you go through it piecemeal, a syllable at a time. Piano is just like that. I'm not a piano player, but as a touch typist, I locate the little nib and never need to look at the keyboard. As a piano player, because of the size of the keyboard and moving your hands freely, you have to visually locate keys. If you're a beginner or intermediate, you look more, but the better you are, the less you look. A real expert piano player does not look at the keyboard at all, even when making large hand movements. If they had to look, it would take too long. They also have to read the sheet music, but they don't read every note. They don't have to look at the keyboard while reading.

Good piano players, unless there's something unusual, never look at the keyboard. That's impressive from a robotic manipulation perspective, because you have to make large moves with precision.

Even for an expert piano player, it would be hard to have their fingers at center C and just play the highest D without looking. You tend to evolve, walking up and down, re-anchoring as you go. It's like typing random letters—you have to think about them. I find the numbers at the top of the keyboard hard, and I can't type those as proficiently as normal letters because they're bigger movements.

It's a bigger distance, and also how often you have to type them. For example, some of my passwords have numbers, and when I type those, I can often do it without looking because I type them every day. Michael, you bring up a good point: it's a physically impressive feat that an expert piano player can move their hand across the keyboard and land on the right key with the right finger. You're using all the inner body senses to do this. Your body has been tuned to do this very precisely, like an athlete making precise movements to achieve performance, whether playing table tennis or something similar.

I like the idea of the long stretch to hit a key that's far away. Most pros will glance quickly if they're reaching a distant key. This could relate to the physical restrictions of grid cells—different modules have different degrees of granularity. When I'm close, I'm dedicating the most granular grid cell modules to a small area, but farther out, the spatial representation is coarser. Path integration also gets noisy, and proprioception isn't exact enough to tell precisely where your hand is in space without tactile feedback—just muscle contraction. It's hard to be precise enough.

From Monty's point of view, this isn't a problem we need to solve. We're not building these systems out of muscles, bones, and tendons. We can assume that if we can locate something in space, we can accurately and quickly move an appendage to that location. We don't have to worry about how piano players move their hands so far.

It's like moving a robot with high-quality sensors and reproducibility. That ability is almost certainly subcortical in the brain, probably involving the cerebellum more than anything, not the cortex. The cortex doesn't do anything precise, so we can avoid that problem. We can marvel at human ability, but here we can just assume that if we have the location, we can move something there accurately.

What were you saying, Hojae?

I just wanted to ask Niels if he wanted to say something about the motor system. Twenty minutes ago, you mentioned something.

That's right. Since you brought it up in the document, I thought I'd mention the open questions and what we briefly discussed. To recap, the key anatomy is that every cortical column, and therefore every learning module, has direct motor outputs. These are potentially primitive and will recruit whatever they're connected to. Another way to implement a policy is through the goal state, probably L6 top-down feedback to other learning modules, potentially recruiting motor cortex learning modules. Those learning modules, or cortical columns in motor cortex, would be modeling objects of the body, like the structure of the hand or arm.

That's a high-level description. We've talked about this, but coordinating it is where the difficulties arise.

If you've projected a goal state like "I want pressure here," there's the question of how we know which learning module answers it. For complex movements—wrapping your arms around something and pressing somewhere—how do all the modules get coordinated? One useful example is grasping objects. As we discussed last time, for infants to learn the pincer grasp takes a while, but if you were given a really weird object—oh, actually, I've got the perfect example.

a piece of coral—so now you want to grasp this. I have a lot of subcortical policies I can leverage to a certain degree, but let's say I was younger and still developing that. It feels like, to a certain degree, as long as we're doing it sequentially, it simplifies the problem a little bit. You put one finger, then another finger. We still need to learn the cause-effect relationship that Scott mentioned: how do we understand that pressure is needed to grasp, or that this will enable us to manipulate it? That could be model-based, directed by the idea that there's a location in space where a finger needs to be. The model for the fingers—the direct motor output—could help in this case. As I touch it, I might feel that a finger doesn't have good contact, and that finger might then decide to readjust for better pressure. It's a mixture of getting your fingers into the correct position through sequential planning, and then using direct motor output to readjust. If you do that repeatedly—if, for whatever reason, I wanted to pick this up every day—it becomes more model-free, whether cortical or subcortical, and your hands naturally form to that object in parallel without thinking about it finger by finger.

That coral is a perfect example of something so novel that you don't even know how to begin interacting with it. You have to think carefully about where to put your fingers to pick it up. That's exactly how it is when I try to play piano, because I was never good at it. I have to do it finger by finger. This is the same with typing. We're getting at some core ideas here: we have novel ways of expressing behaviors that require a lot of attention. We have to decompose them thoroughly, one step at a time, and then we're able to combine them quickly.

I want to return to this idea—I'm stating it so I don't forget. What we have to pass down the hierarchy is both the specific thing we want to achieve and the overall goal. You need the overall goal so the next region down can associate and learn a sequence of individual elements as one thing. It's not sufficient to pass letter to letter; I also have to pass down that we're trying to implement this word—just some SDR. It doesn't mean anything; it's just an SDR, the overall representation, like the object ID or goal ID, and then the individual elements in sequence. The lower region can then say, "Okay, I can handle the whole sequence on my own." You don't have to think about it. That's what we do when we type or when we key. The sequence learning happens within one thing; otherwise, we're passing two levels of object—a feature and an object. That's my observation, because otherwise, how would I learn a sequence? Let's say the sequence is A, B, C, D, or the word Numenta.

In the beginning, the word model has to say N, then U, then M, then E, then T, and so on. Ultimately, I want the lower model to do the whole thing on its own. But if I just say "type an N," it doesn't know that's the word Numenta. It doesn't know anything. So I have to also tell it, "By the way, in the context of this SDR—don't worry about what it is, but the context is this SDR—let's say it's the word Numenta, but I'm going to pass you the individual elements for it."

We're already doing this with the feedback connections. We thought of them before in terms of object recognition: the higher-level module passes down the object it is sensing—like the word Numenta—and the lower-level module can use that as context for the letters it's sensing. It fits perfectly here, since we're passing the high-level object and also the goal states for the individual components, the sequence of letters.

Yeah, it's like the information that goes to L1 versus the location L6 on the site.

Exactly. There it is—L6 and L1. Yes, it has to do that.

Sometimes, if you don't do something repeatedly, it's not going to be able to learn it. But if you do something over and over, it's, "Okay, I've seen this pattern before. I'll learn it."

To avoid all the complicated motor stuff, could you make a simplified keyboard and keyboard controller where you have the notion of a finger, and a finger can reach a certain set of keys? You still have a system with constraints, but the higher-level part has to figure out, "To get a U, I have to activate the left index finger to position three." That's the way I've been thinking about it, Michael. We used the keyboard example, but I'm not thinking we physically build a keyboard like this. It's just a placeholder to think about concepts.

In my mind, this is some abstract object with features at locations. We can call that a keyboard to help think about it. Then we have some actuator that can move to different locations as instructed, and we can call that a finger. But we're not really building a keyboard and a finger—these are placeholders to describe concepts.