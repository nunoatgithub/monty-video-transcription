Yesterday I said I had a few small things to talk about, which I can do. Some of us are thinking about hierarchical actions and goal-oriented behaviors, so I've been considering aspects of that. The thing that's really puzzling to me is how we take an action we know how to do, or come up with one, and implement it using different parts of the body. I might use my right hand, my left hand, my eyes, or even try to get Hojae to do it by talking to her. There are many ways to achieve a goal, even though the goals are the same. Somehow, we're able to decompose this in the hierarchy and figure out which part to use and how to approach it. If I'm trying to open a door and my right hand is occupied, I might transfer what I'm carrying or use my left hand. It's puzzling how this all happens. The good news is that we're starting to have a real understanding about hierarchy, at least in terms of compositional objects. I think the key insights there will apply here as well. I have one small topic related to this, and it may not be exactly obvious how it's related, but I'll present it. For me, it was a nice little insight or at least some clarity. 

Let me share my screen. Here we go. PowerPoint presentation. This is a pretty simple thing I put together.

One of the things that is conventional neuroscience dogma is if you look at two regions in the cortex—I'll label them one and two, which could be V1, V2, S1, S2, whatever—two hierarchical regions, you start out with the first one having small receptive fields. The cells in that region respond to a small part of the retina, a small part of the skin, or something like that. I'm showing five columns here, but even though they're separated, that doesn't mean anything—just showing you some columns. You have these small receptive fields, meaning the cells respond to a small part of the sensory surface. You have simple features; this goes back to Hubel and Wiesel—edges and so on. Then you go up to the next level and find cells that respond to a larger area of the sensory organ, and the features they detect are complex. Very quickly, it becomes difficult to characterize these features. In the beginning, they're called edges, but at higher levels, there are all kinds of complex patterns, and people spend a lot of time trying to figure out what they are. The basic idea proposed a long time ago was that these lower columns, each detecting a small feature, converge onto cells in the upper column, representing a larger area of the retina or skin, combining simple features into more complex ones. That is still the standard dogma in neuroscience. But it never really worked for me. It looks simple here, but in reality, if you draw all the arrows, every column in R1 projects to many columns in R2, and every column in R2 projects back to many columns in R1. It's not even clear if it's diverging or converging. There are a lot of problems, and this never made sense to me. Every time I tried to figure out how information would flow from a column to other columns, I had difficulty with it. I think this is similar to what's going on in deep learning networks, and it works there, but I don't think that's what's happening in the brain. I could never get this to work in a way that made sense for building a model.

The Thousand Brains Theory and our recent work on compositional objects show a very different interpretation. In fact, you can get the same properties with just two columns, one hierarchically above the other. The theory here is that the complex features in R2 are caused because in R1, there's spatiotemporal integration. The simple features in R1 are detected, but the output of R1 is typically a more sparse, distributed representation that's not easily characterized. Because R1 combines inputs over multiple points in space and time, it creates a more complex feature, which we would call the object output of the column. The increasing size of the receptive fields in R2 can be due to two reasons. One, R1 has already done most of the work, integrating information over a spatial area and passing the result to R2. But there's probably something else going on as well. I wrote that increasing receptive field size in R2 is caused by spatiotemporal integration in R1, and/or changing scale.

I've always believed—though I have no proof, just some evidence—that the scale of R2 in terms of its reference frame is larger than R1. We see this in the entorhinal cortex, where adjacent grid cell modules get larger and larger. What does it mean to get larger? It means the rate of change in the grid cells, relative to the movement vector, changes. If you're moving at a constant velocity and the grid cells change more slowly, that's a larger scale. You have less resolution in your space, but a bigger space. I think that's happening here, though I'm not sure if it's required. My first assumption is that R2 will naturally model larger spaces because of the way the system is set up. It's not learned, just like in the entorhinal cortex, but also, R1 is already passing larger-scale information to R2.

Now, the important thing in our theory was that the reason this works is because R2 and R1 are co-located—they're looking at the same physical space in the world. They're both modeling the same point in space: one is modeling a parent object, the other a child object, and we can assign the child object to the parent object on a location-by-location basis. As the sensor organ moves, both of these columns move with it. Interestingly, R2 would be changing slower than R1; the R1 location might change, and R2's location might change less or not at all. But I think that all works. The key to the whole discovery of getting all the coffee cup variations working was that these two are looking at the same point in space, so we can assign them at any point on the cup, but the same point on the logo, something like that. That was the key. We can achieve all these larger RIFs and complex features and smaller RIFs and simple features with just two columns. We don't need more than that; we don't need any convergence or divergence. We'll come back to the actual connections in a second.

Of course, we don't just have two columns—we have multiple columns, and so we have this concept of voting, where columns in the two different regions are coming to a consensus. But I don't think that changes anything. Each of these columns in R2 and each in R1 are going to reach a consensus about what they're seeing, but the learning still occurs on a column-by-column basis. The horizontal voting is just to make sure that we get to R1 and R2 and what object they're looking at more quickly. All along, the columns are still basically one-to-one, voting on location to location. This doesn't mean it's happening in every column in V1 or V2; it just means in the fovea area, at some point where these columns are looking at the same object, it would be occurring.

That's basically our theory about how this is occurring, as opposed to the conventional theory. The one on the right solves a huge number of problems that no one ever even tried to attempt on the left side, so we have great confidence that's correct. This was a really great insight. When we're talking about any kind of compositional structure, especially if we're going down the hierarchy of motor behavior, we should be thinking along these lines. It's more complex than this; in a second, I'll show you.

This is a figure from the paper we're supposedly going to finish sometime. It shows three columns in three different regions: R1, R2, and R3. The feedback connections we were just talking about are these purple ones, where layer 6a—we believe that's the location in this space, in this column, in all these columns—so the location in R3 is projected first to the location in R2. We know this is a very narrow projection; by narrow, I mean that R3 and R2 are looking at the same point in space. We're associating the location in, if you will, the coffee cup with the location on the logo. We're learning, and we have all this worked out in detail, but we're associating at this point in the cup, you should be at this point in the logo with an orientation and scale, things like that.

We do have these long-range connections in layer one, which don't fit what I just talked about. They go beyond this first column, so we have this very targeted projection in layer six, one-to-one, as shown in the previous slide. That's the targeted connection here, and that's the forward one of this one. Yet, we have a broader spread up in layer one that goes beyond that. That's a little harder to interpret, but it seems to be important. So, what do we make of that? Clearly, R3 is able to tell multiple columns in R2 something about what's going on. I've often wondered, how do all the columns learn at the same time? There's some capacity to hold the whole system, so I can't learn everything, but these three columns can't be just learning specific points. Everyone has to be able to take the role of any point in space because the object can move around. I always thought maybe this is a learning signal—like saying, I'm passing to you, to all the other guys, you could learn; if you're not getting any input, maybe I'll help you train or something like that. I don't really understand it.

It may play a central role in how we are able to model and execute behaviors in different modalities. Some of this we understand, some we don't. That's the end of my presentation today. This idea in our theories is contrary to what almost everyone thinks in neuroscience. It was helpful for me to remind myself, when I think about not just compositional objects but compositional behaviors, that I have to think in this paradigm. Somehow, we have to work with these connections. There are no other connections besides this that we know of. There are some long-range connections from other places, but these are the primary ones. Somehow, we have to make this understandable. That's really it. We can talk about this. Maybe everyone got this and knew it already, but just writing it down like this was helpful for me and made it clear. I was starting to think along the lines on the left here—how do I spread from this column out to here? I have to look at it like this. These are the ways to think about it, where it's tempting to think about it another way, but I don't think that's right.

One thing I was just thinking about while we were talking was that we know there's fan-in connections to higher-level columns, and we've accounted for this partially with the current setup. Sorry, on the previous slide—do you want me to bring up a paper? I have the paper here.

I don't know if we actually had a plan, but maybe I can just go on with this. Do you want me to stop sharing? No, this is great, because then I can write on this. Okay, I shouldn't interrupt you. Keep going. Oh, you're going to use your iPad? Yeah. I think you drew one, Viviane. If you want. Oh, no. You draw them off to the side there. Can you see? Oh, okay. It does come, but it's slow. You need to be patient.

I don't know if this really helps, but I'm just thinking, why is it so slow?

We know there are fan-in connections, and right now we're generally still assuming that there's some degree of that. How do we know those fan-in connections? What are we talking about specifically?

It's not just the receptive field or the physiological properties; it's actually the connectivity. I remember looking that up. I was trying to remember whether the details matter. Just like in my neck side, there's fan-out, but they're not equivalent. There's a specific connection, and then there's a fan-out. I was trying to recall if I've seen what we know about those actual connections. All I can recall is that people assume they look like this based on these properties. I thought that at first as well, but I'm pretty sure I eventually found the paper where they talk about the anatomy. 

What I was going to get at is that this co-location assumption seems reasonable in many cases, but in some ways it might constrain us. If we apply the same transformation we're doing for voting, then that's not really an issue anymore. The same kind of transformation could account for any difference in the pose of the sensor module for this one as for another.

What's interesting about this, Niels, is that I think I can do the entire compositional model using two columns. So you potentially don't need more, but that tells me the other stuff, if it's there, is not a requirement—it's helpful. Voting is one example. Another example is helping these columns learn, training columns that aren't actively involved, so we can train multiple columns that aren't experiencing anything right now. There seem to be other reasons, but it's not a fundamental requirement. If I can do the whole thing in two columns, it puts it into the same category as voting—there's something else those connections are doing, but they don't seem required.

By the way, I don't know if we can do the object behavior with two columns. I'm pretty sure we can do the compositional structure modeling with two columns, and I'm going to try to get it to work with two columns because that would be a beautiful result. If nothing else, this was more an aside. I just thought it would be interesting to quickly chat about that. I don't think this relates to hierarchical policies, because ultimately those depend a lot on the top-down connections, and we know what those look like. Like you were saying, those go to L1, so by definition they cannot give some sort of location-specific information, except for the co-located reference frame. The co-location wouldn't be meaningful if the columns were looking elsewhere. I don't really understand it. I think it will become clear shortly, but at the moment, I'm confused by it.

At one point I said those projections to L1 could be just about what object you're supposed to be observing, but not the location. But if it's coming from layer 6A, and we believe those are specific locations, then I'm providing a specific location to L1 to a column that's not at that location. I don't know what that means.

Here's an idea: remember, if the space of R2 is larger than the space of R1—imagine it was quantized—one location in R2 would correspond to multiple locations in R1. Imagine R1 is looking at three different parts of the logo, but those all fit within one location in R2; R2 doesn't have the resolution to distinguish those. Basically, R2 is saying, "Here I am at my location, as best I can resolve it." It's like saying, "This is this location." That's what we're doing at the moment: R2 has a lower frequency, a coarser model.

Maybe I can share the figures we took a while ago, if you don't mind; maybe that would be helpful. Also, can I ask a question I've been holding for three weeks? Since the last time you presented the hierarchical or compositional objects—and I know this goes against quite a lot of literature—but does it have to be true that R2 always has a large receptive field? No, I don't think it has to be. The reason I'm making that statement is maybe R1 actually has the largest receptive field, but as we go higher in R2, R3, when we're trying to do something interesting, we need to focus. I can be looking at this laptop; I know that I'm still here, but it's hard for me to pay attention to everything that's here.

The theory is that the higher level can say it's a laptop, and the lower levels can take care of all the details, so you don't have to be conscious of it. A couple of other observations: typically, the first two regions ending with central medulla are about the same size—V1 and V2. In fact, V2 is slightly larger than V1, at least in some studies, but they're about the same size, so we don't see any kind of convergence in space size. Logically, you might say you have the same number of columns, which is interesting. I'm just separating that out as a separate piece. You do have convergence after V2; it gets smaller after. What gets smaller? V4 would be smaller—just the number of neurons, presumably the number of columns.

Maybe the takeaway is that this works no matter how many columns you have in a particular region. Somehow, we can have small columns on top of bigger columns, and we probably get a bigger column on top of small regions on top of bigger regions. We might be able to have B2, if B2 is larger than B1, a slightly larger B2 over B1. That's tangential to your question. It feels logical to me that receptive field sizes get larger as you go up the hierarchy—the receptive field sizes of the inherent ones in the grid cells. We know that the actual cells themselves do that, but that could be just for integration.

I've asked myself the same question: does R2 grid cell space have to be larger than R1? It's not clear to me. Then I ask myself a different question: how would I feed in an R1 from one modality to an R2 of a different modality? At some point, we're constantly combining modalities in different ways, and I haven't answered that question yet, but I have some interesting thoughts about it. For example, if I have a movement vector coming into R2 representing movement of one sensor modality, and a movement vector representing a different R1, a different sensor, what would we learn? I don't know yet, but your question is a good one. My assumption is that these spaces get larger as you go up; it seems to make sense to me, but maybe you can make the argument why it doesn't make sense to you.

Just to keep in mind, when we build on top of this, maybe there's some assumption, but also think about it the other way: what would happen if the receptive field is closed—not getting smaller? With the cup and the logo, maybe R1 can see the cup, but the logo part is the interesting part. Anything that's really small would benefit from a smaller space. When I'm reading the smallest font I can possibly read, only the region with the highest resolution representing a small area of space would work, and I've always assumed that would be the lowest one down. It makes sense to me; it fits what the cells look like.

Also, if you think about behavior flowing down the hierarchy, I have some overall behavior—maybe signing my name. At some point, I have to actually move my fingers the smallest amount to get the little detail. I can do that with my hand, with another tool, with a pen; there are many ways to do that, but it feels like the large-scale thing is at the top, and the actual motions to make each letter are at the bottom. It seems intuitive to me that we go from large scale to smaller scale as we go down, but that's just an intuition. I'm giving hand-waving arguments for it.

Maybe we can think about the goal of making coffee. Niels presented last week that the goal comes from some sort of goal region. It could also make sense that it goes from near the subcortical to the cortex. Somewhere in the subcortical is telling me, "I want to get energized," and we make the decision to make coffee. Eventually, the higher level kind of gets...

As information goes, a higher level figures out, okay, here's a plan. The plan gets executed differently as you go down the hierarchy. For example, if I'm trying to open the door and I have something in my right hand, I might use my left hand, where I normally wouldn't. The higher goal stays the same—open the door—but at any moment, I have to adjust. Maybe my pen is a little longer or shorter in my hand as I'm writing, so I have to adjust my movement space. It feels natural to have a high-level goal that gets decomposed into lots of small pieces. As you go down, those small pieces are, in some sense, optional; you could have done it different ways, but the high-level goal stays the same.

The high-level goal is coming from the subcortex. We can just assume that someone is assigned, maybe like a desired behavioral state. That could be subcortex, or it could be you talking to me, asking me to do something, which isn't like "I'm thirsty," but "I want to do what Hojae asked me to do." From my point of view, for starters, we can just say we've assigned a goal to this thing. It could be an outer loop of Monty. Now we want to decompose that into a series of actions, and that series of actions may vary, almost every day, moment to moment, based on the circumstances. That just feels like the opposite of compositional structure going up and decomposing going down. I'm going to work on it; you can think about it, but I'm not going to spend too much time on it. Fair enough.

So, do you want to show something? Oh, yeah. I just took it down because we were talking. This was what we drew up after some brainstorming by the Bay sessions, I think a year ago. This is how it's implemented right now: the high-level learning module has a larger receptive field. It also gets direct sensory input, or it can get direct sensory input from a lower resolution, larger receptive field. The columns or learning modules—the assumptions from the lower level that it gets input from—are the ones that have receptive fields co-located within its larger receptive field, but they could be at different locations. Learning module 3 would not be able to distinguish these locations, but they can all feed into it. The assumption here is that the three lower learning modules are modeling the same object, but at different locations. If they were the logo, they would all be saying, "It's a logo," but they're at different locations on the logo. They might not send input at the same time to Learning Module 3; they just have a connection.

If they get noise, or one of them is not sure about what it's sensing at the moment, maybe they don't send input. But let's take the assumption that they're all modeling the logo. In that case, the inputs to learning module 3, even though there are three different learning modules—1, 2, 0—they're all basically saying the same thing: "It's a logo." They're not saying anything about location, just that it's a logo. Those three inputs aren't really converging; you're not creating something new in Learning Module 3. They're all saying the same thing, and if any one of them dropped out, it would still work. That's the second picture—a rough drawing showing that they would detach the different locations on the dendrite. The one closest to the soma would be the one that's at the center, co-located with its receptive field from learning module one. If that one is certain about what it's sensing, it can activate the cell; otherwise, it might look at the actual sensory input or get input from the ones a bit more on the side of its receptive field. It works with either of them. This one is the most confident, if you want to think of it in those terms, but it's a mechanism to add a bit more robustness into the system.

You don't actually have to put them in order on the dendrite. You could, but it doesn't have to be, because as we look at different environments, the one that's most likely to always be correct is zero. The other ones—sometimes it'll be the same object, sometimes something different. You'll naturally form more synapses to learning module zero than to one and two, unless one and two are consistently the same object. The neuron will naturally form synapses to the things that are most consistent, whether you arrange them on the dendrite or not. If learning module 1 is sometimes the same as learning module 0 and sometimes not, maybe those synapses just won't form, or there will be fewer of them. The system is very dynamic, so it's not clear that they have to be in order on the dendrites. Otherwise, I would just keep the picture the same. To me, it was more the general idea. In Monty, we don't have dendrites anyway, but the idea is that if it gets input from all of them, this one is the most reliable. Learning module one or two could be on a different object if it's on the border or might be noisy. In Monty, I don't know how you do that—do you have different weightings or something like that?

There are different options for how you could select which input to use or how you want to weigh them. A learning module can get input from all these sources, but it could weigh them differently. It's up to the person who implements it—how you want to weigh it, or if you want to just pick the winner. 

I was thinking about object motion and where and when that's computed. One thought is that we've got these converging inputs from multiple LMs that are spatially distributed in one layer or tier of the hierarchy. Could it be that even if a corresponding column in the second tier of the hierarchy has roughly the same receptive field area, it's using its converging connections to compute object motion?

I think that's a good argument. I'm not sure how I would implement that. My proposition today is to solve this problem without using converging inputs from multiple columns. We solved sensorimotor input by initially ignoring the hierarchy and voting. It had to happen in a column, and that led to success. Now, if we're going to think about hierarchy, I want to start with the case where you have one column in each region. I should be able to get that to work, and then everything else will become clearer later. That's my hope.

This is the principle of Monty: we don't feed the whole image to one learning module; we feed individual patches to each learning module, and then they can vote. It's the same principle, just one level up the hierarchy. It's a divide and conquer strategy. For years, I resisted going to two columns. People kept saying, let's solve this problem with hierarchy, and I said, no, you have to figure out what a column does first. Now, I'm saying we're going to solve these problems with two columns. We'll solve all compositional behaviors with two columns, at least to get started. Once we understand the two columns, we can add more.

I'm just sharing my current brainstorming, my internal thoughts. This is nothing set in stone. I can tell you what I'm thinking about. It makes sense based on neuromorphology. Across the layers, like L1 and 2, if you look at the direction or angle in which the axons and dendrites are pointing in lower layers, they're mostly closer to vertical than out. That was the key to the image I showed from our paper, where the synapses are made in layer 6a and then get to layer 1 and spread out. I hope that's true. That was a key piece of evidence to explain all this to me.

My point is, Hojae, it can fan in and fan out, but the key part is the one that's co-located. That solves all the compositional structure problems we struggled with for years—how the lower can be curved and oriented and doing all these things—but it solves all of them.

It's beautiful. I'm not denying fan in and fan out; I just think it's misleading, and we don't want to rely on it. If I can solve the problem of two columns, let's do that, and then we can see how to expand beyond that. Going to the question of two columns and the issue of how you do it with arbitrary objects—the point is, you can recruit your elbow or whatever is needed. That's going to take more than two columns, right? What I would do is start with two columns, then go to three—two in R1 and one in R2. How do I take the behavior I'm trying to do in R2 and not just decompose it in time in R1, but also decompose it in space in the two R1 columns? For example, writing my name with my elbow or hand, or pouring coffee with different actions. All that could maybe be expressed in just two columns in R1. Don't even think of it as R1; you have two columns in one level. They could be two different modalities or two different parts of your body. How do I execute the behavior that's learned in R2 in either R1, or in R1A or R1B? Start moving in that direction, because if I can understand that—if you understand how we could take a behavior learned in one of those R1 columns and have it played back in the other R1 column—that gets at the core of the tasks we've been talking about, where you can execute various tasks using different methods.

One thing that might help is if those goal states are in an appropriately abstract representation. For example, if the goal state you're passing down the hierarchy is "apply pressure at this location," there are potentially many body parts that can achieve that. I'm trying to think how we can do it with a body part we've never used previously for a specific task, like pushing your computer mouse around, but we understand we can do things with our elbow more generally. Imagine the two R1 columns, R1A and R1B, represent two parts of your body. Normally, I'd push the button with my right hand, the right column, but now, for whatever reason, I have to use the other one. This suggests we might start getting into things like "where" pathways, because I have to figure out how to move the data around. The sensor attached to R1B is in a different location and has to move to another location. By decomposing the problem into these simple two columns, and then maybe three columns, it feels like we could resolve all these issues.

That's what happened with compositional structure. We had to give up on doing it with multiple columns and just settle on how two columns relate to each other, and that solved the problem. It's a suggestion on how to think about the problem. I agree with you, Neil. I think we can solve all those problems by just adding one component at a time. Go from two columns and one column in R1 to two columns, another kind.

It could be as simple as: the goal is to get the object in R2 in some state—the button is pressed. How do we get that to happen?

I'm just thinking that in those instances, we might not have a learned association between our left toe or elbow and performing that particular behavior. But as long as we have a learned association to some sufficiently abstract primitive, like applying pressure for that body part, then maybe it's easier to generalize in a zero-shot way. Wouldn't the state be, "I want the computer mouse to be in location X" instead of something else? I was thinking a couple steps down; that would be a higher-level goal. At some point, the learning module that knows about computer mice or models where the mouse is on the desk determines that for the mouse to move, there needs to be pressure at a certain point. It recruits something to apply that pressure. Rather than specifying "left hand apply pressure" or "right hand apply pressure," maybe it doesn't know anything about the body parts, and the body parts don't know anything about mice. The module that knows about mice and the desk just knows there needs to be pressure here, and the body parts understand that one of them needs to apply pressure.

Often, the key to success is picking the right problem to think about. That's why I love the coffee cup with the logo on it—it exposes a lot of problems in a simple example. Recently, I've been thinking about typing. It's interesting because when I type, I don't think about the letters at all. In fact, I asked myself if I even remember the layout of the keyboard, and I couldn't recall it. When you first learn to type, you have to learn where all the letters are, but now, when I'm typing, I can't recite all the letters as I go. I have to think really hard about it, but I can type any word instantaneously. It requires that my hands are in the same position. That's why we have the little detents on the F and J keys, so you can start in the right position. Once you're in that position, it's all low-level memory—I'm executing a low-level motor command, maybe S1 or M1 is doing this. If I move my hands out or in, or locate my hands on the G and H keys instead of the normal position, I can't type at all. I have to think slowly about where each letter is. If I were typing with a tooth or a stick, I'd have to think carefully. This is the example I'm using: the simplest version is having a single finger, with my hand located at some point relative to the keyboard, and the task is to move and execute certain sequences. I could learn to type quickly with one finger, just like with two hands. Imagine two lower columns: one represents my right finger, one my left. I've learned to type with my right finger by moving it from my palm's location to ten keys, and I can type quickly. But I haven't learned to do it with my left finger. If I tried, I'd have to pay close attention to where my left hand is relative to the keys, maybe even use vision. This is the example I'm considering, where you could have two simple actuators—your two fingertips. One could learn in R1 to perform complex sequences through practice, with no thinking involved, but if I want to do it with the other, I have to think about it and attend to it. I'm suggesting this as a potential problem that exposes the issues we encounter in compositional behavior. The top R2 might tell R1 to type a word or letters, and R1 executes it. But if I have to use the other R1 column, the one that hasn't practiced, it would be slow and require attention. These are the challenges we face in goal-oriented behaviors.

The keyboard example makes me think of another interesting point. As well as using arbitrary parts, another challenge is coordinating systems—how do you coordinate your fingers and similar things? The keyboard example brought this to mind.

We were talking recently on Slack about how, if we can do anything in a model-based way, even if it's slow and computationally expensive, we can eventually learn in a subcortical, model-free way to do it more efficiently and unconsciously. We've struggled with how to learn even a pincer grip, let alone typing with both hands. To a certain degree, those things always start out as serial, model-based actions—one at a time. When first learning to type, you use one hand at a time. For a child, learning the pincer grip is complex: first, they can move the thumb, then the finger, and very slowly, a model-free policy takes over.

The example I just gave is simpler—it's just two movable sensor paths representing the tips of your fingers, and there's no coordination between anything. It might get to, even with the pincer grip, you're moving two things. What I'm trying to say is that anything that needs to be done, at least slowly, needs to be possible to do in a model-based way so we can learn to do it efficiently in a model-free way. But it feels like coordination, by definition, is always going to be taken over by model-free processes. You can do it really slowly—move your thumb, move your finger, then pick up the object—but what you really want is to learn to do that in a model-free way, so you just execute the pincer grip.

You're using the word "model-free" again. I'm not sure where you're going with that. Or subcortical, if that helps. Why would you use subcortical? Why do I need it to be subcortical? Why can't I just learn this rote behavior in R1? I think I can. I guess you can do it slowly, but we've talked before that everything happening in the neocortex will be model-based policies—otherwise, why would it be happening in the neocortex if it doesn't need a model? I like how Niels formulated it: when you do something using the model, it's always going to be a bit slow because you have to go through the model and calculate your movements. When you want to do motor coordination—dancing, moving your hands, grasping something—you can initially do it clumsily using your models, but after practicing multiple times, it gets smoother and more coordinated, turning into a model-free policy where you don't have to think about everything.

I'm going to argue with that. I don't think it's a model-free policy. I don't deny that some of these things are learned subcortically—the cerebellum is really important, as are the basal ganglia and all that. But I think the whole thing can be done in the cortex. If it's not done in the cortex, it's going to take you a thousand years to learn to do it subcortically. I think the cortex has to solve all these problems. The only thing I'm suggesting the subcortex can do is help with the coordination bit. We can coordinate it cortically using model-based approaches, but it's just going to be really slow.

Take my example of two fingers: one has learned to type and the other has not. The one that's learned to type is still model-based, still operating in the learning module in region one, but it can be very fast. It doesn't have to calculate trajectories; it's learned the trajectories. It's like calculating how to go from point A to point B using my model, but after doing it repeatedly from the same spot, I don't have to calculate it anymore. It's like a sequence or a melody—I've learned it, so I don't have to think about it. I just follow the sequence and it's done.

The evidence from humans and other animals is that when we practice something—dance, typing, whatever—the representations for those behaviors move down in the cortex. It's faster; you don't have to regress multiple regions, there's no communication involved. These are learned sequences, essentially based on the model, still happening in the same learning module, but I don't have to do the calculation in the learning module—I just follow the sequence. Is there any reason why we would still need them to be happening within the learning module instead of subcortically, if we're not using them anymore?

I suppose there's no reason to, but I want to solve it first in the cortex. The cortex has to solve these problems. The cerebellum is obviously helpful for coordinated movements—if your cerebellum is damaged, your movements are jerky and not smooth. We're relying on the cerebellum to make everything work better, but you can still do these things without it; they're just not as coordinated, just clunkier. For me, I want to solve how the cortex does this first—that's our goal: cortical models. Then we can ask what other mechanisms we want to wrap around this, whether engineered or working differently, to help make things move quicker or more efficiently, but I think we have to solve the problem in the cortex itself.

That's the point Niels was making to us, especially on Slack: we need to be able to solve all these problems in the cortex. Once it's being practiced and gets really smooth and coordinated, those might be optimized subcortically. I was thinking about this in a write-up I did in Google Docs. First, I agree that probably everything is model-based, definitely during learning and even when we're doing things quickly. When we're learning, if we're trying to pinch the finger, we need to think about moving the thumb, moving the index, and it takes a while, both for a baby and computationally, because we need to consider a lot about the model to decide on the action. There's some function that chooses that action, and in the beginning, it needs to take a lot into consideration, but as we practice, that function computation can be faster, or even that function doesn't need to depend on any model, so it becomes model-free. The speed comes from the probability of choosing the next action becoming close to one. If I move my thumb one way, then the other, the probability of choosing the next action increases, and I don't need to think because that function has been optimized so that we immediately know what the next action is.

Maybe to paraphrase: even in the model-based policy or in the learning models, there can be probabilities associated with certain actions, and those can be learned and refined. For example, if you want to make coffee, there's an 80 percent probability you'll do it the usual way, 20 percent you'll reach for the instant coffee, and whether that's by the strength of neural connections or something else. When you have high confidence, you don't need to deliberate as much, so you can navigate through that faster. That might be a case where a model-based policy can also improve over time—some things still require conscious involvement because they're complicated, but they can still become faster through practice.

I think this all can happen in a cortical region. The cortex is responsible for choosing and learning the actions, and the subcortex is just the executor. We should ignore anything subcortical for now and solve this problem in the cortex itself. I can see how the cortex, by learning these, can learn to do things very rapidly. You could call it model-free because there's no thinking involved, like when I'm typing, but that's still cortex. It just means I've learned a particular behavior—if my hand is in the correct position on the keyboard, the behaviors I have to execute are rote. The first time I did them, I had to think about how to move my finger from the J key to the Y key, but after enough practice, I don't have to think about it anymore. There's still a model in that column, but I'm not using the model to execute the behavior; I'm just using a sequence I've practiced over and over, but it's still in the column. That's how I think we have to approach this.

That's essentially like a model-free policy in cortex. It was learned based on the model and becomes model-free only in the sense that it's so practiced. My hand has to be in the exact same position on the keyboard, which is why we locate the index finger on the J and F keys. Once it's in that exact position, moving to the correct keys is a single direction and distance. Early in my career, I worked on laptops, and they were changing the scale of keys from the original typewriters to the IBM Selectric to laptops, because laptop makers wanted to make keyboards smaller. They went from a 13 millimeter spacing to a 12 millimeter spacing. Initially, this caused havoc—people who could type on typewriters couldn't type because that millimeter threw off everything. There was a lot of pressure to go back to the 13 millimeter spacing. I don't know what they are now—probably 10 millimeters—but back then, every keyboard was the same spacing until laptops started getting smaller, and people started complaining. People said you have to go back to 13 millimeters, but manufacturers said that makes computers too big. That was an interesting example of how rote learning got disrupted. Of course, after you've used a laptop for a while, you adjust. That was the right solution.My point was, it's very precise. You've learned these rote behaviors, and at that point, you're assuming everything is the same. Once my finger is in the right position, all the other keys are in the right position; I don't have to think about it. It could still be happening in the learning model, but it's not using the model anymore—it's just practiced behaviors.

I think we're all agreeing that this needs to happen: we need to be able to do everything model-based and then learn model-free sequences from that. The only question Niels and I raised was whether the model-free sequences are still in the neocortex or learned subcortically. Mechanistically, we all agree this will happen, but maybe it doesn't really matter. As far as I'm concerned, it'll be learned in the cortex. All these layers of cells—this is something I was working on, maybe I'll open it another time—let's think about minicolumns. I think all these layers of cells work on the same principle as the temporal memory algorithm, which is sparsification in context. When you have minicolumns set up like that, they'll automatically learn sequences if they're presented repeatedly. The temporal memory algorithm will learn sequences if it sees them over and over, so it doesn't have to calculate anything. It's inherent in the way the neural mechanisms work: if you practice something enough, it will become "model-free" in the sense that it just executes the sequence without calculation.

I don't think you need anything subcortical to do this. You might want to think about subcortically transferring it, but to me, that's a distraction. I'd rather not; we can all agree it's the cortex that has to solve this. Some say it has to transfer later to make it faster, but I disagree. I think we solve everything in the cortex. Obviously, some mechanics in the body maybe, I don't know, but from the implementation, it's exactly the same. It's just whether we say this is happening in the columns or in the cerebellum. If I were to implement this now, it wouldn't influence how I implement it. It's just a question of how we would map it to the brain—I think it's still cortex.

The point I think about is that just because we're not thinking about where to move to type, that doesn't mean it's subcortex. Just because we don't recognize that we're thinking about it, I think it's still cortex. In predictive coding, because there's no change in the actions we did before, we're not really recognizing it, but it's still cortex. I wrote about this in "Unintelligence." It's not my idea; others have come up with it too. Imagine the cortex is executing a lot of stuff. As long as you can get the process low down, like typing, it gets handled in the lower region. Only when something requires attention—there's an error or something that needs attention—does it go all the way to the top. If you're attending to something, it's going to the hippocampus, you're able to remember it, everything gets taken over for that. But so much of what we do in the world is handled by these rote behaviors in the cortex itself. Even compositional behaviors are taken care of, so you're not aware of it. You're talking about attention, what you're conscious of. It only becomes conscious when the rote behaviors don't work anymore. Then it pops up to the top and you have to pay attention and think about it.

I think there's a lot of complex behaviors happening in the cortex—compositional behaviors—as long as they've been practiced enough. Even typing on the keyboard: the first thing I have to do is get my hands onto the keyboard in the right position. I don't generally have to think about that. You put your hands down, you've learned these behaviors from feeling, and you just know where to put things. You put your palms down, adjust to the right position, and find the J and F keys. There's a complex, hierarchical behavior going on that I'm not thinking about at all, involving multiple regions in the cortex. Just because I'm not thinking about it doesn't mean it's subcortical.

It seems like we don't have a very good model of the keyboard anymore. Like you said, if I ask you where the O key is, it's hard—you have to mentally type a word with an O in it to know where it is. When I first started learning to type, I took a typing class in grade school. The first thing you had to do was memorize where all the letters are and the whole keyboard layout. Now, after typing for so many years, I've forgotten it. I don't need that model anymore. It's amazing—I tried to walk through all the keys on the keyboard and struggled. We can do the QWERTY row, that's easy, and the ASDF row, but other places, I can't remember. But I can type words with no problem.

Maybe we're in agreement, maybe not, but that's how I'm going to be working on it. As Viviane says, maybe it doesn't matter too much, and who knows if our language will change going forward. The main thing is that we agree some sort of model-based, deliberate, conscious control comes first—or unconscious, once you've learned it. That's why I said it comes first.

The goal I would think about is: I've got two fingers, one has been practicing typing from some location and does it automatically, then I want to do it with another finger—a different column—that hasn't learned how to do this, so I have to think about each one. That's a nice problem, perhaps. Very simple: two columns in R1, one column in R2. It's a behavior we can all think about. Now that we're talking about it, it seems like that's the magnolobal equivalent for sensorimotor, for goal-oriented behaviors. It's an interesting sensory problem too, because you do the action, but you see the change somewhere else, like the letter appearing on your screen, and you have the tactile sensation, but then you...What the change actually affected is the visual input. On that note, it's also in English. Also, what if my hand is not in the right position? I definitely use vision for that. I have to see where the laptop is, or maybe I look at the keyboard. Even just getting your hand or your finger in the right base position could require multiple modalities.

I think it's a rich problem. The mug and the logo present a scenario where just a few columns can expose many of the issues we face. One encouraging aspect is that, because we're doing this in a model-based, slower way, it becomes easier. If the goal is something abstract, like applying pressure at a location, my first concern is how to coordinate pressure at multiple locations. But if we're not worrying about that yet, since that only comes with practice, it's not hard to imagine sending a goal state—apply pressure at this location. If the hand is in the wrong place, the eye can see that, and that's another goal state. It feels like we can serially break down any task into combinations of elements we've solved in simpler tests.

That's the hope. I have my marching orders—I'm going to think about typing with one finger and two fingers.

To make it a concrete hierarchical goal: the highest-level goal could be to type the word "Numenta." This breaks down into getting the hands in position X, Y, Z, then typing N, then U, and so on. I would start with the hand in the correct position and figure out how to move the finger to the correct key. That's interesting because it asks how you calculate that, and with enough practice, you can do it automatically. I would start there, and later address how to get the hands in the right position, which might be a more complex problem.

I was thinking you start with wanting to type "Numenta," so the next goal state is typing the letter N. Whatever knows about a key says, "I need pressure here," so maybe there needs to be pressure at this location in body-centric coordinates. That's a widely sent goal state, something many things can answer. The learning module for the hand might know it can apply pressure, but it's not in the right place, so it needs to move. It might send a sub-goal to the finger: "You can apply the pressure." But I'm going even simpler, trying to avoid the "where" pathway and do this all in the modeling space of the object—the keyboard. The first thing is having a finger located within the keyboard model, and moving it to different parts of the keyboard. For starters, I would just say, "Here's a command to type a word. I know the sequence of letters. How do I pass them down one at a time to the part that moves in object space?" I don't have to move my hand in any other space; it's already on the J key. Now, how do I get it to the other keys? I would start with that, and later add how to get my hand in the right position.

Even just solving the problem of taking a word, breaking it into letters, and learning that sequence is a good enough challenge for starters.

I have a question about how a column decides to take action. If you have a hierarchical goal that goes down to a column and your finger is going to press the M key, does it have to suppress all the other columns that could have taken that action? Do you say, "You're not allowed to take this action because I'm doing it," or is that handled somewhere in the hierarchy? How do you think about that? I'm considering doing this with one, two, or three columns. There isn't a need to suppress; with two columns or just one finger, there's nobody else to do the action. Then you might use a second column, and whether you have to suppress or just direct someone else is an interesting question. The answer is, we don't know yet, but I don't think it's the first question to ask. The first question is, how do I do this with just two columns, decomposing a problem into components and learning to do it quickly, as opposed to just doing it by rote? Once that's solved with two columns, then we can ask how to handle multiple actuators, how to choose which actuator, and how to direct or suppress actions, which is your question, Will.

I have no idea yet how that occurs, but I think it will become obvious over time. I thought a little bit about this and about the "where" pathway. Let's say we want to type the letter N on the keyboard. Another way to paraphrase Will's question is: how do I decide to use the index finger as opposed to the middle finger, thumb, or another finger? When I'm first learning to type, I have ten options—I can press with any of the ten fingers. Through learning, or if I saw someone else type with their index finger, I might have a higher probability of choosing that action. I press the letter N with my index finger, and the next time, I have a history: I have achieved pressing N by using my index finger. Now, I have a probability, but because I have experience that was successful, I have a higher probability of using that again, or I can explore and use another finger. After a lot of experience, I probably have a lot of evidence or a higher probability that I will always use my index finger because that's what has worked. If you've had success once, you'll probably do the same thing again and again.

It's interesting that once you've learned to do something a certain way your whole life, and someone comes along and shows you a better way, you wonder why you never thought of that. My kids come and teach me, "Hey, Dad, you're doing this the slow way, why don't you do it this way?" I've always done it my way and it worked, but then they show me a better way and I realize I should have done it that way. You get into this pattern—this is how I've learned to do it, so you just do it that way over and over. I think it's natural to do that. Why would you explore unless it didn't work, or unless you're feeling exploratory for some reason? I think that's also an answer: how can that be fast? Because now, I'll build it and choose an action as opposed to another.

About the "where" pathway: when we're talking about goal states, the goal is about going from one state of the object to another. That was the proposal I discussed last week; whether that's right or not, we'll see. Going off that proposal, let's say the goal is we're tired and we want to get energized. That's the state I want to change, going from tired to energized, and the object it applies to is us. We're modeling objects outside of the agent itself, but maybe there are a lot of columns modeling the agent itself. When we get to these very high-level tasks, like making coffee, it's clear you have to move your body to different places or pick things up. At least picking something up is definitely a "where" pathway—it's relative to your body. I'm trying to avoid that. The typing example I'm using doesn't require that, at least not initially. It might require it if I have to get my hand on the keyboard, but again, it's about breaking down the problem into the simplest components and then expanding upon that. The simplest one I would look for is something that absolutely doesn't require a "where" pathway. I'm just trying to understand how to manipulate an object. In the typing example, the goal state would be: I want this word. It turns out it's a series of actions that have to be taken to get that word. Maybe the word's on the screen, or someone speaks it—I don't really care. The goal would be to get the computer to have that word on the screen. So the goal is to implement this word, and that requires a series of actions by the actuator. Once my finger is located on the J key, as far as I can see, there are no "where" pathway calculations—just how to manipulate the object with my hand, the object being the keyboard, to achieve a particular state.

It's going to be a difficult example to do with just two columns because of the multimodality property. It would be very difficult to type that word with only one touch sensor. Why is that?

For one, you can't distinguish the keys using touch. You don't really know if the word actually appeared on the screen. You can do the sequence, but making this up, I would try to avoid any of that. I would try to come up with a way of phrasing the problem so you don't need to see the result. Somehow the system can know the result, somehow it observes it. I don't know how that is yet, but I would definitely try to avoid those complexities. As soon as we introduce these other things, like the WEAR pathway, it's opening up a can of worms. It's really hard to pin down a solution. I wouldn't say it requires the WEAR pathway, but it seems like it would require voting between modalities, with the vision modality telling you the state. But then, why can't we just make up a state for the object that's obvious to the two columns? Can we just change it to pressing a button? Maybe, although I want to have different behaviors that are sequences. I like the idea of the fingerprint—pressing a button in Morse code. You could do that, but that's a different task. That's not really sensorimotor; that's just temporal. I guess you still also have audition, maybe, to hear the tones. I don't know. That's right. I think we need to make this task abstract enough—real enough you can understand it, but abstract enough that we don't introduce these problems. I think it's really important that the finger moves, that the sensor patch moves in space. Otherwise, we're not going to be solving the problem correctly, and I'll have to learn different high-order sequences to solve the problem. We're going to use the temporal memory to do that. I'm just arguing for my case. I'm going to try to avoid anything else. I'm going to try to solve it in two columns, at least get the first understanding done in two columns. You can't solve the whole problem, but then you can branch off from that. I think it's hard enough just to say, what does it mean to tell region 2 you want a word, and who knows what that sequence is, how is that sequence broken down, and how is the R1 executed? These are all interesting questions. There's a lot to unpack just trying to do something really simple there without moving the hand or looking at the screen.

That's my argument. Instead of vision, you could substitute it with a linear buffer holding a character, where a movement is moving one space in the buffer.

That might be the least amount of vision recognition feature I can think of. Again, it could be a totally artificial way of the columns knowing what's going on.

Initially. And don't our fingertips have cameras in them anyway? Like we have vision right on the touch sensor, and we have to worry about that. Our fingers have it. The question is, that's an interesting question. The buttons are not equal, right? The buttons have different features, and those features could be tactically detected. So we have a bunch of different features at different locations on an object, and we're trying to activate or press a particular feature, and we have to find out where that feature is. The J key is a feature on the keyboard. It's not just a key; it's the J key. I sense the J, assuming I can tactually touch it and feel it. Take that as a simplifying assumption. Now, I have an object with a bunch of different features located at different points on the object, and my goal is to find the particular feature I want right now and move my finger to it, and then do it for another feature in sequence. 

It's also interesting—thinking about typing—we start with our fingers on the home row and the home keys, but as you type, you don't bring them back there every time. If I type the word "jump," I move up to the U, then straight down to the M, then straight over to the P, if I was checking with one finger. It's not like I'm going back to the home square again. The sequence in which I execute is not completely right. That would be another interesting thing to learn: for the type of word, you don't go from home to A, home to letter. You have to learn how to—so the word is actually the sequence of moving in a two-dimensional direction around the keyboard. That could be all done in one column. 

I think we can definitely work around finding the sensor, finding the key letters, and distinguishing the different keys with the camera finger sensor or something like that. The bigger thing is, for sure, we can use an artificial solution for it, but the state of the system—the state we're trying to influence, like the word appearing on the screen—is not visualized in the keyboard itself. The keyboard doesn't tell you which letters you pressed so far and where you are in the sequence, but it would tell you which letter you're pressing because it has a sensor on it to detect that. Bear in mind, I'm not talking about something practical here. Maybe you guys are all thinking we're going to build something that does something. I'm not thinking that at all. I'm just trying to think of a toy problem to elucidate mechanisms. I'm willing to say you could rephrase the goal as: ensure all the keys for "Numenta" still exist, and once you're happy they're all still there, the goal is accomplished. Why would I do that? Why wouldn't I want to have the sequence of them? Oh yeah, the sequence. It's just like I have a—but anyway, I've already said it. 

You could have an abstract model where you have ten locations, each with a unique feature. We'll call those letters of the alphabet, and when the sensor goes to that location, it detects that unique feature. I don't need a visual confirmation. I just need to know if I got the right input or not. Now the goal is to understand how to go through a sequence of these movements in space to accomplish a broader goal, like a word or sequence, and how the system would learn to do that rapidly. If it was a new word, it would have to do it one letter at a time.

If it's a practiced word, you don't have to think about it—it just automatically plays out. I think it's a very fruitful task, but it's not practical at all. I'm thinking about mechanisms, not about anything useful. The only thing I'm pointing out is that the higher-level goal state of having pressed that sequence of letters is not observable in the keyboard, but we could do something artificial, like when you press the key, it changes the color of that key. No, it is observable in the keyboard. Each key has its unique feature, so the sensor detects whether it got to that feature or not. It knows when it's there, but it doesn't know if it has already pressed that key before.

What do you mean? If I have already pressed N, U, M, how do I know that I've already pressed these keys? The state of the keyboard doesn't change as I press a key. No, but the state of the temporal memory—my motor behavior sequence—is a temporal memory, and I know the state of it, so I know what the next element I need to do is. The agent, the modeling system, knows what you've done so far, but there's no way to compare whether you actually achieved your goal. I guess at a higher level, the learning modules can detect these; they know the concept of this series of unique sensations and guide the overall task. That one will get it—we could say it only feels it when it presses it, like a depression that comes out or something, but it's going to feel that, and just like any kind of Monty system, over a series of movements, it'll be like, "Okay, I detected that feature, I detected that feature." It will satisfy what it's requesting. It'll know if it's successful, because if it's been learned as a sequence, it knows what the next element should be, and if it isn't the correct next element, it'll know it's wrong. So if you do N, U—if it's learned "Numenta" and it's a rote sequence—and you do N, U, M, and it doesn't detect E next, it knows it's a mistake. I think it's built into the system that it'll know whether it succeeded or not. I don't see a problem with that.

I'm overloading, putting it all on this one little sensor. Imagine there are ten unique features on an object arranged in some physical arrangement, and the sensor knows whether it gets to that point or not. It doesn't even have to press it; it just has to get to that location. That would be like a keypad—if it got to the right sequence of elements, it succeeds. In terms of the task, I'm just wondering, it feels like we would rather have something where it's not a learned sequence, but more like, "Okay, you now need to type this series of letters." You can't plan that out because—

It starts off unlearned, right? Like a new typist, you have to first learn the model of the keyboard, where the different features are, then, given your current location, figure out how to move to activate a particular feature, get to a particular feature. So that can't use temporal memory, because you've never done that path before? That's it. You're using the model—it's path integration or the reverse of path integration. I was just concerned that we were talking so much about doing something we'd done before that we'd go down a path of only using that. No, it starts out with nothing learned. First, you have to learn a model of the object. That's pretty simple in this case—it's a set of features in some physical arrangement. Then, the goal is, given a particular goal, to navigate one at a time through those different elements, just moving your finger to those elements in the right order. That's not rote yet, but if you do it enough times, it will learn to do that automatically. That would be one of the goals of this system: you start off slow and methodical, which is even hard enough to imagine how to do, but then once you've done it enough times, it gets really fast. As long as you locate the finger in the right position, it just follows the sequence automatically. Then, if we show it a new word it doesn't know, it would have to think about each letter again.

If I gave you a random sequence of letters that didn't look like any word you knew, you'd have to think about each letter one at a time—how to do this letter, then the next, decomposing the sequence. I think all this could be done in two columns, at least for now. If you formulate the problem this way, it might feel a bit unsatisfying because we're not actually manipulating the state of the world; we're not changing the state of the keyboard, and we're not learning the behavior where pressing a button changes the state. 

I don't think it's a problem. You just have to realize you're doing it in a simple way that may not match how you intuitively think about it. We haven't defined what the state of the object is yet. Maybe the state is simply whether you've reached a particular position in the sequence, meaning you've performed the correct order, and that is the state—it's now in the word "Numenta." Maybe that's not a physical state, but we could modify the problem so that as you touch the keys, they move, and depending on where you are in the sequence, you have to go to a different location. 

Personally, I have to think about this in the most simplistic form, trying to fit as much as possible into two columns. You might think it's unsatisfying, but I see it as a way to make progress. I'll try to do as much as possible like that. Most people might not understand why that's important, but once you have the mechanisms, you should be able to expand rapidly to real-world tasks. That's my argument.

I also wonder if it would be helpful because, for me, the complexity still feels present, especially in the real-world imagining of coordinating the hand and the finger attached to it. That would be non-trivial if it were a robotic arm. But if we imagine a digital space with four arrows on a gamepad, moving a cursor up, down, left, or right to visit locations, it's easier to see how one column could coordinate that. 

We're not solving robotics here, but these are hard problems. We have to get to the core mechanisms, and I think we'll run into dead ends. It's not a criticism that it's too simple; it's more that I'm worried if we're talking about a finger moving, it inevitably comes back to how the hand gets the finger into the right location. Maybe it's useful to have a motor actuator even simpler than that. Here, the hand doesn't exist. There's a finger anchored in object space, and the movement is about getting the tip of the finger to the right location to solve a problem. It's a navigation problem. 

It's not obvious how, once you have a model of the object, you get from one letter to another. We know grid cells can do that, but no one really knows how. There's speculation, but we have to solve that hard problem and the decomposition of sequences or goals into subtasks. We have to know whether we've completed the subtask. There's a lot of hard stuff to solve in this simple problem, so I can't imagine tackling more at once. Until I get the core of this down, everything else will just be wasted time thinking about high-level goals without knowing how to build them. That's been my experience.

I think it can be done by two people. We don't have to agree; we can take different approaches. I'm also trying to suggest keeping it simple. I'm beginning to think it can be done by two columns. Let's say I have one finger, no eyes, and first I need to learn. I'll go through the keyboard and learn a model of it. There's a feature, some protrusion, so I can tell that's N, that's U, and so on. After learning, let's say the goal is to type "Numenta." We're learning the keyboard, not just the feature at a location—N at (0,0), U at (0,1), and so on. 

Starting from the easy case, not just putting the finger on it, but starting at N and wanting to type "Numenta," we break down the goals. Now we want to type U, so the goal is to go to U, using features learned at level 1—N at (0,0), U at (2,2), and R2 will need to integrate that information and move forward. R1 learns the model of the keyboard. R2 has to know the different states R1 needs to get into: first the N state, then the U state, then the M state, and R2 has to know that R1 has accomplished each one before giving the next. 

With practice, R2 won't have to tell R1 all the steps. R2 will just say, "In the context of this word, do Numenta," and R1 will just do it—goal accomplished. That's how I see it. R1 has the model of the keyboard; R2 doesn't need it, just the model of the word "Numenta." Goals can be broken down infinitely. In the beginning, there might be many step goals to think through, but with practice, there are fewer. If we don't have an infinite number of layers, at some point, R01 will need to figure outBut also, bear in mind that in the hierarchy, just like in composition, all the hierarchical steps are not represented at the same time. You can just do two levels and move those two levels up and down in the stack of hierarchy. So, I'm solving this problem now, then jumping down, solving another problem, jumping down again. A task may have 20 different complex components, but they're not all representative ones.

That seems like a big time limit. What else were we supposed to talk about today? This was the topic, this was the aim. Viviane wants to talk about something; she was offering to review some anatomy, but she can do it next time. I think we're already over time.

I'm really excited about working on this now. Today, this meeting was really helpful for me. Just talking through this made it much more clear what my task is and what our task should be. I'm excited about that.