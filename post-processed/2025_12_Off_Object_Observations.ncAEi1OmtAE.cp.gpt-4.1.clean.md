So, there are some new points here, which we'll get into, but one way to think about this problem is: what is happening at the level of the column in terms of modeling and perceptual input when we perceive nothing? To be more precise, it's not that we perceive nothing, but that we're actually not sensing anything.

We might be receiving something, but not sensing it, like we discussed before—no physical sense. This is more common with touch, because anytime it's not in contact with the surface of something, arguably, it's not sensing any edge, morphological feature, or surface. Even with vision, you can think of examples like looking into the sky or some other void where there's no clear surface being perceived.

Monty also looks into the void a lot. I don't get that joke. Not so much anymore.

To be specific, when a sensor patch is not receiving any sensory input, that's what I mean. It might be receiving color, for example, but it's not sensing a surface or morphological feature. My understanding is that every ganglion cell leaving the retina is a center-surround ganglion cell. If there's uniform color on a patch of retina, you don't get any output; you only get output if there's an edge of color. I tried to read about that, and it seems that because of opponency and other factors, you would still get some baseline activity. Otherwise, if you're looking at a full wall of color, you perceive that as a color, not as nothing. That's the mystery: people have written that there's no input, yet you perceive something. How is that?

Maybe even if we have some representation of that constant color, the biggest thing is that there's no location or orientation being detected. If orientation is represented in the mini columns, that would mean there's no activation there. I think this is actually a really important point. It sounds odd to say you're looking at a blue wall and all those columns are not getting input, but that's what people have said. The idea of the center-surround receptive field is compelling: it says, "I'm not going to report anything unless there's some change." It can be a spatial change, like a parvocellular cell, or a dynamic change, but there has to be some change. That's all that matters in the world. That's general neuroscience dogma.

We should ask ourselves if we want to pursue the idea that you might get something like pure color or not. I'd prefer not to. I'd rather ask, how do we explain the perception of color? Towards the end, I'll have some slides that relate to that.

For now, let's assume you also don't get color—you perceive nothing, sense nothing. If it's uniform, there's no output.

That means you could be touching a surface, but your finger has to be moving or something like that. If there's no center-surround output, we can still perceive that whatever we're looking at is uniform or doesn't have any edge. One of the questions is, even though we're not sensing anything by center-surround, that may still tell us that whatever it is has no edge or is continuous. Jeff's point is that there would be no activity, but then the question is, how do you know what color it is? There would be no spiking.

It has to come from the edges of the color. Maybe there are ways we stimulate where there are no edges. That was the example I gave with the checkerboard illusion I wrote about yesterday. The reason that illusion works is because you see the edges of the squares, but you don't see the gradual change occurring within the square. It's gradual enough that it doesn't excite any of those ganglion cells. The brain assumes it's continuous across that area. You don't perceive that it's actually changing from black to white. There are many examples like this in the visual arts, where tricks of color are used. It's mysterious that you perceive a red surface across the entire fire truck, but from the retina, that's not true. From the retina, there are red edges at certain places, and the rest is just perceived. That seems to be well-established science, but it may have changed.

Tristan posted a question: sometimes we model sensors at the single-cell level, original center-surround, but then perception happens because it's retinal ganglion cells plus some other color opponency cells. So, no input, no output from the retinal ganglion, no output from the center-surround, but some baseline in this RG, let's say. Baseline coming from the retina? From color opponency, in the retina. We have axons coming out of the back of the retina. Some cells have constant activity—they increase or decrease. When we say no activity, we mean there's no change in activity.

When you say color opponency, is that another signal coming out of the back of the retina that's not center-surround? Yes, but I'm not sure if it exists. That might exist, I just don't know about it.

Maybe it's a combination of cells that give rise to the perception, even if that particular one isn't involved. But why wouldn't—I'm not sure. I'll get to Tristan's question in a second, but it feels like we should at least have the working assumption, as in ophthalmology and neuroscience for many years, that all the in-between stuff is perceived but not sensed. That could be wrong; I don't know what the current literature says. Maybe they've found new cells that are just red cells or something like that, but I think we should work with that assumption as far as we can before abandoning it, because it was well established a long time ago.

And Trista, I don't think blinking—it's a good question whether blinking would activate things like that, but I don't think so. When we blink, does someone else remember this? It's like the brain shuts off everything. I think it gates the input, but I don't think blinking—the actual turning on and off of the light to the retina—the general idea is, I've never heard anyone suggest that's somehow useful, other than keeping your eyes wet. Spatial gradient, not a temporal gradient. There are mechanisms, as Vivian said, that basically say, ignore this, don't pay any attention, it didn't happen. Saccadic suppression, or something like that? Specifically for blinking, I think, because otherwise the world would be blinking on and off all the time, so it somehow gets around that. All these are possibilities, but I don't think we should go beyond what physiology has suggested in the past unless we have new evidence. Throughout this discussion, I'm going to ask myself, how is it that we perceive the color when we don't get an input in the middle? But we do get it at the edges, so that's a really interesting question. All kinds of weird questions or ideas come out of that.

Interesting question about blinking. I thought this might be a helpful example as well. This is an illusion where you're perceiving what is not sensed. We perceive a kind of blue circle, but in reality, it's pure white.

It's just because of these edges that you fill it in. You can almost see the blue edge between the circles. It feels like there's an edge there. It's maddening, but if you cover up the screen, you can convince yourself that it's all in your mind. That's a strong indication that whatever cells are reading that area are not getting any blue, yet you're perceiving it. The perception is subtle—it's there, but it's not as vivid as when you actually see the red of a fire truck or something like that. It feels different somehow.

Because you actually don't have those blue edge-detecting cells there, right?

But I guess they're active on the edge. I don't know. It's an interesting artifact. Imagine I see a red truck, and then I say, if the center was not red, I would notice it. But then I'd have an edge where it goes from red to white, or something like that. So it's impossible to set up almost the control experiment. Although, if you really isolate yourself and look at a pure color, like all you can see, over time that color disappears. You're not even sure what color you're looking at anymore.

Some of you—I forget the context of this experiment—but you can do this where your entire environment turns red, you perceive red. What changed was important: it changed from non-red to red. But then, over time, you forget, you don't really know what color it is anymore. It happens when you close your eyes. If you're not sleeping and you focus on your visual field when you close your eyes, you start seeing colored patches that move and oscillate. That's a different problem, but you're right. It's not clear. That illusion you showed was a very clear example. There's also the one with the little Pac-Man and the triangle—the Kanizsa Triangle, I think it's called. There, too, you perceive the edge. It looks like you're seeing an edge, but you're not. It's incredible. Here we have an example where you're perceiving something that isn't there. And with the checkerboard, we have examples where there is a transition from black to white, and you don't even perceive it.

For anyone who hasn't seen it before, it's this illusory triangle that appears to be floating.

That's the illusion.

Even though there are no triangles in the picture, you don't perceive the one in the background, you don't perceive the edge—you assume it's occluded. But you can almost sense the edge of the white triangle, which doesn't exist. It's the same as the previous illustration. In this example, again, most people probably know it, but in case you don't, the counterintuitive truth is that A and B are the same color, the same grayscale value. But of course, B looks like it's white.

This one isn't just about the gradual change; it's also playing on higher-level perception with the shadow and such. But I think either in my book or somewhere, it was shown that's not true.

Oh, really, if you cover it up. This is not the exact image I started with, which was not a green cup, it was a cylinder. But you can remove that. I can maybe try to find the image I have. You can remove that, and you still don't know there's a shadow. Are you still—do you still think they're the same? Sorry, I still think they're different. You think they're different. I've narrowed it down. I even took these images and edited them so there are just little, skinny rectangles that go across the A and the B. I really tried to figure out what's going on here. How does this work? You can remove all this extraneous stuff, get rid of the fact that it's a checkerboard.

I concluded, and I could be wrong, but I've concluded that I got it down to this example where it's basically a gradient between edges, and you can't perceive the gradient. There's a lot going on in this picture, so I isolated it down to a very simple version, and it was still very strong. I think the only explanation is that the shadow makes you feel like there's a higher order effect, but it's not really true. I got rid of it, and it still works.

If they don't have—if we can only detect the changes in colors, how are slow-changing gradients detected and modeled? What would happen if the fire truck had a gradient towards the middle that slowly changed from red to white, but there's no clear edge?

Is that just the receptive field size? Some receptive fields are large with low resolution, and for them it is a gradient. It's an interesting question, very similar to the little checkerboard we just showed. I got rid of the idea that there was a cylinder and a shadow, but I didn't get rid of the idea that there were very clear edges in a regular pattern, like a checkerboard. If you don't have that, if you just have a fire truck—

Basically, you're saying, Vivian, how do we perceive this, or how is this modeled? If this essentially drives activity, one possibility is—it's hard to believe this, but this is what I suggest—imagine you really only are detecting the edges, and those edges are all in your mind. If the gradient's strong enough, it would trigger a center-surround cell, but if it's very weak—what about when there are no edges, like the sunset, for example? That's my point: when you get rid of the edges, that's like being in a total green environment. All of a sudden, you start losing your color abilities. You're not even sure what color it is anymore if everything is the same color.

It's like you lose the ability to tell if it's brighter or not. These are all relative. The basic theory, which is hard to believe but seems to have a lot of evidence, is that everything is relative. If you don't have a relative change anywhere, you don't perceive it.

In those pictures, we had a gradient. If the gradient is sharp enough, you might perceive it, but if the gradient is gradual enough, you wouldn't. So a sunset—there's always an edge someplace. You have clouds, the horizon, buildings, something; you have edges someplace. It's hard to see a sunset without any features.

I bet people have studied this. Somebody has studied it. But that was a good primer on illusions and perception of color. I think it'll help with the discussion later. I've got some stuff on this kind of filling-in effect if we want to explore it in Monty.

Do you see my slides? Yeah. There are a couple of situations that can arise. You can imagine we have a mug in Habitat that Monty's perceiving. It has a bunch of hypotheses about where it might be.

This is where the sensor actually is, and this is where the sensor's going to move. This happens to be one of the hypotheses. I'm saying 1 and 2 are where it actually is. We wanted to go in red. Green is—oh, no, red is—yeah, red is actual, and green is a hypothesized location. Moves from 1 to 2. In this example, this hypothesis thinks it stayed within the mug, but it actually moved off and is sensing nothing. Either it's looking into the far distance, or it's the finger, and it's not sensing anything. This was the first situation that got us thinking about this problem, because right now what happens in Monty is Monty, for lack of a better word, kind of panics and then goes back on the object and forgets about the fact that it saw nothing, that it stared into the void. Actually, it doesn't send that to the learning module. No, that's what I was going to say. It moves back onto the object, and that information is not sent to the learning module.

Does it update its hypothesis? Nope. That would be a mistake. It seems like something ought to happen. This hypothesis seems clearly wrong, so it makes sense to process this as a prediction error. You perceive nothing, but you expected something, and you have negative evidence. This seems like a straightforward change, so that's why we planned on doing it. But then there are some edge cases or situations that make it a little more complicated.

One of those is we can also have, in this context, what we call an—can I stop you for a second? When we say we have a hypothesized location—

That's a location on a specific object, right? So this is within the reference frame of the mug. Lately, we've been talking about the idea that attention relates to some volume in space. In some way, when I see that sensor movement from 1 to 2, does it think it's on the right side of the mug, or is it that the hypothesis is on the left side of the mug? It thinks it's still within the reference frame, saying, "I'm still here." So it's going to predict sensing surface, smooth ceramic, whatever, but actually, it gets nothing. That should be a prediction error. This hypothesis isn't a good one, because it's not. We should get rid of it.

But then, in this context and others, you could get what we've started calling an out-of-reference frame movement.

Imagine the hypothesis thinks it's here, and whatever movement takes place, it goes off the object and ends up out here. In this case, it's actually moved beyond the learned reference frame. Imagine the learned reference frame is a point cloud corresponding to the mug. So it's beyond anything it's learned before or knows about the object.

How is this different than the previous case?

If it senses nothing, then this hypothesis could actually be correct. It could be that the sensor was here and moved off the object. So why would it go outside the volume of the object? What if the system as a whole knew where the object was? Then it would know in advance that this would be moving off the object. If I could look down and say, "I know where I am with this mug, I know the volume," it could know in advance that it's moving outside the bounds of the mug. What if it just knew in advance? Yes, I think that's the hypothesis—what Monty thinks is actually correct, so it's correctly predicting that it should be off the object now. So what is the problem there? At that point, this column shouldn't be participating in voting or perception. If this hypothesis is correct, it shouldn't maintain or pass up the hierarchy "mug at XYZ," because there is no mug at XYZ. This hypothesis was correct, but it's no longer relevant after this movement.

We've been moving towards the idea that there's attention in a body-centric space, and all the columns know whether they're attending within that space or not. Only the ones in that space get to vote. If you're outside of the space, you don't get to vote. If you're inside, you get to vote. That's the gating factor. This column is attending to that space, but it's not sensing anything there. It's outside the bottom of the object. If it knew—yes, this one—so at this point, imagine I'm moving my retina around and looking at different parts of this cup. There are many columns that are not on the cup, and columns that are on the cup when I'm looking on the left side will be off the cup when I'm looking at the right side, and vice versa. There are tons of columns going in and out of the actual object all the time.

The way I've been thinking about this is that there's a sort of global attentional volume. Every column is either in that volume or outside of it. If it's in that volume, it's part of the inference process. If it's outside, it shouldn't be. You might enforce that by some sort of voting—if you're outside the column, you don't get to vote. You can get an input, but you don't get to vote. Your input doesn't make any difference. We're only attending to this spot and this area, but that spot is in body-centric coordinates. I agree we should restrict voting based on that, but to me, that feels separate from this particular hypothesis being inside or outside of the reference frame. But you said yourself, does it pass up the hierarchy? No, it doesn't. If it doesn't vote, if that column says, "I'm out of bounds, my vote doesn't count," then it doesn't go anywhere. It doesn't affect anybody. You could just scream in the wind, but it doesn't matter—you're not part of this team right now, you're outside of the boundary.

When we think about what happens in this case, as soon as you said, "does it go up the hierarchy or not," I'm thinking, no, it shouldn't. This is all recent thinking. Would you say it doesn't output anything up the hierarchy or for voting, but would you say it still gets sensory input and maintains the mug hypothesis? I don't know. We don't know. We haven't proposed a mechanism for this sort of global attention space. Does it gate inputs coming from the thalamus and say, "you don't get to pass your input through"? I don't know. Is it—because in this case, let's say the fovea is here, and now the fovea is here.

I don't see why you would gate before the column has even received anything. It seems more like the column's decision, based on its hypotheses, to say, "I'm not sharing this because I'm out of any relevant reference range." But why would I be unable to? The idea of a global attention space isn't just global; it can't be local to the column. A column wouldn't know that. A column knows where it is and has a point in that space, but we haven't proposed a mechanism for this. It seems it has to be somewhat extra-columnar, outside of the column, to decide. There can be multiple kinds of gating or deciding whether to pass information, but if it's the global attention, that's basically saying, "All these columns, you get to perceive stuff." After the saccade, all the columns here get to perceive stuff. No, they wouldn't, because they'd be outside of the attended space. If we've saccaded here, why would we still be attending here? Attending is not necessarily where we're fixating.

What's the term for when you attend separately from where you're fixating? It's not that unusual; it actually happens all the time. For the sake of argument, if we say we're attending to the same location we're fixating, it still feels like a separate issue from this out-of-reference frame movement.

Attention is separate from fixation. Fixation is just where the center of the retina is pointing—the fixation point. The columns cover the entire visual space at different resolutions. We often fixate within the attended region, but not always. It's generally a good idea because you have higher resolution there. For example, if something unexpected happens off to your side or behind you, you'll attend to that space before you can even move there. You'll try to—it's like covert attention. If something flashes over there, that's the word: covert attention.

The basic idea is that you immediately start attending; it's a very fast reaction. It takes time for the eyes and head to move, so you'll start attending to that space, trying to perceive what it is before you actually fixate on it, if possible. I have this vision: you have a point in some location in this global body-centric space, and at any point in time, all the different sensory columns are either in that space or not. They're attending there or not. Whoever's attending gets to participate, and it could be low-resolution stuff initially, but then you'll try to move the high-resolution focus into that space. If I move my fovea from location 1 to location 2, I'm not sure why I did that. Normally, you'd do that because you wanted to attend to something besides the bug. Maybe it'll be clearer with another example in a minute.

I think both these mechanisms are relevant. I'm just trying to say I don't think they're the same thing. What are the two mechanisms? A learning module keeping track of a hypothesis that moves out of reference frame, and a global attentional window that gates what columns are receiving information.

Both would take part in determining how information flows when this movement takes place, but I feel they're different things. Maybe we shouldn't spend more time on it, but it does seem like they're different. For example, when you move your eye, that global attention is definitely going to change, and that will affect which columns are getting information. I think the global attention is independent of where the columns are pointing. There's an attended volume of space, and a column is either in it or not. If a column moves out of it, it's possible that it's still in the reference frame of the object but doesn't get to vote.

If I haven't re-anchored my reference frame, I'm still thinking about that object, just not on it. My reference frame is still anchored, but my point is outside—in that reference frame, it's outside of the egocentric reference frame, so I don't get to vote and my input is ignored. Whether we shut it off or not, my input is ignored. I could still be modeling the object; I'm just outside of it. I'm in the reference frame of the object, but not on the object. I think that's consistent. When you attend to a new location in space, that's not the same as moving your eyes there. This is a big change in my thinking. Attending to a location is not moving your eyes there. Attention is deciding, as a brain, to look at something else in the world, and now I would assume I'm looking at a different object. If the eye moves to location 2 and I don't change my attention, just my fixation point, then everyone's still thinking about the cup. But if I move to 2 because there's another object there and I now have a new volume I want to attend to, then the cup is forgotten and everyone's looking at the new thing. I know this is hard to think about. Maybe the next examples Niels brings up will help, because sometimes it gives you information about what you're sensing. It has practical implications. Okay, let's go for it.

I think generally we agree that even if this hypothesis was right, it's not sensing the object anymore, so it shouldn't continue passing up information about the mug at location 2.

And really, one thing we realized is we don't need to wait for an off-object event. There may be another object we sense, or we may be off-object, pointing at the sky or elsewhere, but we don't need to wait for that. As soon as we move out of the reference frame, we know we're not on this object anymore.

Just one last thing: do we still increment the evidence of a hypothesis that correctly predicts what's in the void now? It might come back onto the object later. This is the question of how to best handle this, which I'll get to. At the moment, it's the same as before—if it went off into the void, it would just move back onto the object, and nothing would happen to the evidence value. We did some experiments where we thought we could prune hypotheses quickly by removing them after an out-of-reference-frame movement. I now think that's maybe a mistake. We found, at least with a small spatial filter, that this made us susceptible to noise. If we were slightly off the reference frame, we would discard hypotheses and say, "This isn't on the object." It seems like we shouldn't be doing anything because we didn't make a prediction. If we're moving out of the reference frame, the model can't predict what's going to be there. There's no prediction—anything could be there. There could be any other object, nothing, or the same mug type again behind it. If there's no prediction, we shouldn't be able to increment or decrement evidence.

I think we need to be more precise in our use of language. When we say "out of reference frame," that's a confusing statement. There's a reference frame? Maybe "out of model" is better. We moved to a location in the model that doesn't have a feature associated with it, but that's also confusing. If we say the reference frame is the space, and the model is the learned points in the space, I've always used "reference frame" to refer to the anchoring of grid cells. That reference frame covers all of the universe—it's an infinite space. Once I've anchored the grid cells, I can represent points everywhere. It's independent of the area occupied by the model. The attended area is also the model's area.

If I say "out of reference frame," it doesn't make sense. Either I change my reference frame to something else, have a new hypothesis of what the object is, or I'm in the reference frame but out of the expected attended area of the object. I'm out of the area generally occupied by the object, which is the attended area if I know the object. We haven't talked about how to do any of that, but out-of-model movement is still in the reference frame of the object—just out of the model's features, where the features are known to exist, which is also represented globally somehow. We're going to have to practice this, because we've been using the term sloppily in the past. Now we understand it has to be emphasized.

There's a practical problem. Vivian first came across this when someone was working on MNIST, trying to get that working, and there was this issue of distinguishing a 1 versus a 7. But I'll use an even clearer example: an "I" versus a vertical bar. You're trying to distinguish these, assuming you've learned models for both.

Now you have this visual field where you see a small part of the world at any given time. You're going to saccade around. You need to predict what you're going to see up here.

We don't know yet. You have two hypotheses at this point, with equal evidence for both—they're both equally valid. You move to that location and see there was nothing. The question is, how should we handle this internally?

Currently, Monty would move back onto the vertical bar, because it was a vertical bar, and it wouldn't update its evidence or be able to deal with this. A natural thing to do is not to send the observation to the learning module if there was no feature in that field. So, a natural thing to do is to pass this off-object observation. The model moved here and predicted something because it has these points in the model, but that's not consistent with a null or absent morphological feature. This hypothesis should receive strong negative evidence. That was the simple example we had before: it's not there.

The vertical bar is trickier. We initially thought this was an out-of-reference-frame movement. In the past, we've discussed reducing evidence for that object because we're moving off of it. But you could also ask, is this actually evidence for the vertical bar, given it correctly predicted and there was nothing there? No, I would think not. I agree.

What I think we've converged on is that if the hypothesis goes quiet for some time but does not receive negative evidence, then, since the issue is that we need some way to build confidence that this is the vertical bar, if it goes quiet until we move back onto the object, and since the other hypotheses that predicted something would get negative evidence, that should be enough to separate these out. I'm confused about the mechanism here. It seems like you have two hypotheses, each with two reference frames, or two anchorings of reference frames.

Whether they're simultaneously held or oscillating between them, I don't know, but there are two different reference frames. In one case, there's a prediction that there should be a dot; in the other, there's no prediction for a dot. As soon as you test, you could differentiate between the two hypotheses. In the object model for the dotted bar, if you go there and it's not there, that eliminates it right away—it just says, "I can't be right."

I don't understand how that's consistent with the term you used a moment ago. Maybe the terminology is unclear, but the out-of-model movement means we want to maintain the hypothesis without adding evidence, since it's not predicting anything and we're outside that model. It doesn't make sense to accumulate evidence for it. I guess you wouldn't move to that location. Imagine I can maintain one hypothesis or the other—A or B. Under A, I would move there to see if something is present; under B, I wouldn't move there. Wouldn't you? If you were trying to distinguish between them, that's the natural place to go. If I'm considering hypothesis A, I would go there because I expect it to be there. If I'm considering hypothesis B, I don't need to. You're considering both, but you're acting on one. I think that's what Jeff is trying to say: you went to a hypothesis, but one is leading. I agree it's hypothesis A that's telling you to go there. There's nothing in B that makes that location interesting. I agree with that. Which is A? A has the dot. I have forgotten. Critical Bar does not.

The weird thing is, as soon as you go there and see nothing, even though you're not on the bar anymore, you instantly know you were on the bar, because it's not the eye. This depends on what kind of models you have in Monty's brain. If it's just eye and vertical bar, then we know it's the vertical bar. But we need to consider every other model that exists in Monty, and we might need to give positive evidence for all those models. If we give positive evidence, then we're increasing in a discriminatory space—the difference between them. Or do you mean negative evidence? In this case, we're only giving negative evidence. Negative evidence with the eye, and then positive evidence with the vertical bar.

Do we want to give positive evidence for the vertical bar? My position is that we shouldn't, even though if we don't see anything, it is immediately the vertical bar, but that's only the case when you have eye or vertical bar only. In general, I don't think we should give that positive evidence to the root bar. I think we agree on that. I think we agree that we want to give negative evidence for the eye model, because it predicted there should be a dot, but there wasn't. We don't want to give positive evidence here, because we're not actually on the bar anymore. But if we move back again, we know it's the bar and not the eye anymore.

When you're moving between these, imagine these two hypotheses have two different grid cell anchorings. They're not the same; there isn't one reference frame. You can either be in hypothesis A or B, and each has its own reference frame. If I'm just looking at the bar, the bar model is fine, but only when I get to the eye model do we have to switch to a different hypothesis. It's not like there's one reference frame and you're moving in and out of it—there are different reference frames and different hypotheses.

To give more context, one thing we tried a couple of months ago was, if we move off the object and the model predicts the object isn't there anymore, like in the case of the bar, we should just delete that hypothesis because we're not on the bar anymore. So we're on the bar, and we move up—our model knows the bar doesn't exist there anymore. But why would I move there? The bar model wouldn't go there. It goes there because it thinks it could be the eye. The way to distinguish them is at that point, it switches to the eye model. It's no longer in the bar model; it's the eye model—a new model, new reference frame.

Haven't we discussed how columns could, within a phase or something, have multiple hypotheses? The moment you move up there, the hypothesis you're testing is the eye model, not the bar model. So I wouldn't move up there on the bar model. But we still have the bar model active, and it's still going to get the movement that integrates to move it up.

But at that moment, the bar model is not active. It's the eye model that's active. I agree, the bar model doesn't provide that hypothesized location to move to, but both models are integrating movement to update your position. They can't both be active; we don't know the mechanism. It could be a union, but let's consider that you're alternating between these models. At one moment, you're saying it's an I; at another, it's a bar. This alternation could be rapid and may not be conscious. If you're only going to move up there, and you're on the bar model, there's no reason to do so. But if you're on the eye model and decide to move up to see if the eye is there, you still have the hypothesis for the bar, and that space would also need to integrate the movement you just took. We don't know the mechanism, but at that point, I'm not on the bar model. The eye model comes along and says, this is me, and I'm good.

You're saying highway limit. Actually, it's perceiving the vertical bar. The eye model will say, "I'm bowing out, this is not me." This is the case where it's a null up there, which is the problem we're considering. It's straightforward when it's not the null and we're still on the object; that's a case we already handle. I would think the harder part is when you see the dot and need to get rid of the bar. That's straightforward because the bar model doesn't store features there, but we get features as input, so that provides negative evidence. It could be the bar, and there might be some noise in the background.

I thought that's a harder one. Why do you think the hard one is—let's walk through it. There's a dot there. There's no dot there. The hard one is when the dot is not there. Two reasons: first, we don't send that observation at all at the moment because there's no observation there.

No sensors or cells are being triggered. Second, even if we sent that observation to the learning module, the eye model would not have a prediction for what should be there, because anything could be there—a textured background, whatever—so it wouldn't make a prediction. It wouldn't think the eye is there anymore because that's not where its model exists. I still think it's easier the other way around than what you're saying. In our code, what's missing is that our movements don't tell us the reason. We move up to the dot part because we think we're acting on hypothesis I, but in code, we don't know why we're moving; we just move there and update all the hypotheses for all objects—I, L, A, whatever. It sounds like what we might need to do is, if I moved because I thought I was an I, then when I do that, I update only the evidence for I. Maybe that's a term. We do have that information; the learning module knows its most likely hypothesis, and that's what it tests with the movement. But I'm not sure if it would help to update only the most likely hypothesis at every step.

Nope. I have a different approach to think about this, but finish your thought with Vivian. I think we will be updating all the hypotheses associated with that object, not just the MLH, not just one hypothesis, but all the I's. If there are 100 hypotheses for every object, we'll be updating all 100 of them, which would include the MLH.

But what would be the benefit of not updating the other model's hypotheses? If we only ever update the evidence for the most likely object, the others don't take advantage of the sensory input we're getting. Even in the 2019 paper, in the model, multiple hypotheses were updated in parallel, multiple reference frames. We've discussed ways that, even without a union, it could be done. I'm not saying it's a good idea, but this is one way so that we don't have to give positive or negative evidence to the vertical bar. That finishes my thought to Jeff. I want to present a different way of thinking about the whole problem. Let's step up a level. Imagine we've learned these two models. Normally, we would infer using multiple models at a time, like voting. Imagine I have a bunch of columns, and they're voting, and each column knows both models. All the components would be observed at once, so there'd be more evidence for the I than the bar, and you would quickly decide—no movements required in inference here, with lots of columns going at once.

Assuming I have these two models in all these columns, now we're introducing an artificial system where we're looking through the straw and moving around, trying to observe the world. In reality, it's very hard to observe everything through the straw. But imagine I was doing that, moving around, and I see the eye. At that point, would I say, "I see the bar"? I'm seeing the bar, and then I see nothing. I'm looking through a straw, so far I'm on the bar, moving around the bar, and then—do I sit there and think, maybe there's another hypothesis I haven't observed, more to see, and go look for that? I might question that. There are situations where you think you understand something, but then I show you another piece, and that changes everything.

Now I have a different perception of what it is. Let me walk through this thought experiment. It may not hold water, but let's walk through it. Somehow, I've got these two models learned, I come out with my straw, I'm moving around, and I see the bar. I go, good, I'm done, I got a bar. I have no reason to look elsewhere. I might be done and miss the fact that it could be an eye. In reality, in a real brain, a couple of things would be going on. One is, I would have somebody observing the dot above it, and that person would know about the models, and that would make a difference. I would know it's an I. If I didn't have that person observing it, I can't tell. I'll have to assume, if some part of the world is occluded and what I see is consistent with what I know, then I'm going to go with a hypothesis. I think we might do that. Also, if the dot appeared all of a sudden, I would say, there's something up there, I have to move up there and attend to that location to see what it is, or something like that.

I'm wondering if this is a case where looking through the straw is not a good example. When Neil was showing the example, it was pretty intuitive that we're looking through a straw, and we are able to recognize the bar. If we have the task set up that it could be an eye or a bar, we would move to where the dot could be. I don't know if I would; I'm questioning that. If I have no other context—if someone told you that's what you had to know, that's what you're trying to figure out—but let's say I didn't know that. I'm just saying, I'm a column. If someone says, you should be looking for an eye, then I would be invoking the eye model, and I know to look up there. But if I'm just asked to sell, I'm looking at the straw and saying, what is out here? What am I observing? Here are a bunch of features, they're all consistent with the model I have, and my straw hasn't gone anywhere else. Would I feel like I need to go anywhere else? Would I feel—assistant with two models you have. I might just make the wrong assumption. I might just say, okay, it's a bar. Good, I'm done. That bar could be part of a picture of a banana. It could be part of a tank. I don't know, it could be anything, but at the moment, I've just said, what am I observing? I found an object, here it is. I'm good. I have no idea at this point if there is anything else. I wouldn't even know to look elsewhere unless I had a different hypothesis. The question is, why do I have a different hypothesis? Bear with me on this, I think there's some validity to this.

With voting, the classic example is a mug with a handle or without a handle. The idea is we'd move to where we think the handle is. The handle is just like the dot on the eye. We're trying to realize what mug we're holding. In this case, imagine if I'm moving my straw around the edge of the mug, I would see an area where the handle comes in, and it's clear that this morphology is different. In this case with the eye, it's off in space. Tristan, you asked, why aren't we doing the 1 versus 7? That was deliberate, because in that case, there's this alternative signal for distinguishing them. Is it an alternative signal? It's just that I get to the point where the morphology of one object doesn't fit anymore.

Imagine I'm looking at the 1 and the 7, and I see—I'm inferring, and I think this is a 1. If I could observe the whole thing and it was consistent with 1, I'd be done. But if there's some other dot off to the side that makes it into something more than one, I'm not sure I'd go there. Again, voting would work. If you had multiple columns, it would reach the right conclusion. If I'm providing impoverished input, where I'm only allowed to look through a straw, I'm questioning the fact that I would leave an object that I recognize to test a different hypothesis, unless there was some reason to do so, unless someone told me I had to do it, unless the instruction was, you should be seeing a 7, or you should be seeing an I.

That kind of thing. This could be an artifact of two things. One is we have a single column, like looking through a straw, and two, we have discontinuous objects. In that case, I'm arguing that the system will fail.

But it feels like a human can do it. A human does it because we vote. No, but with a straw—if I say, tell me whether you are seeing an eye or a vertical bar, feel free to move as you need. The human example would be, look through the straw and tell me, can it be both? Why can't you ask a human to do either? You would set a long hypothesis, oh, I got it, I see what it is. Imagine it's the two theater faces, in the theater, there's a smiley face and a frowning face. Now, imagine they were just two faces, but I'm looking through a straw, and I see one face, and I go, I got a face, I'm done. There's no reason to go anywhere else. I'm done, I got a face, it's all consistent, there it is. I recognize this object. I wouldn't go, what other things might be out there? I wouldn't have thought of that. It just wouldn't occur to me that there's an alternate hypothesis that's different. No, I got an object, it's consistent, everything's done, there's nothing wrong with this, it's a face, I got it.

Even if we vote, doesn't the problem still exist? If we're on the bar without the dot, it wouldn't be voting. The columns that are where the dots should be wouldn't be participating in voting. All the columns that are voting have equal evidence for the bar and the eye, so they wouldn't be able to resolve it. If the dot is there, there's additional information. If the dot is not there, it's just the vertical bars. So what's the problem with the dot not being there? If the dot's not there, then the columns that are observing the empty space where the dot would be wouldn't be participating in voting, so they wouldn't contribute any information about the absence of the dot.

but that is consistent with being a bar. Then, the columns modeling the bar would need to learn about the absence of votes. No, because everyone has models of both, and all the evidence sums up at the bar. There's no other evidence to support it. The same number of columns say it's an I as the number of columns that say it's a vertical bar, but they wouldn't, because I have this additional information on it. I don't get it. They're all silent because there's nothing there. If there's nothing there, it's a bar. I'm missing this. Maybe I'm being dense about this, but the columns don't know it's a bar; they have equal evidence for the bar and the I.

I see.

It's basically one model being a subset of the other. The missing parts from the subset, that information is not communicated anywhere, because we don't communicate about things not being somewhere.

I think it's still going to work, hang on.

I don't see it. Maybe we need to modify voting somehow. It sounds like such a simple problem. We come back to it—this was one of the first items I put on the Future Work Roadmap three or four years ago, because it seemed so simple: send off-object observations and use them. Every time we come to it, there are tricky things. I'm still not convinced there's a problem. Can I go through a few more slides for what I wanted to show, and then people can think about that? I definitely take on board your point, Jeff. How much do we consider the infinite space? I like this example—I think this was from Chris Summerfield—and it was a critique of Bayesian models. If your doorbell rings, you don't think, "Is that the Prime Minister of Finland?" with a low probability. You don't cover the infinite hypothesis space that is possible. I take your point: lacking further reason, would you really consider other hypotheses? But I still feel there are conditions where you can say to a human, "This is the task," and a human can solve it. Maybe I'll just—again, the human being—separate a human looking through a straw versus a human looking at the whole retina. Yes, here's a—okay, go on.

Misha, I'll answer your question briefly. I'm not sure if you were here at the start. We are planning on passing a no-feature observation, but one of the tricky things is we don't want to store that in the model. We don't want to store "no feature," at least, and that's another thing I'll talk about later—maybe how we could get a hybrid. But at least we don't want to store "no feature" throughout all of space. That's part of the trickiness of this.

I'm not sure it means the process and no feature, then. If you don't store it, what's the point of it? What is it? Okay, the very first slide that Nils showed, where we all agreed that if the hypothesis was that we should be sensing the mug, but we're not because we actually moved off of it, then that hypothesis was wrong and we should delete it. That currently doesn't happen in Monty. We've left them up. If I move off the mug, why would I delete the hypothesis of the mug? Can you go to that slide again, Niels?

Yeah. The hypothesis was that we should be on the mark after the movement, but actually, we were off the mark. This is the straightforward condition. That's the straightforward condition why we thought we should add off-object observation—it's just a technicality that we haven't been using them at first. The first time I wrote this down as a to-do, it was like, "This will be easy. Let's just process it and say, 'That hypothesis was wrong.'" But then, if we do sense that we are now off the mug, or in empty space, we don't want to store that in the model of the mug. As soon as you move to 2, the pink 2, the red 2, you eliminate the current hypothesis, right? Yes. But right now in Monty, we don't, because we don't send that observation to Monty—to the learning module.

Yes. Why do you have to send an observation? Isn't just the fact you didn't get one sufficient?

We also don't send the movement.

Now you confuse me. I'm a learning module. I'm in a simulation, and I don't get any input, and I'm predicting one. That's local knowledge. I don't need to send in a null hypothesis. The fact that I didn't get one is a null. It's the same thing. Basically, we need to step the learning module. You have moved, you should have perceived something, but you didn't. You predict something, but we're not giving you anything because nothing was perceived. It's a local thing. I didn't get an input. I'm a bunch of neurons. I didn't get an input; it's not like the retina had to pass me a null hypothesis. In Monty, we need a data structure that at least tells the system this isn't 5. But in neurons, there wouldn't be anything present.

Can I ask, why is it that we don't send anything to the learning module in that case?

In that case, we would want to, because it seems straightforward, but it's all the complications it introduces—that's basically the issue. In that case, we should say, "You didn't observe anything," and it'll eliminate that hypothesis, and it'll be great.

In general, I recall, the sensor module processes the observation, tries to extract features. I guess there aren't features to extract, because we're looking at the void. It decides not to send anything out, right?

Yeah, the useState variable—this is very code-like—I think the useState variable is, if the on object is false, then the useState will be false even if we extract other things.

There are several reasons why we don't send observations to Monty. For example, if the features haven't changed significantly, we wouldn't send it to the learning module. This is something we definitely want to keep on the roadmap. We want to at least send a movement to the learning module, even if we don't have any features—no location, orientation, or morphological feature. We still want to send the location chain and use that.

I agree. That could be a way to tell: if there's no feature sent, just a location, don't incorporate that in your model.

Exactly. At a practical level, Scott, part of the issue is it will change a lot of things. But you're right, it's a change we should make, at least that one. We want to make sure we don't store these during learning, but there are ways we can handle that. Also, the buffer tends to make use of off-object observations to filter and so on. Unfortunately, once we change this, there will be some knock-on effects. Maybe I'll quickly just—I'm not following, I couldn't have told—yeah, it's a cool thing. For a suggested approach, what I'm suggesting is that the learning module, when you've moved to this location and are no longer looking at the vertical bar, should have that representation go quiet. I'll suggest what that means at a neurobiological level in a minute. The representation for an eye is actually going to get negative evidence. What's nice about this is that, as well as potentially helping with some of this, it has a natural biological analog. For the representation that goes quiet, this is the out-of-model movement. Dendritic spikes, which are generally associated with predictions, are known to have much longer time constants that maintain the membrane potential, versus spikes at the soma and the axon.

Practically, in Monty, this could mean the evidence level stays the same or slowly decays when we move out of the model.

If we move back onto the object shortly after—let's say we go up, there's no dot, we move back down—the evidence will continue to grow and maintain its level. But if you remember the I, it's going to get negative evidence because there was an actual prediction error. That would separate the hypotheses and enable us to say, "Oh, it's a vertical bar."

From a biological perspective, a dendritic spike can maintain the membrane potential at this elevated level without necessarily initiating a spike. That satisfies the property of going quiet: when you move out of the model, we don't want those cells to be telling that column or other columns that this object is still here. It would be primed to still expect that object, but it wouldn't be telling other columns the object is here.

That sounds right. Good observation. I'm having trouble relating that to the overall big problem, but I think this is a mechanism by which you could maintain a hypothesis that's not actively doing anything. The cells will be primed by the previous hypothesis, so when you go back, it's not completely forgotten. We've talked about how part of that will be from top-down feedback. Over longer time scales, if I'm walking around my room and turn around, it's going to be top-down feedback telling me to expect the table or whatever. But this is more at a cellular level over very short time scales, as your eyes are darting around. I'm not sure I agree with the second thing you just said about top-down feedback. We're trying to differentiate between inferring an object and not being sure what it is, or having a model and wanting to make predictions for the model. When I come back to the room, that's me having a model and trying to remember where things were, making predictions, but not inferring the dot from the I. I'm just talking generally. Maybe that's a distraction. I think that's a point we agree on, so maybe it was just the way I worded it. We considered that as a possible alternative to going silent—that the hippocampus could be building a very quick temporal map: here used to be the vertical bar, now I moved off, now I moved back on, so re-invoke the vertical bar representation. Maybe, but when we thought about it, it didn't feel like a good solution, especially because we didn't know yet that it was the vertical bar; we thought it could be the eye.

I want to go back to what I thought was a critical observation, or at least I'm arguing it's a critical observation. If I'm thinking about a single column inferring an object, I'm going to be following the edges of the object. That's all the information—there's no information in the middle of the object, and no information from outside the object. I'm following the edges or features of the object, which are discontinuities of some sort. I'm going to move continuously around this object, and because I have no other reason to move off the edges, I'm just doing a model-free movement around it. If I come up with a hypothesis that's consistent with all the data, I can move here and use hypothesis-driven predictions. I'm not going to search for another model or for other features. I find the bar; I'm not going to search for the eye. I'm done.

That bar could be part of many things, but I'm done. I've got something that's consistent with my data. Maybe that dot is off somewhere in space; I don't know. Why do I know to look for it? I have an answer. It's not like my data is inconsistent—there could be other answers. I think the answer here is that it's going to settle on the bar. That's it. If the dot is close enough to the bar so that as I'm going around the edges I detect a dot, then I have more data, but if the dot is farther removed, that's different. 

If you're looking at a line drawing of an airplane or a smiley face, you're not going to explore the internals at all. Again, we're talking about the straw example. In the straw examples, if I explored the internal space, I might say, "Oh, it's a circle. I found it, I'm following its head, it's a circle, I'm good." Maybe the nose, mouth, and eyes are small features in the middle of this big circle. I wouldn't know to look there. I wouldn't say, "Oh, can you explore the rest of them?" You might, but if you had the hypotheses... It's like you said with the Prime Minister at the door: I have a solution. I haven't come across any evidence against the solution. All the evidence I have is consistent with my solution. I'm done.

I think the example, or what we're considering, is essentially that you have the goal state, which is to determine whether the...

Zach? That's a different text, a different time. Fine, let's go there. Before I go to that, let's go to a separate test. Vivian brought up the issue: if we have voting among many columns, we would immediately determine the IR bar, and I assume that's the case, but maybe that's a problem. Vivian, you said you don't think that's going to work. I don't know, but I'm assuming that's going to work. If I have lots of columns voting, I'll get my right answer. If I don't have lots of columns voting and only have a little straw, I will settle on the first hypothesis that is completely consistent.

Now, there's a separate question: if I'm told to look for the eye, or if there's an eye here, at that point I would invoke the eye model. I would say, "This is consistent with the eye model," and I would suspect that it appears consistent with the eye model. But I wouldn't just stumble across it; I'd have to. In this case, it's a vertical bar. Would you have some representation of which model is the simplest explanation? Would they have associated complexity, and you go with the simplest one? I would say, when you come... We're talking about the straw. If I was not given any other priors and just asked, "What are you observing out here?" I would, in some sense, stop the moment that all my observations are consistent with a single object, and I haven't discovered anything else by tracing the lines or features of this object. But they're also all consistent with the eye. 

Again, with the straw example, I can't know that. I think, actually, I'm going to go with the simplest one—the one that solves the problem. I'll reach my conclusion. Maybe there's additional evidence in the world that would change my conclusion—this is a general truth of life—but if all the evidence I have right now is consistent with the conclusion, and it's not partial, it's all there, I'm going to go with that hypothesis. You have a representation of how simple the explanation is, so you go with the simplest explanation. No, I go with the first thing that's completely consistent with the data I have.

Are you tracking how much of the model you have covered? In the bar example, I've observed the entire bar; there's no pieces missing, it's all there, I'm done. It's not, "Oh, I have a partial bar, I need to keep looking." We said, "Oh, I could spoil the whole thing, it's there, I got a bar, there's no question, it's a bar." So you can track if you have covered the entire object. I don't know if I have to keep track of it; at some point, you have enough data to say, "This is consistent, everything I've done here, I can make some predictions." I don't know.

When do you stop looking? I don't know the answer to that question. I'm asking so many follow-up questions because if we go with that premise, we have to redesign all of our experiments going forward. We can't do straw world experiments anymore. We can't do experiments with one learning module anymore. Once we have a more complex environment... No, you can.

Go ahead, RJ.

In Monty, we are naturally driven by one task, which is recognition.

When the task is just to describe what I think the bar is, if the task is, "What could this be?" then it will say bar or eye, because it knows both bar and eye models. If the task is, "What is this?" then Monty will have motivation to explore, because it wants to narrow down to one hypothesis only. Even if it explores and sees only the bar, we force Monty to explore for a minimum number of steps. If you take the Yale dataset and say, "These are known hypotheses, we need to determine between these," that's additional information you don't always get in the world. 

Maybe it would be helpful to clarify: what you're discussing is more the policy. The point I was originally trying to raise is that even with the policy that moves you to where the dot is going to be, Monty currently cannot distinguish the hypotheses. That's the issue.

Maintaining this hypothesis, but negating the one that incorrectly predicts it should perceive something, should help with that situation. Now we're getting down to some important implementation details, which may not be as conceptual as they sound. It seems like all the information is there. We still need to update so that we do the first case you showed, where we need to eliminate the hypothesis if we're off the object but the hypothesis thinks we're still there. 

Think of it this way: a learning module will behave as described. It will reach a conclusion when it has enough data that is completely consistent with one of its models and there's no contrary evidence. Then it's done. It wouldn't recognize—if the dot's too far away and it didn't see the dot, it wouldn't think about the "I." However, if we want to impose other requirements, like "this is what you should be looking for" or "these are one of the 70 things you should be looking for," or "let's go through these in order, is it this," then you have extra information. Every time you do that, the column will invoke a different model and say, "Okay, I have a new model, I can make predictions based on that new model. Is the data consistent with it, and do I make further predictions? Oh, it could be an 'I,' let me see if it's an 'I.' I didn't consider that because I didn't have any evidence for it, but now you want me to look for it. Now I'm invoking the 'I' model, I will go look for the dot." 

The learning module could itself be pretty simple. It doesn't have to search through all the hypothesis spaces. It's just going to search until it finds one that works and then says, "I'm finished." Maybe that is what happens, but then I agree with Vivian that we almost need some sense of model simplicity. What is special about the vertical bar? Why is that privileged over the "I"? In existing Monty, they would both have equal evidence at that point.

James had a model completely consistent with the data, which to me means how much we've covered of the model. We'll be adding evidence proportional to how far we are in exploring that model we have saved. If we're at the end and we've covered 90% of the bar, we'd be adding more evidence than if we've covered 70% or 80% of the "I," because there's a dot. I don't like the idea that we have some sort of tally. It's more like it doesn't even consider the "I" once it's settled on the bar. Pick one randomly. Do we have to be deterministic about it?

It could, if it just said, "Okay, I'm going to pick a hypothesis, and if it works, good, I'm done. I'm not going to look any further."

So if I randomly pick the "I," then I look for the dot, and if it's there, I'm done. If it's not there, I eliminate that. If I pick the bar and I see all the bar information, I say, "I'm done." I don't look for the "I," because it could have been a random choice to begin with.

Just imagine you're looking through the straw, and you have no idea about anything else in the world other than what's in your little view. You're just following this line around until you recognize it, and there's a ton of other stuff that could be going on in the world, but you can't deal with all of it. Just deal with what you have. This is a strained example because we're doing the straw example. In reality, it wouldn't happen that way. In reality, it would be voting, and I think voting would solve all these problems. But in the straw case, I think that's what it would do. You could pick a random hypothesis, a random model, and follow it to its completion, and if it's all consistent and there's no alternate evidence, you're done. I wouldn't think to look for the dot. In reality, you don't spend forever on every object trying to cover all of it until you move on to the next thing. You get a reasonable amount of evidence and move on; you have other things to do.

Besides how the policy would be and how efficient it would be, I think the general issue still exists. The general thing is still that we want to process the observation. If we expected it to be an "I," like Tristan wrote, if it was in a word and we expected an "I" to be there, and we moved up and the dot wasn't there, we still want to be able to deal with that case. Why wouldn't we be able to deal with that case? Why wouldn't we notice that the dot's not there, and now we have a problem? We don't see an expected feature. We would have to make the changes we discussed today, but maybe there's no controversy about that.

I think we've settled on a reasonable approach. There are some edge cases, quite literally. We don't necessarily need to go through them together, but maybe there's time. Just to recap, the necessary changes in Monty are to, first, send the observation to the learning module, which we currently don't do, and second, in the learning module, process that observation so that if the model predicted to see something there but it didn't, it gets negative evidence. If it didn't predict to see something there because it's an out-of-model movement, silence that hypothesis and keep it for a short amount of time so you can get back to it when you move back, the next step.

Does everyone agree with that? I just watched you at the critical moment—my phone decided to switch my audio to something else.

I'm sorry. Can we back up a little bit? Oh, I think we all just agreed, so that's... Okay. Yeah, I was just kidding. Good idea. Let's keep going. I have a question. Where did you lose it? About, I don't know, 10 seconds ago. This is your way of silencing me. This silence just sounds. Sorry. Who was that—Kristen who just spoke? I forget, I didn't hear. Bobby. Will. Oh, Robbie, Rami, I'm sorry, Robbie.

I forgot, I'll ask. Do you want me to repeat something, Jeff? I think there's no harm in saying it again. You said it enough. Okay, so I'll just start at the beginning. The two changes we would make to Monty are: one, to send the off-object, or no-feature observation, to the learning module when the data isn't there; and two, to process that observation. The way we would process it is, if the model predicted sensing something there—like if the model of the eye predicted there should be a dot—but it doesn't get anything, it gets negative evidence. If the model didn't predict anything because it was an out-of-model movement, the model doesn't store any feature at that location. Then we silence that hypothesis and keep it for a short amount of time so we can get back to it if we move back onto the bar in the next step.

What if I had an answer to the problem, I've inferred something, and I just decided to go someplace else and test another hypothesis? I don't want to forget my first hypothesis, which is correct. It looks like a bar, but you want me to see if it's an I. Let me go see if it's an I. No, it's not an I, but don't forget, I was still thinking bar. I didn't get any evidence against it. It's a hysteresis effect, right? You don't want to forget everything just because you got distracted. Once you get older, it feels like you do this sometimes—you're in the middle of something, then the door opens, you look over there, and when you come back, you can't remember what you were talking about. Or like Dory the Fish in Finding Nemo—she gets distracted and has no idea what she was just thinking about. We don't want that to happen.

I think, actually, Vivian, I won't go through the literal edge case. I call it the literal edge case because it's a scenario where, if you're near the edge of an object, weird stuff happens. But basically, as long as we're doing this clamping of the outer model movements, it's not an issue. In standard Monty, one of the main ways we eliminate hypotheses isn't by calling them out-of-model, but when we have a location hypothesis that's far from the existing ones, it gets negative evidence.

Say that again? But those are when they predict to sense something—is it only then? In those cases, they would predict to sense nothing because they're far away from the model, but you do get an input, so there's a prediction error.

You have to be careful, too, because if you go beyond your current hypothesis and into some other space randomly, you might find something there. That's what I mean, because often there will be something there. Up until now, we've been talking about empty space, but actually, in a lot of environments, there's just something there. The key thing is, under hypothesis A, if a column is observing outside of hypothesis A—whether it moved there or it's just a different part of the retina—we should ignore its input. Just ignore it, even if it's some input. If I'm outside of hypothesis A, it's irrelevant over there. It could be another object, it could be something else in the world. If object A makes a prediction about something at some place and I go there, it should be exactly the right prediction. It should be that feature that I'm observing, and that would be consistent. But if it was a different feature, then there would be negative evidence.

Then it's a more radical thing, where we basically say any kind of out-of-model movement—any movement beyond, nowhere near stored locations—we just, I think clamp is maybe the wrong word for the evidence. What we want is for it to slowly decay as long as we're out of the model, like in the EMA neurons that Rami was working on. I'm still thinking about when a bunch of columns are voting—a lot of them are going to be off the model at any point in time. But if they're voting, they all have the same hypothesis maintained in Layer 1. The voting neurons are still voting. Just because I'm a column and I'm off someplace else, my input doesn't really matter at this point in time, but I still know what everyone thinks it is. If I become within object—if for some reason I move and now I'm within object—now I can make a prediction.

In a real scenario, most of the columns will be moving around together, and most of them will be off the object at any point in time. The hypothesis would be maintained by the voting neurons.

That could help stabilize it. I think that would stabilize it. That is persistent. Maybe with the final time that's remaining, I'll just talk a bit more about this color filling-in thing. I think we have an action plan, something we can implement. We don't need to change the policies—I think we agree on that—but we also agree we need to make some minimal changes for Monty to work a bit better. But the question is, can we do more?

Good out-of-model movements provide positive evidence. Does this mean storing some concept of empty space? But what if you move on to another object? Of course, you can't then predict something in that case. To motivate this, imagine how you represent an empty room. That's very different from an unexplored room. You really do seem to have some concept of emptiness when it comes to motor planning and things like that. But why is it—why isn't empty just, at this point in the reference frame, there's no object expected?

But in the current implementation, the current thinking is equivalent to having never looked at that location.

Fine. But maybe I don't know the difference.

In this case, you have looked there and know nothing is there, versus not knowing what's there because you're not storing anything at that location. Another example is the concept that an object is solid—space beyond the object is empty, but if I try to predict what I would feel inside the object, I know there's some solidity to it. I don't see this as a significant problem. Maybe another example is what we predict when looking at 2D objects. For instance, the room is not a cube; it's a set of walls. You've explored those walls, and there's space outside and inside the walls. I'm not sure there's much difference between those two. It's just a set of edges and walls that go around, not a cube with an inside and outside. I'm not sure the inside is treated any differently than the outside. Even if I haven't explored all the areas outside or inside, it's not important, as long as I've explored enough of the walls.

In a situation like this, we look at the entire room, and our retina can quickly scan over all parts of it. We don't see anything inconsistent with a wall or ceiling; nothing is blocking those things. The objects in the room are like the dot on the eye. In this case, there are no objects in the room. The hypothesis for a crowded room will receive negative evidence. The hypothesis for an empty room will be stable and just wait. It gets complicated with vision looking at the whole room, because in some sense, the dot never occludes the bar, but a chair in the room might occlude the window. It's hard to come up with something in the room that doesn't interact with or occlude any other part of the room. When looking from a distance, we're not feeling our finger along the way. If I were blind and walking along feeling with my finger, I might have no idea there's a chair in the middle of the room. From a vision point of view, it's hard to miss, because it would occlude something. I can't observe the wall without seeing the edge of a chair, so I'd have to deal with it. But if I'm just touching, then the inside and outside of the room are all the same. There's no reason to encode that there's nothing in the room, just like you don't have to encode all the things that are not outside the room. The room is the wall.

I want to skip ahead because it ties into what we started with—this idea of filling in. I think we've talked about this before. We didn't solve the filling-in problem at all. If morphological features have a concept of directional prediction—an orientation—that also provides a prediction about what you would sense. The orientation is part of the feature, so if I go there, I expect that. Maybe it'll be clearer in the next slide, but imagine we want a representation that helps us predict: this is empty space, or in another direction, I'm expecting white anywhere. I don't want to store white at all these points. It's going to be a particular color, texture, empty space, or solid space.

It feels like a fairly simple and biologically plausible spreading function. When you sense a point, this could spread until you reach something. If you're sensing here and want to know whether that's empty or solid space, this kind of ring goes out, and the first thing it touches is this point. If each point has, in a certain direction, stored information from when it was sensed—like empty space beyond and solid below—then it's actually stored. So this location, I'm going to assume, is empty. This relates to the color filling-in idea: if this was a red edge, red on one side and blue on the other, and I'm sensing here, I spread out and see that in my stored model, the nearest thing is this edge that's red on this side, so I'm going to assume I'm seeing red.

Can I push back a little on this? I think the general ideas seem good, but I don't think we should be storing anything off the object and saying there's nothing there. Anything off the object is stored along the boundaries. It seems more like this: the golf ball is the object, and the background could be trees, dogs, or mugs. In this case, I'm assuming a finger is moving, hovering above. Let me think about vision for a moment. If we're saying it's color, I've learned that golf balls—imagine you just have a circle, not a line, but a solid circle. We could do a line circle, which would just be a bunch of features at locations around it. Now, we're saying the edge of the line is not really a line, it's a color boundary. 

The first question is, how do we know the white is inside the color boundary? Why is the white part of the object? How do we know where the object is and where the world is? I don't know how to answer that, but let's assume you have some idea of closure. At the boundary, you have a transition where there's a color boundary, and you just assume that everything in the other direction is continuous at that same color. Exactly. And you haven't come across any other learned points. If you had another learned point, you would rely on that to tell you what to expect. In this case, I don't think I'm going to predict anything about the outside of the golf ball, because I don't know what's there—it could be grass, carpet, who knows? The proposal was maybe this would help with the empty thing, but maybe it's more appropriate for just doing color and texture. It's an interesting question: how does it know that, at this point on the object, in one direction is the world and in the other direction is the golf ball?

You could do something similar for edges. Imagine your edge is now a color edge: white on one side, something else on the other, and everywhere on the object you have the same white transition. Imagine there's no other features inside. In this case, you see the dimples, but imagine it was just a pure white disc. Then the idea is you just keep going until you get to the next edge, and you assume it's all the same. You just keep going across.

What I'm struggling with is almost a metaphysical question. Is there a neural representation for the white in the middle of the golf ball? Are there actual neurons firing, representing that white? It's a perception that it's there, but there are many things we perceive that aren't directly correlated to neural firing. An axon could spike and you feel pain; another axon spikes and it's a color transition. It feels like the location representation that's active would be this location in the reference frame, but the feature prediction that's active is deferring. In that sense, kind of white neurons are active. But are they? I don't know, is there a feature there?

We have these color blobs in the cortex, and maybe color in these cases is handled by some special mechanism.

I think the general idea is probably right. You have to absorb the feature from the nearest boundary or from the nearest point where you have a feature. There's no feature here, but why do we perceive that? I have no idea. I don't know what that means. What's nice about this is it would be relatively straightforward to implement in Monty. Even at a retreat a while ago, we discussed a similar idea as a way to have sparser models. For surfaces, we could just store a few nodes, and then you rely on those for all features.

This is something I've been thinking about. We have a bit of that already. In our distance measure, we look further for neighbors along the surface, in the direction of the curvatures, than we do away from the surface, in the direction of the point normal. We don't look far in that direction when searching for nearest neighbors. The direction in which the surface normal points tells you which direction is space and which is the object. By basing it on the curvature—the surface normal. The surface normal always points out of the surface. One of the issues I tried to deal with early in the project is thin objects. At the most extreme, think about a sheet of paper: one side is red, the other is blue. If you're sensing a blue point, you know immediately which side of the paper you're on, but in Monty, if you're just looking at the nearest point in the model, the red points are very close to the blue points. In Monty, I think we still do this: we look at the surface normal and only consider points with surface normals in the same direction as the one we're currently sensing, so we would not consider the red ones, because their surface normals point opposite to what we're sensing.

Yeah.

So that's about as much of a concept of surface as Monty currently has. It's interesting—I've asked myself a question like this: do you perceive the color of the surface of an object as its neural representations, or do you have to go and ask? I could ask the system, "What is the color at this point?" and then calculate it by looking out, finding the transition points, and extrapolating from there. That's a different system than one that is representing red everywhere. It would be more like, "What is the color here?" and then you can answer it. I'm looking around the room, and it's not clear which of those is the right one. It's hard to separate the fact that I'm trying to ask what the color is from what I perceive if I didn't ask. As soon as you attend to something, it's like you're asking. What do you perceive without attention? You can't even do it by trying to use memory, because now you're attending to a memory.

It reminds me of when I grew up in the age of black and white TV. When I was a kid, we had a black-and-white television, and then we got a color television. We had both. I'm always struck by the fact that when you watch the black-and-white television, you have absolutely no idea that anything's different. Once you're in it and watching, you never ask yourself, "Where's the color? What color was that?" It just seems totally normal. There's absolutely nothing you don't understand. It's all completely normal. We have to do this when we see things in the dark—we lose all of our color sensations. If it's dim light, you lose color sensation. We're not even aware of it; we just don't know that color's no longer there.

So it makes me wonder: is color something I can ask about and then give you the answer to, as opposed to something I perceive? I think it's definitely perceived, because we have blind spots. We're constantly missing color at some spatial location, but my point is, there may be no neural representation at all in the middle of a red object until you ask the question, "What is the color in the middle?" Then you can calculate it, find the nearest thing, and bring it over, as opposed to there being a neural representation there all the time.

I feel like vision in the dark is a good example. If I'm looking for a t-shirt in the dark, initially, my vision seems normal, but then I realize I can't actually pick out the green shirt—it's impossible. In dim light, you lose the ability to distinguish color, or you think you pick up the green shirt because it's consistent with your expectation, but then you realize you can't tell. These examples suggest that color is just this extra piece you don't need; color transitions are represented on the edges of objects, but surface color or large areas of monotone illumination may not be represented. You can ask about them and get the answer, but they're not always present.

What would happen if you closed your eyes, put on glasses that are only one color with some kind of light source under your eyes, and then opened your eyes? There are no edges; you just see one color. Would you know what color it is?

Maybe this is related to Tristan's blinking thing, or the transition from no light. When I put on red-tinted glasses, everything looks red, and the only way I can infer it's a different color is because the color of the object interacts with the redness, so I know it's a darker red. If you could only see the world in one color—like watching a black-and-white TV, but it's blue and white—even then, if you're just in a saturated color, your sense of color goes away. You just don't know. There are sodium lights, or something similar, where it's pure yellow, a single frequency. You can see perfectly fine, but basically the only color is yellow. I've spent a long time in a room with that, and your perception of yellow goes away. You know it's yellow because you can still look at something else that's not lit by that light, like a pistol.

Things are either a shade of gray or yellow. All the light is that sodium. Do you lose the perception of yellow at some point?

because it's this Icelandic artist, he's famous for doing it, but I think they also used it before green screens. They could do some clever filtering. No, I'm talking about being in a room where there's no other light except for sodium light. That's the only thing that generates light. Exactly, that's what this Icelandic guy does—he fills rooms with it. Once I'm in there and can't see anything else, I would always recommend that for people. Let me send a screenshot of what it looks like. They're lit by the lithium light. The lithium light could change color when it reflects off something. It essentially looks like this: you see shades of yellow that border on black. I was wondering, if I spend enough time in this room, would the yellowness disappear? Would I no longer notice it? I at least didn't experience that. I stayed in for a while. This one was really cool. I'm sad I didn't go to this one. 

What was it like when you left the room? I don't remember anything unusual. For example, when you're running on a treadmill and you step off, you have that sense of moving forward, or you step onto an escalator that's broken and you have that jarring feeling. I don't remember anything similar in terms of color perception. Tristan, go ahead. He said, do we have detectors or cones? You're right, the cones detect specific colors. In dim light, the cones don't work at all. That's the problem. That's why you lose color. When the light is dim, the cones just don't work. I thought the reason you lose color perception, if you see only one color a lot, is that the cones fatigue, which is why you see afterimages after looking at a very bright red and then at a white wall.

Alright, maybe it's a little off-topic, but I think we have a hypothesis that the color across the uniform surface of an object is not actually represented at the locations. If you go to the locations and ask, what is the color here, the system extrapolates at that moment from boundaries.

Something like that. There would be complications, because then any point in a model is now considered to be making a prediction. No, I'm saying it's not part of the model. My point is that the interior of a blue circle, for example, isn't represented. If you go there, you can check, but maybe with color it's fine. It's more about the off-surface thing. It brings in that whole off-object issue again. I thought you brought back color, and I thought we could just address that. Color is definitely simpler. Should I do a summary? Do we feel like we've reached any kind of conclusion? I think so. We have an action plan for off-object observations. You've made good points about policies and how we think about that in terms of exploring hypothesis spaces. We talked briefly about color filling in, and also at the start. You had a specific problem you were trying to address, and the question is, do you feel like we've made enough progress? I think so. When Vivian and I talked about it yesterday, we thought maybe we now understand it. We've revisited it so many times that it felt unresolved until it was discussed more broadly, and also with you. Now I feel confident that we can update this task. The person in the community who's interested in working on it, Carter, can hopefully get started.

I'm worried now that the problem you brought up towards the end is a problem—how do we deal with it if the hypothesis is too far out of the reference frame? Currently, we just delete it. Probably something to talk about offline. I think it's solvable, because it'll just mean—anyway, worst case, we have the fallback of just saying a human won't go there, and so Monty doesn't have to go there.