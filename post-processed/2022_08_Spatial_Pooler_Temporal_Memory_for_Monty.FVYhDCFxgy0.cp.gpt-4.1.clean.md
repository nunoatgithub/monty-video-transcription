I guess this is a year-end review. There are around 30 slides. I'll go through the parts I've already covered in detail relatively quickly. I want to take time to discuss what I took away from these projects and the key next steps I think could be taken. Questions are welcome.

My time here started last August, and it's August now, so it's been a year. I worked on four key projects: the dendrites project, a deep dive into spatial pooler and temporal memory, integrating that into Monty (the temporal memory for Monty project), and for the last two or three months, working on grid cells for Monty.

The Dendrites Project started before I joined, but I contributed a few things. One of the key intuitions was comparing neurons in deep learning to real pyramidal neurons in the neocortex. The basic idea was that artificial neurons are vast oversimplifications of what's really happening. On the left, you see an artificial neuron diagram, where the soma is replicated as a linear weighted sum, and the axon is the output of a non-linearity. In reality, this is not how dendrites or neurons work. Neurons have thousands of synapses, and these dendritic segments act as independent pattern recognizers by checking if a set of synapses are connected. This depolarizes the neuron, causing the cell body to fire. This deep complexity is completely missing in deep learning, and this was the impetus for the dendrites project.

The first part was augmenting the standard point neuron model. At the bottom, you see the feedforward input, which is the standard part of an artificial neuron. The key contribution was adding context vectors, which mimic basal dendrites. These context vectors have one weight per segment, and you feed a context into this area. You compute which segment has the maximal response to that context vector, and that modulates whether the neuron fires. I'll go step by step here. This full architecture was the key contribution of that paper. The first contribution was adding dendrites to the standard point neuron. The second was adding sparse weights. Instead of a fully connected dense layer, we drew inspiration from neuroscience and decided these connections should be sparse. This reduces gradient interference in deep networks and helps alleviate catastrophic forgetting in dynamic environments and training scenarios. The third contribution was adding sparse activations. Instead of the classic ReLU activation, which at best gates maybe 50% of the neurons, we introduced a variable sparse activation where only the top k neurons become active.

This project was well underway before I arrived. Most of the continual learning experiments were already run. I started working on the multitask reinforcement learning experiments, looking at the Meta-World V2 benchmark, where a robotic arm needs to solve a set of tasks. Ideally, you would learn all 10 tasks together using some reinforcement learning algorithm, but a standard deep network struggles because learning one task while also trying to learn an orthogonal task with no semantic overlap causes a lot of gradient interference during training. We tried to mitigate this by introducing the active dendrites network.

Some results: for the reinforcement learning experiments with dendrites, which used about 7.2 million parameters, we had a model with about 88% average accuracy across 10 tasks. A regular deep network with about 500,000 more parameters was about 11% less accurate. An even larger MLP showed that adding more parameters doesn't solve the problem; even with 2 million more parameters than our dendrites model, it still suffered from catastrophic and gradient interference when training orthogonal tasks. This was good to see.

One of the key figures of the paper shows that a neuron in this model, for both multitask reinforcement learning and the continual learning experiments Karin was working on, responds selectively to particular tasks before and after training. You don't want a neuron to respond to all tasks, and here you see that it responds selectively, meaning sub-networks automatically form in the deep network that correspond to different tasks. These sub-networks are non-overlapping when they don't need to be and overlapping when they share knowledge.

We got some good PR from this. We published a paper in the Frontiers in NeuroRobotics journal. All of us gave a small talk on Yanik Kil's YouTube channel. Recently, Karin and I presented at the Sparsity in Neural Networks workshop about this work. Just last Sunday, Karin and I recorded a podcast for the AutoML podcast, where they wanted to hear more about how dendrites might help with things like neural architecture search.

Just a question for this slide, or the one before with the context: did you have time to play around with task composition, where you could have a similar task that you try to generalize to, and you recall a similar context, or compose two contexts that have been learned for a task that requires both? That's actually an interesting idea. I don't think we tried that. We used fixed context vectors and let the dendrite segments learn whether a particular task was semantically similar to another, but the context vectors themselves were entirely orthogonal to each other. They were just one-hot encoded vector IDs describing the task.

Also, a criticism of Permuted is that it's really difficult to study transfer knowledge across tasks because the tasks are basically the same, but independent due to permutation. A simple example might be to split into the first and last five classes, train separately on both with different contexts, and then see if you can superimpose the context vectors to handle both situations, like classification across all ten.

That is a good idea.

No, but really interesting. I'm sad there's not a screenshot of you with the sunglasses. There's a reason I put this picture up instead of the one where I didn't have sunglasses.

So, what's next? We briefly talked about how we could use dendrite segments with sparse weights instead of dense ones. We didn't really explore that, but it was discussed. Going back to what Niels was talking about, I think more compelling benchmarks would be interesting. Not many papers have explored continual reinforcement learning, but that would have been cool to try instead of multitask reinforcement learning. Instead of permuted, where you learn all ten tasks together for a complicated robot setup, you would learn them in sequence and not see all tasks after finishing learning on them.

Another interesting thing to try is more realistic dendrites. Right now, we treat dendrites as standalone context processors—floating weights that don't get touched except when updated by backprop. We could make more realistic ones by connecting neurons laterally by dendrite segments within a particular layer. That's not really a more realistic dendrite, but more of a realistic architecture. I should say more realistic deep learning dendrites. The change would be connections between cells in the layer. Is that what you're saying? Yes. So, it's not really making the dendrites more realistic, but a more realistic neural architecture. That's right.

Of course, we did do that in other places. The whole temporal memory is built on that idea. This is just for context.

Onto the fun stuff. I did a deep dive into the spatial pooler and temporal memory, because that was something I really wanted to work on when I first started, but we were working on the dendrites project. We wrapped that up and got some really good results. Starting in January, I worked on understanding all the nuances behind the spatial pooler and temporal algorithms.

When I was exploring this, I found that the fastest versions of the code had been written in C++ and linked to Python via some PyBind library. One of the key things is that it worked quickly, but it was hard to decipher and learn. It was also hard to optimize the algorithm. It was built on a very abstract sparse matrix library called Sparse Matrix Connections, which I think Marcus wrote. It's detailed and works well, but it's also hard to improve and optimize. That was my first thought when I started this deep dive.

What I ended up doing was reproducing all of the spatial pooler and temporal memory code purely in Python. Anyone can use that now. If you go to the frameworks/HTM folder inside research, you can find the spatial folder written completely in Python and the temporal memory algorithms written in PyTorch.

This is a snippet of those three. Would those implementations be slower than the C++? I'm getting there. I would like to say this is the most readable code I've ever seen. You can almost read the code and see what's in the paper directly. It may be slower, but it's incredibly instructive.

Thank you, Ben. I'm just trying to understand the point—sometimes we spend huge amounts of effort making things fast, moving away from Python, and now you're going the opposite direction. You're moving to Python. I'm not questioning it, it's just funny because we spent all this time optimizing things to C, and now you want to go back to Python.

You're right, but when I started this, my first intention wasn't to make it faster. I already thought it was pretty fast. My intention was to make a readable version of the code where anyone could understand the algorithm just by reading it. If you really wanted to build upon the algorithm, you would do so here in this prototyped version and then move it to C++ to make it faster. That was my idea.

But in terms of speed, this is a questionable outcome regarding whether anything here was useful. The Python spatial pooler is readable, fast, and easy to build upon, which is good. It's almost as fast as the C++ version, with negligible difference. You could use either and be perfectly fine.

The temporal memory code is reasonable and readable, but the runtime is really slow compared to the C++ version. I moved it to Python specifically so I could run it on both CPU and GPU, but I realized that even without moving it to GPU, the operations for the sparse matrix multiplications are still inefficient. Moving to GPU would really change that. The biggest outcome is that it's a good learning tool to grasp all the nuances of the algorithm for anyone starting with HTM.

How much slower was it? Roughly 26 times slower from my last experience. That's significant. One thing to note is that sometimes these operations take the sparse representation, expand it to a dense one before processing, and then reconvert the results back. I was exploring using some of those sparse libraries, but I never got to do it. What I'm doing here is actually a dense matrix multiplication. I intended to do a dense matrix multiplication, and it was just not optimal. They could have changed it when I looked at it, but at the deepest level, they're just doing dense multiplication. The only savings is in storage representation.

The fact that it's even slower than that is a testament to the cost of moving back and forth between the representations.

There are some good takeaways from this. One thing we discussed a lot, but I never got to do, was using the spatial pooler to create SDRs of image patches. In all our Monty experiments, we faced the problem of how to create SDRs for real-valued data. I presented some work in the temporal memory section where I used a coordinate encoder strategy, taking real-valued numbers and turning them into SDRs. An interesting idea I wanted to try was using the spatial pooler to automatically do this for image patches. I never got to do that, but maybe someone can try it in the future.

Regarding the temporal memory, even though the code I wrote was not much faster than the C++ version, we realized that when I tried temporal memory in the Monty project, the C++ version was still very slow after ingesting a lot of data. The reason for this was twofold: first, you're not deleting unused DTIC segments, so the sparse matrix becomes exponentially bigger; second, sparse matrix multiplication in the C++ version still uses for loops to compute a map, and while it's relatively efficient because you only loop over non-zero values, it's still a for loop. I feel like that could be improved. Maybe the next step would be to write a C++ version without any sparse matrix multiplication at all. I remember we briefly brainstormed the idea of using pointers to calculate the synaptic overlap between a given SDR and a set of D segments, instead of doing sparse or dense matrix multiplication. This is pretty mission critical because if we used temporal memory, the current C++ version, in any of the Monty experiments, you're really limited by how much data you can ingest. To fully use HTM's benefits, we would have to rewrite parts of this C++ algorithm.

It's a little surprising because when we first did temporal memory, I always referred to something I called fixed resources. In brains, you can't just add segments endlessly, and neurons don't get bigger and bigger. Growth can happen, but the number of neurons doesn't change quickly in most parts of the brain, and the size of the dendritic trees doesn't either. A realistic neural model would have constraints: a fixed amount of resources that you have to reallocate as you go. We did a lot of modeling that way for this reason, because if you keep adding dendritic segments, it will slow down; you can't do it forever. I'm surprised that's not in there, as opposed to a resource-constrained network. Lewis told me the same, but those versions are probably no longer runnable because they're written in old versions of C++ or the Python bindings are very old. Right before Marcus left, he translated some version of temporal memory to C++, which is runnable, and that's this version.

If we think about the future, that's probably the direction we should head—going back to resource constraints. The problem is you can't just keep adding resources indefinitely. That's probably the solution to these problems. It's also interesting to consider how the system degrades as you fill up the resources and can't add more, and you have to make trade-offs. One thing we found, which made sense logically, was that the system tends to generalize more. Things that could be seen as different in the beginning get grouped together, which is a nice failure mode in some sense. It's not an incorrect failure; it's just that over time, you lose the differentiation between specific events and things like that.

Everyone should know that if we want to work with this in the future, we should be thinking about fixed resources. Also, I have a question: what's the problem with for loops? Why are for loops inherently bad? I think they're done sequentially, so if you wanted to compute a map, you probably wouldn't want to do it sequentially. There's nothing wrong with them algorithmically; it's just that they don't allow for prioritization. Is that what you're saying? The way it's written, it doesn't allow for that.

One of the problems is if you're going across a structure, deciding whether to multiply or not, each mispredicted "if" is extremely expensive—about 30 cycles. The more you can run the code sequentially and deterministically, the better off you are with CPU architectures.

If you have a logic statement within the for loop, the for loop itself obviously has a condition, but once it goes through the first time, it's able to predict where the next instruction is at the bottom of the loop. It's not so much the "if" itself, but if it's a random value and you can't reliably predict which way the code flows, a misprediction means abandoning a deep stack of partial results and reloading the instruction cache. That's what's expensive. In the time it takes for a mispredicted branch, you could probably do about 128 multiplies. That's why it's really expensive to have mispredicted branches.

In this code, there are no conditions inside the for loop; it's already somewhat optimized, and we only iterate over the non-zero indices. We have that knowledge beforehand. To determine which indices are non-zero, you always keep track of them. The other problem is that you're doing a gather operation, so you have the coherency of the access to the memory not being cache-coherent, and that's the other side of it.

You didn't do it in a way that causes major issues, but when you mentioned the for loop, that's why I thought it might have been that simple.

I presented this project before, maybe two months ago. This was temporal memory for Monty. The whole idea was to use temporal memory to achieve sample invariance—can I disambiguate between many objects while observing features at locations on the object's surface?

This is a roadmap diagram I presented at one of the lunch meetings, also about two months ago. It's a summary of all the projects happening at that time. The first was Vivian's graph learning, which is invariant to scale and orientation. Karin reproduced the vector neurons from the paper, which are supposed to be invariant to orientation. The temporal memory part, shown at the bottom, aimed for invariance to assembly. The main question was: if I revisit part of an object I've never seen before, can I identify what object it is, no matter where I sampled from?

Going back to that discussion: how do I turn locations, curvatures, or any real-valued items into SDRs? I used an old technique called the coordinate encoder strategy. If I want to turn this purple point into an SDR, I find a neighborhood of points, select some pink points around the purple point, randomly pick some of those, hash their values to an SDR, set those indices as on, and the rest off. This is one strategy for creating an SDR from real-valued points. You can use the same strategy for curvatures, which is what I did.

This is the data I was working with. I had two kinds of objects: one where I uniformly sampled the object for training and testing using a clustering technique to find optimal training points, and another where the evaluation points were small parts of the object—what we called occluded objects. In this case, most of the object was occluded, and we could only see part of the points during testing, represented by these green points.

I went over this briefly in the past and don't want to cover the algorithm specifics again. During inference, we want to predict the object with the highest total overlap between two sets of cells: the winner cells collected during training for these curvatures and coordinate pairs, and the predicted active cells collected during evaluation. The sets with the highest overlap are summed, and we try to determine which object the testing point most likely came from. I presented these charts before—hopefully, they look familiar. This was a simple experiment for evaluating on occluded objects. The question was: how many testing points do I need to get high testing accuracy if I only see part of the object? In this simple experiment with about 13 objects and 50 training points, if I had more than 500 points in the non-occluded part of the object, I would get about 100% testing accuracy. There are many caveats to this experiment, which this slide describes. The locations I worked with are globally fixed; if the object moves in the environment, this method fails. So, I wanted to work on achieving translation invariance by using grid cells to encode the location of the sensorimotor on the object's surface at any time. I also want to address speed issues. Because I was using the C++ temporal memory version, I could only ingest data from so many objects before speed became a bottleneck. Is it possible to scale up an instance solution? I know you did some parallelization code—could we use a library instance to make it viable? I haven't explored parallelizing TM like that over instances; I just used one instance. Of course, I have no idea if it was all local. Either way, there's the core problem of creating too many segments, and as Jeff said, I probably shouldn't be doing that.

On the last slide, do you have the same chart for the case without occlusion? Yes, I do. Is it fewer samples? It's significantly fewer—50 or 100 testing points if it's not occluded, since you're picking uniformly random testing points. It's the perfect case.

I started working on grid cells for the last two months. To recap, a grid cell module is a term I'll use a lot. These are sets of cells that share the same scale and orientation; activity across multiple modules can encode unique locations. There are two existing implementations of grid cells. One uses an anatomically consistent version, with a rhombus-shaped lattice of cells. The other is a simpler version that doesn't use Gaussian estimation processes; instead, it uses a simple rectangular lattice, but it's functionally equivalent to the anatomically consistent one. That's the one I used. These were two existing code bases we had.

When was the rectangular one created, and what was it for? I believe Marcus originally implemented it for the columns plus paper. Then, to make the figures look more biologically relevant, he adapted it to the rhombus one. He was particular about these things, and I don't remember him ever talking about the rectangular lattice, but maybe he did.

The key point is that both of these work for displacement in only two dimensions, and I wanted to do this for three dimensions. I recreated both implementations, which existed in an old version of Python 2, and brought them over to Python 3. I was working on making this grid cell layer, which we call L6A melon, operate in three dimensions: X, Y, and Z. What I ended up doing was not complicated—I created groups of grid cell modules, where each group would handle path integration along a specific dimension. As you can imagine, I would have three groups, each handling two dimensions along one plane. That's even simpler than the biological ones. When I say one dimension, I should clarify: it's two dimensions along one plane, like the 0-1 plane.

This goes back to the paper by Merkel, Marcus, and others, where you had multiple 2D grid cell modules that created 3D spaces. You would create groups where each group handled one specific plane. For example, if I had 10 modules for each group, each with about 25 cells, I would have about 750 total cells. One key aspect is that only one cell per module could become active. If you're looking at an SDR output from the grid cell groups, you would see an SDR with 30 active bits out of a total of 750. The 10 modules were at different scales and orientations.

We combined concepts from the frameworks and columns plus paper with the 2D-to-3D Merkel Marcus paper. I didn't have enough time to run in-depth experiments to see how changing the scale of a module would affect training results. My goal was to get the code running as a proof of concept for 3D objects, like point clouds. I also borrowed some anatomical details from the Tank paper, where scales between successive modules had a multiplier effect of 1.5. I included that here, though I'm not sure if it contributes anything, but all this is detailed in the code. To be clear, that wasn't Tank's idea; it was an empirical observation made many years ago.

Just one question: since you're operating with planes, did you consider that using just two planes might be sufficient to specify a point in 3D space?

Yes, you're saying I could use X and Y, and maybe X and Z or Y and Z. Maybe you get more accuracy with three planes intersecting, but it might not be necessary. I considered that, but I wanted to include every combination of every plane possible. If you do that, you end up with elongated fields rather than spherical ones. In 2D, a grid cell responds to a circular area, but when you limit the number of dimensions, you get elongated fields. It still works, but these elongated fields don't seem to be observed biologically.

If the two planes are closely aligned or orthogonal, you get this elongated field. I'll go through this in some detail. This is the training procedure I used. The picture is from Neil's paper: the top layer is the sensorimotor layer (L4, the temporal memory layer), and the bottom location layer is L6A, the grid cell layer. The algorithm here is not strict object classification. It's not about identifying an unknown object after training on similar ones. Instead, it's object recall: if I put my finger or a sensorimotor along one of these objects I've seen before, can I identify that I'm on that object?

So the first step here—I'm confused, but you could classify it, right? That wasn't your goal, but once you can do that, you can classify it. No, it's not strict classification where I can go back and—I'm avoiding the term "cloud." What I'm saying is, with these very sparse representations, we talked about unique locations in grid cell modules. Every unique location is classifiable because it's unique to a particular point on a particular object, and the same with the sensory layer. Sensory inputs are represented uniquely in the context of a location on a particular object. All these things are potentially classifiable; you just maybe didn't do it. You have to learn that with temporal pooling.

That's a much better way of saying it, thanks. That's also how we use the graph match for classification. The graph does the same thing: you can only tell whether you're on an object you've seen before. We actuate for all the objects we've seen before and see whether we're on any, so you could do the same thing here. I'll go to the next step.

The algorithm here was largely borrowed from the columns plus paper. The first step, labeled one, involves providing a motor input to the grid cell layer, which operates across three planes, performing displacement in parallel for all three. I provide this motor input to the grid cell layer, perform path integration, and obtain a set of active cells for this layer. After that, I get a location representation from the grid cell layer, which serves as the basal context for L4. While that's happening, I feed sensory input to L4, specifically the curvatures used in the previous project. Once that's done, I get an active representation from L4, and the grid cell layer is updated with this new joint sensory-location representation. An additional step is to store this sensory-associated location representation for comparison during inference.

You're asking if that representation is used for classification. It's more nuanced than strict classification. Classification would allow generalization, such as identifying an example as a cat, rather than something specific seen before. In this case, the distinction is that the representations are for particular objects, so it's about inferring a specific object, not classifying it. What you're storing is a unique location on a unique object. For example, if you see 10 cats and 10 dogs, at inference time you might find a new input matches six of your dogs and four of your cats, making it more likely to be a dog. However, the algorithm doesn't inherently group similar objects; it learns objects and treats them as different things unless you add something for classification. This is more accurately described as recall, since it's about identifying a specific object.

In the next stage, when recalling a specific object, the same training steps apply: pass a motor input, bias the temporal memory layer, and pass in the sensory input to get a set of active cells. The new associated sensory-location representation is used to tune the cells in the grid cell layer. The inference rule is that the location-sensory representation is only correctly inferred if the inference location representation is a strict subset of the representations collected during training; there must be strict overlap with the previously collected representations.

This approach might be naive, but it's straightforward. For the data, I created two types of randomly generated points. The first, on the left, shows a sequence of continuous paths. Ignoring the red points (used for testing) and focusing on the green ones, you can see they're bunched together in a path, mimicking what a sensorimotor would do on a continuous path. There are a series of these paths, all seen during training and testing. The red points are used similarly during the evaluation object recall phase.

This is the alternative example where I have a sequence of uniformly distributed random points for both training and testing. I ran several experiments, but I only wanted to show these two because they seem the most informative. The first, at the top, used 50 points for both training and testing, with 10 sensations or points per path for a total of 500 sensations on an object. I used four random objects in my set. As you can see from the results, I don't have any visualizations to show—only terminal output. For the first object, object 10, it was unable to correlate the random paths or sensations from testing with what was collected during training, but it was able to do so for the other objects within a certain number of steps or sensations.

The second experiment did the same thing, but instead of paths, I used random sensations. You can see how quickly you can converge on a correct representation because you're covering more of the object. The chances of seeing a testing representation that strictly overlaps or is a strict subset of the training ones is pretty high, so you can converge on a correct representation for all four objects very quickly compared to the time it took for some objects in training, which took maybe 200 steps for one. That same object takes one step here when using random sensations. The first one actually worked with the paths, but in the previous picture, the paths didn't even look similar. Do you know why it worked? There's not much overlap between the green and red points. There might not be strict overlap in the points themselves, but if they're in the same vicinity or neighborhood, those SDRs will have overlap. That's why I think it happened for the uniformly sampled ones when it shows one step.

Is there a zero step? Does one step mean there's still a prediction step, like you have two points, or is it literally just given one point? It's given exactly one point.

Going back, your green and red were both done with paths, right? You trained on little paths and inferred on little paths? Correct. If those paths overall aren't near each other, then it's not going to work. They don't have to be the exact same points, but there have to be points that are nearby. With a small set of these little paths, if the paths aren't near each other, like some on the tail of the airplane, there's no overlap between red and green, so there's no way to recognize anything back there. You could have randomly sampled during training and then inferred using paths, and I think that would work much better. I think that's a valid observation. I could have done that.

Sometimes, doing these short paths means you're highly under-sampling because you have a bunch of points in one space and then a bunch in another, and they're not evenly distributed. The intersection between the inference and learning path could be very low. In my head, I was thinking of creating a realistic training scenario where, if I'm looking at a novel object, I'm not running my finger to random points to understand it. I'm going along a continuous path for part of the object, moving to a different part, and touching that in a continuous path. That's what I wanted to replicate. That's a kind of impoverished learning strategy. In reality, we typically focus on areas of interest—things that are unique and different. During learning, you would cover the entire object. I can't learn what that airplane is unless I've sampled all parts of it, even briefly.

It would be like looking at an object through a series of small holes, with vision restricted to those holes. When inferring, you're looking through a different set of holes, and they may not overlap much. If I could have learned the entire object, then observing only part of it during inference would have worked better. I think that was probably not the wisest choice for a learning strategy.

At least I can see why it wouldn't work very well. Those points are just the scope of that local feature. If you imagine a fat finger taking swaths across it, you see a coarser representation, increasing the chance of intersection. If you're blind and rolling the object in your hand, as Jeff said, you'll encounter all the interesting extensions and smooth areas, so it's denser than just finding a point feature.

That brings up a good point. Something we've suspected and written about, but never really tried, is that in sensory modalities like vision and touch, regions of the cortex work at different scales. One way to look at that is hierarchical; another is modeling the object at different resolutions. You could say, "I have multiple models of this airplane." One is a very fuzzy resolution, giving the overall shape, and another, finer resolution, focuses on details like the propeller curvature, but not the overall fuselage details. There are other ways to address these issues. We shouldn't focus too much on the fact that it didn't work well in this case.

When you sense the point, do you get any features there, or do you only get location? If you recognize the object from just one sensation, is that because it's a unique feature, or because you're using an absolute coordinate system and only that object exists in this location? There's no absolute coordinate system here. The only feature you can get is the curvature. I'm assuming that in cases where it works with one sensation, it's because it saw something similar during training. Viviane asked a separate question: are the locations absolute, or do you just assume the plane's in the same reference frame in both cases? Is it a question about reference frame, or about a global fixed location, like in my past temporal memory project?

I guess both. I'm trying to figure out how you would recognize an object from just one sensation. It's using a fixed reference frame, because if I rotated the object, it wouldn't work. The grid cells use a fixed reference frame, but it's not using a fixed object coordinate location like I did in the past. If I wanted to look at one sensation, it worked in those cases because that particular sensation was something I saw similarly during training. It was very nearby, a unique curvature. Nearby assumes you have this absolute reference. You don't have to infer which grid cells in each module. He said it was a fixed reference frame. To Vivian's question, the feature is curvature, not a point coordinate. But you do have the location of the coordinate of that feature, presumably. If you miss by a little and are still part of that curvature shape, you might still have a close match. It depends on the scale of the curvature you're considering, and how close you have to land to it to recognize it. That goes back to my coordinate encoder strategy: find a neighborhood, and anything within that real value neighborhood is considered overlap. You don't have to have an exact match.

Imagine only one object actually occupied the location (0, 0, 0). In these graphs, although the axes are not the same, imagine only one object occupied (0, 0, 0). All the other objects didn't come close. If I detected a feature at (0, 0, 0), even if that feature has to be the same, that's how you could uniquely identify it: this curvature here, and only this object has a curvature in that location. That's how I interpreted Vivian's question.

That shouldn't be happening here. If grid cells are being used, that should be an internal reference frame. Even if you kept the objects fixed in the environment at training and inference, it should still be an internal reference frame. If you just have one location and you have coverage, temperature should be the same everywhere on the outside of the cup. I think it'd be worth checking, Abhi, how big the location representation is at inference time. In my mind, on the first sensation, you're not going to be predicting anything in the feature layer, so all the cells in any given minicolumn are going to light up. No matter what, you'll get lots of location cells becoming active. I'd be surprised if you can do it in one sensation, but maybe with a small training set and few objects. How many objects did you use?

There are only four.

Not many.

Did you test for translation invariance?

I didn't test for translation invariance, but this is translation invariant the way I'm using grid cells.

Jeff, were you asking something?

I was just making a funny comment. He said there are only four objects: object 75 and object 45. I get it, it's funny.

On the first sensation or data point, how do the grid code activations get established? In the very beginning, the grid cell anchors at a completely random point. By point, I mean a point in the grid cell space. You anchor somewhere randomly, then—are you talking about inference or training? Inference. In inference, that's not what happens. How does inference work? Do you not activate a random point in the grid space and compute displacement to that point?

No, at the start of inference, the first sensation will activate, say, columns two, seven, and twenty, or minicolumns—those receive inputs. All the sensory cells become active in those minicolumns because you don't have any prediction or location representation. All those cells have some learned association with a grid cell, so that input drives a bunch of grid cells to become active. On the next step, all those grid cells are updated with path integration; they predict the next sensation. At this point, you get the paring down of representations.

That is a nuance I might not have implemented. Other worries. Abhi, you had to do a lot in only a couple of days. Don't worry.

That would be translation and rotation invariant, right? Not rotation, just translation. The general question is you don't know your location; you have to infer the right anchoring of the grid cells, regardless of what they're representing. You weren't narrowing down the possible set of active grid cells, which is the general idea. Regarding what you were saying, Phil, in my mind, rotation invariance doesn't fall out automatically because you don't necessarily know how you're moving relative to the features and your learned representation.

You either need to test many possible positions relative to the object or try to infer that somehow. I see. But translation invariance should be automatic.

This is the last slide. Based on the experiments I was running, here are some ideas for what could be interesting to do next. The first is some kind of intelligent action selection. We've discussed this a lot, and it seems useful here. Instead of using the naive paths I collected during training, I could implement action selection to maximize information gain. Also, more realistic grid cell architectures could be used instead of the groups I created here. It feels inefficient to use so many grid cell modules to reconstruct 3D space.

I was talking with Viviane about an idea yesterday. Jeff, you and Viviane discussed briefly the possibility of unwrapping 3D meshes of objects into 2D sheets and using grid cells in standard two dimensions. I thought about this further—if unwrapping is challenging, you could project onto view spheres, as Niels described a few months ago, and deterministically unwrap the sphere. There is a known solution to that problem, but both approaches have challenges. You can unwrap the sphere, but then you have to choose your distortions, losing either angle or area. Mapping is a known problem; a sphere is not composable, and projection introduces intrinsic curvature. You cannot avoid that.

Viviane and I discussed an idea that was quite complex: you can assume that locally, in 2D, everything is planar, but over longer distances, it's not. How could you represent complex shapes using grid cell mechanisms? You would have to assume that long-distance movements aren't reliable, but local movements are close to linear. It's a long topic. The idea I was trying to pick up was a developable surface—one that can be mapped to a plane without distortion. Kevin, what was the adjective you used? Developable? Yes. There are some surfaces you can unfold onto a plane without distortion. This is probably even more complex, because we're trying to take any kind of surface. What we were trying to do is understand how, if we accept that we apparently don't have 3D grid cell modules—there's no evidence they work like grid cell modules in 3D—then how could you actually learn three-dimensional objects? That was the challenge we were looking at.

Okay, that's it.