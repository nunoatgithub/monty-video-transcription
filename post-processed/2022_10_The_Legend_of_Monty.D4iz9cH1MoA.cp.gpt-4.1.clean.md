I'll try a little bit of storytelling about what happened in the Monty project so far and where we're at right now, focusing on the parts I was involved in, which is the graph learning parts. This was a huge team effort to get here.

Let's get started. This is the follow-on to the Shears of Blood.

Once upon a time, the Monty team washed up on the shores of Nupic Island. They figured the first thing they had to do was get oriented and figure out how to use their compass. This meant drawing up the building blocks of the intelligent system they were imagining. We drew up basic principles inspired by the brain and the Thousand Brains theory. One of the principles is to use sensorimotor learning inference: sensing an object and sensing the world is an active process of moving around and sensing the world. A second principle was to use reference frames and not just a bag of features. Another principle is to use a modular structure. We don't have just one input image, and no sensor perceives everything at once. We have multiple sensors that each perceive small patches of the environment, and they can move around and communicate with each other through voting. This brings me to the next principle: a communication protocol, which makes the whole modular structure possible so that every module can communicate with every other module. They all speak a common language, no matter what's going on inside the modules.

Here's another high-level overview. I won't get into too much detail of how this looks. I think we just fast-forwarded about 400 years ago. We lost our ancient motifs. I'm sorry I kept the font.

The basic building blocks we envisioned were to have sensor modules and learning modules. Sensor modules can be modality-specific, such as for touch or vision sensors, but their output is unspecific to the sensory modality and can be the input to any of the learning modules. It includes features and information about movement, location, and space. These learning modules can then communicate with each other through lateral voting or hierarchical connections to other learning modules. Another essence from the principles before is that these learning modules use reference frames for the models they build.

They used their compass to reach the nearest town, then took a break to set up an environment in which they could test their ideas.

I won't get into too much detail here either, but Ben nicely set up this great code framework where each of these boxes is a class that can be customized individually. We can write custom sensor modules, custom learning modules, and so on, all building on these principles. This code framework is the basis of everything that was to come. Thanks a lot to Ben for setting up this really nice framework. Then Lewis set up the habitat environment where we have 3D objects in space, and we can have different sensors. For instance, here we have a camera that can move in space. Since we have this principle of small sensor patches, in the middle we see what the sensor patch is actually perceiving, and it can then move in the world across the object to build up a coherent picture of the image and build a 3D model of it. For most of our experiments, we use the YCB dataset, which contains 77 household objects, shown here with the framework and environment set up. We have sound effects. I can bring it onwards to work on the first prototype.

They set up the first learning module, which could recognize objects independent of location and translation by using displacements starting graphs. The displacements used here to recognize objects are the gray lines. Basically, it's how we move from one location to another. Since these displacements can be represented rotation and translation invariant, we achieved invariant object recognition where we can move along the object with our sensor patch, and through these displacements that we're sensing, we can recognize which object we're on and where on the object we are.

However, their system relied on sampling the same displacements stored in the model. If we sample different displacements—movements that do not correspond to any of the gray lines stored here—this approach did not work. So they continued their journey.

After a long walk through the forest of smaller and larger issues, they developed a new learning module that avoided this problem. They spent some time in the next town tweaking and improving this module. This new learning module is what we used for the past months, until very recently. It relies on storing locations and then taking displacements as input, rotating these displacements to fit the 3D model. With this mechanism, we can still recognize objects rotation and translation invariant, but now we can sample arbitrary displacements.

Let's take a closer look at how this works. We learn an object in an arbitrary orientation by moving the sensor around it. Each learning module receives features from a small patch of input and learns its own model of the object. Features can be pose specific, like point normal and curvature directions, shown as green, red, and orange lines. Pose specific means if the object rotates, these features also rotate. We can also have pose unspecific features such as color or the magnitude of curvature, which do not change if the object is rotating.

We use the pose features—these three vectors—to initialize the hypothesis space of possible object locations and rotations. The rotations are tied to different locations on the object. For example, if I am on the top of the can, the rotation is exactly 180 degrees from the bottom, and 90 degrees from the side. We use this hypothesis space to test observations and eliminate hypotheses. First, we use pose independent features, such as color and curvature, to determine possible locations on the object. For instance, sensing a very curved area on a yellow object, we might be at any of these green locations in the model. Then we use pose dependent features to determine possible rotations at each location. If I sense certain displacement pose features at a particular location, I would rotate my sense vectors to align with those stored in the model, and that would be the rotation of the object. We repeat this process with successive observations until only one hypothesis remains, which is the detected object and pose.

Now we can detect objects independent of the displacements stored in the model. They decided it was time to add voting into the system. They added voting on object ID and pose, which helps achieve faster inference and more robustness. Without voting, the model needs more steps than if we vote between multiple learning modules, each receiving sensory information from a slightly different patch on the object.

Next, they decided to tackle the sampling problem. So far, we could recognize the object independent of the stored displacements, but sampling new locations on the object still caused problems most of the time. They had seen this issue coming for a while, but it was finally time to address it.

They developed an evidence-based learning module. This module does not discard hypotheses based on inconsistent observations, but instead updates the evidence count for these hypotheses. Previously, we had a list of hypotheses, and any inconsistent observation would permanently remove a hypothesis. In the new module, we keep all possible hypotheses and update the evidence for each at every step.

For each hypothesis, we calculate evidence based on stored points in the model within a certain radius. For example, the red radius here is used in most experiments. We use observed features to update the evidence, including the morphology error, which is the distance-weighted angle between pose features. This can add or subtract evidence depending on the error. Optionally, we can add evidence if pose independent features such as color match what is stored in the model, also distance weighted. There is an evidence decay factor applied to all hypotheses at every step to push evidence toward zero and prevent it from growing too large. Additionally, if we have multiple learning modules and sensors, we can receive evidence through votes, which is also distance weighted. All of this can be done with efficient matrix multiplications, making it faster than the previous learning module.

The new model outperforms the old one in detecting the 78 YCB objects. The old one reached about 84 percent performance on the full dataset, with other episodes ending in detecting the wrong object or no match. Now, performance is about 93 percent, and most other episodes end in a timeout due to an inefficient action policy—a random walk. The YCB dataset includes many symmetrical objects and ambiguous views, so some timeouts are reasonable.

In this learning module, voting also reduces the number of steps needed to recognize an object. With voting, recognition is much faster than with the previous module, without affecting performance.

They then performed an extensive robustness evaluation on the new learning module.

The new evidence learning module works surprisingly well. It was tested on a range of scenarios, including new sampling. In green, we have the new module, and in blue, the old one. On the left side, we see performance when sampling approximately the same points, which results in 100% performance. On the right, there are two scenarios where we sample completely new points, and the old learning module's performance degrades. When sampling new points, the new module retains its performance.

We also tested rotation and translation invariance again, which holds up well. Rotating by one or ten degrees from how the object was learned still works, as does moving the object in space.

We tested a change in sensory modality using the touch sensor that Philip implemented. We learned a model using the camera (vision sensor) and then tried to recognize the object with the touch sensor. The old learning module couldn't do this, but the new module performs well. We also tested sensor noise by adding Gaussian noise to the detected features and locations. This ruined the performance of the old module, but the new one, using evidence, still reaches 100% performance. This makes sense because if we get one inconsistent observation, we don't discard the hypothesis; it just receives less evidence, and we can still recover it with more steps.

On the way back, the test crew makes a short stop at the cave of generalization. Here, they find that the amount of evidence received for each object model can indicate object similarities. At the end of each episode, we look at the evidence for each object in the YSV dataset and cluster this evidence matrix. We see meaningful clusters. For example, zooming in on the orange cluster on the left, we see it groups round objects, and within a smaller cluster, round and yellow-orange objects. Another cluster contains cups, sorted by color: yellowish-orange cylindrical objects on the left, then red, then blue. Other clusters are also meaningful: ball-shaped objects like golf balls, apples, baseballs, marbles; elongated objects like forks, spoons, spatulas; airplanes; boxes and bricks.

If we present the module with an object it hasn't learned about, it retrieves the most similar object in its database. For example, the picture on the left shows an object the module has never seen before, such as a mug, and it identifies the object with the highest evidence, which might be a red cup. Sometimes, two different cups with slightly different colors are retrieved. If there is no very similar object, it still finds a reasonably close match. For example, a cylindrical blue object might be matched to the master chef can if the module doesn't know about the object. In this case, the system seems heavily biased by color.

It uses both color and shape. If you give it a red mug, it picks up the red cup, but not the red apple. It's using both features, and the closest match is selected. Other cylindrical objects may have high evidence, but the blue one has the highest because it matches in color. For marbles, if there isn't a small round red object, the highest match might be a white golf ball, which is similar in size, curvature, and shape.

Back in the evidence LM town, we review what we've accomplished so far. We've implemented the Monty framework, set up a test environment, achieved rotation and translation invariant recognition, sampled arbitrary displacements, improved speed with voting, achieved sampling invariance, robustness, and some basic generalization.

This was a short summary of the path the Monty team has taken to reach this point. Many details were left out. Over time, we've explored other routes, implementing alternative approaches such as temporal memory, ICP, and HDCD. The team has faced many struggles and learned new lessons daily. The path was winding and long, but we kept following our compass, moving one step at a time until we arrived here. The evidence LM town is a beautiful place where we can test all capabilities and regroup. But what's up ahead?

Niels has already started forging ahead, working on more intelligent action policies. Currently, we use a random policy to move along the object, which is inefficient. We can make this a sensorimotor system by having the policy actively test hypotheses and seek out features to recognize objects faster. We've also had several brainstorming meetings about hierarchy and how it could be implemented.

Another future direction is attention, especially if we have many learning modules communicating. We may not want to pay attention to everything at once. So far, we've dealt with one object in an empty void, but we want to handle multiple objects in a scene and occlusions. Currently, the objects are rigid and do not transform or move, but we want to cover object behaviors and transformations. Further out, we aim for continual self-supervised learning. Currently, we do some weakly supervised pre-training before testing object recognition, but learning from scratch with very weak supervision, continually, would be ideal. Even further out, we want to represent non-3D concepts, deal with two-dimensional images, or other more abstract conceptual structures.

End of chapter one.

Great version, really great, it was fun and nicely done. There was a lot of great content. The fun wrapper made it easier to understand.

Very nice. Thanks. You created a reference frame for the Monty story. It's interesting—it's a big island. As opposed to just a temporal sequence of slides, if you put them on an island with locations, it's easier to understand. Of course, you need to add avatars that look like us. Start with yourself, maybe in our medieval costumes. How about a soundtrack too? I made some pictures with DALL·E, like this picture here, but it didn't really work that well, so I gave up after the first attempt. On Halloween, you'll have more opportunities. That was great, and the work is great. I really appreciate you starting off with the principles in the beginning again so we don't forget those. I feel like that's really been our compass. It's so easy to forget that.

That's one thing I appreciate. I had one question: could you talk about your algorithm for evidence decay?

Just for the decay factor or for the whole learning module? Do you advance it by the number of steps, or what's the axis on the decay? What's making it decay? Is it just the fact that you haven't gotten evidence recently? Right now, it's pretty simple. It just takes the current evidence factor—if the evidence is negative, it adds a little bit, and if it's positive, it subtracts some. The further away from zero it is, the more it adds or subtracts, so it pushes it toward zero over time. But it's one of the things that still needs to be tweaked, so the evidence can still grow quite high. It's not a working idea yet. My thinking was that if you're exploring an object and develop some evidence, then start exploring a different section, that particular body of evidence doesn't get refreshed while you're concentrating on another aspect. Does it really want to decay, or if you're getting negative evidence against it, that makes sense, but I was worried about decaying it just because you haven't seen anything for a while.

Right now, that doesn't really happen because we always see something. At the moment, the decay factor is mostly just to keep the evidence count in a certain range because we're not normalizing it. I'm not sure yet what the best way to do this is. I wonder if this addresses what Kevin said, because the evidence doesn't stay with that part on the object—it moves with the new displacement. As long as you're still on the object, it's not going to decay.

Maybe in a future research meeting, we can go into some of this in more detail again.