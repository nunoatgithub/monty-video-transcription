All right, I just have two slides, but they build on top of each other. We're going to really pop up a level. First, we talked about the cortex interacting with subcortical behavior generators. The cortex sits on top of the old parts of the brain and tries to control them. I'm going to build up a picture of that here. This little rectangle represents a subcortical behavior generator. Its orange output drives motor activity somewhere, and it gets some sensory input. Examples of subcortical behavior generators would be the superior colliculus, which is associated with eye movements, the brainstem, which is associated with limb movements, walking, and breathing, and the spinal cord. I don't know if the basal ganglia has direct connections to motor neurons, but I put it in there. The point is, there are a bunch of brain regions that are not cortex that actually control neurons that do things, whether it's breathing, moving your limbs, or moving neural rods. I gave examples of eye movements, walking, breathing reflexes, or limb movements. These are all subcortical behavior generators. It's important to recognize that sometimes these things are fixed, like a reflex reaction, but they can also learn general rules. When we talk about the cortex interacting with these, we can't assume this is a fixed motor system that doesn't change over time. Generally, they can still be reflexive, but they change as your body size changes, with trauma, injury, and so on. That's what a subcortical generator is. The superior colliculus is a great example. It gets input from the eyes and other parts of the brain, then generates eye movements, mostly non-model-based, like reflexive movements or responses to motion. 

The cortex sits on top of that. Let's talk about the three properties that really matter here. First, the cortex projects to these subcortical behavior generators. There are no neurons in the cortex that we know of that project directly to motor neurons or muscles. The cortex doesn't control any muscles. The only thing it can do is project to subcortical behavior generators, which makes sense for a couple of reasons. One is you don't want two different systems trying to control muscle movements—the cortex says move left and the spinal cord says move right, and what's going to happen? More importantly, as will become clearer, columns—learning modules, Monty—can only affect behavior by driving these subcortical behavior generators, which are much more body-specific. We have a general system, the cortical column, driving a specific thing, and because the column doesn't know what that specific thing is—a cortical column doesn't know what it's projecting to—it's a generic learning module that has to learn how to drive the subcortical behavior generator. It has to learn to control it via associative learning connections. This is not a hardwired thing; it has to figure out how to control it. To do that, the column first has to learn a representation of what the thing is doing. You can't just say, "If I'm going to tell the eyes to move, I need to know what I'm going to tell them." If I'm a column and I want something to happen, I have to know something about the thing I'm trying to control. I have to know what it does. For a generic learning module or cortical column to control a specific thing below it, it has to first learn what that specific thing does so it can control it. 

Is there any question about that before I go on?

Don't be shy. Okay. Are you still there? I can't see anything. Yeah, no. Sounds good. Thank you. 

Now let's talk about vision specifically. What we think is going on in vision is that the system moves the eyes. This little arrow is the feedback from the world. As the eyes move, the input from the sensor changes, and that input comes back up, not only into the superior colliculus but into the cortical column. We've proposed that in this case, there are two inputs from the eyes: the magno side of the pathway, shown in orange, and the parvo side of the pathway, shown in green. The magno side of the pathway represents flow patterns on the retina. The column can observe flow patterns on the retina, which are a direct indication of how the eye is moving, and it can learn. Remember, a column doesn't know what these cells mean. It has no idea what these inputs mean; it's just a bunch of neurons coming in. It sees changing patterns and interprets that as some kind of movement. If it learns what those movements are, then it will know what the subcortical motor generator is doing and can control it. If this system can move the eyes left, right, up, and down, and the column can see that the eyes move left, right, up, and down, then it can tell it to do those things.

and so it's this feedback loop that allows the column to learn how to control this system. I want to go into detail about how it does that. These principles probably apply to Monty as well, in any embodiment of Monty. If we're going to have a generic learning module, it's very likely that, with some embodiment, there will be subcortical behavior generators. These are our model-free policies that you've been working on, plus others, that the learning module will have to learn to control. Therefore, it needs a feedback loop and this basic process. Now I'll talk about how this might be done in neuroscience, which doesn't mean we have to do it this way. I'm not completely certain how neuroscience does this, but I have some important clues, which I'll share. It's still a bit of a mystery, so let's go into that.

Here we show a column, called B1. The little dots represent neurons, and some of these neurons are in minicolumns, represented by these vertical arrangements. An empirical observation from many years ago is that the cells in these minicolumns in layer five and six—just talking about layer five and six right now—seem to respond to movements of edges in particular directions.

These little blue pictures represent the receptive field properties of these cells. For example, one might respond to an edge moving in a certain direction, another to a vertical measurement, another to a horizontal edge moving down. As you go across the column, you see all the different orientations and directions they can be moving. The cells within a minicolumn seem to respond to the same basic orientation and direction. The next minicolumn over represents cells that respond to a slightly different orientation and direction, and so on. In the lower layers of the cortex, these cells respond best to very long edges, which is what you want for a movement detector—you're trying to detect flow across the entire retina, which is important for detecting eye movement. In the upper layers, the cells have similar response properties but much smaller receptive fields, which would be more aligned with objects moving in the world, versus here, where it's the eye moving relative to the world.

Does anyone have a question about that so far?

It just makes sense. No, sounds good. Okay. I've always wondered why the cortex has all these cells that respond to the same thing. What's the point of that? It doesn't make any sense. In layer four, we propose that all the cells respond to the same thing outside of context, but within context, a single cell becomes active and forms a unique representation of that item. I think something similar might be happening here.

Let's see. Why am I not progressing my slides? Oh, here we go. My interpretation—this first part was an empirical observation—now is that these minicolumns represent a basis set of movement vectors. Each one is like how the eye moves relative to the world. In reality, if this is learned, and I think it would be, it doesn't have to be so simple. The different movements could be into the plane, out of the plane; there could be a variety of things represented beyond just edges moving in horizontal directions.

We would expect to see minicolumns that don't fit perfectly into this arrangement, and I think there is evidence for that. These are the classic, well-understood ones. That's my interpretation: these now represent movement factors.

A prediction of this theory is that—now this gets to the part Hojae didn't talk about—this oscillatory interference model for how grid cells are generated. It's really complicated. I'll show you a picture just to illustrate, but what it requires is that grid cells need a velocity-controlled oscillator. That means there are cells firing at some frequency, and if the animal is moving in a direction, that frequency increases. There's a background frequency, and when you start moving, the velocity increases the frequency. These cells would make the whole column active, indicating movement in a direction. The faster the movement, the faster the oscillation of these cells. That's what velocity-controlled oscillator means. Is it the oscillation or the phase moving faster? It's the oscillation, which I'll explain next.

Imagine it's going at 10 hertz. Now, if I start moving in a direction, all these cells fire at 11 hertz. If I go twice as fast, it's 12 hertz, and so on. This is a prediction based on theory. I would predict that these cells are all firing at a different phase in that frequency cycle. They might all be firing at 11 hertz, but the individual spikes from the cells would be shifted at different times over that frequency period. If it's a 10 hertz signal, that's 100 milliseconds, and maybe each of these 10 cells would peak at different points every 10 milliseconds. They're all firing at the same frequency, but they're phase-shifted.

One way of conceptualizing this is like grid cells for one dimension. If you were just moving in this direction, imagine the spacing is the entire length of this mini column. The cell at the top will fire again after you've gone through all the other cells and returned to the top. If you start moving faster, each cell will fire more frequently. They're all firing at 11 hertz or 10 hertz, but only when you're moving. They're at different phases. If I just looked at these cells, they wouldn't tell you where you are; it's when you compare them to a background frequency.

Let's say there's a background frequency at 10 hertz, and the base frequency of this mini column is also 10 hertz. In that case, only one cell would be peaking at the same time as the background frequency, because the background frequency is going at 10 hertz and the cells are firing at different points in time throughout that 100 milliseconds. Only one cell would be peaking at the same point in time as the background frequency. As I speed up the firing frequency of the mini column, the cell that peaks with the background frequency changes.

Which cell is in coincident activity—peaking at the same time as the background frequency—will move up this column or along here, and it will repeat again at the bottom. If I check my position by which cell peaks at the same time as the background frequency, this becomes like a one-dimensional grid cell module.

It's only when it's moving, which is a problem, but it is like a one-dimensional grid cell module. Even without the background, it's still like a one-dimensional grid cell module, but it's operating so fast that it's hard to read out. Unless you had a coincidence detector, at any given time, because of the phase difference, the cells will spike at different times. The only way to know which is the right one is to compare to the background frequency. Without the background frequency, it's just a bunch of cells spiking at different phases, and you can't tell much from that. With a background frequency, you can detect a coincidence between the background frequency and the active cell. Let me show you a picture.

I'm going to show you a picture here.

This is the paper I mentioned earlier. It's called "A Hybrid Oscillatory Interference Continuous Attractor Network Model for Grid Cell Firing." It's from Burgess. Neil Burgess is an expert in this area.

This figure contains all the necessary information. It took me a long time to understand it. These are grid cells, represented by the black circles. This is one grid cell. These are the cells in the mini column, and these are the cells in another mini column, and so on. The paper does not make this argument directly; it makes a theoretical argument that something like this is needed. My interpretation is that these are the cells in the mini column. Each set of cells in a mini column responds to movement in a particular direction, and they all fire at different phases and speeds. As you move, all of this happens within about a hundred milliseconds. There is a background frequency that is also firing. The red cell is the one whose spike coincides with the background frequency. The nearby cells also show coincident activity, but not as strongly—they spike just before or after the background frequency, while the red cell spikes right on it. This creates a bump of activity.

The input to any particular grid cell is derived from this set. A grid cell is not sensitive to movement direction; it simply indicates location. Burgess argues that a particular grid cell receives input from a specific set of red cells, while another grid cell receives input from a different set. If all these cells behaved similarly, each one would activate just like a classic grid cell, independent of direction, and have all the expected properties. The lower cell here is essentially an inhibitory neuron or a set of interneurons, which keep these cells active even when the animal is not moving. This represents the continuous attractor network, meaning it continuously keeps one or a group of these cells active in the grid cell space. The oscillatory interference model explains why grid cells change where they fire and why, as I move, one cell fires, then the next, and so on. This is a complex picture, but you can think of these cells as a mini column. That's all I'm going to show from this paper unless there are questions.

What exactly is the background frequency? I always forget which one it is—it's called the beta frequency or something like that. There's a set of empirical observations that these frequencies are continually running in the hippocampal complex and the cortex. I think they're generated by the thalamus, but I'm not sure. These observed oscillations of various frequencies exist in the brain and are well known. In the toronto cortex, there's the beta frequency, which I think is about eight hertz.

Let me show you the basic idea of oscillatory interference. This is from another paper. Just to clarify that figure, you could have this system set up so that all the VCs use a rate code. As you move along, let's say the neuron fires with a high rate, and then you move along, and it doesn't fire again until you've moved far enough. It's only because it's encoded in phase that you need this background frequency to decode it. That makes sense and helps to understand that it's less mysterious than it might seem. You're creating a grid cell that can path integrate in any direction by having these six. This is the main point you made earlier. In theory, you could have a mini column, and as you move, the active cell in the mini column indicates your location. There's just one cell active, and that's correct conceptually, but that's not how it works in the brain. In theory, that's what it's doing, but that's not the way neurons do it. I mention this because it might help to understand the figure; it's a bit simpler that way. They're basically like 1D grid cells, sensitive to a particular direction, and the different cells around the ring represent different phases for this 1D grid cell module. These are all the different phases.

To get this to work, you need to add one more level of complexity. The active cell here—the red cell, whether it's active or spiking, or whether it's in phase or not—has to be anchored. This must be anchored from an environmental cue. When you enter a new environment, if you recognize it, you have to reset these to the same point as before.

you have to re-anchor these one-dimensional grid cell modules in the same way that two-dimensional grid cell modules are anchored. I just mentioned that it's a complexity of the system, but it's not like, which cell is firing? How do you decide which one to start with? You could pick randomly initially, but as you learn an environment, or in the case of a model, you learn a model of something, part of recognizing the model is anchoring the location again. That means picking which of these cells will be active at any point in time. You're right. You tried to make it less complex, and I just made it more complex again. That's okay.

Maybe just to quickly recap some basic physiology: neurons will struggle to integrate temporal codes unless there's something to help them disambiguate which is the relevant bit. If all of these neurons are spiking at 11 hertz or 10 hertz, as Jeff was saying, and a neuron is receiving that information, how does it know which of those cells that are all sending a spike every hundred milliseconds is important? That's where phase alignment comes in. If you look at our paper, "Why do neurons have thousands of synapses?", we described that neurons require a bunch of incoming spikes to occur at the same time or very close to the same time. If they're spread out over a hundred milliseconds, the cell just ignores them. But if they all arrive within a few milliseconds of each other, then it's, "Oh yeah, that's it." I think what you're referring to there, Niels, is this need to disambiguate. It's not just about how neurons work, but they need this coincidence of spikes, so this phase thing makes a lot of sense. If these are just some spiking rate, they're all spiking at different points. How do we differentiate? Different cells have to—anyway, it's so complex, I can't keep it in my head at once, but in neurons, the actual timing of the spike arriving is important.

I don't think Monty has to do any of this, but I think it's going to get back to an interesting property that we do have to address. Good point. Anything else you want to say, Neil or anyone else? Another question—this might be really obvious, but while we are not moving in one of those directions, how is it maintained where we are on that ring sequence? I don't know. That's a good question. I've asked myself that question a lot.

The evidence we have is that these cells stop firing. They're one-directional, so if you go the opposite direction, they don't go backwards. I think the implication was the grid cell is all anything downstream cares about. That's what you were saying. The interneurons and such help basically say, if I'm not moving, this grid cell stays active, all the other ones are silent. This one has the machinery to stay active. But when I start moving again, I have to start up where I left off. I may be moving in a different direction, in which case I have a completely different set of rings that are active.

I was wondering if it's a similar mechanism to the interneurons on the grid cells that could be used there, but I think it's actually a nice thing that there is no reverse of direction. That way, they don't need to learn the opposing movement. The grid cell automatically learns that. Maybe this is evolution going to extremes to figure out how to solve a problem, and it's a complex solution.

But apparently, this is what it did.

By the way, if you're following all this, a cortical column has minicolumns that go across all the layers. Our cells are organized in a minicolumn in layer four, layer three, layer five, layer six, and so on. I've come to this basic idea—maybe I should go back to that. The moment here, you can see that there are these minicolumns, and essentially a minicolumn within a particular layer is in some sense always doing the same thing. It represents something generic, of which an individual cell represents a specific instance. A minicolumn on its own is not very unique. Just like one of these movement vector cells doesn't tell you where you are in a larger environment. But if I were to look at all the active cells in this area, it would be very unique. The same thing in layer four: if I look at layer four, it's representing, say, an edge orientation. Even if I pick one of the ten cells in layer four's edge orientation, it's only one in ten locations. But if I look at multiple minicolumns in that area, it would be unique. I think there's this idea of a minicolumn as a processor that goes from a set of cells that are not unique to picking one, which is still not unique enough, but then you combine across multiple minicolumns and that's unique. I don't know if that makes sense.

There's a property of grid cells and place cells, which is almost a guarantee that this oscillatory interference system is working. This is called phase precession. Let me just—don't look at the picture yet, just visualize in your head. Imagine you're looking at a grid cell and an animal's moving. As it moves to a certain location in an environment, the grid cell fires, and when it passes it, it stops firing. That's what grid cells do. You go into its location, one of its locations, it starts firing, and then it stops firing.

When that cell fires, the timing of its firing relative to the background frequency changes as it moves through the receptive field of the grid cell—this is called phase precession. As I approach the area where the grid cell is supposed to be active, it fires after the peak of the background frequency. When I'm at the center of the receptive field, it fires coincident with the background frequency. As I move beyond the receptive field, it fires before the background frequency.

In this diagram, the red line represents the background frequency, and the blue line is the modulated, velocity-controlled background frequency. If the animal isn't moving, both frequencies are the same, but when the animal moves, the blue one is slightly higher. Adding them together produces a wave. At the point where the two are perfectly aligned, the output is maximal and the cell fires. When they are misaligned, the cell doesn't fire.

The physics of this show that as you get closer to alignment, the cell fires, but as they move out of alignment, it stops firing at the peak and instead fires at a maximum elsewhere. This oscillatory interference explains phase precession, and it's difficult to imagine phase precession without it. This idea led to the theory that grid cells operate based on oscillatory interference. The paper discussed here presents a variation, suggesting that individual dendrites of a neuron could each have their own frequency, acting like velocity-controlled oscillators in particular directions. I don't think that's correct, but that's what the paper proposes. The oscillatory interference idea originated from the observation of phase precession.

Regarding the dendrite hypothesis, it would be convenient if each dendrite had this property, as it would reduce the number of cells needed for a grid cell module. However, it requires dendrites to have special properties, acting as velocity-controlled oscillators in specific directions. My memory is a bit unclear, but it sounds similar to the translation invariance concept discussed before. If you move in a certain direction and activate certain synapses, maybe it works, but it seems unlikely. The cortex structure suggests the ring idea is more accurate, but I can't prove it. There are aspects of the dendrite hypothesis that seem questionable, but it's possible.

To finish, if my predictions are correct, these phase shifts in cell firing could be measured in a lab. Another advantage of this hypothesis is that, depending on the number of minicolumns in a cortical column—at least a hundred, possibly several hundred—only a subset are active at any time, forming a bump of activity. For example, if I have 200 minicolumns, maybe 60 are active as I move in a particular direction. If I pick one out of ten cells to be in phase, that's 10 to the 60th possible unique locations I can encode as I move.

and so that's a really big number. I could encode a very large location space if I were looking at the individual cells that are firing. Once I put them into a grid cell format, it's not as effective. I've lost a lot of the property I want—a lot of small modules where each one is picking one event. But once I put them in a grid cell module, I've lost a lot of that information.

One possibility is that nature decided this method I'm showing here is really the way that location is encoded. Grid cells are required as a stable storage mechanism to remember where you were when movement stops, so you can reignite these same cells again. I need some sort of memory to keep track of where I was in this matrix. But this matrix of all these minicolumns fits the bill beautifully for a very high-dimensional, unique location space, though there are some issues with it. It's not perfect, but it's better than grid cells because I don't have enough grid cell modules. I would need a whole bunch of classic grid cell modules in this cortical column to get a unique location, and we don't seem to see that. But you do see this, and by the way, these cells would appear "gritty" in some sense, or at least cells based on them would appear gritty. They'd be like one-dimensional grid cells in some sense. If I consider them in phase with background frequency, I would find cells that seem to be one-dimensional grid cells. We see a lot of that—movement-sensitive, gritty cells in the entorhinal cortex. I'm sure we'll see them in your cortex too. It's not a complete theory; it doesn't all make sense, but it's the best I have right now.

Let's keep going. I want to now bump back up to Monty, or back up to large-scale theory. The theory I have is, if we take vision, the magnocellular cells from the retina are center-surround receptive fields. If I take those cells and feed them into a spatial pooler, the spatial pooler says, "I'm going to represent out of these the n most common patterns." The spatial pooler always outputs, let's say, three out of ten or three out of fifty. It defines a set of minicolumns that each represent a common pattern in its input. I believe if I took the mag set of cells from the retina and put them through a spatial pooler, and said, "I want to represent these by 200 or 100 minicolumns," the spatial pooler would learn to represent these kinds of things. It would say, "A common pattern is these things become active when the cells move in this direction, and these cells become active when moving in that direction." These receptive fields could be learned from the center-surround receptive fields of the magnocellular pathway.

Clearly, the cortex isn't getting any movement input from the retina in primates. The cortex only gets these center-surrounds; they're not movement, they're just change detectors. But I believe the spatial pooler would turn them into these receptive fields. These are essentially the cortex's interpretation of how the retina can move. It's basically saying, "Of all the ways the retina can move, I'm going to represent it with these n minicolumns, each one representing some sort of basic one-dimensional vector."

I believe that the layer 5A cells would share that same representation. The layer 5A cells are now encoding the same basic interpretation. They're saying, "These cells would then represent activity of an edge going this way, or the retina moving this way, or something like motion on the retina." Internally, I have everything I need: a way of learning how the retina's moving, forming a representation of it, and then using that representation to form unique locations in space. I have my output, which I can send back to my pattern generator.

One cell in each phase is sufficient to encode a unique location. Going back to my previous picture, this orange line is the layer 5A cells saying, "You, subcortical learning, generate some movement of the retina." I have no idea what it is. I observe that movement through these magnocellular cells. They come in as a whole bunch of bits. I don't really know what they are, but I run them through a spatial pooler. Now I form a representation of a set of basic movement vectors, one per minicolumn. Now I can send that back to the subcortical behavior generator. It says, "When you're generating the behavior of moving the eyes left, this is my interpretation for it." So I'm telling you, let's associate my representation with your internal mechanism, which I have no idea what it is. That way, in the future, I can tell you to move left or right, because I've now learned how to control you through observation.

That's the upshot of this whole discussion. This answers how the column knows what to communicate to the motor system, but not how it selects which movement to send. It doesn't have the property of knowing how to get from the current location to a different desired location. Layer five doesn't know that. What we have to introduce next is the allocentric model in the column that can generate behaviors by stringing together these basic movements.

The point is, first you learn how to control the subcortical behavior generator by observation—watching and understanding what it's doing. Once you've established that you can tell it what to do, you can use a sophisticated model based on your grid cell mechanism and location system. You have a structured model of the world. If you want to achieve something in that model, you can string together a set of basic controls to implement it. The subcortical system doesn't know what's going on; you're figuring out what to do and telling it to move in specific ways. With vision, it's less clear what a V1 model would be doing, but for manipulating fingers or limbs, it makes sense. In a somatic sensory system, you might observe reflex behaviors like grasping or basic movements of the fingers and limbs. You learn how to control them and then string those movements together for new purposes based on your learned model of the world.

The column would still have to perform a reference frame transform on the motor output. I didn't show that here; I took that picture out. In our recently submitted hierarchy paper, we discussed how the thalamus does the conversion from egocentric to allocentric orientation. Layer five A cells, since they don't go back through the thalamus, have to be converted back somehow to an egocentric framework. In that paper, we noted that layer five A cells are the logical place for this to happen. They have unique physiology, a bursting mode similar to thalamic relay cells. We speculated that the thalamus does the orientation conversion on the way up. Layer five B cells project to the thalamus, and layer five B projects to layer five A. Layer six B tells layer five A to perform an orientation transform.

Layer six B was described as the orientation of the sensorimotor system to the object. We passed it down to the thalamus to convert the input from a retinal-centric to an object-centric orientation. The same conversion has to occur in layer five A. The dashed line represents that it's already in egocentric form because it doesn't go through the thalamus and goes directly to the motor areas. It also branches up and goes back through the higher thalamus, meaning it has to get converted again. These are evidences that this is already in an egocentric format. We briefly mentioned that layer five A cells have a unique bursting property, similar to relay cells. Without further explanation, we suggested that layer five A cells themselves take this input and do the conversion back to egocentric form on its way out.

I think that's what you mentioned, Vivin. Is that what you were talking about?

Yeah. This is really interesting. In terms of Monty, it's interesting to think about because we've talked a lot about goal states and understanding where that fits versus the outputs of columns. It feels like there's still a place for goal states, more like column-to-column communication. Something like layer six, or a version of layer six that goes to the apical dendrite in layer five, or something like that. But then it's the layer five motor output you're describing here. It's more about what neural pattern can be triggered in a subcortical structure to produce the desired movement. In the beginning, this was mysterious to me—what would a layer five A cell represent or encode? It wasn't obvious initially, but now I feel I have a satisfactory explanation. It's encoding a language that is observed by the subcortical motor generator, in a way the subcortical motor generator would understand. It represents movements that occurred and is therefore able to control it. That's also the basis for our modeling. There are still many things I don't understand, but I think this general idea is a good one.

Maybe I shouldn't show this last picture, but I'll just show it. If you look from a top-down view of the cortex, imagine a cortical column with individual squares as minicolumns. The circles represent where the bump of activity would occur in those minicolumns. It's not 1D or 2D like in the upper picture. Did my cursor go—lost my cursor?

That's weird. My cursor's gone. It'll come back in a bit. There's a lot of evidence for this. The tank picker is another example, but there are many examples. What you'd really see is multiple bumps of activity moving around together because the minicolumns repeat themselves within a cortical column. There are multiple versions of the same thing. Even though I may only be representing eight or ten different orientations of movement, they repeat. This is what you actually see if you could observe the activity looking down on a cortical column. You'd see these islands or bubbles of activity, and as the animal moves, those bubbles would shift around. This is reminiscent of grid cells, but it's not the same thing. It's a function of the inhibition created between cells. There are multiple copies of the same thing going on in a cortical column. It's not just one copy; there are multiple minicolumns representing the same movement vector, which gives us robustness and uniqueness. I'm going to stop here—my cursor is back, and I'll stop sharing.

Thanks. I think there are some interesting things to think about. Obviously, we want to move eventually towards learning more about this kind of action output—what's the appropriate action, and so on. A lot of that, we don't have to, right? For example, you could maybe come up with a sensorimotor module. What if all cortical columns had the same basic motor output representation? Then you send it down to a learning module, and the learning module would convert it from the generic form in the cortex to a specific form for the learning module. I don't think that's a good idea. Another question I had is, when we talk about concepts, what is the movement vector? Is it still a movement of a sensorimotor, or is it a movement of something else? It's interesting to think that the mechanism in a cortical column doesn't really know—if it's V1, it doesn't know those are center-surround receptor fields on a retina. It just says, these are some bits coming in, and I'll form a representation of them. What would be the equivalent from someplace else? I don't know. Are all concepts built on vector spaces that are very much like grid cells and based on coordinates in the real world, or are they getting some kind of movement input from another part of the brain that represents a different space?

That mechanism I described did not require any specific knowledge about what the subcortical motor generator does or represents. Could it be that what we label as a subcortical motor generator is actually a different region of the cortex? The same mechanism could apply—a different region of the cortex could be sending information. I'm a column, I'm getting input, I don't know where it's coming from. Maybe it's the motor output of some other column, and I'm going to try to model it. What does that mean? I don't know. It's possible that some other column is just trying to model the behavior of another column and build a model on top of that, and it may have nothing to do with physical movement in the world. These are the ideas we've talked about before, about concepts.

I think it's really interesting to try to think about movement as these one-dimensional movement vectors that then get combined to form space. I didn't really reach a conclusion yet because I was trying to listen to you. I was wondering if the findings with bats could be explained if we think about it as one-dimensional movement vectors being combined, and that's why it loses the grittiness in the third dimension—because maybe the up and down movements are not as well represented. If I took several hundred minicolumns, I don't see why I couldn't represent movements in three dimensions. Why wouldn't it be gritty in the third dimension, in Z? In the case of bats, maybe when they go up and down, it's a very different kind of movement information that comes in, maybe much more sparse than what they get as they move otherwise. But why would that be? I don't know.

If they go down, maybe they are not moving their wings. I don't know.

Here's a potential solution. What if the minicolumns really did learn n-dimensional movements, whatever they observed? We know that in some parts they can learn one-dimensional movements and two-dimensional movements. They can perform a grid along a one-dimensional track.

We know that they fall apart when they do three dimensions in the rat. But what if the minicolumns themselves were able to represent any number of dimensions—let's say three dimensions—and they accurately represented that space? The problem is, remember I showed the top-down view of a column that's two-dimensional. The grid cell array that gets locked in is a physical two-dimensional array, not a physical three-dimensional array. So I'd have to take my three-dimensional representational space in the minicolumns and try to force it into a memory that's two-dimensional.

Does that make sense? If, instead of having blobs moving on a two-dimensional surface, I could have blobs moving into three-dimensional space of cells, then it would probably work. But the cortex isn't three-dimensional; it's two-dimensional, and it would force itself into a two-dimensional array. Why couldn't it work with just a bunch of 1D movement vectors? If you have more of those 1D movement vectors, they could be in different dimensions. The problem isn't the one-dimensional vectors. It's—I'm sorry, let me go share my screen again. Which fits with what you're saying about the grid cells being more like a storage point for your location rather than the core. 

This image down here on the lower left—imagine this. See why those couldn't correspond to movements in the third dimension as well? They could. This is not a good—there was this idea that there are many places in the brain where inhibition is spread horizontally. The continuous attractor network in grid cells—just think about the continuous attractor network in grid cells. This is a horizontal span of inhibition. It produces a two-dimensional pattern of activity. It looks like this one, a different one, but it looks like this. 

The physical structure of the cortex—if I want to store, if grid cells are a storage mechanism for the current location in my space, grid cells are inherently going to be two-dimensional if they work on inhibition that's in a two-dimensional sheet of cells. That's what it appears to be. The neocortex is all two-dimensional in all of its architecture. So I wouldn't be able to form blobs of activity in three dimensions of the grid of the cortical column. I can only form a two-dimensional sheet of blobs of activity. And although the minicolumns themselves could represent three dimensions or n dimensions, if I want to map them onto a set of grid cells, and the grid cells require a two-dimensional inhibition matrix, then I'm going to force my three-dimensional space onto a two-dimensional memory.

Oh, you mean because neighboring minicolumns have similar directions that they encode? That's part of it, but that's not really the issue. The issue is—I'm not sure if this is a helpful analogy—but with CPU design, part of the issue is they have to design it as a 2D array because as soon as you add a third dimension, there's nowhere for heat to go. All the transistors in the center of this 3D grid or cube would overheat. It's also very hard. Yes, problem. So all the transistors in the center would overheat. It's not—I don't know if that's a helpful visualization. No, it's not. But here's a better way of doing it: in silicon design, it's much easier to lay a line down horizontally on the surface of the chip than it is to put a wire that goes vertically on the chip. The process by which they make chips is laying down layer upon layer. It's not a three-dimensional manufacturing process. There's an inherent advantage—it's very difficult to make three-dimensional wires in a chip. They can't design chips in 3D. They have to design layers and put them on top of layers, and they have to connect the layers. That's a good analogy here.

It turns out that neural tissue, by evolutionary design, is essentially two-dimensional, and the way inhibition works is two-dimensional. This could have worked just fine when animals started walking on the earth—two dimensions was good. For whatever reason, this is how it evolved. We have this two-dimensional sheet, and therefore these inhibition things are going to be—imagine these are areas of activity that are reinforced by this sort of Mexican hat inhibition, where local cells reinforce each other and further away they inhibit each other. This kind of pattern is very common in neuroscience, but it's inherently two-dimensional just because of the nature of the neural tissue. It's not theoretically restricted, but in the neural tissue of the brain, it would be restricted.

So then it is about the neighborhood relationships of the columns, right? Or, if not, I don't see why you can't just put columns with direct—oh. You're right. I'm sorry. You're right. Actually, you're right, Viviane. I didn't think of it that way. That wasn't the issue to me. The issue I was imagining was inhibitory neurons and what they look like.

The way inhibition works assumes that neighboring columns are related. But I think I could imagine having that same property in three dimensions, where instead of these being circles, they were—

I think it's worth pointing out, because I'm not sure if there's an illusion of disagreement or something. The argument is basically that these minicolumns, VSOs, whatever, can do n-dimensional path integration, which would fit with the fact that animals don't get lost when moving through 3D space. But if you look at grid cells for this reason, they only show this really clear grittiness when you look along a two-dimensional plane.

That's the argument, and so it would lead to problems representing three-dimensional space. It wouldn't be perfect. It looks like that's the way the brain—maybe that's the way the brain does it. So to clarify, are you saying it would be hard to model three-dimensional space in a good way, or it would be hard? I think it would be hard for biological tissue to model three-dimensional space. All the properties you want wouldn't work very well. For example, you might lose path integration in the third dimension, or you might have trouble re-anchoring in the third dimension, something like that. I don't know. But that's with the mechanism you just proposed with the one-dimensional movement minicolumns. That would be—first of all, in the grid cell literature, most of it has nothing to do with three dimensions. Some of it does for bats. Most of it just represents the map of a room—it's two-dimensional.

No one has proposed how this is done in cortex. This specific idea that minicolumns represent these one-dimensional vectors—I showed you that picture of the rings, that's as close as you got. People say there has to be a bunch of cells that have these properties. I actually think those—probably the minicolumn concepts exist in the entorhinal cortex, but people haven't looked for it.

A lot of this is speculative—my interpretation and proposals. My understanding is that the spatial pooler mechanism should learn N-dimensional spaces, but if we rely on a grid cell-like mechanism to track position in dimensional space, it wouldn't work well in three dimensions. Therefore, there would be deficits in 3D, whether that's path integration or re-anchoring. However, as Niels mentioned, it seems we don't have deficits in path integrating in 3D space, but that's not clear.

One thing I can share is about animals and 2D movement. There is evidence that even animals moving in 3D tend to have a planar bias. For example, dolphins dive to a certain depth, spend time there, then surface for air, and return to that depth—they don't spend much time moving up and down. This could be due to other factors, like survival at certain depths because of light or food. At a conference, there was a presentation about grid cells in bats, and it was noted that they tended to fly at similar heights unless moving somewhere specific, not spending much time going up and down.

With some introspection, when you're in a room, your head is at a 3D location, but you don't have a strong three-dimensional sense. You know your height, but if asked where you are, your first thought isn't about being above the table—it's more about your location in the room. Grid cells in rats seem to represent this as well. In experiments with rats climbing lattice structures, it looked like they combined two-dimensional representations to make it work, rather than truly representing 3D space. For example, in my house, I know I'm upstairs, but I don't have a sense of my exact 3D location—just which room I'm in. That's anecdotal introspection, so not worth much.

If you can represent a plane with grid cells, you can represent different planes, but it doesn't help with full 3D path integration. If you have mini planes, like in a house, and a vertically oriented reference frame, it helps you move through levels, but you still can't path integrate in a truly three-dimensional way. The scale also matters—a house is a big space to model, and even in 2D, moving through a city is difficult to path integrate. You lose track at some point. Even in your house, if you think 2D, drawing a line from the kitchen in a vertical direction, it's hard to say where you'd be in 10 meters. But with a water bottle, I feel like I have a very three-dimensional model and can path integrate accurately on it.

However, with objects like a cube, if you have images on the faces, you don't really represent a good 3D model of the cube, and it's hard to represent its features. Maybe if I think more about it, but let's consider Monty. What do we want Monty to do? We have 3D working fine in Monty, but there's this N-dimensional space. I wanted to go back to the VSO thing because I've always had uncertainty about one-dimensional path integration. Even with the model you showed, Jeff, it seems that problem still exists. Is it possible to pull up the paper? Which problem? The six ring VSOs or VCOs, sorry, DCOs. What was the problem with it? I can explain by looking at this—let me find the paper. One second.

That was in this one here—the second time. This one. Yes, that one. Perfect, thank you. Let's say you move in this direction, then in that direction. I was saying, let's say you move right for a while, then left. With these 1D grid cells or path integrators, that's basically what they are.

When you're moving to the right, the five-one would be active; when you're moving to the left, five-four would be active. Presumably, this grid cell that's receiving all these inputs knows where it's meant to be active—it knows to reactivate based on which inputs it's getting, like when it gets either five-two. Let's say it's magic; somehow, when you start moving again, it knows which are the correct right cells to be in phase. That makes sense in my head. Right now, module one, cell two is active. It's really unhelpful that they called it capital "fi" and lowercase "fi." I'll call it module one and cell two, so module one, cell two is red. In module four, cell two is also red. If we move the full phase to the right, cell two in module one will be active again. If we move a full phase to the left, cell two in module four will be active again. In both cases, we've moved over the grid cells, become active, moved back, and the grid cells are active again.

What this tells you is that you have a completely different neural mechanism representation if you're going left versus going right. If you're going left, a bunch of these rings are active; a different set of rings are active when going right. The active neural representation is completely different between those two, but once you've completed the movement, the set of active cells is the same. That's what tells this grid cell to become active again.

That's the path integration: you moved away, you moved back, and you know you're at the same location, so the grid cell should become activated. We're getting path integration when we do a full phase movement away and then a full phase movement back. The issue I see is, what if we move half a phase—half the way around the ring? Now, cell six in module one is active, and cell two is still active in module four because we haven't moved left at all. My understanding is, if you're not moving in that direction, these cells stop being active. If you're moving right, then module four is not active at all. That causes problems if they don't stay active; you have to somehow reactivate them correctly.

If you just reactivate it based on when it was last active, that doesn't seem sufficient. You'd have to somehow know otherwise; when you move back left, it's going to be the wrong cell that's active, and that grid cell won't activate. As far as I know—and I haven't looked at this in maybe four years—no one has addressed that question. I've never heard a complete theory of how grid cells come about that makes sense.

My proposal doesn't completely solve every problem either. You brought up a good one—it's really complicated. How do these cells know which one to become active, and how does a grid cell handle the different representations going left versus right? By the way, that is actually useful—knowing if you're going one way versus the other. Sometimes grid cells reflect that.

The point is, I just wanted to check that I wasn't misunderstanding something. It's not a complete theory. It's so compelling to look at those cells in layer five and six that are large field, edge movement detectors, and having all those cells in the minicolumn represent the same basic receptor field. That's exactly what's needed, at least in this diagram—the one we're looking at here. I don't think this paper resolved all those issues. Maybe it did; I'd have to look at it again.

I thought the grid cell has to learn how to path integrate between the different movement vectors. It wouldn't just activate with one specific combination of those phases in different movement columns, but with a bunch of different, very specific combinations. One way to think about this is that grid cells anchor based on environmental clues. In this case, the environmental clues would have to anchor the individual cells in the rings.

Somehow, the environmental clues tell me which one of these things to be active. I don't know how that works.

We started out today, and I made the observation that this is a very complex area. I'm going to stop sharing this unless you want to see it still. It's a very complex area. As far as I know, no one's figured it all out. It's still an area, as Hojae said, where I don't know how many people are working on it now because it was so hard. I don't know if they got enough rewards on it anymore, but as far as I know, it hasn't really been worked out in sufficient detail to answer all the questions we have.

From Monty's point of view, it doesn't matter. We can pick up certain ideas from it and try to go with it. One of the bigger takeaways for me regarding Monty today was to think more about the direct output of each column and how we want to represent that. It's something we've been discussing recently, even last week with you. I feel comfortable with the idea that it's a learned representation based on what the actual subcortical motor generator did, and therefore it could control it.

The question is, broadly, maybe there are two options: one is a movement or displacement, and the other is more of a target, like a location. That's how we sometimes conceptualize it. Maybe there's a bit of both, depending on what's projecting to what, but the location makes sense in terms of specifying what is wanted and letting the system figure out how to get it. The movement also has its own appeal because it specifies what you want to do, which gets closer to controlling a muscle contraction. The problem is that it can put more responsibility on the subcortical structures by just saying, "figure out how to get this." 

Try to follow this logic. There are two ways you can communicate: you could communicate the location you should be at, or, as we originally thought with the Thousand Brains Theory, communicate the location of this and that. But communicating location is really complicated.

If you communicate a movement vector, it's pretty simple. All I have to say is convert this movement to your orientation and figure out where you are. For example, I'm the eye movement; I don't know where I am relative to the coffee cup. How am I supposed to know that? I can tell you where I am relative to the head, but how do I communicate that in a language you would understand in terms of location? I can tell you how I'm moving, and that's easy to translate. You can figure out that you don't know where you are either, but I'm telling you how to move, and you infer where you are on the object. You have to infer both your position on the object and your orientation, but that was a solvable problem, and we did solve it. When we tried to communicate locations, it felt impossible. Maybe I was shortsighted, but it just felt impossible. Then I realized it's all about movement vectors—that's so simple. It works. Let the recipient figure out what it means.

I don't know how the cortex could tell a subcortical system a location when the models in the cortex don't know locations in egocentric space and know nothing about it, and the sensorimotor system knows nothing about locations in allocentric space. I guess that's the conversion we're doing. In Monty today, we currently communicate locations as input to Monty, but then we use movements for recognizing and learning objects. We communicate the locations because they're necessary for the voting to work; otherwise, the columns don't know where they are relative to each other in the world. If I were to guess, voting is a mechanism we haven't really discovered yet. Maybe it's related to the claustrum or something. I don't know.

We've said before that the way voting works now is probably too complex or involves too many reference frame transforms for the one we've implemented. Too bad. Okay, you would do it that way, we'll do it that way. It's not a problem right now, but in terms of understanding the brain, it would be nice to find a simpler version. I think the brain likely uses a simpler method. The idea of communicating movement vectors was a revelation that simplified everything. Of course, it has to be that way. If you have an idea of how voting works with that, we can bring it up as a separate topic.

I've never really thought about it. We've talked about voting, but not about how to know your relative positions to each other. That's the key. With flash inference, the assumption is almost like there is no movement. If someone flashes an object in front of you, it's a bag of features. But humans don't want to do bag of features because these are not independent. We've known for a while that columns need to incorporate their relative positions to each other when it comes to voting. They have to agree on a common reference frame. Maybe the assumption was always that the reference frame is egocentric and they know where they are, and then somehow, something happens in the cortex that makes it work.

You have probably implemented this and thought more about it, but I haven't really thought about the neuroscience much on that one. I could think about it for next week.

I've never read anything about it. Many of these ideas are speculative, but maybe interesting to explore further. One idea was whether the claustrum could be involved, because when we looked at connectivity, we saw lots of connections to and from the same layers involved in voting, like L3 and L5. Whether there's any evidence for that, it seems we could even approach it from a pure theory point of view. We could just say, here's what we know about cortical columns or learning modules.

Just figure out what they have to do. What does that require from mechanisms to do it, regardless of whether it's a column or something else? From a pure theory point of view, we need to have a representation of location. How could we do it? Maybe grid cells. What does the column really have to do to solve this voting problem? It's complex because it's changing all the time—your fingers are constantly changing positions relative to each other, which is a much harder situation and forces you to think differently than with the retina.

I can grab this cup, bring my Thousand Brains cup, and grab it with my hands in different ways. They're completely different, and the voting is completely different because their relative positions are different every time. Somehow, it has to be calculated very quickly.

We can think about what has to happen. If we had an answer to that, we could change Monty to use movement inputs and outputs. I'll think about it today and tomorrow and see if I come up with something. Please, other people think about it too. It might not solve as pressing an issue as some other topics, like modeling object behaviors, because things work fine with communicating locations right now. That's a good excuse for me to work on it since I'm not doing anything else. Maybe one topic to inspire brainstorming is the idea Viviane brought up: maybe behavior is just in behavior columns, or some columns just get behavior. Can we get that to work with the things we want, like reusing behaviors for different objects? Does it maybe even work better than what we had before?

Maybe I wasn't a fan of that idea, so I haven't spent any time thinking about it, but that doesn't mean it's not right.

At least it would be nice to know a reason why they need to be in the same column, because right now there isn't a good reason. It would simplify things a lot and seem like a nice solution, because then every column does the same thing and it just depends on what input it gets. Does every column do the same thing? Does every column have the same motor output? I guess it really would. Morphology models would need a temporal dimension in that case. What would they do with the matrix and all that? There could still be state conditioning—how an object can have different morphological states in a sequence. You could say when the object stops moving, you learn more morphology again, so that could be the state conditioning for that. It's definitely worth thinking through. Everyone think through it again. I think I said once before, that could be the whole MT-aware pathway. With the current solution, we're basically drawing the exact same thing two times in the same column. I like it if I think about it as the MP-aware pathway. If I think about it as co-located next to each other in every other column, I don't like it. I don't know why; it's just my personal bias. Part of it is thinking about what those long-range connections would need to do as well. We need to solve this even if they're co-located and behaviors are in every column. Right now, the behavior is trying to apply that movement to offset the lower-level column, and how that's happening, if it is happening.

It feels like that's worth explaining because it's something new that we haven't discussed. Viviane, if nothing else, it'll make us more confident that we do need behavior in every column. I'll retract my objection to the idea.

This reminds me of so many things. It seems so complicated now, but I think in the end, when we figure it all out, it won't be that complicated. It'll make sense, but at the moment, it still seems like, how do I keep all these things in my head at once? It's empty colostrum, movements up and down, representations of behaviors. That's what I was thinking when I first thought of putting it in a separate column. We thought about all this stuff, so many things, but then it might boil down to just having a different sensorimotor module that sends changes instead of features to the learning module.

But we do know that even V1 columns have those receptive field properties I mentioned, with the directionally sensitive large receptive field cells in layer six and movement detection in superficial layers. What are they doing that would still be required for the morphology models? You still need the movement input—not the movement in the superficial layers, but the movement to layer three, the movement of the object itself. We can double-check that, since classically there's the idea that MT is more movement sensitive, so we should clarify what that's about. I think you would still need that input to the compass. Why would I need the object's behavior movement in upper layers in a morphology model?

Can I suggest we visit this next week? I feel like everyone's probably starting to flag a bit. Thanks very much, Hojae, for the summary of grid cells. We can discuss more next week. I think it would be interesting to talk about the re-anchoring work you found, and maybe someone can do a quick summary of what's known about the cluster—that might be useful. I have nothing new about it, but I can pull up the same material I found before. Let me make a note of that.

Otherwise, thinking about dedicating an hour or two to grid cells, and then considering behavior, this idea of total separation of behavior—what does that mean in terms of anatomy? Viviane, you have a clear idea why that makes sense, and I think you can share that. It's a small idea, so I'm not sure I can elaborate much, but I could look into the literature again to understand how you could have a column for behavior and a column for morphology. It's not obvious.

If we want to constrain it by neurophysiology measurements, it would be useful to double-check why everyone says MT is movement, and how much movement is in V1. MT gets input from V1, so there must be movement in V1 as well. The movement has to apply to morphology models, right? By co-locating them in a column, you can say, "I have this behavioral model, and now I'm looking at a different morphology. I can apply the behavior to morphology." We were doing that using hierarchy, with connections between two columns. I thought we were doing that within a column too, but maybe I forgot. In the main proposal, the higher column would have the behavior model and the lower column the morphology. We said it might be an option to do it within a column, but originally the idea was to have both within one column. That was back in February. It's gone—I forgot. If I had a nickel for every time we swapped between one column for both versus two columns, or every time I forgot something...

If the reason we need them in the same column is to apply behavior to morphology, that would be a good outcome to figure out. The idea that the "where" pathway, MT, is doing this is interesting. I proposed earlier that it's modeling the space around the body, but that was just an idea, not the best one. This is something else to think about.

All right. You wanted to end this, Niels? I think this is probably a good stopping point.