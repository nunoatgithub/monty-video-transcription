I was prepared to talk a bit about behaviors. Wasn't there someone else who wanted to say something? Did someone else? No, we didn't have anything else planned.

This is, again, part of our ongoing effort to get our heads together and see if we can figure out what's going on. That's how the research seems to go in this area.

Last week, we also talked about behaviors, and the way Niels was presenting it was very different from how I think about it. That bothers me because I think we all need to be thinking about these things the same way.

I wanted to go back and go through how I'm thinking about the problem, starting with some basic assumptions, and work through it to make sure we're all on the same page. At least we should all understand what we're trying to argue before we can say it's right or wrong. I still think there's a lot of confusion and lack of understanding of some of the concepts here because they're quite difficult. It takes time, so I feel it's good to go back and review this material.

Here's Vivian. Unlike everyone else here who's really good at putting together presentations and making them look polished, I only wrote up some text. It's not because I'm refusing to make presentations; I've just been extremely busy these past few weeks with family and health issues. I just don't have all the time I would like. I'm going to walk through one very simple slide, but mostly just text, and we'll discuss them. I'll share my screen, and you'll look at my word file. I'm finished apologizing for that. Let's see here. Okay, I'm going to share this one. Hopefully, you can all see that.

Yes. All right. I started off with background assumptions. These are just to remind us of some of the things we assume. I'm viewing this completely from the neuroscience point of view. This is not a Monty presentation, so we have to bear that in mind. I don't think about Monty as much as I think about neuroscience. That's my area.

I started with some background assumptions I want to go through. We should all know these already, but if you don't, just speak up and I'll go through them quickly. Then we'll jump into object behaviors, which starts at the bottom of the screen.

I think about all these problems in terms of columns, layers, cells, neurons, and synapses. I'm not sure everyone does, but it's worth going through again. The basic idea is that columns learn models of things in the world. You can see my cursor here, I assume.

We associate observed features with locations in space, and we do that through movement. That's how we build up models. Part of the models is that we have to learn the orientation of the feature independently of the object ID, which is something we didn't know in the beginning but do now. We think the orientation is encoded via many column sets, and this hypothesis seems to fit V1 and a few other places, but it's not necessarily 100 percent certain.

I've come to believe that all models are compositional, or at least that's the way I want to think about them. All models are composed of other child objects. I want to banish the word "feature," or at least understand that a feature is more than we typically think. If the feature or child object comes from a sensor patch, then the child object is very limited. The way I think about a column in V1 is that it's learning compositional models, but the inputs are coming from the sensor, and the sensor patch can only recognize a certain number of things. There's a limited number of child objects there. That's just the way I like to think about it. Even V1 is a compositional object—an object composed of other things.

If the feature or child object comes from another column, then it represents a more structured object, like the other columns can learn. The logo on the coffee cup, for example—any individual column, although it's learning a compositional object, doesn't really know what the inputs represent. A column doesn't know if its input is coming from one place or another, or what it means. It's just associating some SDR with the location. Through back projections, a column can basically say, "At my location," and tell a child, "You should be observing this." These are just learned associations. The parent object doesn't know what; it's just sending back, "We're both observing these things at this time."

Locations are defined within a reference frame. We often use the word "reference frame" to have metric properties. This came up last week when we talked about changes in reference frames. I'm not sure what the technical definition of "metric" is, but for me, metric means that the spacing between the locations is the same in all directions—it's consistent, or the same starting at any point. This gives us really nice properties. You can path integrate from any location in any direction and know where you're going to be. It also means you can calculate the distance and direction between any two points.

Generally, reference types can't morph or change without losing their metric properties. Although, as Neil pointed out last week, grid cells do this a little bit. It's a mystery, but they don't do it a lot—just a little. It's been studied, and as far as I know, no one understands it. It's not like the whole thing is lost; it just gets slightly distorted in certain situations.

In the brain, reference frames are created by grid cells, but it's not just grid cells. When we talk about grid cells, there's a whole series of other cells with grid-like properties. People talk about grid cells because they're the simplest to understand, but there are actually many others, and people are confused by that. There are other types of cells. There's a lot not understood about how they work. One thing we've talked about is that they appear to be two-dimensional. I have all this evidence why grid cells are two-dimensional, but we would like them to be three-dimensional.

Okay. The same idea applies: grid cells themselves do not represent unique locations. Grid cells repeat over and over again, which is true for any neuron unless you have grandmother cells. It takes multiple grid cell modules, or grid-related cells—they don't have to be grid cells themselves—to create unique SDRs that are specific to the object and its location. It's well understood that when an animal recognizes a new environment, the grid cells re-anchor, meaning which cell within a grid cell module is active changes. If you have enough of these, you essentially define a new set of locations for the new object. 

It's important to remember that when we talk about locations in the brain, it's unique to the object, yet they still have these properties of path integration. 

Can I ask a question real quick? The re-anchoring—from what I recall, the idea is that it's random, like a random initialization for the re-anchoring? We don't know; it appears to be random from a neuroscience observation point of view. There may be some underlying mechanisms that no one knows about, but there's no random number generator in the brain. The assumption is that you can treat it as random which cell becomes active. 

Do you recall off the top of your head whether, if I put a rat in environment A, it's got its anchoring, then environment B, new anchoring, then back to environment A—does the familiarity of the environment help re-anchor in the same way? Yes, of course. Once the animal sees enough of the new environment to know where it is, then the cell is re-anchored. It has no other way of knowing except through some sort of observations—it could be smell, tactile touch to the whiskers of a rat, things they see, any set of clues that lets the animal say, "Oh, I know where I am." That's when the grid cells re-anchor. 

Would it re-anchor in the same way? Absolutely. That's the whole idea, and it's one we're relying on: when you come back to the same environment—or in our case, the same object—the grid cells would re-anchor exactly the same way, and you're back in the space. Once you're in a particular object space, all the predictions make sense and you know what to expect at every location. It has to re-anchor to the same set of anchoring points for anything to work, and that's what's observed, if I understood your question correctly. 

I was just thinking about the random initialization and how that could be a problematic issue. Why would it be problematic? If we're doing object recognition and extend this to the world of object recognition, we definitely want to be able to return to the same overall space when we recognize an object for the second time. However, in the very first use case of encountering an object for the second or third time, we haven't fully recognized the entire object. If you haven't recognized it, if you're not certain where you are, then they haven't re-anchored.

There's a lot of evidence for rats in environments on this. My recollection is, when a rat's in an unknown environment—if someone else knows this, correct me—I think it's not random. I had the impression there might be some background, unknown environment in space, but I can't remember the result. I remember a study from the Moser group where they put rats in dark rooms where they couldn't see things. The animals—yeah, I need to remind myself—do they always anchor the same way, or was it random? I think it always anchored the same way. There were also some other interesting tidbits, which may be misleading. The spacing between the grid cell firing was slightly larger, and when the animal recognized the environment, it tightened up. The space becomes looser when they don't know where they are.

Maybe that helps them infer. There's a lot of strange things that aren't understood about grid cells. The important idea is that once you recognize an object, once you know where you are, once you know what you're seeing—it's like the "aha" moment. "Oh, I know what this is." That's the moment the grid cells re-anchor. At that point, all your predictions come true. If I was in a dark room and couldn't see things well, didn't know what room I'm in, and someone dropped me in a room I know, it's dark and I'm searching around confused. The moment I say, "Oh, I know what this is, this is the bathroom," then instantly you can visualize the bathroom and know where everything is. It's that crystallization of the grid cell modules re-anchoring that defines every other thing in the environment. We're basically saying the same thing happens in the column.

To close the loop on Monty: in Monty, we would start with a hypothesis space where we test multiple ones in parallel until we know which space we're in. The question in biology is, is there somehow a multiplexing or simultaneous encoding of multiple spaces that could be possible until we converge to the true one? Or, as you were alluding to, Jeff, is there some sort of background generic space that is used to pass and degrade through multiple hypotheses until we can converge? I'm not sure. I'm not sure it's passing into multiple hypotheses.

We don't really know the answer to that question. It's a legitimate question. Brains clearly can do this. We've proposed one possible mechanism: the union of SDRs is one way of maintaining multiple hypotheses. That works up to a certain number, and it may be sufficient, and that may be how brains do it, but there might be other ways. I don't know.

Is it happening before the grid cells re-anchor? It's not like there's a denser firing of grid cells, is it? No, it doesn't seem to be that way, but it could be.

Again, there's a huge amount of literature on grid cells, and many unknowns about how they work. If we want a more in-depth discussion, I would need to spend a week reading papers. But we do know they exist, and we know these other grid-like cells exist. We know they re-anchor. It is our hypothesis, and others have hypothesized as well, that multiple grid cell modules lead to unique encoding of space, and that this is almost necessary. Without something like that, you can't encode any space. At this point, we have to say there are many details we just don't understand.

Can I go on?

This last paragraph basically says we've modeled this successfully so far in Monty for learning models of static objects. Now, we're trying to understand how to model objects when they're not static, when they change. We've used multiple examples, like a stapler and a traffic light. Many objects exhibit behaviors—computers, keyboards, and so on. When we say they exhibit behaviors, it's the same object, but things are moving or changing. We have to learn a model of not just the static components, but also how it moves and contains different parts. Even something simple like a stapler has many behaviors: lifting the lid, opening the internals to access the staples, the rotating deflection plate on the bottom, little catches to open the springs, and so on. We have to learn all this, and these are all part of the same model of a stapler.

We're trying to extend our column model to learn how objects can change over time. Along the way, we had some surprising insights. One is that pretty much everything about a model can change. I can come up with examples where features move, change orientation, change scale, or where the ID of a feature at a particular location changes. These can occur singly or in combination. Any or all of these types of changes could occur at one or more locations. So, pretty much everything about an object can change, though they typically don't all change at once. There seems to be no limit to the types of changes an object can exhibit. This led to the idea: what is an object if everything can change—if all the features, locations, and orientations can change, what defines it? The answer is two things. One is that the anchoring of the grid cells for a particular object will not change, even if the object changes. We can prove that. Also, these changes occur sequentially through time. If I saw an orange on the table, left the room, and came back to find an apple, I wouldn't assume the orange turned into an apple. But if I was watching the orange and it turned into an apple before my eyes, I would think something magical happened. Proximity in space and time are important for us to realize it's the same object. If I see a small change, like a traffic light turning green, I don't think it's a different object because it's almost all the same. But there can be enough changes that sometimes you wouldn't know they're the same thing. It helps when these changes occur while you're observing, and they're close in time. That's basically the problem space of object behaviors.

We have to extend our models to include that. I propose the anchoring of reference frames defines the object. If the reference frame re-anchors, we perceive it as a different object. Therefore, object behaviors change what's associated with one of our locations on an object. To learn an object behavior, we have this reference frame with features on it, and now we have to say, under certain conditions, the features are different at different locations. Some objects exhibit no behaviors, some a few, and some many. I just talked about an object completely changing to something different.

These changes are another thing we used to talk about. For example, the stapler can be open or closed, but it has many states and they're independent. This can be true for many objects. Multiple changes can occur independently, or they can be causally related. For example, with the stapler, typically when you raise the top, it doesn't change the deflection plate on the bottom. But you could build a stapler where raising the top rotates the deflection plate, and that would be fine. An object can have multiple changes, some causally connected and some not.

We have to be able to accommodate that. These are all things we need to do. Often, behaviors consist of a sequence or time series that changes the object. That could be along a trajectory in space, like the lid of the stapler rising and going down. Sometimes changes can occur at disjoint locations. For example, with traffic lights, the lights generally go in the same order—red, green, yellow. There's an old game called Simon, where you have to remember the sequence in which colored buttons light up, and it's much harder to do that. Things can change randomly, but generally, it's easier when we see something moving through space continuously.

And then, this was the key insight a while back: when we learn an object's behaviors, we don't forget the rest of the object's model. I don't forget anything about the stapler when I've learned that the lid goes up and down.

This is the proof to me that the reference frame has to remain anchored, because if you change the reference frames, all learning is lost. Within the same reference frame, we have to be able to say, at any point in time, under different states, what features are we expecting to see where.

I got confused on this topic—object states versus current state. I was using the word "state" for two different things. One is the different ways an object can behave; for example, the stapler has multiple states, like the deflection plate being in one of two positions. Those are different states of the deflection plate. But we also use "state" to refer to its current settings, which is not part of the object model. The object model has to represent how the top can move and how the deflection plate can move in its different positions, but we can also say, "What's the state of the stapler right now? It's open." That's not part of the model; that's a temporary condition we might want to remember.

I don't have a good answer to this question. I was getting confused because I was using the word "state" for these two different things, and they're really quite different. We will probably run into this confusion more in the future as well, because in the Monty implementation, we actually call the message class that we use to send information between learning modules the "state" class—like the hypothesized state, the goal state. It would be good to think about how we can distinguish the two in terminology, maybe having "behavioral state" and "current state," or something similar. I started trying to use the word "behaviors" of an object, but I didn't want to redo the whole document. It's something we have to come to grips with, and maybe we can define some better language for it. Even "behavioral state" and "object state" are confusing. Is the behavioral state the current state of it, or the things the object can do? I was using the word "behaviors"—the object has certain behaviors. That's not a state; these are things it can do, whereas "state" typically refers to a temporary condition. What is the current state? I don't think we have to solve it here, but if someone wants to propose something, that's fine, or maybe I'll do it later. Sometimes I refer to these as object behaviors, which is clearer than object states. "Object behaviors" doesn't have that confusion.

This is all about being conscious of this distinction, because to do anything interesting, the system almost certainly has to remember the current states of an object—its current configuration. For example, when making coffee, the coffee maker has different behaviors it can have. As I'm making coffee, I have to remember where it was last—was the lid up or down, was the coffee in or out, was it on or off, was the plug in? I have to remember these things. Even if I take the coffee maker out, put it on the counter, and go get the coffee, I have to come back and remember: did I plug it in, did I turn it on? There needs to be a sort of episodic memory to solve any kind of interesting task, and we haven't really dealt with that yet. In the brain, this is almost certainly done in the hippocampal complex—the episodic memory. It almost certainly wouldn't be done in the individual columns in the cortex. It doesn't mean we couldn't do it in the columns in the cortex—in Monty, we can do anything we want—but this is something we haven't really dealt with before. We will have to deal with it to solve interesting problems. We'll have to have some way of remembering the state of all the objects we've recently dealt with, so we know what to expect and what has to be done next.

Does that make sense?

Just to clarify, how is that different from us remembering the recent locations and orientations of objects? I feel like that's the same class of thing. Do we do that?

In a sense, yes. In the past, we talked about having learning modules that very quickly build up temporary graphs, which is a bit more like the hippocampus. We talk about it in terms of states, but just in terms of where things are relative to us. It may be exactly the same thing. I thought about that, thinking of going back to the dinner setting example and trying to understand it.

That's like a temporary configuration. I got really confused by it, so I didn't bring it in because I couldn't make sense of it, but it might be the same thing. Maybe you can solve it the same way. I don't know.

I didn't deal with that any further. The next big thing here is a really big insight that occurred, and I've talked about it in the last few weeks: I believe these different behaviors—behavioral states, if you will—of an object have to be learned on a location-by-location basis. It's not a global thing. I can't say, "What's the state of the stapler?" I can say, "What is the position of the stapler top?" or "What is the position right now of this deflection plate?" but it's not a global thing. The right way to think about it is on a location-by-location basis. This is still a bit of conjecture, but I think it's going to hold.

And it's very similar to how we had the same breakthrough for compositional objects. The only way to think about it is not, "Where is the logo on the cup?" That was the wrong way of phrasing the problem. The way to phrase the problem for compositional objects is: at this location on the cup, what is the child's object orientation and scale? At another location, the child's object location and scale could change, or maybe not change, but it's on a location-by-location basis. I think that's how we have to think about behaviors of objects—on any location, that location can change from what it was before. Previously, I would have said there's some feature at some orientation and scale, and now I might say there's nothing there, or there's another scale, or a different orientation, or a different feature. But it's on a location-by-location basis, and that seems required to solve a whole bunch of problems.

To do this, I propose that a column has a layer of cells that represents the current state. It has a layer of cells because that's how neurons represent anything—a layer of cells, mostly, not completely, but usually, that represent the state of the current location. As I move through points in space, this other set of cells would indicate what should be under the current location—what should you observe at that location. At one moment you might observe A, and at another moment you might observe B at the same location. That's the definition of a behavior. We need some way of telling the neurons to predict something else. If this is a column like layer four, we have to have some way of telling the neurons to predict something else. At this point, I'm going to switch to my one slide. I'm going to share again. I shared just that one thing. I'm going to share everything here.

This is a PowerPoint slide. Hopefully you can see that.

This is a very crude idea. Typically, we talk about layer four as the input for the feature from a child object feature. Layer six is going to be representing a location. Our model today is that you form a unique representation of the feature at that location in layer four. The idea is that there would be another input to layer four. I just picked layer two as a placeholder. I didn't go back and look at what would be a good candidate, so this is just a pure placeholder. I have no idea if it's where it is. The idea is there's another set of cells representing state. The three of these together combine to tell you what, at some location, based on the state, you would predict one feature, but based on a different state, you predict a different feature, or no feature. This is the thing that would be gating and changing the different behaviors on a location-by-location basis. The mechanism for all this I don't really understand yet, but that's the basic idea.

One thing, just if you're not aware of it—this picture here shows a neuron and a dendrite. One of the things we see often in real biological neurons is that when a neuron receives inputs from different sources, like from one layer and also from another layer, or maybe from the thalamus, they tend to be segregated on the dendritic branches. They're not all mixed together; they'll be in some sort of order—the first from one source, the red source, then from the green source, then from the blue source. This has been documented in many places, but I'm not sure it's been documented everywhere. We do know this kind of thing exists, and this is one way that this could work. For example, you might say, "All right, red input may be coming from state, blue input from the feature, and green input from location." Each of these has to be combined in a certain way that gates this whole process.

You're unmuted, you're talking about something else.

Michael, I'm sorry. Cat dilemmas. Sorry, Will. I assumed it was Will. I was trying to do voice recognition.

anyway, there are mechanisms by which this can occur. This is not a weird idea; in fact, our hypothesis and theory about how the thalamus does routing is based on this, and there is evidence that it occurs in the thalamus. This is not a crazy idea. If you're thinking that this input could gate the rest of it, you have to have the combination of all three of these things to get this neuron to be at a predicted state.

Is that somewhat clear?  
I have a quick question about that. Wouldn't it be easier biologically to bind a state with a location if the sources did mix, so they could lie on the same dendritic compartment?  
Say it again.  
I'm just thinking about a dendritic compartment with 20 synapses or something, and we need a certain threshold for the neuron to fire. It would seem that if you're trying to bind features, locations, or things from different domains together with a neuron, it would be better to intermix your signals so that they mix in dendritic compartments.

Maybe, but let me explain why that might not work. For example, if the state was the red section, this cell has to recognize the state. It has to form 15 or 20 synapses to the neurons in layer 2. If I intermix all these things together, those 20 synapses may get far apart and may no longer be in a particular integration zone. The integration zone is about 40 microns wide; you might get 40 synapses in 40 microns, no more. If I start spreading them out over a longer distance, the neuron can't recognize the pattern in layer two because they're spread out too far. They need to be within a limited zone for that pattern to be recognized. Regardless of the merits of your idea from a theory point of view, from a mechanism point of view, it probably wouldn't work.

So, Scott, do you mean if it's an "and" relationship, like we want this neuron to be active if the feature and the state are there? We're just trying to bind together, say, location or something like that, so it's depending on both of them.  
Oh, I see. I misunderstood the question. The way I drew this, it looked like the neuron would fire if the red ones are active, or the green ones, or the blue ones, so wouldn't we want to bond together?

That's what it looked like when I drew it, but that's not what I meant. In reality, there is a dendritic action potential that has to travel along the dendrite, in this case from right to left. That action potential could easily be blocked if the red pattern wasn't there. These are not independent; they're in order, and the order matters. Some of these could block the others. They're not independent. Am I getting at the question?

To your proposal, yes, if it's learning the association of both things, that could be learned in one compartment, I think. Is that what you're saying?  
Yes. I was reflecting on the idea that if we want the neuron to behave like a coincidence detector—a coincidence between a particular location and a particular state—in that case, you would prefer to have a single dendritic compartment receive input from both location and state.

Maybe, but look at it this way: what if this neuron would only become—by the way, we're not even actually spiking, we're just depolarizing, but let's say it's a spike—that all three of these patterns have to be recognized: the blue, the green, and the red. If they're not all recognized, the dynamics of a dendrite are such that if they're not all activated, maybe even in the correct sequence, then nothing gets to the soma or it doesn't happen. The dendrite, that black bar, is not just a summer; it's a complex processing element, and the dendritic action potential has to travel along this distance and can be blocked along the way. There were several proposed mechanisms for how this works by other researchers, and it's part of our thalamus theory.

Entire dendrite sections can be disabled by a particular pattern.

Go ahead, Vivien.  
Just sharing a neuroscience paper about this that I recently read and thought was really nice. In thalamocortical neurons, it shows how input from different sources like cortex, brain stem, and TRN synapse on different parts of the dendrite. They have a theory around what kind of mechanisms this could enable. This segregation, like I've shown with the red, green, and blue, is seen in many places.

This is not speculative. The question is, what are the actual mechanisms that Vivien just mentioned, and how do they work? There's a lot of complexity, and some of it is not known. For the moment, we don't have to worry about that here. This may become important in the future; it may be some form of generalization. All the things recognized by the red are generalized, and there could be other things going on. The main point is that the idea that a neuron could fire or be depolarized based on the confluence of location and state, and that those two have to be unique, is supported by mechanisms. It's not a crazy idea; in fact, it's almost a given—there are examples of it.

I don't think we need to worry about it much more than that. But it was a good observation, Bill. You were right; I just didn't explain it completely. Are you okay with it now?  
Yeah, I didn't want to derail too much.  
Oh, is that Scott? I'm sorry, Scott. I'm not looking at anybody, so I don't see what you're all saying.  
Sorry, Scott.  
All good.

Where was I? Going back to that other image, as the location changes, sometimes the state will change. I don't really understand when and how it changes, but if I have the correct state—given a state and a location—it predicts a particular feature or no feature, and a different state at the same location would predict something else. This mechanism allows the model to have different predictions under different states of the object.

One thing I was interested in discussing is that it makes sense to have the state be location-specific in some of the representation for behavior. At the moment, the object IDs or features are also location-specific. The way we do that is in the lower-level column, where we'd have a location and an object ID, like in L3.

Are you talking about the child object ID? Yes, that's being fed up to L4. I'm just wondering if there's a reason you feel we wouldn't have something similar for state, where state is location-specific, but the handling of the location-specific component is delegated to the lower-level column.

I'm not following that. Explain it again to me.

Go back to this image. As well as the feature coming in—which, if this is a higher-level learning module, is going to be the object ID from L3 as well as direct sensory input—it might also be getting the state of the low-level object. I thought about that. Should it be passing in the state?

I don't think that works. For example, I can imagine an object that has a lot of behaviors, and the child objects themselves have no behaviors. The child objects don't have any behaviors; they would have no state to pass in. But the parent object would be in different states, expecting different child objects in different locations. We can come up with some examples of that. That tells me I need to have a local state in the parent column, because it's not something that can be passed up from the child, since the child doesn't have any states.

Equally, you can imagine objects that have behaviors that, when they are a child of a new object, carry over those behaviors. A button can appear in different contexts, and whatever it's a child of, a button has button-like behaviors. The high-level learning module that's learning the compositional object needs to learn what effect the button has on the other child objects in its representation, but it doesn't need to relearn what a button is—that's the responsibility of the lower-level learning model.

So, if I understood what you're saying, there are two things we're talking about. How do I associate changes in state on different objects? If I push one button on an object, maybe another button elsewhere changes. Or it could be more complex: I could push a button on one object and a completely different object that I'm not even attending to right now changes. I flip a light switch in a room, and a door opens to the next room. We have to learn those things, and I'll talk about that, but I think that's a separate question from whether a child object has to pass its state to the parent.

Let me state this, Niels. It's easy to prove that the column has to have its own state to tell how its model should change. I can prove that because the child object may not have any states, but the parent object can have behaviors, so there's nothing to pass from a child object. That doesn't mean your proposal is wrong or that we aren't passing state from a child to a parent at the same time.

Maybe it isn't. I haven't really had a need to do that yet. Maybe I will, but so far I haven't needed to do that. I need to have the state in the column—that I absolutely have to have. I haven't needed to pass a state from a child yet, but I'm thinking about it right now. Maybe I will. Maybe you can come up with some examples.

Yeah, no worries. I just thought it was interesting to talk about location-specific state. I do think I can very likely prove that it has to be local, but that doesn't mean it's only local. We might pass states as well, if layer two is involved. What I will get to in a moment is that I do think states are things that have to be associated across columns, across hierarchy, across modalities—that a change in state in one column could affect changes in state anywhere else in the cortex, in some sense.

Those have to be associatively learned.

I'll get to that in a second. We'll have to leave it as an open question whether state is actually passed in with feature. It seems like if you think about it, the state of the child object is like another feature. For example, with a traffic light, the lower level might be just the light bulb of the green light, and that might be in the on or off state. That is just a feature that goes into the parent traffic light model. When the traffic light changes its state from stop to go, the feature of the green bulb changes from on to off. I also have to associate that with the changes to the red bulb and the yellow bulb, and those child objects are not necessarily talking to each other, but I would have all the information I need in the traffic light object.

The way I would attack this problem is that it was a very difficult problem we dealt with regarding compositionality. It took us some time to figure out that a parent object doesn't know what its child objects are. It just gets some bit pattern. That's all it knows and all it needs to know. In fact, it doesn't need to know where on the child object it is. It doesn't need to know the orientation of the child object—maybe it does—but it doesn't need the location of the child object. All it needs is for the parent object to say, "Oh, there's some bit pattern here," and then back-project to the person sending that bit pattern, saying, "I'm at some location X, Y, Z; you need to associate my location with all the details of your child object." In the traffic light case, it's very similar in the sense that the child object doesn't have to send extra information up to the parent object. The parent object projects back and says, "In this state, you should have been in this state in the traffic light; you should have been whatever state you were in the child object state."

In the traffic light example, the high-level model of the entire traffic light doesn't necessarily need to model how each of the lights change, like how the color changes everywhere, if it's on versus off. The high-level model is used to coordinate how the red light is on, then the orange light is on at the same time, and then those both turn off and the green light turns on. For it to do that, it has to have some sort of concept of state internally so it knows where it is in that cycle—the order in which they occur. That would be more like a global state, or the state for the light as a whole, but it would be on a location-by-location basis. You can't have a state of a whole object; we've already determined that. There's no such thing as the state of a whole object. An object has many different things that could be changing. That's consistent with everything I'm saying. Basically, whether one of the lights is on or off is just a feature changing in L4. The fact that it is on or off is already detected by the lower-level learning module, and it's just a changing feature in L4. How this feature can change and how the lights depend on each other—imagine we had a traffic light that was completely flat black, with nothing identifying where the lights were, but then the lights would come on and there would be a red spot in this black area, then a green spot, then a yellow spot. If the light wasn't on at all, there would be nothing to indicate there were any kind of things there. It's hard to say that there's a child object at the locations where the lights are because there's nothing physically there until the color changes.

It seems to me that there is no child object, but then you might say, "Okay, there wasn't a child object, but now there is a child object, and the child object is a green circle." You could model it that way, but my point is the child object wouldn't be sending up a state. The child object would be either a green circle or not a green circle. It's either there's a green circle or there's no green circle. It wasn't like there was a green light and now it's off; it's just that a moment ago there was no green circle here, and now there is a green circle here.

I was just proposing that whether the light is on or off, which could be the state of the lower model, can be treated the exact same way as all the other features that come in. There is a green circle or there's not a green circle. There is bright light and there's a dim light. I would agree with that. The question, going back to what Niels is asking about whether we're passing state into the parent column, is that we might be doing both. We have the state in L2, like you suggested, and we can also receive the state of a child object as a feature and interpret it as a feature in L4.

The language gets a little confusing. I'm open to these ideas. What I'm going to be adamant about is that there is a local state to the column in the model, and that state can change. That state represents the state of the current location and not anything else, and it has to be local. Whether we're also passing in state from a child object, I'm open to that. It seems like a possibility, but it is confusing to think about exactly what's required. Maybe we can leave that as an open question. We could say, in this case, we might say something like, how about that?

Will that address our confusion for the moment? It wouldn't be treated any differently than the features that are coming in, and it wouldn't be equivalent to the state that's being modeled in L2 or whatever layer. If we're passing a state as a separate variable, I assume it would be handled separately somehow. Would it? Or are the feature and the state just combined? I'm not sure. I would argue it could be treated the same way as other features that are coming in. From the column's point of view, there's not really two things—it's just feature and state. The state becomes another feature at that location.

I'm open to all these ideas. I just find it a little bit confusing to think about. Maybe clearer examples will help tease apart these issues. One thing the state has to do, which I'll come to in a second—the state labeled in this picture—has to associate a link to other states. It has to associate a link between changing states within this column, like a change in one part of an object leading to a change in another part of an object. Or a change in this object could lead to a change of state in a different part of the world, a different object. Those links are sequential associative links, and that's not the same as voting. They're more like a sequence. It's similar to how features change based on movement or anything, really. The whole trick here, the bottom line, is to relate changes in one thing to changes in another. That's how we learn to use tools, turn on things, or manipulate stuff. How do you change one thing to create a desired change somewhere else?

The state variable, wherever it is, is going to be associatively linked within its own column. For example, if this was modeling a traffic light, imagine you're looking at the traffic light through a straw and can't see the whole traffic light at once. You're looking at some benign part of the traffic light, and all of a sudden the light comes on. As we talked about previously, some external action policy detects there's been a change. The column moves to that change and says, "Oh, there's a green light here." The last time I was here, there wasn't a green light. My model says there was no green light at this location. Now it's green, so something has changed. This is a new state at this location.

Then the yellow light turns on, and the column is once again redirected to the place where that change occurred. The same column says, "Oh, now at this location, I was just looking at the green light, now we're in this new yellow state at this location." The pattern in L2 at one moment represents the green light coming on in one location on the object, and the next moment it represents the yellow light coming on at another point in the object. You can associatively link those two. When the green light is followed by the yellow light, this column literally went from the location of the green light to the location of the yellow light as they changed. You could follow through this, and the single column would learn that this is the sequence: when I see the green light, then it's followed by the yellow light, and it could expect that, and then it's followed by the red light, and so on. It could also learn the timing of this, for example, if there's a two-second delay in the yellow light.

You can imagine this changing in the state variable over time represents the different object behaviors—how an object changes over time—and it can learn that. Maybe when the green light comes on, a bell rings. There's some other column in the cortex, in the auditory cortex, that is listening, and the bell just came on—a change occurred. There would have to be a link between the green light in one column and the bell changing in another column, but that would be easy to do. It's just another association if the connections could be made. This is a way of tying these events together. Associations of states within a column and between columns are how we would learn how objects behave over time.

Did everyone follow that? That was pretty helpful.

Now I'm going to make it complicated. Let's talk about the stapler top opening. The problem with the stapler top opening—imagine I'm watching through a straw. There are a lot of things moving. It's not just like the green light, where there's one point in space where the green light came on and I go to it. The stapler top is moving, and there are multiple things moving at once. Imagine you're looking through a straw at the bottom of the stapler, and now the top side is moving and a detection is made. The column can only jump to one new location, even though many locations are changing, like all the locations along the stapler top are moving. Where does that column go, and how does it learn that all those locations are moving at once?

I don't know the answer to that question yet. That's introducing a potential problem. I got the traffic light—that's pretty easy. Pavlov's dog—that's pretty easy. Stapler top—that's a whole bunch of positions changing at once in the space of this object. I had that question written down: if multiple things change at once, how is it that the column doesn't re-anchor and say, "Oh, this is a completely different thing now?" It's trying not to re-anchor. Typically, you wouldn't re-anchor. If something just changes in the world, something you're looking at, you wouldn't think it's a new object. You'd just think, "Oh, this object I'm looking at is undergoing some transformation." So I'm not worried about that, but I am worried about the issue of many things happening at once. Imagine I had a toy with a button, and I pressed the button, and then ten different things on different locations on this toy changed.

You pushed it and then a little window opened up in one place. You pushed the button and at the same time a little wheel spun, a light came on, and a sound played. All these things would happen—let's keep it all visual—there are different things visually changing about this object. When you first experience it, you're trying to learn this, but you can't learn all those things at once. It's impossible. You can't attend to all the different changes at once. What you tend to do is repeat the action. I see my grandkids do this: they'll push the button and focus on one thing that changes, then push the button again and focus on another thing that changes. You can't absorb all these changes at once. The straw can only look at one thing at a time and say, "Okay, how does that change when I push the button?" Push the button, see a change. Push the button, see a change. Then I'll push the button and watch a different change someplace else. Somehow, we bring all those together. I think it's going to work.

I think it actually will work, but I have to walk through the mechanisms to see how it would work. You can't learn it all at once; you'd have to learn it sequentially. Once you've learned it, then when I push the button, I can have an expectation about any one of those changes that would occur. I'd say, "If I push this button, I know that over here this will change, and over there this will change." But you have to learn them sequentially, through additive practice. One thing we've talked about is how, with hierarchy, if the stapler top is a child object, it's easier to imagine how that's represented with a single representation. Vivian, you mentioned on Slack that if you can vote on a state—sorry, an ID and pose—and do that quickly enough to infer it, that makes it easier to learn a behavior. The connection here is that as the stapler top opens, all the columns looking at the stapler see a consistent pose and orientation of the top. They agree on the pose of the top, so we quickly determine, "Okay, this is the new state, this is the new pose of the thing." But someone dancing, or a complex toy, is going to be harder because each arm and each leg is basically creating a totally new configuration. We need to break it down and learn: first, how do I move my feet, then my hands, then this and that?

The thing about the stapler top, which is interesting, is the idea that maybe the whole top is its own child object, and it may well be that way, but it wouldn't start out that way. I can easily imagine an object like the stapler where I have no idea it's about to move, or that it even has a seam or a hinge. I learn a model of it, and that model does not have the top part as an independent child. There's no evidence for that; I can't possibly imagine it. So I've learned this object, and there is no child object labeled "top of stapler" or "top of box." Now it starts moving. The first thing is, there is no child that I can say, "Oh, this whole child is moving." Multiple points on the object are changing at once. Maybe I have to practice and learn that the top is a separate object. Maybe that's the right thing to do, but initially, it wouldn't be the case. Initially, I would have an object—imagine it's a rectangular box with no seam. I don't think about it. Suddenly, the top opens separate from the bottom. I didn't have a model with the top separate from the rest of the box; there was no visible evidence for that. Now, what do I do? Maybe, as you're suggesting, when I first see this happen, I start focusing on and learning a new model of the top. That's the first thing I do. Imagine looking through a straw at the box, and then the box changes—the lid opens up—and my straw is redirected to the point where the lid is now partially up. At that point, I say, "This is different." Maybe then I scan along the lid and say, "Okay, this is a separate object," and I'll form a child object. But it's complicated. It's not so simple. I can't always assume there's a child object that's changing.

I like the idea that in layer two, or wherever the state is being modeled, we can learn associative connections between states. If I'm detecting that the top of the stapler is in a certain state, I expect every other point on the top of the stapler to be in a certain state as well. But I have to observe that; I can't assume it. I have to observe it.

Tying into that, we would have to learn that once, and it would be tedious, but then we should be able to transfer that to other objects—solid lines moving in this stapler way. We wouldn't have to relearn it every time with lots of objects, though there are examples that make this harder. For example, making the stapler more like a box where you're just lifting the lid. What if the lid was hinged on both ends and lifted up in the middle, like two pieces opening up? That's a different thing. I have to learn all these. I have to be able to say, "Okay."

How many, and I wouldn't know that, by the way. I wouldn't know that until, imagine your straw visual system could go up and see the right side of the lid going up. You might say the whole lid is going up, but it wouldn't be, because the left side would be going in a different direction. This proves the system has to observe all these different parts. It has to go around and try to figure out what has moved and where it's moving. The strong example gets really hard because there are multiple things moving at once. By the time you get to see where the other things are, they've already changed. Everything's moving. If just one thing was changing, you could observe it, but since multiple things are changing, it's difficult to know where to go, how to observe, and how to know what happened in between. It's a challenge to understand how a single column would learn all the complexities of a complex behavior. The traffic light is a simple behavior and easy to understand, but an object with multiple things moving at once is difficult. We can't assume there are already child objects and only the child objects are changing. Somehow the system has to learn this. You can imagine looking through a straw and trying to do this. If I was looking at an object and didn't know it had any behaviors, and then something moves and my straw is redirected up, but I'm only seeing part of it, and then something else is moving, I can't see everything that's going on at once. It would be very confusing. I wouldn't be able to figure it out. I'm not sure how it happens.

I'm almost done here. We have 15 minutes more. Let me go back to my tech side.

I'm going to stick with the object behaviors I learned on a location-by-location basis. We're open to the idea that state could be passed in from a child object, as we discussed.

I'm going to jump down to this paragraph. On layer four cells, which get input directly from the retina, less than 10 percent of the synapses on those cells actually come from the retina. People say layer 4 is driven by the retina, but only 7 percent of the synapses come from the retina. About 40 percent come from L6, which is consistent with L6 being the location. That leaves about 50 percent unaccounted for.

That could be state. I just wanted to pass that on, in case you didn't know. Behaviors are associations of state changes. When we talk about a behavioral object, we're referring to a set of state changes that are causally linked. Something changes here; it could be one thing that just changed by itself. An object could be green one moment and red the next. Typically, something happens in one place and something happens in another, especially if it's something I had to manipulate. Most object behaviors consist of multiple changes to the object. These changes can occur simultaneously at multiple locations. For example, the lid of the stapler going up, or a switch turning on lights at multiple locations. You'd have to learn each one. If you're looking at the world through a straw, you wouldn't be able to see those multiple lights at the same time; you'd have to do them sequentially. Sometimes these changes occur in sequence, like the top of the state, but it's a sequence—it doesn't jump around, it flows in order. If the stapler deflection plate rotated as the stapler top was raised, you'd be raising the staple top, but something else would be changing elsewhere on the object. A single column can't learn all this at once; it would have to learn it through multiple repetitions of the behavior.

Another key part of the conditional transition in the states is actions. I don't know whether that's represented in L5 or somewhere else, but it would probably play a role in how we move through the state space, depending on where we're pushing or similar factors. I've been avoiding that difficult problem, but you could imagine if it's a button, I can at least say I don't have to worry yet about who pushed the button. I could just say the button went down and the top opened. I'll come back later and ask how the button got pressed. It's complicated by things like capacitive buttons, where they don't move at all, but something happens. It's a good question, but I think that's an extension of this basic theory, at least I hope it is. Niels, I hope everything I've described so far is consistent and doesn't have to be changed, and we can then add on how the button got pressed. That would be ideal. We were just talking about how we learn these changes in states. I'll read through some of these comments. A column must be able to learn behaviors of objects. It needs to learn which changes occur together and how these changes occur sequentially in time. The basic method would be learning associations between the SDRs and the state layer.

A change of state in one location would be associated with a change of state in another location. That all hangs together really well, I think.

A state location for location A can be associated with a link to a state in L2 for location B. If I know that something happened in A, then layer two would predict another state, and that new state would be specifically linked to a different location. This tells me that states are tied to locations. If I know the state of something is changing, I know where it's changing, and I can direct my attention to that point. If I go to that location, I know what will be there.

The learned association, the L2, can be one-to-one or many-to-many. This relates to the idea we discussed: a simple example is pushing a button and the light comes on, but lifting the stapler could cause multiple things to happen at once. Many things can be associated with changes, and many changes can happen simultaneously. These changes can be sequenced or not; I could push a button and everything changes instantly, or there could be an order in which changes occur.

Some of those sequences could be learned with or without timed intervals. For example, a traffic light is a sequence, but that involves timed intervals, which may not be the best example here.

A single column will automatically learn associations between the states of the object it's modeling. If multiple columns are simultaneously serving the same object at different locations—imagine multiple columns looking at the stapler at the same time, multiple patches of the retina—then the long-range connections between layer two will associate changes occurring across columns. If I have multiple columns observing different locations on the stapler at once, some observing changes and some not, the ones observing changes could link to others also observing changes at that moment. For example, if my eye is fixated at one point and the lid of the stapler is rotating up, a group of columns that were observing the stapler top would stop observing it, while others that weren't observing any part of the stapler would now observe part of it. Many columns would go through state changes simultaneously, and they could all be linked.

The change anywhere could predict a change elsewhere. This may be a way of dealing with the fact that the top of the stapler is not a separate child object; I need to learn all these things at once. This is just an observation: if multiple columns observe the stapler at once, many would be observing behaviors and state changes, and they would be linked. All those links would make sense and associate somehow, though I don't fully understand it yet. Would this be like voting on states? I don't think voting is the right term.

I considered whether this is voting, but it doesn't feel like voting is the right concept. Voting is about reaching a consensus, but these might be doing something more. It's a great question, Vivian—maybe we should leave it unanswered. It feels like voting, but it's more than that. In my mind, voting is about reaching consensus on a thing—what is this, A or B? This is more like voting on what should happen next. Everyone is saying, "I see what's happening here," and there is a sequence to it. The order in which these things occur is essential. In voting, the order doesn't seem to be essential. Here, everyone is voting on what the next thing should be. So, in a sense, it's like voting, associated with long-range connections, but with more of a temporal dimension. The order is essential. Things can happen at the same time or sequentially, but not backwards. If B usually follows A, I don't expect B to predict that A is going to change.

B might predict the state of A, but not that A is going to change. It's an interesting question. I even asked myself if all the voting could be done like this. Somehow it seemed like voting, so maybe this is how voting occurs. I got confused thinking about this.

There are only two more points. Behaviors can be associated across modalities; any layer two cell changes, another layer two cell changes, and you can associate them.

The last point is about the state variable: what does it default to, what does it change to, and does it have meaning? Niels and Rami have been discussing this, and how a new variable is chosen when a change occurs. I don't have answers to these questions. I just don't really understand this yet.

It's not complete yet.

I'm done. I feel really good about this. I feel, as I've said in the past, that I'm making progress. Clarity is coming in, which feels great, even though many things are still unclear. The important thing is that it feels like we've nailed down a few concrete items, and we can go on from them with a much better understanding of what's going on. That's where I am.

I'm done. Hojay, did you have a— I was thinking about the question Jeff had at the end and also relating back to Vivian's question in the middle: is state part of features? I thought of another way to think about it. On my screen, probably the easiest—so we're talking about features, and depending on the lower level, we might have pose, color; higher level might have ID, and of course, they're in some kind of location. Let me just put that as well. Are features composed of pose, color, and ID, or are those separate things?

When we talk about features at a location, we're talking about all of these in the same location. So, the word "feature" is composed of pose, color, and ID. I like to think about it as a dictionary of things that we are cleanly sensing. If we think about state as part of features, I would add that here—this would be whatever we call s1 state. But another way to think about it is as a whole thing being a state of itself. This is not exactly clear, so I want to explain a little more.

In the learning setting, we're at an object, and we don't know what the state is or what the variable is. We just call it, at least in the program, s1. This could be part of an object that never changes, sometimes changes, whatever. Later, it comes back to the same location, but this time it notices a different set of features. Is there a way to move this? In a new visit, the second visit, we have some features, but now color is—let's say the initial color was something else.

We're at the same location. Yes, because I'm in the same location box. Now this thing has changed to red. So now, it came back and the feature has changed. I'm going to say this is another state.

I'm going to call this state two, and the location would store states. I think this is in line with the idea that the states are tied to the location, and at that particular location, the feature sets have changed. It could be a location plus a particular feature set. Instead of thinking of state as features, maybe we can think about it this way. I think this is still aligned with everything. I'm confused—is this different than what I proposed? I'm a little confused. It's the same. I'm just trying to clarify where state might be residing: is it part of features, or is it—it's a very good question. Is a state associated with a location, or is it associated with a feature? Is that what you're asking about? State is definitely, I agree with you, dependent on the location. If this is more programmatically, whether we define state as part of features and therefore it gets sent to higher-level things, or whether we define state as just another way of defining it—implementation terms, anyway.

I think it gets to the semantics of how we refer to state, and maybe also the point Jeff was raising that it can be confusing how the term quickly gets overloaded. Vivian was also—this is actually how a state is defined in the code base at the moment. There's this state object. But then we talked about behavioral state, like "on" or something like that, or a behavioral state like "crushed open," whatever. We need to find a language that enables us to talk about it in a way that we don't get too confused. Maybe in code we can just change the state to CMP. Obviously, what we actually call things is not essential, but it's very helpful to pick good words. We have to agree on the principles—what do we mean by it? That's the most important thing.

Some really good ideas came up here. It's worth thinking further about the child-parent relationship and whether state can be passed in that way, or what that means. It's still consistent to say there's a state variable on an object-location basis, and maybe that's passed on to the parent. It's not clear yet to me.

I feel like this was at least the most coherent view of modeling object behaviors we've had so far at the end of a research meeting. I can feel like it's not saying much, but we're making progress. I think so too. I've been through this so many times—it starts out as muddled confusion, and then you slowly pick away at it. I recognize this process, and it's going to work. We're going to get this. I feel really good about it. A lot of clarity came along. I'm going to continue working on this whenever I have time. Maybe next week I'll have some more things. I'll see.

All right. I think that's everything.