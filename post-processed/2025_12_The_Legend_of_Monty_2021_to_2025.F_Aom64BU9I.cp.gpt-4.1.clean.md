Viviane Clay: Thank you. Welcome everyone. I'm Viviane Clay, Director of the Thousand Brains Project. Today, we hope to give you a high-level overview of the Thousand Brains Project and the new type of AI we're building. We've set up a series of nine short talks, with each team member presenting a different aspect of the project. Not all talks are the same length, so don't worry if we're not halfway through the talks at the halfway mark. The event will take about two hours, but if you need to leave at any point, we're recording this meeting. If you have questions, feel free to post them in the QA section, and we'll answer them at the end. You can also upvote other people's questions if you find them interesting. We hope you'll leave today's talk series with interest and excitement about what we're doing. There are many ways you can help us: contribute code, build applications, or tell others about the Thousand Brains Project. We believe what we're creating is exciting and important, and we hope you'll feel the same way at the end of this.

A brief introduction to the Thousand Brains Project, in case you haven't heard much about us yet: The Thousand Brains Project is a non-profit company created at the beginning of this year. We believe the future of AI and robotics will be based on sensorimotor learning, so we're building a sensorimotor AI platform called Monty. If you hear the term Monty throughout today's talks, that's our AI platform. This is a new type of machine intelligence based on the principles of the neocortex, as described in the Thousand Brains Theory. It's not deep learning or LLMs; it's a different type of AI specifically designed for sensorimotor learning—super efficient in terms of energy and data use, capable of continual learning, and other exciting features inspired by the brain. You'll hear more about that later. We also want to make it accessible to everyone: everything is open source, we upload all our meetings to YouTube, associated patents are under a non-assert pledge, and we're building a vibrant community around this project, together with all of you, if you're interested.

Many of the talks later will touch on this part of the project. To start, let me tell you the story of this special project, what we're doing, and how we got here. To tell this story, I'll use a visual analogy. We often talk about how the brain uses reference frames to efficiently represent structured knowledge. You can think of reference frames as coordinate systems or maps that represent how different things in the world are related. Here, I'll use a map as my reference frame to lay out the structure of the project, and I hope it helps you follow along or at least provides an entertaining way to hear the story. Without further ado, here's the legend of Monty.

On a foggy October morning, the crew at Numenta, where this project started, arrived at the shores of Monty Island. We didn't know much about this island, but the captain of the ship, Jeff Hawkins, had a clear vision: first, that we can reverse engineer the brain, and second, that machine intelligence based on the brain's principles will be the most important technology of the 21st century. With that vision, we began exploring the island. We also had a useful compass—the Thousand Brains Theory. After almost 20 years of studying the brain, this theory was developed, and we used its principles as a guide. Scott will talk more about those principles later, but at a high level, they include sensorimotor learning—everything is about learning through movement. Our eyes move about three times a second, our whole bodies move. There's no intelligence on this planet that doesn't move. Everything about the system is about learning through interacting with the world. Second, having a general repeatable processing unit, modeled after cortical columns in the neocortex. Third, local information and learning—each of these general processing units receives only very local information about a small patch on the retina or a small patch of skin, and there's no big picture anywhere. Learning is not a global process, but very local. Fourth, reference frames—a way to represent how knowledge relates to other knowledge.

These principles have served us well as we explored the island, and we've used them as a compass at every decision point. Based on those principles, we built a basic framework for a new AI system, which we call Monty, after Vernon Mountcastle, the neuroscientist who first proposed the idea of the general repeatable processing unit in the brain.

Next, we set out to build the components of Monty. We established a first settlement to construct learning modules, designed after what is known about cortical columns and their architecture and connectivity. Another settlement specialized in constructing sensor modules, which are the interface between Monty and the world. They take in raw sensory data from any modality and translate it into a message—a language that learning modules can understand.

We also had to define the connectivity between learning modules and sensor modules. We defined a set of traffic rules for messages sent between sensor modules and learning modules within the Monty framework, called the Cortical Messaging Protocol. We iterated on the learning module design several times until an individual learning module could achieve robust object and pose recognition. We could learn an object by taking one path along it, then perform inference on the object in a completely new orientation, taking a never-before-taken path under noisy conditions. We call this the Evidence Learning Module and still use it today.

Over time, we improved sensor module capabilities and built a new settlement specializing in the design of intelligent policies. Initially, Monty moved largely at random, but now we have several more sophisticated model-free and model-based policies that Monty can use to efficiently explore the world using either sensory input or its learned models and hypotheses. A new monastery in the forest began investigating scaling the system to more learning modules. They implemented and tested the first versions of voting between learning modules to reach rapid consensus during inference.

Settlement with craftsmen specializing in optimizing the implementation led to significant speedups of the system.

Another scouting party explored stacking learning modules to learn more complex compositional models through hierarchy.

We also tested the innovations in the real world during a week-long hackathon called Monty Meets World. It was exciting to see Monty work outside the simulator for the first time. We could put an object in front of an iPad camera and recognize it, which felt special at the time.

After the initial infrastructure was established, we were convinced the system worked. We returned to the mainland to help with larger projects and gather more resources. About a year later, in mid-2024, a small team returned to Monty's Island, and the Thousand Brains Project was officially announced with funding from the Gates Foundation and the Korean Electronics Technology Institute.

We grew the team from two to eight full-time employees. You will hear from all of them today. We decided to broadcast the existence of this island and the work we're doing. We set up a small harbor village in the south of the island, where ships from the mainland could easily dock, hoping to build a welcoming community around the project.

Next to the community, we set up a lighthouse to send messages to nearby boats. This allowed us to publish our advances, and interested people could visit Harbor Village to learn more. We cleaned up our private framework code and published it under the MIT license. All related patents were put under a non-assert pledge, allowing anyone to build on this approach, commercial or otherwise. With the project announcement, the first boats arrived at the southern community. In December 2024, we hosted our first community event around the lighthouse, and today we hope to continue this tradition.

We published a white paper describing the project's goals, implementation principles, and algorithm.

We also created resources for newcomers to Monty's Island, including documentation, a YouTube channel with almost 150 videos, and a discourse channel for exchanging ideas and questions. Our team will answer your questions.

On January 1st, the Thousand Brains Project began operating as an independent nonprofit, separate from Numenta, with 501c status. Donations to the project are tax-deductible, and if you're interested in contributing, you can find more information on our website.

As part of our communication efforts, we set up a large watchtower and sent surveyors to map the settlements on the island. Surveying and reporting all the innovations on Monty's Island was a huge effort this year, but we were able to send an impressive report of Monty's capabilities to the mainland. This preprint was just accepted at Neuralcomputation, and you'll be able to see it there soon, or check out the preprint. Niels will cover many of the exciting results in his talk.

We added more checkpoints along the roads to measure traffic, such as the ability to measure the floating-point operations Monty uses during learning and inference to compare it to transformers. Later, we added an observatory on top of the watchtower to make it easier to observe what is happening inside the learning modules and Monty's brain. Ramy will tell you more about these interactive visualizations in his talk. There's a repository if you want to test these yourself. We documented more of the project and broadcast new tutorials and explanations in our documentation to the Lighthouse. You can find a variety of information and tutorials to get started.

Over the years of discussing and testing how to model compositional models, the theory solidified. In our recent preprint, we describe the long-range connections in the neocortex and their possible functions. It contains detailed descriptions of our theoretical advances, such as the need for reference frames, the role of the thalamus, and how the brain could model compositional objects using hierarchical connections.

The theory around modeling compositional objects was turned into code and tested, so Monty can now stack learning modules hierarchically. We set up new benchmarks and metrics to measure this ability, such as a dataset of different logos on various shaped objects. The team of architects is working on improving this ability through adjustments to sensor modules, learning modules, and the policy.

Throughout the year, we improved the usability and accessibility of our implementation. Monty is now versioned and distributed on Anaconda. We steadily improved our infrastructure, fixing bugs, making larger refactors, introducing rough formatting, switching configuration management, and many other improvements.

We also mapped out a plan to turn our code into an easy-to-use platform, and Tristan will talk more about that later in his talk.

In May, we held a robot hackathon and built more piers into the lake of reality. We tested Monty on several real-world tasks, such as a drone, a mobile ultrasound device, and a Raspberry Pi with motors and a camera on a LEGO setup. It was exciting to see Monty control a motor in the real world for the first time and learn and infer objects this way.

If you are interested in building something similar, we documented all of this in our documentation.

We ventured further into the lake with Monty's application to ultrasound and created a small dataset to evaluate Monty's performance. We put household items into a bag of fluid and let Monty learn these objects through one scan each. We will publish that dataset soon if you want to check it out and test it yourself.

We set up a camp below the magical tree of simulation and did several things with the simulator. One is the ongoing work on integrating Mojoku as a second simulator, as well as simplifying Monty's simulator protocol and prototyping the complete separation of Monty and the simulator code. Throughout the year, Learning Module Town has worked on improvements in how Monty can sample and delete hypotheses dynamically. Ramy will give an intuition of how Monty's hypotheses evolve over time in his talk. At a high level, this led to improved noise robustness, speed, and the ability to quickly change hypotheses and handle multi-object environments, allowing Monty to notice when the sensor has moved from one object to another and quickly update hypotheses. We also improved the connectivity between sensor modules and the motor system to allow for better model-free policies. To coordinate between goals from Learning Module Town and the sensor module market, we added a checkpoint before the mortar policy city called the Goal State Selector in Monty. Together with a new sensor module that can detect saliency, Monty can now move much more efficiently to interesting parts of the world. This makes both learning and inference more robust and efficient, and we are currently extensively testing this feature. We also began constructing another sensor module specialized in extracting 2D features like edges and texture on objects, such as the Thousand Brains Project logo. We made several theoretical advances, particularly on how the brain models object behaviors. We published a write-up of these advances on our documentation earlier this year and continually refined it throughout the year. We also published and developed a concrete plan for implementing this ability in Monty, enabling Monty to deal with a dynamic world where objects can move and have behaviors, instead of just static objects.

We have also made progress on learning causality and using models to effect change in the world—causal interventions. As part of that, we started exploring hierarchical decomposition of goals into subgoals and more complex action planning, tying together our theory on hierarchy, behavior models, and goals. Our theory discussions led us to explore the concept of attention further, and we gained more clarity on how attention could tie into other aspects of the theory. In the process, we explored related topics like fast learning, the role of the hippocampus, categories, and time. Over the past year, we've mapped out new areas of the island and have a much better understanding of the big picture, though many open questions remain. If you're curious about how we develop this theory, we spend a lot of time in Zoom meetings, deep thought, and intense discussions. We meet every week for up to four hours, occasionally in person for all-day sessions on big whiteboards, and every couple of months, we have focused brainstorming weeks with virtual sessions for four to five hours and individual thinking the rest of the day. A lot of time goes into developing this theory, and over time, the simplest ideas stick and key insights evolve. If you're interested in this process, we record all our meetings and upload them on YouTube so you can observe how these ideas crystallize.

As these ideas crystallize, we make action plans to implement them in Monty, prototype them, integrate them as new features, and publish results.

Throughout the year, more boats from interested contributors have arrived at our island, and it warms our hearts to see the many exciting ways people contribute their time and skills to this project. Will is going to talk more about that in his talk. A big thank you to anyone who has forked or starred our project on GitHub, and especially to those who have contributed code. We've had almost 2,000 posts on our forum, Discourse, which have been a pleasure to read and interact with.

To make it easier to see different aspects where we could use help, we turned our project roadmap into an interactive feature work table that's filterable by different aspects, and Will will discuss that further. Looking back, it's been a really exciting year. We started the Thousand Brains Project as an independent nonprofit entity. We published many exciting advances and are especially excited about the growth of the community, with great conversations on our forum and amazing contributions to our codebase. We've made a lot of progress on the theory, prototyped new ideas, and integrated new Monty features, moving Monty closer to a platform that's easy to use. We tested Monty in several real-world applications. Overall, we're proud of our settlements on Monty's Island and excited for what awaits us in the next year. With that, I hand it over to Niels, who will talk about how our approach compares to deep learning, what Monty can do today, and its many advantages.