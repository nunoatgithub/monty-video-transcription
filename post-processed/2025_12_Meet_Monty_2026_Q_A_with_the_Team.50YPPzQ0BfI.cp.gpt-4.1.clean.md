Thousand Brains Project: First question, do you expect that this sensorimotor approach can also be useful for cognitive functions? Who would like to answer?

Jeff Hawkins: Any one of us can answer that. I'll start. We know from studying the brain that the neocortex performs all these different functions, including all the cognitive functions we think about, yet it has the same or very similar architecture everywhere. The evidence strongly suggests that all cognitive functions are built on the same principles. Only recently have we begun to deeply understand this. We haven't discussed it publicly yet. We realized that by focusing on how learning modules work in a generic sense—how we sense through vision, touch, hearing, and understand the world—the principles we learn there would apply to high-level cognitive thoughts, and we're now beginning to understand that. So the answer is yes, but we haven't fully explained it yet. If someone else wants to add, they can.

Thousand Brains Project: Thank you, Jeff. Next question. Is Monty capable of moving through time in time series data?

Viviane Clay: Fundamentally, Monty is always moving through time in the sense that it senses different locations in space over time, but you're probably referring to basic time series data where the progression in time is not under Monty's control. That's one of the things we'll work on a lot next year, which Jeff alluded to: modeling object behaviors, how things change over time, and learning models of those changes. At the moment, Monty's models don't have a representation of time, but we're planning to prototype and integrate that in the next year.

Thousand Brains Project: Great, thank you. Next question. Can inference and training run at the same time as I suspect our brain does? If so, is there a moment where it decides whether it needs to create a new model for a new object?

Viviane Clay: I'll answer that, since I worked a lot on that part. In the brain, there isn't really a distinction between learning and inference. We are always learning new things and always doing inference. We actually need to do inference to recognize when there's something new to learn, and Monty can work the same way. In our configs, you'll see setups for training and evaluation, but the only difference is whether the things Monty observes during exploration are stored permanently in memory. Everything else is identical. If you do learning, Monty will first do inference and recognize what it is sensing. If what it senses doesn't match any of its models, it will create a new model and store all the points it just sensed in that new model. If it recognizes a model, it will integrate the past observations into that model and fill in any gaps, adding more details. If you run Monty in training mode, it will do both learning and inference.

Niels Leadholm: To add to that, Vivian mentioned we've had more ideas on attention recently, though most haven't been implemented yet. Our view is that attention is essential for many things, including learning. At a cortical level, you don't learn much about the world unless you're attending to something. In your question, you asked about teachers and their importance. You can learn without a teacher by exploring the world, but in humans, shared attention between parents and children or teachers and children—where you both look at something and the teacher describes it—is essential to how learning takes place. You can imagine something similar working in Monty, which would bootstrap and speed up the process. As Vivian says, you can move flexibly between inference and learning.

Thousand Brains Project: Thank you, Niels. Agent Rev asks: so far your focus has been mostly on vision, with discussions about touch. In your documentation and some videos, you mentioned eventually dealing with abstract concepts, language, world models, logical reasoning, and more. I suspect this might require more types of sensor and learning modules for other brain regions, such as motion, audio, scene mapping, attention, reasoning, prediction, and so on, along with a distributed associative database of sensory SDRs and a live global workspace to solve the binding problem, all optimized to run a standard workspace. I'm curious how far the theory of columns can be pushed.

Jeff Hawkins: It's a tough question. It relates to the earlier question about whether the column theory can apply to cognitive things. Recently, in the last few months, we've had further insights into this, which I alluded to. I don't want to get into too much detail because it's very speculative. We accept that all columns are reference frame based, and all columns have a reference frame. The question is, what does a reference frame refer to? As we currently understand it, they're very much tied to physical things in the world. Even when you learn mathematics, you do it by looking at images of equations and things. When you learn about history, you look at pictures or images of the world.

An answer that might go beyond what most people can capture right now is that as you go up the cortex, you end up with representations of objects that are very compositional—objects composed of objects—and the power of abstract thinking comes from that, not from the reference frame itself. You might use simple reference frames and movements to learn mathematics and language, but the features being fed into those reference frames are themselves much more abstract. The abstraction comes less from the reference frame and more from the accumulated compositional objects represented in reference frames. I hope that was at least somewhat helpful.

Viviane Clay: I want to emphasize that we don't think we need to build custom learning modules for reasoning, attention, or language. They all receive fundamental sensory input at the lowest level, and as you move higher in the hierarchy, they get more compositional features, like outputs from other learning modules that have learned entire models. Reasoning and prediction are processes happening within each learning module, not specialized to a specific region. All learning modules are doing prediction.

Thousand Brains Project: What's the largest scale Monty has been trained on in terms of data and compute? Do we have confidence it will scale well in performance, and if so, why?

Niels Leadholm: I'm happy to take this one. In our experiments, for example in the paper, 77 objects at 32 rotations is about the most we've looked at—not because we couldn't do more, but because we're focused on showing what Monty can do with small amounts of data, similar to how humans learn. In terms of scaling, there are more details in the paper, but at learning, we could train on 10,000 objects and it would still use fewer flops than a very short deep learning run with the same amount of training data. The difference is huge, and we're not concerned. At inference, it's more subtle, because in the current version of Monty, the amount of flops scales linearly. The current implementation is very naive; we haven't designed it to be computationally efficient beyond some basic optimizations. There's a lot that could be done to make that nonlinear, including sparsity, hierarchy, and model reuse—things we believe the brain does and that we want to implement in Monty. For both learning and inference, the amount of flops is not our current concern; it should come naturally from the system's design.

Jeff Hawkins: I'd like to add a question. We've talked about scaling to the task, but another way of scaling is how many learning modules we've used at once and sensorimotor integration. In the brain, we have 150,000 of these, yet we can do a lot with just one learning module. We've spent a lot of time exploring capabilities, but another question is our confidence in scaling up the number of learning modules. I don't know the answer, so someone else can answer.

Viviane Clay: Concretely, we haven't scaled beyond 16 learning modules, mostly because we haven't done much hierarchy yet. At some point, you can't just add more sensors and still get meaningful input on different parts of the object. Once we add hierarchy, we'll run more experiments with more learning modules. Since Monty is inherently very parallelizable, you can use many CPUs and parallelize all the learning modules, or use custom hardware. For example, Xavier, in one of our university collaborations, is exploring other types of hardware to scale to thousands of learning modules.

Thousand Brains Project: Thank you. Next question. Ray asks: do you use spiking neural networks in Monty? If not, do you think the spiking behavior of biological neurons is relevant to artificial intelligence?

Niels Leadholm: I'm happy to start on this one. For anyone following Jeff's work over the last couple of decades, first at Numenta and now TBP, you'll know there's been various levels of biological realism in the systems being built. We're always trying to figure out the right level of abstraction, including whether to model spiking neurons. Our approach is to find the most abstract level that still captures the key computation we want. For example, with HTM, the neurons found a balance between simple point neurons and overly complex multi-compartmental neurons, focusing on the key role of dendritic branches and predictive states. That's still present in Monty today, even though we don't have neurons in the same way; we still capture prediction following movement. The question is what key things spiking delivers. Sometimes in research meetings, we discuss temporal codes and how spike timing might be important, but much of that is about how the brain does it and doesn't necessarily constrain Monty's implementation. At the moment, we don't see a definite need for spiking, and given the complexity of training spiking neural networks, we want to avoid it if possible. However, we haven't ruled out integrating elements of what spiking neurons do, such as STDP, causal learning, and other key features, into Monty.

Jeff Hawkins: That was a nice explanation, Niels. I think something like STDP, spike timing-dependent plasticity, may be critical, but you can achieve that without spikes. It's really about capturing temporal causality. We've only included in the theory what we think is absolutely necessary to support the principles. Vivian often points out that you can build learning modules any way you want as long as you adhere to the cortical messaging protocol. If someone wants to build a learning module based on spiking neurons, they can do that. Maybe it would be more impressive in terms of performance or power reduction, but I don't think it would provide fundamentally new capabilities. It might be a great way to build things if you have the right semiconductor substrate and know how to do it, and you might get tremendous efficiencies. We don't rule it out, but from a principles point of view, it's not essential.

Thousand Brains Project: Thank you. Avinash asks, how soon is Monty expected to be capable of language learning?

Viviane Clay: We have several posts on this topic in our discourse where we go into more depth about how language could be modeled in Monty. One important point is that we don't think language is where a system should start. A baby comes into the world and starts by interacting with the world and learning many things; language is one of the last things to develop, after learning to move, walk, and interact with objects. Once language comes into Monty, following the principle that columns learning models of words have the same basic structure, the idea is that it will emerge naturally from other learning. If you place Monty in an environment where it hears language or reads words, it will learn models of letters, words, and sentences, and associate meaning with words, grounding them in physical reality. But since we haven't scaled Monty or made it hierarchically deep enough for multimodal integration and language associations with meaning, we haven't been able to test this yet.

Jeff Hawkins: To add to that, if you think about language, it's mostly a way of transferring a model I have in my head into your head. We often express language using the same communication protocols as in Monty. For example, "Imagine this, you're at this intersection, there's that store over there, inside you'll find such and such," and you're recreating models from one person's head into another's. Once you realize that, it becomes less mysterious. But it does require existing models, as Vivian said—you don't start with language, you start with models of the world. Once you have those, we can talk about how to communicate them, and I think the process will turn out to be fairly simple.

Thousand Brains Project: Next question. Falco asks, is the hypothesis generating and pruning based on neuroscience?

Ramy Mounir: I'll take this one. We've worked out many details about how a single hypothesis in a column, such as orientation, can transform the movement sent from the thalamus. We think there are hypotheses about these orientations in the column. The question is how many can exist simultaneously in a column. That's more complicated. We've discussed in research meetings whether different populations of these representations can exist and oscillate between them based on phase or oscillations. We could also imagine a fixed-point attractor network where these can compete. It goes back to the abstraction of how biologically plausible we need to be and what we need in our system. We think there are multiple hypotheses. Whether they can all fire together or we oscillate between them is a different question.

Jeff Hawkins: I agree with Rami. In neuroscience, we have a strong hypothesis about one way to keep these hypotheses going, which is a union of sparse representations. You can activate multiple hypotheses in the same set of cells at the same time without confusion. We've shown how that works mathematically, and there's a lot of biological evidence for it. We don't do it that way in Monty, but we could. We don't have to model it that way; we can achieve the same result in other ways. The answer is yes, it's happening in neuroscience and biology. We know some ways it's happening in biology, but we're not emulating those directly; we're just achieving the same result in Monty.

Niels Leadholm: To add to that, Rami showed some recent results where, in the latest version—not necessarily the one in most configs—Monty will expand its hypothesis space dynamically. This is similar to how we think sparsity works in the brain: when there's a prediction error, many neurons become active. Monty does this in the version Rami's been working on—if the system fails to understand what it's seeing, it bursts and expands the hypothesis space. It's another example of something inspired by neuroscience without needing to be implemented exactly as in the brain.

Thousand Brains Project: Thank you. Two more questions. Robin asks, how is the sensor module able to infer curvature? Is it analyzing the 2D pixel data, or does it get depth input from the sensor?

Viviane Clay: I can answer that. It does get depth input.

We use depth input to determine where the sensor is in space. For example, with a camera, the patch on the object is not where the camera is, so we need to know the distance to the object. From the patch and the depth values within that patch, we can calculate curvature and surface normal.

Niels Leadholm: It's worth adding that, right now, if we want to apply Monty in the real world, we're limited to using some sort of depth camera, like time-of-flight. One of the future work items is to use parallax, both motion and binocular, to extract depth, which is more similar to how the brain does it. If you have experience with that and are interested in working on it, that would be great.

Viviane Clay: It also depends on the modality. For the ultrasound project, it wasn't a depth camera but an ultrasound image, which includes information about how far the probe penetrates into what it's scanning. You can write custom sensor modules for any sensor you have, and the modules can extract whatever features you want. The important thing is that sensor modules need to extract where in space they are and what orientation they're sensing, as part of the cortical messaging protocol, so the learning module can infer how it has moved from the previous step.

Thousand Brains Project: Thank you. Last question: do you believe Monty could achieve knowledge generalization and creativity without modeling the hippocampus and neuromodulators as reward signals?

Jeff Hawkins: There are several aspects to this. We believe we have to model the hippocampus, not just for creativity, but also for attention and fast learning that occurs there. The neocortex is thought to be derived from the hippocampus, so there are analogous processes to cortical columns in the hippocampus. We don't need new fundamental features, just new concepts of fast, slow, and temporary learning, which fit within the current framework. Regarding neuromodulators, there must be something that tells the system when to learn, which could be as simple as a switch or value-based learning. Neuromodulators are mostly about deciding when to learn and are involved in understanding causal relationships, like reinforcement learning. We'll model some of that, but generalizing knowledge doesn't strictly require it. The system learns models and can generate knowledge from them, but we will need at least some equivalent to neuromodulators to decide, per application, what the system should care about and how much it should learn. These are application-specific issues, less about the overall theory. It's a complex answer, but it's a great question and something we think about often.