Some of you probably know a lot about grid cells. Hojae, you may know more than I do—my knowledge is a bit old. I haven't read any papers about grid cells in the last few years, maybe three or so, so I'm not up to date on the latest. We were into it pretty heavily for a while back in the day. It's not clear how much of this is relevant or necessary for Monty and the Thousand Brains Project, since we're not planning to emulate or model grid cells at the moment. I've put down some basic points about grid cells—what we know, what we don't, and how we thought about them for the Thousand Brains theory. Some of this will be simple for those already familiar, but it could be challenging if you haven't been exposed to it. I remember it took me a few months of deep confusion before I really understood it. I kept getting confused, but eventually it sunk in—oh, that's how these things work.

If you're like me, it may not settle in on the first couple of exposures. With that, I'll just jump right into it unless someone wants to say something before I start. I agree, it took me a while for it to sink in. It's really odd. I'll get going. I'm going to share my screen and show you a PowerPoint presentation.

There's not a lot of PowerPoint here; really, the best way is to make this interactive if we can. Hopefully you see that. Do you see the list of text? Yes. These are just talking points I wrote down. I have images to go along with these, which I'll show you in a second. Let's start at the top of the list. The idea is that the brain has to have some kind of reference frames for various things. It needs to know where things are in the world. We deduced that each cortical column also had to have some sort of reference frame. This is a pretty solid deduction, especially for somatosensory, for touch. That's the most obvious place—when you touch something, if you're going to have an expectation of what you'll feel, the brain has to know where that finger is, or the patch of skin, or whatever you're touching. It has to know where it is relative to something in the world, like the object you're touching. As you move your finger, it makes new predictions, so it has to keep track of where your finger is. You can't know unless some neurons tell it this. That's a solid deduction.

It was hard to imagine initially how neurons could create reference frames, but then we said, there are these things called grid cells in the entorhinal cortex, which is related to the neocortex. You can follow the end of the neocortex to the very end, and it turns into the entorhinal cortex and hippocampus. A lot was known about grid cells. We didn't know much in the beginning, so we said, obviously, evolution has discovered a way of doing a kind of reference frame using these grid cells. It turns out they're pretty complicated. We guessed that the cortex uses grid cells too, and now we know there are many places in the cortex that have grid cells. We also made an early mistake—the third bullet here—we didn't deeply understand this one either, but columns not only need to know the location of a sensor, they need to know the orientation of the sensor patch. You can touch something with your finger, but you can rotate your finger on that point, and the input from the finger changes. The cortex needs to know not only where the finger is, but its orientation to the thing it's touching. These are similar to grid cells and head direction cells in the hippocampal complex.

We think that cortical columns are doing something similar to what's happening in these other parts of the brain. Grid cells have been heavily studied, and there's a huge amount of literature on them. A lot is known, but many details are still mysterious and debated. No one really understands everything about them, or if they do, I haven't read about it. There's still confusion about how these things come about and how they work. I'll talk about some of the things we don't know. Now, I'll go into grid cell basics, and later I'll come back to some things we don't understand. If there are any questions, just shout out.

One interesting thing to consider is the connection between evolution and how the entorhinal and hippocampal complex likely predates the neocortex and may have served as a blueprint for it. You can imagine how a more primitive animal just needs to have a reference frame for environments. The most important thing is to have a reference frame for where you are in an environment and how you move around it. The idea of having a reference frame as a way of structuring information is powerful and general. What was once a specialized organ for mapping environments could potentially be turned into a more compact, general structure—cortical columns—repeated throughout the cortex. It's speculation, but it's an interesting way to think about how you go from representing environments to using the same concept for all information. There are other things that suggest this as well.

Imagine the cortex as a sheet of cells, often described as six layers. When it reaches the end of its sheet, it becomes less structured and uniform. One of the first things it becomes is the entorhinal cortex, which is roughly three layers thick. It continues and folds over itself, with the hippocampus now on top, like a sheet of paper folded over. Now you have three layers of the hippocampus on top of the three layers of the entorhinal cortex. This is a rough approximation, but it's generally true. The idea is that the three layers of the hippocampus on top of the three layers of the entorhinal cortex, at some point in history, became the six layers of the cortex. There are analogies between the place cells in the hippocampus being the upper layers of the cortical column and the entorhinal cells being the lower layers. I've always kept that in mind. Not only did evolution discover this, but physically there's a suggestion that the cortex became the fusion of these two three-layer structures aligned on top of one another.

Are you going to mention place cells as well? I will in a second. Let's get into that. Starting at the upper left of this picture, the first cell in this collection that was discovered was the place cell. The discovery of place cells led to a Nobel Prize for O'Keefe. The picture on the upper left shows a rat walking around in a square box. The black lines show where the rat was walking, and the red dots indicate where a particular cell in the hippocampus fired. This cell fires when the animal is in a specific location in the field. That was the beginning of the study of place cells. This is just one cell, but it seems to fire regardless of which way the rat is facing. If the rat is in a certain area of the box, the cell fires and marks a place. That was a key discovery, and a lot of research followed.

Later, another team discovered a similar cell in the entorhinal cortex called grid cells. The middle image shows a circular field—a big circular box with rats running around. Again, we're recording from a single cell. The black lines show where the rat was moving, and the red spots show where this particular cell fired. It's consistent: whenever the rat enters these locations, the cell fires, regardless of the direction of movement. It's like a location signal. The heat map shows where the cells were most active. This particular cell fires in many different locations, forming a triangular or hexagonal pattern. That's pretty interesting. Of course, this one cell doesn't tell you where the animal is; if you just look at where the cell is firing, it could be any of these locations. It's not a very good way of locating the animal, but that's what it is. There are many other grid cells, and they vary in a couple of ways. First, there are many grid cells that share the same properties as this one. If you look at the bottom right, you'll see the square labeled "phase."

They're showing the activity of two cells here, one green and one blue. These cells share the same spacing and orientation in their firing patterns but have different phases, meaning they fire at different and sometimes overlapping locations. You can find a whole set of grid cells that tile the space with the same orientation and scale. Whenever the animal is at any location in this environment, there are a few grid cells firing within the same grid cell module. As the animal moves, the specific cells that fire change, but all the cells in the same module have the same orientation and scale.

There are other grid cell modules in the entorhinal cortex. If you move horizontally, you'll find the next grid cell module has a different scale, so the cells fire at different spacings. There are somewhere between five and ten of these modules that can be easily identified. You get larger and larger scales for the grid cell modules. 

Here's an interesting historical anecdote: the people who discovered place cells didn't discover grid cells. O'Keefe wrote about this because they were looking at cells with a much larger scale. The cells they were recording from were actually grid cells, but the scale was so large that the cell only fired in one place in the environment. A rat has grid cells that scale to cover very large areas, maybe 50 meters or so, and if you're looking at those, you won't see that cell fire in multiple locations in a smaller environment like this circular arena. 

At any point in time, they basically have a map of where the larger and smaller scale grid cells are, and that's how they can now identify where they recorded back then. There's a particular direction across the entorhinal cortex where the modules get larger, and this has been mapped out. I'll show a picture of that in a second. This is a little ahead, but there are multiple grid cell modules that increase in scale by a factor of about 1.4 each time—maybe 1.7, I can't remember the exact number. This reminds me of the change in scales in cortical regions like from V1 to V2 and V4, where you see a similar increase in scale. I've always wondered if it's the same process, with the entorhinal cortex having a set of grid cell and place cell pairings that bump up to the next size, much like V1 and V2.

Is the hypothesis that place cells are basically disambiguating the grid cell firings, or what's the interaction between them? No, the basic hypothesis here—heavily oriented towards my view, though there's a lot of literature on this—is that place cells are more sensory driven. They're based on observations the animal makes, such as visual or tactile input, or input from whiskers. Grid cells are driven more by movement. There's an analogy here with the cortical column, which is driven primarily by movement and sensory input, and the lower layers, which are driven primarily by motor path integration. That's a hypothesis I've always adhered to. There's a lot of detailed research about the correlations between grid cells and place cells, and I can't recall all of it. For example, the invited speaker we had a couple of weeks ago, Hagai, looked at how they influence each other bidirectionally. 

A simple way to think about it is that place cells are similar to layer four cells, driven by sensory input, and they're paired with locations in the lower layers, which are more equivalent to grid cells. There's a bidirectional connection between these, and those connections exist between the hippocampus and other regions. It's a very reasonable hypothesis.

There's also an orientation aspect. These cells are spaced in a triangular or hexagonal grid, and there's a change in orientation when you go between grid cell modules. Not only do you have a change in scale, but you can also have a change in orientation, though the orientation only varies slightly. This is suggested in the middle image, but they do change, and you'll see in a moment why that's important.

When an animal enters an environment and recognizes it, the grid cells anchor. Once the animal knows what environment it's in, the grid cell that was active in a particular place before becomes active again. All the grid cell modules anchor as if they were in this place before. If the animal goes into a different environment, the grid cells anchor differently. For example, if there are 50 grid cells in a module, one may be primarily active, a couple may be less active, and the rest are not active. Which cell is active at a given time is a choice the system makes, and that's the anchoring. In a different environment, a different cell becomes active. You could have two circular environments, and if there's a picture on the wall, a rat will see these as different environments, and the grid cells will anchor differently but consistently between the two environments.

Recognizing where the animal is, once the animal recognizes it, the process ends. These grid cells lock in and anchor in the place they were before, establishing the same reference frame as the previous time the animal was in this environment. That is a big part of what it means to recognize where you are. One study shows that when you realize your location, all the grid cells align, and now you have this reference frame and everything can be predicted. Before that, your grid cells might have a different anchoring position, and you don't know where you are or you think you're someplace else.

The place cells are really important in this anchoring process. Sensory input is very important in the anchoring process. It's like sensory input coming into layer 4 of a cortical column, suggesting where you might be in the world, but it's not definitive. Once you've had enough observations, the layer six cells anchor, and you know where you are—on the coffee cup, on the stapler. Sensory inputs are required to anchor the grid cells. The grid cells are not directly sensory driven; they're motor driven. When the animal moves through the environment, the active grid cells change. Even if you turn off the lights and the animal is in the dark, with no input to tell the animal where it is, as the animal moves, the grid cells update. This is the idea of path integration: the animal moves forward in a certain direction, and the grid cells update their internal representation, even without sensory input. It's a noisy process, and after the animal moves a bit, the grid cells are updating, but you can be off. If the animal gets a sensory clue, the grid cells re-anchor properly and settle in the right position again.

You can do this experiment yourself. Get up in the middle of the night when it's completely dark, or put on a blindfold. Say, "I'm at the edge of my bed, I'm going to start moving towards the bathroom." As you walk, there's no sensory input, but you have a sense of where you are in the room: no longer at the corner of the bed, now in the middle of the room, now approaching the wall on the far side. This is path integration. As you approach the wall, you have a lot of uncertainty. You know you're getting closer, but then you stick your hand out and start reaching around to feel where the wall is because you don't want to walk into it. Path integration is not terribly accurate; it's a noisy process, and quickly you accumulate enough error or noise in the signal that you get worried about walking into something. As soon as you touch the wall, all your grid cells re-anchor, and you know exactly where you are.

Any questions on that?

With the different grid cell modules with different scale and orientation, if you map it onto the brain, is it like in one direction the scale changes, and in the orthogonal direction the orientation changes? No. If I recall, and Hojae, if you know this, you should speak up because you have a lot of experience, but the orientation is also something that gets locked in by sensory input. It can vary slightly, so two grid cell modules next to each other do not always have the same relative orientation. The relative orientation can change under different environments.

In the neocortex, is anchoring like identifying the pose of the object you're looking at? Anchoring would be like reaching your hand into a dark box with one of ten objects inside, and you don't know which one you're feeling. You start moving your finger around, and at some point you realize, "That's the computer mouse." At that point, the grid cells in the cortex, according to our theory, anchor correctly for the computer mouse. If you recognize it's the coffee cup, they anchor differently for the coffee cup. If you move on the coffee cup, as soon as you touch the handle, you know how to predict every other point on the mug and identify the cup and its pose.

When you recognize the object, you also recognize your location on the object. These are not two separate things; they go together. It's not, "I'm on the coffee cup, but where am I?" You know where you are and what you are at the same time. For every object, you anchor the grid cell modules differently and have a different location. Once you know what the object is, you anchor the grid cells, and once you anchor the grid cells, you know where you are. They're not two separate activities.

Which relationship is fixed versus not fixed? Is the scale between the different grid scales fixed or does it depend on the situation? You said orientation is not fixed. The orientation can vary, but the scale, I believe, is fixed. Within a grid cell module, the phase of individual cells is fixed as well; there was an experiment that proved that. When you're anchoring, what varies is which cells are currently active at some location in each module and the orientation of that particular module to the object being observed.

This is still not a very good way of determining location. If you think about it, take one grid cell module—you've got a bunch, but if I just look at one grid cell module, it doesn't tell me where I am in the environment. It tells me I'm at one of these locations, but not exactly where. If you start looking at multiple grid cell modules, then you can really narrow down where you are.

Before I get to that, the next slide is from a paper by Tank. I love this image; it shows what grid cells physically look like in the rat. I think it was a rat. This is a slice through the entorhinal cortex, showing three different grid cell modules. One module, with all the colored dots, is called module one. Another module is represented by a green line, and another by a yellow line. These are three modules that change in scale; he's not showing the others.

What does it actually look like in a grid cell module? It's a bit complicated. First, imagine that a cell can fire in multiple places in an environment. This little colored rhombus or square represents all the places that can be represented by a grid cell. If you keep going along the surface, it repeats. A cell might be active in the upper left dark red one, but that same cell would be active in the next dark red one, and so on. This is one non-repeating element, and this shows all the different phases within an element that repeats.

A grid cell module in this case had six of those phaselets, or whatever they're called. These dots are actual individual cells, showing where in space they respond. Imagine I'm a dark red cell here—that's one cell, and it will repeat over and over again as you move. There's another cell over here, a different cell, but it has the same phase as this cell. There are about six cells that all respond in the same location in an environment. As the animal moves through the environment, there's a bump of activity in each of these six phase blocks. There are six copies of the grid cells, so as you move, those lumps move across, sliding into the next one as the animal moves. Eventually, the activity will run off one end, but others will start on the other side, so you always have two or three of these lumps of activity—actually, six in this case, because there's a lump in each one. You wouldn't want just one set of grid cells, because when you run off one end, you have to start on the other. Cells are noisy, so there are six copies of everything going on in one grid cell module. That's just redundancy.

It's what's observed. It is redundancy, but it could be more than redundancy, and we'll get to that in a minute.

In this experiment, it looked redundant. However, there were other interesting observations in this paper suggesting it is more than redundancy. Some of you may remember this. Some of these cells—let's take this cell—when the animal was in a particular environment, some of these cells would never fire. Not all the dark red ones, but this particular one didn't fire when you would expect it to. In other environments, it did fire, and it was consistent for the environment. In environment A, that cell never fired, but in other environments, it did.

This was a clue to me, because if we want to have unique spaces for all the objects we know, there aren't enough grid cell modules to do that. We'll walk through it in a moment, but we need more than five or ten grid cell modules to create large spaces for all the things a column might know. There needs to be another layer of encoding, and it almost certainly exists. We have speculations about what it is. This was a clue, because the experiment was done in a very thin layer of cells—about 10 microns thick. You're just getting one thin layer, but there's depth as well. The thought was that as you go deeper into the tissue, you'll find sparser codings of the same grid cells, allowing representation of much larger spaces. That was speculation, but there was one empirical clue: he found cells that were consistently inactive when they should have been active in particular environments. That suggested there's some other encoding going on, making the system more powerful.

This is a really complex topic, so I'll keep going and can go as deep as you want about these questions. This figure is from our paper, which we call the frameworks paper, and it shows a couple of things. First, in figure A, this is your classic single cell grid cell module. We've been talking about that, and we put a little rat here to show that as the rat moves around the box, at different points, this cell fires. You can see it there. In figure B, it shows two cells in the same grid cell module that are just shifted by phase—a blue cell and a brown cell—but the same basic idea as before. Of course, there would be a set of cells, and everywhere in this environment, some cells from this grid cell would be firing. We're only showing two.

Now, in figure C, there are two grid cell modules shown, one in brown or red and one in green. These grid cell modules have different orientation and different spacing or scale. If I look at one grid cell module, it doesn't tell me too much about where I am—some, but not too much. The same goes for the green one. But if I overlay the two and look at cells in both, then at any point in time, there will be one cell in the upper grid cell module and one cell in the lower grid cell module that are active coincidentally. That is a much more unique location than in each individual grid cell. There is always some cell in the upper grid cell module active, and always some cell in the lower grid cell module active. It's just which two cells are active at the same time as you move about is quite unique. If I move around in this environment, it won't repeat in the same hexagonal pattern as before. These coincidences will occur much less often. This is the basic idea: you can take multiple grid cell modules, and if you look at the active cells in each one, they don't tell you much individually, but together they create a much more unique location.

Now imagine having 20 or 30 grid cell modules, each with 30 cells representing different locations, so the tiling is done by 30 cells in each grid cell module. If you want to know the size of the space it could represent, it's basically 30 to the 20th power. You can take one of the 30 cells in a grid cell module, one in another, and so on, and anchor all those at the same time. It's essentially 30 to the 20th different locations you can represent, which is a very large number. If you have multiple grid cell modules, all slightly different—either in orientation, phase, or scale—you can represent a very large space by looking at multiple cells across the grid cell modules.

This is why I was asking how the orientation change is distributed over the brain area. Over how big of an area would a coincidence detector need to integrate this information? If the scale changes in one direction and then orientation, I'm not sure how this is distributed. Do we need long-range connections to detect these coincidences? You can look at this figure here and see the scale bar—120 microns.

In this case, this is the entorhinal cortex. If you wanted to integrate 20 different grid cell modules, which I don't think exist, you might have to go over a couple of millimeters, which is a lot. I don't think that's what's happening.

You can almost think of this grid cell module as the base of a proto-column in the entorhinal cortex. Maybe it's like half a millimeter by half a millimeter. We originally said, wouldn't it be great if we could just integrate across all these grid cell modules scaling across the surface of the entorhinal cortex? There aren't enough grid cell modules to do that, and it's too long a distance. There has to be more to it, and there is. There's a lot more going on than I've just shown here. What I'm showing is what experimentalists have shown initially, but it gets more complicated. I actually believe the higher encoding we're talking about, when we talk about locking in on a particular part of space, is not even represented by these grid cells. It's represented by something else. I can get into it in a bit—maybe I should wait until the end because it's pretty speculative.

We can talk about it at the end. Just remember it for later. This is the basic idea, and we didn't come up with this idea for how you could create these large spaces. We really liked it and hoped we'd find enough grid cell modules to make it work. As more research came out, it didn't look like there were enough grid cell modules—maybe five to eight of them—and that's not really enough.

but that was what was in our paper.

Quick question. So about this, isn't there actually more locations that can be encoded? In A, the firing field of a single grid cell in a module—each one of those, the overlap in D—it's actually two active cells overlapping, based on the previous picture. It's not really SDRs; it's just two active cells, one from module C (the red one in C, the upper one) and one from the green one. These are just two individual cells firing at the same time, and when those two cells fire together, it's just a coincidence of two cells. We actually need a lot more than that. We need at least 20 active cells to detect a coincidence, because some neuron is going to do this.

So one of the dots here corresponds to just one of the dots on the previous picture. Got it. Understood. Thank you.

On D, we're looking at the activity of two individual cells and where those cells become active. As the animal moves in this box, the green cell becomes active in the green points and the red cell in the red points. In one location, the green and red cell are active at the same time. Individually, the green and red cells don't tell you much because they're repeating over the environment, but together, you know exactly where you are, because that's the only place where the green and red run at the same time. At any point in the box, there will be one red cell and one green cell active, so there will always be a coincidence between one cell in module A and one in module 2. Looking at these two grid cell modules, you can tell where you are in this box much more precisely than by looking at the individual module.

So what do you mean when you said there's not enough modules? Not enough for what? First of all, you don't want anything to rely on just two cells. If I'm going to detect a pattern among a bunch of neurons, I want at least 20 cells active, so a dendrite or a cell could recognize a pattern. Imagine these grid cells are in layer six in our cortex and we're sending them up to layer four. I want to be able to say, this sensory input is at this location on the object. I need enough cells to do that; I can't recognize the location with just two cells. Cells are not always reliable—they die, neurons aren't good at recognizing unless they have 15 active synapses at once. So I need a bunch of grid cell modules, or something like them, to recognize the location.

With the number of grid cell modules we actually observe—I'm sorry, I think it's eight, but I'm not sure—that doesn't create a very large space. Imagine I said, it's 30 to the eighth power. That's not that big. I'd like a bigger space, unique for all objects. I want a very unique space so I'll never get confused; when I have a coincidence of cells, it's going to be, this object on this location.

I really want more modules, more cells active to recognize it, and more cells to guarantee a unique location in the world. Just to clarify, the redundancy you were showing with the tank paper would help slightly with the first problem, in terms of the number of cells, but it's still the issue that you have to integrate over a huge area. Good point, Niels. When we wrote this paper, we didn't know about the six phaselets—that was a new discovery the tank came up with. So we might have six cells active in each grid cell module, but that doesn't really make the space any larger. If I could selectively deactivate individual grid cells under different contexts, then I'd have a much larger space—this cell only fires in certain locations. There's a lot of confusion about how this actually works.

We argued in this paper that a rat moving in two rooms—both rectangular, but one with a blue object on the wall and one with a green object—will see these as two different rooms. The same locations in these rooms will be encoded completely differently. Here's location A, B, and C, and in a different room you'd have a different set of cells active, D, E, and F. All the locations in this room will have completely different encodings, even though the two rooms are essentially the same size. This also illustrates a point of path integration: if an animal goes from A to C, or indirectly from A to B to C, it doesn't matter—whenever you're in C, you have the same encoding for that location. It doesn't matter how you got there.

We said, this is what's going on in the cortex, but let's imagine your finger is now the mouse, and the finger is moving around in the world, just like the mouse is moving around in the boxes. You can talk about points and objects—this point here, we'll label it X, another point Y, another point Z, very equivalent to A, B, and C. Here's a different object and different points, and you can path integrate as you move from V to T, passing through other points like W. It doesn't matter how you move from V to T; any direction, you'll end up with the same encoding once you get up here. That's the whole path integration component.

This is the basic idea, the foundational principle of the thousand brains theory: moving our sensors through the world—a sensor patch—is very much like a rat moving around in an environment. It's the same idea, the same basic principle, the same neural mechanisms underlying it. Just like there are head direction cells that indicate which way the rat is facing in this room, we have orientation cells that represent which way the sensor patch is rotated.

on the object. That was the frameworks paper, which was the first time we really spelled this all out completely. I debated whether to talk about this topic because it gets really complicated, but it's a big part of the literature, so I thought I'd mention it. If you don't follow it all, that's okay too. There's a question about how grid cells work. How do they actually path integrate? How does the animal moving in the dark cause the cell to change and indicate a new location? What is doing that? It's not any kind of input from the sensors. There's something internal as the rat moves that causes grid cells to change.

This figure is trying to show how a single grid cell, moving in one direction, would become active, then inactive, then active, then inactive. If you're moving across an environment, like in the bottom right here, as the animal moves, the cell becomes active, inactive, active, inactive, active. Even if the animal moves in a strange direction, it would be active and then active again. People wonder how this could be, and there are a number of things that point to this particular mechanism. It's really simple and beautiful, which is why I wanted to include it.

There are basically two frequencies involved. One is the baseline frequency, which is constantly present. This is the theta frequency in the entorhinal cortex, around 8 or 10 hertz. Then there's another frequency, shown as red and green in the figure. The green one is the VCO, which stands for velocity controlled oscillator. This frequency varies based on the velocity of the animal. There are cells that represent how fast an animal is moving, and if you create another oscillator that speeds up or slows down based on the animal's velocity in a particular direction, it achieves the desired result.

These two oscillators are added together, resulting in a carrier wave, shown as the purple one at the top. At some points, they're aligned and you get more activity; at other points, they're antiphase and you get no activity. Imagine you're a single cell looking at these two frequencies. They don't even have to be on synapses; they could just be basic background frequencies. When these two frequencies align, you're much more likely to become active; when they're disaligned, you're less likely to become active. If the animal is not moving, the two frequencies are exactly the same and your activity won't change. If you're active, you'll stay active. But as soon as the animal starts moving, the frequency of the green one shifts, and your activity will change over time. The faster the animal moves, the bigger the frequency shift on the green one, and the more rapidly your activity will change from active to inactive. Essentially, you're taking a velocity controlled oscillator and turning it into an on and off signal, which corresponds to how far the animal has moved in a particular direction.

There are a couple of theories about how grid cells work, but this is by far the better one and almost certainly true.

An interesting observation comes from this. When the green and red frequencies have their maximum sum, the position relative to the baseline frequency changes. You can see in the vertical lines that the peak shifts relative to the baseline frequency. This is a natural result of oscillatory interference: the actual peak shifts relative to the peak of the baseline frequency. As the animal moves through the environment, both for grid cells and place cells, the cell starts to fire in advance of when the animal actually reaches the preferred location. This is called phase precession. It starts on the far side of the peak, then as you pass through, it's right on the peak, and as you go beyond, it's to the left of the peak. There is additional information here about where the animal is, as these prior peaks are shifted in phase relative to the baseline. This is observed and is an important clue that this is what's happening. It's a simple mechanism: if you can get movement and turn it into a frequency shift, and compare it to a base frequency, you can get grid cell activity.

This shows one cell going in one direction only, but we can go in multiple directions. The basic idea is you need multiple velocity controlled oscillators in different directions. That's what's represented by the lower ones in B. A cell actually looks at not just one green velocity controlled oscillator, but multiple ones in different directions. If you sum all these up, you'll get the grid cell pattern and the activity of a cell. This means there have to be multiple cells representing activity in different directions, and they should control velocity controlled oscillators in those directions.

Scientists have looked for them, and there are two basic hypotheses. The one I prefer is that these correspond to mini columns in the lower layers, where the mini columns are directionally sensitive. The cells in the mini column are directionally sensitive, and to get this to work for one grid cell, you need multiple velocity-controlled oscillators with different peaks in each direction. Imagine a mini column with ten cells, all responding to movement in one direction. This is observed in areas like V1 and V2. You need a set of cells, each shifted in phase from each other, all using the same velocity-controlled oscillator. One hypothesis, which we've never written down, is that within a mini column in the lower layers, all these cells respond to movement in the same direction but are offset by phase, providing the input needed to create grid cells.

Mechanistically, it seems straightforward to generalize this to three-dimensional spaces, but that's not really observed. Do you have any idea why not? It seems easy to generalize. I thought the same thing—maybe the entorhinal cortex isn't good at three-dimensional spaces because it didn't evolve for that, but in the cortex, why not? Why couldn't every mini column represent a direction in three-dimensional space?

Now you have a basis set of movement vectors in three or n dimensions, and whatever they represent, that's what you would do. I agree.

The question is, where do these directionally sensitive mini columns come from? Are they hard coded? That's debatable. I like to think they're not hard coded, but learned.

I've argued that the retina has all the information needed to determine movement direction—forward, backward, left, right, up, down. The simplest explanation is that the retina sends magnocellular cells, which are just on-off cells, and the cortex turns them into mini columns representing movement directions. If you run these magnocellular on-off cells through a spatial pooler, you get multidimensional representations of direction. Whatever the data contains from the retina—if it's three-dimensional, it represents three dimensions; if two, it represents two. A spatial pooler would do that. If the system learns and the information comes in as we think, I don't see why it wouldn't represent three dimensions.

If the data was n-dimensional, it would represent n dimensions, but we typically don't get that from our retina.

In the grid cell literature, they've done experiments to see if grid cells can represent three-dimensional space. Rats climbed on lattices in three dimensions, but results weren't definitive. Then they did experiments with bats, recording from cells in the bat's entorhinal cortex as it flew through long tunnels. They could see where in three-dimensional space the bat was. They asked if grid cells represent three-dimensional space. What they found was that grid cells work as expected based on ground landmarks, representing a two-dimensional map of the surface while the bat is flying. Grid cells do become active in vertical dimensions, but they lose their normal grittiness. They no longer act regularly—a cell active at some point in space no longer has the same scale, phase, or orientation. It's wonky. Grid cells still become active in three dimensions, but the third dimension disrupts them, so no one knows what to make of that. I still believe that in the cortex it can be three dimensions, and I don't see any problem with it in terms of Monty. Even if it's not that way in the brain, I don't see why Monty has to stick to the historical artifact of 2D.

Oscillatory interference is one theory. Another is continuous attractor networks, which try to recreate the same basic behavior of grid cells but require too many neurons. There's not enough neurons to make it work. Someone argued in a paper that both oscillatory interference and continuous attractor networks are active, and I think that's correct. In this system, when the animal stops moving, the velocity-controlled oscillators stop firing, so there's no persistence of memory for location. This works for path integration while moving, but when the animal stops, these signals disappear and you might not remember where you are. You need an attractor network to lock in cell activity even when the animal isn't moving. The argument is that a combination of attractor networks for permanence and oscillatory interference for path integration is the right answer, and I think that's probably correct.

It's all complicated. Now I've gone through most of my material. Go ahead, you have a question? Probably just wild speculation, asking you to speculate. So, looking at this representation, it seems like you can create a grid in n dimensions, like you were saying. Is that a path to representing any abstract knowledge spaces?

Yes, and there's a fair amount of research on this. First of all, the entorhinal cortex is known to represent all kinds of things, not just spatial location, but general maps. Remember, the hippocampal complex, which includes the hippocampus and the entorhinal cortex, is where we store our short-term memories, including episodic memories. It's not just about where you are, but also what you did, what you had for breakfast, and so on. All these things are stored in these same cells. Research shows that grid cells and place cells can encode all kinds of information, and even the same exact cells can encode different types of things. The basic belief is that this representation of space underlies all kinds of information—both time and space, and various abstract types of data. I don't know the current research in detail; it's a little confusing, but this is clearly happening.

I've speculated that in the cortex, when we try to understand abstract spaces, the main thing we need to explain is the equivalent of the velocity-controlled oscillator. What is movement when you're thinking about politics? What is movement when you're thinking about mathematics? You're not physically moving something, but you need to store information at different locations, so you have to do some sort of path integration. It's very confusing. I've argued, and wrote in my latest book, that you might think of a mathematical operator as a type of movement. When I perform a transform or add a property, that's like a movement, and by performing some operation, you're moving to a different space and storing locations in different spaces. It's not clear exactly how this occurs, but it's clear that this mechanism underlies it all. I think it's extremely likely that the same mechanism underlies all kinds of thinking processes. It's a bit of a puzzle how it does that, but it's doing it. There's no other mechanism anywhere. You look at cortex, it's cortex. It doesn't look like it's going to be different anywhere else. We know the entorhinal cortex does multiple things, so it clearly can. It almost seems that even with abstract concepts and spaces, we try to map them onto a three-dimensional space. Even when we think about math, we usually try to visualize and plot it somehow, like in 3D. It's really hard to think about concepts in higher than three-dimensional spaces. Even in language, we use metaphors that refer to spatial navigation and movement.

It's clear the brain wants to use this mechanism because that's all it has, and it works to some extent for everything humans do. It's interesting to speculate about transformer networks and large language models—they're so good at language, sometimes better than humans. Why would that be? It might be because language is a poor fit for this mechanism.

This mechanism evolved to explore environments and objects, and we're shoehorning language onto it. We're proud of it as humans, but maybe the reason that ChatGPT or large language models are better at modeling language is because that's what they're designed for. They would do terribly at tasks like recognizing how to fill a coffee cup—they can't do that at all. But if you design an algorithm just for language, it can be better than us because we're fitting language into this mechanism, and that's why math is hard for us. Calculators have no trouble with math, but we do, because we're fitting everything into this system. This is not bad news for the Thousand Brains Project, because the bottom line is that we want to create intelligent machines that interact with the world. If you're going to create intelligent machines that interact with the world, this is the way to do it. You don't want to use a large language model or transformers.

Our project, the Thousand Brains Project, is really biased towards robotics and physical manipulation of the world. It can do other things too, but it may never equal the language abilities of ChatGPT.

But it will understand what a cat is, what it feels like, how it sounds, and have deep, real knowledge of the world as opposed to just knowledge through correlations. Okay, let's see how we did on my list. I just want to point out a few things. In the unknown section, we just talked about whether these are 2D, 3D, or n-dimensional. I'd like to point out that in the entorhinal cortex, you don't find just grid cells, place cells, and head direction cells. The majority of cells are combinations of these things. I think the word is conjunctive. I have to look that up, but I think that's the word for it.

Most cells are somewhat like grid cells and also somewhat like head direction cells and other types. It's not just a huge mixture of in-between types. There are also cells that, instead of being just place cells, are object-oriented, like object vector cells and border cells—cells that respond to particular things in environments. It's a messy mix. Scientists often look for the "griddiest" grid cells for research because the others are part grid cell, part direction cell, and so on. The picture I showed is misleading; there are many cells in between these types. I also mentioned in the tank paper that sometimes grid cells don't behave as expected. There could be many of those.

The interaction between place cells and grid cells in the hippocampal complex is very much analogous to the layer 4 to layer 6A interaction in the Thousand Brains theory. We speculated that layer 6B is orientation, similar to head direction cells in the entorhinal and hippocampal complex. This is a strong analogy and seems almost certainly correct. That's all I was going to present about grid cells at the end of my talk.

Question about conjunctive cells: as you said, there's a mess—some do a bit of everything. Is that a biological constraint? Do they grow that way, or is it exposure to the environment and they settle? I don't know. I assume it's part of the mechanism.

Grid cells on their own are not really sufficient for representing the spaces we need. There just aren't enough grid cell modules. If a grid cell module is equivalent to what you get in one cortical column, there aren't enough grid cells to make the space.

We know there have to be cells that are velocity and directionally sensitive, phase-shifted cells. We've observed these conjunctive cells, which combine gridness and direction. It seems clear that when we recognize a location in space, we're not just looking at grid cells but at all these other cells. Layer 4 isn't just looking at the output of the equivalent grid cells in layer 6; it's looking at all these cells in layer 6. There are many other cells doing different things, and the combination is unique.

I don't have a better answer right now. I think it's part of the mechanism. What they represent will be learned, but the existence of direction cells, grid cells, and their combinations is likely part of the evolutionary mechanism.

Regarding encoding unique locations in cortex, given the limitations on the number of cells: imagine these as cells in individual mini columns, directionally sensitive. In a minicolumn, you need multiple cells at different phase shifts to get a complete grid cell module—cells at all different phases of the theta cycle. You might have 10 or 20 cells firing at the same velocity-controlled oscillator frequency, but shifted from each other. Looking at all the cells in the minicolumns, which represent different directions of motion, some minicolumns are active and some aren't. If you look at all the cells in those minicolumns, all phase-shifted, you get a unique coding for location.

For example, if a minicolumn is half a millimeter by half a millimeter, you might have 100 to 120 minicolumns. Each minicolumn may have 20 cells. Maybe a third of those minicolumns are active—say, 40—so 40 times 20 is 800 active cells at that moment, representing a unique location. If you look at which cells are in phase in these different minicolumns, that's the information needed to update your position and direction. Those 800 cells, if you look at which are in phase at any time, uniquely code a space and direction—a highly unique location in a particular object and direction of movement. The information is encoded there, though not in the form we might want.

All those minicolumns can anchor differently; which cell is currently in phase with the baseline frequency is the anchoring process. With 40 active minicolumns and 20 cells each, you have 20 to the 40th power of unique codings at any time. The information is there. How it gets extracted is another problem. When you stop moving, that information would be unique, plus direction. Sometimes you don't want it to be direction sensitive. If we wanted, we could brainstorm exactly what's required to get the information we need. But my point is, there is enough information in these cells to encode exactly where I am on a particular object, with very high spatial dimensionality.

The cells and mini columns required to generate grid cells would have the necessary information. It may not be in the form we want, but it's there.

We're not really combining all these grid cell modules; we're actually combining the information across multiple mini columns. There are at least 100 mini columns in a small cortical column, so that's enough.

I think it would be interesting to talk more in detail at some point. I'd also be curious to better understand how we could do path integration in these n dimensions without having to create the n-dimensional grid cell. If these are unidirectional, then even in the simple case—if you move right and then move back—you won't have the same result. That's why I said the information is there, but it's not in the form you want, because those mini columns would be directionally sensitive. The hand-wavy argument I have near Niels is this: what we actually see is grid cells, the output of the continuous attractor network. It's the memory of where we are at some point in time that persists even when I stop moving, but the actual encoding of space is not in the grid cells. If I move in one direction and then try to move back, I have a completely different set of cells active in the mini columns because different sets of mini columns are active. What's required is that we anchor the new reverse set of mini columns appropriately. I have to have memory of what that should have been. In some hand-wavy form, I would say the grid cells are how we say, "Okay, we're here right now, we're going to lock down." If I start moving in another direction, I have to reinstate the active cells in the mini columns, in the basic phases. It's a combination of these two things: a permanent memory, or memory that stays active and allows me to reinstate the phases needed for path integration. It's a combination of oscillatory interference, which is very direction-specific, and the grid cells, which are more like a continuous attractor network. This is all very hand-wavy, but what I feel good about, Niels, is that I think the information is there. If I couldn't figure out how this could possibly be represented, I'd be nervous, but it's there, and these things are all observed. This phase precession is observed, and these velocity-controlled oscillators are observed.

Grid cells must happen inside a mini, inside a cortical column, so the information is there somehow. Even if we don't understand all the details yet, I'm sure it's understandable somehow.

I have a question about velocity. Velocity itself is a calculated quantity; it's not a primary signal. If you're moving in the dark, you can't see or feel how fast you're moving, so how does that come about? I don't know the answer. I would suspect initially this might have been hardwired from an animal, like a rat moving in an environment. It could be hard coded, somehow subcortical. It's clearly there. I don't know how the neurons calculate it, but when the animal starts walking, these things change, so there has to be some way of doing that that doesn't rely on sensory input. I'm not worried about that because it could be a hard-coded evolutionary thing. It's just calculated from the subcortical motor system, right? Subcortical motor system—I don't know how it does it.

I think in the MEC, besides grid cells, place cells, and head direction cells, some people say there are also speed cells, and maybe it might be related to that. There aren't, that's what it is, but they're there. The question is, how do those speed cells know how fast you're moving? That's a research question for somebody. My argument is it's not a generic algorithm; it's probably a hard-coded thing that animals had to evolve at some point. You can imagine a little animal walking in one direction, and there's a signal that says, "Move your legs," and move them faster or slower, and that signal becomes the velocity. I don't know; it doesn't bother me.

For machine dead reckoning, it's typically gyroscopes, which are internal only. For humans, it could be the vestibular system as well, so as you're moving forward, that gives you an internal gyroscope. I'm reminded of my vestibular system after I had my bike accident, because it got messed up, and I had some bad vertigo. But that's an accelerometer, not a velocimeter. You could use that to set a constant speed and then change it—who knows? It must be learned, at least in some cases, like when you start driving a car, you have a model of velocity even though we couldn't possibly have evolved that. Good point. You could fool people; you only detect acceleration with your vestibular system. It's very easy to fool people. In theaters with moving seats, you can accelerate someone quickly and then slow it down, and they still perceive that they're moving. You can fool the body into thinking you're still moving based on just acceleration.

There are many ways to approach this. I find it interesting to think about the retina, because it's clear that we get all this information from it. If you imagine watching someone play a first-person video game, you know how that person is moving—forward, backward, jumping, turning their head, ducking—all from the visual signal. As they're walking forward, your grid cells are updating, even though you're not controlling anything; it's all coming from the visual signal.

As far as we know, the visual signal from the retina consists of on-off cells, able to take patterns changing on the retina and see which ones are turning on and off, and the relative distribution across the space of the retina. This is turned into a clear signal: you're moving forward, backward, turning left or right, looking up and down. All that information has to be decoded from these on-off signals, and almost certainly it's learned in the cortex. It's fascinating that you can determine your movement just from these bits turning on and off in your visual space.

I think of movement as not just involving grid cells and place cells, but also all other sensory information—what Tristan mentioned about the vestibular system, and what you just mentioned, optical flow information. Even proprioception and possibly signals we send to our muscles can tell us something, though I'm not sure about that one.

Just a note on the vestibular system: it is awful at estimation. The first thing you learn for flying is not to rely on your vestibular system and to ignore it completely, because it will crash you. If you're truly isolated and just have your vestibular system, it's terrible at figuring out what's actually going on.

I'm trying to find our paper that we've been working on. I think if you go on TBT paper drafts—oh, here we go.

I want to show you this figure that's compiling it. It looked like you had it open as a PDF, too. Oh, did I? I looked for it, I didn't see it. TBT paper drafts 15. Oh, right there. You're so much more observant than I am.

Thank you. What would I do without you? I'm going to show you this figure in here.

Come on, fasten that.

Oh, did I do it? No, keep going here.

Here, this figure.

and this, and then zoom this in. You can see this, right? Thank you. This is the figure from this paper. Notice here, we're showing a V1 column with two inputs: the green one is the sensory feature, and the other is a motor signal updating the grid cells. There are multiple sources for that. One is from the retina itself—this large red arc suggests these are the magnocellular cells detecting motion on the retina, which is ground truth. The eye movement ground truth comes from the magnocellular cells. We also know that projecting to the exact same cells in the thalamus, the superior colliculus projects to the same cells as the magnocellular cells. The relay cells in the thalamus get input from the retina, the superior colliculus, and the vestibular system. These are known connections; all are valid sources of movement data. The vestibular system projects to the LGN in the thalamus and, given your head movement, indicates you're changing the orientation of your head. When I had my accident, I messed up my vestibular system. This happens to anyone who gets dizzy—the vestibular system sends incorrect signals, saying you're turning your head when you're not, and the thalamus tries to correct that orientation, rotating the signal so it looks like the world is rotating. The world starts spinning because the thalamus is trying to compensate for head movement that isn't occurring. There are many different sources that can provide movement data. If you look at the inputs to the relay cells in the thalamus, you can see the different sources, including the motor efference copy from the motor output, which comes from the superior colliculus. This also becomes an input to these relay cells in the next columns. This column can say, "I'm telling you to move the eyes," and it takes that information and assumes you're moving the eyes. The point is, there are many sources of movement data.

When it comes to the Thousand Brains Project, we have to be open to new ideas and things we can create. We don't have to stick to any of this if we can invent new mechanisms. There are also direction-selective retinal ganglion cells that project to the shell LGN, which project to layer 2/3. I had read for years that all the cells from the retina were center-surround cells, but in rodents, they're directionally selective. They have both types. Rodents do. Do humans have any directionally sensitive cells? I'm not aware of that. That's a good question. I don't know if they've been able to investigate that. It's not just humans; it's also monkeys and other primates. This is interesting because you would think the retina would just tell you the direction it's moving, so the cells coming off the back of the retina would be directionally sensitive. We see that in simple mammals like rats and mice, but as far as I know, we don't see directionally sensitive cells coming off the primate retina. This is really cool because it relates to how many dimensions we can deal with. In us and other primates, determining which direction the eye is moving isn't determined by the eye itself; it has to be calculated in the cortex. The ganglion cells in the retina and the relay cells in the thalamus are not directionally sensitive, but as soon as you get to the cortex, they are. This means the cortex takes raw information and determines the possible movement vectors, whereas in the rat, it's more hard-coded—the retina tells you the movement vectors. This could be related to the fact that rodents rely more on the superior colliculus, so it's more beneficial for them to perform some direction-selective processing earlier in the pathway. They also have a shallower visual hierarchy, compressing everything.

We go from the idea that this system started with navigation on the surface of the ocean or land, a two-dimensional system designed for getting to shelter, finding food, and returning. Over time, it became more general purpose. This is one of the things that makes it more general purpose. In humans, we don't make assumptions about what that motion actually is. This also relates to the idea of abstract thinking.

In primates, there is a more primitive signal—the center-surround signal—that is decoded in the cortex into the actual movement directions. One could imagine if you lived in a four-dimensional world and the retina viewed it, the column would learn to represent those four dimensions, because it makes no assumptions about the space. It starts with a bunch of on-off, center-surround bits, and deduces from that the different directions of movement. We see this evolution from simple animals to more sophisticated mammals, where more general-purpose principles emerge.

It's fascinating. These presentations always make me appreciate how amazing the project is—gathering all this information and making sense of it, all within a single column.

It's amazing. I love that constraint. It's going to have to happen; it's there, and we can find it. These things all exist. I want to stress again that while we want to understand this, we also have to build systems that work on whatever principles are practical. We don't have to emulate all of this.

Everyone understands this pretty well, but it's worth repeating. I don't want anyone to think it's so complicated that we can't do it. We don't have to do all of this; this is just how the neurons do it.

Thanks a lot for presenting this. This was a great discussion and a really nice overview. I was worried it might be a little redundant for some people, but maybe not. Every time we revisit this, I learn something new. It was fun.