Hi everybody. Welcome to the Wednesday research meeting. Today we're going to be talking about morphological structure in the context of everything else we've been working on. I don't have many slides, but I'll lay out the basic bullet points in PowerPoint.

Did you present something? Independently, I just took your bullet points and put them on slides, but that's it. We don't need to do that. I might as well just open the Word document. That's what I was thinking about doing. Do you want to open the Word document next? You can open it, or maybe I should, because then I can scroll through it easier. But I don't mind starting off with your bullet point slides to see what you thought. We'll see how easy this is.

At least I get to shut this off again. That's too bad. Stay tuned for more Thousand Brains Mugs and Monty.

These are the main line items, which we'll get into. I started trying to break them down a bit, but I think we'll just automatically get into them. Morphological models themselves: objects in the world have a shape. Deformations can feel overloaded sometimes—talking about some deformations as variations of objects or deformations as behavioral, like in-time deformations. We can work out how to not confuse ourselves. Permanence: I'm thinking in two ways. Things in the world can be temporarily or permanently arranged the way they are, or they can be temporarily stored in memory versus permanently stored in memory. These are two related but slightly different things. Objects that are composed or nested in some way. And, as we've been talking about a lot, objects that change within their own reference frame, and spatial and temporal structure.

I don't need to take any more time on bullet points. I'll throw this over to Jeff.

As I suggested in my Slack post, I felt like our world is getting very complex in terms of our models, and sometimes it wasn't really clear what was going on. Sometimes I forget things and have to go back—oh, we talked about this, etc. When things get complex, it's sometimes a good idea to go back and review everything. I started thinking: we're asking this learning module, a cortical column, to learn models of the world. We've got a lot of different types and flavors of those models. I started thinking about all this structure in the world, and I'll try to enumerate or spell out the different types of structure that we know exist and that we know we can learn. I thought that would be a useful exercise. In the process, I was hoping some clarity would come out of it—a unified theory of structure and modeling. Science often does that: you start with a bunch of confusing things, and then it solidifies and gets less confusing. I can't say the exercise was successful in that; I didn't get through everything and I was still confused as I was writing it up. But I do think it was useful, and I think it's useful to go through it as a team, to discuss it, and for people to propose if I've missed something. Scott, you pointed out that permanence can apply to both the permanence in the world or the permanence of the model, and I hadn't made that distinction. That was interesting and useful.

I started out thinking along the lines of just thinking about the world: what are the structures in the world? It wasn't a taxonomy of models; it was a taxonomy of structure. But it was hard to keep the two separate, because I just kept sliding back and forth.

Maybe we can just go through it item by item, like you started to do, Scott, and then we can discuss them. Asking questions or proposing alternate ways of thinking about it would be useful. I think this is useful for me, and I hope it is for everyone. I don't want to take everyone's time, but I thought it would still be worthwhile going through today, even though I didn't complete the exercise.

Is that okay? Does that make sense? Any questions? I think what you wrote already sparked some discussion, or at least things to say in Excel. Let's do this.

Should I just share my screen? Sounds good. I'll do that. I'm looking for the share button and I'm going to share that screen right there.

I'm sharing a screen, but it's showing me the wrong thing. We're sharing. We can see your screen. Oh, you can? I see. So I'm not seeing what you're seeing. Why is that? It's just a bit zoomed out.

We might be in full screen somewhere. It looks like I'm using two screens, so let me just bring this back over here. Maybe that'll help.

I won't see what you're seeing.

Okay. That looks more like what I expect. You're seeing my Word file here, right? Now, I'll scroll up. It's more zoomed in. It looks good on my screen. Is it a good size for everyone? Should I zoom in?

The beginning of this document was a preamble of things we already know. I don't know why I wrote it down again, just to complete this. Setting the stage here: the whole idea of a brain or intelligence system is that it's taking advantage of some kind of structured universe. The goal is to learn the structure of the universe or the world it inhabits, build an internal model, and use that model to recognize where you are and your situation. You have models of how things typically behave, and you use that to guide your actions. I didn't even talk about the acting here; I just said, how do we build models of the world? That's the most important thing. When I think about deep learning, I always wonder what kind of model it builds, or with large language models, what kind of structure can it learn. It's a different type of structure; it's not the kind we learn. It's a linear structure of tokens in some sense. We, of course, learn the structure of the physical world by movement. I talked a bit about sensorimotor learning, which we all know now.

Then I came up with a list of ideas. When I started writing, I started with the easy ones, like composition, because we've talked about that a lot. Then I decided to go back and start with the hard ones, the ones that are most difficult and not well understood. That was just a rough process, so I moved morphological and deformations up to the top.

The morphological model was actually really tricky for us to figure out. I don't remember if we did it as this team or earlier, but we were really confused in the beginning. We had this idea that models are just objects composed of other things at certain locations or poses relative to a reference frame. Yet, we kept bumping into issues, like with a line drawing—how does that work? I didn't enumerate all the issues here, but we ended up going to something unexpected: the base model that we learn, or that the Thousand Brains Project should learn, is morphological. They don't really define what the actual features are; it's just that the relative position and orientation of the features is the first level of description of an object. That would mean if I'm looking at a visual object, it would be the edges, but it wouldn't include more qualitative things like color. It wouldn't easily handle something like loading the logo on a coffee cup. It would be more like, there's an object on the coffee cup at some orientation, but the model itself doesn't say exactly what it is.

Of course, we do have models of details with qualitative features, and we have a good model for how this works in the brain. We have the minicolumns, which represent the orientation of the feature, and then picking individual cells in those minicolumns gives us almost an unlimited number of particular instances of a morphological model. I could have a morphological model that describes the edges and boundaries of an object, and then have many different flavors of it using the same morphological model, because it's the same set of minicolumns that are active. We can pick individual cells within those minicolumns and say, this is a particular cup or a particular item. It also allows us to recognize objects when we have fewer qualitative features, which explains why we can recognize an object when it's black and white or as a line drawing.

This also explains how we recognize line drawings. We often talk about a face made of fruit or vegetables; we still see it as a face because the morphological model is the same, even though the individual features have the right orientation but are not the right features—it's not a nose, it's a pear, for example. That was the first thing we wrote about morphological models. I stumbled upon that topic again when thinking about deformations earlier. One thing we talked about, maybe a year or two ago, but I'm not sure if we ever incorporated it into the theory, is whether the morphological models are separate from the feature models and if we can mix and match them easily. Basically, the morphological model, no matter which features are on it, will always have the same object ID. For example, the cup—no matter which logo or pattern is on the cup, if it's shaped like a cup, it'll have the cup object ID, but you can apply different feature maps onto it. That's how it would be disentangled.

Did you come up with an answer? I have some thoughts. I concluded that if we can disentangle it, it will solve some problems, but I didn't come up with an answer for how to do it. This also gets to some of the problems, like object distortion. It does play into object distortions, because when I was asking myself about deformation—I use the word defamation, is that okay? It's the same thing, right? Yes, same thing. I said, what are we deforming? My first intuition is that we're deforming the morphological model. You can apply deformations to a morphological model, and particular ones would carry over their features.

This also relates to something Niels has brought up many times—the issue of classes of objects. It felt like this is the sort of class I could have: a class of objects that share morphology, and that's what a class is. You have very different features, but that class has to include deformation. There's a morphology model, you have deformations you can apply to it, and you can also apply individual feature sets to it. That seems like it might cover the universe of these things.

It's interesting in the biology. When we first started this, the temporal memory algorithm—which is the algorithm we think is being used for this, even though it's sensorimotor—we assumed the minicolumns themselves were the actual detailed features, but that never matched the biology. The biology always said the minicolumns look like orientation; there just aren't that many of them. In our first paper, "Why Neurons Have Thousands of Synapses," we mapped out features onto minicolumns as an SDR type of thing, but that's not what it looks like in the brain. So this fit: if the minicolumns are just orientation, that's great. Then the individual cells, because of sparse properties, can have many individual representations of those minicolumns.

But then we have the question: what do I want to infer about the object? What am I labeling? What am I not inferring? If I wanted to create a class or an ID, we assumed there was temporal pooling going on in the upper layers, so we were temporal pooling over the individual cells. The algorithm we outlined would say, "I have a label for this specific instance of this object." But we also want to be able to say, "It's a generic object—it's a cup, not just the Memento or the Thousand Brains cup."

At least for what I was thinking about, it would already be sufficient if all the cup-shaped objects were in the same reference frame. If we can use the same unique location space for any features that might be on the cup, then I think it would be okay to pull it into different object IDs. As long as they would be in the same reference frame, it would solve some of the issues of object distortions.

This is a similar issue. At one level, you'd want to be doing temporal pooling over minicolumn activations. That would be the idea—maybe the idea of... I don't even know how that works. You want to be able to do temporal pooling over the cells and have a unique representation of the minicolumns. You want a unique representation of the cells, and you're suggesting the same for the reference frame. You want a unique representation for the class of objects, like cups, and also be able to tie that into a reference frame specific to specific cups. There's this duality between the class of the object, which is shared among many objects, and the reference frame, which is also shared among many objects, but then we want to specify specific versions of it.

If I were to say your point, it's that we should be able to treat the morphological object as if it has its own reference frame, and that we can path integrate to it and temporal pool over it, but also do the same for specific instances.

Does that make sense? Is that what you were saying? Yes. Basically, having the ability to apply different feature maps onto a morphology model, but also, no matter which logos are on the cup, it will activate the same minicolumns and locations in the same reference frame on the morphology of the cup model. The problem is, if we had a reference frame just for the morphology model—the class reference frame—if I know where I am on that reference frame, how do I make a specific prediction for a particular instance?

How I would propose it is that we have one reference frame for all instances of mug-shaped objects, no matter what the features are. That reference frame is for anything with those particular orientations at those particular locations. Then, whatever cells we activate within the minicolumns can be additionally conditioned by the specific object ID or by a feature map ID. That tells you, if I'm on this feature map and on this morphology, then at that location, I should expect this particular feature.

But thinking about neurons, I don't know how the neurons do that. Maybe you can do that easily in software, with apical biasing. But with top-down feedback, that would also potentially be unique.

In general, with the feature maps, I feel like we avoided the need for feature maps by having point-by-point compositionally. With the logo on a mug, I'm not sure what you mean by feature maps. At least my interpretation of what you were describing, Viviane, is having some sort of large representation that you wrap around a morphology model or bind to it. That's what our hierarchy is meant to be doing. In the hierarchy, the object idea of the child object is also just specific activation within the minicolumn, so that would be treated as features. I was thinking of it as the feature map defining which specific cells you would activate in a minicolumn.

I'm just confused about where we need an additional feature map representation. What's insufficient about the compositional one? The main case that makes sense to me is if we want to say, for example, the color of this object is blue, or it's covered in checkerboard patterns. We want to predict that everywhere without having to do a lot of binding, which could be something relatively simple happening in L4 or something similar. I don't know if it's necessarily a feature map, but it's just about predicting the same thing unless told to learn something different. Maybe after we go through the document, I can draw some of the diagrams I made on Xca Draw, and it might be clearer when I'm talking, because it's a bit hard to describe with words.

Before we go on, it's tempting to think this can be solved in the hierarchy, because that's the way you can relate reference frames to each other, and that mechanism feels really good. I think that's what you were suggesting, maybe Niels. I wrote down some ideas: we've got these two—I'll call them object spaces—the morphology and the detailed (I call it something else). How can they share a reference frame? It seems like we need to be able to fluidly go back and forth between these two. We need to be able to pattern-engage one but predict in the other. If we can't predict a specific one, maybe we just predict a morphology. There's a lot of that happening.

It's also worth asking: can hierarchy solve these problems?

One thing I realized is that when we can learn an object with vision and then infer it with touch, or learn with touch and infer with vision, we can only do so with the morphology object, because the qualitative features of touch and vision can't be shared. So, when I learn this thing, I end up with a detailed model of an object that is shared across modalities, but that one is really just a morphology object. It certainly can't have color, texture, or temperature. That's another way of thinking about it. In the hierarchy, I would imagine this pure morphology shared model would be higher up, since it's across modalities. Or it could be voted on by modalities. I'm just spouting ideas here.

It felt like we've been talking a lot about deformation. Did we cover everything I wrote about morphology here? Let's see.

Deformation: whether it's on the spectrum of t-shirts to bent logos to Vivian's balloon, these are all confusing. The examples are a flattened ball, a bent mug, a crumpled t-shirt. I observed that in all cases, the relative positions of features are maintained, but the overall morphology has changed. The relative position of two things on a t-shirt or a bent logo hasn't changed; it's just the morphology that's been altered. The neighborhood relationships between features—the distances might have changed, but things that were next to each other are still next to each other. There's a concept that you could stretch the reference frame, and as long as you keep stretching it and don't break it, the features go wherever the stretching takes them. If it's three features, A, B, and C in a row, they can't become A, C, and B. They can get stretched out or bend, but they're still in the right order. On a t-shirt, they could fold on top of one another, but if you follow the surface, they wouldn't be out of order.

It struck me that we have two possible explanations. One is the compositional idea, where you can take an object like a logo and apply it to any morphology model. You could take the logo and wrap it on a mug, across the top of a car, or on a sphere. The idea is you have a hierarchy: a morphology model in one region and the specific object you're modeling, like the logo, in another region. Through hierarchical connections, you're using the morphology of one region to do the path integration, then go back and pick up the assignment in the other, which is the specific thing like the logo. I still think that's valid; it has a lot going on.

Viviane, you proposed a different one, which is more of a deformation of the reference frame. It wasn't that the reference frame was deformed, but how you update your location, orientation, and direction through path integration would be modified on a point-by-point basis, which is similar to deforming the reference frame.

It wouldn't actually involve the thalamus itself to do that. Like you said, it would just update the location you expect to be in, instead of actually deforming the reference frame. I thought originally you suggested it was done through the thalamic projection. That's why I liked it. I didn't say where it would actually be connected, but I don't think it would require the thalamus; it would just require sending a movement command to the child object's reference frame.

Alright. But what the thalamus is already doing is scale, and we think it's handling scale and orientation changes. Originally, when you proposed this, you said we could do it on a more point-by-point basis in the thalamus, and that's one of the reasons I liked that theory. You put it out there and said it might be crazy, but I actually liked it because of the thalamus. If you hadn't brought up the thalamus, I probably wouldn't have liked it as much. 

It actually builds on the compositional solution rather than being an alternative. I tried to make a visualization of it, which I can show later to illustrate the similarities and differences. What you wrote is pretty much fitting. I still like the compositional approach because it's a mechanism we're fairly certain exists, we're already using it, and it would be nice to use it again. Just like we discussed the relationship between morphology models and specific models, and how that could be solved with hierarchy, this is very similar. 

The mechanism I was proposing takes the compositional solution and adds an additional mechanism for updating location instead of just predicting orientation changes. I'm not sure how that would work if you didn't go through the thalamus, but when we last talked about it, it felt like it would be easier not to go through the thalamus because it's more like a translation rather than a rotation or scaling. That feels like something that fits better in layer 6, within a column, rather than involving the thalamus.

My preference is not to introduce a new mechanism if possible. I'd rather build on the existing one or even a new concept. Maybe I don't understand it well enough, but it felt like this is a whole new thing we have to get these cells to do, and we're already asking a lot. But that's what layer 6 cells are already doing—integrating translation and movement—whereas we've never talked about the thalamus doing translation. It just doesn't feel right to me. I think we can make the case, and maybe there's an in-between, like you said. I want to start with the idea that composition can solve this problem, and if it can't, and you want to add some details to composition, I could go for that if I'm convinced the existing mechanism is insufficient.

I don't think I did a good job explaining how it relates to the composition mechanism. Maybe I can clarify that later. This is still an area we don't really understand. It reminds me of the confusion we had before we understood the morphological model. It wasn't easy to come up with or accept at first, but then it made sense—the biology supports it and it looks right. This reminds me of that. There's something odd going on here, and maybe it's simpler than we think, just a different way of thinking about it. But right now, it's confusing to me.

One thing I've been thinking about, though it's too early to present, is whether associative connections between certain cells in layer 6 could do something similar. That might have slightly different properties and help with biological plausibility. I'm hoping to write some of that up.

If we can still brainstorm, when I was thinking of examples of deformation—like when the cup was bending—I realized it feels like I already have a pre-learned memory of that shape. I've seen that shape before, so now it's familiar and I can infer the bent shape. It's like a bent pipe or straw; I know that shape. Then I tried to imagine deformations I'd never seen before, and I felt like I'd have to spend time learning them. It wasn't like I could just infer them; it was almost like I'd already learned the image of that particular bent thing. I might even know it as part of a behavior, like an intermediate state in some process. But it didn't seem like deformations were arbitrary—I was trying to fit them into some previously learned shape, like the bent straw or pipe shape, and then apply that. 

When I thought about shapes I'd never seen before, it became much more difficult to do inference and prediction tasks. Even those deformations may be recognizable morphology models. That's how I get back to the morphology versus feature model idea: if we can separate the two, then most problems could be solved by recognizing a different morphology and projecting a certain feature model onto it. With the bent cup, you might recognize the bent morphology and then project the logo onto it, which will automatically bend with it, along with any other colors on the mug.

The big point is, I want to know more about this territory. I thought that morphology models might be—anyway, to what Viviane said, we might need to also do the reverse: project morphology into some features. For example, mirrors can have many different morphologies, like bathroom or handheld, but they share the feature of reflecting. So it's not that we're mapping—I don't know how to handle that. If I have a morphological model of mirrors, they share something. You got me confused about mirrors. Let's step back. Say this again. I got confused.Morphological model of a mirror is so I can imagine us grouping bug-shaped things. A morphological model of a mirror is weird. I don't know what that means. You're saying a mirror image—isn't that more like accordance, almost like the fact that something illuminates? Like literal, or the fact that something is a chair and you can sit on it—bathroom mirrors or body mirrors. Okay, morphological model of a hand mirror. Okay, I got that. But there are so many different morphologies of mirrors: handheld versions, bathroom mirrors, body mirrors. They can be weirdly bent, like in mirror houses. In that case, the morphologies that a mirror can take are wildly varying.

Again, I think you're mixing two things up, aren't you? One is just the shape of handheld mirrors—I can imagine handheld mirrors. Now you're saying the shape that the mirror produces if I'm in a fun house. Those are two things. Aren't they just two separate things? Are they two different IDs? No, I think it's more the fact that you're saying all these mirrors have the same property of reflecting things. Yes, and that's why I was saying that gets more to what we discussed before: you can also classify objects by something like avoidance, or the fact that a vessel can hold fluid. Vessels can have different morphologies. The fact that you can eat something. At least briefly, when we've talked about it in the past, I think our feelings—yeah, that's probably represented as an ID as well, but it's something different. Whether that's in a different layer in L2, L3, or what it is, it's learned in a different way. Context and whether you can use something for certain tasks and things like that.

What you were talking about, Viviane, with a feature map, is something different. I think, if I understand it, it's actually a good example of applying features to different morphologies. You have all these different shapes, but you're applying the same features to them, which is the reflective surface. You can apply reflective surface to all kinds of shapes without problem. The reflective surface is a feature, and that can be on a bunch of different morphologies.

Maybe I'm on the side of disentangling more. I'm finding the mirror difficult. I'm not sure how to—that's fine, we can drop it for now. I think the mirror is maybe adding a lot more complexities because it has to model what's in front of it and what it is reflecting. I'm fine. To me, it's not working for me because it's—yeah.

But just briefly, dimension on the logo with that being like a feature map. Doesn't the logo have a morphology as well? That's how we recognize the logo. It does, but once it is just an object ID on the parent object, then it is a feature map. Where does the logo exist? That's why I'm confused how that's different from just the hierarchy we have at the moment. But maybe the drawings you mentioned will help.

And maybe we can just go to drawings now. If you want, I can stop sharing, but I think you only had one more. I can try to finish up here if you want. The permanence one was interesting.

Permanence is a real practical thing we have to deal with in our systems. The world has varying levels of permanence. Things are always changing. In fact, I couldn't think of anything that was permanent forever. It's just a scale of relative permanence, and the meaning or structure can change over time for various reasons. It could be objects moving, or a behavior. I was thinking mostly about objects moving, so I gave some examples. One that I think about all the time is when I'm about to cross the street on my bicycle near the coffee shop where I work. I have to look around and see if there are other people on the other side of the street waiting to cross with me, or if there are cars in the turn lanes. I quickly look around and build up this model of the intersection.

It's unique every day because there are different cars, maybe someone has a stroller, whatever. I build this thing up very quickly and use it to act appropriately in the intersection, and then it goes away. It never occurs again. This is a really important part of living in the world. Our systems have to be able to do this. It brings up the question Scott mentioned earlier: the system has to be able to deal with various levels of permanence, not get confused, and somehow know that some things should always be the same and some things are always changing. I don't expect them to be the same. If I went back to the same intersection 20 minutes later and all the same cars and people were there, I'd think something was wrong.

I just thought it was an interesting problem of the world. There are different ways you could solve it, but in the brain, it seems there are some parts dedicated to quickly learning models and forgetting them quickly, generally the hippocampal complex. There's some evidence other parts of cortex can do quick learning, but mostly we associate this with the hippocampus. We think the hippocampus works basically on the same principles as the cortex—not exactly, but similar ideas. In biology, you have to break it out because it requires a different biological mechanism to learn these things very quickly. That may not be true for Monty in the Thousand Brains Project. The point is, as we think broadly about Thousand Brains and building robots and intelligent machines, this is a very important practical issue. We have to deal with quickly learning models, and some may have to be permanent or semi-permanent. It occurred to me that in a computer, we don't have to get rid of previously temporarily learned models; we could just store them in some data store so that if I wanted to, I could go back and see what the intersection looked like at a specific time. I could recall that, whereas a human can't, but our Monty systems could. I thought it was worth bringing up so we're all thinking about it as an issue we have to address. The brain provides some insights into how we might go about it, but I'm not sure the brain's particular mechanisms are essential for Thousand Brains.

That's it. I just want people to think about it. I was thinking along similar lines when working on these mug models in Blender recently and making new objects. I saw that you can make random surfaces. I thought Monty could learn all of these random surfaces perfectly. By random surface, I mean a mesh—a 3D mesh where you can click a button and it randomly generates one. It's like a folded t-shirt that does something weird, like a big spiky surface. Imagine taking a reference frame that represents a t-shirt and crumpling it up. That idea. I was thinking Monty could learn all these perfectly as it is now, if it had enough time and saw them individually. It would basically memorize them, which is like memorizing noise. I wouldn't be able to do it very well. The combinatorial problem of remembering all the ways a t-shirt can crumple is impractical—more than the number of atoms in the universe. You could learn one or two, but certainly not all of them.

I was thinking there might be features about the object to help inform this, especially the idea of a shared reference frame. If there are features about the object that help put it into a shared reference frame with other things, that shared reference frame is a space where we can operate temporarily and not necessarily need to add new features to it, like that particular baby stroller in the intersection. It's a vague thought. Can you say that again? Sorry, I have a vague understanding.

So I started thinking about the morphology—the zoomed-out, coarse versions of the morphologies, especially in an unsupervised setting where I'm looking at a new mug. If I want this mug to be learned and recognized in a way that's aligned with my general mug model's reference frame, I can use the coarse features of the reference frame in a model-free way to initialize the axes of the grid cells, the scale, and things like that. This helps put something into a common reference frame. Maybe having a common reference frame is a way to—I'm still off, Scott. Maybe I'll try to write this up. You can probably write it better.

It might be helpful. Maybe it's worth briefly mentioning the common reference frame. A nice aspect of that is the issue of the search space when you first start recognizing something—you have all these objects to search over, and maybe that could help constrain things. It doesn't seem biologically plausible that we can search in parallel over all these objects, but that's another reason to think more about that idea.

Can I throw out an idea? I just thought of it and don't want to forget it. There's a classic example: you're looking for a place to sit, but there are no chairs. You find things you can sit on, or you see something and think, "I can use that as a chair," or, "That's a different chair than I've ever seen before, but I know I could sit on it." What that made me think about is, let's say you have a chair morphology model—your classic chair—and now you're willing to morph or deform it in different ways. If I'm looking at another morphology model and can attend to a subset of it, I could see that subset as similar to a chair, in the same way that behavior models are a subset of the overall model.

It's an interesting problem: how do I see something and say, "Oh, that's good enough to use as a chair"? With morphology models, you have to be able to look at them. I think I do the opposite: I take a model of myself and imagine sitting down on the object, instead of trying to take a model of a chair and matching it to the object. It's more like, I'm looking for a place to sit—that's the starting point. I guess I'm looking for a stable thing; we can call that a chair. It's got to have a horizontal surface at a certain height, maybe a back, maybe not. To determine whether it's suitable, that's where you do the mental simulation.

You might bias your search for chair-like objects to be more efficient, but almost anything could be a chair, or so many things could serve that function. Maybe this isn't a good example, but it's interesting because it has an extreme case. I recently sat on one of those huge yoga balls at the gym. That ball doesn't have any flatness; the only thing is, if you put your weight on it, you can't really sit on it—you'd roll right off. It trains your core; that's the point. It's supposed to be good for you as an office chair.

I realize I was conflating two different things. One is the classic example of chairs of different shapes and morphologies—how do you know they're all chairs, even if they're baroque or unusual? That's a separate question from finding a place to sit. I was conflating two things. I'll take that back. But I do think the idea that, on an object, we might see a subset of its morphology that's recognizable as something we already know is important. In the same way, behavior can be a subset of a bigger object. Composition plays a role.

I have one more thought on what Scott said about the random objects generated in Blend and the crumpled t-shirt. It reminded me of the solution we suggested for the practicality of learning object behaviors: at first, you learn very few points of the behavior and interpolate between them to make predictions. That would also apply to learning topology models and distortions of morphology models. If the object is simple—say, a random shape with only six edges—they might just be in different locations than on a normal cube, and that would be relatively easy and quick to learn. You could just store six points or a few more and interpolate between them to make rough predictions. But as complexity increases—like a hundred edges on a weirdly shaped object—you need to store more points to make accurate predictions.

Similarly, with the t-shirt, when it's crumpled, there are many local changes you need to learn to make good predictions. It's difficult, but you can learn a coarse model of the overall shape—like whether it's L-shaped or in a pile—and make rough predictions. In terms of Monty, maybe it's about gradually moving toward learning models with very few points and interpolating between them to start, then only adding more points or higher resolution in certain areas if necessary.

That's a good point and a theme we see everywhere. The example I often use is recognizing trees as a child: if you don't know anything about trees, they all look the same. As you get better, you learn what details to look for and fill in those details later. You don't throw away the original model; you just add more information. It takes time and memory, but that's the idea.

I have a thought on the topic of permanence. I see a clear distinction between permanent models and temporary models, but it seems more like a spectrum of how things are permanent or temporary. There are no completely permanent models; it's just a spectrum. I'm not sure if this relates to the problem of key frames in a behavior, where some key frames are very frequent and act as permanent arrangements of orientations, while others are more temporary. I'm wondering if these two problems are related.

I didn't think of it that way. In behavior, you don't learn the morphology as the thing is moving. There is no memory of the in-between states unless it stops or moves very slowly. If it stops temporarily, you can learn that as a morphology model.

I don't think we're learning and forgetting the in-between states. That doesn't feel right to me. Maybe you think that's right, but it doesn't feel that way to me.

There was an early scientific use of motion pictures to resolve debates about how horses run and what their feet do. No one could figure it out by watching, but when they took video and looked at each frame, it resolved the debate. There's no way to see the morphology of the legs while the horse is running. That's a great example. I'm wondering if it's more of a statistical thing—if you see something 90% of the time, it becomes a more permanent model you can store. If it changes a lot, you don't store it. The basic biology is that with repetition, learning is not about increasing the strength of a synapse, but about increasing its permanence. The synapse becomes a thicker structure and gains a permanent physical quality. If something is repeated or emotionally salient, the synapses become permanent and don't fade quickly. If you see something only a few times, you get a little filament that can go away. It's all about permanence, not just strength. Many neuroscientists still don't fully understand this, but there's a complex formula for what leads to the permanence of a synapse. Emotionally salient things are remembered strongly, even with few exposures, but generally, you need repeated exposure for something to become permanent and slow to fade. Childhood memories can stick around for decades.

It's interesting to think about how we would implement this in Monty. We're not modeling synapses right now; in the neuron paper, we just used a variable called permanence. The closest thing in Monty is the grid object model, which only adds points to a model permanently in the top K locations where they were observed. Only the most consistent features on an object become part of the model used for predictions.

It may be necessary to have some forgetting. If other features are more consistently observed, less consistent ones are dropped. Do things ever become truly permanent, or is everything on a spectrum? Right now, it's a top-K mechanism—if something else becomes stronger, the lower ones drop out. There's no truly permanent feature. An object isn't completely forgotten, but the locations used to represent it can change.

We don't have much forgetting in Monty right now. Another related point is the hippocampus, which forms episodic memories quickly. For example, I can recall details of an intersection even an hour later. The memory in the hippocampus is already a composition of other high-level objects—cars, people, strollers. You look at someone and infer their intention, like identifying a child or a parent with a child. The rapid memory is built on having good models of these things. If my models change in the brain, it's okay to change them at the hippocampal level, but I don't want to change them in V1 or V2 often, because that would disrupt everything above. If I forget what bicycles are or have a new memory of what a bicycle is, all my old memories based on bicycles would be affected. There seems to be a hierarchy of permanence required to keep your sanity.

All right. If you want, I can stop sharing my file here. I wrote a lot about composition and realized we already know this, so I deleted it. I don't think we have everything; there’s more to say about behaviors and about spatial and temporal structure, but I didn't get to it.

If someone else wants to bring up a drawing, I could go over what I drew, but I saw Hojae also drew some things. I'm not sure if you wanted to go over that first or not. I think it makes sense, before I forget what we were just talking about—my short-term memory is very short, and my long-term memory is also short—so just to keep it in context. 

I have two things I drew here. One is after reading your document, Jeff, I realized I didn't make it clear what the proposed solution was or how it related to the compositional model solution. I went back to the document and realized I never really showed how it relates. I did write that it is as in our standard hierarchy framework, but that was just a bracketed comment, so I tried to clarify that. The second thing is that I tried to think about circles and ovals, and whether we can solve this with pure morphology models. After thinking through the second one, I'm not completely convinced we need this mechanism anymore.

I think I have to think more about which way would be easier. Maybe it would be more interesting to start with the second topic, but I don't know what you think.

The second one—did you feel it supersedes the other stuff? Is the second one the circle thing? Yes, and the feature map. It turns out the circle seems to be the hardest thing in the world to understand.

It was more of an afterthought after I applied the proposed solution to circles and ovals. It's just this orange text, basically saying that the whole point of modeling these object distortions in a separate model from the morphology model is so you can apply the same distortion to different models. I can learn this kind of circle-to-oval distortion and then apply that to a ball that squishes, or anything that might be on there, like a logo or stripes—the distortion applies to anything on it. But as I tried to put some example images, I realized all the examples I could think of were just changes in features on that morphology. All of them would still have the same oval morphology. The thought was that if we could just learn separate morphology models for the circle and the oval, instead of learning the circle morphology model and the oval distortion model, and then apply any kind of feature map onto it, isn't that what I was saying earlier? We just learn these as separate objects. Yes, exactly. Here, blue represents a morphology model defined by orientations at locations. We have the circle, and the model of the circle is defined by specific orientations at those locations. We have an oval, and the model of the oval's morphology is defined by those orientations at different locations. That's option one: we just learn two separate models for these two shapes, two different morphologies. Option two is we learn just the morphology of the circle and then learn how to distort it. I went through an example for that. But what does this help us with? Wouldn't we have to learn as many distortion models as morphology models in option one? Yes, if we just have ovals and circles and they're all just morphology models. But it helps a lot once we start applying the same distortion to different models.

That was my reasoning. Can I interrupt? Why couldn't we, instead of calling it a distortion model, just call it a behavior? I have two morphologies, the circle and the oval. We could have just learned them separately. I see circles and I see ovals, or I could say, this circle becomes an oval, which would then be a behavioral model. But I'd still have the two morphology models because I observed the circle and the oval after it's been squished. It just feels like the transition is really just a behavioral model, not a distortion model. The behavioral model could say, yes, sometimes these go between these two, but they're still two separate morphology models. What's the difference between a behavior model and your distortion model? I didn't get around to that; that was the third item on my list to think about today—the relationship between the two. I think they're very related. The main problem I see with this just being a behavior model is that I don't know yet how we would use that to make predictions about the morphology. I'm not sure. Imagine I have two morphology models, circle and oval, and they're just separate things. Then I say, a behavior model—if I see a behavior, I can then predict the resulting morphology model. It's just a way of tying them together, to predict the next morphology model.

These are tied together in time, sometimes through a behavior, but they're still separate models—still separate morphology models. You would have both models: a model of the circle and a model of the oval, and then a behavior model that lets you recognize when the circle is transitioning into an oval. The behavior model is optional; you may or may not have it, depending on whether you observe the transition. Option one is to learn two separate morphology models, and then optionally have the behavior model if you often see them transition between each other. The behavior model would predict, for example, with a stapler, you might have an image of the stapler in the closed position and in the open position—two separate morphology models. If you never saw them transition, you might think they're two different types of similar objects, different morphologies. But if you see the behavior between them, you can say, once the behavior starts, you should end up at the second morphology model.

One thing I was considering is that you learn this morphology model for each object, but then you see a circle with the Thousand Brains logo on it, and it's distorting. How do you make predictions about how the features on that object change? That's where I always got stuck—how do you have an object with a similar morphology but different features, and apply that different morphology or distortion to it? If we can think more about having separate feature maps to be applied to the same morphology, that could solve the issues I was seeing with option one, and I would be very happy with it.

Did you want to say something? I have a question, but Niels, did you want to ask a question first? I was just curious, because originally our naive approach would be to have learned a different morphology model for the circle and oval, but I remember you had concerns about that, Jeff, that it felt wrong for those to be separate models.

I'm curious if that issue has disappeared. I don't remember making that comment, but it's certainly possible. I wear my thoughts on my sleeves. Maybe it was about when a circle turns into an oval—at what point is it a new model? You could have many different ovals. Do we want different morphology models for all different ovals? I don't necessarily have a concern with that. If it's a slightly different oval, we'd probably just learn it on the fly. It would be close enough to previous ones, but it's interesting to think about ovals existing on a continuum.

I'm not sure I have an infinite number of models of ovals, but it's possible to have quite a few. I can have a model of a circle, a model of an oval, maybe a couple of flatter ones. I certainly don't have a lot of them. If you asked me to draw an oval, I might draw the same ones over and over again—those might be my canonical ovals. It's an interesting question.

I never thought about the distortion here. If I see an oval with a logo on it, I don't expect it to be distorted because ovals are their own thing, and I can put a badge or logo on it. Many logos look like that, like the one you have down there. I don't think that one is squished. I was thinking about actually observing the distortion. My point is, I have two different expectations: if I see a logo on an oval, I don't expect it to be distorted; if I see the logo on a circle and know the circle is behaving or changing, then I would expect the logo to be distorted. If I were playing with a ball and we squished it, I would expect the things on the ball to be distorted. There isn't a single answer to this question.

With the behavior, I expect distortion; without the behavior, I wouldn't expect distortion. In a more general case, if we have behaving or distorting objects, we'd want to generalize to different versions of those objects. If I learned the stapler once in its open state and now see a different stapler that opens, I would expect the features on that stapler to move in the same coherent way. But I thought we solved that problem. We solved it for the stapler, but not for actually distorting objects like a balloon or a squished ball.

If I saw a balloon, it's not just an oval or an ovoid; it's an object I recognize, and I know it has a behavior. I know how it got into its current state—it was inflated. If I just saw a ball, I wouldn't think that; I'd say it's a ball made like a ball.

For the ball, my first expectation is that anything printed on it wouldn't be distorted. For the balloon, I expect that something printed on it would have been a different size or state earlier, and now it's either distorted or just bigger. Because I know there's a behavior to the balloon, I expect the feature or logo on the balloon to change as the balloon changes.

This is an observation. It's not an answer to how it happens. It does seem to be tied to knowledge about the behavior of an object. Maybe an example, like a pear—a pear doesn't have a behavior, but if we are projecting a sticker or a face onto a pear, I don't think we can predict it particularly well, but it would definitely distort. The sticker on a pear is the same as the logo on the coffee cup. It's not distorted on its own surface; it's only wrapped around another surface. So the sticker is not distorted, it is just mapped onto a different morphology. Whereas the balloon really is distorting the logo or the image. I think these are clues—these things are clues as to the solution here. It feels like we're making some progress.

One other random observation, and this is pure introspection, so I'm not sure how accurate or useful it is, but it seems like when we see a distorted object, like the spent coffee mug, you usually imagine how it got there. At least in my mind, it seems like I'm imagining the cup being original and bending into that state. Any kind of distortion I'm thinking of, I can imagine how it got there. If I saw a pipe bent, I would say, "Oh, that pipe got bent." Pipes are built straight. If I saw that mug, I would say ceramic mugs can't bend, they break, so it had to be formed that way.

I'm just pointing out that there's something subtle. Unless it was a rubber mug, I guess it could be a rubber mug. Does it make sense how option one would work—basically learning separate morphology models and then generalizing them to different instances of that object? If we can separate out morphology from features more, I think the approach is a great one. Niels did bring up the issue that I brought up: there's an infinite number of ovals, so how do we handle that? I was trying to dance around that, because we don't want to have an infinite number of morphology models, but perhaps a few would do. For what it's worth, option two would have the same issue, because you would have to learn an infinite number of distortion models as well. So maybe it's a combination where we don't have an infinite number of ovals, we have some small subset, and similarly the distortions—they can't be anything, they can just be relatively limited small local changes. They interpolate in the short range with distortions, and you have to do some sort of interpolation in the short range. You don't do infinitely accurate predictions. With the distortion, one of the advantages is that you can reuse that a bit more than you can use the separate reference frames in this example. That doesn't just need to be used for ovals that could be destroyed. I guess egg shapes, stuff like that.

One of the other things I was thinking we could use these distortion models for is more fancy stuff, where you have a blob or whatever, or you have a clay mug. You're just forming it and then you press your finger here and it's indenting like that. I wouldn't imagine that you have learned a separate morphology model for this indented specific cup, but you can just apply a local distortion here. Not sure how that ties in, but that's interesting. It relates to morphology changes I keep talking about with separate—what's the word you use for the distortion model? Or what's the word you're using for this? Like a distortion model. To me, I think I'm going to just make that into the behavior model. It's a way of tying two morphologies to a behavior. The properties—I'm just stating and not knowing how it's going to work—but the properties, this is what I'd like it to be. If I could, the properties that are desirable are being able to apply distortion models to new objects. If it's the same as applying behavior models to new objects, could it be exactly the same thing? It's very similar. I guess the main distinction I made, why I made it a separate thing, is that behavior models are like a stepwise thing. If you want to apply a behavior model to a morphology model, you have to step through the whole sequence of changes. But we've thrown away that idea by saying we're going to remember these different morphology models, so we don't have to do this stepwise. It's just saying, I'm bridging two morphology models. I'm not predicting the second morphology model by path integration or by these trajectories. I'm just saying this morphology model leads to this morphology model through this behavior, so I only have to be able to predict the next morphology model. I don't have to be able to regenerate it on the fly.

With a local distortion like this on the morphology of the cup, would you just apply a local morphology model of an indent? Let's be specific. So we've got this mug. I now have a mug that looks like this—is that what you're saying? Like I have a mug made of clay that hasn't been dried yet, and I take my finger and poke it into the mug and it indents, and I know how to predict how it will indent and how it will look after I remove my finger. But now did you fire it? And I have it and I'm using it every day. I'm not going that far. I just know how to—it feels similar to the t-shirt, that you probably learn a temporary model, or it might become permanent, but you could immediately learn that at that location. In terms of predicting the distortion locally, I guess you have to apply some sort of physics-type behavior model of clay. It feels like this is just like a key frame in the behavior model of a stapler. Just like you open a stapler and leave it there, it becomes just like poking the cup and leaving that indent in the cup.

Just like this distortion becomes a stepwise key frame in the behavior model, if we have a set of distortion maps, that becomes a behavior because each one represents the changes at a key frame. I'm okay with that; you have a good example. I have two examples, more for illustration than education. Here's a mug with some indentations on it. This mug is probably about 50 years old, made by my neighbor. You can see a bunch of indentations, but I remember this mug because of the face. Here's another mug made by the same potter many years later, and this one's really distorted, or some sort of vessel. I can't really remember this one because the distortions are more complex; I'd have to study it carefully to learn them. You pick up on the feet quickly, but if you ask me what the top lip looks like, I wouldn't know. If you ask about the face, I have a very good memory of that. She made this by putting her fingers in it—I just thought I'd show it.

That's a good point: we might just learn a new key frame for the distortion, and if it's too complex, we wouldn't actually learn it, or it would take a lot of time. I'm still puzzled, and this might be outside the scope of what we're discussing, but as the potter makes these cups, he knows how to get the cup into the shape he wants. He knows how to apply these local distortions and how they will change the shape of the cup, and how those predictions would be made. That might be a bit out of scope. That feels very much like the T-shirt, like modeling clay.

Let me try to recap what we're saying. We assume multiple morphology models. Using the compositional hierarchy, we can map objects onto those different morphologies, and they will automatically adapt. For example, if it's a curved surface, I can map the logo onto the curved surface automatically.

Morphology models can be tied together through behavior models. If two morphology models are key frames or stopping points in a behavior, that can make a difference. It means I can predict a new morphology model, but we don't path integrate deformation. That was the issue you were trying to solve with displacement factors, so let's not do that.

Returning to the balloon example: if we have a balloon that's inflating, we would learn a few different morphologies—uninflated, slightly inflated, more inflated, fully inflated. Then we would learn a behavior model that tells us how we transition through those morphological states. If we have certain features on the morphology model, that's the point I'm least clear on, but it sounds like you're suggesting we could potentially solve that with the hierarchy solution, the same way we would map the logo. That's a goal. I'm not sure I've walked through all the issues, but I think it's a desirable and likely outcome that we can use this mechanism.

I'm a little confused because it feels like what we've just described is almost where we started. The reason we went down this path, with what you suggested, Viviane, was to enable prediction with the balloon. I don't think we could get it to work with just separate key frames; it didn't enable us to generalize with a known behavior and then map a new logo onto the balloon. Maybe now it makes sense. If I can briefly walk through the diagram, that gets at the issue you mentioned about the hierarchy solution being specific to a certain parent-child combination.

If you glance at the diagram, the main difference is that the feedback connection is missing. The compositional solution we have already works with any kind of object distortions to recognize them—static ones, not behaviors. I'm talking about static distortions, like the logo on the cup being distorted.

The only issue I was trying to address is that this is specific to a parent-child combination and requires us to learn location-by-location associations in the backward connections to make predictions about the child object. This is like a unique reference frame to the child and a unique reference frame to the parent. In the forward direction, the morphology model is already quite general. We can recognize the morphology of the distorted logo, independent of the logo, because we have the relative orientations in the minicolumns at relative locations. In the forward direction, it is already independent of the child object ID, at least to a certain degree. But in the backward direction, when we make predictions about what to sense on the child object, it is very specific to that child object. The way we got around that was to send back movement commands.

That was in the context of behaviors. Here, I'm just talking about static object distortions. This shows the general hierarchy for compositional object solutions, where we have the orientation of features on the lower-level object. For the logo, we have the orientation of features at certain locations, and specific features like color at those locations. Then, the orientation of the logo and the orientation of the cup are calculated together in the thalamus and combined into the orientation of the logo relative to the cup. That activates the minicolumns in the parent object and associates those relative orientations at relative locations. The logo ID is basically just a feature here.

Didn't we have two backward projections? The purple one shown here is very specific. It says that on this particular object, there's this particular feature. The point of sending back the movement commands was to predict a new orientation of a generic feature. I could say there's a different image on the cup, and I predict it's going to undergo the same orientation changes.

I'm not sure I ever thought of sending a movement command in the static case. There's no behavior model, nothing moving. It's purely static. In this case, we have a second backward projection, which is the orientation of the logo. Let's take out the movement idea. Don't we tell the child object what orientation it should be at any point in time, at any location, to make the correct prediction? In a compositional object, not the way we've thought of it. We tell both the ID on a location basis—here's the ID and here's the orientation at that location. I could ignore the ID, but I would still have the orientation. That's exactly what I wrote here. It's also general in applying the child orientation. We are telling it what orientation to expect for a child object, and that's very independent of the child object ID. It's just not independent of the location. The location is in the specific object's reference frame.

Earlier in the meeting, we brought up a big issue: is the reference frame location space a morphology reference frame or a specific object reference frame? This would work fine if it was a morphology reference frame. That's why I like that idea—thinking of morphology objects as very separate from the features themselves. In fact, a week or two ago, I said we need to think about all objects as morphology, as the base item. In this case, I don't know how to do this yet. We need to change our mindset, because we've been thinking about specific locations and specific objects. How can we get this to work? If we had specific locations on a morphology object to a specific location on a morphology object, that would work, or at least that's an area to explore and see what happens.

I really like that idea, and I only thought of it at the very end of drawing all of that, when I wrote the orange. I think that's an alternative solution. This solution is very general—not specific to parent-child combinations in all mechanisms except for the purple connection, which is specific to a reference frame. The idea I had to circumvent that was to apply movement to the child reference frame, because a movement command is independent of that specific reference frame. The idea of just using a general morphology reference frame should also work. By the way, I think we have to have both the specific and the morphology predictions going on. A particular mug may only have a Thousand Brains logo on it—that's what I would expect unless I see something else. The same thing here: the feedforward from layer three to layer four says, "Here's a specific object, perhaps, on this thing." We want to be able to learn specific associations between specific parents and specific children, but we also want to be able to learn the morphology. We need to do both. There's still a need for region two to tell region one, "I'm on a specific cup, therefore I'm expecting you to have a specific object in region one." We don't want to get rid of that purple line altogether. Maybe it doesn't project to the reference frame. Maybe the problem is we're tying the two reference frames together, and maybe that's not right.

Or if it is, it's two morphology reference frames, but then I need some way of actually saying, "This is the specific object you should be referring to."

I guess we all use the same reference frame, but maybe which cells are activated in a minicolumn could be conditioned on some kind of specific object ID in some of the upper layers. This requires some thought—we don't know how this works yet. Even for a morphology model, I want the point on the morphology model to be unique to the morphology model. It can't be just a minicolumn; minicolumns are not unique. There have to be cells. Now, I'm going to use the cells to represent a particular point on a morphology model. How do I then create a feature-specific point on a morphology model? I don't have a mechanism for that yet. Wouldn't it be enough to have it be a unique location down here, and then the minicolumns themselves? The problem is, we don't want a unique location; we want locations that are unique to the morphology model. If we go all in on morphology, our cells represent unique points on the morphology model and unique orientations at different points. We have a unique reference frame for morphology models.

Now, how do I add in the specific features on top of that? I would propose using the same reference frame for a specific morphology. All objects with that morphology, like all circles, would use the same reference frame. They store orientations at locations and activate the same location representations. Depending on the print, pattern, or colors on the circles—say, circles with different company logos—you might have a specific object ID that conditions which cells in the minicolumn are activated. But those cells also use the same location space. Are those cells representing a unique point in the morphology? If they are, then I can't change them to represent a red thing. They can't be unique to the morphology and unique to the specific object.

I can think of ways to solve this problem, but I'm not sure if they're biologically realistic. One thought, in terms of this unique versus shared morphology model, relates to something I've been thinking about recently. If you had two subpopulations in L6a, one could be the actual reference frame, doing path integration and related functions. Another could be more like L4 or L3, just a unique pattern for a particular location that can be reused across different instances. If that's what's sending the back projection to the column below, it could be reused by multiple objects. You could have unique reference frames for different mugs, for example, but on all these mugs, if you're on the handle, they would always activate the common location fingerprint for being on the handle. That would be responsible for setting a projection backwards. It's almost like a form of location pooling across different morphologies, or sending the back projection. Justin knows we need a separate set of cells that represent the specific piece. This gets more toward enabling generalization, but I agree with your earlier comment that we don't want to give up making specific predictions.

Let me bump up a level here. The mechanism—the space, the temporal memory—is really interesting. You have a representation in minicolumns; it's some representation by the activation of minicolumns. Associated with each minicolumn, you have a set of cells, so you can have two representations active at the same time: the minicolumn representation and the individual cellular representation, and they're tied together. You can think of the minicolumns like a cell, and for that cell, you have a number of cells that represent it, and you pick one of those. Now you have this level of detail, but you haven't thrown away the model. You just have more detail. If we want a morphology model, it feels like you need another level. You have a minicolumn, and within that minicolumn, you have cells, and for each of those cells, you want multiple cells you can activate individually. Therefore, you have a unique representation of the morphology point, which is a unique representation of the grid cells.

It's a great mechanism. I just haven't thought yet how this could be extended one more level down. Maybe there's an easy way of doing it. The minicolumns are an obvious choice; all the cells represent the same thing, you pick one, done. Now I need a minicolumn equivalent for a cell, so for this cell, I have a set of other cells that all represent that cell, and I pick one of them and I'm done. That's not out of the question. There could be multiple cells that fire together under a morphology model and then become specific under an individual detail model.

I can think of some ways that might happen, but—

It doesn't completely work all the way through because I'm not sure how realistic it is, since we already said that we can't represent object ideas in minicolumns. When you mentioned feature maps or the t-shirt distorting, the neighborhood relationships are preserved. If we have a logo that wraps around a cup or a logo on a t-shirt that crumples, the neighborhood relationships of all the points on the logo are still preserved. There still seems to be some local structure to the features themselves as well.

Basically, when we apply different features to a morphology, if we just encode the features in the minicolumn of the object, there might be a problem: we would have to learn the same kind of feature map for different morphologies. You'd have to relearn the logo on different morphologies. An alternative could be that we have some minicolumns that represent orientations, and we store orientations at locations—those are the morphology models. Then we have other models and other minicolumns that represent features like color or blobs, and those learn relative relationships of features. For example, in a checkerboard pattern, there are the black ones and the white ones, alternating, and it just learns this kind of neighborhood relationship in a simple 2D reference frame. That's technically a separate model in a separate reference frame, and you can apply them to each other using hierarchy.

I thought I was following what you were saying, then I got lost a bit. In a nutshell, for learning feature models or features, it wouldn't happen in the specific cells activated in the minicolumns, but they would also be minicolumns. It would be exactly the same mechanism as we use for orientations at locations. We would have minicolumns presenting specific colors, and we learn colors relative to something else.

I was thinking of a slightly different variation. In V1 in the cortex, you have minicolumns representing orientations, and as you go in some directions, the orientation regularly changes. But in another direction, the orientations don't change—the minicolumns all have the same orientation. There are actually multiple minicolumns that represent the same orientation. Maybe on the morphology model, all those sideways minicolumns are active, but then on a specific feature location, it's a subset. It's just another level, another branching out. You have two dimensions of minicolumns.

Another thought is whether you could use phase of activation to separate out morphology models from specific models. I haven't worked that out yet, but it's clear that phases of cell firing are a big thing in the brain, and we don't really take much advantage of it. It's in the grid cell theories, but nowhere else are we thinking about it. Maybe you could say these cells are in phase and then become out of phase to specify which specific object. You could even go through multiple hypotheses by going through phase over time. Those are just two basic ideas. None of them strike me as very fun to think about, but I don't like them that much.

It's a little bit like what you were saying, but you're saying something different. You're saying to have minicolumns that represent the actual features, like the color blob, which are there—the color blobs are there. They're not really pathology; they could be if the color blobs activate. I think when there's an edge of color, it's like a color transition. It would be nice if those color blobs and similar cells were like what we talked about a couple of weeks ago, with a model-free representation of things—where we don't necessarily have a way of describing how to break something down into its subparts, but we just say, "Oh yeah, I can see that architecture," or "That's just glossy white," or "That is all these things that we can have."

I have the impression that all the retinal ganglion cells, the ones that are color and not color, are all center-surround, edge detector types. You would never have just red; a cell would have to represent a transition from red to something else. They're like edge detectors.

We're almost out of time, but I want to leave on a positive thought. It feels good to say everything is about morphology models. Composition is about morphology models, behaviors are about morphology models, and deformations are about morphology models. Then we have to ask, how do I make these specific predictions? How do I get a specific, particular thing?

I think it's a good suggestion to think about the egg and the oval without any features on it, because I think we can actually solve all of our open issues with behaviors if we just think about morphology. Now we have to think about how to separate the features out from the morphology model and how to bring them back in for the predictions. The irony is, we started the other way around and had to figure out how to add morphology, and it was so hard. Now we say, let's start with morphology, and we have to figure out how to add back features, which maybe won't be so hard.

What you describe is like three streams in a cortical column: one morphology, one feature, and one behavioral. You won't have a presentation about behavior, but through two parallel streams, and now it'll be three—we have features separate. That's actually what we talked about at a retreat two years ago: the three models—morphology, features, and behavior. I don't remember all of it; my memory is temporary, nothing's permanent anymore, so I can't remember the conversation at all.

It's a liberating idea. Whether it's right or not, I don't know, but it's always fun to have ideas that at first feel liberating.

because it does feel like morphology is the basis of behaviors, the basis of deformations, and the basis of recognition in general.

Almost everything we recognize starts with morphology. Then we consider how to add specific features to create unique versions of the morphology.