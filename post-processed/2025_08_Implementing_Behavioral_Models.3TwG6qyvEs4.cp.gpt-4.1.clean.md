Okay. This presentation is about object behaviors in Monty, how they could be implemented, and related open questions. Here is the high-level structure.

The behavior capabilities we want to add to Monty are: one, to learn models of object behavior—to model how objects change; two, to use those models to recognize object behaviors independent of the morphology those behaviors are on; and three, to learn associations between behavior and morphology models.

Currently, the idea is to use hierarchy to speed up recognition and learn compositional models, assigning behaviors to different locations on morphology objects. Four, to compensate for object movement to make accurate predictions in the morphology model. For example, using the behavior model to avoid updating where we expect to be on the morphology model as it's moving, or, and this is partially an open question, using model-free signals for this. If I were to implement this in Monty, I would start with one and two as a first block—just learning and recognizing object behaviors, which is already substantial. I feel we have the most concrete idea of how to do this, and it would be very powerful to add this capability. Three will likely follow automatically once we have implemented the general capability for multi-model composition objects, because we can use the compositional mechanism in the same way to associate behaviors to morphology models. Four is the one I'm least certain about regarding the solution, and I don't think it's required for being able to do one and two, so I would probably have that as the last step. The general structure of the presentation is to go through each of these four, then concretely discuss how I would implement them in Monty and the related open questions.

The only thought I had was that I thought number three would occur naturally as you do number one. Exactly. That's why I put the dashed box—I feel if we implement one or two, three should naturally come out of it. When we learn about behavior, we almost always learn it in the context of a particular morphology model, and then we do three again later. On the slide for three, the main thing to do would be to have some test bed and evaluate that capability, but hopefully not need to make any changes to Monty's internals.

I'll jump into number one: learning models of object behaviors. There are a couple of requirements that would need to be added to Monty. The biggest would be to add a new type of sensorimotor module that can detect changes instead of static features and send them to a learning module. What we currently have is a sensorimotor module that extracts a pose and features at that location, and sends them to the learning module. The features can be color, curvature, and so on. The learning module learns a feature morphology model—learning features at locations. The addition would be another type of sensorimotor module that, instead of detecting static features, detects movement or changes at locations. It would still send a location in space, but also a movement direction, orientation, or a change in a feature, like a color change on a traffic light. The learning module would remain unchanged—same mechanism, same internals. It would just get a different kind of input and learn a behavior model from that.

Ideally, the sensorimotor modules would be set up so that a static one only sends input to the learning module when the object is not moving, and when the object is moving, only the sensorimotor module is sending input.

Before we go on, just one minor point. We've made this analogy before between vision for the cellular and magnocellular pathways. In primates, it seems the cortex gets these parvocellular and magnocellular center-surround fields without edge detection or movement extraction yet, and that gets extracted in the cortex. In non-primates, it seems the information is passed up already in a form like you've shown here. I think it matters why nature decided to send in some magnocellular cells, and the cortex itself figures out the movement direction.

I just want to point out, I don't know if it's important, but nature went in that direction for humans and other primates. I don't know if it's important if that was understood.

You mean that in primates, this would happen in V1 and not earlier? If you follow the cells coming out of the LGN to the cortex, there are center-surround fields. They're chain sensitive, but not directly sensitive, and the parvocellular cells are not oriented—they're just bits surround bits. In a rat, it looks like what we just showed; in a human, the processing that extracts the edge and direction occurs in the cortex itself.

I don't think we should make any changes right now. It just always struck me that evolution went through the effort of changing that. Maybe it's a more general-purpose adaptation, and it's just worth mentioning.

It's interesting. Not a concrete idea, but I wonder if it could be related to depth perception. Maybe by pre-processing, you limit what can ultimately be done with depth. I always felt that vision for a rat must be a little different than vision for a human because of this. Humans have some ability to extract information that rats have lost because it was pre-processed.

I just wanted to mention it again. I'm not sure I would change anything you're doing. I think that maybe relates to the proposal I made a few weeks ago about the extra layer for Alpha, which directly feeds forward the input to MT, with behavior models being learned there. That's a special kind of connection that doesn't go through pooling in layer three. The input, maybe just a bit of pre-processing, and the path that goes through V2 might be extracting more three-dimensional movement of objects, like the rotating cylinder. It might need some extra pre-processing or a bit of object detection to know which thing to associate those flow patterns with.

Another way to think about it, following up on what you just said, is that these extra layer four cells—maybe we shouldn't think of those as cortex, but as sensorimotor module extras. Something like that. They have to be in the cortex. All right, Steve, you should keep going. I'm sorry I debated whether to bring this up, but I think we should keep it in the back of our mind.

About the point Jeff made, I think we'll have to understand the significance of why evolution bothered to change that mechanism from non-primates to primates. In Monty, we can put in all kinds of sensors that we can't even detect biologically, like a LiDAR sensorimotor or sensors that can see in UV or infrared. There's no biological equivalent for those. If we understand why this change was made, we can better consider whether we should make changes in Monty. I don't know how we can figure that out now. It's just an empirical observation—dogs, rats, and cats don't see the same as we do, but they still see. I don't think it's a fundamental limitation. Especially if we take the view that those extra layer four cells are just more pre-processing, so we just throw them into the sensorimotor module.

We currently have a feature change SM that only sends signals when we detect a significant change. Do you think we will have something similar for behavior, where we only send if the changes are significant? The difference between the two, as far as I see, is that the feature change looks at changes between movements of the sensorimotor—so if I've moved, has what I sense changed significantly? Here, the sensorimotor is not moving, but something is moving through its receptive field. That's my next point: we need to disentangle feature movement from sensorimotor movement. It's easy when the patch is not moving and it's just detecting that something is moving through the patch, but if the sensor is actually following the object, then the sensorimotor location relative to the body is changing, but there's no feature movement within the patch. In reality, what we want to communicate to the learning module is that the feature is moving in the world, not just that the sensor is moving and the feature remains static. We might need to figure out a mechanism to deal with calculating how the sensor is moving versus how the feature is moving. Maybe this isn't a problem. We're talking about smooth pursuit here, right? I don't know enough about smooth pursuit—maybe someone else does—but it's not clear that you would do smooth pursuit on something like this. I know you do smooth pursuit of a bird that flies across in front of you; your eyes will track that. But on an object like this, it's not clear that smooth pursuit would work. The scenario in number two may not really ever happen.

Maybe there's a third case: if we observe an object behavior, we would still be moving our sensorimotor sometimes. We wouldn't just keep the sensorimotor static, but we generally don't process vision during saccadic eye movements, so that's a null input at that time.

I'm just saying I wouldn't worry too much about number two here. We don't want to sense anything while we're moving.

I think it will become more relevant later on. For example, if we see a car driving by and we follow it with our eyes, we still want to use that information to update our location in the reference frame on the car, or to keep it static in those cases. In some sense, the car itself becomes the reference—the car itself is not moving relative to the eye, so you can observe the car as if it were standing still or moving. It's a separate thing. Noting the car moves is not the same as noticing things in the car moving relative to each other.

Viviane, your point is that even knowing the car is moving ties into what you've often talked about, Jeff—this local versus global flow. The car is the kind of local flow that's not moving much because of smooth pursuit. It's not moving much on the retina because of smooth pursuit, but the global flow is changing because our eyes are moving. It'll be more complex, I guess, taking the difference. The car moving is like a street scene object where a feature is moving—it's an object behavior. The car is one of the features of the street and it's moving. But that doesn't seem like how we perceive it. We perceive it more as, "Now I'm looking at the car and I'm tracking the car."

Basically, in the current model that represents the car, if the sensor is tracking the car, that sensorimotor movement would update the location on the car we expect to be at, but we would have to compensate for the actual movement of the car to stay on the same location on the car model. If smooth pursuit works as I think, you don't have to do anything—it's automatic. I think we should keep going. I don't think we should focus on this number two very much. We can go a long way before we get there. I have some slides later that show a lot of this can be solved with model-free information and doesn't require anything specific with behavior models. Smooth pursuit is model-free.

The second big change that would need to be made to Monty is to the learning module, which is to include time or state in the models that a learning module can learn—basically, a fourth dimension in the models. In the behavior models, the fourth dimension could be time, but I would propose to add this fourth dimension to all of the learning modules, including those that get static feature input, so morphology models can represent different states of the same object.

We could model that explicitly as a fourth dimension—X, Y, Z, T coordinates. You're saying time and state for the morphology models; it's not just different states, it's the order in which the states occur. I'm not so sure about that. I'm currently tending toward just saying "state," and then it can be associated with different points in time in the behavior model.

It's an interesting question. It feels like it could be time, because we've talked about behaviors that are just instantaneous changes, and it almost seems better to represent that with key frames in morphology models rather than in a behavior model, because there's no actual change to observe—it's just a switch between states. Unless there's a time component, like a traffic light change in color, which definitely seems like a behavior model to me. It's behavior, but maybe it's more a morphology model with time that would learn it, rather than a model that gets incoming changes, because there's no smooth change stored in the model. We've often talked about how you don't really store "red to green"—that's the problem, the "red to green" problem. It's more like you store the result. When we talked about the traffic light before, we had some issues with it. It's different, because moving is just changing, and maybe those are actually represented in the morphology model. I could have time in the morphology model, saying yes, the red changes to green, whatever. In that case, all models are behavioral, but the kind of behavior they model is different. In the biological theory we have about time, as far as I know, the time signal goes everywhere, so every learning module would have time available to it if it's useful. It doesn't have to take advantage of it, but the timing signal is there, and if something has a time element, it could learn it. I like that idea. We perhaps don't have to deal with it right now, but the idea that time is available to both models is a good way to go forward. You could apply that because you had time/date. I want to make sure I understood what you meant by that. Some open questions remain, but it's a good idea to think that time could be available in both models, as Niels just suggested. I'm not sure about the best name for this yet, because it could be several things: time, a specific state, or something that's action-conditioned. Sometimes you only go from one state to another if you perform a certain action, and it's not really time-dependent. It seems like there's some kind of context signal that transitions you from slice to slice. I would suggest, in terms of architecting Monty, that we think of time as its own thing, because that's how it is in biology. Then you can add additional context from elsewhere. In neuroscience, you could say there's a whole bunch of different things projected to layer one; you can think of them all as context. Time is one of them, but you can add others too. My recommendation is to consider time as a separate entity that every learning module can potentially use, and then add other contexts as well.

When we talk about traffic lights changing colors, it feels like this is a different kind of change than morphology being changed, like a stapler opening or closing. I think it's happening in a different layer. When the color changes, this is represented in layer four, but when locations change, that's represented in layer six, because the features are the same—just the locations are changing. When the color changes, the locations are the same, but the features are changing. It's a different behavior being applied to a different layer. Layer six would be movement on the object reference frame, whereas for these kinds of changes, like changes in locations, I was thinking of those as inputs to layer four. At this location, I'm detecting flow in this direction from the directionally sensitive cells, which are in layer four. It would be layer four, not layer six, because layer six represents the location of the sensorimotor or the sensed feature on the object.

It's not quite the same thing. Would the change in color be represented in layer four? I don't know. That's too detailed a question for me right now. The color processing is wonky to begin with. You can see I'm anxious to get going on implementing this, so I would say, let's take this color thing and acknowledge we have some questions about it. Maybe it's the same as these other behaviors, maybe it's different. Let's not worry about that for now. Let's focus on Viviane's proposal and see what we can start building. Maybe that's not traffic lights, and we can start with just movements that we learn. We don't have to go straight to recognizing something; walking or running would be fantastic.

The third requirement for learning use models is a way to keep track of time. Everything is gray here—these are things I'll come back to in the open questions or we can discuss now. One question is whether time is an additional input to the learning module from a global clock, or whether each learning module has some kind of internal clock that it can use to transition through different states in the model. Jeff, do you have a strong opinion on that? I do. I think it's definitely external. I've talked about it; it would be an external signal to apply.

The basic idea is if I have a complex behavior like a melody, I can speed up or slow down the tempo for everybody. It's not a column-by-column thing, so everybody has to be told. Same thing if I'm executing a complex behavior, like signing my name—I can speed up everything or slow down everything. It takes a lot of conscious effort to do it separately. The whole projection from the thalamus was suggested as global. My model has always been that this is an external input that's applied to all learning modules within a modality.

Now that you say that, I remember that makes a lot more sense with being able to speed it up and slow it down globally. Also, if one learning module notices where it is in a sequence or that the sequence just started, it can reset the clock for everyone. Okay, great, then this question is already answered. A way to keep track of time should be the last of the three requirements to add for being able to learn models of object behaviors and then to recognize them. Requirement number one would be that our hypothesis space extends into that fourth dimension. I tried to illustrate that here: we might have evidence for different locations in the 3D reference frame we have now, but that might extend into the temporal dimension of where we think we are in that sequence. Another question is whether we can interpolate in temporal space the same way we interpolate in 3D space.

Second, the output from each learning module—the CMP signal—will need an additional entry, which is the state or time point in the sequence. It will be the location, orientation, and also state or point in the sequence, and the ID, like hinge behavior or whatever behavior ID we recognize.

Are you saying this is in addition to the object ID? This is a behavior ID? Yes, the behavior ID. Oh, I see. So behavior ID is like the idea of the behavior model that was recognized. If I think about it, when I hear a melody, I recognize the melody and know where I am in the melody, but the recognition of the melody is not particularly tied to where I am. The name of the melody is a separate field. Oh, I see.

They're independent, but including this in the message it sends out will be helpful to make associations between different morphologies at different points in the sequence and also for voting. In any case, putting this into the message can't hurt. Also, for our internal evaluations of whether it's correct. In the board meeting yesterday, Tristan was talking about our process for integrating code and research and breaking systems. By putting more information in the message protocol now, even if we don't need it, it's a good way to insure against breaking things in the future. This would also be a relatively minor change. We add another field to the state class, though now the term "state" is really getting overloaded. Maybe this will be called "point in sequence" or something.

and then third, this is a question that we may debate, as well as updating our voting algorithm to take into account that state. We could basically be voting on locations in four-dimensional space, or vote on it as a separate variable, or vote on ID, state, and orientation separately. I think, especially with behaviors, it will be very useful to do fast enough inference on a moving object if we can also vote on where in the sequence we think we are.

One reason we might not need it is if we have a global clock, and instead of voting on state, a learning module that's very confident can just reset that clock for everyone. But remember, the global clock is not object-specific, so at least that's not how I think it works in the brain. The clock doesn't tell you much; it just tells you how fast you should be transitioning. You would have to already know what behavior you're doing, and then it would just tell you how to progress through that, but it wouldn't communicate where you are in the sequence. The evidence we have is that the clock is just a countdown between events. There's no global time involved; it doesn't represent where you are in the sequence, just the time between elements in the sequence.

If you have a metronome, you can have a setting where it does a different sound at the first of four beats—beep boop, beep boop. Would this global clock just be doing the normal sound the whole time? It's basically just telling you the intervals between each beat. Or does it also tell you if you are on the first of the four? I think it's the former. The evidence, not just from neuroscience but also from other sources, suggests that all sequences have to be broken into events, like beats or notes in a melody, or an attack on a note. The time tells you how long until the next attack on a note, and then it starts again. You're basically just learning intervals—the duration between events. That's how it seems to work with music, so that's how I always thought of it. It's only good for up to about a second, and then it doesn't work after that.

In that case, I think it would be very useful to vote on the state. You'd have to vote on the state, because time, as I understand it in the brain, is not going to be helpful for voting. Voting on state is important and feels analogous to voting using locations for morphology. If we don't vote on state, it's like a bag of features in a sequence, where two behaviors that contain the same events but in different orders won't be distinguishable. I had the same thought. There's our pattern recognition working—this is like another problem.

Though, I think it's maybe slightly different because with morphology we can have multiple sensors that are not moving and doing flash inference, but with time you have to move through time. You can't recognize a behavior without moving through time. The analogy is not the same, but I think the analogy that Niels mentioned still holds. You can recognize some states, but you won't know for sure where you are. Slash inference on a sequence—right? I suggest that we think of time, for the moment, as a non-model-specific signal and see if we can fit it into this idea of an interval timer that everyone gets, which gets reset on certain events. In music, all notes have an attack—a beginning of the note—which is helpful to hear, and then you have a duration to the next note, and then it starts again. It's less clear, for example, if you're watching someone walking, what would be the equivalent of the attack on the note. I don't know how time would play into that, so it's a bit of a mystery.

I don't think the view that if you know where your time is, you'll know where you are in the sequence is going to work. I have a question on state voting.

The reason we can vote between learning modules is that every learning module learns reference frames for an object independent of other reference frames, but there is a common body reference frame, so there is a way to translate from the other. If we have learning modules with zero state, it will be arbitrary, and if you have two of them that learn different arbitrary zero states, how—without a global clock—are they going to know what zero state is? Is that like the beginning of the sequence? How are the two modules that learned the states independently going to have a common reference frame to translate the vote?

When learning modules learn objects in different reference frames, there is a body reference frame through which they can skip and learn, and there's a translation that can happen. But what is the translation when they learn states in different reference frames? How do I translate from one learning module? I think it's actually easier than the location thing, because it's just an associative connection. They both have that state represented at the same time.

The issue with voting on morphology is that there needs to be some common space for communication. I don't think we need a common time; you would just associate the two states, like snapshots, with each other. It works like a sub-id—just another id that changes faster than the big id. This state is not specific to behavior; it's like a global rotation of the reference frame. No, it would be specific—a specific state in the hinge sequence. You build associative connections between states that are specific to behaviors. It would require a lot of connections because every behavior has its own set of states.

I'm confused here. I thought this was all obvious, but now I'm confused by the questions. If the state is specific to a behavior, then voting becomes more complicated because we need to associate more states together. In neuroscience terms, this state is like our sequence memory, the temporal memory algorithm, where you're representing a particular pattern—imagine some feature movements as input to this learning module. Features move in different directions, and you represent that in layer four, which is then represented in minicolumns, and individual cells represent that particular set of movement patterns at some time. These have to be high-order sequences. If I'm dancing, running, or doing something else, I might have the same individual pattern, but it must be represented differently in different sequences. The state seems like an SDR that's unique to the particular behavior, and those would be difficult to vote on.

To your point, Rammi, it is a lot to learn. That fits with the fact that learning behaviors is harder for humans than learning morphologies. It takes more time to study, and we tend not to learn as many behaviors compared to how many morphologies we learn and recognize. Morphologies are more widely used, and learning behaviors is a more effortful process. If you think of it as the same mechanism used for different states in a morphology model, both the open and closed staplers are a stapler—they share the stapler id but are different states of the stapler, like a sub-id. The sub-id wouldn't be shared between other objects because those two looks of the stapler don't generalize to other objects.

What we're trying to vote on is where we are in a sequence, not the actual state. We're not voting on a halfway open stapler; we're voting on being halfway through a sequence or behavior. If we could extract meaningful information about just a location in a sequence rather than the full state, isn't that almost too general? Every behavior will have a start of a sequence, so if they're all saying, "I'm at the start," and that's what they're voting on, that feels confusing. But that's fine because we're also voting on the behavior id, so the information is there. It's similar to how, in layer 6B, we had the global rotation of the object or reference frame, and we voted on that global rotation without regard to the object itself. Here, we can extract information to vote on without respect to the object or behavior itself.

That makes sense. May I suggest a way forward? In the last two weeks, we spent a lot of time discussing how columns could vote by taking advantage of relative positions. We don't have a problem for a single learning module; single learning modules are fine. The complexity comes when voting between learning modules. I don't think we resolved all the issues with voting between learning modules for the morphology models. I've been working on that recently, and I don't think it's resolved. I don't think we'll resolve it here today either.

The solution will probably be the same or similar. Updating the voting algorithm to account for state needs to be done. We have to update the voting algorithm to take advantage of state. I know we've done it in Monty, but I'm not sure if it's correct for morphology models. I think there are some holes, so maybe we can just say it needs to be done, but we don't know how yet.

If we're voting on the state specific to a behavior, do we need to vote on the behavior id? Isn't that redundant? It flows from it—once you know the unique location in a sequence, it's included in the object id. It's a robustness thing; you can go from one to the other. You can go from the unique one to the object id. That's temporal pooling. But you still need to know where you are in the sequence and on the object. When we vote on locations, we're not explicitly voting on object IDs; we're voting on unique locations on each object. Since the locations are unique to each object, we don't need an additional vote on just the object id.

That's part of what we discussed before about what we are actually voting on between locations. I think it has to be abstracted from the object itself. Even though the location in layer 6A is a specific reference frame for a specific object, what we're voting on is probably not specific to the object. I don't know why you reached that conclusion. I thought I reached the opposite conclusion. I feel like all voting has to be specific to the object or the behavior ID. Maybe we can come back to this at the end if we have time to discuss voting. We're going to fall into that, but definitely write that thought down, Ramy, so we remember to talk about it at some point. Sorry for not getting back to voting. We're trying not to do that this week, although I'm still stuck on it. Let me continue and then we can get back to voting. Let's keep going as we all apparently want to.

Now, I think the biggest thing we need—these are still bigger changes—but another big thing is that we'll need a test bed, an environment where we can learn and recognize object behaviors. The requirements for the environment I was thinking of are: the objects move repeatedly in the same way, there can be different morphologies with the same behavior found in varying orientations on different morphologies, the same behavior can happen at different speeds, and objects can stop moving at some points in the sequence.

Potentially, this might be a next-level thing: behaviors could also involve changes in features instead of movement.

Some questions for the first setup to test: is one learning module enough? My proposal would be yes, we could start with one learning module, but probably only if we supervise the learning and tell it at each step. How do we supervise? We would provide the object ID, pose, and state during learning. We would tell it where it is in the sequence so that as the sequence repeats and it moves over the object, it knows where to lay down the points in the sequence. We would evaluate the same way we do now, just including state as one of the outputs.

Another thing I was thinking is we could start testing this in a 2D environment, since that would make it easier to visualize the third temporal dimension. I'm not sure if you have thoughts on why that would be a bad idea, because we might miss some 3D issues. When you were talking about this, I was thinking about walking. Imagine I can recognize someone walking left and right, walking towards me, or walking away from me. Those are really different patterns, so behaviors can be three-dimensional movement vectors. I have to be able to infer, just like I can infer an object via rotation through a plane, behaviors in 3D through plane, a rotation through plane. Somehow, I have to be able to associate someone walking away from me with someone walking left to right, even though they're very different patterns.

That's a complexity that's difficult. I think it's a similar complexity to recognizing objects in 3D space at different distances and orientations. Basically, it's the sensorimotor module that takes into account the depth information. It estimates depth, then takes that into account to get everything into a consistent 3D reference frame. Same here—I'm thinking it would be the burden of the sensorimotor module to take depth into account to calculate how the flow detects actual movement in three-dimensional space. If you feel comfortable that the same methods we use for three-dimensional rotations of morphology objects would work for three-dimensional rotations of behavioral objects, then I think you could start with a 2D environment just to test out the code. I think it's a good point, though, because I don't know the literature or methods for 3D flow estimation from a 2D camera image. I'm not sure how accurate those are, so it would be good to look into that before proceeding, because it might be noisier if you only have a small patch that recognizes some flow pattern and then have to figure out how that corresponds. Can you detect flow in-plane and flow in the Z direction? I don't know. In the end, even if we're not good at it, just consider it as noisy input, and the system should work well with noisy input. It may not be a fatal flaw if it's not as good.

I think it shouldn't be a fundamental problem. It might just be a bit noisier, so it might be harder to detect someone walking away from me versus someone running. There are certain behaviors that are more difficult to infer if most of the movement is in-plane. I can imagine it would be more difficult sometimes to see what's actually going on. I'm not a decider here, I'm not going to implement this, but it sounds like if you wanted to do 2D, it would be pretty safe. Also, Jad, if you remember, Jeff had implemented a dataset we could potentially use for this, with a stapler-type object and so on. Now that you mention it, that sounds familiar. We could use the environment he set up in Blender. We could also animate any of the objects, like take a YCP object and make it animate if we want to, just by changing the mesh.

If that's not too complicated, then we would be back in 3D space. We could do it in 3D if we wanted to. In theory, we should be able to create a system that works in 3D and then start by testing in 2D. The general approach I would take is to make it so sending the learning module to 2D space wouldn't change anything, but just only show input with X and Y coordinates and Z always the same. Why did you ask about supervision?

It seems like you're providing a lot of detail here that wouldn't be available. I'm thinking of this as a first step. If we started with unsupervised learning, we'd encounter many problems that we'll eventually have to solve, but as a first step, how is this different from when we did unsupervised learning for the sequence memory algorithm?

It's not obvious to me how this is different. We solved most of those problems in the sequence memory algorithm.

Are you resetting it when a sequence starts? Does it detect when it starts and ends, and when it repeats? That's a difficulty. We never provided state during learning, but with HTM, you were getting a unique feature each time, so you could immediately know where you were in the state. No, just the opposite. The whole point was you didn't want unique features; you wanted common features and relied on the high-order sequence.

If I recall, the problems we had were along the lines of trying to infer something while also learning something new. Maybe the thing you're trying to infer doesn't exist yet. For example, you're hearing a new melody, but you don't know it initially, or the beginning of the melody is similar to another melody, but then it deviates. We had issues with knowing when to start creating a new sequence, when to learn a new sequence, and when to keep trying to infer an existing sequence. That's similar to the issues we have with the morphology models and unsupervised learning. The one that caught me here is that you're providing state during learning. The main thing I would want to provide is telling it when the sequence starts and when it starts repeating. That would be very difficult for the learning to figure out if it doesn't have a model of the sequence yet.

Going back to my earlier comment, I agree that the whole point of HTM is it can handle a particular feature appearing multiple times, but here, we have this issue of a tiny receptive field. We need to know the state of the object to work out where we are in it. The alternative is voting, but this is just a way to avoid sorting out voting from the start—just provide that supervision. I don't think you need voting. Imagine we're just looking at something moving up and down, like the stapler, or maybe it has a fancy pattern—down, up, down. Throughout its sequence, there are points where the input is the same.

It would automatically learn to represent those equivalent inputs uniquely because they're in sequence. Viviane hit the nail on the head: the problem is knowing when a sequence begins and when you're starting over. We never completely solved that. In something like a melody, there's usually a break between the beginning and when you play it again; you don't just go from one melody to another repeatedly. But there are sequences like walking, where there is no beginning or end—it's a repeating loop with no demarcation. We never resolved that. That doesn't exist in music much, except maybe background trance. We could use signals like the stapler stopping at the top and bottom, but as Niels pointed out, the sensorimotor might be at different locations on the object, so it won't sense the same feature again because we're moving the sensorimotor while the object is also moving. During inference, that's not a problem, but during learning, it is. Is that correct? Yes, exactly—only supervised during learning. Now we're back to the idea that maybe the problem is we don't have a good voting mechanism for learning. When we learn behaviors, many are learned by interacting with the object and actively moving it through the sequence. We open and close things many times and control how we move through the sequence, so maybe that makes it easier to learn them. Although there are certainly cases where we don't do that—melodies, someone walking, or a metronome sweeping back and forth. In those cases, we have to solve those too.

I'm happy to agree and go for the supervision as you suggested. We're just going to say the problem has to be solved as a voting problem during learning. While learning, I need other information about the rest of the object. Is that a fair statement? Would you be happy with that? Yes, I agree. There are two problems: the unsupervised learning bit—the start and end of the sequence—and learning when the behavior happens relatively fast, which is most behaviors. We need to vote on state. We had this from the beginning. It's easy to imagine a learning module has a complete model of the behavior internally; it knows what movements should occur at every location at any point in time, but it can only observe one location at a time. Learning through a straw would be impossible unless it's moving so slowly. I think the real problem is the learning problem. The inference problem—maybe you can imagine it, but I'm talking about learning.

This is what I was focused on last week, which I'm not prepared to talk about today, but I keep coming back to this issue: in Monty, we have to have all these learning models learn the same model, yet they don't get exposed to the same things. How do they end up with the same model when they don't all experience everything? We can just leave that as a problem to be solved, and then I'm good with your proposal here.

This is just a proposal for a first environment, a first way to get Monty to learn and recognize object behaviors, and then there will be further steps on top of that. I just want to make sure that we're not going down a path we'll regret, and I don't feel that way. I'm exploring these corner cases because we have to solve this problem. It's unrealistic to provide the state during training, so we have to solve that somehow. As long as we understand that's the problem we've been working on, we'll find a solution. That's one of the open questions I would keep on the table to discuss in future research meetings so we can figure it out.

Any other thoughts on the kind of environment, the first test environment?

A couple of open questions regarding one and two, besides the environment. One I mentioned earlier is, can we interpolate in the temporal dimension? For example, in Monty, that would mean performing K-nearest neighbor search in 4D space where all the points in the graph are X, Y, Z, T coordinates, and we look for nearest neighbors in the temporal dimension as well. I feel like that's a nice property if we're able to interpolate in time.

Are you saying basically that just like we can't store every point on an object, we can't store every point in time? Yes. The biggest implication is that we would represent state in the sequence as part of the location in the four-dimensional model, which is easy and elegant to do in Monty, but it's hard to imagine how that would work in the brain if we have locations represented in layer 6A and time in layer 1, and how that temporal dimension would become. The way we've been talking about time, it's not a regular dimension. Time is a timer between events. Let's assume for the moment that you have a series of SDRs representing points in the sequence, and they're associated with each other. In the temporal memory algorithm, you go from state to state, so the sequence and the order in which events occur is dictated not by time, but by the association between different SDRs in layer 4. The only thing time does is add how fast you transition between these states. If time doesn't represent a point in the sequence, it's just a duration between points in the sequence, then time doesn't really represent a fourth dimension in the sense that there's no representation of where you are in that time sequence.

Let's say you have four slices of time, four beats that you perceived, and you store different changes at different locations for each of these beats. If, for the second beat, you haven't observed a change at this location, could you still retrieve the changes at the slice before or after, and if they're sufficiently similar, that tells you what you should be expecting at this time step?

I don't think you can say, "What will my state be six time steps from now?" The only way to do that is to walk through it, to actually flow through the patterns to get there. You can't just jump ahead. You could say, "What's time T2? What's time T3?" From T3, you go to T4. It's possible you might be able to predict one or two steps ahead and one or two steps backward. The system might represent those in the same way we've talked about phase precession in grid cells, where the actual activation of the cells in the cortex might go through a sequence, and the same cells would fire at different phases to represent a little time ahead and a little time behind. There's no way to jump ahead ten steps.

I'm not thinking ten steps, more like the step before and the step after, to interpolate when you're missing points. I think that's a reasonable assumption. If we take the example of phase precession in grid cells, that's what they do. As you move through space, grid cells represent where you're going to be, where you are, and where you were, all rapidly within one cycle of a background frequency.

Maybe to paraphrase, when we talk about these slices, referring to them as state or state in a behavior is better, because time is on top of that. Time has to do with how long between these, but that can vary. These are independent of time, or not totally independent, but they're just slices in behavior. It's like in a melody: you can imagine the next note, then the next note, but it's very difficult to imagine the previous note. You can't really work a melody backward. That's how the temporal memory algorithm works too. From this state, what's the next one, and the next one? It's a sequence. The neurons themselves represent a sequence without time, and the timing signal is just the transition between individual elements.

If we're going to infer through time, we can only do a little bit, like you suggested—maybe a little bit forward.

That sounds like, in Monty, we could do a lookup in the state space to the slice before and the slice after, to do a bit of interpolation.

The time signal gets provided externally to all of them.

How does the learning module know when the sequence repeats? We will talk about this later, but this is still an open question to think about.

How would the learning module infer and adjust the speed of a sequence?

I think we already have a fairly concrete proposal about that. I just included it here because I didn't include it in the changes to Monty. What I was thinking is that whatever temporal offset the learning module notices between one state and the next will be the speed it communicates back to the global clock.

Exactly. It can only say, "My event occurred sooner or later than expected." Is that what you're saying? Yes, it's the actual time. That's a nice way of signaling that. Saying "half a second occurred" doesn't tell me anything. It's more like, "I was expecting it to be four-tenths of a second, and now it's half a second, so things are going slower. Gotta speed it up or slow it down." Things are going slower. I remember we talked about this tempo thing as being similar to scale in the morphology, like in the object. I also remember that scale from the morphology columns used to be a signal stored inside the column, and it was just sending back information to modulate the inputs.

I was wondering, because now we're talking about the tempo as being dictated to the column from external sources, it's a reciprocal thing. Neither the global clock nor the learning module is sufficient. The global one tries to define a tempo, and then the learning module makes behavioral-specific predictions. Depending on whether those predictions are coming too early or too late, it feeds back to the global one to say, "You need to speed up or slow down." If it's getting multiple inputs, it could infer that tempo and send feedback. It doesn't infer the tempo; it just says things are happening quicker or slower than the model I have. The way I'm thinking about it, it's a very different idea of time than most people have. This is not what most people think about time. In Monty, do we want to do time the way the world thinks about time, as a dimension you move along, or do we want to think about it the way the brain does? I don't think the brain thinks about time that way. I think the brain, at least in certain situations like melodies, thinks about time as an event timer between events. This is one of those cases where we should probably go with the way the brain does it until we know why we wouldn't want to. It's weird, but there might be good reasons the brain does it this way. I don't know. I'm just rambling, but it's a unique way time is represented. It's similar to how scale is represented, but not quite the same. It feels like things like rotation and scale could have model-free signals that indicate, "Everything's happening really fast right now, start your hypothesis of what the tempo is quickly," or something like that.

And the same thing applies to scale. Model-free model scales, as you just said. "This is too big" or "This is too small."

I think we have a fairly concrete idea of how we would do this in Monty. Can I throw one crazy idea at you? We were talking about voting last week, using relative positions. That also relates to scale, because if I have two learning modules detecting features and they're further apart or closer than expected, it tells me something about scale. Maybe the scale of an object will also be solved with voting based on relative positions. Maybe they're tied together.

I'll go on with the questions before we drift off to voting again.

Another question is, can we have other state change signals besides time? For example, actions—other things that get us from one slice to the next.

I don't think we necessarily need to answer this for the first implementation or prototype, but it's still an interesting question for the general design. I think you're right. The answer is we have to have this.

The light only turns on when I hit the switch, or the stapler only opens when I grab it. We know the answer to this question: yes. Then the question is, how can we go forward without it for now? I think we should, otherwise we sit here in research hell forever.

I would turn this around and say we know this has to occur. Can we go forward without it right now? Is that how the sentence should read? Yes, I agree. For the first prototype, I wouldn't include actions as a way to move through this, but we should design with the idea that this will need to be included at some point. This is actually one of the big remaining questions after this presentation: how do we get actions into this picture? That'll be a good topic for future research meetings, but we don't have to answer it before we can implement something that works for just temporal behaviors.

Then maybe related, but does the fourth— we also briefly touched on it—does the fourth dimension in the morphology models correspond to time, or just discrete states that are associated with times in behavior models? We touched on it when you asked about this label here. Basically, do we get temporal inputs here that transition between those, or is it just discrete slices, and then there are associations between behaviors that get time as input? The answer sounded like at least one possibility is these labels should almost all be state, and all of them can get time, but whether they use time depends on the behavior. Some might be driven by time, some might be driven by actions.

It strikes me that if I have a morphology model and it transitions between states—if the neural circuitry would say, "Hey, if those movements are predicted, they're in a sequence"—it would learn it. It would just say, "Oh, this state follows this state."

To get back to the traffic light example, there was no movement, just a change. There are states, and they follow one another, so I'm going to learn those transitions. At a high level, if we assume the same neural mechanisms are occurring in the learning models—the learning behaviors and the learning models of the learning morphology—if the morphology goes through a sequence of predictable transitions, those transitions would be learned. It just seems like they would be, so I don't know what to make of that.

Maybe also what Niels mentioned earlier, that sometimes there are just morphology changes that happen without a detectable flow or anything. Like when my laptop is in standby and the screen is black, and then I touch the keypad and the login screen pops up—it's a predictable sequence. I wouldn't necessarily say there's any particular change stored here, but more like one morphological state going into another one, and those associations would be learned as part of that model. Now it seems a little clearer that we have these two learning modules: one focusing on changes and the other focusing on static features. They both can learn sequences.

But the sequences in the morphology model are not flow; it's not movement. If things move, the morphology model doesn't know about it. It's when they stop moving that the anthropologist says, "Here's what it looks like now." Then they learn this—those are your key frames, something like that. It could learn the transition between key frames without learning the flow, the motion. If I was walking and my arms were in continual motion, I wouldn't necessarily learn that in the morphology model, but if I stop and freeze, I could learn what that pose is. It would be nice if the learning modules all worked the same and could all learn transitions. It feels like it's moving in that direction. If it fits really nicely with using the exact same learning module for all of it, there's no difference in whether it gets morphology or change input. Both can learn temporal sequences or non-temporal sequences, and they can both take advantage of time. The traffic light could be timed.

Another question, maybe related to the environment and setup, is: what would be the learning module's policy? As we touched on, we don't want to do a smooth pursuit of the behavior, but when should the sensorimotor be moving to learn that behavior? Or should it remain static while it goes through a sequence once, then move to the next location, then go through a sequence again? This is a critical issue. It seems impossible—how can a learning module observe all these things, jumping around? You've already proposed that for now, maybe we do a shortcut and just train it, give it everything it needs to know, cheat, supervise it. I think the answer to this question will come back to us again, having to do with this voting, because the idea I'm working on right now is: how could columns train each other at any point in time? One column is getting input and learning something, but it's going to transfer that knowledge to a whole bunch of other learning modules. The answer may not be, "How does the single learning module do this?" It may be impossible for a single learning module to learn behaviors. Where it wasn't impossible to learn morphology—we could take our time moving around—but it might be impossible. You can imagine just looking at the world through a straw and seeing something moving. Is it ever possible you could learn what that behavior is? It seems doubtful. We're moving all over the place—what is going on here? This is so confusing. The answer might be a form of supervision, that it's being supervised by the columns that are observing. I wouldn't put too much time on this yet. I would say supervise, and then we'll figure it out. It may not be a single column's learning policy; it would be the group's learning policy. That makes sense. For inference, I think our current policies should still work.

We also touched on if the learning module follows the object doing smooth pursuit, how do the sensorimotor movements get processed, like feature movements? For now, the conclusion is we're not going to do smooth pursuit.

Okay. Those are all the open questions for one and two. I know there's more in the section on how number three, learning association, comes naturally from one, but it's also close to 9:30. I know Niels had something he wanted to present, and we also want to get back to voting. Should we do a part two? I didn't think we were going to get back to voting today. I'm not prepared to do that. I said I'm working on it, but I'm not prepared with anything. I think it's fair to wait with the voting, but I appreciate you bringing it up, Hojae. I think this is good progress. Niels, are you up to speak? If you want to continue, we can see if there's time at the end. I can only do point three because I think it naturally falls out of what we already have. Can you remind me what points three and four are? I don't remember. Three was learning associations between behavior and morphology models. Four was compensating for the object movement to make accurate predictions. Three feels like a simple one; four is like a can of worms. I don't have to open that today. Maybe just do three.

When you said "can of worms," I imagine a can with all these worms wiggling around, and I'm trying to learn the behavior of the can of worms. That's like the t-shirt equivalent of behaviors. We'll have to have Monty learn the can of worms behavior at some point. You can look at it and say that's a can of worms, but how do you know what it is if you can't make any prediction? It's like a t-shirt all crumpled up. I won't open that can of worms today. I'll hand over to Niels after the third section, which I hope is quite short.

Learning associations between behavioral and morphology models using hierarchy requirements: I wrote down nothing in Monty after the compositional modeling milestone is complete, because we basically want to use the exact same learning module structure and the exact same communication between them. The only difference is the kind of input they receive. We can just reuse all of the composition modeling machinery that's already there. The idea is that if we want to assign a behavior to a location and orientation on an object, we would take the behavior ID and it would become input to layer four of the morphology model. That would assign the behavior on a location-by-location basis to the morphology. I didn't draw this here, but it would also get the orientation of the behavior relative to the parent object. On the feedback connections, when we just see the morphology, that can bias which behavior we expect at that location as well. This is really elegant.

The only tricky part is that for this to work, like when we talked about V2 and V1, the column on the right had to be observing the same location or basing input from the same part of the retina as the column on the left. That had to be co-located in the sensory array, right? Now, if we've separated out the behavior learning modules, and they're someplace else like MT or something like that, we still have to have that relationship. There still has to be a one-to-one relationship between a column on the right and a column on the left, representing the same location in retinal space. That may be fine and easy to do in Monty. The evidence is in the MT mapping. This goes back to your proposal of separating these two out, which I still think is the right thing to do. I just want to point out that it makes me a little nervous, because now we have to say there's got to be this retinal topic mapping between MT and V2, for example. Maybe there is, maybe it's not a problem.

I just throw that out as something to think about, but Monty doesn't care. You can do this in Monty. As long as we agree that's the requirement, those have to be co-located in some sense. I can look back into the literature. MT still has this kind of structured organization, but I'm not sure what the relationship is between columns that have feedforward and feedback connections with each other. They are co-located, but I'm sure there's some research on that. Don't worry about it too much. It's good to worry about it, but we have a lot of other stuff to do too. I think we've committed to this direction, so go ahead.

One other point I added is whether we need to associate date in the sequence. I'm not sure if it might be sufficient to just associate that the behavior exists at this location, but if we are in a specific morphological state, that tells us something about where in the behavior sequence we should be. This is not right after what we discussed, that this kind of global temporal signal wouldn't be able to tell us this, but maybe there needs to be some additional mechanism to communicate state in the sequence as well and associate that.

In Monty, this would be relatively easy because the status is part of the CMP message that gets sent to the parent learning module. Laying that down as another feature at that location and slice in that model shouldn't be too difficult, but in the brain, I'm not sure how it would be done. It's not clear to me this is bidirectional. In some sense, the behavioral models are a child of the morphology models, and it wasn't clear to me that's the right order.

Could it be that the morphology models are a child of the behavioral models? It's a little confusing to think about, but I can say, if I see the stapler, I would predict the movement. If I see a movement, that would be the feedback connection here. If I see a movement, would I necessarily say it's a stapler? If you see the movement, it's just the feature. Any kind of morphology model that has a hinge behavior would get that feature as evidence for that model, so we just bias this towards any object that has a hinge behavior. Maybe I was jumping ahead, trying to predict what the morphology would look like based on the behavior, and we didn't solve that problem.

That's 0.4. Okay, this is good.

At least in my mind, the morphology model would be the child and the behavior model would be the parent. The behavior model would only get input if something about the child object is changing—if the child object is rotating or moving in space, then that would become an input of the behavior model.

Lastly, we potentially want to add associative voting on ID. This is nothing we need to talk about more since we already talked about voting on, not ID, on stage. Is that right? That would be saying, I see, I'm looking at a stapler. That wouldn't tell me that—you're saying in general the stapler has this behavior. I'm not saying where it is, I'm just saying it would just bias, and it's just saying bias. Oh, staples. That would work well with this issue of maybe receptive fields aren't always co-located and things like that. You see a dog, and maybe the barking behavior is a bias, but it's not like you start hallucinating barking or something. You don't hallucinate it, you don't predict it, you don't even predict any details about it necessarily. Maybe some dogs you could, but in general, if a sound occurs, you'll be more likely to interpret it as a bark than as something else.

That's good.

The main thing for 0.3 would be to have some way to test this capability. We'd want to be able to assign different behaviors to locations on one object—basically, have objects that have different behaviors at different locations and orientations. Then we'd want to test seeing a static object and inferring the behavior, seeing a behavior and biasing which morphology object to recognize, and the state in the behavioral sequence can activate a state in the morphology model, and vice versa. If we're at the end of the hinge behavior, we would activate the open stapler morphology representation. On point B, we were just saying that you would see object, static object, and biasing behavior. There I was thinking of the feedback connections. We could solve it just with the feed forward and feedback connections, but if we don't always, then that would be seeing a static object at some location with it for behavior or something like that. Exactly, that would be the ideal solution. But like Niels mentioned, if we don't always have co-located receptive fields, then maybe we would need to add some associative voting in addition. If I take these words exactly, literally, then they need some more elaboration, because there are different ways you can interpret this. But generally, we need to test out these feed forward and feedback connections to see if they work. Just a question here: we can associate multiple behaviors to a single location with this mechanism. It seems reasonable to think that maybe some morphology can have multiple different behaviors. A body part, like a leg, could be jumping or flexing or stretching its toes, something like that. We should be able to do that. It would be biasing, because inferring implies one-to-one. This is basically what Jeff is saying also. In the general case, it biases. I can just change that to biasing. If there can be multiple behaviors at one location, then it's still biasing. But if there's just one behavior stored, then it should be predicting that one behavior. The example of the leg is a good one: you have a bunch of motion dots on different points of the leg, and then you could say there are lots of behaviors that all those dots would participate in.

Okay, so last slide.

I think this relates to what you mentioned, Jeff. If the morphology model is the child and the behavior model is the parent, what would the input represent?

We wanted to represent changes in location and orientation of the object recognized in the child column. If the child is the stapler column, it says, right, the stapler top is changing its location and orientation, and that change, relative to the behavior model, is sent as input to that behavior model. Does this feed forward make sense? You're saying it does.

What's an example of what this represents?

The child column represents a morphology model of the stapler top. They recognize the stapler top—not the whole stapler. The stapler top is changing its location and orientation, like it's rotating as the stapler is opening. Really, we should just say that the child object is representing a child of the parent object. Remember the staple top and staple bottom and the whole staple? That's a weird one because they have to figure that out. But we could just say there's a child object on the left and we recognize the child object. Is that okay to say? I think that's fine. Now we recognize a child object. Let's keep going forward. I see, and we would want to send that kind of rotation change that is being recognized of that stapler top to the behavior model that learns the behavior of the stapler.

For the orientation, that's relatively simple with the existing mechanism. We calculate the relative orientation change to the parent's column's reference frame and give that as input to layer four.

But then for location changes, I don't know how that would work because locations are represented as locations on the stapler model, not as locations of the object in the environment. Those locations change as the sensorimotor moves over the object, and what we would want to communicate to the parent column would be location changes of the object in the environment. This relates to what we talked about last week about having a sense of the object, but I'm really confused by this. If I just see a child object and then the child object changes—like it changes its orientation—it seems like the parent behavioral object would detect that as motion. So you mean this behavior model would just rely on rough sensorimotor input to detect that movement, maybe?

That's another potential answer: it just doesn't use that information. If I see a child—say, I'm looking at the logo on the coffee cup and the logo rotates or has some big change—the left column doesn't really note that change. It just notes, "Oh, there's a logo at this orientation. Now there's a logo at that orientation," because it's not looking at behaviors. The left object can't know; it can infer the logo at different orientations, but it doesn't see the motion at all. So it can't tell the other column it's moving. It can just say, "Hey, by the way, there's a logo at 30 degrees. Now there's a logo at 60 degrees," but the actual detection of the motion seems to have to be done on the right side, from more raw sensory data.

Maybe the smile could at least gate the attention of the sensorimotor to restrict it to that area, that movement. But if the column on the right is getting any kind of sensory input, it would detect a change on its own. It would just say, "I'm detecting changes." That's what that column is doing. I think this one can already do this without the other. I thought if we could use info from here, it could make it more robust and more based on the actual model and shape of the object. In some sense, we at least want some of that, or it feels intuitive that some of that change is communicated up and would be useful. Like you say, it gives more information here, but also, if we want movement to work in abstract spaces that cannot come from direct sensory input, presumably that is going to come from a representation in a low-level column changing in some way. But that could be from another behavioral model.

My recommendation is, I think the goal today is to get started implementing this. We need to do that. I would put this into the category of, "We don't really understand this too much, so let's get going." If we're lucky, it'll become obvious how this works, or it doesn't work, or we don't need to worry about it, or it will take care of itself along the way.

I can see this particular issue could take a couple of weeks of research meetings to discuss. It seems complicated. I agree we don't need it for a first implementation, but it's still an open question in my head if we want to allow for communicating object location at some point, because it seems useful in several applications. It's just something to keep in the back of our minds for future discussions.

When we did the compositional object research, we had a set of very specific problems—many were based on coffee cups, mugs, and logos. That probably didn't capture all the world of compositional structure, but it captured a good portion. So we said, "Let's go for it, we're going to implement that," and we knew we'd probably find places where it doesn't work in the future. I think the same thing will happen here. We'll have to pick a set of problems we want to solve for behavior, say, "This is a pretty good set, we can write a paper on this, it's going to work," and just move forward. I'm arguing that we should think about this problem, but not spend too much time on it as a group.

After this meeting, I'm going to go through the questions we discussed today and sort out the list of things to move to later discussions, so we have a record. For now, I have a concrete idea of any open questions that need to be answered before starting implementation. Some questions we have to answer now, some we can answer later. I think all the ones we need to answer now, I at least have an initial idea of how I would implement, and then we might run into issues and talk about them more. It's exciting to start building this. I'm excited to do this. The reason I'm pushing for structure is because once we start implementing, we realize a lot of things we're not thinking about when just conceptualizing. It's good to do some early prototypes to think about these issues. Compositional objects is already a mature idea, but we're still noticing issues. I'm not saying the whole team is going to start building object behaviors now—we still have a lot of work to do for composition objects—but I would at least like to start exploring some initial prototype of object behaviors. I see an analogy to what Tristan's been doing with our platform and breaking it. Here we are, wanting to incorporate all these changes, do behavioral learning and inference, and reduce the chance of breaking things later when we introduce new features. There's no clear answer to how to do that, but it's fun to see analogies.

Should I give a short summary now before we switch to Niels? Okay. I hope I'm centered. Done. Move your head. Perfect. All right. We talked about how we could concretely implement the current proposal for modeling object behaviors in Monty. Specifically, we went through four capabilities we want to add to Monty: learning object behaviors, recognizing object behaviors independent of morphology in different locations, rotations, and states, learning associations between behavior and morphology models, and compensating for object movement to make predictions in the morphology model. We didn't go into detail on number four and are not planning to implement that in the first prototype of object behaviors.

The proposal is to start with number one and number two for the initial implementation. We went through the concrete steps needed to implement learning and recognition of object behaviors and discussed open questions around that. Capability three, associating them, should naturally be available to Monty once we have modeling compositional objects implemented. Number four is another big topic, but we didn't go into that this week. We went through a lot of open questions; some will be figured out later as they're not relevant for the first prototype, but all the relevant ones have concrete answers now. I think we have all the information to start on a first prototype of object behaviors in Monty.

Conceptually, one thing that came out of the discussion is that morphology models and behavior models are both doing the same thing. Behavior models get incoming changes, and morphology models get incoming static features, but both can have concepts of state and model transitions between states. Time sits on top of that and can help inform as a global signal. All the columns are doing the same thing; behavior isn't really unique. Behavioral columns might need a different name.

Maybe we should just call them learning modules, with two types of input, and maybe there will be a third type. The beauty of the column architecture is that, in theory, it's one learning module that does everything. You give them different inputs, and they learn different things. We've always thought about different modalities, but here's another type: movement. It's not really a change at all—that's Viviane's argument. This simplifies the implementation because we barely have to change the learning module. We don't need specialized learning modules for behaviors. One quote I wrote down was that the neurons represent sequences without time, and they can move through those sequences at different speeds. That's what Niels said: we have a sequence of states and can move through them at different speeds or by applying actions. It's really just these different states of objects.

Did everyone else take those things away, or did you have any other takeaways, or were you unclear on those?

Sorry, Niels, this took a while. No, that's okay. I could also just record myself saying what I was going to say because it's not a big thing. There was a little thing that bothered me about the behavioral connectivity—just an observation about a potentially interesting anatomy thing. We've agreed that our meetings can go longer than two hours. Is there any reason we don't want to do it now? Cool. If anyone needs to take a small break, that's fine. I'll listen while I fill my coffee cup, but you get started. Don't wait for me. I'm listening.

Do you see my slides? Cool. What this is about is what felt like an asymmetry—not all columns are doing the same thing—that potentially arose when we were talking about behavior, particularly how you can apply a motion from a learned behavior to compensate in a model.

This comes from the idea that Viviane and Jeff had, and I don't want us to get into detail about this today because it's part of the can of worms: how do you correctly predict what you're going to see in this example on the left with the logos moving up and down? You've learned this behavior of motion up and down, and there was this idea that we could apply the motion to move us through this reference frame as a compensatory movement. That would enable us to predict what low-level feature to expect. I still think that's a good idea, and it was just this thought about the connection in terms of sending movement to the lower-level column in this hierarchical correlation. I'm not seeing your cursor. Which connection? The purple line. The purple connection. Specifically, I think it's the purple dashed line. There are so many things moving on this screen. I can't find your cursor. Do you see the flicking arrow here in the middle? Yes. Okay, that one.

but it wasn't so much the fact that we didn't have an anatomical connection for it that bothered me. As we often say, there are lots of connections and you can always find something. It was more this feeling that it seems specific to movement. It wasn't clear, if the higher-level column was a morphology model, what it would do with that connection. Now, suddenly behavioral columns and morphology columns are sending different types of connections, which potentially starts moving away from the idea that all columns are the same—Mountcastle, etc. So, just a simple observation.

This is just showing our usual connectivity. We have a behavioral column, a morphology column, and, of course, the feedback has these local synapses in layer 6, which predict the specific location. This simple observation, for which I'll show anatomical evidence in a moment, is that often these synapses also go into layer 5. I think you've mentioned, Jeff, that maybe we have cells in layer 5 that are the equivalent of the double bouquet cells, but for movement vectors.

If they were to synapse on those, it could be a way for a hierarchical connection to provide movement through the space of this reference frame.

Importantly, this is the same projection—at least, that's what the evidence suggests. This isn't a totally new connection. This projection going up through the column could choose to form synapses on these double bouquet cells or in the reference frame, depending on what the parent column is trying to predict in the lower-level one. That would mean all columns, when they send hierarchical feedback connections, go through the column and up to layer 1. The difference is that, depending on what is being learned, they would primarily form synapses in layer 5 to predict particular movements, or in layer 6 to predict particular locations. But they all have this same core hardware or connectivity. 

This is just showing some diagrams from Rockland's papers, where many show this local connectivity. This long connection here is the feedback axon projecting up through a column. At the top, you see this widespread layer 1 connectivity, and you see the same thing here, but lower down in layer 5, you see these more local projections. When he writes about it, he comments that it's layers 1, 5, and 6—not just 6.

This summarizes that they can form in layer 5 or 6 as necessary, and maybe it would fit with the double bouquet cells. We can think of probably half a dozen other things that this layer 5 connection could be doing. It could be involved in goal states, action policy, all kinds of things. I'm not saying this is clearly what it's doing, but at least this addresses something that had been bothering me about how this would work anatomically. I felt like this maybe helps a little with that.

So, are you saying if the higher-level column is a behavior model, the idea is that it would synapse in layer 5, but if it's a morphology model in a hierarchy, it would synapse in layer 6? Yes, and that would be driven in a learning-dependent way. It's not predefined; it's more that if there's a lot of movement and this one's picking up movement, it can be more predictive by forming that synapse in layer 5.

Something like that.

I have a couple of thoughts on that. Do you want me to show slides again? Let's leave that one up there.

This one? Yes. I was lost. I remember we talked about this, but I got lost. You don't need to explain it now. This is so complicated, I can't get my head around it. I definitely think moving around—oh God. I think this is a complicated picture of the idea you also proposed about having the behavior model send a movement down to the morphology model. I can't remember, Niels, what you didn't like about it, but I have a general answer to the question—a biological answer.

My interpretation of biology is as follows. The genes specify much about the overall structure of the brain, including which cells exist in different layers and, to some extent, their destinations, but they don't specify any particular synapse. In the cortex, unlike in some other animals like sea snails where every synapse is specified, there isn't enough genetic material for that, and there's no evidence it's happening. All synapses are essentially associatively learned; you could almost say all synapses are Hebbian learned, Hebbian association.

Your proposal is that this axon rises up through all these layers. My interpretation is that the axon will form synapses with anything it can associate with. It doesn't know where it is, and as it rises, if it encounters other axons or dendrites nearby that can be learned to form a pairing of synaptic connections, it will do so. This is for excitatory cells; some inhibitory cells are a bit different. So your proposal that it could form in layer five or layer six, depending, seems totally reasonable to me. The reason it doesn't form synapses in layer four is because whatever is going on in layer four doesn't associate well with what's going on in that axon. Otherwise, it would. It's not predetermined; it's very opportunistic. Nobody is telling it where to go, though genetics say, "Go up here and spread out." Even how much it spreads is determined by associative connections. It's shown repeatedly that axons and dendrites will continually grow in different directions, and if they form synapses, they continue growing; if not, they retract and go in another direction. They're just looking for things to connect to that are associated or paired.

If this solution solves the problem you were dealing with in the first diagram to your satisfaction, I'm totally good with it. Whatever the solution, it seems reasonable.

To paraphrase the problem again, it was about understanding this biologically. We hadn't discussed exactly what connection is providing this movement information from the higher-level column. It had been suggested that maybe it's a totally different connection, like one we haven't discussed before—maybe L5 sends a different kind of top-down connection. The reason that made me uncomfortable wasn't just because we were adding a connection; there are lots of connections in the brain we don't explicitly talk about. It was more the feeling that it was asymmetric, specific to the relationship between a behavioral and a morphology column, which was breaking the symmetry. Morphology columns don't send that to others—why not? That's an interesting question for Monty. What does Monty do about this? Do we handle it similarly to biology and say this is a signal and we figure out which one it connects to? Or do we do something else? I don't know. It's an interesting point.

In terms of Monty, if we say the brain can connect to different layers depending on where it's most useful and predictive, then it's fair for Monty to interpret them in the most useful way as well. The main thing is that we don't do synaptic learning in Monty.

We would have to explicitly say these things can form associations, whereas right now we only do certain things. How would you know whether to learn it or not? Monty might need a Hebbian equivalent, but we don't have that right now. We say these things should be paired, but not right now.

On the general proposal, the reason I never mapped it onto the feedback connections is because I thought of those as general movement factors, not specific to any object. It wouldn't make sense for them to be the same as the context signal that goes to layer one, or for them to be object-specific. They would likely need reference frame transformations applied to them, so the reference frame might be an issue. The double bouquet cells are also non-specific to a particular object, so I get the general movement vector. Synapsing to those makes sense, but the same axon also goes up to layer one, which is supposed to carry information about the object ID.

I just don't think that layer one would benefit from information about a general movement vector.

That's a biasing signal, so that's okay. It's the stapler behavior: you need to do this movement to compensate for what you'll see. You're also potentially going to be seeing a stapler, but you're not going to hallucinate a stapler. I'm going to bias that, but it's not saying anything about staplers. They're just saying, apply movement in this direction. It doesn't have anything else. The L1 connectivity could be biasing the ID of the stapler, but it's just one movement vector stored in the stapler model, not the whole hinge behavior we're sending down there. It's just one movement vector, which is movement in one direction, and that can change if the orientation of the stapler changes. There are only so many directions, and all of the directions can be on all of the objects if the objects are in different orientations. I don't see how we could learn any useful associations from that.

Isn't this all under category four from your earlier presentation? Is that right? Yes, and we said we're not going to do that. We don't have to proceed further. I actually have an alternative or an additional proposal to this mechanism, but that's definitely too much for today.

Good point, Niels, and I agree with your points, Viviane, especially about the orientation. I like the reminder that they also go to layer five and can form wherever it's most useful. It might still be something useful to keep in mind for other things like goals.

That's me done.

So, we could spend weeks talking about number four if we want to, and voting. Are we about done here? I think so.

Can I just throw out the idea that I'm working on? I don't have any images or anything like that. It would be great to also prepare for next week's meeting. There's a problem of learning: we require all these learning models to learn the same models, but they don't all experience everything. That was true in the morphology models, and it's doubly true now in the behavioral models.

There are only a couple of ways this could get solved. How do these different learning modules all learn the same thing, even though they're not exposed to it? One way is to use hierarchy: all the learning modules don't learn all things, but once you get higher up in the hierarchy, there's modality independence, and you can learn up there and then feed back down. The other possibility is that there is some sort of local sharing between learning modules.

The basic idea I'm working on right now, and I have no idea if this will pan out, is that all learning modules, or some large set of them, even if they're not receiving anything—so I'm a learning module not getting input—I might be fed the movement vectors of nearby learning modules that are actually sensing something. I could move through a reference frame space. I'm a learning model that doesn't get any input, just sitting there, but now I'm being fed movement vectors. As if I'm modeling something, I'll move through some space and essentially model an equivalent space to the learning module that is learning something. I would also have to be told what the other learning module is learning, so I can learn the same thing. I'm trying to figure out mechanisms by which two or many learning models are near each other—one getting input, the others not—so they could share and train the nearby ones. That may be sufficient, but it would certainly help a lot, and it may be required to have this hierarchical method as well.

I'm working on the idea: is it possible that input-null learning modules are actually learning all the time, just getting input from someone else? I'll see if I can make that work.

Are you thinking of this mostly for touch, or are you thinking with vision, that columns receiving non-foveal input might get higher information? Remember, in vision, most columns—all the inputs are center-surround, right?

If I am a column that's not getting sent—if I'm looking at a blank space, it could be blue or green, it doesn't really matter—I'm not going to get any input. There's no center-surround activity. I could be looking at the object, but I wouldn't be tracking my location on the object. I would just be saying, I don't see any input because there's no center-surround activity.

Good point. I forgot about that. I was thinking all of them get input, but they're receiving axons, and most of those axons aren't active.

That's the idea I'm working on.

Sounds interesting.

Okay, that's it.