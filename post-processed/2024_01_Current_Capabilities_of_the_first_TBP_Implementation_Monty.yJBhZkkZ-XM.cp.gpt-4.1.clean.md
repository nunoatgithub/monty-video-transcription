I volunteered to give a presentation today about Monty's current capabilities. I think it will be a good reminder for all of us and a helpful overview for anyone new, to understand what Monty can and cannot currently do. This way, we have realistic expectations, know what we're discussing, and can identify the next steps, including where we have solutions and where we don't yet.

I put these slides together today, and I hope they won't be too confusing. There's a lot of content—three years' worth—condensed into a one-hour presentation. Please interrupt me if you're missing context or not following, but this should provide a high-level overview. I'm not going into the details of how the algorithm works; for that, we have meeting recordings and new documentation. This presentation is focused on capabilities.

Let me share my screen. Can you see the first slide? Great. So, what Monty currently can and cannot do.

First, very briefly, what is Monty? It's not a magical wizard system, but it is like a wizard in the sense that it's a sensorimotor modeling system. You have a motor system with actuators, sensors that move through the world, and a brain that processes all the sensory information. We can have different types of sensors and different types of movement through space. For example, in our current implementation, we have a vision agent, which is fixed in space but the camera can tilt up, down, left, and right to explore. We also have a touch agent, like a finger touching an object, which always moves perpendicular to the surface and can move along the surface of a cup to explore it.

Monty has a modular structure, and no sensor sees everything. We have sensor modules and learning modules. Unlike classical computer vision, where you get one picture of the entire object and recognize it, here we have small sensor patches that move along the object and recognize it through successive movements. The system never sees the whole object at once; that's just for visualization. It receives a small sensor patch as input, along with movement information. You can think of the sensor patch as a small area on your retina or a patch of skin on your finger. A good analogy is looking through a straw. This is a shift in thinking if you come from classical computer vision.

Models also use reference frames. We get motor input and have motor output since it's a sensorimotor system, and the models are optimized for that kind of data. They use reference frames to structure incoming data and are loosely modeled after cortical columns. You can think of a learning module as a cortical column, where sensory and motor input are integrated. We have a way to maintain a reference frame and store how features are located relative to each other in space. There's also a cortical messaging protocol, a common agreement on how messages are sent between modules. When learning modules communicate, it's all in a common language, regardless of the sensory input type. A column receiving touch input can talk to one receiving vision input, or to a higher-level module, all using the unified protocol. This makes it easy to plug and play different modules and enables cross-modality information exchange. That's the only slide on the basic principles.

Now, let's get to the capabilities.

We have a project overview spreadsheet that provides a high-level view of the project, major tasks, and ideas. Monty is split into components like sensor modules, learning modules, motor system, voting, hierarchy, environment, and capabilities, which is what I'll focus on here.

Whenever we complete a task and check it off, it adds progress to these capabilities. We can see where we've put in a lot of work, where we haven't done much yet, and what we have planned. This presentation is structured along these lines. I'll go through each capability, explain where we're at, what we've done, and what we can currently do. None of the capabilities are at 100% yet; everything can still be improved. This is a huge project, and we're not close to having a general system that can do all these things perfectly.

I'll go through them one by one.

Object detection: as a brief disclaimer, object detection currently works through evidence-based learning. If you look at the code, it tracks how much evidence we have for each hypothesis. As we move through space, after each movement, we take the sensation from the sensor patch and use it to update the evidence for different locations on different objects. As we move along the cup, we narrow down the possibilities. Blue indicates low evidence, red indicates high evidence. It's already clear we're not on the banana, dice, or bowl, but there's still high evidence for areas around the cup. We're not sure exactly where yet because the cup is symmetrical. With more movements, we can narrow down the location on the cup. Once we reach the handle, we can be certain where we are. You can also see how much evidence we've accumulated and what rotation we think the cup is in, because the system recognizes not only which object we're on, but also where on the object and how the object is rotated.

So it says the current most likely hypothesis is the one with rotation. Are the correct options learned, or is it trying to choose from a set of correct options? It has learned about the coffee mug and has an internal model of it, but it could infer any rotation of the mug. When it first senses the coffee mug, it initializes hypotheses about its possible rotations given that sensation. As it moves, it narrows down those possibilities. 

Could it learn "cup" or "cylindrical" instead of "mug"? In classification, is it trying to choose from one of the correct 20 answers? In many experiments, we use the YCB dataset, which has models of 77 objects in memory. The system has seen all these objects before and learned models of them. When it senses a new object and tries to recognize it, it considers all 77 objects plus any possible rotation. It then narrows down which object and rotation it might be.

At the moment, there's a discrete list of 77 objects. Some recent implementations start to capture similarities, like different cups being more similar. We have ideas for consolidating that knowledge, so you could imagine developing a cylinder representation that would come to mind, rather than always identifying an exact cup.

It's gotten complicated because there seems to be the ability to recognize objects using different sets of attributes. Sometimes we call it the morphology of an object, which can be independent of its actual features. For example, you can have a face made of shrooms and still associate it with a face, even though the features aren't typical face features. This leads to the problem of classification of categories—what is a face? Does it have to have eyes, a nose, and features in the right position? We've spent a lot of time thinking about these issues in object modeling. It's not just a unified thing or a single model. When the system is working, it can have evidence for the orientation of a feature and separate evidence for what the feature actually is, and the models have to work in this gray zone between them. We haven't quite figured out how to represent different classes of objects. We have a lot of ideas and have spent a lot of time on it, but it's still a challenge because we can clearly recognize things that are distorted or different and still know what they are. The system has to be able to do that.

If we have a Monty robot, there are more than 77 objects in the world. It needs to learn these objects. Viviane didn't talk about that, but you learn objects through the same method of sensation and movement, so it can learn more than 77. It can learn as many objects as you want. There is an issue of capacity for an individual cortical column or learning module, which is another complicated topic. 

Two quick asides on learning: it does well with continual learning and is very sample efficient. Unlike classic deep learning, where you might need to revisit an object thousands or millions of times in batches to avoid forgetting, you can just add a 78th object and learn it after a few passes, similar to a human, and start using that knowledge. There is no separate training phase versus inference phase; you can mix them and continue to learn as you go. All the measures we have don't really address learning and sample efficiency, but that's one of the system's big strengths: you can learn a new object very quickly without forgetting anything about already known objects.

All right, now some numbers on object detection. Here, we have 77 objects in memory that we've already learned, and now we're trying to recognize them. The system performs fairly well—not perfect—but in blue is the distant agent, which is like an eye fixed in position that tilts up, down, left, and right. It's a little worse than the surface agent, which is like a finger moving along the surface. The main reason is that with the fixed agent, there can be quite ambiguous views of objects. The surface agent has more flexibility, moving completely around the object and getting the whole size and shape more quickly.

When we add noise and test these objects in random, previously unseen rotations, performance gets a bit worse. It gets less worse with the surface agent, since it can move around the whole object quickly. All of these numbers are with one learning module and one sensor. If we use five sensors and five learning modules and let them vote with each other—again, with noise and random rotations—performance is a little better than with just one. I'll show some numbers later, because voting is really about cutting down on the number of steps. That's mostly what voting does: it lets you infer through your vote.

To give more insight into these numbers, we don't just have right and wrong as potential results of an episode. We can also have the correct most likely hypothesis. That means we've moved along the object, but we have multiple potential hypotheses and didn't get enough evidence to make a certain decision about which object it is or what rotation it's in within the given maximum number of steps. To keep experiments in a reasonable timeframe, we have a maximum number of steps, and after that, we stop and look at the most likely hypothesis the system had. This is the count of where the most likely hypothesis was correct, and this is where it was the wrong object. It can also happen that we actually classified an object, but it was the wrong object, not the one we saw.

Here are some examples of when it confused objects. For example, it might confuse a spatula for a spoon, a fork for a knife, or a toy airplane for a B toy airplane—objects that are similar in morphology. When it has the most likely hypothesis confused, it might confuse a Phillips screwdriver for a flat screwdriver, but both were still in the hypothesis space. If we had moved more, it might have figured it out and corrected itself.

Are you going to talk about action policies, Viviane? Yes, briefly. It might be worth mentioning that for all of these main experiments, including the 77 objects, the distant agent can still go to other sides of the objects because there's a policy called the hypothesis-driven policy. The learning module has its internal model of what it thinks is the most likely object, and, for example, another model of another likely object.

Just in case you're watching this video later, regarding the difference in accuracy between the distant agent and the surface agent, there are also subtle differences in how they move. Even though they can go to different parts of the object, the distant one is more of a random walk, whereas the surface one moves more purposefully along the surface. That small difference could be due to not converging quickly, making it more likely to accumulate noise and similar issues.

At the start, we didn't have any kind of sophisticated action policy; we just went along. If you were presented with two screwdrivers and wanted to know which was which, you would immediately know where to look—the end of the screwdriver—and attend to that point, whether with your finger or visually. There's a more intelligent action policy to differentiate objects, but in the beginning, we didn't do any of that. I'm not sure where we ended up, but we do have more sophisticated action policies now than we started with. In the beginning, it was all action policy. A big part of differentiating certain objects is knowing where to look to tell the difference, and we do that all the time.

Speaking of action policies, I had a question while reading that TBP document. I didn't go through the whole thing yet, just the first part, where I think we do a good job distinguishing Monty as not your classical computer vision or supply sort of problem.

I was wondering how different it is from RL, especially since we're talking about policies as well.

Are we learning policies? Will Monty learn, for example, if I have these two screwdrivers, will it learn on its own? Yes, it'll be a mix. The policies we have right now can be considered innate policies, but we'll definitely have elements of reinforcement learning as well. Even now, we have policies where there's learning involved. We have model-free policies, where you just get the sensory input and that decides how you move next. Then we have model-based policies, called top-down policies, where you learn the model of objects and use that learned model to decide the next action. We're not saying that reinforcement learning is at odds with the system. For example, you could use reinforcement learning for the motor system and train a reinforcement learning agent for that part. Here, we're setting the framework for it, where we also have models that we learn using reference frames. We're not using deep reinforcement learning, which is not very sample efficient. There's a tendency for people to think about Monty in terms of what they've learned about machine learning in the past, and almost always that's wrong. You have to rethink things: the sample efficiency, the reference frame, the integration of movement—it's really different. You have to be careful. The hardest thing to be successful with Monty is to realize that we don't want to be constrained by the way we thought about things in the past. It doesn't mean the brain doesn't do reinforcement, but you're not going to just drop it in and solve the problem with that.

One question: when you do these object detection tasks, are we looking at a fixed number of steps, or is it allowed to terminate itself once it gets to a certain confidence level? What if the object is brand new and looks 90 percent like another object but has a new distinguishing feature? How do we prevent it from terminating too early before it gets to that new feature? If we're looking at a new object that has to learn, there is a terminal condition that says, "This is an object I don't know about." It does not force a choice between the 77 objects; it can say, "This is a new object," and then it would learn about that object and create a new model. Next time, it should be able to recognize it. But if, as you say, you're moving along the object and it all matches with a known object and you never get to that different distinguishing feature, then we would recognize it as that known object.

It will be difficult to prevent that. If you've never observed it, how would you know? For example, if I grab these headphones and they've been swapped out slightly, maybe the first grab I'm satisfied these are my headphones, but if I grab the other side, I feel something brand new. In your old headphones, if you've explored the entire object and what's on the backside, then you'd see something's wrong. You'd have a model, and if it's an incorrect prediction, you'd realize that's not right. But if you've never observed the backside, how would you know? At the moment, we don't have a condition to handle that. There's a lot we want to improve about the unsupervised learning aspect, in terms of how you naturally add more objects. There's also the issue of learning in a multi-object environment and knowing when you're on a different object. All of these are related to concepts like prediction error and surprise—thinking you know what object it is, then realizing something is very wrong and needing to take a step back. It's not something we've implemented yet, but I get into a lot of these topics in a moment. For the sake of getting through the presentation, I'll move on.

One quick thing about the policies: although many of the policies we have now are intrinsic and designed by us, they would also apply in more abstract spaces. For example, following principal curvature on an object can have parallels in abstract spaces, as can hypothesis testing. We're not just going to design all policies for recognizing physical objects; this is a long-term plan. I don't want people to see Monty and think it's just another RL system—it's not. But to your point, we can emphasize that better.

To be clear, none of these results use deep learning or reinforcement learning, even though we could use them in places, but we don't need to. Let me show you the results from the best and worst cases. This is the distant agent with noise and random rotations. Here, there are more cases where we reach a timeout condition—it's just not confident enough to terminate the episode and say, "I'm on this object." Most of the time, it still has the correct, most likely hypothesis. There are a couple of objects it's confused about, but all the ones where it's confused are very similar, either in morphology or color. We shouldn't view this as a failure of the system to recognize; those 40 steps are all local on the object, and you're not going to figure things out. It goes back to the action policy and how you efficiently sample the object completely. There's a lot going on there, so these incorrect results are natural, given the way we set up the action policies and sampling.

Definitely. That's why I'm saying there are many areas where we can improve, and we have concrete ideas on how to do it. We just need the time and resources to implement these things. 

Hey Vivien, did you run an experiment with an unbounded number of steps to see what the correct percentage was? We did, but I don't have the results on hand right now. It would be easy to rerun if you're interested.

Now, pose detection. This happens at the same time as object detection—it's actually the same process. This is the rotation error in degrees. To give you an idea of what those mean, I put these little icons here. If that was the correct rotation up here, then this is what it would have detected on average.

It is again better for the surface agent than for the distant agent, and better without noise than with noise. All of these are reasonable. The five learning module condition is not very good, but that's another action item we have on the agenda. Voting right now is not happening on pose, only on object ID. Once we incorporate that, it should help a lot. Another note is that if we look at the distribution of rotation errors, it's a bit of a bimodal distribution. There's a spike at 180-degree rotation error, which is essentially mirror symmetry. Many objects in the dataset are symmetric along one or two axes, like cups and dice. We have symmetry detection in the algorithm and account for it, but it doesn't always catch it. In those cases, we get a high rotation error, which skews the results. 

One useful thing when thinking about these experiments is to imagine the surface agent as sticking your finger into a black box and touching something. If the cup is completely upside down, you might at first think it's just the cup, and it takes more effort to realize it's inverted. Until you actually touch the handle or the top, you don't know. You can always relate this back to personal experience, which is helpful when trying to figure out what's going on. 

Here you see another example of the symmetry case. We have the target, this dice at rotation 90, 0, 180, but it detects 90, 0, 0. If you look at the image, it's almost identical. There's some artifact down here that might distinguish it, but it detects this rotation and a symmetrical object, so we count that as correct. It's good that the system doesn't overfit to these small artifacts on the dice.

Next is the number of steps—how fast do we recognize it? There are two approaches to cut down on the number of steps: voting and policy. Voting means we have a bunch of patches that all move together in the same way. They could move independently, like five fingers on a hand, but in our case, they move together over the cup. Each patch feeds into a different sensor module, and each sensor module feeds into a different learning module. Since they all get different sensory input, they might have different hypotheses about the object or its pose. They vote with each other and can narrow down those hypotheses much faster together. If you look at the numbers, it cuts down the number of steps required by more than half and slightly increases accuracy. We're not voting on pose here yet, but that's something we need to add. It's tricky, and we don't know how it's done in the brain, but we know it has to be done.

The second way to improve the number of steps is policy, as mentioned before. For example, the touch agent has a policy that follows principal curvature, then changes to follow a different principal curvature, and at some point changes again to follow the orthogonal direction, and so on. It's more directional about where it goes and explores the object more efficiently than moving randomly on the surface. That helps a lot, as Nils mentioned earlier.

Maybe it's worth quickly defining principal curvature. If you imagine the side of a cup or cylinder, the maximum magnitude of curvature is going around the cup, and the orthogonal direction is the second principal curvature, which would be flat in a cup. On this cup, the flat one is in this direction, and the curved one is in this direction. Principal curvatures are always orthogonal to each other and span a reference frame together with the point normal, which always points out of the surface.

The second type of policy is the top-down or model-based policy. These are based on the models learned in the learning module. One we use is hypothesis-driven jumps. For example, we have two hypotheses—a fork and a knife in certain rotations. We look at where to move to distinguish these two objects, which would be the red point, and then jump with the sensor to that location and sense there. 

Here's an example: we first follow the principal curvature, then jump to the top of the cutlery since the handle is not very distinguishing. That should narrow down our possibilities a lot. With five learning modules in a distant agent, we can also jump to different locations on the object.

I haven't seen that, it's good. Niels made those. That again cuts down significantly on the number of steps needed to recognize an object.

It seems like you would start with a non-model-driven action policy. You have some hypotheses, and then you switch to a top-down one. Exactly. That's what's happening here. First, we follow the principal curvature, which is not model-driven—it's just based on the sensory inputs. Once we have some hypotheses, we do a jump. Imagine the finger of the box has no idea what it's touching. You follow some edges and think, "That might be this," and you make a jump to a hypothesis to see if it is correct.

It's also interesting in this animation that you have high confidence about the rotation of the object. You're not sure what it is yet, but you're very sure it's lying along this plane. Your hypotheses are almost overlapping, which gives you greater confidence about how to use the hypothesis to move around the object. With symmetry, it will sometimes fail in that it might suggest going to the other side of the cutlery, but it's not really a genuine failure. It just can't know until it tries to sense one of the tops, to determine which way it's actually oriented in space. At the moment, when we go to an empty point in space, that doesn't update our hypotheses. Instead, we just make sure we move back onto the object, but it would be very natural to integrate that into the system so that sampling empty space tells you more about the orientation of the object and similar properties.

It's very useful to know where the object is not.

That's the nice thing—it comes naturally out of the system and how we do things, so these hypotheses will align with each other. The models for fork and knife might have been learned in totally different orientations, but once we use the hypotheses about how they're oriented, they will be automatically aligned, and we know where the most distinguishing features would be.

Now, on speed and efficiency: that's related to the number of steps, since fewer steps mean it takes less time to recognize the object, but this is really more about how long each individual step and hypothesis update takes. I won't go through these in too much detail because those are older numbers. I just pulled together slides from the past two years. We've already done a lot on speedups, including vectorizing a lot of operations, testing only the top K hypotheses, using faster libraries on our infrastructure, and using multiprocessing and multithreading. That helped us get a lot of speedups already. 

This is the current setup: we vectorize the evidence update, so those are all matrix multiplications and operations. We use multithreading for the loop over learning modules, for the loop over objects, and for memories. The loop over learning modules could be parallelized but is not at the moment. You'll see in the speed numbers that's one of the bottlenecks if we use more than one learning module. We also parallelize episodes in an experiment during evaluation. During training, we can't do that because when you're learning and constantly adding new information to your models, the sequence matters, so we can't do that in parallel during training.

At the moment, we tend to have a lot of objects—up to 77—whereas we mostly have one learning module, maybe five. That was the reason for choosing to parallelize over objects, but that might change as we add more learning modules, and sorting that out will become more important.

We also analyzed what the slowest operations are now. Currently, the slowest thing is a KD tree query operation. If we have a hypothesis of where we are on the object, that location usually doesn't exactly correspond to a location where we have stored information in the object's model, so we have to do a nearest neighbor search to find the nearest points in the graph storage. That search takes the most time. We've looked into and tried alternatives but haven't found anything faster so far. 

This is an example where it's interesting—I think I know how neurons do this, and it's not a separate step. It's very efficient and clever, but it requires neurons to do it using synapses that are locational on the dendrite, which is a completely different mechanism. Here, we're not trying to emulate neurons exactly in the details, but when we don't, sometimes we run into issues like this. This might be something we could solve with a more biological implementation, but that would require adding a lot of other biological details. That's one of the big challenges: knowing where to put the dividing line between neurally constrained solutions—how neurons do this—and not wanting to emulate everything, but also not knowing exactly where to draw that line.

The second slowest operations are just matrix operations combined. We've already managed to cut down a lot of this time by only testing or updating the most likely hypotheses, making the matrices much smaller and faster.

Here are some numbers: runtime per step with 77 objects in memory. Many of our smaller benchmarks use only 10 objects in memory, which is faster. This was on 16 CPU cores. If you use more, you can make it faster; if you use fewer or run on a laptop, it's also faster. Under this setting, just to get some relative numbers, it takes around two seconds per step. It takes about five times that for five learning modules, because we don't parallelize the updates of learning modules yet.

If we take the total episode runtime—the total time until the object is recognized—that difference isn't five times anymore, because five learning modules take fewer steps to converge and recognize the object faster. It takes about two to six or seven minutes. The left chart is seconds, the right chart is minutes. It's still not as fast as we would like, but these are the current numbers.

Noise can be injected in two main places: onto the raw data going into the sensor module, or onto the data going from the sensor module into the learning module. A third option is injecting noise directly into the models. Injecting noise before the sensor module tests the robustness of the sensor module, while injecting noise after allows for more controlled testing of the learning module's robustness.

For example, adding noise before the sensor module could mean adding noise to the RGBD image, which skews estimates like curvature and point normals. Adding noise after the sensor module allows for more targeted noise on specific features, such as location or color, and varying the amount of noise on different features. The typical amount of noise added to locations in previous experiment results represents the noisy conditions.

To address raw noise, we implemented more robust calculations for point normals, thanks to work by intern Jack last year. This led to significant improvement using more robust point normal estimates. Another outcome is the system's ability to switch between different modalities. For example, we can learn an object with a touch sensor, then unplug that sensor module, plug in a vision sensor, and still recognize the objects with similar performance—slightly higher rotation error, but about the same number of steps needed.

Our sensors are quite similar; the main difference lies in the action policies. I was surprised and delighted that this works. It's remarkable that you can plug in a different sensor and still achieve recognition. I am not aware of anything in deep learning that can do this plug-and-play style. This is a major strength of the system: you can swap sensors as long as they can estimate movements in space, and generalize the morphology of objects. Of course, if you switch from a vision sensor that detects color to a touch sensor, you lose color information and can't distinguish, for example, a blue cup from a red cup, but that's unavoidable. The features that remain can be generalized easily.

Additionally, cross-modality voting and communication are straightforward. In most deep learning solutions, you have different subnetworks that need to be coordinated and merged into a mixed vector or embedding. Here, all modules use the cortical messaging protocol, making integration simple. This concept traces back to neuroscience, where Mountcastle and others observed a common structure repeated throughout the cortex, with the same processing occurring everywhere in the hierarchy. Many neuroscientists doubted this, but the key is to accept it as true and then figure out how it works. The essential elements are that each module is sensorimotor, knows about movement and reference frames, and uses voting. All these aspects work together to enable this functionality.

Looking ahead, people will build these modules in silicon, making them identical and able to connect to various sensors in different ways, just as seen in biology. Mammals have different types of sensors, but they all feed into the same cortical learning algorithm and function effectively, even with different sensors and sensor types. We need to accept this and focus on understanding how it works.

Is the difficulty in accelerating the nearest neighbor search related to attempts at locality sensitive hashing?

Yes, I did try locality sensitive hashing. I don't recall exactly what the problem was; I'd have to check my notes. It does create a dimensional reduction, which helps jump to likely candidates more quickly, but sometimes multiple hashes are needed for enough specificity. I remember there was an issue—either learning became much slower or something similar. There was a problem when we looked into it, but I don't remember the details. If you're using KD trees now, it seems like it could be a swap-in, but that's just my impression from your presentation. I'll check my notes to see what the problem was. I'd be interested to know, since we're considering it as a possible accelerant, and if Monty encountered inherent problems, it would be good to know. I'll look into it.

Let me move on. Unsupervised learning, as mentioned earlier, involves learning and inference as an intertwined process. Although many of our current experiments are just evaluations, we envision the system eventually working continuously—learning, recognizing, learning, recognizing. To do any kind of learning, inference must happen at the same time. For example, as you move along the cup, you recognize it as a cup, then explore the object further and add new observations to your existing model. You keep learning and adding to your models, and we want to continually update our models of objects.

Here's a more concrete example: we have a model of a banana, recognize the banana, explore it further, and add new points to it, ensuring we don't add redundant points. That's the basic idea, and we have a couple of experiments in our benchmark test suite on this. We don't get great performance here yet, and I think this relates to what Scott mentioned: if we have very few objects in memory, like just one or two, it's easy for the system to confuse objects—they are not disentangled yet. For example, it might merge a model for a mug and a cup, or a model for a strawberry and a golf ball, or a fork, knife, and spoon object. Once you have this kind of merged object, all following episodes recognize these merged objects and add points to them. This is partly a symptom of the dataset or benchmark as well; we want the mug and cup to be merged at some levels of the system, and that's something we're working on. In some ways, it's good that it confuses or merges similar objects. We'll need to separate the fork at some point if it's important.

What we really need is a hierarchical dataset where we can test recognition at different granularities: the most precise object (fork versus spoon), a broader category (piece of cutlery), or an even broader one (cylindrical object). This probably boils down to a learning parameter—how high to set the threshold for recognizing a new object. That might need to get higher or lower over time as more objects are stored in memory. If you only know one object so far, it's likely the next thing is a new object, not just that object again.

This is not an unsolvable problem and is actually a desirable property that the system merges these objects when they are correctly oriented and aligned. Currently, it only uses bottom-up signals to detect when it's on a new object; there's no scene-level understanding or object permanence yet. We could add that with a hierarchy, so it understands when it's moving away and encountering a different object.

Talking about multiple objects, this brings us to newer capabilities that are less developed. Niels implemented a multi-object environment, which we've been testing by placing several objects in an empty space. We now have more evaluation criteria, such as having a primary target object to start on, then moving to a different object and recognizing it, or returning to the primary target. These are results from last year, showing that we can use shifts in evidence when moving from one object to another to detect that transition and use it to return to the object or reset our hypothesis space. These are initial results, and Niels is continuing this work, though it's not yet integrated into the current code. This is work in progress.

Categories and generalization are also important. In one experiment, I took the memory of 77 objects, deleted one (for example, the cup), and observed what object was recognized instead. Without the cup, the system would recognize the red cup instead, or a mug, or instead of an orange, a peach, or instead of a spoon, a knife. It's almost always the most similar object remaining in the dataset.

There's much more to do on categories and generalization, but first we need a new dataset with object categories and new testing measures. Another experiment involves clustering the internal representations of learning modules, which results in clusters that express object similarity, both in morphology and color space. This plot is from Rami. Using t-SNE to map all objects into two-dimensional space, you can see meaningful clusters of cups and ball-shaped objects.

We've also implemented, but still need to test, adding more constraint graphs that update over time. Through these constraints, the system can learn more generic objects, focusing on features that are consistently present—like learning a generic cup shape versus a specific cup with a dent in the right corner.

Then compositionality is another area we've brainstormed extensively over the past year, and we have many concrete ideas for implementation. We have the basic routing implemented and some initial Monty setups and experiments, but we don't yet have a good dataset to test it. Rami's internship work tied into this, encoding object IDs with a similarity measure so the higher-level learning module can meaningfully interpret these features. This was also our hackathon project in the summer, where we assembled a scene dataset to test compositional scenes with lower-level objects—fork, spoon, knife, plate—and higher-level scenes composed of these objects arranged relative to each other. The next step is to turn this into a benchmark experiment, add performance measures, and test how well our current implementation performs.

Deformations are something we currently have no solution for. It's a recurring topic, but we don't have any promising leads on how to implement or handle this. At the moment, all recognized objects are solid, not deformable like a t-shirt that can change shape in many ways. This is an area we need to consider further.

Different features can exist on the same morphology. For example, a cup can have different colors, patterns, or logos. Similarly, the same feature can appear on different morphologies—a Numenta logo on a cup, a squishy brain, or a baseball cap—and we can recognize it on all of these. We've brainstormed this recently and have some ideas, but nothing implemented yet.

Scale is another challenge. You can have a children's chair, an adult's chair, or, in Germany, a furniture store with a giant chair as its trademark. Despite the size difference, recognition is not an issue for humans. Currently, our system cannot handle this. We have some ideas about how the brain manages this, but we haven't translated them into an implementation.

Regarding real-world sensors and agents, we briefly explored this during a hackathon about a year and a half ago. We used the iPad camera and wrote an iOS app to take pictures with the iPad's depth sensor. A patch would move over the image, making it "2.5D," and we could recognize objects. This was Monty's first real-world demo, and there is a recording of it. It was a significant moment.

In that project, we also created a dataset of real-world objects, all currently at Neil's apartment, with 3D scans for simulation and additional photos taken under various conditions. This allowed us to test sim-to-real transfer, where performance drops but remains significantly above chance. There are about 12 objects, and we tested under adversarial conditions like darkness, bright reflections, hand intrusion, or multiple objects. There's still much room for improvement, but we now have a small dataset for testing.

Object behaviors are another area we've discussed and brainstormed, reading papers and considering possible solutions. Jad, an intern, created a toy environment with simple objects and behaviors to test solutions, but the current system can't handle object behaviors yet.

Achieving goals relates to the policy component. If you want to bring the world to a certain state, how do you communicate that to the system, and how does it act to achieve that state? We have some ideas here. One implemented feature is the goal state generator, part of the learning module, which uses models, memory, current hypotheses, and sensory input to generate a target state and send it to the motor system for action. For example, we use the skill state generator for top-down policy, telling the motor system where to move to resolve ambiguity, such as moving to the top of the knife.

We've also planned how to set up more test environments for goal policies, where the system must bring the environment from its current state to a desired goal state, even under adversarial conditions.

Implementing and testing this will require modeling compositional objects, such as recognizing a table scene, before we can address this. Someone just started sawing outside, so I'm going to close my window.

We didn't hear it.

All right, these next points are quicker since we don't have actual numbers yet.

Abstract concepts and spaces include n-dimensional spaces. For example, we have the temporal dimension for object behavior, such as melodies. There's the classical 2D space, like grid cells, and 3D objects, where we know less about how the brain represents reference frames. There are more abstract spaces, like family trees, which might be a subspace of 2D space with additional constraints, very abstract spaces like math, and even four-dimensional space, like quaternions, though I'm not sure our brains are equipped to handle these, at least from my experience.

I think we're generally dealing with the assumption that brains can represent 3D reference frames plus maybe a temporal dimension, which may be why quaternions are impossible to understand for most humans. It's also possible that a cortical column or a learning module, based on the algorithms we think underlie it, could learn n-dimensional spaces. The neural methods suggest that, but early in our lives, we don't experience certain dimensional spaces, and if you don't, those become set. That part doesn't change much in your lifetime because everything else is built upon it. If you didn't experience those as a child, you wouldn't develop representations for them and couldn't do it later. It's similar with math—if you don't learn math early, it becomes very difficult to understand math concepts later in life. Many things are like that; music is another example. So it's possible that the underlying algorithms are capable of learning n-dimensional spaces, but once you've learned the dimensions you're working with, you don't want to change them because all the models might get disrupted. Just because we're not good at four-dimensional space doesn't mean it's impossible; it may be that the brain could do it if you expose a child early enough. I wouldn't say that's out of the question.

I was going to ask about things that get abstracted away. If I understand Monty at the moment, there isn't an inherent representation of time—it's a sample data system, and it doesn't actually matter whether it runs slowly or quickly. It's still essentially ignoring time, just processing a sample and then the next sample, so in that sense, it's synchronous. The models don't have any temporal dimension at the moment. They definitely will need to have it to model object behaviors, for example, or melodies and similar things. Right now, it's a property we want: if I move along this cup, it doesn't matter if I go this direction or that direction; it should be the same. But it doesn't represent time in the models.

I have a solid hypothesis about how time is represented in brains and in the cortex, and how it handles variations in tempo and similar things. I've talked about this many times, and I think I know neurons are doing this, but it's not in Monty. Whether we would do it the same way or not is an interesting question, but it's something I've thought about for a long time. I figured we have to solve this problem, and I think I know how brains do it. I was thinking about it in the context of control systems as an engineering discipline, where one of the harder things in controlling something like a robot is dealing with delays. There are temporal problems that have to be solved, but at the same time, it's nice to abstract away and ignore them. The moment you put time into it, you have to deal with that throughout your whole system. You have to simulate with the notion of time, and you might see time as a problem because of delays and similar issues.

I was thinking of time as part of the modeling, as something you have to represent in the model. In melody, for example, you can speed up and slow down the melody and still retain it. Many of our motor actions, like signing my name, involve certain velocities and speeds at which my fingers move, and those vary over time. Object behaviors have certain velocities too, and some are faster than others. So it's part of the modeling system; it has to represent time and delay. The separate question is how to implement this and how to deal with variations in the real world. There's a lot of suggestion that another part of the brain, the cerebellum, takes care of delays and compensates for variations in time, not the cortex. I don't know what that means for Monty, but this is a theme in this presentation: for many of these things, we have decent hypotheses about how the brain might do it or ideas of how we want to implement it, but we have a huge list of action items to get it into the system.

Obviously, once we start implementing it, we'll encounter a whole new set of problems. I think we're at the end of the big topics that haven't been brought up yet. I'll finish up—there are only three more, and it'll be quick.

We're still in abstract spaces here. I grouped language with abstract spaces. One of the things we discussed before is how language might be represented in Monty. For example, if we read a sentence, that's basic object recognition, like recognizing other objects. We recognize the characters and their relative locations to form words, which we then recognize. But how do we know what it actually means? We can learn associative connections similar to the voting models we've learned in other modalities. For instance, we might have learned about a cup through touch or vision, and we can form an associative connection so that when we read "cup," it invokes this model. The same applies to relative displacements—if I read "on," it might invoke a relative pose, and then "table" invokes the table model, until a complete scene is formed in our head. That's at least how it might work in Monty.

Similarly, if we hear language, we would have auditory learning modules that can recognize words based on temporal features. I think this is the opposite of large language models, in the sense that we start with physical models of the world and understand the scene, whether or not we have language for it. Language is then a way of translating this structured modeling into communication. It starts with the structured model, not with the language. In deep learning large language models, there's no physical relationship—it's just the language itself, just the words. In the brain, it's completely flipped, and we shouldn't forget that. We figure out how to represent these structures. Imagine you're looking at a scene and attending to different things: there's a table, there's a cup. As you do that, you're forming the representation of the relative position of the cup to the table, visually or tactically. Then you can translate that into language to communicate it to someone. You can also go back the other way, but it starts with those models.

I've always felt that language is dangerous to focus on early because it's really subservient to having representational models of the structure of the world first. Otherwise, you end up with deep learning systems that are good at language but don't really understand anything. The advantage of this system is that language is grounded in physical reality, so it will truly understand what these words mean—what a "cup" is, for example, and also how the word sounds. What you said starts at the top and works its way down. You said "grounded in physical reality," but it can also be grounded in abstract reality—whatever the modeled reality is.

When I say physical reality, I really mean a reference frame. There are things in the reference frames at orientations and locations, and those can be physical or abstract spaces, but either way, it's the same thing.

So, regarding sentence structure, you have nouns and their positions relative to each other. How do you handle verbs?

Those will probably be behavior models. For example, we might have morphology models, feature models, and behavior models, as well as models of how something changes over time, all associated with that. Your question is exactly what I was trying to address. You start by thinking about different types of words, but you have to start at the top level, as Viviane was saying, and ask how we understand language to describe things here. Many verbs relate to actions of objects or relationships between objects.

Verbs come from reality, rather than starting with a concept of a verb and trying to understand it. There's a big gap there, but that's the way to approach it. Otherwise, you get lost in the details of linguistics. The notion of a behavioral model makes sense to me, so you can segregate that functionality in that way of thinking.

I'm satisfied for the moment. Let me quickly go over this one. Deep learning networks suffer from problems with adversarial examples. If you add a tiny bit of noise, it can make a totally wrong prediction with higher confidence, or in a more real-world scenario, you add a patch to a stop sign and it detects a different sign. Deep learning vision systems often over-rely on texture. For example, something might be recognized as an elephant instead of a cat, or as a flamingo, volcano, or Arctic fox instead of an elephant. I think Monty would be inherently good at these tasks and would not be tricked by a different texture on the morphology of an elephant. Adversarial examples are optimized to the system, so you might optimize adversarial examples for Monty, but hopefully those examples would be more like visual illusions that humans also perceive as illusions. That's my hope, but it's something we can explore in the future. It's hard to imagine how that wouldn't be the case.

Lastly, recognizing objects learned in 3D and 2D: you learn an object by touching and looking at it in a three-dimensional world, form a model of the object, and then see a two-dimensional picture or a sketched line drawing and recognize it. I don't think this will be a difficult property to add to the system, but it's another generalization we might test at some point. 

Does Link sign the media release? I'm his guardian, so I get to sign his media release. Get some higher, some n dimensions into that baby, test that hypothesis. I think he's actually not so good yet at recognizing the 2D version. I didn't see anything else other than a cute baby. I've seen parts of him doing very well. How old? Almost nine months. Wow.

He's younger in that picture. This is from today; I made it for this presentation. You're not supposed to look at screens, but I didn't want to print out the picture.

To sum it up, back to this overview. These capabilities—object detection, accuracy, pose detection, number of steps, speed, noise, and learning, whether unsupervised or continual—are regularly benchmarked. We have benchmarks in our code base, and you have to update them every time you make a significant change to the code.

We have benchmarks in progress for multiple objects. We have one there already, but I marked it as in progress because we still need to solve it. For real-world sensors and agents, we also have a basic benchmark, but we still need to achieve decent accuracy. For compositionality, that's the one we started during the hackathon; we still need to add proper accuracy measurements and make it into a benchmark experiment. Current work is focused on dealing with multiple objects and compositionality. Recent brainstorming has mostly revolved around different features on the same morphology and object behaviors. Before that, we discussed compositionality, and we have some concrete ideas on how we want to implement it. Sometimes we also touch on abstract concepts and how to represent space in reference frames. I'll talk about compositionality in a few weeks.

There are a couple of areas that are not as relevant right now, and some are still quite untouched, like deformations.

That was the overview. Does anyone have more questions? That was great. That was amazing. You covered everything. I feel like this is my life. We would be disorganized after 40 years of being disorganized. We've worked on these problems in a sort of ad hoc order because it's so hard to solve them. For so many years, it was confusing how these things happened or even just what had to happen. It took a long time just to make this list, and yet I think it's pretty complete now. It's really great to see that.