I have some slides I can go through on object behaviors and ideas around that. Viviane, you mentioned if there was time, we could get to some of your material—just very short, but no worries if we don't have time.

I don't have any slides, but I've been working on object behaviors a lot, so I have a lot of interesting observations. I'm not sure if I should interrupt and share my thoughts or just let you go through it, but I think interrupting as much as possible is good, and we can always adjust. Sometimes brainstorming is too slow, and sometimes you just want to get some material out. This is an attempt to bring together different thoughts about graphs and sequences and how this relates to object behaviors. It's nothing really new, more a summary of things we've discussed, reframed in a hopefully useful way, especially regarding how we might represent behaviors in Monty or in the brain. Part of this will touch on how far we can get with sequences and how universal they might be. After what you shared, Jeff, about attending to changes, it feels like how we learn object behaviors is clearer, but the substrate representing object states still needs clarification.

In discussing this, I'll also touch on how we represent reference frames, morphology, and path integration space—whatever you want to call it. You have a series of states and behaviors, and at any given time, there's some morphology that lets you predict what you'll see if you move somewhere on the object, and how that might tie in as well.

As alluded to before, this is just the start of thinking about how sequences might be useful for different parts of this. By double or triple roles, I just mean lots of different things. I'll walk through a bunch of things, and another related goal is to look at the interesting work out there on representing structures, from graph networks to hierarchical temporal memory and sequences. You don't need to know all these terms, but if you've come across them, it can be confusing how they relate. I'm trying to put some structure to that, looking at the big ideas of graphs and sequences, how they're related, and how they relate to representing states and space.

That was a kind of preamble.

Hopefully, I haven't assumed too much. We've done similar brainstorming sessions many times, so I may assume you're familiar with hierarchical temporal memory or some of these other things. Stop me if I can clarify anything.

In terms of how we might represent object behavior, one way to frame it is with a graph view, where we have a logical model of an object as a series of nodes. Those nodes are locations. They're not features; it's a continuous space, not just three nodes or something. We don't represent it in a totally continuous space, and as we get more hierarchy, we expect it to be more sparse, maybe, but the space itself is continuous. It's embedded in a continuous space where we've located features at different locations, and other points in that space, so we don't see everything.

If you wanted to encode behavior with a graph, you could lay down edges connecting nodes—not necessarily all the nodes. For example, there are edges between some nodes and between others.

The behavior of the system could then follow this, where the state of a given node or location at a time point depends on several things: the state of connected nodes (based on incoming edges, so information from neighbors transformed by an edge affecting this node), some local actions (additional influence in the system, like someone pushing), and for the edges, that includes a self-edge. It's not necessary, but you could also have some global state, like a pooled state describing the object—maybe it's hot, maybe it's cold—which influences all the nodes at a more global level.

I'm not claiming this is the way we should do it; I'm just trying to describe one way you could frame it. Is A representing a node? It's like the top of the stapler. So it's not a location—this goes back to what I mentioned earlier. I view a model of an object as a bunch of space, and then we assign features at different locations in that space. I could say a point in space is a node, but that's not what it is. In this case, A is a feature located at some space location, but that feature can move. I've always had trouble thinking about the brain as graphs like this, because the language we use for graphs doesn't fit well with the idea of location spaces and features at locations. So I'm confused about what A is. I understand what a node in a graph is, but is it a location, a feature, or a node on a graph?

I would say it's a feature currently associated with a location, but that location can change. We can call that a node, but if it's a feature, I wouldn't typically think of that as a node. I usually think of a node as a point in some sort of space, with all the things associated with that point. If it's moving, then all those things move together. The language of nodes and graphs is just hard to apply here. Maybe one way to think about it is like a place cell—it's bound to a bunch of information, but the mapping between the place cell and the grid cell could change, so where the place cell actually is can change. But we don't see that; place cells represent a location in space. Grid cells are different, but they also represent a location. You don't typically see a place cell move, but it can remap. I'm not sure if it remaps in the same environment. If we're changing the environment, that's what's happening. The individual cells can remap if the environment changes. The question is, when we change the state of an object, is that like changing the environment? If you start moving the walls of the environment, things like that, the grid cells and place cells can distort. I've argued that when the stapler moves, it's the same object and therefore the same space. If we switch to a new space, all the learning about the stapler would be forgotten. If you have an object and a model of it, and some part of the model changes, you need to represent that change without forgetting everything else. I don't look at it and think, "That's not a stapler anymore," it's just changed.

I'll talk later about the space element, which might help clarify things. In my mind, there's a fundamental impedance mismatch between models based on locations and features and models based on nodes and edges. I don't know how to bridge those two. It feels like part of the issue is that it was easier to use the language of features at locations because we were always talking about static objects.

Now, it almost needs new language. I take the opposite approach: the language of features at locations works well, so how do I explain behaviors within that language? That's how I'm approaching it, and I think I'm making good progress. I'm open to other ideas, but it's hard to go in different directions at the moment. I'll let you keep going and see where you pick it up. I'm just trying to really understand what A and B are exactly. I understand nodes in a graph, but mapping that to real science is hard. Hopefully that clarified it a little, at least. Not really. It's hard to map.

Do you mind if I give an explanation? I'm just trying to find the zoom controls so I can draw to explain it. If you hover above the top, I have an annotate option. Thank you, Will. You want to draw something? Let's say instead of a stapler, we have an image like a grid—a 10 by 10 image. We can think about this as a graph where each pixel is a node. Is this not an object, just a pixelated display? Instead of a stapler object, I have an image object—a picture. Is the object this grid, or does the grid represent where I represent the object?

Are we actually recognizing a grid, or does the grid represent a pixelated image? The latter. I think, as Jeff said, he understands the concept of a graph and what a node is, but the tricky part is mapping that onto this context. I'm trying to get there, but I haven't yet. I just wanted to clarify—let's say we have a physical picture, like from an A4 paper, and we cut it into little pieces, so each piece is a node. Does that make sense for now? Can we imagine that? Sure. So then each pixel, or each of those small pieces, could be thought of as a set or vector containing the RGB value and the XY location from where it came, representing that node.

But that's just describing an image array—there's no object, learning, or model there.

The things moving on the stapler are not just pixels; they're objects, features, and part of a model of the stapler. It's not just an image. When we learn features at locations, we do learn some kind of color or HSV principle, but I assume Niels is presenting A as not just an undescribed pixel block—it's the top of the stapler, associated with an object at a lower level. The issue, Jeff, which is reasonable, is that we've always described features at locations, framing the location as static. Here, there's a reference frame, and the graph is embedded in it, but the elements of the graph can move relative to each other—it's a stretchy graph.

Exactly, it's a stretchy graph. The problem is, there are two ways: either the graph stretches, or things are nudged around as a function of time. You could modify the reference frame, but in grid cells, while they do get distorted, not much, and it doesn't explain this mechanism. The problem is, as soon as you start modifying the reference frame, it's difficult to do path integration and to compute metrics between two arbitrary locations. I think that's the wrong direction. You have to assume the reference frame is a reference frame—it's not a flexible graph. We can consider the possibility, but my baseline is that the reference frame is fixed, because otherwise you lose desirable properties.

It will be interesting to discuss this, because with abstract spaces, by definition, they're generally distorted and don't follow Euclidean space. Maybe these distortions are a continuum, and that's a good hypothesis, but I'm not willing to say that yet. I don't understand abstract spaces well enough to make conclusions. What I am willing to say is that it's very likely the neural mechanism is the same. For physical objects, I start with the assumption of a fixed reference frame. For abstract spaces, I just don't know yet. My intuition might say things are moving around, but I don't trust that.

Intuition is not really any kind of evidence for that, so I don't want to reach that conclusion. You said there are two options, and I think the second one is actually how a lot of graph neural networks work. In that approach, we're not distorting the reference frame itself—locations remain in Euclidean space relative to a common point—but depending on the state of the entire graph, each node could be in a different location in that reference frame. A node is a feature of the object, and the features can move. All possible object behaviors can be expressed as features moving locations, changing orientation, changing ID, or any combination of these, independently or simultaneously, and causally related. That explains every kind of transition, even discontinuous ones or whole object changes; all of them are remapping the model. For example, in the stapler, features move and change orientation, while with a light, the feature changes (on/off) but doesn't move. All possible behaviors can be represented by features moving, changing, and changing orientation.

That would be my preferred way of doing this, but I thought there was some concern around the feature. Will, go ahead. There was one thing that confused me, Jeff: you said a feature can change its ID. What does that mean? For example, if I'm looking at a screen and there's a letter on it, and I push a button and the letter changes from A to B—same location, same orientation—the feature has changed from A to B. It hasn't changed location, orientation, or scale. Or I could push a button and the letter A could move to the right, or rotate, or multiple things could happen simultaneously or independently. That's an example of a feature changing its ID.

That's why I feel some terminology is needed to capture the idea of a binding element that can move and whose attributes, like feature ID, can change. That's why my first question was, is A a feature? If A is a feature, then I'm with you. The confusion is whether, if the feature can change, is it the same feature? It's the same node, even if the feature changes. Now you've switched node to mean location. But what if both are happening? For example, a button that displays a letter can move around, and the letter on the button can change.

If you have a model with a reference frame and features, each feature has an ID, orientation, and location. That defines a model. Any of those components can change, in any combination. The feature can move, change orientation, and change ID.

That's pretty consistent with graph terminology, at least as it's used there. The node A can both change the features associated with it and move in space, without distorting the reference frame. What makes it a node? It's a collection of information and how it's connected by edges to other nodes. For example, you could say all parts are connected to the hinge at B, but there are examples with no obvious connections.

It doesn't have to be a physical connection; it can be a causal connection. But sometimes there's not even a causal connection—a feature could just change on its own, like an object that randomly changes color at a location. It's not moving, just changing. There's so much language associated with nodes that when we use it, we start thinking in terms of graphs. I'm trying to say, given a reference frame and features, where is the graph? A could be a feature. From my view, the proposed new language seems to fit what you're describing, but I haven't heard that yet. I want to believe that. I can say it's a feature, but you're not willing to say that. What if we say it's an object? The ID of the object can change, the location can change, the name of the sub-object (which we often call a feature) can change. Can I try one definition?

What about defining a node as its feature at a location that has edges to other nodes? It seems to me that a node is defined because edges exist—if you don't have edges, you don't need nodes, you just have features at locations. So what makes it a node is that these features at locations have edges. You could say that, but I have trouble with that too, because what does the edge represent? I could say there are two things that change and are causally related, and if the edge is a causal relationship, then maybe A and B are changing in a causal way. But I could still have a node A, which is just a feature at some location with no edges—it's just its location. It's located relative to everything else based on where it is in the reference frame.

Maybe feature is a sufficient description. Sometimes when you say feature, people just think of colors, but to me, a feature is whatever bit pattern is coming into a cortical column at layer four. It could be an entire object, an edge in V1, or something simple on the retina, but it could also be a hierarchically represented feature. The reason to move from feature to node is because we want this thing to also be able to change location. So it's not just a feature; it's a feature and location that can change. For example, with a pendulum, whatever is on the bottom is a feature that can change location, and the node is connected to the top of the pendulum.

The language I use exploits all those things but doesn't assume a physical connection—there might be, but it doesn't assume it. In my language, you have a reference frame and features, and those features can move, change orientation, appear, disappear, or be replaced. That describes every possible object behavior. It's a complete basis of object behaviors—there's nothing else that could happen to an object. From that point of view, I don't make any other assumptions. So you would suggest calling it a feature instead of a node? I don't mind any language, but typically the word node means something else and leads to confusion. To me, a feature is just whatever is located at some location—it could be an entire object, anything. In our models, we have locations and things at those locations, which we call features. That's all it is.

Maybe I'll move on. I just know I'm going to get lost on this, but I'll try. Let's keep going.

We can come back to it if it's an issue. One of the key challenges in representing behaviors is learning the properties of edges, for example, a rigid relation or something more interesting. With a hinge joint, through serial attention and knowledge of what a closed stapler looks like, and attending to movement at A, a learning module might observe movement at A and have some expectation about where B is. Because of the movement of A, there's a fixed distance but a change in orientation relative to B, whereas with C, the displacement continues to grow. This isn't necessarily a sufficient or elegant way to learn it, but hopefully it captures the basic idea that, through the information observed at a node and what we know or believe about other nodes, you might infer some relations or edges.

Bringing back what we discussed a couple of years ago, this approach allows very fast generalization. If we've learned an edge type before, we can apply it to other objects where that type of edge exists, like a rigid or spring relationship. There's another way to do that same learning, but I'll leave that for another day. If necessary, another example is a hinge joint with a spring that's constantly trying to close the stapler. A is moving on its own, and there's a rigid relation with B, but also a closing relation. Whatever is learning this doesn't need to have a concept of a spring—they just see this behavior happening repeatedly, where the hinge closes itself.

Let me clarify this. Deforms as a function of features or nodes moving, which I'll get back to in a minute. I wanted to quickly contrast this with another point on language people may have come across. In the graph networks or graph neural networks literature, you often talk about message passing, which is the dependence of one node at a time on the states of its neighbors from a previous time step. You can iteratively pass these messages and see how the state of the network evolves. An interesting concept is the hypernetwork—a network that generates weights for another network. As Viviane mentioned, weights here mean weights in a normal neural network. If you assume the way an edge is encoded, the influence of one feature on another takes place via some weights, those weights might not always be the same and could change. In deep learning, one way to change them is to generate a set of weights, and I'll touch later on how this might connect to more SDR-style HTM weights.

Contrasting this graph view with a global sequence view of behavior, which I think we've discounted now, but that isn't to say sequences aren't useful. Previously, we might have talked about behavior as a stored sequence of full object morphologies—morphology at time t=0, then a hundred others, then time t, but that's a lot of information to store, and the amount explodes with the degrees of freedom of the behavior. As you pointed out on Slack, Jeff, it's hard because a single learning module generally only observes part of an object at a time, and objects don't have a global state, or at least not one that's so specifically constrained. Even this example isn't fully defined. For instance, the deflection plate at the bottom can be in one of two positions or in the process of rotating, and its behavior is completely independent of the stapler top. There are also ways to remove staples, sometimes with a spring at the back or in the top, and that's also somewhat independent.

There are many examples where objects have lots of independent parts moving. In a car, the doors are open, the hood is open, the windows are down—there are many possible states for different parts. There is no global state for a car, so you have to have the state for individual features or components, and it has to be on a location-by-location basis.

I agree that a state like this isn't what we want. That was a brief discussion of behavior and sequence of states. What about space?

One way to think about reference frames and how morphology is represented is what I'll tentatively call an embedded graph: features at locations in a reference frame, embedded in some defined space like Cartesian space or grid cells. This is what we're currently doing with features at locations.

As discussed earlier, features can change as a function of time or due to external influence, changing their location in this embedding.

The advantage is that you can generalize to novel displacements. You don't have to do the exact same displacement as before. If you just type out certain things—I'm not sure I understand that, generally, what is a novel displacement? Now I'm just talking about how you path integrate over the object. The word displacement here refers to the movement of the sensor, not behavior. This is about a sensor moving over the object.

The general advantage of grid cells is they path integrate, and you can do that in novel directions and novel displacements.

Sorry, I thought about changing the object to something without a clear behavior, since we're just talking about path integration here. I don't often use the word displacement, so I'll just say movements, path integration movements.

Question? We're still talking about space reference frame per object, right? So it's not a universal ether—it's Cartesian space per object, grid cell space reference frame per object, yes?

For us, we're currently representing a unique Cartesian space per object. In the brain, grid cells would encode multiple objects in their distributed code. This probably would work. Be careful, because we've done the same thing in two different ways. In our code today, we've just picked different places in an infinite space and said, "Over here is where we'll represent staplers, and over here is something else." They're in the same universal space, but so far apart there's no interaction. If I know my location, I know what object I'm on. Grid cells achieve the same result in a very different way. If you don't know it, I can't describe it right now, but you have the same set of cells, and once you've locked onto an object, the patterns in those cells are unique for that object and you won't see the same pattern on any other object.

So it achieves the same result in a different way, but both approaches essentially provide a universal way of representing space. It's a finite set of resources to represent space, but when you actually represent an object, you've locked into a particular part of that large space that only exists for that object in both grid cells. 

This is very different from the typical Cartesian view. It's not that we have a three-dimensional space and represent one object in one moment and another object in that space at another moment. The objects are so far apart that you'll never get between them—technically you could, but it's almost infinite, so you can't. They're like completely different spaces.

To match this with the code, it's not that we move them far away in the same space; the objects are all represented around a 0,0,0 location. Mathematically, it's equivalent to giving each reference frame a unique prefix, and you only index that reference frame when looking at that object. You couldn't accidentally move into a different object's area. The object ID acts as an index, so the reference space represents object A, for example.

If I want to predict what feature I should expect to see, I can't just look at the location. Each location has a unique prefix depending on the object, so the object ID tells you how to interpret the space. It's almost like a fourth dimension, but not exactly. It would be equivalent to placing them far apart, except for the slight risk of interference. In practice, you need to know the prefix to do anything, and that prefix represents the object type, like cups or staplers. You can test different prefixes at the same time. If we placed them far apart, it would be equivalent to using a prefix, like adding a large number in front. I'm not sure if that's truly equivalent, and when doing inference, it might make a difference. I see how prefixes are like more bits in space, but I'm not sure if that's equivalent, so I won't accept that yet.

The last thing to say about these embedded graphs is that this works well with 3D Euclidean space, but it's more confusing how to embed abstract or arbitrary spaces, like a family tree, into this substrate. How do you understand that certain parts of space connect to others, but some don't?

There's some evidence that grid cells can distort, but as Jeff pointed out, this is generally not very significant. A better way is to have features move in this kind of space.

Another way to conceptualize space is as graphs as sequences. This is related to what we tried in Monty before, with features of displacements, where you learn space as a sequence and build a graph from that. In normal 3D space, displacements or movements are simple and follow expected behavior, but in more complex or abstract spaces, sometimes you can move between points and sometimes you can't, so those displacements carve out the space. This creates an abstract graph-like structure. The green arrows here are not the same as earlier edges; earlier edges were about how one feature at a point in time influences neighboring features (causal relation), while these are connections in movement—showing you can move between points by following a displacement.

When we tried features of displacements learning modules, this approach struggled with novel displacements. You end up trying to match movements exactly, which works in a discrete grid world, but if you move only one centimeter or 1.5 centimeters instead of the two or three you did during learning, it can be hard to match.

I've been thinking about hybrid approaches to address this, but I haven't made as much progress as I wanted. I'll talk about that another time.

This could be very confusing for people who haven't been here before. We were trying to represent models, and there are two basic ways to do it: you can say features are at some location in a reference frame, or you can say features are at some displacement from each other. If you're trying to figure out physical displacement, like Niels mentioned—some direction, distance—there's real power in using physical displacements, but there's a combinatorial problem. If you have many features, representing the displacements between all of them is impossible to learn and takes too much memory. We were struggling with what the basic representation of an object should be.

These were the two options we considered. The very first learning module implemented in Monty used this approach, and it had some nice properties. We could represent displacements in a rotation-invariant way and recognize objects in any rotation without specifically testing rotation hypotheses. There were some nice properties, but as Niels and Jeff said, the issue was that it wasn't really integratable—we couldn't take new movements that weren't stored in the graph.

If you're operating in a discrete grid world, where you can only move up by one, left by one, and so on, those issues go away. This approach has been used in work like Dileep George's clone-structured cognitive graphs and the Tolman-Eichenbaum machine, which show that if you visit all the edges, even in a complex graph, you can build this representation and path integrate, following entirely novel paths.

In this case, learning is done with a kind of sequence memory, similar to hierarchical temporal memory. If you follow a sequence, you can build up all these edges, forming a graphical structure.

Going back to behavior, if we look at this model where edges represent more causal relations, the local states associated with a feature or node will follow a sequence over time. That depends on local actions, incoming states, and edges. I think it still makes sense that there's some global state pooled from all the local states, so it has some information, but it's not a one-to-one mapping. For example, summarizing that the stapler is closed is useful when looking at my office desk. However, there isn't a true global state, and an object can have multiple types of states. In one context, one state is important to pass up; in another, a different state is important.

I don't have a solution to that, but there isn't a global state.

It feels like object ID. I put together a slide for this: you can have subtle changes in morphology, but the object is still the same—it's still a mug, for example. Knowing it's a mug doesn't tell you exactly where every feature is, just as knowing the global pooled state doesn't tell you the state of every local element. The trick for compositional structure is that, even though we can pass up an idea of the feature, the pairing is done by location. On the downward path, we associate the location of the parent object with a specific location on the child object, specifying the local state of the child. 

It's useful to know the overall object ID and where you are on the object. If I'm going to make a prediction, I have to do that. That was the whole compositional object struggle. It feels like providing that information at L2, L3, or similar levels is important. I think the answer will be different depending on the case. The surprise of the compositional structure was that I don't pass up the state to the parent, but the parent associates its location with the state of the child.

I'm not certain yet, but my guess is that's what's going to happen. It should become clear when we get back to the motor policy. Remember, there are different states, but they're not all tied together, and many objects have lots of states. The parent object can't know what those states mean. For example, if I'm representing a car, I can't represent the window being up or down or the door being open or closed at the same time. I might represent that the car is on, but I can't accept all the individual states of the car's features. I have no way of interpreting or storing them, and I can't store everything everywhere. I agree with that. I started where you were, Niels, thinking we had to pass up the state, but now I'm not so sure. I'm working on the assumption that I don't have to pass up the state.

If we have a more global state, how would broader state information spread? For example, if I touch a coffee cup and notice it's warm, I immediately know the whole circumference is warm. A similar problem is associating an observed feature or change in one modality or part of an object with a change in another modality or part, like pressing a button on a lamp and the light turning on in another room. I have to be able to associate these changes, even across modalities.

We have to learn these associations. Imagine a bunch of columns, some observing changes at a moment in time and some not. If two columns, no matter where they are, change state at the same time or close in time, we might want to associate them. If these changes occur together, even across modalities or with a slight delay, we can catch the causality. If the delay is too big, we won't. The general question is, if a column observes a change, how is it associated with changes in other columns, whether nearby or far away? That's a simple mechanism I'm working on now. You can also learn sequences of those changes using sequence memory, so A follows B follows C, in different locations or modalities.

It's a different language than graphs, but that's how I think about it. Unless you wanted to ask more, Viviane, I have a few more slides to finish. Go ahead. Global states may or may not exist, but if they do, they could follow some sequence over time. Morphology, or how a movement maps onto what you observe, will change as a function of the local state. If you're on B and move up, you'll predict something different if B has moved than if it hasn't. Maybe that's a change in the graph space or in the sequence describing the space.

Thinking through this, many things can potentially come down to sequences. I found it interesting because, when thinking about graphical representations of object behavior, I was still thinking in terms of nodes interacting, like in a graph neural network, not as sequences. But it feels like it could all be framed as sequences, with local sequences dependent on one another. By sequence, I mean time-based—one follows another—but also conditional on other things, like actions or the state of neighbors. Sequences sometimes occur temporally, and we have to learn them, but often state changes are not temporal. Things can change simultaneously, and that's it—no sequence, no time involved. Features of an object can change all at once and then stop. Sometimes those things are correlated because they consistently change together, or they change in sequence. But not everything is a sequence. Sequence can mean a state changing to another state, not necessarily involving time. Sometimes time has expired, and sometimes things change simultaneously.

To me, the word "sequence" implies more than one element changing—a series of things. But there are behaviors where no time is involved. In those cases, I wouldn't use the word "sequence." That's why I asked if time is involved, because that's how I interpret "sequence." I guess time is always involved in the sense that nothing can change without some infinitesimal amount of time, but it's not always a sequence where something happens every second. Often, things are like that—the stapler opening is a sequence—but not all sequences are like that. I've learned to challenge every word because we often use the same word for different things. When you used "sequence," I wanted to clarify if you meant a time sequence, because there are many examples where there isn't a time sequence. Time may have expired, but it's not always a series of elements over time.

One thought, irrespective of whether time is involved: I worry about representing everything as sequences. Sequences are similar to displacements. If you have one degree of freedom, like a stapler opening and closing, it's easy to imagine as a sequence. But with more degrees of freedom, like a joystick moving in all directions, it's harder to model with discrete sequences. However, we might be able to learn all the different sequences and have small subsequences, like moving left to right, up to down, and so on. I couldn't think of an example where we would encounter a completely new behavior that we couldn't have learned as possible subsequences.

With sequences, it seems we would have to observe all possible sequences at least once to path integrate. It's similar to observing all the edges, but if you do it locally and can generalize, it's less of an issue. Otherwise, there are too many states to learn. One nice thing about HTM sequence memory is that it tries to learn indefinitely long sequences, but if it doesn't get exposed to them, it learns shorter segments. For example, if you're learning how a joystick moves through space, it only picks up the local actions that repeat, and if there's a long sequence that repeats, it'll learn that too. It handles that problem nicely.

On the HTM stuff, it's not a fully developed thought, but when you say HTM, it doesn't mean anything to me at this point. If you say HTM sequence memory, I know what that means. If you say HTM neuron model, I know what that means. But HTM alone is overloaded. The sequence memory, conditioned on object ID, could be useful for conditioning on states of neighbors, actions, or global state. I look at it differently: if I have a neural representation that changes in a consistent, repeatable way, it'll learn that. Anything that transitions over time and repeats, it'll learn. You can put global state or actions—if they're changing over time, it'll learn them if they're repeatable. We have different mental pictures when we use these words, and it's hard to know if the words represent what I'm thinking.

In the original temporal sequence memory, you had the L3 layer with the object ID, and you went through different features, predicting different SDRs for those features depending on the object. That was the unique fingerprint for the different objects. The sequence memory is the same as using reference frames and features of reference frames. You have a series of SDRs representing the same object, and then you do temporal pooling to map them to a constant ID. Whether the sequence is determined by sensor movement and what you sense, or by time, it doesn't matter. It's a sequence of SDRs pooled to the same ID. There are two ways to find a unique representation of a point in space: through time, or through movement of the sensor and tracking location. Maybe it would help to put together a slide showing this diagrammatically.

I just want to make sure that's clear for new people here. The sequence memory and memory reference frame are really just two flavors of the same thing—they're not fundamentally different. In fact, Subutai wrote a paper on that.

You can mix and match them; they naturally combine. For example, if I move my finger over an object in the same order every day, it'll learn that as a sequence, like a melody. But the first time, I could have moved my finger in a random order—it all leads to the same result. That may not help.

That was everything I had. I'm just trying to find the paper, but we can do that another time. I want to reconcile what you're presenting with how I'm thinking about this, and I think I need to do a similar presentation to what you did, to see if we can come to some agreement.

I'm working on that. I wasn't working on the presentation, but I'm working on the ideas, so maybe I can present that next week. Is that okay?

Yeah, that's good. My presentation won't be as nice as yours—probably just words, but maybe I'll include a few images.

During today's conversation, I threw a lot of ideas out there because I'm trying to match them up to what you're presenting. I want to reach a consensus on this.

Yeah.

I hope I didn't assume too much in terms of HTM and stuff. I thought we could look briefly at this. I made this figure, so maybe it's helpful for others. I'm also reminding myself, but maybe this is better for another meeting. I'll try to do a recap of HTM sequence memory and how it might relate to sequences of behavior states. Again, just to repeat: you have a layer of neurons representing something in a sparse way, and the HTM sequence memory explains how you might come up with a sparse representation and what happens if you go through a series of these SDRs in the same set of neurons. They can learn arbitrarily long, high-order sequences in a powerful way and make predictions about what's going to happen next.

It does this in a simple way that matches biology and explains things no one explained before. That's all it does. So I'm constantly thinking, if I have a state variable that's changing, that variable is going to be on a location basis, or it can change on a location-by-location basis. If that state variable changes for any reason, I can learn sequences if it changes consistently. If it's not consistent, I wouldn't learn sequences. That's how I'm thinking about learning behaviors and causal relationships—it's always in the back of my mind.

This stuff is hard.