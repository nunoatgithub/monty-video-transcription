Welcome to the second video in the core video series. This follows the last video where Jeff introduced the idea of the AI bus, which was the key spark that started the Thousand Brains Project and forms the foundation of the entire project. The upcoming video was recorded a couple of weeks later, after more in-depth research discussions about Jeff's ideas. Here, he presents those outcomes to the company and clarifies some misconceptions. He also discusses the advantages he sees and why he believes this is such an interesting path. After a few years, I still agree that this is a fascinating direction, and it's been a great journey. He talks about the AI bus, now called the cortical messaging protocol, and the idea of communicating information in a common reference frame, which is exactly what we're implementing today. He also discusses details about voting.

He mentions that this approach may differ from previous work because, for some parts of the implementation, we might choose to deviate from how the brain does things. This is a huge undertaking, and while we want to build the whole framework, we know some details are implemented differently in the brain. We can't do everything at once. For example, we've chosen to use explicit graphs to represent models of objects and Euclidean reference frames, even though the brain doesn't use them. We use these because they make it easier to debug, visualize the system, and understand what information the system needs, rather than working with a black box algorithm where we can't see what's happening inside. This approach has worked well, helping us develop the sensorimotor learning framework and the cortical messaging protocol. Now, we feel confident in this framework and can replace explicit modules with more biologically plausible ones. The idea is that you can implement custom learning modules based on different principles, as long as they use the cortical messaging protocol. You can mix and match, swap out, and try different implementations of learning modules. So, without further ado, grab some coffee or tea, and focus all your 150,000 cortical columns on this video. Cheers.

Alright, let's jump into it. This isn't in the images I sent for today's meeting, and I have two purposes for this meeting. One is to correct a misconception I created—that we're abandoning neuroscience in our work. That's not true, and I want to make that clear. The other is to better explain why I'm excited about the AI bus and its opportunities. This turned out to be difficult to explain, and I spent a lot of time thinking about it. Sometimes you know something, but it's hard to express it in words. In the end, I decided to go through a summary of Numenta, what we've achieved, and where we're going. In that context, I think it becomes clear. I'll do my best to clarify that. I know there's confusion about this. We're not making any decisions today; there's no reason to decide anything now. This is just for information sharing and can be interactive. I tend to get into presenter mode, but please ask questions. I'm going to avoid technical questions—if Christy, Charmaine, or our new employee can't understand, we shouldn't be asking those.

I'll make sure we don't do that. That's the purpose of today. You don't have an image of that, but those are the two things I wanted to address. This is the agenda, shown in this image. I'll go through these in order: our company goals, how we tackled goal number one, what we learned, a roadmap, the AI bus (sometimes called the cortical communications protocol, since "bus" can be confusing), a segment about the birth of industries (mostly about Amazon), and the second roadmap. This isn't to replace the third roadmap, but to provide a way to think about the AI bus and what we might do in a roadmap for it. Then I'll discuss company options—what we might do next. These are just options and may not be an inclusive list, but they're for discussion. I'll go through these in order. Each has a box: number one, number two, number three.

This is a method I use often, and I think Marcus uses it now too—putting things in boxes when working with someone. I considered doing this in a PowerPoint, but it's too difficult.

If there aren't any questions, let's get into it. Number one: goals. I paraphrase our goals. Everyone should know this. We have two company goals. The first is biological goals: reverse engineering the neocortex. This is really a neuroscience goal, not a machine learning goal. It's a neuroscience goal. I often use the term "biologically constrained."

Which one of them is not muted? It's my fault.

I often use the term "biologically constrained" because, from a neuroscience theory perspective, we don't have any theories about what's going on in the brain that conflict with known neuroscience. Occasionally, we'll accept something and say, "This isn't quite right, we know that," but we'll move on anyway. The goal isn't to explain everything happening in the cortex, but the things we do explain can't violate any of the principles or details we know exist. That's how we view it.

The second goal is about machine intelligence, and it relates to the first goal, but it's specifically about machine intelligence. There are different ways of expressing this. We've talked about being a leader in the future of AI based on what we've learned from the brain. I always like to use this phrase, which I've used from the very beginning: to be a catalyst for the age of intelligent machines.

In this century, we're going to see a tremendous explosion of truly intelligent machines and robotics. It's going to impact the world in the same way computers did in the last century—maybe even more. The idea of being a catalyst is that this is going to happen one way or another, whether we're here or not. As an entrepreneurial team, all you can do is accelerate it. You can't stop it from happening, but you can make it go faster or happen sooner. That's how I view it. A catalyst can accelerate a reaction manyfold, and I think we have the potential to do that.

There are a bunch of assumptions underlying this, and I think it's worth pulling them out. One assumption is that today's AI is not intelligent. It's not really where we're going to be; it's really dumb in some ways—spectacularly stupid, as someone once called it. It's not that it's not useful or cool, but it's not really intelligent. It's not where it's going to be. The AI industry will be something different in the future. The future of AI will be based on different principles than we have today. We don't have to throw away everything that's been done in deep learning and so on, but it's insufficient.

The second assumption is that the fastest way to discover those new principles is to start by reverse engineering the brain. This was a very unpopular view. I ran into this when I applied to MIT many years ago, and they said, "The brain is just a stupid computer. There's no reason to study it." That's literally what I was told by professors at MIT—that was close to a verbatim quote.

It's not clear; it could be that a lot of smart people working in machine learning who don't know anything about the brain could discover these principles. There's nothing to say they couldn't. So this was a bet. It was a bet that, although it seems like a long way to go to figure out how the brain works first, that's actually the quickest way. Other people said, "No, we'll just go right there and start building these things." This is not a foregone conclusion; it was a bet and an assumption. So far, I think it's held up pretty well. The machine learning world is converging on some of the same ideas we've discovered in the brain. I thought it was worth spelling out those assumptions.

I was once asked by one of our employees, "Which of the two is the most important? If you could only pick one to succeed at, which one?" They thought I was going to pick the second, but I said, "No, totally the first." I personally want to understand how the brain works, and if we can do this too, that's amazing. That's great. It's super good. Those are our goals. Any questions about that?

To know that today's AI is not intelligence, and we want to define intelligence as knowing what is missing—what is that gap. We don't know that. So that's the way to start: if you could study the brain and figure out what the brain does and how it does it, then you could look back and say, "Oh, this is what we missed." Sometimes the question I ask when people say, "How do you know, when you start with the brain, which principles are important?" I say, "I don't know, but you'll know it when you figure the whole thing out."

It wasn't even a fundamental question of "What is intelligence?" That's a bigger question. But we have to answer that question, right? We have to answer, "What is intelligence?" It's not, "Oh, we need this feature or that feature." No, it's "What is it?" It's not about playing Go or solving some machine learning problem. It's about the essence of intelligence. My book is all about that, and I think we've really got a good handle on it. But we started out not knowing that at all. My original interest in this field was to understand what my brain was doing, and there was zero information. What is it doing? It's obvious it speaks, but what are the core principles at work? How is information organized? What's its function? That was a real black hole—nobody knew any of that. Did I answer your question? Anything else? We'll go on to the next one.

Now we're going into box number two for those online who didn't follow. So, how do we go about tackling this? There's an order to this: do number one first, then do number two. There's a point where you can't work on number two.

So, how do we tackle number one? It's worth reiterating how we approached it. I said we would focus on the neocortex, not the whole brain. Fortunately, the cortex is made of many identical cortical columns. All mammals have cortical columns, and they're all very similar. We have more than other mammals—about 150,000 in our brain—while rats, dogs, and cats have fewer. Even non-mammals, like birds, have something equivalent to a cortical column. 

We can break the question into two parts: what does a cortical column do, and how does it do it? Since there are so many of them, Vernon Mountcastle suggested that if you could figure out what a cortical column does, that would be pretty important. The second question is, even if you know what it does, how do they work together? With thousands of them, how do they communicate and collaborate? There's not much point in thinking about how they work together until we figure out what they do. The analogy I use is that if you're trying to understand how society works, you should first understand what a person does. So, we focused on what a cortical column does and how it does it.

When we started, we had no overall picture or understanding. We had absolutely no idea what a cortical column does. There were some vague ideas in neuroscience, like "it extracts features," but we knew it was much more complicated than that.

We didn't know any of the things we know today in the Thousand Brains Theory. We identified subfunctions that a column had to perform and tried to tackle them one at a time. If the neocortex is made of identical units, then everything the neocortex does must be done by each unit. If the brain can learn sequences and all columns are doing the same thing, then all columns must be able to learn sequences. That was the idea: if we could pick things we knew they had to do, we could focus on those. We focused on prediction because we knew the cortex made predictions. We started thinking about sequence memory. We then had to consider how to represent inputs in different contexts, how to handle uncertainty, and so on. Tackling these led to the theories and papers we published. Then we moved on to how we make predictions based on movement, still not fully understanding what a cortical column does overall, but knowing these are pieces of its function. We also studied other brain regions as needed, like the thalamus, entorhinal cortex, and hippocampus. The cortex doesn't exist in isolation, so we brought in these areas as necessary. Now, neuroscience has a lot of knowledge about the entorhinal cortex and hippocampus, which relate to the cortex. We have theories about the thalamus, but those are still somewhat mysterious.

We mostly restricted our studies to columns that get direct sensory input. In the cortex, many columns receive input from the eyes, ears, and skin, while others get input from other brain parts. We focused less on those and more on touch, vision, and similar modalities. That's how we learned about them, and that's what we've been doing for the last ten years—maybe more.

Any other questions about that? Luiz, you probably recognize this. I want to put it in context because there's a logic to it. We didn't lay this out up front, but this is basically what we were doing. Let's go on to column number three. What did we learn? I'm not going to go through this in detail, but I'll start with the big picture. We started with all these little details, but in the end, with the Thousand Brains Theory, we have this big picture. Each column is like a miniature brain, and associated with each column is something like a sensor patch—a patch of your skin, retina, or part of your ear. That patch moves around in the world. The key insight from the Thousand Brains Theory was that the column knows where that sensor patch is relative to the thing it's sensing. That required a reference frame. For a column to know where its finger is relative to a coffee cup, or for your ear to know where it is relative to a sound, was a total surprise to me. Once we realized that, everything came together. As a sensor moves, the column can learn a model of the structure of something in the world by moving and sensing, because it knows where it is. Even columns in the low sensory regions of the cortex can learn complete spatial models of objects through observation and movement over time. That was the big insight of the Thousand Brains Theory.

I want to talk about a couple of things here because I think they're relevant to the AI Bus. Here's how we've been thinking about this recently, based on work Marcus, myself, and others have done. Here's a picture: out here we have our chairs and the little circular tables. Here are two chairs, a little circular table, and another table for good measure. Imagine this is an abstract representation of what's happening in a cortical column. You have this arrangement of things, and you want to learn that arrangement. A model is a representation of things in the world at relative positions and orientations. I want to know what that seating area looks like. I have a picture in my head right now of what it looks like. You probably do too, if you've been here a while.

How is that represented in the brain? The brain represents it essentially by saying there's a chair at some orientation and position to the table, and there's another chair around the table. These green links here represent the relative displacements. That's the model. It is composed of locations and orientations of chairs and tables relative to each other. That's how everything is learned in the world—things are relative to each other. That's what's going on in your head. Everything is composed this way: bicycles, tables, buildings. It's how we learn things.

The interesting thing, which really boggled my mind for many years, is how our sensors move around relative to these things. Your fingers are different from your eyes because your fingers are touching something and your eyes are not, and so on. Here's the basic idea: you have a sensor patch, represented by these little rectangles. This could be your finger or an eyeball. In this case, think of it as part of the eyeball. The way you learn this model is you're at some position, you observe these components, and as you observe them, the brain calculates the links to the relative displacements. If I were observing the same chairs and tables from this position, I'd end up with the same model. It's in the model—it's independent of where the sensor is—but the cortex has to know where the sensor is. That's incredible; it has to know where the sensor is at any point in time, but the model it builds is independent of that sensor location. It's this interaction between local knowledge of where I currently am and the model you're building, which is independent of that.

We also know that columns are sensing things in different locations and orientations and building models. Later on, you can learn it from different positions, infer it from different positions. I can learn the arrangement from here, observe it, and recognize it from here. I can extend it by observing it from a different direction. We also know that columns have to learn the behaviors of things in the world. Some things move—using a stapler, for example, or a smartphone—things move and change. The models of things can change and move. Columns also move in a sequence of these displacements. To learn how something behaves, you're learning how these things are moving relative to each other. There are sequences of how these things are changing; it's not random. A stapler opens and closes in a certain way. There are a lot of details we don't know about this, but the big picture is pretty clear.

Down here at the columns, they perform very complex functions. It's really complex and was very hard to understand. We know a lot about how they do this. We started with these details and ended up with this big picture. We learned a theory of sparsity—we didn't know that when we started—why sparsity is important and the roles it plays in representations. We had to come up with a theory of dendrites and how neurons use their dendrites. That, to me, is one of the biggest discoveries we made at Numenta—a real theory of dendrites and neurons changing that. We had to come up with a theory of some parts of what many columns do. If we have a laminar circuit, that would be our temporal memory circuit. We've also now included things like grid cells and vector cells—not things we discovered, of course, but we were among the first to realize they're inside every cortical column. We made the prediction that every cortical column would have these grid cells, which come from the entorhinal cortex, before we had any evidence for it. There was evidence, but we didn't know it. I think it still counts. That was a remarkable prediction. Even today, people wonder if V1 is going to have grid cells. Yes, it is, and even as people are starting to get evidence for it, others dismiss it.

We have a lot of details here. I'm not going to go through all of them. We spend hours reading about these things. The point is, there are many things that are still confusing to us. Even though we have the big picture and lots of details, we don't understand it all. To be frank, there are a lot of things we just don't quite get.

That doesn't take away from what we've done; it's just being honest. One of the things that came out of the Thousand Brains theory—this idea that columns can do all this stuff—was the realization that they can vote. We talked about voting, but that wasn't even our focus. Voting is actually in the question number two area: how do columns work together? Up to that point, we hadn't been thinking about that at all. There was no concept; we were just thinking about cortical columns individually. Then we realized columns can vote. They can share their hypothesis about what they're sensing. My eyes and my finger can agree, or multiple fingers can agree, on what they're sensing. We did a lot of simulations of this, many by Luiz. They share a hypothesis about what object they are sensing, and this explains a lot. It explains how we can sometimes recognize something with a single glance, instead of having to move our eyes or fingers. Importantly, voting works across modalities—between different types of sensory modalities like touch, vision, and hearing. It's a pretty cool idea. We modeled that, simulated it, and included it in our papers. We did know there was a hole in the voting. The simplest way to explain it is with the example of touching a coffee cup: you want multiple fingers touching the cup, and they all vote on what they think they're sensing. But we also now know they need to know their relative position to each other. In the machine learning world, if you don't know your relative position, it's called a bag of features. It works, but it's not very powerful. We knew that, and we put it in our papers and in my book. We didn't know how it worked; we just said somehow they're going to know where they are relative to each other. So we put it all to the side.

That summarizes what we've learned in the last 10 years. Any questions about that? Disagreements? Additions? No? All right, let's go on.

Now we're moving on to number four, the roadmap. For those online, I didn't take a picture of the roadmap, so you don't have this little picture here, but it is in the post I made on Slack. It's pretty much the roadmap we're all familiar with, so it shouldn't be a surprise. We have this great idea—what do we do with all this? Are we just going to start building a brain? We don't really know how to do that yet. There are a lot of things we don't know, but maybe we can start implementing these principles in existing deep learning networks. That's the idea behind applying them to DNNs. It's worked really well. We started with sparsity, thinking it could help with robustness and speed, and we've shown that it really does work. We've made great progress, and the work from the hardware team has been amazing.

Next, we worked on applying dendrites to deep learning networks, which has also gone well. This relates to continuous learning and robustness, and the work there is exciting. But all of this is about applying these ideas to DNNs. The next point on our roadmap was to take the idea of reference frames and apply it to deep learning networks. We were ready to start but didn't really know how to do it. We were struggling to understand it, maybe we could figure it out with more time. Some people are trying, like Jeff Hinton with his Capsules and now Glom. It's also related to SLAM-type robotics. We were struggling with that, and if you recall, we had a meeting about a month ago where everyone contributed ideas about how we might add reference frames and what to do next. We were trying to figure out the next step.

Then a couple of people mentioned voting, which was a real awakening for me. On our roadmap, we had another category—a catch-all for jumping over to the Thousand Brains theory, building intelligent machines. Voting was part of that. What struck me was that we hadn't thought about how the columns work together. We'd been so focused on understanding what a cortical column does that we hadn't reminded ourselves to look at this again. After that meeting, I started thinking about voting because it seemed interesting and maybe there are things to do there. That's the lead-up to the AI bus.

Any questions about that? Thank you. Does this mean we're not doing the fast learning reference frames? No, it just means we didn't know how to proceed. We didn't have a specific task or way of implementing it. I didn't have it, and I don't think anyone else did. It's not that we didn't want to do it; we just didn't know what to do. It wasn't as clear as applying these principles to existing DNNs. Maybe there is a way, and I'll come back to that. It's still a viable option; we just didn't know what to do. It wasn't a question of wanting to do it—we just didn't know what task to start on, what the machine learning goal or benchmark would be. We're thinking about it, but we didn't have a clear direction. If someone disagrees, speak up, but as far as I could tell, we couldn't figure it out. We didn't know what to do there. But remember, this is always on the path to the final goal. It was never the end goal itself. It was a way to experiment and learn, like taking sparsity down to the CPU, which will be important for the final theory. The dendrite itself will be important for the final implementation, and the reference frame will be important too. The idea is to break off things along the way that add value, maybe monetary or PR value, but ultimately, we want to go further. We're not in the business of just enhancing DNNs; that's too small a goal.

Is that clear? Can you talk about the relationship between voting and learning? I can, but that might be a longer conversation.

Almost everything here is learned. There's not really anything that isn't learning something. When you say the word "learning," you might have a specific idea, like what it means to learn as a human, but columns have to learn how to talk to each other during voting. They have to learn their reference frames. From both a neuroscience and a machine learning point of view, all these things are constantly learning and adjusting to each other. If you're talking about how I learn something as a human, I think I'll adjust that more. Maybe we could just leave that question for later. I want to get to a bigger picture, and I think I'll cover that. 

What voting really was at this point in time was a way for different sensory patches—parts of your skin, eyes, ears, whatever—to work together to reach a common consensus. If I touch a coffee cup with one finger, I have to move it to recognize it's a coffee cup, but if I grab it with multiple fingers, I may not have to. How does that work? If I look at the world through a straw, I have to move around. That's one column in your cortex. But if I show you an image, somehow all the columns know what it is. How is that?

One of the basic things I know from lab work, and now understand, is that the brain works by moving its sensors. Under certain circumstances, we can recognize things without moving, but those are the exception, not the rule. The common method is moving, sensing, moving, sensing. Let's put that aside. Good question. I don't want to discourage questions, but I'll get to that later.

Now we're going on to number five: the AI bus. We were just joking earlier—I'm also calling it the cortical communications protocol, but now Marcus says that's a communist party or something. I just threw this name out. We don't have the right names for these things. A lot of people here didn't even know what a bus was. It's an old computer term. If you've been around early computers, you know what buses are, but software engineers might not, so there's a lot of confusion about that. Basically, it's a communications protocol. It doesn't have to be anything physical. In the brain, it's these long-range connections between columns. You have all these columns, and there are these long-range connections. Some go elsewhere and come back, but there are these long-range connections. So it's something physical in the brain, but it's really just a communications protocol. If you're going to implement this in machine learning, it doesn't have to be physical; it's just a way of saying, "Here's how we're going to talk to each other. This is how you put information in a certain format."

This is a marketing problem to figure out what you want to call this, so I want to talk about that next. Up to this point, what we're dealing with now is question number two: how do columns work together? As I pointed out, voting was incomplete. For voting to work, the columns need to know where their sensors are relative to each other, and the voting system we outlined and tested didn't do that. It was a bag of features.

Thinking about that, we already had all the pieces. We've already been talking about these components, and that led to the idea of the AI bus. I'm not going to be able to go through it in detail, but I'll try to give enough detail to get a flavor for it. A lot of the details we don't know yet. The idea is that these are like cortical columns. We're now starting to call them modules because we think they might actually be more than one column—maybe a "where" column or a "what" column. It's a little confusing. From a machine learning point of view, I might say "module," because I don't think there's a direct correspondence now with cortical columns; there could be two different columns combined. I'll use that language interchangeably.

Each of these columns has a sensor patch, which is moving around. It needs to know, as it's observing some object, what that object is. It knows the sensor is at some location relative to the object, and the object is at some orientation to the sensor. This kind of information, along with a lot of other information, is inside each column. This is the object being sensed, and this is the location and orientation of the object to the sensor. These locations and orientations are all different—one finger is over here, another finger is over there, my eye is out here. These are all different positions in the world, different orientations. At this point, there's no commonality between them; they're all little worlds.

What I realize now is that, in addition to sharing what they think the object is—that was the voting we wrote about in our papers—they share something else: a sense of location and orientation. The way they do that is by picking a common point, like your body, and saying, "Okay, relative to the body, where is my sensor?" Each one says, "Relative to some central point, where is my sensor?" That's what they communicate to each other. They say, "I'm seeing some object at some location or orientation relative to a common point," and each column has to convert its local sense of location and orientation into this common one. Then they can also take this common one and convert it back into a local one. This way, everyone knows where everyone else is—or at least, they all agree that they're all looking at the same thing at the same point in time and space.

This struck me as a real revelation. It reminded me of when we realized there was a reference frame in each cortical column. Initially, we just proved the existence of a reference frame, but we didn't know how the columns learned or anything else. When I understood there was a reference frame in each cortical column, I realized this would be the core piece to explain how the columns work and build models. That turned out to be true. I feel the same way about this. It's a little hard to explain why, but I feel like this is the key thing that's going to break open the whole problem. Let me walk through some of the things you get from this architecture.

Imagine that I might be looking at this cup and notice a feature at some position relative to my body. The column representing my finger knows where it is relative to that feature too. I know where I am relative to that, and I'm going to do it relative to the body. This column knows where it is relative to the body, so if it wants to touch the object, it knows how to get there. When it gets there, it knows what it's going to feel. Even if I close my eyes, I can touch it because I saw it there and that was communicated. I'm going to feel the same thing I just saw in the same location.

Is the communication between the columns passive, in the sense that they're just sending information on the bus, or is it active, where they can actually request specific information from other nearby columns? I think it's active. I'm not sure they're going to request, but I'll give some examples in a moment.

I'm going to talk about attention. I think it's more active in the sense that these columns have models of things, like the chairs over there, but they don't all have models of everything. For example, in my house, I have a visual model of the rooms from my bedroom to the bathroom, but I don't go around touching everything, so I don't have a tactile model. In the middle of the night, I can use the visual model in my head, even though I can't see anything because it's dark. I know that at some point I should be feeling something at a certain location. Essentially, the visual system is saying, "I need someone to go out and find out where that edge is," and my fingers respond, "Okay, I'll do that." When I find it, I know where I am. They can share knowledge about the models and inform each other. You can request information. I'll talk about attention in a second, which is not purely active, but it's an active system, and I don't understand all of it yet. I think it's active.

Some of the things you get from this include sensory fusion. This means I can build a system—an animal or a machine, it doesn't matter—and combine any number of sensors or sensor patches, few or many, in any location, even outside of my body, and in any modality. They all work together almost seamlessly, as long as they agree to share a certain type of information. It's all going to work.

In nature, there are all kinds of senses in mammals. There are many types of eyes: eyes that move sideways, eyes that go forward, eyes that see black and white, eyes that see color and black and white. Cats have slit-shaped eyes, which help them. There are different types of eyes. The same goes for touch—there are multiple sensors in your skin, not just one. For hearing, bats use echolocation, which is another type of sense. Rats use their whiskers as active senses; they can feel objects with their whiskers and recognize objects. Each whisker goes into a single cortical column. It's the clearest example of a sensor patch: a single hair sweeping back and forth, detecting something at a distance and determining where it is relative to the body. In a sense, it can "see" with its whiskers.

If you haven't seen it, take a look at the star-nosed mole. It's really ugly, but it has another sensory system with probes coming out of its face. It's like its face exploded, but it just plugs into the cortex like anything else. The point is, you can build almost any sensory system as long as you provide the right information, and it's going to work. When we build intelligent machines, we don't have to be restricted to what biology has. We're right here.

Radar, ultrasound, or any kind of unusual sensors you can imagine don't really matter. Anything that can determine a location, sense something, and identify its features—as long as it can be isolated in space—is important. Even sound is isolated in space; when you hear something, you know where it is. That's essential.

When it comes to biological systems, consider how this applies to machine intelligence or robotics. Imagine a robot walking around a room with sensors on its limbs, eyes to see at a distance, and hearing. You could add new sensors, like infrared or radar eyes. You could also place a camera in the corner of the room. As long as the robot's sensors and the camera know their relative positions, the camera becomes an eye for the robot. It would be seamless, like extending your hand to touch something. Some sensors could move, others remain stationary; as long as the camera can attend to a part of the visual scene and know its location, this works. It's mind-blowing to consider how sensors can be arranged in different places.

You could have two robots in the same space, each with its own sensors, but if they share a common reference point, they act as one—like how your two arms work together. They're separate, but they function as one because they share a central idea of their relative positions. This allows for the creation of temporary robot arms. It's a wild concept, but I think it's going to happen.

Another realization is that sensor fusion is a byproduct of the AI bus, not just voting. The key isn't the object ID, but the relative location and orientation of the object being sensed. Each column has its own specific sensor, and fusing the output of the columns also fuses the sensor input. The crucial part is that all columns know they're attending to the same thing and understand their location and orientation relative to it. It's not just about identifying the object; the more important aspect is the shared knowledge of location and orientation. This allows for coordinated sensing and movement, binding all the columns into a single entity.

Voting is a way of aggregating information, including location and orientation. Not everyone is certain about their orientation and location, but they can help each other determine it. For example, if you're watching me and hear a noise, you might not know exactly where it came from. The auditory system detects a general area, and your eyes quickly look there to identify the source, like someone opening a door. These systems work together, refining the sense of what, where, and how something is oriented. Others can contribute, and through this process, a common agreement is reached. But the bus concept is more than just voting, and that's something I'll try to explain further.

These are new ideas, and more will emerge. The concept of attention is another area I've struggled to define. It seems like isolating input to a part of the sensory array, but that's not quite right. Attention is a way of communicating an area of interest in a shared space. It's a broadcast that invites everyone to focus on a specific location. For example, your eyes might converge, you might move your hand, or turn your head to attend to that spot. It's less about restricting input and more about directing where to focus sensory input in the common space.

If you hear something and turn to look, or see movement and immediately focus on it, or feel something unusual and look at it, that's attention in action. Anyone can send an attentional signal, asking others to look at something unclear. It's an automatic system.

The bus also relates to other high-level concepts, like episodic memory. In the brain, the cortex is a sheet of neural tissue with many similar columns. If you follow this sheet, it folds underneath and becomes other structures, such as the entorhinal cortex and the hippocampus.

These are structures that are not cortex, but they have similarities. We've been thinking that the way cortical columns work is picking up in ways that these structures work. The hippocampus is associated with episodic memory. If you're not familiar with this, consider questions like: What did you do today? What did you do this morning? What did you do yesterday? All that information is stored in the hippocampus, not in the cortex. In fact, everything you can recall from the last three or so weeks is in the hippocampus, not the cortex. We know this from cases of people who lost their hippocampus; they could no longer form new memories and remembered everything up to about three or four weeks before surgery, but after that, no new memories. Every day was like waking up with no memory of the day before. For example, a famous patient could be talking to someone, turn away, turn back, and not recognize the person.

Over time, long-term memories belong elsewhere. If I ask you to remember where you grew up, that's stored in one place, but if you move to a new house tomorrow, that memory would be stored in the hippocampus, and if you remove the hippocampus, you never form these new long-term memories. There has been a theory that these are very fast memories and that somehow they are transferred back to the cortex over time. I never believed that hypothesis because I couldn't see how it could work. I couldn't imagine how the hippocampus could transfer memories to something else. There is evidence, but it's not that simple. There's evidence to suggest these memories are restored anyway, but you do need the hippocampus. It's complex, but the point is that it became clear how this might work.

The information going over the bus is the kind of things you are perceiving from a body-centric point of view over time. For example, picking up a coffee cup in front of you, moving it to the side, seeing someone else do something—this is the kind of information going over the bus. The things you're absorbing at different positions relative to the body are exactly the kind of memories you have in episodic memory. Episodic memory isn't just "I generally eat muffins in the morning," but rather, "This morning I had a couple, the refrigerator was empty, and I ran out of milk." It's very momentary, specific, and egocentric.

If you implement a fast memory here and just record what's going on in the bus, you can play it back. The hippocampus could transfer memory back to the cortex because it's just transferring it back in this format, which is what's needed to learn models. If you tell me there's an object at a certain location, you can build these models in the columns. You can implement episodic memory this way, and it provides a clear way for how this would work in a machine intelligence system.

Today's AI doesn't do anything like this. You can't ask a deep neural network what it was thinking a moment ago; that doesn't exist. But with this kind of system, you can ask, "What steps did you take to get here? What were you doing? What were you thinking?" In the brain, this is a limited amount of memory. Memory is difficult, and fast memory is difficult, so we have a limited depth of episodic memory. In a computer, you can dig as deep as you want; you can have deep episodic memory if you want to.

I also realized that language is going to emerge from this as well. These are partial ideas; I'm not claiming to fully understand language. However, I believe the bus idea provides a framework for building language. If you consider what language is—if I want to communicate something to you, whether spoken or written—I need to take what I'm thinking or imagining and express it in a form that language can capture. For example, if I'm on the phone and say, "I'm looking at the Numenta coffee cup in front of me, the handle's turned away from me," and then ask, "Why did you put the logo on like that?"—by putting this information on your bus through language, I can have you imagine and view the same thing I'm seeing. It works both ways: I can share my experiences or memories, whether episodic or long-term, and express them so they enter your brain and short-term memory, or you can use your models to understand them. Language is a way of sharing this kind of information between entities, and this information can be associated with words and so on. It's complicated, but the framework exists for doing this.

Language is not some distant part of the neocortex that requires many steps to reach. Most of the time, everyone is looking at the same things, and you can choose to express what you're seeing, thinking, or have memorized. This provides the foundation for language.

Finally, I think the AI Bus will provide a foundation for robotics. I don't know much about robots, to be honest. I know that today's robots are not very good; they're not fluid and can't perform tasks well. However, it struck me that cortical columns all have a sense of location in their space and a motor output—they can move themselves. Moving the sensor could mean moving your head, eyes, or hand. All robotic movements essentially come from cortical columns. The challenge is coordination: how do all these cortical columns, each knowing only a little about itself, coordinate to achieve something unified? I don't know the answer, but I now understand how they could coordinate and unify their sensory experience, how they can all see something and recognize it together, and how they can coordinate actions. For example, someone might say, "We need to attend to this, move your hand there," and so on. Robotics will be built on the same concept. I believe that if we spend time on this, we'll figure it out, whereas before I wouldn't have known where to begin.

These are some of the basic capabilities the AI Bus provides. I feel like it's completing the picture—not that we know all the details yet—but it's clarifying how the cortex, as a collection of cortical columns, works together. It's all about locations, common locations, and understanding and manipulating space together. That will be the unifying concept for all these functions.

I'm not going to go further on that, but let's keep going unless there are questions.

You mentioned motor output. Yes, that's part of the bus. We know every cortical column has a local output and must know how to move itself, but they have to coordinate. The translation between local knowledge and shared knowledge is the same for robotics. I don't know how it will play out yet, but it's going to happen. For example, globally we might want to push a button, but locally, who is close enough to do it? Maybe I want to specify, "Use your left hand," or "Use your right hand," and then that gets translated into a local signal: "I know where I am relative to the button, so I'll push it." At a higher level, we're just saying, "Somebody needs to push the button." There might be another rectangle labeled "motor command." I don't know yet; I don't want to pretend I have it figured out. I just noticed the connection.

Sensor fusion also involves movement. Everyone has to attend, which is a form of movement—whether you move your finger, turn your eyes, or covertly attend to something. So sensor fusion already involves movement, but now we need to extend it to movement that changes the state of the world or accomplishes a task. It's very much related. I've always thought robotics and AI would be unified, as they are in the brain, and now I'm beginning to see how that will develop.

Alright, that's the end of that topic unless there are more questions. Now I'm going to switch to something completely different: the birth of industries. I don't have any slides on this.

This doesn't apply to all industries, but it applies to some. I want you to at least understand what I'm thinking so there's no confusion.

Let's start with Amazon. Jeff Bezos saw the internet early on, and Amazon was founded in 1994. There were just the beginnings of web browsers, and there were no established protocols. It was really crude. He saw an opportunity to create the largest marketplace in the world. That was his goal: to create the world's largest marketplace. At that time, web browsers were terrible, the protocols were terrible, and no one wanted to put their credit card information into a computer. People thought, "Who's going to do that?" But that's what happened.

How was he going to do that? He chose to start with books. He wasn't a bibliophile; he didn't care about books. He didn't say, "I love books, we're going to sell books." There were other online bookstores doing that. He said, "No, I'm putting it in the world's largest marketplace. Books are a good place to start." There was a disconnect—he was talking to people about the world's largest marketplace, but then he was selling books. I'm sure he was losing money hand over fist, and there was a lot of criticism on Wall Street. People were saying, "This guy's losing money, he's crazy, he's never going to make any money, he's just spending all his money. Why can't he make a profit selling books?" It's because he didn't want to sell books; he wanted to create the world's biggest marketplace and was using books as a way to build out infrastructure. 

Books for him were a way to establish regular relationships. People had to write things for him. He had to figure out rankings, recommendations, logistics, and shipping. All the things he needed to do, books were a good way to practice. Yet, if you worked for Jeff Bezos, you might think this was boring. What happens in the big marketplace? What happens in the world? Amazon is the world's largest river, and that's why he picked the name. He said, "It's the world's largest river, collects the most water anywhere in the world. That's us. We're the Amazon of retail." That's literally why he picked the word Amazon. He knew this, but he was doing something that looked pretty boring. There were other book vendors, and some people thought a book vendor was better than Amazon. He said, "I don't care. I'm not building a bookstore. It looks like I am, but I'm not."

That is a common property when you're doing really big things. You're often on the edge of what's feasible, and you have to pick some subset to get started. We had our own experience of that at Palm. Palm was formed to be, literally, the one-line summary of Palm at its inception was, "We're going to build the future of personal computing." I used that phrase over and over again: future of personal computing. Everyone's going to have one of these things. There are going to be ten times more handheld computers sold than desktop computers. It's going to be the dominant personal computing platform. But we couldn't really build that initially. That was the vision, but it wasn't possible. Too many things were missing. So we ended up building an organizer. I had a lot of employees who were pretty disappointed about this. Some were basically ready to quit, because an organizer—what? Address book and calendar? This is the future of personal computing? It's not even independent of the PC; you have to plug it in. It's an accessory. I said, "No, trust me. We're really independent from computers. We can't do all that other stuff yet, but here's where we can get started today. Just don't forget that when I'm in the organizing business, that's not what we're doing."

I remember a colleague who was on the board, the VP of marketing, and we had a big debate about what to call the first product. I wanted to call it a hands-on computer, and he wanted to call it a connected organizer. He wanted to sell something, and he said, "That's what it is." I said, "Yes, but Ed, it's not that. I don't want to mislead people. I want everyone to know we're building the future of computing." He won; we called it the connected organizer. I don't think it matters—he's probably right. I wanted to go for the big thing. He said, "Don't worry about that. We need to sell something today. We'll get there." 

Now I have a big debate with Donna about whether we should expand sales internationally or put our engineers on developing the development kit so independent developers could write apps for the organizer. What are they going to write for us? I don't know—they'll do games, they'll do something, they'll figure things out, but we have to go in that direction. I think in the end we ended up doing both, but there was that kind of dynamic we had to deal with. I think we're in a similar situation here. I want to explain why I think this is such a huge business opportunity. Yet today, you have to get started at something that might seem a little dull or boring. That's the point of that story. I don't want anyone to get discouraged by that. I don't want anyone to think, "What happened to that big vision of the future of AI?" Just because we have to do something that doesn't look like that vision for now.

Okay, that was the end of number six. Let's go on to number seven. I'm going to propose a sort of roadmap, the AI bus roadmap. It doesn't replace this roadmap; this roadmap is still here, but it's a different sort of roadmap. What would we do with the AI bus? The first thing to focus on is sensor fusion. That's something you can do today. To do this, we have to start building something and test it. We can define prototype bus protocols, and we've already started that. The prototypes will change—what is actually being sent, what's the representation, how do people talk to it, and things like that. We don't know yet. We know what it is, but we also need to build test systems to evaluate the protocols. Do they work? This is not a product. This is not deep machine learning. This is just seeing if we got this right. It's along the lines of what we did when Luiz created the virtual robotic arm that touched things. When we first started, it was almost embarrassing. Luiz started emulating the robotic arm, and as soon as the robotic arm started touching something, it didn't work at all. I was like, "What is going on?" It was so obvious. I don't know how we missed it, but we did. We didn't have any concept of orientation of the fingertips. What it sensed would be different depending on its orientation, and we didn't have that. We just missed it completely. We didn't discover that until Luiz made the robot, and we realized it didn't work.

I don't know if it's going to be like that, but we have to do things like this, just to start testing out, "Oh, we forgot about this," or "We didn't do that." To evaluate the test system, we're going to have three modules talking to each other, something like that. We have to create test modules. We have to have something to plug onto the bus, something equivalent to theseNow, this is where I misled a lot of people. I said, in my mind, I'm thinking we're going to build these test systems, and I have to have modules to plug into it to test the protocols. I don't really care how those modules are built—they can be hacked together, they can be kludged, it doesn't really matter. We had this conversation in this room where I said, build it any way you want, as long as it works with the protocol, I don't care. That was construed as abandoning the neuroscience, and that is not true. That was just what's required to test the protocols. This is all neuroscience. The whole AI bus is neuroscience. It's derived from neuroscience, and I just misled people there. I apologize for that. That's what I meant. It's not that we're going to use the ultimate—these can be hard coded. I don't know what the right answer is. We do unit tests and so on. It doesn't really matter too much at the moment. We're trying to understand the bus and its protocols, how it works, and how we build the modules and plug onto it. We're not going to adhere to any strict purity system here. That's what I did, and I apologize for creating that. We're probably going to have to iterate this a few times, just to experiment, just like we did with the other systems we built. We iterated with the sequence memory, we iterated with the sensorimotor learning. I think we should do this. There's no rush, but we should do this because I think it's going to be really important, and we can start that today.

On the robotics side, as I said, it's a natural extension of sensor fusion. It's just an extension of it, but I don't really know enough, and I don't know if anyone here knows enough to do this. This is mostly an education exercise. We need to educate ourselves about robotics. What are the core problems in robotics, current path approaches? I don't know any of this. Maybe some of you do. I personally feel I need to get educated on this because I can't build anything here. It's like we're doing basic research. I think one of the things we're going to have to figure out to do robotics is we need a better understanding of how the bus, how we represent state of objects in the world—is a stapler open or closed, what's the state of your phone. We don't really know how to do that yet. That's going to be required. We can start thinking about this now, but there's nothing, there's no code to be built or anything like that, but it's something we can work on.

Ultimately, in the end, we're going to have to figure out how this is going to work with cognitive and abstract thinking. I don't want to think about that now; it gets too far out. We'll get there, but right now we need to really work on these components here. Mostly, we can start implementing this and testing it, and this we just have to think about and start brainstorming.

So that's how I view what we need to do here, what we should do. In some sense, that's filling this column over here on the original roadmap. It's a way of attacking the Thousand Brains theory.

Number eight: we have options for what to do next. We're not going to decide anything today. Let me just go through some of the things. We are going to continue to work on sparsity on GPUs and CPUs. That's really important, and it's also going to be important for the AI bus, so that has to continue. We're doing that, and it's great. 

On the dendrites and deep learning networks, we can continue working on deep learning networks if we have a PR or dollar benefit. It wasn't really clear what we would get out of working around dendrites for deep learning networks. We may decide there's an opportunity there, but that's not where we want to be. Eventually, we want to fold that into the AI bus opportunity. For example, right now, we don't know how the AI bus is going to work. My current vision is that it's going to work probably like the brain, but that requires sparsity and dendrites. Maybe we're going to use that in the bus. Maybe our bus will be based on principles of sparsity, and then maybe we'll figure out another way of doing it. I don't know, but at the moment, that's my assumption. The work we've done here has a near-term benefit, but it also has a long-term benefit. That was always the plan: to test these things here but bring them over. Let's continue with that. There's less clarity if there's a dollar or PR benefit for the current work than there is for sparsity. That's why we're willing to invest more in this one.

We had this question of reference frames in deep learning networks—should we work on that? We couldn't identify what to do there. I said we can pursue it if we can identify something useful to do in deep learning networks. Is there some PR or dollar benefit? If we could figure that out, we might want to work on it there. If not, we'll just fold it into the modules, because all the models are based on reference frames, and the whole thing here is translated from one reference frame to another. The whole system is built on reference frames. Even if we don't pursue that in deep learning networks or deep neural networks, we're still going to use it. The question is, we couldn't figure out what to do with it in DNNs.

Now we're saying we're going to do this AI bus research, which is just this stuff here. We're going to add that to our list of options for what to do next. I'm proposing we add that to our list of options.