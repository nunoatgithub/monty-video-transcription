Niels Leadholm: Good morning, everyone. My name is Niels, and I lead the research team at the Thousand Brains Project. Today, I'm going to talk about Monty's capabilities, why we're excited about them, and why they give us confidence that we're on the right path to developing more general, more capable AI systems. In particular, we often ask ourselves how we know, even at this early stage, that we're heading in the right direction—referring to the compass Vivian showed earlier.

To understand the importance of knowing whether you're going in the right direction, it's worth considering the story of evolution, specifically the development of eyes. Imagine you're a simple organism in the ancient oceans, and a predator is swimming above you. If you've developed a sheet of light-sensitive cells, that's already a useful innovation—you can notice the predator approaching and swim away. There are two ways to make this even more useful: form the sheet into a mound or into a dimple. These seem like subtle distinctions, and at first, both give you a sense of the direction of incoming light, helping you know which way to escape. However, this subtle decision has profound implications over millions of years of evolution. The mound leads to compound eyes, familiar from insects, while the dimple leads to camera-like eyes, as found in humans and other complex animals with large brains.

Neither solution is inherently better; each has advantages. Compound eyes are good for a wide field of view and small body size, while camera eyes provide high acuity vision, especially when powered by large brains. Once you go down one of these paths, it's almost impossible to go back and redesign from the ground up—you become committed to the initial template, which constrains all future solutions.

We believe a similar story is unfolding in the development of intelligent AI systems. In the 20th century, it was recognized that neurons are the brain's core computational unit, and various mathematical models were proposed. By the 1980s, it was clear that getting networks of neurons to learn was difficult, so backpropagation was invented as a workaround. At that point, there was a fundamental forking between biological intelligence and AI systems relying on backpropagation, which is not a biologically plausible form of learning. This led to what we know today as deep learning.

What we are pursuing is an alternative path, relying on local learning, as Vivian mentioned, and other key principles like reference frames and distributed modules, which you'll hear more about today. We believe these are the principles that enable the sensorimotor intelligence found in nature and are essential for developing artificial sensorimotor intelligence. Once you base your design on these early assumptions, it's very difficult to remove the resulting issues without starting over. We're seeing this in how some key players try to improve AI systems today, where the only option left is to scale systems larger and larger. As you've likely heard, these approaches are not sustainable from an energy or data perspective.

To illustrate why this is a problem, imagine evolution trying to develop an eye with the same acuity as a human eye, but based on the compound eye template. You don't have to imagine it—Kirschfeld in the 1970s calculated this and provided a drawing. This underscores the importance of identifying the key principles necessary to reach your end goal early on.

Every technology has its ideal problem set. Calculators are great for numerical operations, and deep learning is powerful for function approximation and generative sampling—AlphaFold is a great example. But if you want a system that can interact in open-ended, real-world, embodied settings, you need to embrace a different set of learning principles. That's our focus at the Thousand Brains Project.

So, what gives us confidence that we're on the right path? To answer that, I'll show some of Monty's current capabilities. The high-level summary is that we've designed Monty by looking at neuroscience evidence and reasoning from first principles about how the system should work. What's exciting is that a whole list of capabilities has emerged with very little effort. Any one of these would be a significant finding in machine learning. For example, symmetry recognition—the ability to realize that rotating an object around certain axes results in comparable rotations—is very difficult to achieve in a learning system without explicitly programming it. Yet Monty developed this naturally. Continual learning, a major challenge in deep learning, is also something Monty essentially has, simply as a result of its design.But importantly, we didn't set out to solve symmetry or continual learning. Instead, we aimed to design a system based on these first principles, and these are the kinds of capabilities we are discovering it possesses.

There's a lot on that list, and unfortunately I don't have time to talk about all of them today. If you're interested to learn more, we have this paper that Vivian mentioned, which we posted online last summer, and as she said, it's just been accepted in Neural Computation. I encourage you to take a look if you're interested.

To give you some context before I show results, Monty is looking at a series of objects—around 80 evaluated in a simulated setting, all everyday household objects. Monty's task is to look at these in turn, learn about them, and then, when presented with an object, both recognize it and estimate its pose, its orientation in space.

On the right, you see an example for context. On the left, the wide view is just for our benefit, not what Monty sees. It's to orient you in space—Monty is near a red mug and moving over it. What Monty actually sees, as Vivian mentioned and Scott will discuss further, is a very narrow perceptual input, like looking through a straw. By combining this with movement, Monty is able to do all the things I'm going to talk about.

The first capability is rapid learning. Humans naturally learn very quickly. If you look at a new object and study it for a few moments, you can develop a representation of it. You don't need to study millions of images to learn a useful representation. We've observed this in Monty. For example, with those 80 objects, if we show just one view of each—so Monty sees each from a single side and spends a single episode studying it—then present a novel, unseen rotation and ask Monty to recognize it and predict the pose, Monty can already achieve a classification accuracy of around 50%. In this dataset, with around 80 objects and many rotations, chance is just over 1%, so this is a significant finding. The way Monty does this comes down to how it represents objects based on their actual shape and how it transforms inputs based on its hypotheses.

Comparing to a deep learning system given a comparable amount of data, it only achieves around 30% accuracy.

This is an extremely challenging problem for deep learning systems, given so few views of an object. This finding holds as we extrapolate the data: if we give 8 views of the object—imagine holding it, rotating it, looking from multiple angles—and present novel views to Monty, we get around 90% accuracy, approaching ceiling performance. This is consistent with the idea that presenting Monty with different sides allows it to build a fuller representation. Looking at rotation error—how well Monty predicts the object's orientation—it does quite well, with about 40 degrees of error (lower is better). In comparison, a deep learning system with the same data achieves around 70% accuracy, but the rotation error is significantly higher, around 110 degrees.

Monty not only learns quickly like humans, but also develops robust representations. By this, I mean we can make the task harder and Monty still performs well. If you're familiar with machine learning, you might think object recognition is a solved problem, but even the best systems today struggle with unusual and adversarial conditions. For example, a pink elephant is obviously still an elephant to us, but deep learning systems, which often over-rely on color and texture, can be thrown off. In this case, one network thought the pink elephant was a flamingo, presumably because of the color.

For Monty, we took the objects it had seen in simulation and perturbed them to make recognition more challenging. One way was to add noise to Monty's estimated location on the object, which it constantly tries to estimate through movement. We also rotated objects in unusual ways it had never seen, and in the last condition, changed the object's color to something totally different—something Monty had never seen for that object.

Along the bottom, you see these different conditions, increasing in difficulty to the right. On the left, blue bars show Monty's classification accuracy; on the right, purple distributions show rotation error (lower is better). Importantly, all these perturbations are out of distribution—Monty hasn't been trained on them or seen this kind of noise during learning. For example, with the new color, Monty has never seen this mug in any color but its natural one. What's encouraging is the steady performance as we increase task difficulty. For context, chance performance here is around 1%.

It's also remarkable that, even for humans, in the case of changing color, some objects in the dataset are inherently ambiguous if you remove color information entirely.

Going back to deep learning, this is exactly the kind of perturbation that these systems struggle with—out-of-distribution scenarios they haven't been trained on. This leads to the kind of odd results I showed, like the pink elephant.

I've discussed the rapid learning and robust inference Monty can achieve, but we also want the system to act and move intelligently in the world. One encouraging finding is that, with our architecture, it's been relatively simple to add more intelligent policies. Policies determine how a system moves in the world, given the learned representations it develops. Here, I'm showing something we call the hypothesis testing policy. In this case, Monty is looking at a spoon. It starts at the handle and, like a finger, moves along the surface until it reaches a certain point. While sensing, it develops hypotheses about what it might be feeling. By the time it reaches the bottom of the handle, its top hypotheses are a spoon and a fork. It then uses its learned models for these objects to determine, based on what it sensed, where it should move to most quickly distinguish between them. This leads to a goal of moving to the tip of the spoon, since that's the point that differs most from the fork. Monty generates this goal, passes it to the motor system, and moves there. As Monty moves to that location, it quickly realizes the observations are consistent with the spoon, and belief in the fork representation drops. In larger experiments, this allows Monty to recognize objects more quickly and robustly.

A fundamental problem in deep learning, open for many years, is continual learning—adding new representations without forgetting previous ones. Humans are continual learners; we are lifelong learners without difficulty, so it can seem counterintuitive that continual learning is an issue. To make this more intuitive, consider how your brain learns: if you're looking at an object, your eyes move over it, you study it, and if you were holding it, you could interact with it physically. Very quickly, you develop a representation such that, even after 10 seconds, 10 minutes, or the next day, you can recall details about the object without issue.

In deep learning, things don't work that way. To learn about a new object, you can't just present new data and have the system learn; the new information must be interleaved with previous representations, and the system must retrain on all the data. If you don't do this, previous representations begin to fade—this is known as catastrophic forgetting. Monty does not show evidence of this catastrophic forgetting.

To demonstrate this, we use a dataset of household objects and change the task setting slightly. We present an object, like a LEGO block, and ask Monty to recognize it. Then we show another object, like a fork, and ask Monty to recognize both the LEGO and the fork. We present both objects and observe performance, continuing this pattern with additional objects, always testing performance on all objects seen so far. We did this for both Monty and a deep learning system for comparison. To do well, the system must remember the most recent object and all previous ones. On the x-axis, we have the number of objects learned so far; on the y-axis, the accuracy across all observed objects. With the VIT network, the deep learning system, we see catastrophic forgetting: when a new task is introduced, the network becomes hyper-focused on it, and previous information is forgotten. With Monty, there is a gradual drop in performance as new, similar objects are learned, but overall, it maintains strong accuracy, even up to 77 objects.

And it's worth emphasizing again, we didn't build Monty to be good at continual learning; this emerged naturally. The last capability I want to discuss is computational efficiency. You've probably heard that the brain uses about as much energy as a light bulb, while state-of-the-art deep learning systems require massive data centers powered by nuclear plants to learn. The question is, how can we create systems more like the brain that are much more efficient? Monty is already showing signs of this. 

If we look at the compute used for learning, quantified by flops—the number of computations or mathematical operations performed during learning—Monty is remarkably efficient. This is a logarithmic scale. We compared Monty to two deep learning systems: one trained on the same dataset as Monty, and another pre-trained on a very large internet dataset. Even with the much smaller amount of data Monty sees, it requires about 34,000 times less computation than the network trained on the same dataset. Compared to the pre-trained deep learning system, Monty requires about 530 million times less compute. The key point is that, given this small amount of training data, Monty is significantly more accurate than the other two systems.

The compute numbers are abstract, so to make this more concrete: if the compute Monty needed for learning is the length of a grain of rice, then for the deep neural network trained just on the small task, it would be the length of a shipping container. For the deep neural network also trained on the internet dataset, it would be the distance between Manhattan and Los Angeles—a significant difference.

I don't have time to cover all of Monty's capabilities today. Please check out the paper I mentioned earlier if you're interested. We're excited because these capabilities emerged from relatively little effort once we had the right design at the beginning. This is fertile ground, and there are many exciting capabilities not yet on this list. If you're interested in working with the Thousand Brains Project, this is an area where you could contribute.

With that, I'll hand things over to Scott, one of our researchers, who will discuss these principles and how they relate to the neocortex.