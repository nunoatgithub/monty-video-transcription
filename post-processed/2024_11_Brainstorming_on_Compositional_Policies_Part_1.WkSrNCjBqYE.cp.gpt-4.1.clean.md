Okay, I thought it would be useful to talk through an example. This is all very high level, but it's about how we might conceptualize hierarchical action policies, goal states, and related concepts. We talked about making a cup of coffee, and to your point, Misha, each learning module has layers. As a reminder, we generally break it up so that L4 is where sensory features come in, L6 is where the reference frame is represented—whether that's grid cells or something else—and L4 sends information down there. There are recurrent connections between those, and L2/L3 is more the object-level representation. L5 is the bigger mystery in terms of what we are going to implement and what we think is happening in the brain, but that's where direct motor subcortical projections go. Clearly, it has some importance in motor function, and the main thing missing from our systems is the planning part. There's a good chance that would be there. What's not shown, because it's very thin, is L1, but it is important because L1 receives top-down connectivity.

I didn't have a framing slide because I was expecting to show this at the end, but to frame this discussion, we've been talking about hierarchical action planning. If you have an abstract, complex task—make coffee—how do you actually achieve that? What would that look like in terms of the different learning modules? The exciting thing about Monty is that in reinforcement learning and policy planning more generally, there's a divide between model-free and model-based policies. Model-free is a simpler trial-and-error style learning, which deep learning systems can generally do well. Model-based is where you have an explicit world model that you simulate—like a kitchen scene—and you can understand how that model will change over time based on certain actions. If you have this explicit model, you can plan out entirely novel actions; you can choose a different path through your kitchen every day because you have a structured representation of it. What's promising about Monty or the Thousand Brains Theory is that each learning module is an object-centric representation with reference frames, so it is ideally suited to enable model-based policies at that level of representation. If we recognize that the learning modules are arranged hierarchically, as well as with lateral connections, then it's natural to think about how they might coordinate to carry out more complex actions.

Getting to the actual example, imagine you have a very high-level learning module responsible for planning your day. It determines the abstract structure of what you're going to do. The highest-level goal coming in—I've just put hypothalamus as a placeholder—but somehow the cortex is being told, "The body's very tired, let's get into a less tired state." That's the received goal. The day-planning learning module, given this goal of being in a less tired state, might know that making coffee can address this requirement. It's going to recruit a lower-level learning module, but still a fairly abstract one that models kitchens. It's going to send, or at least one possibility is sending, this information via L6, which predicts to L1, which in turn has apical neurons in L5. That means there's a natural way for this information to go down through the hierarchy to cells in L5, if those are the ones making plans.

The kitchen modeling learning module needs to achieve the target goal state of having coffee made.

This might be a setting where, rather than immediately recruiting another lower-level learning module, we might make use of immediate subcortical recruitment. You walk through the scene until you're at a coffee machine. This is a simpler policy that can be more model-free: you just need to be at a particular location. Maybe this learning module can send a series of target places to be so that it actually plans the route through the kitchen, but it's the subcortical structures that deal with, "I need to be here, I'm going to move legs until we're at that location."

Now, this has been shifted up. If you imagine, this is still the kitchen modeling learning module. Assume this has now been achieved: we've walked until we're at the coffee machine. Then, this learning module might send another broken-down goal state to an even more specific learning module, which is the coffee machine on state.

This is to the learning module that models coffee machines. Here, subcortical recruitment might be something like "find the coffee machine button." This learning module, which knows about coffee machines, knows that to be in the on state, it needs to find the button. Because it knows the model of the coffee machine, it can direct subcortical structures—those that affect the orientation of the eye, moving the head, and so on—until you find the coffee button. Once that's achieved, there might be a learning module that models machine buttons, and you want that in the pressed-on state. The subcortical recruitment in this case is pressure at a location. What this shows is how, depending on the task, it can either be decomposed further by recruiting a learning module that knows about that object, or the learning module will be in a setting where it knows enough to recruit subcortical structures to actually move the body to achieve the required state.

Can I ask a question at this point? Please. So that recruitment mechanism—what's the selection mechanism that picks from a plethora of behaviors so that it recruits that specific one to achieve a particular goal? That's a fair question. I think one of the biggest unknowns is exactly how L5 would learn and remember this, but you can imagine if you've previously encountered an environment like this, you would have, for example, in the kitchen, coffee was made at a time when the coffee machine was on. So you could have some association between that state represented here and a location, but it's not any location—it's a location associated with a particular goal. The analogy is, rather than moving to a location in physical space, we're moving to a location and expecting a particular state at that location. It's definitely very fuzzy at the moment how exactly we would do it, but it would be some sort of associative learning. In the future, if you want to reinstate that state, you have that association to say, "Okay, the learning module that was active at that time when this happened was this one, so I'm going to try recruiting that one." Jeff's word doc was basically outlining the fact that seeking sequential goals is tantamount to navigation in physical spaces.

With physical spaces, you have a limited configuration space of how you move yourself, body, obstacles, and so on. Goal states are much more abstract. In the kitchen scene alone, navigating to any interesting point is finite, but all the behaviors you could do at any one of those points is another level of complexity. I guess you're depending upon the hierarchy to cut down the number of choices at any particular point—these are the options, I'm at this point, I want to go, and somehow I've... Exactly.

The goal state that this learning module is going to be sending to this lower level one is going to be conditional on where you are, but also the goal state it's receiving from the higher level. In the kitchen modeling learning modules, as you say, there's a lot of stuff you can do in the kitchen, but it's received the goal state: coffee made. Once it's in the location where it can interact with the coffee machine and conditional on this, the natural thing is, "Okay, I remember at this time, the coffee machine needs to be in the on state." Just to be clear, the only path integration mechanism that we currently know about is through grid cells. Are we hypothesizing that is necessary and sufficient for this type of navigation, or do we just not know yet, and maybe there's another mechanism involved, but we're just positing that something exists? I think it's fair to say in general, we think that in abstract spaces, it'll still be something similar to a path integration. For example, in a family tree, it's a very different action space—if you have the parent relationship or the child relationship or grandchild relationship—but there are still some rules about what steps will bring you back to the same location and things like that. Whether grid cells can do that is unclear, but in general, there's evidence that's true. Simple recurrent neurons that can model grid cells can also model those kinds of abstract spaces. Right now we're just using these 3D Cartesian spaces, and in general, we think it'll be very simple to embed even abstract spaces as long as they're three-dimensional or less. The weirdness, more about abstract spaces in general, is how do you really learn the actions that are possible in that space? Maybe in some sense that just emerges naturally through a hierarchy—the movement information comes in and you learn, "Okay, this particular movement information is associated with this kind of displacement in the space." But it's definitely very uncertain how exactly we would implement it. Thank you. No worries.

The other thing to touch on is, with this hierarchy, you can imagine how making coffee isn't just about turning on the coffee machine—there's going to be a series of steps, of which turning on coffee in the kitchen scene, or even in the coffee machine space, there's only going to be one.

As you break down the task hierarchically, you go to the coffee machine, you turn it on, it's in the on state. You will then start receiving sensory information consistent with this being in the on state, and that's going to come into the hierarchical connections that go in the opposite direction from L2, L3 here to L4 here. Then I can tell this learning module, "Okay, the goal state I just generated, which was machine button press state, has been achieved because I'm getting sensory information from this lower level learning module that it is in that state." Now L5 at this level can create a new subgoal, which is maybe put a coffee pod in the machine, or whatever it might be. That's a high-level overview of how we were generally thinking about goal states the last time we discussed it, summer 2023.

Just to recap, that was a really great overview, Niels. When you conventionally think about hierarchical action policies, you think about decomposing an action into sub-actions and into smaller actions. But what we're proposing here is that everything communicated between columns are states, not actions.

That's everywhere. That's the cortical messaging protocol we've defined. Learning module outputs are hypotheses—a hypothesized state of the world that it recognizes, for example, "there's the coffee machine at this location and pose." That's the input to the next higher learning module. It also outputs possible states through the votes, and then it gets goal states through these connections that Niels described. Everything between columns is in the format of states of the world. Actually translating these goal states into muscle movements is done subcortically.

I think that's important to emphasize. That's part of what makes the feedback through the hierarchy of achieving goal states particularly simple, because it's literally just a comparison of what you have achieved.

We're going to get started a little bit late. Niels just recapped some of the Monty plans we had for actual policies and things we talked about last year. Let's get going then. I'm going to share my screen. This is the document I posted on Slack yesterday. Do you see that?

We see it. I don't know if everyone had a chance to read it, but I thought I would go through this document a little bit, not read everything, and then I have some images to work from.

Just a little bit of background here. In the last couple of weeks, Niels has been presenting our action policies, but these are all related to inference and learning. They're central and really important, but the output of the sensorimotor system is behaviors, not just for learning and inference, but to achieve change in the world and achieve goals. We really haven't addressed that at all in any formal way so far in our theories and the code. For us to have a really working system, we have to do this.

We need this, so let's get started on it in a serious way. With the recent advances we've made in modeling compositional objects, that was an essential first part because goal-oriented behaviors are also compositional, and the same relationships columns have when modeling objects are going to apply in modeling compositional behavior. That's what these notes I wrote up yesterday are about. If you have any questions about them, I'm just going to talk about them briefly before I go on, but I want this to be interactive, so if you didn't like something or didn't understand something, just speak up immediately. 

Goal-oriented behaviors are complicated. That's obvious, but it's worth stating. This is going to be a very difficult thing for us to solve. I'm confident we can get it all done, but it's not something we're going to get done in a day or a week. It's going to take months to figure this all out, if we're lucky. There are a lot of things that have to be put in place. I listed some of them here. In the end, we're going to have to figure out how columns work together horizontally or laterally and hierarchically in achieving goal-oriented behaviors. But as always, I think we should start with a single column. That's what I did today. I'm going to talk about how we could think about goal-oriented behavior in a single column. I'm making good progress on the problem of single columns. We've talked about some of these ideas before, but they weren't really crystallized as much as they feel to me now. Maybe there's some totally new things here, I don't know, but I do think I've made progress in putting it all together.

The main thing we're going to talk about today, or at least as a starting point, is this idea that we've always known that models of objects, or objects, can have behaviors. We've talked about the stapler opening and closing, we've talked about apps on a phone changing, and so on. I'm trying to get serious about understanding how a model learns behaviors, because behaviors are going to be key to understanding objects and organisms.

Today, our models are defined by a location space. At locations in that space, the model can observe features at orientations, and that's how a model is built up: with location, space, features, and orientations.

The way we modeled it, based on the entorhinal cortex, is that we're using grid cells, and grid cell modules can re-anchor. Each object has its own space in some sense. The SDRs representing the locations in space for any object are unique, so an object is associated with a particular set of grid cell anchorings. We're going to introduce a concept of state, and this is my proposal. State is another variable that says within the context of a particular anchoring of grid cells, within a particular space, an object can have different features, different feature locations, different feature orientations. It can be completely different. Anything can change on an object based on its state. The state allows us to say staplers don't always look the same, or this object doesn't always behave or look the same. It can have different shapes and different features. Anything can change, actually. Yet it's still the same object—a stapler is a stapler whether it's open or closed. We're going to stick with the idea that the object is defined by its anchoring of grid cells, but a state lets us say what it looks like now and what it looks like a moment later. That's what I'm describing here. I have pictures of this we'll get to in a second. The idea is that these two variables converge. If I think about what defines an object, I have a location in its space, some observed feature, and the state. What is predicted at any point in location will be determined by both the location and the state of the object.

Then I define the idea of a goal. A goal is essentially a column trying to get an object into a particular state—a state that it's not in right now, but we want to get into that state. This is a very small part of goal-oriented behavior, but it's what an individual column can do. It says, I'm observing an object, it's in one state, I want to get it into a different state. Someone told me I should get it into a different state. This provides a basis for setting a goal for the particular object.

I then move on to what is called the behavioral space. How is this state represented? I propose that states are represented the same way, using grid cell-like modules. There is a space of behaviors, or states of behaviors, of an object. This is a parallel space to the space of locations of an object, and it exhibits the same kinds of behaviors. In the behavioral space, we can do path integration, set a goal, and know how to get to that goal. I'll walk through this with a picture in a moment.

One advantage is that we know the entorhinal cortex, when an animal is in an environment, does path integration, but it can also do the inverse. The inverse of path integration asks, given where I am now and where I want to go, what movement should I make to get there? This can be a novel direction—an animal can wander around and, when it wants to get home, know which way to go, even if it has never traveled that way before. We assume the same thing here: within the behavioral space, I have an existing state and a desired state, and the column can determine how to move in behavioral state to reach the desired goal.

This means the columns have two sets of grid cell-like modules—this is my proposal today. There are many questions that arise, which we'll discuss. What is the equivalent of movement in a state space? What does it mean to move in a state space? It's not a physical movement. What does that mean? What is the equivalent of path integration in state space? How are movements in state space converted to movement in physical space? Ultimately, the column has to do something physical, but it will be thinking in terms of a different space—a state space.

As we're voting and temporal pooling in this system, just a quick aside for those reading this: I thought this addition of state to an object might be a solution for context classes, like different mugs that are completely different but both called mugs, even though they don't transform from one to the other. I was excited that this might solve the classes of objects problem, but I don't think it does. I'll leave that aside for now. I'm going to continue with the presentation. Before I do, are there any questions? I want to go through the same ideas again, but with images. Are there any questions before I do that?

Large or small questions, big questions?

Are you still there?  
Yeah, nothing specific at the moment. I thought it would be interesting at some point to talk about grid cells and how they might map onto states, but maybe it makes sense to wait.  
I'm going to talk about it a little bit right here. I'll try to get to it. I don't have answers to all these questions, but I feel we're getting pretty close. This slide is labeled "Two Parallel Systems." You can see my slide here, right? Yes?

I'm just confirming that we can see it. I'm still trying to understand states and goals. I'm still processing this in my mind—I only read this yesterday, and it's a big concept. I'm seeing if this presentation will help me; the visual approach might help more. Hojae, I've been struggling with these issues for years, and it's really hard to understand. Sometimes I get clarity, and I don't even know how to describe it. I'm trying to put words to things that aren't really verbal concepts for me. I expect this to be hard to grasp right away, and I don't even get it completely. We'll go through it a couple of times, and it may take a few presentations before it sinks in, or we'll find out where the holes are, what's good, and what's bad about it. I'll try to present it here in this picture, and it might help a lot. There's an idea in neuroscience that's been around for a long time—50 or 60 years. Decades ago, people observed that the upper and lower layers of the cortex seem similar. There's a parallel construction. I've talked about this before. It's an idea I read many years ago, and it's intriguing, but I don't really know what they're made of. Why do they say it looks similar? Why does it look like there are two parallel systems? What's happening in the upper layers that's similar to the lower layers? The first thing to notice is that there are cells arranged in minicolumns in both the upper and lower layers. It doesn't go all the way through the cortex—layer four is a no man's land in between—but in these minicolumns, the cells are directionally tuned, meaning they respond to movement. This is mostly in vision, so they respond to movement; if you put a grating in front of the animal's eye, the cells in the minicolumn all respond to the same directional tuning of an oriented line or a grating moving in some direction.

That happens in both the upper and lower regions. I didn't show it in this picture, but there are also horizontal, long-distance connections in layers two and three, which go to other columns. There are also long-distance arrangements in layer 5B, which go to other columns. We might call this a voting connection today. That's not shown in this picture, but it was a striking observation. There are many parallels here, mostly around these minicolumns, which is unexpected. It's also not clear why you would have, and traditionally neuroscientists don't have a good idea why you would have, cells that are directionally tuned for movement. You might say the object is moving. We've now said the lower cells are representing movement of the sensor itself, but this hasn't really been explained by most neuroscientists. One of the big observations about the difference between these upper and lower layers is the size of the receptive fields of these cells.

If I put a sinusoidal grating across the retina of an animal—a very artificial stimulus—you'll see that the cells in the lower layers respond to this movement. In one minicolumn, maybe it's horizontal; in the next minicolumn, maybe oriented 20 degrees differently. They respond to very large patterns on the retina. In fact, they really like large patterns on the retina. If you put an oriented line and it's moving, making it longer is better. The upper minicolumns have smaller receptive fields. Some of these cells, many of them, are what they call end-stopped, meaning if the line stimulus moving across your retina gets above a certain size, the cells start being inhibited and don't respond. 

The big difference between the directionally tuned, motion-sensitive cells arranged in minicolumns in the upper and lower areas—perhaps the biggest difference—is the size of the receptive fields they respond to. Other than that, they're very similar in many ways. This observation was made many years ago, and people suggested that maybe something similar is going on in the upper and lower layers, but there haven't really been any theories about that, at least none that have stuck around. This idea has been around a long time, and it's been sort of unspoken: why would you see these minicolumns like this? Most neuroscientists just say, "Oh, this is what it is. It looks like this," as opposed to asking why it looks like this or what the theory behind it is.

Part of what I'm proposing today fits in with this very well. There's evidence that these two things are doing something similar. As I discussed recently, minicolumns fit a requirement that grid cells have. To make grid cells, you need a set of cells that respond to movement in a particular direction and change their frequency based on the speed of that movement. They are the precursors to grid cells, and you need a set of these cells all firing at the same time but at different phases. This is a requirement to create grid cells. That was a theoretical proposition, and the minicolumns look like that. We don't know if they're distributed in phase—that would be a strong prediction of the theory—but they look like what grid cells would need. We know that grid cells do exist in the cortex now. We theoretically derived that need, and now it's been seen in many parts of the cortex. This is no longer speculative; something is going on with grid cells and grid cell-like behaviors in the cortex. It fits our theories and the anatomy and physiology we see.

Those large receptive fields in the lower regions are going to capture the movement in vision—they capture the movement of the eye relative to the world. If your eye moves, all the bits on the retina will be changing, and the entire pattern will be moving. The entire pattern moving on the retina is a strong indication that the eye is moving and the world is not. The smaller receptive fields in the upper region won't respond to movements of the eye because only a small part of the retina is changing. That's what the small receptive fields are telling us: these cells respond not when the eye is moving, but when the thing I'm looking at is moving. The thing I'm attending to—the central area that this particular column is looking at—must be changing or moving, and therefore this is somehow detecting motion of the object or changes in the object.

This was the state of the art until fairly recently in how we've talked about it. Before I go on, are there any questions about this?

This is helping me understand a little bit more, but I don't have questions yet.

Okay, feel free to interject at any time if you want. Now, I'm proposing the following things. The bottom layers are representing the location of the sensor in the space of the object. That's not new. The upper layer is representing a state of the object. The state of the object is as I described before: it says what the object actually looks like in a particular state. The state will open and close on each point; what each point predicts in the location of the space will depend on the state of the statement.

Now a model is going to take advantage of all three of these things. The model will say, I have a location, I have some observed feature at that location, and I also have a state. The state means I need both the location and the state to make the correct prediction or to learn the feature. If I change the state, I could have a very different prediction—a different set of cells. We're just adding another variable; you need both location and state to make the correct prediction. When I'm learning the model, I will have some state variable and a location variable, and I'll know how to assign the feature to the layer four cells—specifically, which layer four cells will be active at that point in time. Sometimes it's a very simple addition to the model. There are complexities, but I won't go into them right now. The basic concept is pretty straightforward. If I have a different state, then this model—this middle object—could look different. The model can be different. There's no restriction on how the object may change under different states, but what they all do is the stapler object shares the same space of the object locations, the same grid cell, in the layer six grid cell module anchoring. That's just an extension of our current model. It allows us to learn predictions of how objects change at different times under different conditions, yet still think of it all as the same thing. An open stapler and a closed stapler are still the stapler. We have no question about that. That's determined by the anchoring of the grid cells. In some sense, the location space in the lower layers defines the object, and then the state defines how it currently appears in the world—what its current state is.

I just said that, so I don't have to say it again. You can read it.

Now, the way neuroscientists classically describe these cells in the minicolumns is that they're directionally tuned. They'll say, for example, this minicolumn—all the cells respond to movement horizontally to the right; another one, up at 20 degrees; another, up at 40 degrees, and so on. I assume all of you have seen these pictures. I didn't include them here, but they show that as you move across the cortex, each minicolumn changes orientation from the next. Essentially, the minicolumns represent a complete set of orientations of movement.

I think that's not actually correct. What I'm going to suggest is that directionally tuned isn't the right answer. I'm going to suggest something else: the minicolumns respond to some type of change. One type of change is movement, like a directional movement, but there are other changes that could occur—a color could change, or an orientation of a feature could change. It's not that these things are just recording movements. In fact, think about what we've talked about in the lower layers.

As our eyes move, we have to update the location on the object.

As I pointed out before, if you're watching someone play a video game, you don't just see the eye moving up or down; you also know if the player is moving forward, backward, or turning their head. In some sense, the information from the retina is encoded into a series of movement vectors, not just the classic ones of left, up, and down. There needs to be a movement vector for going into the screen, moving forward, or moving backward. That can't be explained by directionally tuned cells alone. If you think about what the bits on the retina look like, when moving forward, all the bits are changing except those in the chin area, and there's a radiating field coming out. This isn't captured by classic directional tuning, which is how minicolumns are often described. We're certain the visual cortex is getting that information from the retina because, as you watch a virtual player move forward, you know it's moving forward. That information is encoded in the image on the retina and changes your sense of location in the game. That information is coming in, and I argue the same thing is happening in the upper layers.

This is a subtle point. We tend to think of minicolumns as being directionally tuned, like a compass pointing in one of many directions. In reality, it's more subtle, and directional tuning is just a subset of all possible behaviors, such as moving forward or backward. In the cortex, there are many minicolumns, and they tend to report what they're looking for. There's evidence of many cell responses in the cortex that don't match the classic view of directionally tuned, ten-different-direction types. You can find this in the literature or by talking to neuroscientists. It's not out of the question that what is classically reported—the Hubel and Wiesel model—is just a subset of what's happening. This is important for understanding movement of the sensor in the lower layers and for understanding state space in the upper layer.

Supporting evidence comes from magnocellular inputs to the cortex. There are two basic types of inputs from the retina: parvocellular and magnocellular. Magnocellular cells have center-surround receptive fields (RFs). An axon from the retina doesn't represent movement; it represents a change in a center-surround receptive field. There has to be a line going through it or a dot in the middle, and it has to change. Magnocellular cells only respond when there's a change; they don't respond to static input. Parvocellular cells respond to static input, but magnocellular cells signal that something has changed. All these bits coming into the cortex from magnocellular cells represent different changes; they're not directionally sensitive. There's nothing about them that says the retina is moving in a particular direction. They just indicate that a ganglion cell from the retina has detected a change in its center-surround receptive field. No concept of movement has appeared yet—it's just change.

In primates, the axons coming off the retina as they exit the thalamus and go to the cortex are still center-surround. But immediately upon reaching the cortex, you see directionally sensitive responses. The cortex extracts directional sensitivity from these center-surround receptive fields; it wasn't present before. I argue that the cortex has to learn these things. We've discussed that the cortex must learn what movements are exhibited. It receives these center-surround RFs and processes them, perhaps through something like the spatial pooler, to identify the top components representing the changes it's observing. It might represent changes corresponding to looking left, right, up, down, forward, backward, and so on. These are observed, not hard-coded. The cortex learns from these changing bits what movements correspond to them. We'll call them movements, though they don't have to be physical. It's just a set of changing bits processed through the spatial pool, resulting in minicolumns that represent abstractions of all the changes observed. This becomes a winner-take-all concept. These are no longer directionally tuned but change-sensitive. Minicolumns represent common changes observed in the inputs; some correspond to movements, but not all. They're just common changes in the input. It's a subtle but essential point for understanding behavioral spaces.

I've said a lot, and some people may be confused, so I'll clarify. The magnocellular and parvocellular pathways project into the LGN; as far as I know, they don't go directly to the neocortex.

They've measured and determined the receptive fields of the relay cells in the thalamus, and they have not changed—they are still center-surround. This is why they call them relay cells. Observing the thalamus, it just looks like you have a center-surround receptive field coming in and a center-surround receptive field coming out. We now know it's remapping those to create a change in orientation, but the inputs to the cortex themselves are still center-surround, with no movement in those fields.

Now, since you asked this question, Kevin, it's confusing because in rodents, they actually see the outputs coming off the back of the retina and out of the thalamus having directional sensitivity. In rodents, there isn't this generic center-surround receptor; it's tuned for movement. In primates, it's not. I view this, in evolutionary terms, as: in the beginning, all of evolution was about whether the animal is moving and how it's moving. But now, with sophisticated movements and especially when we get to state spaces, movement isn't the right term anymore—change is the right term. That's why in primates, including humans, these bits don't represent movement per se. That's a limiting assumption, and we don't want to assume that. We want to assume the cortex can learn any pattern.

I tend to focus on numbers rather than the number of cells, because I want to make sure everything is right. I don't want to call it the number that's in there. What I want to do is move that into the data I'm looking for—I'm looking for these sequence numbers.

There are temporal shifts in the spike, or the receptive field gets a little larger or smaller. There are many papers trying to figure out what the thalamus is doing, teasing out very subtle changes without really changing the basic idea. They're looking for what the thalamus is doing and searching for really small things they can detect. In reality, they missed the big thing: it's rerouting those signals. It's a multiplexer type of function, and at least that's what we believe. I'm pretty sure that's correct.

They missed the big thing because, under any particular context, they'll say it's a spike in, a spike out—what's the difference? The difference is that under a different orientation concept, it would totally remap the output from the input, and their research paradigms just don't detect that. They don't know how to detect it because they weren't looking for it.

All right, so how do we understand what's going on in these two things here? In the lower layers, we've already discussed this quite a bit. We all know this is part of Thousand Brains Theory. What these grid cells in the lower layers are doing is detecting movement of the sensor patch, and through path integration, updating the representation of the location of the sensor patch relative to the object. There's nothing new there.

What's going on in the upper layers? I have to admit, I'm still foggy about a lot of this, so I don't have the answer, but this is where my current thinking is. I said, what is the equivalent here? The equivalent is that the feature is changing at this location. There are cells in the retina that have detected a change. I'm not going to say it's a movement, just a change, and that change is going to be a movement in the space of states, behavioral states. Maybe this part of the object changed color, or maybe the orientation switched, or maybe a new feature appeared. We don't really know; it could be any of those things. The feature is changing at whatever location the column is looking at, and that change will affect path integration. In the space of object states, it's going to follow the same principles. I've got a bunch of vectors representing change, and they may not correspond to movement in the world—they're going to represent some kind of change in the object. If I set up the mechanism to represent the most common changes I observe, which would be in the minicolumns, then I will be moving through the space of states of the object. I can say those words and still not be able to visualize it, but the mechanisms would lead to this. It's still very confusing.

That part seems challenging for me. In integrating location, there's a fixed number of dimensions. In integrating states, it seems like there's an infinite number of dimensions possible, because the states could be anything. Maybe. First of all, it's not the number of states; it's the way states can change that we're enumerating. For each state, maybe it's helpful to be more concrete about what a state is. You said a state could be color or moving forward. A state is a point in the behavioral state space. It's an SDR, just like a location is an SDR in the location space. If you think about the location space in the lower layers, there's an infinite number of locations—no limit. You just keep moving around. But a finite number of dimensions; you're only doing it in three dimensions.

Now, actually, there's an infinite number of ways you could move. We can't think of it like Cartesian X, Y, and Z. I could say there's a movement that goes at one degree north, another at one degree off north, two degrees off north, and so on. There are movements that are spiral, or movements that represent doing somersaults. What we want to do is take all these changing bits, run them through the spatial pooler, and end up with a set of movement vectors. That's going to be your representation. You have to fit everything into that set of movement vectors. Those movement vectors are going to be the most common things that are observed. If you have some other movements, like someone doing a somersault and what's happening on your retina while you're doing a somersault, it's not going to be very well represented in those minicolumns. It's just not a very common movement, but almost all common movements will be represented in some way or another.

Let's say in a particular minicolumn, I might have 100 to 200. A typical column might have 100 to 200 minicolumns. That puts an upper limit on how many different movement directions or vectors I could have. But I can represent a lot of different types of movements in that. It's a pretty big dimensional space. When it comes to behavioral states, there's an infinite number of potential behavioral states, but the number of ways you can move through the behavioral state will still be limited to a hundred minicolumns. So there are a hundred different ways an object might change at any particular location, and that's going to have to be sufficient. That will cover the vast majority of almost anything we can imagine. At any point, all you have to have is a feature, an orientation, a scale, a color perhaps, and those can only change so many different ways. I don't think there's a representational space problem here. I think it's fine, at least I'm arguing it is.

I guess the other thing, related to that, is whether in theory there's an infinite number of behavioral states or feature change directions you could go through. But there will probably be some representational capacity in terms of how complex an object you could learn. Practically, a column or learning module might just learn a couple of dimensions of changes, and if that's insufficient, then you may need to start recruiting new learning, almost as a compositional object if it's that complicated.

But let's review again. There's an infinite number of locations. In the lower layers, there's an infinite number of potential locations, but a limited number of movement vectors. That limited number of movement vectors is in the 100 to 200 range, not 2 or 3. We don't know, but there are maybe up to 100 per cortical column. That's your base. That's a lot. There are a lot of movements that can be represented like that, and many of them are very similar. The same thing is true in the state space.The number of points in the state space is unlimited, but the number of ways you can move through the state space is limited to, at most, 100 to 200—probably much less than that due to redundancy. It won't be just two or three. The column is essentially identifying the patterns it has seen, focusing on those center-surround bits that are restricted to a part of the retina, a small part of the object. As those bits change, running them through a spatial pooler yields a certain number of representations. These must account for all possible changes that can be observed, but this isn't very limiting—just as the possible movements in space aren't very limiting either. Maybe you've said this already, but in the lower layer, we're integrating movement to get location. In the upper layer, we're integrating something else to get something else.

Indicating changes in the state could be changes in color. They're just coming off the retina; they're not states. Coming off the retina, they're just bits that are changing. It signals that something at this location is changing. The fact that those center-surround bits are changing, the fact that those ganglion cells are firing, tells you that something is changing. If the pattern on that part of the retina is static, the magnocellular cells don't fire. They only fire when something changes. So, it signals that something has changed here. Through learning, this is turned into what we might call movement factors, but they're not physical movements. They're a way of representing the different ways an object can change. That's what they represent—the different ways an object can change.

Then, these are fed into a grid cell module, which essentially says, "I can path integrate in this space of states based on my movement," which is really just change—the way the object can be changing. I will path integrate and get to a new state. What comes to mind is that we're categorizing the changes into common changes, or most frequently occurring sets of changes. They're not changes to a particular object; they're changes that can occur to any object at this point.

Just like when we path integrate locations in the lower layers, we're just saying, "Here's the next location." It doesn't tell me what's at that location; that requires layer 4. Here, I will be path integrating in the upper layers, and all I'll be able to say is, "Here's a new location that I've gotten to because of the way path integration works." I'm just going to say, "Here's an SDR." The critical thing is that path integration works in the upper layers too. Once I've defined a set of possible changes that can occur on the object, I've got this basis set of "movement vectors," but they're really just the different vectors of how objects can change. I should be able to path integrate. I don't even know what this means, but it will work because grid cells path integrate. We're going to have this property, and the important thing is we want the inverse property: if I'm given a desired state, how should I move to get to that state? How should I move in state space first, and then translate to physical movement later? What I'm doing here is following the consequences of the assumption that there are grid cell modules in the upper layers, and the consequences of what those grid cell modules are fed as input. I'm describing what those consequences would be before I even understand them intuitively. This is what would happen, and there's evidence that we need a way of understanding movement through state space because that's what behaviors are about. We're trying to manipulate objects to get them into certain states. But I need to have a model of states to do that. I need a way of understanding how objects behave. 

For example, in these upper layers, I might model a sequence of states, like opening and closing the stapler, but I can apply that same model to other objects. The upper layers are modeling state spaces, not a particular object. The lower layers are, so I could learn certain behaviors in the upper layers and apply them to new objects in the lower layers. I could say, "I'm trying to get this particular object into a particular state. How would I go about doing that?" because I know something about how states transition. I realize this is already getting fuzzy, but yes.

I really like this framing of needing to be able to path integrate through state space as well. When we have the state for opening and closing, we know what path it needs to follow. If we have something with more degrees of freedom, like a joystick, there are multiple ways I can move to get to a certain state through state space. I like this way of framing it—that maybe the state spaces are also using grid cell mechanisms to get this path integration property. One thing I'm still having a hard time with is making the connection between how the state we have in the upper layers then influences the location space in the lower layers. If I have the closed stapler, the object will exist at different locations than if I have an open stapler. I would expect features to be at different locations in that case. That goes back to this idea and the green arrows here.

If I got rid of the upper green arrow and just kept the one coming from feature and the one coming from layer six A, that's our current model. We associate a feature with a location, but now I'm going to say we have this other state and the location in the state. If I change the state, I may pick a different set of grid cells, a different set of layer 4 cells.

It's like I can have a completely different prediction, a completely different representation in layer 4 if the state changes. You can literally make predictions about the stapler, whether it's open, closed, or even if it turns into jello. When you push the button, you can do anything, as long as you say, "The state has changed. What are the features now, at what location?" For the stapler location space in layer six, we would have associations to features at all the possible locations the stapler could be in, whether it's open or closed. But then, depending on the connections from layer three to layer four, a lot of these will not be active. 

Imagine a layer 4 cell with dendrites and thousands of synapses on its dendrites. What do we know about those synapses? One surprising thing is that the bits from the retina, if we go to V1, represent less than 10 of the synapses on that layer 4 cell—typical numbers are like 7, some people say 5.

The number of synapses coming from layer six is somewhere between 40 and 45, much more than from the actual retina itself. This makes sense because layer six is passing up an SDR, whereas the retina is not, but that leaves about another 50 synapses on layer four cells coming from someplace else.

Where do they come from? I don't remember the answer to that question. I don't know if we know the answer. But they're not coming from the feature, the retina, or LH6A. We also know that when these synapses form on the dendrite, they're not mixed together. This is standard throughout the brain, at least in the cortex. If I imagine a dendritic branch extending a millimeter or so, the bits from the retina are clustered together in one section, the bits from layer six are clustered in another section, and the bits from elsewhere are clustered in another section. They actually know what those sections are—I don't remember, but I was once surprised to realize that the bits from the retina are not proximal to the cell. I used to think they'd be proximal, but they're further out on the dendrite.

There's another theory, with a lot of evidence throughout neurons in the cortex, that the order in which those three zones become active matters. The evidence is that if you activate the furthest region of the dendrite first, then the mid-region, and then the closest region, the cell is really going to get excited or depolarized. If you do it in the other order, it doesn't work as well. These are not randomly assigned; there's an assumption that neurons make about what should be seen first, then next, and then next, and now a prediction can be made. For example, if the state variable from layer three was at one point further out on the dendrite and that didn't occur, that cell is never going to go. It's just saying, until I get the right state, I'm not even going to consider being active or depolarized.

I'm pointing this out because the state and location have to be together. They have to be independently recognized on the same cell, on the same dendrite, along with the feature, for this cell to say, okay, we're good. If you change one of those, that cell won't get depolarized. It'll say, not my turn, someone else is going to do this.

I was wondering, with this influence of the state on the location, whether, depending on the kind of behavior or state that the learning module is modeling, something like grid cell distortion would also be useful. For example, with a stapler, it's simple to just learn to predict in a different state that there's going to be a different feature at the same location, using the exact same reference frame, undistorted. But with something like a stapler that can be continuously opened, it could be inefficient to have to learn a bunch of independent reference frames, whereas grid cells can be distorted, at least in the entorhinal cortex, by certain factors. So I'm wondering if you warp the reference frame instead, you can still path integrate and predict the same feature, but based on a different movement.

I'm confused, Niels. I don't see why I need to do that. It sounds like you're saying we don't want to learn every point of an object, and we can't do that. We've talked about this—the neurons have to do something. If you're worrying about how many points you have to learn, or if you have to learn all the stages of the stapler, it's like you don't have to learn every point on the surface. It's partly that, but also the commonality of the locations. When the stapler is open, I understand that the top of the stapler is still the top. If the reference frame is just distorted so that it's a different movement to get to the top, rather than a totally different location, I'd rather not go there. The distortion of grid cells is controversial, it doesn't occur everywhere, and I don't think it would be useful. I don't understand it, but I don't want to go there yet. I don't think we need to.

It's just my strong intuition that that's a red herring. We don't need to go there yet. There are legitimate questions about how the state space and the location space interact, and I don't have the answers yet, but I'm confident we can figure them out. I need to review the anatomy and what we actually know about how these things connect; there's a lot of information, and I need to think about it more. I don't see a need to introduce that complexity yet, Niels. It's simpler to think about grid cells and spaces and locations before considering distortions. I don't know if it's been shown that distortions in entorhinal cortex grid cells perform an important computational role. I'm not aware of that. I think it's an observed behavior that isn't understood, but from a theoretical point of view, I don't see a need for it. It may just be an artifact that has to be dealt with, but I don't want to go there yet. There's complexity we don't want to introduce if we don't have to.

I wanted to confirm something I thought I heard you say about the various patches on the dendritic tree where things are segregated, synapsing according to function. You said there was a sequentiality, that one particular patch had to fire before the next one, before it would depolarize the entire neuron. Could you explain that sequential mechanism? Is that really what's happening? Here's what's been observed: if you take a neuron and activate its synapses, like with photoactivation, and you activate going one way along the dendrite sequence versus the other, you get very different responses at the cell body. The researchers doing these experiments aren't thinking in the same way we are; they're not saying, "I have to have 15 active synapses to generate an NMDA spike in the dendrites." They're just saying, "If I scan from one direction and then the other, I get different responses." There's speculation on the mechanism, and I can't remember if I'm sharing my speculation or others'. Dendrites get narrower as they reach the end, and they have unusual properties at junctions with other dendrites. You can imagine that as a dendrite gets smaller, it's easier to activate because there's less leakage and less capacitance, so it's easier to activate a thin dendrite than a thick one. If you activate the thick end first, maybe nothing happens, but if you activate the thin end, it depolarizes the thick end and makes it more receptive to the next input. In some sense, there might be different thresholds at the two ends, and by activating along the way, you're priming the thickest end to generate a spike. If it wasn't depolarized at that point, it might not. That's one theory of how that could happen.

There are other possibilities. There are different types of receptors, and the ionic receptors vary in density at different points along the dendrite. There are many ways this could happen. The reason I was drawn to it is that it provides a very compact mechanism for sequence memory, where things have to be actuated in a particular sequence for things to work. Whatever the manifestation, the mechanism is important. To me, it's a really important principle.

You're not the first person to suggest that. Personally, I think it's wrong. It's a simplistic way to think about sequences. The way we do sequences in the temporal memory algorithm is far more sophisticated and powerful. This is a very crude way of doing sequence memory. It would require every dendrite to learn a sequence, which is very limited. With the temporal memory algorithm, you can learn almost an infinite number of sequences, making it a much more powerful mechanism.

I think the right interpretation is that if there is a preference for moving along the dendrite, it's to ensure the cell is at the right location and state to make the correct input prediction. It's not just about being at the right state or location; you want an "and" of those two variables. Our current dendrite model is too simplistic, so it doesn't matter if I get depolarized from the state or location—it's good enough. We need to make sure both happen at the same time, so there has to be an additional mechanism in the dendrite.

The way you described it, everything in your SDR is operating at the same level. Here, different types of stimuli are synapsing at different points along the dendrite. It's almost like a nested "if this, then that." It's not like following a set of SDRs from the same source; it's more like needing an SDR from one source, another from a different source, and another from yet another source to get this to work. That gives you a mechanism to combine these various qualitatively different signals—location, space, state, or whatever. When combining things that are disparate but necessary for activation, this mechanism ensures all those things are in place before you get that particular stimulus output. From there, you can go into your SDR mechanism and say, "Now we're on a level playing field," combining things that are like to like. This is a low-level mechanism.

This came up because someone—maybe it was Viviane, I can't remember—asked a question about how a different set of cells in layer 4 would be active at the same location under different states. We got down this path by talking about mechanisms that would allow that.

This is a detailed dive into one part of the presentation. I don't want to spend too much time on it, but it definitely will happen. I have another short follow-up question on my question, so I understand how this works. For one object, we have the state from layer three, which picks which features should be expected to come into layer four. But how does it work to use the same kind of state space or behavior and apply it to different objects that use completely different location spaces? I have no idea yet. Can I also go through an example to solidify my understanding and just point out if I'm wrong? I want to make sure I'm understanding. Let's say, with our features at a location with Monty, we look at a patch. I'm going to draw—hopefully you can see it. Here we go. Look at a patch, and it's yellow. Let's say I moved enough to know that this location or this object is a banana. It's automatically making it into—okay, got it. There we go. It's a banana. Let's say we're just on a patch for now: yellow. Over time, so there's a change in time, delta t, and then this patch turns brown. We haven't changed our location; it's just that there is a change here, not movement, but in color, from yellow to brown. Now we know the state is ripe banana, not just banana, but a ripe banana. That's a little confusing because bananas don't change instantly from yellow to brown.

You could argue that's a behavior of a banana, but it doesn't fit the way I'm thinking about behaviors right now. You could have a sunset where the color is changing relatively quickly. Or take something simple: you're looking at a stoplight. It changes to red, green, yellow. That's the kind of thing we're talking about here, right? Those are changes to the object. If a column was looking at one light bulb in the stoplight and it changed from green to dark, I'd say that's changed suddenly. That's a change of that light. But the same mechanism should apply to the banana or sunset example as well. It's just that we don't see anything between the states. I don't know if it does apply; we're really bad at noticing changes that occur slowly. There are examples of things that completely change, and if they change slowly enough, we don't even notice it. I don't mean we see the movement between states; it's more like we can recognize a ripe banana versus a non-ripe banana and we know it's the same object. In this case, I would say those are two states of the banana, but there's no way I can move between them. There are states that change—objects change states that we can't interact with or make happen.

They just exist in a couple of different states and that's it. In this case, the state cannot be reversed; we can't go from ripe to unripe. Maybe a criterion here is that it's not just about speed, but whether we can change states between them. I think it's possible to have different states of an object that you can't transition between. Take our mug—the coffee mug with and without the logo. One of the problems we have right now is that to learn the mug with the logo, we have to learn an entirely new object. There's no way to preserve all the mugness and just add the logo to it; we have to learn it as a new object. I could argue that the mug with the logo is the same mug; it just has a different state. In one state, it has the logo, and in the other, it doesn't. I can't transition between those states, but I see it as the same base object. It's the mug. I know how to work with it; all the behaviors I know about the mug apply. If it's in the logo state, then I know I should predict the logo at some location. If it's not in the logo state, then I don't predict the logo at that location. Everything else is the same. I don't re-anchor the grid cells. I don't learn new features. The state allows us to represent the two versions of the mug very quickly and simply, and there's no way of transitioning between the two.

Here's an interesting counterexample. Viviane recently sent me a coffee mug that had the Thousand Brains Project logo on it, but it was a mug that was black until you put hot liquid in it. Then it becomes white and you see the logo. She sent me this as an example of a behavior that a mug might have. It fits into this definition, right? It's like the mug with and without the logo, but this is one I can control. I can put hot liquid in the mug and make the mug change color and make the logo appear. There are no hard and fast rules here. Objects can change. It's still the same object. Sometimes they transition between states, and sometimes they don't. This gives me an example: instead of saying I have two classes of mug, I can say this mug has different states. I can have the same mug with 20 different logos, and once I know which state I'm in, I know what to predict. There is no transition through the states based on any behavioral action that might have occurred. It just changed; it's a different state. It would still be right.

Another example of a state change that could happen to a mug is if you had a spray paint can and a decal cutout; you could add a logo if you wanted. That's a complicated case—where would that be modeled? Am I creating something new? There are many different state-based points you could be in. In some sense, as long as you recognize this is a new state space, you need to learn what is associated with it. Now there's a different logo or image.

Another example is having a bunch of mugs, and one of them has a chip in the edge. I see it on the shelf and don't see the chip, but as soon as I feel the chip, I recognize it. Now I can predict the chip, but I don't have to learn a new logo or a new mug. Everything else is the same; I operate it the same way, but whenever I go to that location, I know there's a chip there. The brain does this. We don't want to learn the mug every time; we need some variation. We need a way to say this is the same object, but now I can identify it uniquely because it has some difference from the others. Sometimes those changes occur gradually, and sometimes they don't occur at all—they're just different states of the same base object, even if there's no observable transition. They would still be represented at two different locations in the state space above.

I had some thoughts on that, but I won't go into it. I was thinking about filling the mug, which might relate to where you were going, Hojae. As you add liquid to the mug, there's a constraint: it only accumulates in the mug. As you pour in more, the liquid level only gets higher; it never goes lower until you start pouring it out. There is a sequence of filling constraints that could be modeled as states of the mug.

That's an interesting example, because pouring liquid into something is a common action. If I need to pour liquid and don't see anything available except a shoe, I might pick up the shoe and put liquid in it. The idea of pouring liquid into something is represented in the upper layers. It's a behavior I want to do that's independent of the object itself—adding liquid is something I've learned, and I can try to apply it to different objects. I can look at different objects and ask, can I do that with this object? I don't know how this is done yet, but I think this framework allows us to do that. The goal is to store a liquid, and if you're adding liquid to a strainer, it won't accumulate. I can look for different objects, even ones I've never put liquid in before, and ask if I can do it. I don't know how this is done yet, but this framework will allow it. If the goal of the column is to store liquid, it can check if that's possible with the current object. If not, try another object. The goal stays the same until it finds an object that works, and then it executes. I don't know how that's happening yet, but it feels like it must be, because we can solve those problems. Having these two different spaces gives me the framework to ask exactly how that's done.

The object is, in some sense, independent of the behavioral states that are learned. I can associate behavioral states with a particular object, but I can also associate them with novel objects I haven't seen before.

I'm not sure how many more slides I have. Can I ask you to erase your blue there? Thank you.

I talked before about different types of changes defining spaces similar to movement vectors. The types of changes you observe become the movement vectors of the state space, but they're not really movements—just movements through the state space, not physical movements.

I was asking myself: in the lower layers, anchoring the grid cell modules defines a unique space, and we associate that with the object. We're on object A or object B. What would anchoring be equivalent to in the upper layers?

I don't know the answer. It's really hard to think about.

I can speculate. Anchoring up might be like anchoring on a set of behaviors applicable to certain objects. Maybe now I'm in a space of things I can do with liquids. If there are grid cell-like modules in the upper layers, they should exhibit the same properties we see elsewhere: anchoring to represent a particular subset of all possible things and also path integration. What does that mean? We can ask these questions about a space that isn't physical space, but state space. I'm just starting to get through this—these ideas are only a couple of days old. I worked on them over the weekend. There's a lot of movement and change, and it should be easy to make progress on this in the coming weeks and months. Let me see if that was it. That was the last slide I had for today.

That’s very interesting. I’d be interested to talk more about how behaviors might help us categorize objects, because I thought that was an interesting idea. I know you said there were some problems with it, but I think it could be about classes of objects, solving something. Let me tell you the classes of object problem. I literally thought about a cup with liquid and wondered, what if I have a bunch of different mugs with different shapes and sizes—some fancy, some straight. Could we anchor the lower layer grid cells as a class that represents all mugs, things of a certain type, even though they’re physically very different and don’t transition between each other? Then the state would say, I’m on a mug that looks like a little dainty cup, or I’m on a big one made of wood. I could do that, which is pretty cool. But I also might have a state like it has liquid in it or it doesn’t, and that’s independent. I can’t use the state to say this is the wood mug and this is the little dainty mug, and also use the state to represent whether it’s full of liquid or not, because I can’t do both. I need a third variable. Okay, with a union of states?

What does the state represent? I just couldn’t get it to work. Maybe you can get it to work. Maybe not necessarily that the state itself is the class ID, but more that the fact you can apply the same type of behaviors to a group of objects means those objects are similar in some way and therefore might be classified as the same type. I think that’s the right answer. Exactly. That is probably the right answer. There’s a class of objects that can hold liquids you might drink from. Then I could say, does this new thing fit in that class? It’s interesting—often an object can have two roles at once. It can look like a mug, or it could look like something else. A chair is a classic example. I can show you a big shoe, and you say, that’s a big shoe, but it’s also a chair. So what’s happening is you’re applying a behavioral model to an object and asking, would it work as that? Then I can say, I guess that’s in the class of chairs. I think that’s the right way to think about it. Maybe a class of objects is what you get when you anchor the grid cells in the upper layers—it’s like a functional definition of the thing. The class is a functional definition. It’s not like each object is just in one class; you can categorize objects by different aspects. I can associate the class of shoes, or the class of chairs, or associate it with a class of pop art, or something like that.

It’s the same physical model of the chair, but I can group it into different classes of behaviors or affordances. Those two are definitely fundamental to how we think of things. There’s a lot of philosophy around those distinctions. Philosophers have been thinking about them for a while. I’d be curious to know what they thought. I don’t usually find much useful stuff out of philosophers, but maybe. I like it. Maybe this answers the question: what would anchoring in the upper layers be? Maybe anchoring in the upper layers is a behavioral class—things that share behaviors.

That sounds good, just saying the words. I don’t know if it works well or is meaningful, but it sounds good. At least that’s what your write-up initially made me think, and I really like that idea. But that’s not what I said. It’s not the state that says, it’s not like the anchoring of the grid cells defines a class of objects. The anchoring of the grid cells defines a specific object. There may be different behavioral states, but the state SDR does not define the class, but maybe the state anchoring defines the class. That sounds good. I like that. The state behavior—do you want my take on this? Maybe this is me not understanding the minicolumns idea, but if we have the class of objects that hold liquids, do we want to restrict that to a minicolumn that only represents a certain set of objects? Would it not be more useful to have that more generic? Remember, the minicolumns don’t represent a set of objects. The minicolumn represents a common change in an object and therefore becomes one of the basis vectors for path integration in state space. But can I apply it to any object? Anything that observes that change would invoke the same minicolumn. Where I’m getting stuck is, in the case of liquid-holding objects, is the set of liquid-holding objects too big to confine to a single small structure in the brain? It wouldn’t be a minicolumn capacity. The minicolumn wouldn’t define that.

Maybe to answer, also Michael, it would be lots of different columns that would learn this behavior. It wouldn’t be just one column that has that behavior represented. No, the minicolumns don’t represent, they have no direct correlation to any particular object. They just represent a type of change that has been observed in lots of different objects. Also, any given column might learn some objects, but there will be many other columns that will also learn the same objects. The same would apply to behaviors. In fact, we might vote—one of the questions is, do we vote on behavioral spaces? We want to vote on the object space, so this is an interesting thought.

In this current scheme I proposed, instead of voting on objects in layer three, we would vote—maybe the voting on the object would be the layer 5b projection, because it's voting on what is the base space here, which is the stapler. It doesn't matter if it's open or closed; we're looking at the stapler. The voting in layer 3 would be voting on the state. What state are we in? Are we all agreeing this is the same state? You're trying to figure out what object it is; I'm trying to figure out what state it is. These two are obviously going to interact with each other. It changes what we think about the upper layers doing. It also raises the question: what does the layer 3 output to layer 4 going up the hierarchy represent?

It may not be clear; it's confusing. If I were looking at a compositional object, the feature I'd want to pass to the next higher region would include the state of the object. For example, if I'm arranging stuff on a desk, I would want to say, "There's an open stapler or a closed stapler at this location." That would be the model of the desk if it's always open or always closed. It's not just that it's a stapler. It does beg the question: how would we interpret these upper layers? They might be different than we thought.

I'm just throwing that out. These are all brainstorming ideas.

What I'm going to do next is review what we know about the connections between layer five, layer three, and layer two.

There's a lot of literature on it, but it's confusing and inconsistent. I've forgotten a lot of it, but it might give us clues as to how these things interact. How does the state space interact with the location space? Another big question is, if I want to make a change in state space, how does that translate into movement? The movement will always be in object space, right? It's really confusing because, in the tasks we humans do, we often use different modalities. If I'm trying to put water in a mug, I determine the state of the water visually, but I pour the water with my hand holding a carafe. I'm judging the state change visually, but I'm affecting the state change with somatosensory outputs. I find that hard to think about, but we have to figure that out too.

Anyone else want to bring up anything else?

This was great. We're already a bit out of time, but I'm definitely up for talking more about this next week if you make any more progress. I'll try. I've got a lot happening in my life right now, but I'll be motivated to work on this. Anyone who wants to chat or send questions back and forth, let me know. I'm happy to have a conversation about it. I'm definitely going to continue to work on it, because I feel like this is—I've been through this process many times—this feels right to me, even though we don't understand a lot of it. The basic idea seems quite promising. It's not the first time we've talked about these ideas, but it's the first time I'm putting some detail on them.

I'm going to keep working on it.

Maybe just a follow-up thought. It would be interesting to see what connections, if any, because I feel like often when we've discussed behavior in the past, at least at the cellular level, the focus has been on sequence learning. You learn the state where there's a sequence of states. That would happen naturally, right? Imagine I'm taking these bits from the retina that are low, they're sending them to the upper layers, so it's limited, and—oh, I just lost my train of thought. What was your question again? It was about the standard sequence memory being unidirectional. But here, we're doing something that path integrates.

As in, back and forth through the state spaces in different directions depending on the action. In the temporal memory, it's very easy to learn sequences if they occur. I'd have to be more precise, but the general idea is if something repeats consistently, the neurons will naturally learn it as a sequence. It's a series of states.

If it occurs over and over again, it will just say, "Hey, that's the sequence," or even if it's just a couple of times, "That's the sequence, I'm going to learn it."

It can even learn it very quickly in one shot, if you wanted it to.

I think learning sequences is a natural outcome of the way the neural models we've developed work. The first time you do something, you have to think about it, observe it, and watch what's happening, but if you practice, it becomes second nature and you just follow the sequence precisely.

I started to think about the stapler as a generic version of a hinge, or even more generically, a constraint under some kind of rotation. That is maybe the behavior you learn, and then you can apply it to many objects. I used to think about it like a hinge. If I think something might have a hinge, I look for the pin that affects the hinge. If I was looking at an object and didn't know what it was or how it worked, I might try it, but I would also look for physical evidence. Or the part lines that it's going to segment on. I think you're right. There's a behavior of hinging that we've learned, and we can apply it to many things. My laptop hinges, and my paper hinges.

And then once you make that assumption, you have expectations about how it will behave. You don't necessarily have to learn every point in the space of a stapler being open or closed, because the constraint lets you predict the novel states. There's an ongoing issue we always have to deal with: the system has to interpolate between points that are learned. We've talked about that a number of times, so you don't have to learn every point in the trajectory of the stapler lid. The way I think the neurons work, it would be as if you learned every point—there won't be any difference. The way the SDRs work, it would essentially cover every point along the way, so you can learn four points, A, B, C, D, but any other point in there would act as if you learned it as well.

Neurons can do that. I can go over that again if you want, but I've talked about it a number of times. I've talked to Niels and Viviane about the fact that in Monty we're using discrete numbers for representing locations, and that always leaves a problem of interpolation between points. We've come up with mechanisms to deal with that in Monty, but they're very different from the mechanisms that neurons use, and I've always worried about that.

In our implementation, we might say, what's the nearest point that we did learn, or the nearest two points, something like that. Whereas the neurons don't do that. The neurons can't tell what points they learn; they just form a continuous representation along a trajectory of SDRs. What we discussed last time was that, basically, how we implemented it has the exact same behavior—it can do the same thing—but it does it probably less efficiently. It does it less efficiently.

It's dependent on the hardware implementation. Will we ever have to move away from that? I don't know, but let's not worry about that now.