The official title of one of the two papers we are publishing right now is "Thousand Brain Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference." As Viviane said, we often refer to it as the demonstrating Monty's capabilities paper, or DMC for short.

This paper is the culmination of about three years of work, going back even further to when Jeff and researchers at Numenta laid out the principles of the Thousand Brains Theory, which Jeff wrote about in his book, "A Thousand Brains." These ideas were then developed at Numenta and later in the Thousand Brains Project to create a system that implemented these concepts. That became TPP Monty, the first code implementation of a Thousand Brains System—a system that implements the concepts and algorithms described in the Thousand Brains Theory.

We had that system and ran internal benchmarks, but we wanted to compile an overview of its capabilities into one paper. Of course, we're constantly working on new features, so this paper represents what it's capable of today. That's what became the DMC paper.

It's been a huge effort from everyone on the team, with particular thanks to Scott and Hojae for their work. The main focus of the paper is on robust inference, rapid inference, and rapid, continual, efficient learning. We'll break these down shortly.

Today's presentation will focus on the results in the paper, especially giving an intuition around the experiments, the metrics we used, the results, and their implications. I'm going to assume you have a working understanding of Monty. If you're new to Monty or the Thousand Brains Project, I recommend starting with resources like the December symposium from last year, our documentation on ReadMe.io, and the methods section in this paper for a detailed description.

Briefly, Monty is an architecture defined by a series of semi-independent learning modules that work together, receiving information from sensorimotor modules and outputting goals via a motor system. These semi-independent units communicate through a common protocol called the cortical messaging protocol, defined by features that have a pose in space in a shared coordinate system, as well as other features like color and object ID. This gives an overview of what the Monty System looks like when perceiving something like a cup. The input Monty gets is from a very small region of space—the receptive field of a learning module is defined by the sensorimotor module it receives input from, typically a small patch relative to the size of the total object. The sensorimotor module outputs pose and features, which is the CMP signal sent to learning modules. The learning module receives pose and features, which are then learned in reference frames. As the system moves, it builds up a representation of where these poses and features are sensed. These same reference frames are used during inference to develop hypotheses about which object, and where on the object, the system might be. It can also gather information to hypothesize about the most likely identity of the objects it's perceiving and generate goal states to enable intelligent action. Essentially, it determines the desired state of the motor system to achieve a goal.

This shows how the learning module changes over time as it receives new sensory input and outputs goal states. That's a quick overview—please check out the other resources if this is unfamiliar. This paper starts with these principles: sensorimotor modules with reference frames that perform learning, and examines the broad range of capabilities this provides.

Before discussing our first results with inference, it's important to emphasize that Monty, as a sensorimotor system, is all about movement. Whether learning or performing inference, it's constantly moving. The animation here shows Monty as a finger-like system, tracing its movement along the handle of an object and then jumping to another part. During learning, this might involve a policy that densely samples an object if it's familiar. With a camera-like agent, for example, we might scan an object like a mug, and Monty would develop part of a model of this object. If the object is observed again from a different angle, Monty uses those observations to update its memory. During inference, it also moves, but generally in a more unstructured way, as in this paper, because the goal is rapid recognition rather than developing a new representation. You can see the path as the system moves over the object's surface, then jumps to the handle and the rim of the mug.

One of the first figures showing actual results in the paper is meant to give you an intuition of what imprints in Monty actually look like. This shows the internal model Monty has of a mug, as well as a representation of where in space its sensorimotor module is at that point in time. What you see is an extremely ambiguous sensation at that moment, equivalent to touching the mug with your finger or looking at it through a straw. On the right are the evidence values associated with different objects Monty knows about. At the top, the first step initializes some hypotheses about what object it might be on based on what it's just seen. Given the red color and the curvature of the surface, this matches quite well with the side of a mug, to a certain degree with the side of a bowl, and not particularly well with the side of a white golf ball. Over a series of movements, Monty integrates the movement taking place to update these hypotheses and receives new information. Over the course of an episode, it narrows down its hypotheses about where it might be. You can see how it gradually eliminates the bowl and quickly eliminates the golf ball as possible locations. In the case of the mug, it narrows down in space where it might be based on what it's observing.

The dataset used for all experiments was the 77 objects in the YCB datasets, a collection of household objects such as a fork, banana, screwdriver, and so forth, that have been 3D scanned and made available. During learning, Monty is presented, unless noted otherwise, with these objects at 14 canonical rotations: the six rotations corresponding to the faces of a cube and the eight rotations corresponding to the corners of a cube, for a total of 14. There is a distant agent that uses a more scan-like policy, developing representations across these views, and a surface agent, which moves more freely across the surface of the object and develops representations all over the object when given a single view. Because it follows narrow paths, it also benefits from multiple exposures to develop a good model.

That's the preamble. It may make sense to leave any discussion to the end. If anyone on the research team or otherwise notices I left something out, feel free to jump in and mention it.

The first main result in the paper looks at inference, particularly the robustness of this process. The same 77 objects that Monty has learned on are now used to evaluate the model's performance. We looked at classification accuracy—does it correctly recognize what object it's seen—and also rotation error, measuring degrees. How well does the predicted rotation align to the actual rotation of the object in the environment when Monty sees it? In this first base condition, although it's the simplest, it's worth pointing out that although the object is presented at one of those 14 rotations Monty has seen before, because of the way random seeds and policies work, the sensory path Monty follows on that object is actually different. It still needs to generalize to a certain degree, as it's not getting the exact same observations as before as it moves over the object. The accuracy is extremely high, close to 100 percent, and the rotation error, shown in the violin plot, is very low. 

To make things more interesting, we started introducing noise. We added noise to a variety of features of the model, or rather the perception Monty is getting in the CMP signal. This included the location of that signal and the color associated with the color feature. This starts to obscure some of the finer details of the models, such as the rim and the general shape of the handle. Fortunately, Monty still shows robustness in terms of accuracy and rotation error; there is not much change. The noise is not just applied to color and location, but also to the point normals, curvature directions, and the amount of curvature being sensed—all the features that come into the learning module that are important.

To really test Monty's limit, we introduced new rotations—rotations Monty has not seen at any point in training data for any object—as well as combining those new rotations with sensorimotor noise. Finally, we added a new color, setting all the observations Monty gets for any object to an intense blue, so it loses any visual texture information it might otherwise get from the object surface. There is some drop in performance, but it's worth emphasizing that this is a dataset with 77 objects and five novel rotations; chance performance is very low, on the order of 1 percent. Monty in general does quite well, given that even for a human, if you obscure information like color and add noise to the locations of sensed features, it becomes very difficult. For example, the fork and spoon prongs become a cloud of points. There are also certain objects in the dataset, such as a series of cups, which are distinguished only by their color. The fact that Monty achieves such high accuracy and reasonable rotation error is encouraging. This is especially notable because all the noise added at inference time is out of distribution—Monty is never trained on any of these perturbations to increase its robustness. This is generally a very challenging thing to achieve; robustness often relies on interpolation, where you densely sample the space and then test something in between those points. Here, we see generalization that goes beyond the distribution Monty has seen before.

This is the first result we were really excited to share.

We'll talk more about how Monty achieves this and how it relates to the concept of shape bias and using shape to recognize objects.

To understand this better, we wanted to examine the internal representations that Monty develops and how these relate to different objects and their shapes. One approach is to look at the evidence values associated with objects. When Monty senses an object, it maintains a series of hypotheses inside each learning module that increment over time based on movement and the next sensory observation. Here, we're plotting, for a particular learning module, these different hypotheses and how they increase over time. In this diagram, the gold point marks when the spoon head is sensed, and the learning module becomes confident that it's seen the spoon rather than something else. Up until that point, the evidence for these objects is quite similar. These objects all have, as we'll see, an elongated, narrow portion of their shape. On the next slide, I'll show evidence values across a series of experiments to see if Monty is recognizing and sensing one object like the spoon, and what other objects it has similarly high evidence values for versus those with very low evidence values.

This figure from the paper clusters ten different objects based on the evidence values Monty has when recognizing a particular object. It shows that Monty naturally clusters objects based on shape or morphology that humans would also identify. There are cutlery objects that are elongated with an end, box-like objects, and cup-like objects. It's important to emphasize that Monty doesn't have an objective function with supervised labels encouraging this clustering; it naturally emerges from the representations it develops and the hypotheses it forms when recognizing objects.

This also shows, for example, that the dec cup is a yellow object, as is the sugar box, but they are very far apart in this clustering space, whereas mug-like objects are much closer. One mug with a distinct handle is more separated from the other cups. There is a lot of semantic consistency in this clustering.

The significance is that Monty uses, or is sensitive to, the shape of objects. This is almost certainly the source of its robustness to noise, including situations where it encounters an entirely new color. If Monty uses global structure, then perturbing locations, adding a new color, or rotating the object doesn't change the global structure—the shape—of the object. If we added a huge amount of noise so it became a random point cloud, Monty's performance would fall, just as a human would no longer recognize the original object.

Recognizing objects by shape and using that as the primary basis of recognition is something humans do naturally, but this property has not emerged naturally in deep learning systems. Back in 2019, it was noted that if you took an image of a cat and replaced its texture with elephant skin but preserved the global shape, deep learning systems would classify it based on texture, not shape. This became known as the texture versus shape bias. Even today, after training on vast amounts of data, these systems still show this texture-shape bias. Monty looks at objects in 3D, but prior work with reference frames in 2D grid cells showed a similar effect: using reference frames with a sensorimotor system that moves through the frame and binds observations to locations leads to a stronger reliance on shape and global arrangement of features, rather than sensitivity to high-frequency textures that correlate with object classes and are picked up by deep learning systems.

This figure shows the degree of texture bias: one end is classifying entirely by texture, the other by shape. Humans are almost entirely based on shape, and the figure shows where different deep learning systems fall on that spectrum.

This is an exciting result, even though we haven't applied different textures to the objects as in the prior study. It's exciting because of what it implies and what we can explore in the future. Adversarial examples are instances where you add an imperceptible amount of targeted noise to an image, and a deep learning system will confidently misclassify it. Like the texture bias, this is something that scaling deep learning systems and giving them more data has not solved, and it's almost certainly related to shape bias. By adding targeted noise, you perturb the pixels in a high-frequency way, but the global shape remains unchanged to a human. We didn't look at this in the paper, but you could imagine creating an adversarial example for Monty by rearranging the locations of features to convince it that it's seen a different object, effectively changing the object's identity—something that would also fool a human, and a totally different phenomenon from these adversarial examples.

So far, we've been looking at robustness under the umbrella of noise, but that's not the only aspect. There are various types of noise and perturbation, but robustness goes beyond that. In particular, it's important for a good representation to have properties known as equivariance and invariance. Equivariance means the representation changes as the state of the world changes, while invariance means it doesn't change. For an intelligent system to be useful, it should have a mixture of equivariant and invariant representations under different conditions. I'll try to make that more concrete.

You can imagine looking at these pencils. Although the one on the right has a different color, a slightly different shape, is shorter, and a bit wider, you still recognize them as pencils. Your representation of a pencil is invariant to changes in details such as color or certain changes in shape, and you would also be robust to, for example, seeing this pencil in a dark room.

But if you turn the pencil, these two different ways of holding it in a hand mean your representation of where the pencil is in space and how it's oriented changes. That representation is equivariant. On the other hand, if you rotate it along the long axis of the pencil, then as far as you're concerned, it's basically in the same orientation, so those rotations are symmetric. For certain rotations, you want the system to be invariant to recognize symmetry, but for others, you want it to be equivariant. This sounds simple because it's something humans do naturally all the time, but it's actually a difficult property to build into a learning system. Another exciting result with Monty was that these kinds of symmetry representations emerge essentially naturally.

In particular, you can imagine having an object like this cup made up of points, and it can have a variety of rotations that are inherently ambiguous. These different examples show the same model rotated by different amounts. As far as a human is concerned, these look like they're the same rotation, but a rotation like this looks different. What's interesting about Monty is that as it's recognizing an object and sensing it, it has hypotheses about the potential rotation of the object, informed by what it's sensing. This is a detail best seen in the paper, but it's not like Monty has a list of rotations it has experienced before and only looks at those. It senses the object and can have a hypothesis about its orientation, but as it moves across the object, it will have a series of hypotheses that are all valid and getting sensory observations consistent with the model and that rotation. On the other hand, the hypothesis associated with this rotation of the object would not get consistent evidence, so the evidence value associated with it would quickly decay and decrease. In Monty, if it has a series of these hypotheses that persist together for a long time—so for one object, a set of rotations that are all persistently active—it eventually reaches a threshold, a state at which it says, "I believe these objects to be symmetric." This is what we called sensorimotor symmetry, because the way symmetry is determined is through sensorimotor exploration of the object. Monty can continue sensorimotor exploration as long as it wants to verify with greater certainty the existence of symmetry.

To examine this further in the paper, we wanted to use an additional metric. I mentioned earlier that we use rotation error when reporting how good Monty is at predicting the rotation of an object, because it's a very intuitive metric. But a common metric in the literature for measuring more symmetry-type properties is something known as the Chamfer distance, which you can think of as a metric for the distance between two sets of points. If you had a point cloud like this blue one and this orange one, for each point in the orange set, you would find its nearest neighbor in the blue set, calculate that distance, and do the same for all the other points. Sum those up, then do the same for the blue set comparing to the orange. What you end up with is a metric that's high for two sets of points like these and low for two sets of points like these.

The advantage of using this metric is that if an object's model is symmetric and you apply different rotations, as long as that symmetry holds, you get a low Chamfer distance. This figure in the paper illustrates that. Here, you see a single example of this object, the green cup, showing its ground truth rotation. There are a series of rotations reported. On the left is the rotation error in degrees, which is generally reported throughout the paper. On the right is the Chamfer distance. The different types of rotations shown include the minimum—of all the rotations Monty hypothesizes, this is the one with the smallest rotation error to the ground truth, the most likely hypothesis. Another is the rotation with the most evidence at that point in time. In general, the evidence values for these different rotations are extremely similar. A symmetric rotation here is another example from the set of symmetric rotations according to Monty; it is not the minimum relative to ground truth and not the most likely hypothesis. Lastly, there is a random rotation, which Monty does not consider symmetric. This shows that there is a very low Chamfer distance for all three of these rotations when applied to the learned model, consistent with true symmetry and human perception. The random rotation, represented by the purple bar, does not have a low Chamfer distance. The figure also shows that the rotation error can vary widely depending on which rotation is reported. This is why the paper reports the minimum rotation of the set, to capture the true error while avoiding the lack of intuition that the Chamfer distance gives, but also as an additional sanity check.

From one example, the same metric was run over many different objects that Monty sees, looking at the rotation error and Chamfer distance associated with the different hypotheses Monty developed, particularly those it believed were symmetric. Across all these examples, the Chamfer distance for any rotation Monty deemed symmetric is very similar to the one with the minimal rotation error versus the ground truth, supporting the metric ultimately used. This goes into detail about how symmetry is measured, but it's worth emphasizing how exciting it is that Monty developed this. There was no specific objective function requiring Monty to report symmetry or access ground truth models to measure its ability to detect symmetry. This emerged naturally through having a sensorimotor system move over an object, trying to recognize its rotation and developing hypotheses.

This is a great attribute for the system in terms of long-term future capabilities. For example, with a pencil, depending on its orientation, learning to write or erase depends on the orientation along certain axes or rotations of symmetry. However, spinning the pencil in your hand does not affect the ability of the pencil tip to write. It's important that every time you spin the pencil, the system doesn't treat it as a new rotation requiring relearning how to use the object. This applies to behavior and interaction with objects, as well as to compositional representations where one object is part of another. We were pleased to see this kind of symmetry emerge as it did.

That covered robust inference. Next, I'll discuss rapid inference and how quickly Monty can recognize what it sees in the world. Biological perception is inherently about movement. When we look around, only a tiny fraction at the center of our visual field is in high acuity. If that part of our eye, the fovea, had the same visual acuity just a few degrees out, we would be legally blind due to the sensory impairment. We are highly dependent on rapidly moving our eyes, and this applies to other modalities like our hands.

This is naturally built into Monty, but until now, we haven't discussed much about how Monty actually moves. One of the exciting aspects shown in this paper is how it can use policies to act intelligently, using a mixture of model-free and model-based policies.

Model-free policies involve taking sensory information as it comes in and, without an explicit model, deciding how to act in the world. In the paper, we examine a surface curvature-guided exploration policy where Monty follows the surface of an object, maintaining contact based on its sensory input. When it identifies areas of prominent curvature, like the rim of a glass or the bottom of a cup, it sometimes follows that feature for a period of time. This is our model-free approach, which is similar to what older parts of the brain handle in humans, but can still be quite powerful.

The other policy we examine is model-based, called the hypothesis testing policy. Monty maintains a series of hypotheses and acts to test and eliminate them. The figure in the paper shows Monty sensing an object—in this case, a spoon. It starts on the handle, moves along the bottom, and then one of Monty's learning modules outputs a goal state to move to the head of the spoon to disambiguate it from another object. The decision to move to the head of the spoon uses Monty's internal models, hence the term model-based policy. At this point, Monty has strong hypotheses for the spoon and the fork in particular orientations. It can mentally compare these hypotheses and determine that the largest difference between them is at the head of the object, the tip of the spoon, making it the best place to move. If it moved to the neck, which it hasn't explored yet, it would observe the same thing whether it's on the fork or the spoon, making that a less useful observation.

The next part of the figure shows the evidence values Monty has internally as a function of the step in the episode. When it takes a goal-driven step, the hypotheses for objects with similar shapes—spoon, fork, knife, marker—increase as time goes by. When Monty executes the goal state, there is a divergence in the hypotheses because all subsequent observations correspond to the head of the spoon and are not consistent with a fork or a knife.

The same approach can be applied to poses. If Monty is confident it's on the mug but unsure of its orientation, it can use the same principles to infer this. In this case, the two most likely hypothesized rotations result in different handle orientations. Based on one hypothesis, Monty attempts to move to the handle, and once it does, all hypotheses associated with incompatible observations start losing evidence.

We then looked at the effect on imprints as a whole. Looking at accuracy and the number of steps in an episode, with the baseline policy—more like a random walk without consistent direction or curvature guidance (the model three policy)—and without model-based goal states, we get reasonable accuracy but many timed-out episodes. These are cases where Monty still has the correct hypothesis at the end but is in a low-confidence state, likely because it hasn't seen a feature that would definitively disambiguate it from other objects. We set a maximum of 500 steps per episode, so these timeouts correspond to episodes that continue until the end. When Monty becomes very confident about what it's observing, it can terminate the episode and declare the object. We see a significant increase in converged episodes when we introduce the model-free and then the model-based policies.

That's reflected here in the number of steps associated with the episode. It's interesting that we already get strong performance with the model-free policy, and the model-based policy only gives incremental improvements in accuracy. It's not a huge change at that point, but we're still excited about the implications of this policy because of what it means for acting intelligently in the world. For the benchmarks we have now, being able to efficiently do something like move to the head of the spoon doesn't result in dramatic improvement, but as you introduce more noise or need to act more quickly to achieve certain goals, this distinguishing benefit is likely to become more apparent. Another element of the model-based policy that we're happy with is that all of this learning is taking place independently of explicit rewards. Monty is just exploring these objects and learning about them in the same way that, if you were in a new city or given a new object to hold, you would quickly learn its properties without being told you'd be paid or have a task to complete. It's a natural ability to quickly learn about the world. This fits well with what Tolman observed in 1948 when he studied learning in rats: if you put them in a maze, they naturally learn the structure of the maze even without a reward like food. This is key to developing representations that enable flexible behavior, because reward signals in the world are sparse and rarely perceived. What you want is a system that naturally goes out and learns about what there is.

That was all about rapid inference and policies. Next, I'll talk about rapid inference and voting. Voting is an algorithm that Numenta originally proposed in the context of cortical columns and has since been adapted in Monty and evolved to work well.

To put it in context, we've been talking about the importance of movement and how biological perception is inherently sensorimotor—it's all about movement. But we are also able to integrate inputs from multiple sensory organs, either from different parts of our retina or from our hands and eyes. We don't perceive the world in a totally fragmented way. The question is, how do you integrate this information to maintain robustness and enable more rapid inference?

The voting algorithm is fairly involved, so I won't go into full details here. There's a helpful description in the methods section of the paper. At a high level, you have different learning modules receiving inputs from different sensory modules, such as a left hand and a right hand. Each learning module has hypotheses about what objects it has seen. With voting, they're able to share those hypotheses. One important thing is that when those hypotheses are sent, we account for the sensory displacement between the sensorimotor modules when transforming them for use by the other learning module. This is important because we don't want to just vote on, say, "I think I'm on a mug, are you on a mug as well?" That would result in a bag-of-features form of recognition, where as long as they're sensing something that could be a mug, they all agree it's a mug, even if it was totally scrambled. That's the kind of texture bias and lack of shape robustness that humans don't have and that we don't want Monty to have. That's why we use the sensorimotor displacement when carrying out voting.

Practically, this means you can have multiple sensory patches. For example, here you have different sensory patches arranged in an approximate grid, each seeing a different part of an object at a given time. They're each connected to a learning module, and those modules can communicate with one another via voting. By each seeing part of the object and sharing their votes, we can dramatically speed up inference. The animation shows that at any given time, a particular learning module can be seeing something different. No learning module is seeing the whole object—this is a viewfinder view for the benefit of the experimenter.

When doing voting, we can parameterize how many patches we have. In this vision-like case, this is the size of the grid—the number of C modules and associated learning modules, the number of pairs. Here there are five; in another example, there are eight.

We can also parameterize how many learning modules must converge for the Monty system as a whole to converge. We don't necessarily want to wait for all the learning modules to say they're confident, because some may not be getting much input. In this paper, we set this to two: we need two learning modules to converge for Monty as a whole to converge. We then look at what kind of agreement they have on the classification. The experiment varied the total number of these, essentially the size of the grid.

That's what's shown here, where we see the number of steps until convergence as a function of the number of learning modules. As we hoped, we see a rapid reduction in the number of steps required for the system to converge as we add more learning modules, almost immediately eliminating many timeout episodes associated with 500 steps and getting down to the minimum number of steps required before an episode can converge. It essentially reaches that point.

At the same time, we don't want this rapid inference to come at the cost of accuracy. The plot on the right reassures us that we can achieve this without a reduction in robustness. If we use a more naive algorithm, like a bag of features, we would almost certainly see a decrease in accuracy as we start adding non-meaningful hypotheses.

To wrap up the discussion on voting, it's worth pointing out that it's a great advantage for Monty to benefit from multiple sensory inputs at once. However, while Monty benefits from this, it is not reliant on it. Just as you can feel a coffee mug with one finger or look at the world through a straw and still recognize what you're seeing, Monty can operate in a single learning module regime without issue. It simply benefits from having multiple inputs. This is a defining characteristic of Monty that we want to maintain. No matter how artificial the setup, in the real world, your sensors are inherently limited in sampling information. You can never sample everything at once. Accepting that information needs to be integrated over time leads to architectures like Monty, where movement is the central motif of the system.

We talked first about robust imprints and then about rapid learning—sorry, rapid inference. The final main section of the paper looks at rapid, continual, and efficient learning. I'll break that down.

In this part of the paper, we compare to VIT—vision transformer networks, a form of deep learning architecture—to ground the results. We're not implying that deep learning doesn't have useful applications; we use deep learning systems ourselves all the time. The point with these results is that Monty is a fundamentally different approach with unique advantages, which is what we're excited to discuss today.

The first thing we looked at was how rapidly Monty could learn what's sometimes referred to as few-shot learning—in particular, how quickly it can develop good representations given only a few observations of a particular object. To evaluate this, we take all the objects in the YCB dataset and present each one at a fixed number of rotations. For example, when the system—either VIT or Monty—is given one object view across all YCB objects, it gets a single view. If it's given two views, it gets two views, and so on up to 16 and then 32 views. After each of these conditions, we look at how well the system performs in terms of classification accuracy and rotation error.

We find that Monty learns extremely rapidly. Monty, shown in blue, reaches 50% classification accuracy after only a single observation and quickly approaches optimal performance. The systems we compare to are VITs under various conditions. The strongest performing VIT is the pre-trained one, which only needs 25 epochs of training to perform at its best, but this model has been pre-trained on 14 million labeled images scraped from the internet. Classification of household objects is not out of distribution for this network. It's telling that Monty, which knows nothing about these objects, can achieve essentially comparable performance. In a moment, we'll see how they differ on rotation error and how Monty performs in terms of computational efficiency. If the VIT is trained from scratch for 75 epochs, although it's only getting a certain number of rotations, it's revisiting those rotations many times to get better performance, but still significantly below Monty, which only sees each object once. Just as humans can learn extremely rapidly—we don't need to pick up a new object and look at it a hundred times before recognizing it—Monty only needs to see something once. As a comparison, we include a VIT with one epoch of training, which is the only one that gets the same amount of data exposure as Monty. As you can see, it performs approximately at chance; it's just not enough data for a deep learning system to learn.

A stark difference appears when we look at rotation error, which isn't surprising because even for the pre-trained VIT, it has never had an objective to explicitly predict rotation error. For the network to develop this ability is not feasible with this amount of training data. Monty, however, shows rapid learning as it sees more rotations.

To understand where this generalization comes from, one key aspect is that Monty naturally infers the potential rotation of the object based on what it's sensing. For example, if it senses a corner at the start of an episode, that could be consistent with the spam can in one orientation or another because of a similar edge. Those hypotheses will both exist, and Monty will move over the object. Importantly, the hypothesis of the can being inverted is not dependent on having ever seen the can inverted, just as you could recognize an object upside down that you've never seen that way by inferring its orientation. Monty can do the same. After seeing only a single rotation—one view of each object in the dataset—Monty already gets 50% classification on a dataset with 77 objects, and these are shown at novel rotations. Many objects have natural symmetry, so even if we present different views, some aspects will be similar to the view Monty or the VIT saw first. However, without handling rotation, a system cannot rely on natural symmetry alone to do well. That's why we don't see similar performance in the VIT trained from scratch.

It's also helpful to understand how Monty is able to learn so quickly. A useful intuition is to conceptualize the reference frames as a map. If you were a tourist visiting Rome, you might go to St. Peter's Basilica, then travel east and see a bridge, and eventually reach another landmark. As you move through the city and see different parts, you naturally associate what you've seen with each location. This process happens quickly and naturally with spatial navigation. Much of this relies on an older structure in mammals, the hippocampal complex, but the principles are the same: you perform movement and bind information to a location in a map. In this case, the map is the reference frame that the learning module, or in humans, the cortical column, is using. It's unambiguous where the system needs to update its representation when learning something new because of this use of a map. This leads to other benefits, such as continual learning and computational efficiency.

A longstanding problem in machine learning, especially deep learning, is catastrophic forgetting. If you train a system on one task and then use, for example, a deep learning network with backpropagation of error to learn a different task, you perform global updates to its weights and essentially forget the previous task. This is catastrophic forgetting. Continual learning is the inverse: if you are resistant to catastrophic forgetting, you can learn continually. This is something we found in Monty, and it was something we expected simply from how the system works.

To show the task setup more concretely, we have 77 objects, which we break up into 77 tasks. In any given task, Monty or the VIT will learn one object, given all its canonical rotations—those 14 rotations discussed before. We then evaluate on all the objects that have been seen up to that point. For the system to do well, it must learn about the objects it has just seen and retain memory and information about the objects it saw before. We then learn the next object and repeat.

To illustrate this, imagine the first object is from the Lego dataset, learned on 14 rotations. The rotations aren't shown here. We then infer with some novel rotations and see how the system performs, either the VIT or Monty. For the second task, we learn the fork and assess performance on both the Lego object and the fork, since both have been observed. The third task is the banana, and the system must perform recognition on all three objects.

With that outline, you can see the actual performance observed. On the Y axis is the accuracy on all observed objects. The system's performance depends on how many objects it has seen. On the X axis is the number of objects learned. With Monty, as expected, it does well at first because it only knows a few objects and quickly identifies the observed object. Over the course of the tasks, there is some interference as it learns about new objects, such as a peach similar to an apple, making future inference more difficult, but not catastrophically so. This contrasts with the pre-trained VIT, which was the strongest performing network in the previous rapid learning task. For the first task, it can only classify one object and achieves 100%, but almost immediately begins overriding its weights to predict the new object it has seen, obliterating information in its weights about recognizing other objects.

This is shown in an alternative view, where we have the number of objects learned and the target object. Along the diagonal is the performance on the current task. In general, if the system can learn the current task, the diagonal should be green, which is what we see for both models. Below the diagonal is the performance of the system on all the objects it has seen before. This shows that Monty is able to retain a memory of many of these objects. There may be certain objects that are difficult for Monty to learn, perhaps due to ambiguous shape or other reasons. The VIT, again, catastrophically overrides its weights and shows almost no evidence of recalling previous objects it has learned. It's important to emphasize the significance of continual learning as a task structure, especially in this extreme case where each task consists of only a single object. Deep learning systems are inherently dependent on contrastive learning to develop representations, needing to see one object and then another to learn to separate those representations, doing this constantly in a shuffled order. However, when moving through the world, the statistics are constantly changing, leading to the problem of catastrophic forgetting. The inputs are also highly temporally correlated. If you were learning about an edible fruit as a hunter-gatherer or animal, you would naturally find these things clustered together. You wouldn't get a lineup of all possible fruits to compare side by side. Being able to learn in isolation, given what's in front of you through movement, is crucial. What we explored with Monty is really crucial to understanding why Monty is robust to continual learning.

It's useful to consider a math analogy. If you are in Rome, seeing new things, and you make a note to remember that, you're only going to update your internal representation of your map of Rome. You won't make changes to all the other maps you have for other cities that may be familiar to you. This targeted, local, or sparse updating to the model is exactly what Monty does when it sees something new. This prevents catastrophic forgetting. As we'll see in a moment, there are also benefits in terms of computational efficiency.

In particular, we see dramatic improvements in the computational efficiency of Monty when it comes to learning, compared to these VIT models, and actually some decent performance as well in the domain of imprints.

To look at computational efficiency, we quantify this with floating point operations. You can think of this as any floating point operation happening in the computer. A variety of algorithms, whether it's a VIT, a deep learning system, or how we've implemented Monty, need to execute these operations to carry out a task. We're counting how many of these flops in total are being done, not how many happen per second. Sometimes "flops" refers to per second, but here we're just counting the total number of floating point operations.

At learning, we see a huge difference between Monty and these other systems. We have training flops along the x-axis, and this is a logarithmic scale going up to 10^20, where Monty is all the way down at 10^11. This is all the learning that Monty does. The VIT, which did significantly worse in terms of accuracy, classification, and rotation error, still uses orders of magnitude more flops to perform learning. The pre-trained VIT, which was the only one that could do comparably on the classification task, uses far more flops in the fine-tuning stage, but particularly staggering is the amount needed for the pre-training stage.

This is where Monty really stands out in terms of computational efficiency.

For inference, we can do a similar comparison. We show a variety of different VIT models of different sizes. Generally, as you go to the right towards more flops, these are VIT models with more parameters, and often more parameters in deep learning result in better performance.

These are either the pre-trained variant, if they have the filled-in circle, or from scratch for Monty. Then we look at a system that has a random walk. This is the distant agent, like a camera performing a random walk in all our VIT comparisons until now. Here, this is the agent we use, even though it's the worst performing Monty system. The reason was to make the comparison fair because the VIT sees the object from one side, so Monty also sees it from one side. However, Monty is a sensorimotor system. It's more natural for it to engage its policies to act intelligently in the world. That's what we see with this system that has access to the hypothesis test and policy. Even the baseline does extremely well in terms of the accuracy versus flops tradeoff, particularly with the rotation error. When we add in this policy, rather than making the system more complex and resulting in more flops, it dramatically reduces the number of flops while also improving accuracy and reducing rotation error. This demonstrates the real benefits of bringing sensorimotor concepts into learning intelligent systems.

To understand flop efficiency, it's a concept similar to learning. If you're in Rome and laying down a representation, you're only creating a memory in your map. You don't have to update all the other locations or maps for other cities. It's a very local, sparse change to the system, requiring very few floating point operations.

During inference, even though Monty considers a broad search space of hypotheses at the first sensation—since you wouldn't know what you're touching just by feeling the surface—it generally considers all 77 objects as possible, along with many rotations. It starts with a large search space, which involves many floating point operations. As it senses more of the object and becomes more confident, it quickly rules out many hypotheses. This is visualized here: during inference, you're trying to figure out what city you're in. Because of the landmarks you've already seen and the movement you've done, you're pretty sure you're somewhere in Rome, just trying to figure out where. You can eliminate all these other hypotheses as ones that need testing, significantly reducing the amount of flops that accrue.

To conclude this section on rapid, continual, and efficient learning, it's important to emphasize that these are natural consequences that emerged when working with sensorimotor learning and reference frames. We didn't carry out laborious or targeted optimization to extract or bake in these properties; they naturally emerge from the way learning happens. This is one reason we're excited about the direction the Thousand Brains Project is taking and the progress it's making. All of this was originally informed by hypotheses about how the brain works, based on neuroanatomy and neurophysiology. We implemented the system to achieve basic properties like recognizing objects, and very naturally, properties that have been difficult to achieve in machine learning—such as shape bias, symmetry detection, continual learning, and efficient learning—are all emerging. We think that's a good indication that we're approximately on the right track.

Lifelong learning is a huge goal for developing intelligent systems. Often, the focus is on continual learning because it's a major hurdle that hasn't been overcome. But continual learning and avoiding catastrophic forgetting is just one element. Being able to learn rapidly and efficiently—without needing entire GPU clusters every time you want to train your model—is extremely important to unlocking that capability. It's exciting that Monty shows evidence of all three: rapid, continual, and efficient learning. The images here show lifelong learners, some adults, and an example of a totally novel object you've never seen before. After a few glances, you already have a reasonable representation of what it is. You didn't need to compare it to other objects, or sleep to initiate recall-type contrastive learning, or do any of these things. You were able to learn it almost instantaneously.

That concludes the results discussed in the paper. I'll briefly discuss the conclusion and future research we're working on. One key area we're excited about is compositional objects. Viviane will talk about this separately. Our hierarchy paper, which looks at neuroanatomy, has interesting ideas for how the brain might do this. In the same way Monty is a manifestation of the Thousand Brains Theory from a couple of years ago, we're now taking newer neuroscience theory ideas and bringing them into the latest version of Monty. This will allow us to develop more complex, hierarchical representations of objects, like a mug with a logo on it.

Another main area of research is object behaviors. This is more theoretical, as we're still figuring out how the brain represents objects that move and can be interacted with. We've made some interesting progress and look forward to sharing more soon.

All this has been fundamental research, but our long-term hope is for a variety of beneficial applications in the real world, from agricultural robotics to the energy sector—ensuring equipment is maintained—to medical ultrasound. These are instances where you may not have much data and the real-world distribution can constantly change. You can't rely on features that may change due to lighting, wind, rain, or weather. You need to rely on things like shape, and all these things Monty demonstrated in this paper should be useful in downstream applications. That's something we're looking forward to.

Thanks very much for listening.