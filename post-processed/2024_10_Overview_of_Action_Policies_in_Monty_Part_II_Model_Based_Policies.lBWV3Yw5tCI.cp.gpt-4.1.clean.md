Perfect. Today I'm going to continue the overview of action policies in Monty. Although we're recording this to share in the future, the emphasis is on discussion, so please stop me to ask questions or raise objections.

That's par for the course.

Continuing from last time, I'll briefly cover a few points: the core anatomy, the philosophy of learned versus hardwired policies, a discussion of model-free policies with reference to abstract spaces, model-based policies, and finally hierarchical policies and future directions.

Last time, we talked about how model-free policies generally map onto subcortical structures in the brain, whereas model-based ones map onto cortical structures. I thought it was worth pulling up a figure so people can see the basal ganglia, as these regions are most associated with model-free learning. They have significant involvement in action initiation and control, and there are significant dopamine projections from the substantia nigra, which are implicated in reinforcement learning. For model-based learning—by the way, on the previous slide, the cerebellum, which is at the bottom right, is very heavily associated with learning and movement, apparently mostly small movement changes and timing changes. It's a huge part of the non-model-free learning system, doing fine-tuning of many things. I wanted to point that out because someone listening might say, "Wait, you didn't mention this at all." The brainstem is where the actual motor signals go out and affect the muscles in the body, including spinal muscle. The thalamus is involved in everything, and the spinal cord handles reflex reactions. There's a lot going on—pretty much the entire body is a model-free learning system. The basal ganglia get projections from the cortex, and my understanding is the recurrent loop is then backed by the thalamus.

Model-based learning is where you have object models learned in each cortical column. You can think of these as represented across the fourth and sixth layers of each cortical column, where the model generally resides, but there are many different elements to a model. It seems L5 is probably the most important region for action planning in each column, because this is where you get a direct motor projection to subcortical structures—both a connection to the thalamus and one to brainstem nuclei. What's interesting is that this occurs in every column in the brain, regardless of whether it's prefrontal cortex, sensory cortex, or motor cortex; this is a very prominent projection, which is unusual in terms of how motor outputs are often formulated. You have this direct projection from all levels of the system. It's worth pointing out that this is really the only direct output from the cortex: it's motor. We think of it as a sensorimotor system—the only output is motor. There are other modulator connections, but this is viewed as the output from the cortex.

Last time, we discussed the separation of model-free and model-based learning. With the example of riding a bike, if you're teaching a child to ride, the initial stage—getting onto the bike, being told to pedal, or seeing an adult pedaling and having a basic idea of what to do—is probably more model-based. The fine motor coordination required to actually balance a bike, and the innate knowledge that develops over time, is more model-free. Many actions end up involving both types of policies.

So that's just revisiting model-free versus model-based with a focus on the anatomy. Another point worth touching on regarding the philosophy of policies and our approach in Monty is that much of what we've implemented might seem like hardwired policies, as if we're trying to cover the entire space of any policy the system might need and build that from the ground up. That would be reminiscent of good old-fashioned AI, but that's not the intent. The intent is to design a few key policies that we believe are important, which can be seen as primitives that more complex policies can build upon. Not everything is from scratch or needs to be learned through random exploration; the system has these starting points it can leverage.

We touched on this briefly last time, but that could include state switching of model-free policies controlled in a top-down manner by a more model-based policy, as well as parameter adjustment. With state switching, you could, for example, start following minimal curvature, then switch to maximum curvature. With parameter adjustment, you can imagine momentum in terms of how much you persist in moving in the same direction or move randomly. If you're trying to sense an area repeatedly because it's a noisy input and you want to clarify the information, that's a simple adjustment. If you're trying to quickly gather as much information as possible over a broader area, then a higher momentum movement is natural.

It's interesting to think about hardwired versus learned, and model-based versus model-free. A lot of model-free learning is still learning—like riding a bike, it's model-free learning. The cortex has to deal with a body that's changing, with model-free action policies that change over time as our bodies grow, as we get longer limbs or injuries. The cortex doesn't just deal with hardwired policies; it also deals with learning policies that might be model-free but are still learning. The cortex has to interact with a world and other brain parts that are not static. There might be some hardwired things like reflex reactions, but most subcortical processes are also changing over time, and the whole system has to constantly adjust. It's something to keep in mind as we think about this.

In terms of finishing up the model-free policy discussion, I thought it would be interesting to touch on abstract spaces, which we didn't get to last time. In general, abstract spaces are really defined by abstract actions. There's nothing particularly special about the space itself—something like a family tree or simple multiplication fits naturally into some subset of 3D space. What really defines it as abstract is the movements you make through that space. In a family tree, there are discrete relations like parent, grandparent, and grandchild, which move you around the space in very defined ways, and similarly with mathematical operations.

Last time, I talked about model-free policies we've implemented, such as following principal curvature on object surfaces. It's interesting—though speculative—to consider what analogies might exist in more abstract spaces.

I think it's useful to imagine that, generally, information is clustered. 

I'm confused, Niels. Are we talking about abstract spaces in the model-free action policies?

Yes.

That seems odd to me. I would assume that abstract spaces would always be model-based, by the very definition that they're not real-world things we could have evolved with. So why would you consider them part of the model-free space here?

Just thinking of it as simple heuristics for moving through space that could apply in abstract spaces as much as in physical ones.

I guess I'm challenging that. It seems to me that abstract spaces would almost by definition have to be model-based.

Do you disagree with that? My spinal cord can't know about family trees or mathematics. I agree with that. I can imagine there would be ways for the cortical structures, if it's useful, to learn simple heuristics about how to process more abstract inputs. In terms of the model-free subcortical stuff, it's very physical—you've got your finger touching something, you're trying to balance, you're walking, or you have reflex reactions to physical things in the world. But when it comes to abstract spaces, I can't imagine what a subcortical relationship could be. There's nothing physical about it; it's all in the model. It's something I've learned about the world that isn't physically manifested. The parent-child relationship is not something I can detect physically; it's something I have to know or learn about. 

So, I'm just thinking if action in abstract spaces is more like how you move your attention through that space, wouldn't that be cortical? How could that be subcortical? I don't get that. It comes back to whether there could be some sort of more model-free type stuff in cortical circuits. All right, so I'm not arguing this would happen in subcortical structures. What I'm arguing is that, generally, the simpler of model-free and model-based is model-free. If the cortex is capable of model-based, then maybe it uses some kind of more model-free heuristics. That's an interesting question. Are there model-free action policies in the cortex itself? I've never thought about that question. My initial assumption would be no, but I haven't thought about it. Whatever it is, it has to work across all modalities, across everything. Up to now, everything in the cortex, everything in a learning module or cortical column, has to be generic enough to work for everything—physical things and abstract things. Now we're asking if there's an action-free policy that similarly works. It's an interesting question. I see where you're coming from. It could be in the cortex itself, but it would have to be something that applies everywhere. Maybe that's what you're talking about now. 

This is very speculative, so maybe this is totally wrong. I was just thinking there seems to be some kind of symmetry between what we intuitively do when we're exploring an object with our finger and what we might mentally do when exploring a more abstract space. If you imagine the dimensions of information are often clustered in high-density regions—if they're Gaussian in each dimension, with an ellipsoid structure—then following the minimum and maximum curvature on this manifold is essentially following the local principal components, the dimensions of most variation. To make that more concrete, if you're thinking about someone's political views, that might fall into a two-dimensional space of authoritarian versus libertarian and left-leaning versus right-leaning. If there's some analogy to how your finger notices the curvature of a cylinder and finds those two directions most interesting, maybe there's something similar in abstract spaces where you would explore primarily along these dimensions when trying to understand someone's political views.

It's a bit of a stretch. If I were to pursue this further, I'd say if I'm going to come up with some model-free action policies in the cortex, I'd want to see that the same policies apply to vision, touch, and hearing.

Not just political persuasion—if it's in the cortex, it has to apply to everything. What would be the equivalent in vision? What would be the equivalent in audition? You couldn't just consider abstract spaces; it has to apply to other things too, which are universal. It's something to think about. That's what I was trying to get at with the analogy of the surface of manifolds in abstract space and the surface of objects. In vision, you have principal axes of the object, which are the dominant directions and points of symmetry around which the object tends to be reflected. That's often how we tend to understand objects.

Maybe there's some similarity there. If you're studying a metro line, which is a graph, your eyes are going to follow minimal variation in perceptual space, like the line color. That might be subcortical, but in a more abstract space, you follow the line name. I can also see how that could be a model-based policy. Maybe it's not that relevant. In our work, we have to get the basics—Monty first has to solve all the physical world interactions. It's fun to think about abstract spaces, but it's not our first priority. If anyone's watching this video, please watch the first one first, which gives broader context. Jeff points out that abstract space is definitely more niche, something for the long term, not the current work.

Maybe that's a good way to segue to model-based policies, unless anyone has other comments or questions.

As we touched on last time, model-based policies leverage learned models of objects to make more planned, deliberate action proposals. We have two that are currently implemented in Monty. This is a situation where we're crafting these policies, but they could be used as primitives for more general policies. Are both of these basically about learning and inference? 

When I think about action policies in the cortex, I think about how the cortex achieves results in the world—movement. But we're not talking about that here, right? We're talking about how the cortex learns objects and how to infer them, is that correct? 

No, that's correct. That's a good point.

Not yet changing the state of the world.

The key one of these is the hypothesis testing policy, which is the one I'm going to focus on. This is about how we can efficiently move to features that disambiguate object identity and/or pose.

The concept is: if you imagine we have multiple models in our memory, like the fork, spoon, and knife, and we have some observed object—say, we're feeling along the handle of this knife—we don't yet know what object it is. We'd like a policy that enables us to efficiently move to the part of the object that will quickly disambiguate between these three.

The way we're going to do this is called local graph mismatch. I'll talk later about how hierarchy can fit into this. Essentially, we take the most likely object hypotheses we have, use the most likely poses associated with those, and look for a mismatch in these aligned graph regions.

Breaking that down: imagine our two most likely objects are a spoon and a hammer. The true object we're observing is the spoon. Our object models are quite simple because we don't have any hierarchy, so we have lots of points, and all these points are associated with morphological features—point normals with some principal curvature directions.

We have some most likely poses associated with these, and what we do is transform, internally in the mental space of the learning module, both of our two most likely hypotheses by their most likely poses. That gives us some alignment, just naturally based on how Monty works. When you have these two graphs aligned, the algorithm in its current version finds the nearest neighbor pair across the graphs, then takes the Euclidean distance between those and finds the maximizer. It's the nearest neighbor that is most distant across these graphs.

For every point in the red graph, there's a nearest neighbor in the blue graph. For example, this point in red has a nearest neighbor in blue, and so on. If we take the distances to their nearest neighbors, the point with the largest distance to its nearest neighbor is the maximizer. That point is the one we want to test. I'll talk later about how this can also apply in abstract or feature spaces, which is a more natural comparison.

The everyday logic behind this is that these two things are aligned, but they're most different where they don't align. That's what this is looking at. A more natural way to measure and test this is, if you have hierarchy—say, a representation of a spoon head and a handle, and for the hammer, a handle and a hammer head—you do the same thing, but if you compare them in feature space, the handles are very close together, but the hammer head and the spoon head are very far apart. That's where you go to. Rather than Euclidean distance in physical space, you're looking for the maximum difference between these two models at some point.

In the physical, non-hierarchical model, it's linear Euclidean distance, but in object models, it doesn't have to be physical distance at all. In the abstract case, the big difference could be just different features—for example, SDR difference. The SDR representing the hammer head and the spoon would be more different than the SDR for two different handles. This assumes the SDRs are conveying some way of representing feature alignment, whether that's with SDRs or something else. I'm assuming features aren't just random points in high-dimensional space.

This works when we've already learned the spoon and the hammer and those points are in memory for Monty. In the unsupervised or from-scratch case—say, we explore the handle and only have knowledge of the hammer, but the actual object is a spoon—we don't have the whole graph of the spoon. Maybe this doesn't apply to the unsupervised case, but how would it work then? That's an interesting question. I should have clarified: maybe this is only for inference once we've learned many objects.

You can imagine doing something similar during learning, where you only know about the hammer, so you think you're on the hammer. You might move to the most interesting part of the hammer—the hammer head—because of other objects you know about, but let's say you don't know about a spoon. With the way objects are structured, there will be similarities in where the most interesting parts are. In general, this isn't set up for use during learning. For the learning part, you can't use the nearest neighbor concept. All you can do is say, if all I know is a hammer, I think I'm touching a hammer, and then suddenly I feel something that doesn't fit the model. If you already knew some objects, maybe that could help bootstrap how quickly you learn, but learning requires explaining both learning and inference at the same time. Generally, you'll sense something that doesn't fit your model, and then the question becomes how to decide if it's something new or an extension of the existing model.

To answer your earlier question, Jeff, if your most likely hypothesis was the hammer and your second most likely was the spoon, the algorithm would start with the hammer. For every point, it would look for the nearest neighbor in the spoon graph. In that case, it might be this point, where the most distant nearest neighbor in the spoon graph is this one, so there's some asymmetry depending on which object you start with as your base object.

If you're searching through objects for nearest neighbors, it would work, but it's hard to understand how neurons would do that. They can't search through lists.

One reason we value the union property of SDRs is that it allows you to handle ambiguity without sorting through lists or trying all hypotheses. I don't know the right approach for Monty, but clearly neurons can't do that, at least the way we've implemented it.

In practice, this looks like the following: this is the spoon as it's oriented in the real world. The agent's finger is touching it here. After a few steps, it's on the fifth step. It has some sense of what object it's on, and in particular, it thinks it might be the knife or the spoon. In its mental space, it aligns these based on the most likely poses of these two objects. That gives us what we expect: the spoon and the knife are both here at the heads. It uses the most distant nearest neighbor, which gives this red point here, just like on the back of a spoon, as the place it should test.

The policy then sends that as a target location to the lower-level parts of the motor system, and the agent moves there. In the current implementation, it moves there instantaneously—it's teleported to that location—and can then keep moving around from there.

It seems like what we have here is a hybrid, which might happen between a single column and multiple columns. In a single column—thinking in terms of neurons, not Monty—let's assume it can't check multiple hypotheses or go through a series of objects to find the nearest neighbor. All it can do is move along until it discovers something that doesn't fit the current model and signals that something is wrong. 

If multiple columns are active at the same time and know their relative positions, one might say, "I think I'm on the spoon," and all the columns, even those not receiving input, would agree they're observing a spoon. A column may say, "In my current location, I'm not on the spoon," or maybe, "I am on the spoon." Another column could be observing a distant point of the spoon or the hammer. If it says, "I think this is a spoon," but another column detects something beyond where the spoon is supposed to be, it can signal, "We ought to go over here because it doesn't fit." If I'm not supposed to be seeing the spoon but I am, or I'm seeing something unexpected, the system decides to investigate further. 

I'm trying to imagine how neurons would do this. They would probably act more like multiple columns, testing different points on the object simultaneously, including places that shouldn't have anything. If one detects something unexpected, attention is drawn to that area, whether visually or tactually. For example, if I'm grabbing an object and one patch of skin doesn't feel right, attention is drawn to that part, which achieves the same effect.

In a single-finger setting, we could also work out where to test, but you wouldn't test anything until you encountered a problem. If I think I'm on the spoon, I move my finger along and assume it's the spoon until I find out otherwise, rather than considering multiple possibilities at once. It's an interesting question. Even with one finger, you'd probably have multiple columns active, and if there was mutual inhibition, some would represent one hypothesis and others another. If you have a single hypothesis, it would be voted across the columns, and the system quickly settles on an answer. As soon as it does, different columns can signal if something isn't right. 

The general thing to consider is what a single column does versus multiple columns in the same region versus columns in a hierarchy. These are the three basic scenarios: single column, multiple columns working together, and columns in a hierarchy. Some action policies might mix and match these scenarios. For example, if you have a mug versus a handleless cup, the process might propose testing the handle. If the knife is the most likely hypothesis, the system proposes a location to distinguish it from the second most likely.

One nice element of Monty is its rotation equivariance. It always accounts for the rotation of the object in its hypothesis space, and the rotation it was learned in is arbitrary. If you plot the rotation of the internal models of the knife and the spoon, oriented as they were in the real world, they don't align, but when the system senses it, it can sense the orientation of the handle and naturally align these in its hypothesis space.

and then you can see how, over the course of inference, the hypotheses evolve. Eventually, you might reach a point where you are convinced it's the mug, or you've eliminated all objects but one, but you might still have some uncertainty about the pose. You can also use this process to infer that. A better example is here: early on in inference, you've sensed the handle, but it can be ambiguous whether the spoon is oriented one way or its mirror reverse. You might test a point like this, but later on, you could have subtle uncertainty about the object's pose, such as whether it is rotated 10 or 20 degrees.

Bringing everything together, this shows the model-free curvature-following policy with model-based hypothesis testing. Here, the surface agent, shown in white, follows the minimal curvature of the handle, moving to the end. When it reaches the end, it has high certainty that it's looking at something like the spoon, fork, or knife. It then performs a hypothesis-based jump to the head of the object. When it does this, it has one hypothesis in mind, such as, "Is this a fork? Let me test that." If it isn't, it moves to the next hypothesis. It's not testing fork and knife at the same time; it's selecting the most likely hypothesis, such as the knife, and testing that. If it could also be the spoon, but the knife is more likely, it tests the knife based on what is most different from a spoon.

Mentally, it feels like if I'm touching something ambiguous, I consider, "It could be X," then I test X. If it's not X, I consider, "It could be Y," and test Y, rather than testing all possibilities at once or jumping around. I explore one hypothesis, then the next, until I find the correct one, which is consistent with what you're describing.

You used the second most likely hypothesis to inform how to test the first one.

It's actually the second most likely. How do you determine the second most likely?

It's just based on the evidence scores. If I have a whole set of cutlery, each with the same handle, they might all be equally likely—maybe I use forks more than knives, so some are more likely. You might compare it to any of them, but you are already certain it's not something unrelated, like your cat. There may not always be an obvious choice.

There may be a set of things that are equally likely at some point—all the cutlery has the same handle, and there may be four different types. There's no one that's more likely than the others, so I would just have to pick one. The system wouldn't have a problem with that; it would just pick one at random if they're equally likely.

This also works with the distant agent. The distant agent, because it's like a ball-and-socket, eye-like agent, moves around a point but can jump in space to look down at a different part of the object. After that, it jumps to look down at the handle.

Whenever it arrives at a particular location, it starts doing a random walk over that area.

Again, with the distant agent, it first tries to test one side of the object, such as the spoon, and then tests the other side.

You can plot some of the hotspots for testing based on this policy. As you'd expect, these are at the extreme edges of the objects, like rims and handles.

and again, with things like the fork, it's the two ends of the object. That was more qualitative, so I'll show a few quantitative results. This policy was implemented a while ago, but just to show the effect on the system: all these results are from learning and testing on all 77 objects in YCB, with noise in the data. For example, this shows the amount of location noise present. It's worth mentioning, as you asked earlier, Jeff, about the model being correct in its final output versus just getting the most likely hypothesis. 

If the true object is a mug in scenario one, the mug has the most evidence, so it's correct in its most likely hypothesis. It's only in scenario two, where there's a significant difference in evidence, that the system can converge. In scenario one, it hasn't converged yet. It should eventually, but it might not always. The reason I mention this is because when we report Monty results, sometimes we look at whether the most likely hypothesis was correct, and sometimes we look at whether it actually converged to the correct hypothesis. It should always converge eventually, but in experiments, we set a maximum number of steps to keep runtimes reasonable. Sometimes it just times out, and we say, "You didn't have enough certainty to make a classification." Even with lots of time, if the data is noisy and can't disambiguate, it might never fully converge. Often, just going with the most likely is good enough.

Imagine I have a cabinet full of coffee cups, and one has a chip—a little defect on the side. I know that one is a separate object. I reach in, and nine out of ten cups are the same. I pick one up and assume it's one of the nine. That's good enough. Maybe later, my finger brushes across the broken part, and I realize it's the broken one. My point is, I don't have to search through all my hypotheses all the time. At some point, you just say, "This is good enough," and go with it until you discover otherwise. That's the strength of this evidence-based approach: we can act at any point and use the most likely hypothesis. We don't need to 100 percent recognize the object; we can act on our current hypothesis. 

Right now, in this research stage, we want to get to scenario two as quickly as possible, which is why we want efficient policies that quickly reach high confidence about the object. Scenario two, in your example, would be moving to the location of the chip on your coffee cup to see if it's there. There's a scale here. Obviously, I don't want to confuse the coffee cup for the fork, but confusing it for another similar cup or a fork with a slightly bent tine is less critical. I don't want to spend all my time examining minutiae. Let's keep going until we discover otherwise, and when something unexpected happens, we notice. There's a spectrum of when scenario one is acceptable and when scenario two is needed. This isn't a criticism of scenario one—it's just a preamble to showing results from its accuracy when it's sufficient, and its accuracy when we require higher confidence. 

When people do benchmarks on object recognition, they might ask if you determined this particular version of an object, but maybe that's not always necessary. In a practical system, you wouldn't always need to go all the way to scenario two. For the objects shown here, you would, but for a fork or a slightly different object, you wouldn't necessarily have to.

With the distant agent, before this policy, it was essentially confined to its starting location, looking around in that space. We saw a significant improvement in accuracy—this is recognizing all 77 objects with noise. The two scenarios with the top-down policy: red is the baseline, moving randomly; blue is where it can also jump to desired locations. Here, there's a smaller improvement, and on the next slide, I'll show a bigger difference. In this case, we're not required to converge; we just need the correct most likely hypothesis. There's a reasonable improvement, but not a huge one, because often our hypothesis was correct, but we couldn't look on the other side of the object to confirm it.

In the second scenario, we have to converge for it to be considered correct, and here there's a much bigger improvement. It's very hard for the distant agent to converge when confined to one side of an object. Often, the disambiguating feature isn't present, so there was about a 45 percent increase in accuracy. With these model-based policies, the main measure we want to improve is the number of steps—quicker convergence. Ideally, we'd have good accuracy in both cases. This is more an artifact of the distant agent only getting one side of the object; if it can't jump, it can't disambiguate some objects. It does significantly speed up inference for the distant agent.

This shows the number of steps to convergence, where each point is an episode and the cross is the mean.

For the surface agent, the improvement was smaller because the surface agent was already able to fully explore the surface. When convergence wasn't required, there was only a four and a half percent improvement. Where convergence is needed, it's 14 percent, because the surface agent can move around the object, while the distant agent can't. Fourteen percent is still a significant improvement, especially considering typical benchmarks. The closer you get to 100 percent, the more you encounter ceiling effects. If you're already at 80 percent with the baseline, there's only a small improvement in speed for the surface agent. Some objects in YCB are very difficult to disambiguate, potentially only differing by color. If the surface agent doesn't have access to color, it will never converge, even if it can jump to different points.

This gives a sense of the quantitative effects of having that policy. In the YCB dataset, if you have two objects that only differ by color and the surface agent is like a finger, then there aren't two objects—it's just one object, as the agent can't tell the difference. This is recognized, but it's not currently reflected in how we measure accuracy on YCB. It would be nice to have that. It reminds me of auditory objects, like the sound something makes—a cup on a desk, for example. Many objects can make the same sound, so the auditory column wouldn't say, "That's a coffee cup" or "That's a phone" or "That's a plate." It would say, "That's a sound I recognize—a ceramic hitting a countertop." It doesn't know about the other classes, just the sound. Through voting, we can associate that sound with a particular object, but it's similar in that a touch object without color wouldn't know the difference between objects that only differ by color. It's not that it can't tell the difference; it's that they're the same. The visual system would have to distinguish between the two. When we do these tests, it's important to remember that it's not always appropriate for the system to differentiate objects that are undifferentiable in the particular modality. That's not a failure of the system.

There are some interesting cases—not necessarily failures, but quirks in how it works at the moment. For example, we don't currently use the absence of a sensation, or negative information. We haven't implemented a way to update the evidence for objects based on that, but it's in our pipeline. If we're on a part of the handle and move to empty space, expecting the spoon or knife to be oriented that way, and don't see anything, the system just moves back to where it was before and doesn't use that information. But that information tells you a lot about the orientation or identity of the object—the fact that you expected something to be there and it wasn't. This is just something we haven't implemented yet, but we think it's important. Instead of calling it a failure, it's just work to be done.

The word "failure" seems harsh. It's not a failure if it doesn't recognize the sound in an object because we don't have an auditory sensor. I've mentioned that eventually we want to do this more in feature space, calculating distance not just as Euclidean distance between points. In some instances, it might say one handle is more different than another, focusing on a part of the object rather than, for example, the head of the fork versus the head of the spoon, even though those are the more interesting parts.

In terms of policy efficiency, often the same locations are visited, which can be inefficient. You can imagine model-based ways to avoid revisiting the same locations repeatedly.

Personally, when I touch something, I often do exactly that—touch the same things over and over. If I'm holding a pen, I'll move my finger over the same parts repeatedly, playing with the clip. I'm not sure why the brain does that, but it seems like my cortex is just testing the same thing over and over. I'm not consciously thinking about it, but I'll be fidgeting with the pen, touching or twirling it the same way. Maybe the cortex doesn't want to do nothing and just keeps doing this. I'm not actively testing or consciously checking, "Is this still the pen?" Maybe that's what the columns are doing. I don't know if it's meaningful, but it's an observation. How else would you learn behavior and changes? But once I know it, why do I keep doing it? Some pens have a springy clip, and if I have one, I'll often stick my finger under the edge and push it up and down, constantly.

It's clear the need for that is obvious in adversarial situations. When you have an opponent working against you, you always want to verify that things are the way you think they are, because through actions not of your own, they will change in surprising ways. That makes sense, but it doesn't really make sense in the case of the pen. I've been holding this pen for 20 minutes; I've held it every day. Yuri Ivanovich says it's the same mechanism for all these things, so maybe it's just a degenerate case of that mechanism. Maybe there's a mechanism sitting there going, "I have nothing else to do, so let's keep testing this." You basically see that there's a subtle reward involved in it. In an extreme case, you would have OCD, where you keep repeating the same actions altogether. There's some pleasure in it, some pleasure in verifying that this is the pen that I know, or just the tactile sensation is a reward in itself. You don't have to be goal-oriented for everything.

It does feel like when I fidget, it's not just doing the same thing over and over again randomly. I do the same behavior repeatedly. There is a certain pattern to fidgeting—it's often mechanical things. I don't know if it has some relationship to reward associated with tool making or something like that, like snapping a stick. It feels more similar to that than to any possible random thing, like touching a flat surface. There is something salient about it. I'm fidgeting with the salient component.

If I just sit and talk to someone across the table, we know that the eyes are continually saccading between the eyes, nose, mouth, and ears—three times a second, for 10 minutes, as long as you're looking at them. That's basically just looking at the same features over and over again; maybe they're changing. I don't think it's worth a lot of time on this, but it's interesting to point out that there is some sort of reward for verifying this hypothesis, and you just keep doing it. It could become OCD at some point. Some people have these kinds of tics when they're trying to concentrate on something. Sometimes they listen to certain music and the rhythmic patterns in the background, or some repetitive motions, trigger something in the process of concentration that is reassuring or reestablishing something.

We're getting pretty far afield here. I just want to point out that I observe myself moving my finger all the time over the same things. But I agree, we don't want to change this in terms of what you're talking about here.

In terms of people who are actually using the policy, just be aware that at the moment it's implemented in a way that the agent kind of teleports to the location it wants to go to. The analogy is that we have a higher-level policy goal state of being in a particular location in the environment, which gets passed down to more subcortical structures that would actually initiate the movement through space—simple walking or whatever is necessary to get there. To avoid having to implement and coordinate that with the realities of the environment, at the moment, we just move there instantaneously.

The cortex, if you think about its output, I always imagine the inputs are movement vectors: you go in this direction, at this speed, for this amount of time. The output would probably be something similar. The output would say, "I want to get to some location; I should move in this direction, at some speed and time," but it's not actually doing it—it's just sending that signal to somebody else who's going to try to do it. For the eyes, it's sending a signal to the pericalliculus, which actually knows how to do those things. So it's basically saying, "Go here," and letting somebody else take care of it. There's nothing embarrassing about that at all; that's what we should be doing.

I think that will tie into the second part, the goal states. We can talk about that later—how output goal states and then the subcortical areas can take care of executing and getting to those states.

And then just the final point on the local graph mismatch being used is how, when we bring in hierarchy, this can be much more efficient. It's still the same idea: you have learned models aligned, most likely poses, and then you use distance in feature space to determine where to test. This is not currently implemented, but we could add it quite easily once we have our scene-level dataset fully implemented, using Rami's SDRs. Another alternative that might make it more plausible—easier to imagine how the brain would do it and possibly more efficient—is just storing these hotspots you showed earlier in the model of the object. For example, for a face, we would just store the eyes, nose, mouth—those kinds of hotspots. You could calculate the hotspots a couple of times from the graph mismatch, or infer them whenever you make a movement that gives you a lot of evidence or helps you recognize an object. Then you add that as a high-salience point in the model. As Jeff was alluding to earlier, when you're testing an object, you don't have to be as deliberate as in a hard task—like at a dinner party where you have to test what's in a cardboard box and really think about the possibilities. In most day-to-day life, you're probably using a simpler heuristic, like "I tend to feel here, and that tells me what it is." This requires some cognitive resources to figure out where to go to best distinguish two objects, and you need the whole model of the objects to calculate that. But just having some hotspots in the object model that are good points to recognize the object would be a much more efficient way to have a general policy for each object. Having minimal distinguishable features—a set of those—might be enough.

One thing we have to bear in mind: if you go back to the picture of that one there, the whole feature—you're going to look for this feature, the knife end as a feature. That's the idea here, the hierarchy. In this example, with hierarchy, you could use fewer points to represent objects, making it much more efficient because we're not comparing thousands of points. The key to solving the problem of hierarchical compositional structure is to say that a feature doesn't have a single location—like the hammerhead doesn't have a location. It has many locations, all part of the hammerhead or that feature. We do it on a location-by-location basis. This is in the paper we're writing: there isn't a location for the hammerhead. You could go to any point of the hammerhead. As long as you know which point of the hammerhead you're at, you can say it's supposed to be a hammerhead here, and you happen to be at this location. But you'd still test it. We don't want to get in the habit of thinking a feature has a location in space—it has many locations in space. A composite object built of other objects has no specific location; objects are located on a point-by-point basis, relative to other objects. That's a good point in terms of how we represent it. We don't want a single point—"that's where the hammerhead is." This is the big lesson, but we can still use fewer points; we don't need to cover the whole thing.

If I was moving from the handle up to the end to see if it's a fork or a spoon, it doesn't matter exactly where I land on the fork or spoon part. As long as I know the location and what I'm expecting to see, I don't have to go to the same location—any location on the spoon part or any location on the fork part will work. It's a subtle but important point to keep in mind.

On the distance in feature space: for example, we're representing object features coming in from the lower level in the hierarchy, with something like SDRs where there's some overlap based on the similarity of the objects that have been learned, similar to Rami's work where you saw clustering. In SDR space, this mug is very far away from the spoon, but with hierarchy, you might have the concept that the two handles are nearby in SDR space, but the two heads are distant. I still feel uncomfortable with the SDR overlaps—maybe it's like Einstein being uncomfortable with quantum mechanics. He couldn't accept it and spent the rest of his life trying to prove it wasn't right. Maybe I'm like that—I've just gotten too old, I can't get it, but I still want to raise it. I don't have an alternate solution, so I'll have to think about it.

We're going forward with it, so it's being baked into the system. We're baking a lot of things into the system while they work. This is a pretty fundamental idea—what is the nature of representation? That's pretty basic. I'm not going to stop him. If it works, it works. I'll just be a curmudgeon.

In terms of multiple objects, I'll briefly touch on this. We have a move-back-on-object policy that uses information about the most likely hypotheses and the learning module's position on the object, making it model-based in that sense. In our multi-object setting, you have a target object, like a pink Lego block, and other objects placed around it in space. The system saccades over the objects and may move onto others. The basic principle is that as you move over the actual object—say, the potted meat can—the evidence for that object builds over time. If you suddenly move onto another object, you'll get observations that aren't consistent, resulting in a steep decline in evidence. On the x-axis, the lines indicate the ground truth of what the sensor is looking at; here, it's the potted meat can, but then it starts looking at other objects.

The learning module can internally detect a change in evidence and recognize that it's now sensing something very different from before, then initiate an action to move back to where it was. The question is, what should the goal be? Should it keep going back to the meat can, or should it recognize that it's now on a second object and start building a composition? We can also do that with hierarchy, but here the assumption is that we want to recognize when we've switched objects. In the higher-level learning module, it will represent that at the previous location there was one object, and now we're looking at something different. There are instances where we want to figure out what we're looking at before moving on to other things. If it's a learning module, you can't learn hierarchy—there's no concept of where this object is relative to another. There are two objectives: this plot shows detecting when we've left the object we were previously on. We have two choices: either reset the evidence and start detecting a new object, or invoke the policy Niels mentioned and go back to the previous object to continue recognizing it.

This is shown in action: we start on the golf ball, do a few saccades, get on the mustard bottle, and then immediately move back because we realize it's something different.

This approach gives about a 10 percent accuracy improvement when moving back to the object in settings with distractor objects on either side.

That brings me to model-based policy in abstract spaces. In general, this is speculative, but there are many instances of model-based abstract tasks. They're not directly analogous to classification, but similar principles apply, like mentally rotating and aligning different representations and focusing on differences and similarities, especially the differences. For example, you might have a taxonomy for classifying species—one based on DNA analysis, another on fossil records. These are two different taxonomies, and if you align them in representational space, some parts of the tree structure will be the same regardless of the system, while other regions will differ. If you know one system well and are learning the other, you want to focus on the differences to understand it.

Similarly, when reading a paper on a machine learning technique, the introductions are similar, and you might jump to the early figure showing the main technique in abstract form. These don't have a direct analogy in physical space; it's more of an abstract space representing the structure of the paper. When scrutinizing the algorithm, you're operating in a mathematical space, aligning similarities, superimposing them, and then focusing on the key differences.

That covers abstract space. Next, I'll briefly talk about hierarchy.

We're switching to hierarchy, focusing on interacting with the world as opposed to inference, and bringing in hierarchy for policy and goal-oriented behavior.

For most interesting tasks where we want to change the state of the world, some degree of hierarchy is needed. We discussed hierarchy in inference, but now we're talking about hierarchy in goal-oriented behavior. This is speculative—ideas we've brainstormed but don't have concrete proposals for. For example, making coffee can be broken down into hierarchical spaces: day planning, kitchen space, coffee machine space, and within each, specific actions that can be further decomposed.

The approach we're considering is to have goal states that a learning module can send. It can send these to a low-level motor system, which would initiate a model-free policy—essentially sending tasks to subcortical structures for simple actions—or to another learning module, which would be a hierarchical model-based policy.

and with the latter, you can break down complex actions into goal states that can be further decomposed by lower-level object models.

Going back to the cortical anatomy, I mentioned the direct motor projections, which go from L5. L5 also receives top-down feedback via the apical dendrites in L1, and there's connectivity between L5 and L6. There may be a circuit that allows a higher-level cortical column to signal the initiation of a particular type of goal state. Where appropriate, a lower-level learning module can then output its requirements to the subcortical structure, which, as you were saying, Jeff, is the only actual output in the cortex and the only way it can change the state of the world.

This is a fascinating area. I'll let you finish, because I'm assuming we don't really understand this very well. The classic motor output goes up the hierarchy along the red lines here, which at first made no sense to me. In the coffee making example, you want to decompose actions going down the hierarchy, but the purple line is your feedback. If we're decomposing things going down the hierarchy, it's got to be the purple line. Maybe it's like layer six—maybe region three says this is the location I want to be at, and the column below it says, okay, that's where you want to be, I'll help you get there. Then it says, I want to be there. In the way we're discussing it in the paper we're working on, these three columns are co-aligned, so we're all looking at the same location in space, but that's not always the case. It's very confusing. These are some of the parameters we have to deal with, but I feel we can solve this problem. I think there's a really clean answer to that. Maybe you've got something else to present.

No, not really. The only thing I was going to say, in terms of this picture, is that it might give the idea that we want to have a direct motor connection back to layer five, where a higher-level column tells the lower-level column exactly how to act. Actually, it just tells the lower-level column the state it wants to be in, and then the lower-level column uses its models to figure out how to get to that state. We might not need that pink direct connection; it might be going through layer four, layer six, and whatever else is there to figure out. That's correct. If you watch the purple line, the major connection from the purple line is up in layer one. It's basically saying to a whole bunch of other columns below it, this is the state we want to be in. In some sense, you all want to be in this location, or I don't know what the state is yet, but you want to be in some state. The little synapses in layer 6A are more for the columns that are actually co-aligned. You're right, the upper region isn't telling the lower region what to do; it's saying this is the state we want to be in. It says, okay, I'll try to implement that state for me, and then it tells the next column below, that's the state I'm trying to be in, and so on. But it's still really confusing, because all these have a motor output.

One of the questions I've always had: we label these two red lines as motor output, but are they equivalent, or is one hierarchically higher than the other? Do they both go to the same motor system, or do they go to some hierarchical representation subcortically of the motor system? If they were both going to the superior colliculus, would one be implementing a higher-level superior colliculus function and the other a lower-level function? I don't know. You can imagine this might be the scene-level representation of the kitchen, and you're trying to make coffee. It sends the goal state of having the coffee machine on to a learning module that knows about coffee machines. Maybe this direct motor output is more like moving through space, as in walking, and this one will get you towards the coffee machine, while another will direct your arm to actually turn it on. There are a lot of things in flow here, and obviously a lot of complications.

This is one of the things that bothered me for decades: how do we understand decomposition of motor behaviors, given that the cortex is the sensorimotor system and its output? I never felt I had enough information to answer this question, but now, with the idea that hierarchical composition is a point-by-point compositional structure—not always object-to-object—I feel like the new insight about columns and being able to represent the same space location in two different columns hierarchically arranged is the clue that will help us unravel this problem. I feel confident that we now have enough information to come up with a very solid, compositional, hierarchical, goal-directed, or model-directed theory of motor behavior. We have a long way to go, but I don't think it's going to take that long. We have a lot of things to solve, but I think we have the information we need now.

In terms of non-aligned receptive fields, a natural idea is that with this kind of decomposition of hierarchical policies or goal states, you would recruit the motor cortex where necessary for more complex motor actions. These are essentially just learning modules specialized in coordinating the movement of a body part. How those anatomical connections look and how exactly you coordinate that is obviously complex. It's something we can work on, and maybe it's something we can solve when we get together in January. I feel like this would be a perfect subject for the brainstorming week. It would be a great topic to think about beforehand and really try to spend a few days working on it. We made progress on the whole hierarchical composition—Viviane and I worked on that a while back—and maybe we can do the same thing here.

Bringing this together, this is the overview diagram you've seen, where the outside world—with a mug on a table, a hand, and an eye—sends input to the sensory modules. There are motor modules and learning modules that model objects. The red lines represent motor projections, and model-free policies generally exist in the domain of a direct sensorimotor loop, not significantly involving the cortical areas or learning modules. It's worth noting that the input to a sensor module and the output from a motor module are not compliant with the cortical messaging protocol, but all the other solid lines throughout the system include projected goal states. 

A model-based policy involves a learning module coming up with an action to send to the more subcortical motor system. A hierarchical model-based policy involves learning modules interacting together through top-down connections.

On the last slide, in terms of future work, the quick thing we've wanted to do for a long time is to develop better model-free policies for the distant agents, focusing on cadence, sailing features, and medium features. The grayed-out areas represent more exploration-focused policies. We've focused a lot on inference, and those wouldn't be too difficult to implement. The more complex task, which is also grayed out, is hierarchical policies for multi-step goals, like setting the dinner table or making coffee.

As you said, that might be a good topic for the coming sessions. You might solve it before then, but in my experience, you have to think about these things for a long time before they come together. That was a good summary of everything. Thanks, Niels. That's pretty cool. We have a sensorimotor system, and we need to figure out the motor part, so this is where we are.

Now, if I could ask one question, it has to do with hypotheses and estimating pose. As I understand it, the current hypothesis system is about object identity. Let's say I've figured out that I'm on a cup. It also tests for pose. I tried to show that, but maybe I didn't make it clear. Do we have that in the code today? Did we build that? I can show you a specific slide—this one, about disambiguating the pose. In this instance, we know it's a spoon, but we don't know the orientation. It first tries, so it has two possibilities, but in this case, to say it was a spoon, it already had to have the orientation, which could be off by 180 degrees. 

The more general question is, what if the spoon was rotated 90 degrees? How did you recognize it, or how did you come up with the hypothesis? We don't know for certain yet that it is a spoon, but it's much more likely than the other objects. For example, we've already sensed the top of the spoon, and that area isn't consistent with any other object we have. It's not clear to me how you decided on the particular orientation when you started. I have no idea what the orientation is. Here, it's felt the handle. I don't remember exactly how I got this figure; it might not be the most realistic one. If it's only felt the handle, how does it know it's a spoon? To Viviane's point, how did it even get to that testing orientation? If it's felt the handle, it knows the handle is oriented like that, but the handle could be reflected at that point. What if the handle is not introduced to that? The spoon couldn't be oriented like this, because it's felt all along here, so it knows the handle has that orientation. That orientation is consistent with this orientation. When you start moving, you have one point of observation, and you don't know the orientation. This isn't after one point; it's after multiple points. 

At the moment, it already starts with many hypotheses for the possible orientations, if that's what you mean. It starts with an exhaustive set. How did it do that? I think that was the question. Sorry, Scott. Can you make a hypothesis-driven jump? Let's say we already know we're on a spoon. Can you make a hypothesis-driven jump just to resolve the pose? Yes, that's a great question, and the answer is yes. That's what it's trying to do here. In this case, it's making a hypothesis-driven jump to this red location to determine if the spoon is oriented like that or like this. Here, it's trying to disambiguate a more subtle difference in the pose—is the spoon like this, or like this? It's jumping here, and that's a good question.