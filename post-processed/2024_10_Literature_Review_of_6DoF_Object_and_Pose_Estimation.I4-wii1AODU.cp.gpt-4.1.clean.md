Thanks a lot, Scott and Hojay, for volunteering to present today on such short notice.

The two of them have been reviewing the literature over the past few days, looking at other approaches people have taken for object and pose recognition tasks similar to what Monty is currently doing. This gives us some baselines to compare against when we write up a paper about Monty's capabilities.

We can talk about that now, and if there's time left, we can address some questions people have sent in, or save those for next week.

I'll hand it over to you two. Sounds good. We'll put together some slides that primarily cover state-of-the-art methods for pose estimation, and I'll explain why we're focusing on this first.

About a week ago, Scott, Vivian, Nils, and I had a research brainstorming meeting on how to demonstrate capabilities for a paper we can write up by the end of the year. The starting point was broad objectives like demonstrating rapid learning for computer vision, whether that's from the number of images or the total data collected by Monty, rapid inference with mobile sensors, improved and robust recognition, and multimodal transfer. We came up with additional things we might be able to show, but I don't think these are current or fully implemented in Monty yet. Other ideas suggested were generative or predictive reconstruction of input—so if we're seeing part of an object, Monty could predict the rest—and continual learning.

Can I just comment on that? Predicting the rest is the right thing to do. Reconstructing doesn't seem right; there's no place in the brain where images are reconstructed.

We're not good at that, but we are good at predicting what will be someplace and seeing that. I don't think our system will be good at generative models. Humans aren't good at that. The predictive ability of Monty should be really good. It should be good at using its models to predict, but if it's supposed to be photorealistic generation, for example, we'd need to train a decoder on top of that, which might also be pretty good because it would have better information for photorealistic reconstruction. It wouldn't be too difficult to add as another readout layer, but it's definitely not what Monty is. That's a good way to put it. We don't have to give up on the idea completely. It could be a flashy demo because people are really into generative AI right now, but it's not really the focus. We have to be careful not to fall into the trap of trying to do what deep learning models do, trying to match them, because we probably won't. I like the idea as an add-on, but I would make it clear that brains and Monty are not generative systems per se, certainly not for images. Humans aren't good at this. We can't draw things or picture things with detail in our heads. As an add-on, it could feed into another system that does generative modeling. That should be good. I don't think we'll include this part in the paper we're planning.

On the second part, continual learning, I would rename this to unsupervised learning because we're already doing continual learning all the time. For example, point A is all continual learning: you see the object from one angle, build a model, see it from another angle, add points to the model, see a new object, and learn about it without forgetting the previous one. What we call learning from scratch in the benchmarks—the big difference compared to the current default setup—is that we don't provide any labels to the system, and we don't make assumptions about the order of the data presented. That's something different from deep learning. We can present the objects in any order. We can learn one object from all angles first, then the next one, or move your finger over an object in one direction to learn a model, then in another direction to learn a model. It's really a motor-based system.

Is there a reason that A and C are bold? Yes, because from these four milestones, the reason we went deep into pose estimation was to find a common basis to compare Monty against. We wanted a task that people in the deep learning world understand that Monty can also do, and that task is highly related to robust recognition.

That's why it's bolded.

I tried to put this in the deep learning lingo of current Monty capabilities. With what's currently implemented, we can do object recognition and six degrees of freedom pose estimation. Some future capabilities may include 3D reconstruction—again, just use the word prediction instead of reconstruction—and possibly robotic grasping and manipulation, semantic and instance segmentation. These are not yet implemented, but may be compared against in the future.

Because pose estimation subsumes object recognition or often involves it, many models that do pose estimation can already do recognition or sometimes segmentation, depending on implementation. That's why I focused on existing models for six degrees of freedom pose estimation. That's how we went from the milestones to this specific task, rather than just doing a random literature review beyond pose estimation.

I think what we need to do is lay out the basic capabilities of a sensorimotor system and then show that we're tackling each component, rather than aiming to be best in class. I'm a little concerned about the focus on robotic grasping and manipulation, which is a very robotics-centric perspective. What we really need is a motor output theory for Monty—a higher-level theory about how columns and learning modules can move their sensors logically to achieve goals. That may or may not involve robotic grasping; it could be many things. We don't want to focus on that yet. That should come after we understand a broader theory of motor generation.

We shouldn't focus on that particular type of task right now. This is a future capability, and those three areas won't be included in this work. I'm just trying to correct potential misconceptions about these topics. Eventually, motor behavior will be one of the prime applications of Monty, so we need to generate a general theory of motor behavior in a sensorimotor learning system. Then we can implement that theory in different ways, such as robotic grasping and manipulation, but we shouldn't start there. We need the general theory first.

For now, our focus is on object recognition and pose estimation. We're outlining that. To give more context, we're not saying these papers do exactly what we're doing. As far as I know, nothing does exactly what we're doing. We're writing a paper to show results from Monty, and it's good to have something to compare to, to show Monty's strengths. Many of these systems will need more data or have other drawbacks that Monty addresses better. We want to know what's out there and what we can compare to, at least on some dimensions, but there won't be any perfect comparison.

Pose estimation can be approached in different ways, and for each, we can try to relate it back to Monty and compare on similar dimensions, as Vivian mentioned. The six degrees of freedom refer to the pose of the object relative to the sensor, or vice versa—those are two sides of the same coin.

When you say pose estimation, there are actually two types. The more common one is human pose estimation, which is important for applications like preventing cars from hitting people. Another category is object pose estimation, which is generally about estimating six variables: translations along X, Y, Z, and rotations along X, Y, Z, with respect to the camera, Monty agent, or sensor. I first looked into datasets, and after discussions, I think there are categories we prefer. Our top preference is datasets containing 3D meshes, so Monty can interact with them. The next preference is datasets with point clouds but not meshes; from point clouds, we might be able to generate meshes, but it's an extra step. Most datasets are actually RGB or RGBD images, which aren't really compatible with Monty. Monty would need more preprocessing, like extracting point clouds from RGBD and then reconstructing the mesh. In general, Monty could learn from RGBD images, but you'd need to know the movement between two images. If we had the exact pose of the camera for each image, we could use that. We wouldn't have active movement determined by Monty, but we could still learn from it.

To clarify, when we reference 3D meshes, we're not giving the meshes directly to Monty. We're using them to render environments that the sensors can perceive. For example, we could also learn in the real world, so we only need meshes for simulation, to render the environment. Monty could also use an actual sensor to sense real objects.

There are plenty of 3D mesh datasets available. The three most common are LineMod, YCB Video (which is similar but not the same as the YCB in Habitat), and TLS, which stands for textureless objects. YCB Video is used in many models, possibly because it emphasizes pose tracking over time. Many papers benchmark against YCB Video rather than regular YCB. LineMod is the original, longest-standing benchmark dataset, and TLS is often used when there aren't significant features—textureless objects like a baking sheet, which is just flat and gray. For deep learning models, these are harder for pose estimation. 

Why wouldn't there be an edge on a baking sheet? For most of the image, it's just a large, flat, gray pan, so there's not much change. But if you run your fingers along the baking sheet, you feel an edge—that's captured in the 3D mesh.

This is maybe closer to our idea of a morphology class of objects, where you're not paying attention to other features, just the surface morphology. Would that be correct? Monty would do fine on TLS. Deep learning models typically do not perform well on textureless objects. That could be interesting to test and compare. In some sense, it's getting at one of the core capabilities we think brains have. In some modalities, maybe all, you're able to detect the morphology of objects even as features disappear.

It seems like this would be testing that exactly.

It's like vision without color, or touch without texture or heat.

I won't go into all of that, but there are more datasets, some already implemented, like ShapeNet and ModelNet. From what I see, they don't have pose annotations. I'm not sure how it was used in Monty before, but they don't need pose annotations. We just initialize them in an environment, like a 3D rendering engine, and set how we want to initialize them.

People have summarized the different datasets that exist. We need to dig into them. I'll look into how pose estimation is done. The good news is that there is no existing model that I or Scott could find that does pose estimation like Monty. I'm thinking of it as a model or agent that can explore, touch, or sense something and then try to estimate its pose. I did find some reinforcement learning-based models, but these are still not the same. In the brain and in Monty, you don't do pose estimation independent of object recognition. Those things happen at the same time. As you explore, three things are resolved: what the object is, where you are on the object, and the pose of the sensor relative to the object. You can't really separate those three. When we talk about pose estimation in a Monty setting, we also have to talk about inferring the underlying object. You can't estimate pose unless you know what the object is and where you are on it. Does that make sense?

There are several ways existing models also do object recognition. These problems are linked.

There's no way to do pose estimation unless you know what features you're sensing. The reinforcement learning methods I found—how are they different? Why don't they learn through moving along the object? At least, from what I found, they use reinforcement learning for inference, not for the sensing part.

I'll get to that when I talk about iterative refinement. There are four different methods for pose estimation: regression, keypoint detection, template matching, and iterative refinement. Direct regression is the brute force approach: you feed in RGB or RGBD data and try to regress the six parameters that define the pose.

This is the most deep learning brute force way of doing it. The black box learns a function from RGBD to the six parameters. In terms of comparison to Monty, this is just a thought, but maybe we could extract color in HSV from Monty and feed that into deep learning models for training. I'm not sure if deep learning models can learn rotation and pose with such a small number of patch data, but it's a way to try to put them into the same dimension for comparison.

The keypoint method is where deep learning, instead of directly predicting the pose, tries to extract interesting points like edges or places where features change. It then matches those keypoints to an existing 3D model. This requires a 3D CAD model. If you have the 2D and 3D keypoints matched, there are algorithms, like Perspective Endpoint, that can solve for the pose. This seems closer to what we're doing with Monty. In Monty, you don't sense the whole object at once. You'd have a learning module that moves over and detects different points on the object, and through that sequence, infers the object and pose. That seems similar to what they're doing here: creating a series of points that help define the pose and the object itself. Is that correct? The difference is that this system gets the entire image at once and uses it to detect keypoints, whereas we get a small patch that moves along the object and uses whatever points it sees. But it's similar in that we also have 3D models in the background to match the sensed points to. We take a series of points until we have enough to infer the object, pose, and location on the object.

This also reminds me of SLAM, which is a different scenario where you're trying to locate yourself, not an object, in the world, but they use keypoints a lot. You can use SLAM to map an object in the environment and compare that to Monty. SLAM is conceptually very biological—it's something brains do. The details may differ, but it's the closest existing research area to what Monty is doing.

How about training those models? Do they just get point clouds or meshes directly, or can you actually get them to learn from observations? I think they use a sensor, like a LIDAR sensor, for the learning procedure. For key point detections, these work pretty well, but they don't do well in occlusions. If you have occlusions in the image, it can't extract the key point where the object is hidden. Textless objects—meaning objects without interesting features—won't have enough key points to match between 2D and 3D, and symmetric objects are also a challenge. One way is to use this as is, but in settings with occlusions, textless objects, or similar issues, we can show robust recognition compared to existing key point detection methods.

In this case, they process the entire image at once. How does it do segmentation? Does it first assign these key points to the different objects on the table? I think it depends. Some literature assumes a world with only a single object in scenes, which is simpler and doesn't require segmentation. Others try to do some segmentation first. Many models do segmentation as part of key point detection.

It's a mystery—you go from the third image to the fourth image, but how did that happen?

Another method is template matching, which I thought was similar to Monty's graph matching. In this approach, you need 3D CAD models. From the model, you take 2D projections at various angles and try to match the input image to the 2D projections of the 3D model. For example, if I have an object, I can take a bunch of pictures at different angles or poses, then compare two images. Once there's a good match, you know from which image it matched and what the pose of that object was. This feels like graph matching in the sense that we show the object in different poses and then recognize it in any pose, or when we see a single patch and later try to construct the object, we need to match the pose of the object in memory. The object in memory doesn't really have a pose; we just have a hypothesis of how the object we're sensing is rotated. We use that to rotate our input and then match it to the 3D model in memory. It's not really multiple templates—it's just one three-dimensional model that we have. In that sense, we have to avoid thinking along those lines.

Is this method able to interpolate? If the object is in a new pose not in the templates, can it recognize that pose, or will it just match to the nearest template? That's a good question.

This seems less similar to Monty than the previous key feature method.

Do you know how it's trained? It's trained on the CAD model plus a bunch of views, right? The nice thing is that if we wanted to train one of these models, we can generate all those observations from different angles in the habitat, and we have the models. In this method, there is a longer training procedure involved. It's not just having 2D views of the object and doing a kernel operation over the image to see if it matches anywhere.

Do they actually train on thousands of images before this works, or can this work out of the box with just a few images? This method definitely involves segmentation because the models would exist without any background or scene. One key aspect is to do segmentation, remove the background, and try to learn to minimize whatever kind of difference function you set. For example, the models might not have the colors on them, so it still needs to learn to minimize a loss function, whether it's a structured loss or something else, to say, "This 2D image matches this point of view of a 3D model."

Is this still a state-of-the-art model? I think this one takes a long time because there can be so many poses the object can be in. I don't think the SOTAs you found are template matching based. Maybe this multi-view convolutional neural network, but that's also an older one—almost 10 years old now. I don't know if that would be considered state of the art. Probably not.

The final way to do pose estimation is iterative refinement. This starts with an initial hypothesis of what the pose could be. This requires an initial pose hypothesis, which can come from direct regression or any previous methods, and then tries to rotate the target or object until they match. These are all parallel vision methods—they're not really sensorimotor systems. It's not Monty-like. There isn't a sensorimotor model like that. That's our strength, actually.

Let's leave it at that. Are we looking at these different methods for pose estimation as ways we might implement them in Monty, as ways for testing Monty, or as ways for comparing Monty to other systems? It's just for the last one. We're not thinking of using any of these methods to implement in Monty. We want to show the capabilities of Monty, and to do that, we should compare it to the current state of the art in the field. I think Monty is already doing better, so we don't need these methods, but we need to find some common ground to compare them. That's why I was looking into the different methods and what common grounds we can use for comparison. The reinforcement learning methods pose the problem as navigating through a hypothesis space. Instead of trying out many different rotations and randomly matching, the RL method learns a better action policy to rotate or move the object in a more computationally efficient manner.

They're not actually navigating or sensing by themselves; it's just a method to speed things up. After template matching, this is usually the slowest step. Often, in pose estimation, there's a big goal to make it fast—real time—for applications like autonomous cars. For those applications, iterative refinement isn't used because, while it improves accuracy, it comes at a huge computational cost. That's why some of these RL-based or other methods exist: to make this process shorter, since we want both real-time tracking and high accuracy. In this method, is there a model of the object?

It's not clear. I think they just give the raw ground truth CAD models and then put that pose into the renderer. You would have some initial pose for your object, like a point cloud or mesh. For example, one is flat and there's an object that's flat on the table, and we have an object in the CAD model that's upright. The object on the table will try to move around until it matches the orientation. The CAD model has a preferred pose, and it tries to find the relative pose between the CAD model and your existing object. I understand your question, Jeff—where does the image go in here? Where are we trying to detect the pose from? In this case, the input is usually a point cloud, which comes from poses estimated from images. There's another preprocessing step because we need some kind of initial guess, which often involves direct regression. It often comes from an image, where we have an initial pose estimate, and then we use that to convert the 2D to 3D. From there, we try to rotate the 3D object to match the CAD model.

This is a little confusing to me because unless it's doing all those CAD models in parallel, it seems to do them one at a time.

From the initial step, there will be an object that was identified, so we already know what the object is. Looking at the flow diagram, there's nothing that explains what the model of the object is. It's just initial pose, change the pose, initial pose again. I don't need to get into the details unless it's something we're really going to rely on, but I think this one probably isn't.

I have a question. My Zoom was cutting in and out, so I missed part of the discussion. Did you cover RANSAC methods for pose estimation? Yes, I didn't say it here, but during perspective-n-points, it's RANSAC to generate possible pose hypotheses. I hadn't seen that one particular image where they were going from features to pose. I copied these diagrams from a review paper.

Thank you. Which of these methods would you say is the best state of the art at the moment? For me, it's keypoint. So you said there was nothing out there that uses the same kind of inference regime where you have a sensor that moves over the object, right? Yes.

Let me ask a higher-level question. In Monty, in the brain, we have learning modules. Each learning module can do this independently of all the others, but they also vote on this as well. That's why we can do flash inference.

Are we going to try to demonstrate both of those initially, or just show how a single learning module does this? Both. The voting is one of the capabilities we want to demonstrate. Rapid inference with multiple sensors. That includes multiple vision patches or multiple sensors. That's where I got confused, because maybe if it said multiple sensor patches, that would be clearer. I took that to mean, and I think most people would take that to mean, vision and touch or vision and hearing or something. That would be multimodal transfer. Maybe we could just change it to say multiple sensor patches. I also confused that, Jeff, when I was first going through it. It is a general rule here.

Monty works very differently than almost every other kind of learning system, and we need to demonstrate progress on the components of how Monty works. That's more important than showing that Monty is better than other systems. Competing on benchmarks is a losing game initially; we don't want to go there. We want to highlight Monty's unique capabilities and show how they work. It's about illustrating and teaching people how Monty and the brain actually do these things, which is more important than saying we're better at a particular benchmark. To me, it's more important to explain how a single learning module works, how modules vote to reach consensus more quickly, and to demonstrate that. I'm not against comparisons, but we have to be careful. If you compare to other systems and you're not better, the machine learning world will ignore you. They only care about benchmark results, not how you achieve them. We don't want to give people that impression. We want them to see that Monty works differently and that it's impressive in its own right.

I think we would outperform other approaches if they had the same amount of training data as Monty; they would probably fail completely. Since you didn't find anything that works similarly to how Monty does inference, maybe it makes more sense for us to introduce new benchmarks. We could try to train a reinforcement learning agent or a transformer model on the same data, but then just put it out there and say, "Humans can do this, our system is very good at it, try to solve it in a better way."

You could do that as a demo.

That would intrigue people about how we're able to accomplish it, rather than comparing against a thousand-object library or stacking up against something else. If the demo illustrates Monty's unique features, it could serve as a teaser to get people more interested.

The demo could also function as a benchmark—not just something cool, but a challenge to solve the task better than we do. I wouldn't want to make that the main challenge. What we want to do is show how the system works. That's the most important thing. How does the system work? A single learning module can do inference over time and movement. Multiple modules can vote to do this more rapidly, and we can characterize that. Maybe we do some capacity or performance testing for ourselves first, and after that, compare to other systems.

The real goal isn't to get people to say, "This is better than system X." The goal is to educate people that there's a whole different way of doing these things. Some people are already fans of the thousand brains theory, and that's our audience. The audience that believes deep learning can do everything isn't our target right now. We have to stay away from trying to convince them. As long as we focus on how Monty works, its capabilities, limits, and potential, that's what matters. If we can say it's better than other systems, that's fine, but it can't be the main focus, because someone will always try to do something different with another system. We don't want to get stuck in benchmark hell.

We also need a screen sharing session to show Monty's capabilities—not just how it performs, but how it's a different paradigm. It will be a while before Monty beats other systems at various benchmarks. Maybe I'm being too pessimistic, but I don't see that as our goal. Our goal is to continually explain that this is the way the brain does it, this is a different way, it's built on movement and sensation, and that's unique. Here are the components, and we're making good progress. If we can say it's performing at state of the art, that's okay, but it shouldn't be the main goal. I would characterize this as focusing on qualitative differences rather than quantitative ones.

We already have a lot written about how the system works, the principles, and our methods. I was planning to turn that into a white paper to put on arXiv soon. Once the project is open source, people can read about how it works. But people will ask what it can actually do and in what cases they would want to use it. We want to show that it can learn from very little data. If you have an application without internet-scale datasets, you might want to consider this.

That's the right approach. It's about unique capabilities and where they might be useful, not just performance benchmarks. We need to be careful about that. We don't need to compare to a bunch of other approaches. This is just an initial literature review to see what's out there and if there are similar approaches we should look at. It doesn't look like there's an approach that uses the same kind of data to infer from, so it might be difficult to do a fair comparison. It would be nice to take the exact same data we give to Monty and give it to a transformer or reinforcement learning agent, and see it fail because it's not enough data. That would be nice, but I don't think it's absolutely required. If we can do it, great.

Sounds good.

I think it's going to be hard to find a one-to-one comparison. Even if we trained a transformer with the data we collected from Habitat or 77 objects, we'd still need to feed that transformer a whole image, like what a viewfinder gets, unless we want to use architectures that can process image patches and try to detect an object from those patches. We could give it a series of image patches, plus the movement or the change in location and orientation over time. With transformers, I was thinking of patch views—the small patches Monty uses to recognize objects. It takes a thousand steps, resulting in a thousand patches, and the positional embedding can represent the difference in location. But in this case, we're designing our own transformer network to do this; these aren't preexisting models. To my knowledge, there aren't any prebuilt systems that work with these patches. If you look at vision transformers, they break the image up into patches as part of the training, with encoding of relative positions. It's not totally divorced from what we're talking about.

We just need to figure out how to get our patches. They realize an image by breaking it up, and each patch is an input that is encoded. You could think of it as serialized, but they try to keep some of the two-dimensional aspect. The transformer doesn't have to learn the whole serial-to-2D transformation.

They're using the word "patches," but it's somewhat different. It's a subdivided image, not free-floating patches, at least in conventional training. That doesn't mean you couldn't try to do something like that. I'm just trying to find some space between using only transformers and using Monty. Vision transformers have an intermediate aspect, which is not totally divorced from breaking the image up into patches.

Could you, in theory, simulate having looked at three image patches by masking out parts of the image? I think they tend to subdivide the image, but as long as you have some encoding of the relationships between them, you could have patches that overlap or vary in some other aspect. You could gradually move from a fully rectilinear, Cartesian orientation for the patches to something looser. At this point, I'm speculating because I can't point to a paper where that's been done. In vision systems, whether convolutional networks or vision transformers, there's a huge amount of redundancy because things move slowly across the image, as opposed to language, where things are less coherent. Within that space, if those systems work, you can start relaxing some of the hard constraints and see how well it does. There is a framework there if you want to draw an analogy or make comparisons; it's a starting point.

It's worth looking into, but I feel the best way is to keep the exact same training regime we have for Monty and then try our best to train a transformer model or a deep reinforcement learning agent on that same data. I wouldn't want to spend a lot of time on that—it could take forever, but maybe not. 

Can I ask a slightly different question? In mining today, do we already vote on pose across multiple learning modules?

Not really. We take the relative pose of the two sensors that are voting into account, but they are not communicating their hypotheses of the object pose to each other. Is that something we should be working on? Yes, it's on the to-do list—a very long to-do list.

One thing that occurred to me is that we want to highlight capabilities unique to the system, regardless of how they compare to others. Almost everyone thinks about vision as an image with patches on the retina next to each other, but we're not limited to that. You could have an image system with three cameras in different corners of the room, each attending to a patch in its visual space. As long as they're all looking at the same object from different directions, it works fine. People just don't think along those lines. It would be nice to demonstrate what happens when two fingers touch the object in different locations.

It would be nice to illustrate the benefits of a system that can handle sensors in different locations and unite them rapidly.

Are there any approaches that do object pose recognition from touch sensors? I didn't see anything like that. I can refine the search with that modality in mind. I'm not surprised it didn't come up, given how I was searching, but if it exists, I can look for it.

In neuroscience, they almost completely ignore somatosensory sensation because they have no idea how to handle it. The classic hierarchy of feature detectors model doesn't make any sense with fingers and skin.

People have avoided this topic. On the neuroscience side, I'm not aware of any theories that explain how we recognize things through touch. I want to point that out because I wouldn't be surprised if there were fewer studies in the machine learning world as well. People often say, "Oh, it's an image or these patches," but when you have fingers touching an object at different places and locations, what is it? What's going on? They don't know what to make of that. There's likely much less, if any, machine learning research on recognition from touch. It might actually be good for us to phrase it as recognizing the object through touch. As you said, people are used to thinking about the entire image in vision, and for them to understand that those are a bunch of small sensors moving across the image is going to be hard. We might lose a lot of people there, whereas saying we have three fingers moving on the object might be easier to understand. 

This has really confused AI, machine learning, and neuroscientists for years. The real breakthrough is understanding that patches of the retina are really no different than patches of your skin. It's something that most people just don't understand.

Our job is to educate them.

For touch data, the closest analogy I can think of is depth, because you can think of depth as how much you need to move your finger to reach the object. If I phrase the question as, "Are there any models that do pose estimation from depth?"—not just from depth, as it usually involves RGB as well—I don't think I've seen a model that works just on depth, but those might exist. However, they probably get the entire image, not just a sensor that moves over the object. In the autonomous driving space, a lot of the new LIDAR sensors are not scanning; they're just fixed in orientation, looking out from the car with a specific focus and moving with the car. Maybe there's something there. The Google cars are still using fully scanning LIDARs, but those are much more expensive devices. There's a whole slew of companies producing essentially LIDAR cameras. They still scan the laser within a defined window, but they don't rotate. It's a matter of degree. I'm thinking of a place where someone is getting a small patch of LIDAR information. They probably build up a three-dimensional image from that and then process it. I don't know how they're doing it.

Did you have any other approaches to discuss? Those are the four major categories of how pose estimation is done. The rest of the slides are models.

Metrics for pose estimation performance are also important. If we compare deployed models, we probably want to use the same metric for comparison. It's not the highest priority now, though.

The rest is a collection of what we consider most promising. If we want to create a model on Monty-type data for comparison, these are some promising options for object recognition and pose estimation.

They work in slightly different ways, and to know how much effort it would take to get Monty data into a compatible format, I'll need to download some training data and see what format it's in.

Some are pretty interesting. FoundationPose is probably up there as state of the art; it just came out this year.

It has two modes of training: you can use CAD images or files, or you can use images from different angles. That's its model for training. From that, it develops category ideas, so it can encounter novel objects and estimate their pose by associating with a similar object or category.

It takes a lot of training.

Is that for the model-free or also for the model-based version? The model-free version, to my understanding, is a preprocessing step. It gets you to something like what a CAD file provides by reconstructing a mesh.

So it's more of a preprocessing step, but it seems fair to compare it to the model-based approach, saying the CAD models are a bit like our 3D graph models. We can disregard that the training is different and that they just get them. I'd be curious about what kind of data they use for inference and how they process it to recognize the pose. I think it takes single RGBD images for inference. It's pretty fast; you can do it in real time. You can click on these links and see demonstrations.

I wouldn't call it continual learning. It's not going to keep adding to its database of objects, but it does have the ability to estimate the pose of novel objects by associating with a category. It's less brittle than something that can't handle new objects. It's not continual learning, but at least it doesn't fail when it sees something brand new. Can you open up the paper and show the figures?

They're funny. Oh, they use language too.

The objawars—is that a universe of objects?

Good question.

Okay. This looks a bit too complex to understand at a quick glance, but I think this kind of comparison is where we're going. If we want to, and it's low priority, I agree that we can use a transformer to feed in patches, but it will be patches that Monty sees. I don't think we can train these models with 1 million images, nor do I want to. I'm not trying to recreate a state-of-the-art deep learning system. I think what we can do is start from a basic vision transformer, possibly add some bells and whistles from other techniques, but ultimately we'll probably be training our own custom transformer. It's the same as Monty, but for patches. That would be a comparison to Monty. It strikes me as odd that we would take Monty's training data, which is a series of patches and movement information, and then try to train a transformer model on that.

I'm trying to put my finger on what's odd about it.

Our goal isn't to say our algorithm is better at doing this; it's more that our algorithm works differently. These systems don't work this way. If we train a transformer on a series of image patches that Monty gets, it would be hard to argue that's a fair comparison for the benefits of doing it Monty's way.

People might dismiss it as, "Oh, you trained this thing on weird data, why would you do that? I don't trust that you trained it properly," and there's that kind of disconnect.

For all these things, just because of the way Monty collects observations and does inference—the actual training process, the inference process, the information for these different types of models versus what Monty sees—it just makes it very difficult to try to do a one-to-one comparison with any of these other models. Possibly what could be similar is using a transformer or possibly an RNN. For every step, the patch they process might have a pose, and then the more patches it sees, it can try to refine that pose. Maybe that's closer, but at least that takes into account some sequential processing. Otherwise, it's probably nonsensical to download these foundation models with a million images and try to train and compare that to Monty. There's no point in doing that. Agreed.

It looked like they tested on YCB. Can you show what the results were there? YCB video, I believe. Oh, okay. It does have a potted meat can. Some of the objects in YCB—21 of the objects from YCB were made into YCB video, I think. These all sound familiar: the MasterChef can, seen that one many times, mug and bowl.

Are those numbers accuracy on pose detection? This stands for average distance. After getting the pose, they make it into 3D, and then they compare that with a ground truth pose rendered in 3D, and calculate the Euclidean distance between every point, then average that. So, bigger is better?

No, smaller. I would have thought with distance that smaller would be better, but here they've got the largest values. I guess it's 96 percent correct or something like that, maybe? Possibly. I know, that's confusing. I wrote a metrics section, but that doesn't help.

Never mind. By the way, if you're interested in any of these papers, there's a folder where we've dumped a bunch of them on the Google Drive.

I wonder if it makes more sense right now to just focus on creating plots that show Monty's capability—how we're good at learning from little data and so on. Then we mention all of these approaches and say, "Okay, this transformer-based method uses a million images," instead of actually trying to retrain it on the exact Monty data and compare it.

I'm not in the machine learning world enough to know whether or not this would be fair, but to actually just side by side show how much information is being provided to the system before it comes up with a positive result—something like that—comparing that with other models. That's a step beyond just the training data, because if we're just using patches, that could ultimately be a lot more parsimonious. We can show efficiency on inference if we're talking about sensor patches versus whole images.

We could have one of those log scale plots with number of images seen, and Monty is at the bottom and transformers are all the way up there. The problem is that the performance measure is also different. We have patches and movement data, and they get full images. How do you compare those numbers to each other?

If the reader is truly concerned about Monty performance, they can just read these papers and see how people are doing it with these YCB video datasets. The way we're doing it is so different that it's basically incomparable for the most part. There's the input data side, but also the intrinsic number of parameters it takes Monty, once it's trained, to represent things compared to some morally equivalent transformer out there. I'm sure you'll win on the parameter side of things. That's another comparison we can make.

I agree with what you said earlier, Vivian, about focusing on visualization and plotting to show Monty's capabilities.

Then we can do more of a literature review of the other approaches and highlight how they are so different from what we are doing, and maybe have a plot or table comparing number of examples seen and number of parameters internally. That could go into a full paper, like a background on what's been done previously, and it would be much easier to pull together a table of number of images and how it's doing than trying to test a bunch of these models. I agree with that. We just don't want to do that, because we already know. Every machine learning reader would know that if we trained a transformer on the amount of data Monty is trained on, it would just fail.

We don't need to train a model that would fail, right? If you need someone to train a model to fail, I'm your man.

I think that's about it on our side. That was helpful—useful to see what the state of the art is out there.

It was also good to have this discussion about what we're trying to do and what we need to do. Thanks for putting together those slides; it's very interesting.

Now you know more about object pose estimation, probably more than you care to. I wasn't even aware of any of the state-of-the-art methods out there, so this was a good introduction. I dropped the salient image of vision transformers into the chat; it came from the Wikipedia section on vision transformers, just to show the parsing that goes on.

Sorry, I can't leave the chat on the screen, so you'll have to open the chat yourself to see it.

I guess we took the whole meeting, but I think this was a good discussion. Next week, we can go over the other smaller topics, and I'll have a bit more time to prepare answers. Jeff, you also offered to give some answers to some of the questions—remind me again on Monday or something. All right, we'll do it. If anyone else wants to send in more questions or topics you'd like to talk about, just post them in the research channel.