I was dealing with the basic issue where you have layer four and layer six A. That was the basic idea, and you have these bidirectional connections. This is a location on the object—it's unique. We assume this is a sensory input in context. It would already have been sparsified to indicate whether this input is in this particular context or a different one. You represent the association between these two.

One of the things we said is, if you don't really know where you are, you may have an input but not know the context yet. Therefore, you wouldn't be able to predict or accurately determine your location. If you have some sort of un-contextualized input, it would lead to a union of locations, and then we propagate the union, get another input, and narrow it down. I never thought that would work, because we could never figure out how you would propagate a union by a motor input. The mechanisms we know about for grid cells don't make it clear how you would propagate a union based on a motor output. You get the next set of unions, a set of locations, and maybe it works, but I could never figure out how it would work unless they're all oriented the same way. If your movement is just the same movement through all of these spaces, then it doesn't matter, but in reality, there's nothing to constrain the spaces to be the same.

Another issue is, if you think about how grid cells are supposed to work, you're essentially assuming there's a subset of cells that are in phase, and those are the ones that represent something. Then, I guess you'd be propagating all the cells. We should walk through that.

The way we've always assumed this worked is you have some sort of columns—minicolumns. These are minicolumns, and you've got a bunch of cells in each minicolumn. This is the temporal memory algorithm. Sensory input comes in, it runs to the spatial pooler, and you select some set of minicolumns that represent that sensory input. Then you would randomly select one cell in each minicolumn. All the active cells in each column represent the columns themselves, which represent the sensory input, but the unique combination of individual cells would be in context. We've done a lot of math on this, looking at how many minicolumns would typically be in a real column, and there seems to be enough—a good number. You could get a high enough unique representation here, maybe 200 minicolumns, with maybe 50 or more active.

This has always been a random thing. The assumption is, if you had it out of context, all these cells would fire at once, and therefore all the things associated with all the different possible features and parts of the world would be projected down here at once. This also has minicolumns down here. They're physically the same, but there's a lot of evidence that they're logically separate from these, and that the minicolumn active here doesn't necessarily mean it's active down here, except under very constrained context—like an anesthetized animal looking at gratings out of context. But what do the minicolumns down here represent?

First of all, in layer six A, there are lots of different types of cells. We're not just talking about any; there are multiple types of cells down there, so we have to be careful about that. One of the things about grid cells—a grid cell becomes active at multiple locations on an object. The question is, what generates or activates a grid cell? The basic idea is there are two signals, two sine waves coming in, and when they have the same peak, the cell becomes active. One is a voltage-controlled oscillator, and the voltage is related to the speed of movement in a particular direction. If you're moving in a particular direction, the oscillation frequency of this one will change, and therefore this cell will become out of phase for a while and then in phase again later on. To make this work, you have to have a whole set of cells that all share the same movement vector. They all have the same frequency but are out of phase, so they have different phases going down here. This is not something we made up; it's in the literature. It's a bit speculative because not everyone writes about it, but I can't see another way of doing it. Most people just don't write about it much; they just talk about how a cell comes about. But if you want to have a whole set of grid cells, for each direction, you have to have a set of cells that all respond to the same movement and frequency but are out of phase. At any point in time, you have a set of these, and I'm going to assume those are minicolumns, because they all seem to have the same response, which is the receptive field. At any point in time, some of these cells are in sync, and that becomes your unique input for your location. If I were to look at all these cells and read out from them, at any point in time, some are actually in phase, and that is a unique location of a movement in a particular direction—how far along you are in that direction.

I've always speculated that's what these minicolumns are doing because they seem to have the same basic response properties. You see voltage, you see speed-moderated frequencies down here, and the oscillation frequencies. That's been my assumption. By the way, these aren't grid cells. These don't fit the definition of grid cells. A grid cell would be reading these out, but this is what you need to generate location. Let's stop there for a moment if that's clear so far. The oscillation part is for learning the grid cells, not just for using them. Actually, it's for both.

Essentially, if I just have movement—imagine each of these minicolumns is a slightly different direction of movement. I don't know if there's a base frequency; I don't know how to describe that. The base frequency, which is like coming from the Almas, doesn't change. That's our speculation. That's how you change the scale of something. But there's a base frequency, and each of these shares that base frequency. If you're moving in a particular direction, the cells in the minicolumn that match that direction also oscillate, but at a slight variation of that frequency based on how fast you're moving in that direction.

If I just assume there's one cell active in each of the minicolumns that are active right now, there's only a set of minicolumns active. They represent anything—like we're moving north, so the north minicolumn and slightly less than north to the side. These are all active with different amounts, but the north one would be most active. The others would have less frequency shift. If you look at the oscillation, the phase of all these cells depends on whether this column is moving faster than that column. If this is north and this is northeast and I'm moving north, the phase on these will shift slower, and the phase of these will shift faster. They're all still oscillating, but it's just a phase shift that matters. When I say phase shift, it's a slightly different frequency, which means the peak phase is different than it was a moment ago.

This is the premise, and you would have to do this during learning because you'd have to randomly associate the active cell here—meaning the one that's in phase with the base frequency. As you move, the one that's in phase with the base frequency would shift in some direction. Essentially, these would act like one-dimensional grid cells. I might have 15 or 20 cells here, and I have 15 locations I can represent in a particular direction by which one is in phase with the base frequency. As I keep moving, I'll cycle through them again—360 degrees, that's what I have. The combination of these cells would be unique, so it's like a one-dimensional grid cell. If you look at the phase relative to the base frequency, which ones are in phase tells you your location along one dimension, and that will cycle back around again and again. It's not unique, but it's clear to me that it's like a one-dimensional grid cell in that it has path integration in one dimension and will loop back around. But it's not obvious that it would have path integration in multiple dimensions. If you took a new path around the object and got back to the same location, you're not guaranteed to have the same cells in phase.

Let's think about that. If we just did a really simple example—maybe this is going left, then up, then right, then down. If a minicolumn means going up and another means going down, you're right: this one would be active going one way, the other would be active going the other way. That is a problem with this theory. Grid cells are almost like what read off of this, and then they have the path integration in the higher dimension. We've never resolved this. I don't see any other way to do path integration. This is the only proposal that really makes any sense. The continuous attractor network doesn't work; it takes too many cells. The only viable method of doing path integration is the one laid out here. This isn't our idea—having a set of cells, each representing a different phase. As far as I know, that's the only way anyone's proposed for path integration to work and have all the right attributes, because you have this phase shift—what do you call it? When a location comes after, before the camera, and then afterwards—phase possession, I think it's called.

So this is a problem. There's another question: these aren't really grid cells, because for the reason you said, grid cells are always active in the same place. It feels like there's another set of cells that reads these things out that we would call grid cells.

Grid cells have these unusual properties because we couldn't figure out how to get a unique representational location using grid cells in the literature. There aren't enough grid cells or enough variation in them, but this approach could provide that uniqueness. Others have written about this as a way to generate path integration, which then gets converted into grid cells that form a continuous attractor network. There was a paper discussing how you need both a continuous attractor network and the oscillatory interference method to make this work. You activate the grid cells, which are persistently active, and they can reinstate the correct states, allowing for path integration. As far as I know, no one fully understands how this works, but these are the pieces that seem to make sense. The idea is that you have these kinds of cells, a continuous attractor network, which is actually the grid cells themselves. These are proposed ideas, even if not universally agreed upon in the grid cell literature. The unique location code would be in the vector cells rather than the grid cells. This is the only way I could figure out how to get a unique location code. I don't know how to handle path integration through multiple dimensions; I've never figured that out. I haven't worked through it, but this is all preamble to the main problem.

Let me try to paraphrase your idea. We've always assumed that in our temporal memory algorithm, when input comes into layer four, all the cells in the active minicolumns would become active. This is the input out of context, and then a moment later, these cells would get inhibited. You randomly pick an active one. If the cells were depolarized, meaning you had a prediction from, say, layer six, and you predicted these cells, then only those cells would fire and you wouldn't have this burst, resulting in a unique activation.

There are issues with that approach, though I can't recall them all right now. That was the basic way we thought about it, and we always thought the selection of these cells would be random. Now, if I have a union of predictions—multiple cells in these minicolumns are active—we always said to activate all of them and propagate that as a union. I think what you were proposing, Ron, is that if I have a union of cells here, we expect the same representation will occur in the right context. If I select a bunch of cells and narrow it down, we always assumed it would narrow to the unique one that was there before. You're suggesting that maybe it's not the unique one we had before. We have a union here, and we may not narrow it down, so we probably get the union, but maybe after propagating, I select a different set of cells for the momentary representation.

Now we're getting to the heart of the matter. We always assumed that when input comes in, you activate the minicolumns, and if it's unique, you know exactly what it is. If the prediction is not unique or you don't have a prediction, you get a set of active cells. When you activate that set, you may reset what the actual representation is. I think that's what you're saying. If cells A, B, and C were correct for that representation, but I have a union coming in and activate 20 cells, then narrow those 20 down to three using the same algorithm—burst and pick a winner—I might not pick A, B, and C. I may pick D, E, and F, depending on the overlap. If it's a big union, I could pick random ones; if there's a lot of overlap, maybe just a few. But then you're essentially resetting what the representation is in this context. That's how I interpreted your suggestion. The representation we're mapping to is going to change, possibly moving toward the other predicted ones. It could be random. I think you're suggesting starting with a totally new one.

I would pick from the ones in the union of predictions, so it's not totally random. If I didn't have a union of predictions, I'd pick the same ones. If I have a perfect match, then there's no drift. If I predicted exactly one thing and the input matches, no drift. But if I have multiple predictions, say two, the only place I'd get drift is if there's a common minicolumn. The two inputs must share a minicolumn, and if I had two predicted cells in that minicolumn—one correct and one different—after depolarizing both, you randomly pick one at that moment. Not all cells would drift; only those in a minicolumn with multiple predicted cells. If another minicolumn had only one prediction, there would be no drift. So, the more uncertainty, the more drift you'd have. I don't know if that fits the literature, but that's what it would imply. The whole point was to take your suggestion, map it into minicolumn temporal memory theory, and walk through the implications. I'm not sure if there's an advantage to this.

If I have a bunch of predictions, I have multiple predictions coming in, which means multiple cells and multiple SDRs are active in some of the minicolumns. I could randomly pick the wrong cell for the new representation and start learning that. For those minicolumns, I would exhibit drift. Once I pick the unique ones, I have a unique representation to pass down, not a union. I still don't know how to deal with the union down here—maybe it works, maybe it doesn't.

The next thing to do is figure out what happens if I just proceed. I was supposed to do SDRA, and now, because I had a union, I have SDRA prime, which is slightly modified from A—that's a drift from A. I pass that down and now associate the drifted, slightly modified SDR up here with the location down here. That slightly modified SDR is going to be more consistent over time. Is it? I don't know. It's supposed to be. We have a union of features at layer four, and every time, I'm not sure about randomly choosing from the union of predictions, but if over time we pull them towards some sort of average or sample from these unions into a single representation, both features would be the same. How would you do that?

Do you want to explain that?

Is that also what you were thinking of, Niels? I don't understand.

Was that helpful so far? I think it was good to take the detour as well.

Can I erase? Maybe we just take a quick photo. We have a high-resolution photo. Great.

Erase everything this way.

I guess it was the big two rectangles.

The idea was that if we have a union of predictions, then we would have multiple, so they don't have to be in the same column. Sometimes it happens in the same column, but sometimes you get a prediction here and a prediction there, and maybe SDRV only has this column in it, and SDRV only has that column in it. When you make a union of predictions, you could have this one and that one—you only have different columns if the sensorimotor input to layer four was different. If you have the same sensory input, you have the same minicolumns.

Are we talking about the example where we're on a food bowl and it could be an apple or a banana? No, this is the extension of that where Jeff is trying to apply it in a single cortical column between the location and the—oh, why wouldn't that be applied if apples and oranges? No, it would apply. The original idea was to use it for the hierarchical, but generalizing that, as our conversation is coming back to, I was more interested in whether this could be occurring right here in layer four. Maybe even in all parts of the cortex where the units are formed. Personally, I think it would be easier to almost abstract away whether it's layer 4, 6, 3, whatever, and just describe it as helpful to have that concrete. In this case, if I have different minicolumns, that means I have a different sensory input. That's why I'm asking if this is the case. If it's a banana and an orange, it could be that totally different features came in, so different minicolumns are active. But if you have feature input that's consistent with two objects, that's important because they have completely different features. The way I described it, there would be no drift because the minicolumns have shared, or almost only have shared, features.

That's the example where we have a can and a cup, and so far we haven't reached a handle yet, so it's consistent with both of them. This would only occur when you have similarities between the objects, which is nice because it suggests it may have something to do with classification and forming more general representations. Observing a banana and a can, there's no drift. But if I have a can and a cup, then I might start having drift because there are some common features at those locations. I think it could happen both with overlap in the features and overlap in their structural representation. The same thing would happen if you've learned two different objects that are relatively similar at a structural level. If you have two different types of coffee cups and now you're sensing one of them, you're going to have this union active here, which is initially unique. If we had two different location representations active, the same thing is going to happen—these are going to be feeding back, and you have this kind of overlap. Some cells are going to receive input from both, which could bias those cells to be more active and push these two locations to be more similar.

Or to just sub-select a union. I feel like it's just basic Hebbian learning. As long as you have some sort of sparsity constraint, if you have two different location representations—the one in black and the one in green—and these are for two different objects that are similar, some cells will get green inputs, some will get black inputs, and there will be some overlap in those receptive fields. These cells have inputs from both, but there wouldn't typically be many. If I have two SDRs with similar sparsity, the overlap would be very low in terms of those upper cells. There would be very few cells with input from both if the two inputs are sparse. Mathematically, there's very little overlap between any particular cell. There will be cells, maybe not the ones that were originally active, but a significant number of cells that get inputs from each. No, I think that's not true. That's why I'm arguing if it's sparse, that won't be true.

You could ask, what's the overlap between two representations? It's very low. There will be some, but not many. The learning rule here is that those representations will try to have more overlap over time. We don't want them to be exactly the same, but they'll have more overlap. The union of predictions is going to be much less; fewer cells are going to be active over time because they're always choosing from that subsample, always converging to be more similar.

Along the lines of what Niels is saying, the problem I have with that is, if I think about layer four, you don't want to think about it, you want to abstract it, but I'm thinking about it. On a particular input, I'm specifying which minicolumns are active. That's it. I can't pick a different set of minicolumns; that's sensory driven. True. I can't pick a different set. All you do is pick from within those minicolumns. The thalamus is sending—what chooses which layers get active over time is really which minicolumns get active.

Our assumption is you take these bits from the retina, for example, and you run them into a spatial pooler. You essentially say, "Okay, I have so many minicolumns, I'm now going to activate 30 out of 20 of them," and the spatial pooler determines that. There are all these possible inputs, and you narrow it down to a representation—not a very sparse, distributed, but not very sparse—representation of minicolumns. The spatial pooler is trained and also learns to do that. You have to assume the spatial pooler doesn't change very quickly. It essentially says, "I've seen the world, I've seen all the sensory inputs from my patch of the sensorimotor system, and I've learned to represent them using a set of minicolumns via the spatial pooler." That becomes your basis for all learning. If the spatial pooler changes, everything gets out of whack; it could learn differently. My assumption so far is that the spatial pooler is not changing. We slowly drift the spatial pooler. If the world changed and you no longer have lines, and now you have little curves everywhere, the spatial pooler would change.

I'm thinking, to a certain degree, maybe this drift could be a bit of a feedback, recurrent thing that pushes it more and more. Maybe there's not that much overlap initially, so only a few cells have enough input from each to form a new subset. That helps. You're going to have these covariances, so that's going to push these unique location cells, and you get overlap there. Those cells are going to be more active. Over time, if you're activating the same cells here, then by definition there's going to be some set of cells up here that receive that input. If you deactivate the same cells, then— I had a thought while you were talking, maybe what you're saying, but I lost your train of thought, so if you want to do it again, or I can—

It's a bit of a recurrent feedback loop. I see what you're saying now. There's only going to be a very small number of cells that, if these patterns are two totally different SDRs, are going to receive input from both. Let's say there are a few of those cells, so they're more likely to be active when you see either of those objects. These cells are always active in the context when you're looking at either of these objects. There's going to be some new location cells that receive input from these, that are more likely to become active. Then there's going to be some new cell up here that receives input from these, that's more likely to be active. Over time, it might drift to more.

I'll take that— that's a good idea. I'm not rejecting it; I'm going to add something to it. One of the things about grid cells is they don't seem to form units as far as we can tell. If an animal or rat is in an environment, a grid cell is active in a certain location in that environment. If the rat is confused and doesn't know where it is, you don't see a union of grid cells—one grid cell active, and then when the rat realizes where it is, it shifts to a different grid cell. There's no evidence of unions of grid cells. There's more evidence of having a representation for generic space, and then, "Now I know where I am," and you throw away all the old grid cells and lock into a new set.

I'm curious, do you know a specific paper about this? I remember when I was working with Marx's stuff, I was trying to find a paper about what grid cells do when a rat is confused. There are a lot, because I'm curious how much it fits with— I don't have papers. I can't remember. You should know this by now. This is not a rare finding. I don't believe in that question. In the past, I said that and then it turned out to be rare.

This is a general way. There have been a lot of papers that talk about when a rat doesn't know where it is, and then it doesn't know where it's right, and it locks it. I believe it's a re-anchoring, but it's not like a union. You narrow it down. It's not, "These are all the possible places I can be." It's, "No, I'm in the generic space." Because I'm in the room, I have no idea where I am until I start seeing landmarks, and then I start narrowing it. When I do that, the grid cells change, but they're not like a union being narrowed down. I believe there's strong evidence for that. It's one of the reasons I reject the whole union of locations idea, because grid cells don't represent union implications. Now, grid cells aren't the things I'm talking about here. The minicolumns, that's the derivative of minicolumns, but the same could apply between L3 and L4. Any of these things could apply between layers. I'm not sure how to parse that statement, but yes.

I think it could, but let's consider that grid cells might be unique too. Locations could be unique; it's a different mechanism. Where I was coming with this is, I'm trying to avoid using a union of locations. I'd rather stick to saying, "I have a set of assumptions about where I am right now." It may be generic, like I'm in the unknown space. When I start narrowing down, I jump to a different set of locations, like I lock into the correct set of grid cells.

So you're saying that the representation that gets activated is some random subset of the union of predictions, and we will not have—no, all I was saying is I'm trying to avoid having a union of location positions. I'm trying to avoid having the green and the black lines going up simultaneously. That's really interesting, but that feels like a radical shift in terms of how we update the hypothesis space, because the whole basis for knowing which features to reactivate based on which reference frame you're in is based on the location cells being active. I'm not challenging it; I'm just interested in where this goes.

As I started off today, I said one of the problems I've always had is I can't imagine how unions of locations work. It doesn't fit the literature, it doesn't fit my models, it doesn't fit other people's models. It's difficult to imagine. I never felt it worked. When we said, "Oh, we're going to propagate this prediction of locations," I thought, "That's not going to happen."

It doesn't seem possible. Maybe it is possible, but I've never felt comfortable with it. Now, in this conversation, when this came up, I thought, maybe this is a way of revisiting all this. It is a radical shift in some sense. I'm not saying that union prediction doesn't occur anywhere; I'm just saying right now I'm talking about the location cells and how we know the location cells work. I'm enticed with the idea of maybe we can get away from the idea that you have a union of locations, and somehow this drift works with the fact that, if I'm going to understand the drift in layer four, between layer four and layer six A, I have to accommodate the fact that layer six A is not doing a union. It may be like shifting completely from one location space to a different location space.

Could that mean we might need to question the assumption that objects have unique locations associated with them? If we have a generic location space that lets us move through the feature space of different objects, that might mean we don't need unique location representation for each object, other than the fact that when an animal actually does recognize where they are, the set of grid cells that are active becomes unique to that space. Obviously, any particular grid cell will become active in many different places, but before that happens, it must have still been possible to move through the feature space in a correct way to recognize the object. I think you're posing the problem more than the solution—more like saying maybe we need to question that assumption if it's always unique locations. I don't know about that. It could be possible. I'm not sure why I need to throw away that assumption yet.

It looks like when the animal is in your environment, the set of grid cells that are active tends to be unique. Obviously, any individual grid cell can't be unique; grid cells can't only fire in some environments—they have to fire in lots of environments. But the set of them has to be unique, and it seems that's what it looks like. When you look at the set of grid cells, maybe I'm extending it beyond the data. I don't know. I'm not willing to throw it away yet. I don't know if I want to throw away the nucleation yet. Is there an advantage to throwing it away?

I don't know if this is a cop out, but it feels— and I don't know to what degree it's just a cell population question. Almost like there's something a bit more like place cells here. I don't mind place cells, but that's supported by the grid cell population, or whatever vector cell population, which isn't unique.

I don't know how we work place cells into this. I've always wondered if the layer four cells are the equivalent to place cells, and the layer six A cells are to—there's a grid cell population down there along with these minicolumns, which are the basis of itself. I guess we just start with what assumptions are necessary to enable reference frame-based object recognition, and then think about what cell populations might enable that. For example, during uncertainty, we have to be able to move through multiple reference frames to test hypotheses. Maybe not at the same moment in time—it could cycle serially through them—but there has to be some way to test multiple hypotheses; otherwise, how would you ever narrow it down to the correct one? As long as "test multiple hypotheses" is used loosely, there may be different ways of doing that. Obviously, you have to settle on the correct one.

It's odd the way grid cells re-anchor so quickly. As soon as the animal knows where it is, there's a transition from layer three to layer six A. I don't remember exactly, but I saw it in a couple of figures. You can find all those predictions from any cells. If we're assuming there's voting and associative connections between all the columns, once a group of columns agrees or has more consensus, that's going to force the layer three of the column to have a specific object representation, which will quickly drive the process. That would be a good way of doing it if it's from voting. If I have multiple columns active and my input is ambiguous or obscured, and all the other columns are looking at a banana, then I get layer three banana. If that layer six—you're right, I think there is a layer three to layer six direction. Check exactly what it was. Then you'd be able to say, "I've associated banana," but you won't associate banana with a particular location. You have to associate banana with anchoring the layer six A cells, because banana doesn't tell you the location. Both a banana and a feature are going to set the location, right? If I had the feature, layer four says, "I know the feature," so it's going to be one of these sets of cells—they need minicolumns. Layer three says, "I know it's a banana."

If I have an ambiguous feature, or if it's just an edge and I don't know what the edge is but I know it's a banana, that could help me figure out where it is. It would almost mean that each column is just testing one hypothesis, and once they reach consensus, this only works for the voting scenario. I'd like to solve it without the voting scenario. I think that's important. I added a second one: a location and reference frame should recall the correct feature. I haven't said a unique feature, because if we're throwing out assumptions, maybe the question is whether it's a unique location, which I've always wanted. But I haven't written that; I just thought these are the computational requirements. What do we mean by location? Is it unique to the object? If it's unique, it should recall the correct feature, right? It should predict the correct feature. This is unique in the sense that it's a location in a reference frame. It's not ambiguous which reference frame it's in.

Maybe it's worth thinking about the serial aspect—this could relate to frequency and similar things. At a particular frequency, one hypothesis is being tested. That's exactly where I was going with that idea: at a moment in time, you have one set of grid cells in phase, and the next moment, a different set is slightly in phase, but not as much. This is like the phase progression in grid cells—they become active early, not as strongly, and then everyone knows phase precession. The idea is you look at an individual grid cell, and it says, "I'm active at this location in the room." That grid cell starts to become active before and after that location. The phase at which it becomes active changes. There's a base frequency, and that cell's peak activity is in phase at this location. If you move a little this way or that way, it's still overlapping—the two peaks of the waves are not perfectly aligned, but slightly off, so the cell still becomes active in both places. As you're approaching, the further one becomes active first. If you look at the grid cells as you're approaching the point of maximum representation, the position represented by the grid cells moves in phase space from here to here, with maximum activity here. At different points in the phase, different sets of SDRs would be active. Wasn't phase precession in place cells? It's in place cells and in grid cells.

Here you have a location in space, and over here you have SDR one, then here you have SDR two.

We're talking about individual cells at this point. This cell starts firing as you're moving, and there's an underlying oscillatory frequency. The cell starts firing for all these locations, but when it fires, it first starts firing here, when it is overlapping. I'm trying to remember exactly how this works. The phase precession is that the cell actually becomes active over multiple locations, with the maximum activity at its direct activity. But that's not phase precession. Phase precession is when it fires earlier or later relative to the timing of the spike.

I don't think that's inconsistent here. The timing of the spike is like a phase shift. You have an underlying base, and this is the cell. They're aligned here, and now this one's spiking here and this one's spiking there. It's different. When I think about this, it's more like each grid cell has a preferred receptive field—when it's on its exact location, it has maximum firing rate, and then it tails off as you move away from that location, eventually going back up again as you get into the phase of that grid cell. The reason is because the input to the grid cell is slightly different than the base frequency.

You get the maximum firing rate when they're aligned; when they're slightly off, you get a lower firing rate. So, from this input, it's a lower firing rate. These would be these cells here, but anyway, it starts firing.

Going back to whether we can test different hypotheses at different phases—ultimately, that's the question.

As you're moving through space, it's saying you might be here, a little more likely here, less likely there. There's a distribution of where you might be. Let's go back to your two points. The thing I would focus on is how do we make this work without a union of predictions coming up here? Should we just focus on the L3/L4 case? Because then we're not talking about locations, but I want to understand how it works. Maybe this is wrong, but I feel like drift and pooling SDRs is orthogonal to this issue. It didn't feel that way to me; it felt like they're related somehow. Let's walk through the drift idea a bit more.

Let me review the drift idea. You have two inputs to this layer. If they're very different and don't share any minicolumns, there would be no drift. It's only when the two inputs are similar or identical that you can have drift, because then you have the ability to pick a different cell active in that column, and that would only occur if the prediction from here led to multiple activities here—multiple predictions.

I'm trying to figure out how the drift idea works with the idea that you are re-anchoring down here as opposed to representing a union down here. What would it mean to re-anchor? Maybe I'm stuck at that point. If you want to talk about layer 3 and layer 4, that's fine.

I still want to solve the other problem, but maybe I don't have enough ideas about it yet. How would you face the problem until later? If we assume that we have drift, that would mean that over time we would represent similar objects with similar SDRs. Let's say we have the cup and the can—the only difference is that the cup has a handle. Over time, they would have similar SDRs. That sounds good, but can you tell me how that would happen because of representational drift? Let's say we're on the cup and moving only on the outside of the cup; we haven't sensed the handle yet. Do I have any unique implication? That's what I'm getting at with the re-anchoring. Let's say we think we are on the can and are testing the can hypothesis, in the location space of the can. Once we sense the handle, it would quickly re-anchor to the cup's location space. Then what happens? Where does the drift come in at that point? The drift would just mean that is actually possible because, while we're not sensing the handle yet, the SDRs at the top would be the same and would work even if we're in the—

So, I think on the cup, I think I'm on the can, I have a unique space for the can. Then I come along and have a feature that doesn't fit the can.

It's a misprediction—a feature that doesn't fit the can—and therefore my prediction from the location space is wrong. That has to somehow re-anchor. It's possible that with one new feature, I know I'm on a cup now. That has to re-orient the grid cells, saying, "I'm not on the can, I'm on the cup." But ideally, if that actually happened, you'd still have a sense that the cup is oriented like this in space, because you just felt its body, which you thought was a can at the time. But if, up until that point, you'd only been testing the can location space, then when you feel the handle, all you will know about the location of the cup is that you're on the handle.

So I throw everything away. I feel like we still would want to be testing, maybe because if multiple locations share an SDR up to that point, that would inform testing the cup hypothesis in those locations. Ultimately, we still need to be testing movements to the location space. We talked about how to merge graphs as well. Let's say you have a graph for a cylinder, and until you know whether it's a can or a cup, you're moving through the cylinder graph, updating both. What I was saying doesn't work with the idea that they have unique location representations.

If they would share a location representation, like the cylinder, that could be possible. Going back to the grid cell literature, when you look at grid cells, we don't see these SDRs. We just see some grid cells are active, and then they re-anchor. There doesn't seem to be enough grid cells to form a unique representation. It's two things at once: it does re-anchor when you go to a new object, but it doesn't look like there are enough cells for a unique representation. They're not sparse enough. Imagine you have a set of grid cells, and you're seeing when one becomes active. For the most part, grid cells will always become active in every representation, just at different locations. You don't see ones that are not active at those locations. If you had a really sparse representation, you'd see a grid cell that wouldn't become active at some location where it might have otherwise. Do you follow what I'm saying? There actually is evidence for that. I have a paper on my laptop where they say, "We expect this grid cell to become active." Once a grid cell becomes active and you know its direction, it's supposed to be coherent. People have observed that it may not become active at this location reliably on a particular object. This grid cell should become active here, but it doesn't. On a different object, it would be a different grid cell. It's not active, but it's not like most of the points are not active—it's like one drops out. I can show you this paper; one drops out reliably. If I'm looking at the can and moving along it, this grid cell should be active here, but it doesn't become active. If I'm looking at something else, it does become active, and another cell may come in. It never becomes active at that location. I can show the paper, and I talked to the researcher about it. It's very reliable, and multiple people have seen this. They don't report it much because they don't understand it. The common literature on grid cells is that they should be active at every location. It's not just less likely to connect—it's reliably inactive. I talked to the researcher about it; it's reliably inactive. The majority of the cells are active and reliable. This is the annoying thing about it, because the majority of the cells are correct. I have a picture of it here.

This is one of my favorite papers. We're talking about grid cells, and then that phase of the grid cell is never active, or it's not active at a particular location. The grid cell just doesn't become active at that location at all.

I think it's at the end of the paper. Oh, it's the tank paper. Where is it? Oh, this is supplementary information. Not in the supplement—it's somewhere else, at the end of the paper. See, there's a figure that represents it. Let me see if I can find that.

I've got a great set of notes that I wrote about the tank paper. It just says "tank paper plus," as in I like to write more notes about the tank paper. Here it is. Now I'm entering the main part of the paper. I want to read it again. I remember it's one of these things—it's in here someplace.

Let's see here. "C misfield patterns." Misfield patterns are highlighted fields in B and six cells. So what is that?

One by one, significant.

It's in here someplace. What is this one? Missed patterns? I have to read this carefully. It's in here, and I think I wrote the tank about it and asked him.

I can't remember exactly where it's represented here. I have to read it more carefully—it's in here someplace, in this figure for these two figures.

What is that? Unless you want to sit here and read the paper, which would take an hour.

Missed field pattern on the field in each cell.

So this is a missed field. The cells should be active throughout the entire run, and yet there are places where it was reliably inactive. It should have been active, but it was reliably inactive. This is the actual data—almost half of the cells. I thought it wasn't a lot, but maybe I'm misinterpreting it now. I can't remember; maybe I got that wrong. I just want to make sure I understand it. It was more about the misfield—we're saying that when we change the environment they're running in, some of the cells should become active and they're not.

There are almost two conflicting things here. One is, the animal doesn't know where it is, so you get one set of grid cells. Then, when the animal knows what it is, it re-anchors. The cells anchor at different locations, but they're still active. Once they re-anchor, you'd expect, "Now it's going to be here, not here." It's an array of activity, a grid of activity that was here and now is shifted someplace else. But in that scenario, it doesn't reliably become active in every location that it should. It's reliably inactive. For this object, when you're at this location, it's never active. That's like the sparsification of a grid cell pattern, saying, "Great, I now have a set of grid cells that all should be active in some place." That's not enough to create unique representations for all objects.

Now you start dropping them out, and these cells somehow don't become sparser. This was a piece of data that no one has an explanation for. It wasn't experimental error or a misbehaving cell; it's a reliable behavior. These are multiple things I'm throwing into the mix. That would be a useful mechanism if we want a unique location or representation. My recollection is it doesn't do it enough to make a unique SDR. When I first started this, I thought, "This is great, now we have an SDR formed with grid cells," but it could be a clue to something. There might be a base anchoring for cylinder objects. For individual cylinder objects, you can drop out different grid cells at different locations, creating a set—not a very large set—of cylinder objects.

You're predicting a sparse pattern of features as well. Before you drop them out, you're predicting all the potential features. If there's not a lot of dropout, you'd have a set of grid cells that are active, and at a particular location, it's a subset of them, generally predicting the same features. There's a lot of overlap; it's not a totally unique representation of that location on the object. I have a partially unique representation—some things dropped out, some are still there. I would almost think, on a different level, that the specification of the grid cells tells you what kind of object structure you're on—like cylinder, sphere, or square—but which specific cylindrical object it is depends on the features in layer four. If you want to encode unique locations for all objects—

That basically, the specification of the grid cells tells you the rough shape of the object you're on, instead of which specific object with that shape. The location representation for two cylindrical objects is the same in L6A, but which specific cylindrical object it is depends on the active cells in layer four. But then I can't predict the specific object from layer six. No, you wouldn't be able to. It's more than just generic—you're saying, "I'm on a cylinder," as opposed to some other shape. In that way, it wouldn't be a big deal that it's not as sparse, because we only need to encode a limited amount. But if I have to make a specific prediction, you predict a union.

If I know I'm on the coffee cup—I'm on the cylinder, touching a cylinder, but I know it's the coffee cup—if I'm not touching anything related to the coffee cup, I'm just touching the cylinder. But if I know I'm on the coffee cup, then when I move, I'll make a very specific prediction. I'll say, "It's not just a cylinder; I should feel the handle." I'm not surprised the handle is there. At this point, I'm on the cylinder, but I know it's the coffee cup. Let's say I touch the handle, then move away. Now I know I'm on the coffee cup, and I have to make very specific predictions for every feature. Could that be biased from L3 connections to L4? It could be, if you can come up with a way of doing it. I need to think about that. There aren't many L3-to-L4 connections; it's sparse. There are some, but not a lot.

What you're saying is, when we have the hierarchy and the layer three to layer four connections, and then we do the generalization and mapping, the two representations of the cup—one with a handle and one without—are going to be very similar. The location in 6A in the higher region will also become more similar, because the representations in the higher region are similar. When that feeds back to 6A in the lower region, they become more similar, representing a cylinder more than an actual object in the high region.