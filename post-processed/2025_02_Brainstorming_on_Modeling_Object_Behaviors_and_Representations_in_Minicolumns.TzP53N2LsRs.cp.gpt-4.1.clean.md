I thought we just pick up where we left off.

We're working on this idea that color could be represented by a set of minicolumns that are independent of, say, the edges—like color blobs. I was reading more about color, and I need to correct myself. I remembered that color is mostly considered to be processed in V4 with stripes in V2, but the blobs are in V1. I was mistaken in thinking color isn't really processed or understood until V4. When they say "processed," I just read it again and realized I didn't have time to read it thoroughly. While writing here with John Chen, I read a paper by Catherine Rockland about blobs, focusing on extremely low-level details and minute differences in structure within structure. It's really mind-boggling. I can't make any sense of it; blobs aren't just blobs—there's substructure, more structure, interconnections, but no explanation of what anything means. Also, very creative names: blob and interblob. There's a blob, and the thing not in the blob is the interblob.

Remember we were looking at the big green color blob in layer three and then the other, red, whatever the other word is. Those are just more examples. As I was looking at it, there are little bloblets in layer four, but bigger blobs in layer three. Maybe they're all doing the same thing in the following way that we started talking about. I was thinking about this model: we have two reference frames, both driven by the same motor behavior, so they're actually in sync most of the time. We're going to assign one of those reference frames for behaviors and one for static things—one for static objects and one for modeling behaviors and structure. Structure versus behavior, structure versus change.

Up to now, we were just talking about edges—these minicolumns that represent edges. Those minicolumns would represent the morphology of the object, and if those edges are moving, then it would be the change in the morphology of the object. We could basically learn stick figures dancing or doing whatever—staplers, for example. It doesn't really matter what the color is; it's just about edges moving, and that's cool. But then we didn't have the color aspect. The next idea is, what if we represent another set of minicolumns with color? These representations have to be in minicolumns. To represent orientation, I have to say, "this minicolumn is this orientation, this minicolumn is that orientation," and so on. There aren't a lot of them; that's the point. It's the output of the spatial pooler—k winners of a small number. We'd have to do the same thing with color, representing color as a set of minicolumns. I don't have to say each minicolumn represents a particular color, but I'd have to be able to represent my color spectrum with a set of minicolumns.

The idea here is, what would you learn? If I just looked at a particular object, like a stapler, and there's a color, I could assign that color to the stapler. If the color of the stapler changed at any point in time—not by location, but over time—that would be captured here and then fed into your behavioral reference frame along with anything else that might be seen. Maybe it's just colors that are changing.

If the whole thing changes from red to green, it would learn that. If it was associated with particular locations on the traffic light, then it would be associated here with those locations. You'd be going by the green, and this is the general-purpose idea: you can have any attribute as long as it's represented by a set of minicolumns.

If you had a static and a changing version of that, then you could learn some other attributes—change of color or change of whatever these things are we're talking about.

and then it made me wonder if that would work. I think you could basically say you mix and match here—any one of these things could be enough to infer the correct behavior. It could be a combination of movement and color change, just color change, just movement, or something else like a sound. Whatever it is, we have to represent it in this column. Of course, on the original object, we might have assignments of these things to the static reference frame. I haven't worked out more than this, but the point I'm trying to get to is that I could learn the behavior of an object using a combination of attributes that seem important. If the color changes as it goes and that correlates to other changes, then that would be part of the behavior of the object. Maybe the color changing would be independent of the morphology changes, so the thing changes from red to green every three seconds, independent of whether I open or close the stapler. Those would be two separate behaviors.

Somehow, we want to be able to say, on a new object, maybe I recognize it by morphology and see the actual color, but there's no change in the color. This has to lead to my prediction here, because color is not tied to a particular location—it's just tied to a section on the object that is not changing. Therefore, if I open the stapler, there would be no reason for this to change. It doesn't have to be persistent; you have to say if it was red, it's going to stay red, or if I'm looking at a place and there's an edge and the color changes, I'm just going to assume it's the same. I haven't worked through all the details, but it seems like you could transfer anything that's not changing—you would just assume it's constant because you didn't record a change there as part of the behavior. Or, if the color changed at different locations, that's not a behavior. If I recognize another object, I might assume the same color change on the new object. For example, if the top of a stapler is always blue, and I see a new stapler with the same morphology, I would say, "Oh, stapler—these locations are always blue." I haven't worked through all of this; I'm just trying to test the ideas.

That sounds like what I was suggesting, but I'm developing it. Did something change in how you were thinking neurons could be doing this? What's new for me today is the idea that I have to segregate the minicolumns into these different attributes. Up to now, when we thought about columns, I always thought color or other attributes would be integrated into the same layer four as the edges. I felt like an object has some morphology, and then we put other attributes on top. This is saying objects—actually, edges aren't even preferential here; they're just another type of attribute.

How does this help with the problem that the color needs to stay the same as the stapler moves through different locations? It doesn't yet solve the problem of predicting where the stapler top will be, but it does solve the problem of color constancy or color changing because it's a separate column. It's not just a model of colors for this object. In some sense, I could have a static object with changing colors, or a changing morphology object with static colors—they're separate models. This is a color model of the object, an edge model of the object, and other attribute models of that object. We've already had this idea in some sense, like combining the sound of an object or the touch of the object—the feel, the vision, what it looks like, what it feels like to touch—but I always thought those were in separate areas, different parts of the brain. This is all in one column here, one sensor patch at one location, sensing different things.

Basically, having color in its own columns makes it possible for us to keep it constant. The color part of this model dictates how colors change or don't change; the edge part of the model is how the edges move or don't move; morphology and other attributes would do the same. The nice thing about this is, the next step is to say this is nothing special—this is what we see at low-level visual and somatosensory regions, but elsewhere in the cortex, I may have nothing like that.

It frees my mind from thinking about the morphology of concept space. I just have to think about a bunch of attributes that are changing, and this is a big difference in my mind. I'm divorcing things we thought we had to have in every model, like morphology and edges, and saying that's just convenient in V1 and S1, but it may mean nothing higher up. It's just a different set of attributes. I feel like it's quite a shift in thinking; before, we talked about features and morphology being treated very differently, but now they're all minicolumns—separate minicolumns, but all the same conceptually. Our old model is a subset of the broader superset of how a column works. When we think about primary sensory cortex, especially vision and touch, it's hard to avoid these edges as a feature.

I already have a problem with this, though. While you were in your meeting with John, I thought of a potential problem. I was trying to imagine, and this is the one you brought up earlier, changes in orientation. Whenever I have a feature, like a little logo, and it just rotates—so it's a compositional object where I say, here's a feature at this location.

The thing that changes the behavior is its orientation. You can imagine opening a stapler, and as you open it, a logo on the side spins around. I can easily see and learn that, whether it points up or to the left. I was trying to figure out how orientation plays into this—changes in orientation are part of the model, and I need to be able to represent that.

Could that be done similarly to how we handle normal changes in orientation, where the behavior model encodes changes in orientation and those get sent up? The terminology gets confusing. I thought, why not add more minicolumns to represent orientation? Then, if an orientation comes in for a feature and the orientation changes but the feature doesn't, I would encode that here. But then you'd have to send that orientation to the system and make it orient something, and it got complicated. This orientation applies to child objects, and I thought that's what this was for. Originally, I thought this was the orientation, and the child object would be like color—somehow infused into layer four, with orientation columns. The child would be a sparse representation dictating layer four cells, but it doesn't work beyond that. If I do this, it doesn't work anymore. I'm not saying it's a big problem, just that I'm confused right now. Do I have a separate minicolumn for orientation? I thought that's what these were. Or is it that if the logo is at a certain orientation, that's the orientation, and the logo is here, in a separate minicolumn? As a child object, there can be so many that you can't represent them all in minicolumns. I can't take a child input and say, that's a new simple attribute—let's represent it in 20 minicolumns. That's not possible; there could be millions or hundreds of thousands of child objects. I can't just treat a child input as an attribute at a location. I have to do something else.

Wouldn't it be just a projection of the temporarily pooled representation in the lower level? But where do I store it? How do I indicate there's a child object at some location? I know how to send the representation for the child in SDR, and I can calculate the orientation in the thalamus—the relative orientation. The information is coming in, and my assumption was that I would use that to dictate which cells are active in the oriented minicolumns. This was the orientation of the child object, but I can't do that here. This is just morphology; I can't add it in. Changing the orientation of a specific object—I'm a bit confused. Did you mean to draw these the other way around? Here it's changing and there's static, but then—

Oh yes, I did that wrong.

That was the only one these didn't specify, so these are tied together.

One thing that's different in this static edges view is that, at least in Monty or in general, you don't need to store all these different edges. You just need to learn a model of one edge and then have corresponding orientations you apply to that edge. What do you mean by "apply to it"? You're saying the feature is the edge, and these are the only ones—the edge is the feature, and it can occur in different orientations. I thought about that here. Maybe it still works, but I was having trouble with it. For a while, I adopted the idea, but now I'm struggling with it. Whatever is encoded in the behavior model would be translations and orientations being perceived, and there are minicolumns representing angled edges. Is that a universal orientation, or is that a feature at that angle?

I'm not sure why we see that in the brain. Algorithmically, you would just need to store one edge and be able to rotate it to any orientation. Let's say that's what I adopted in this picture. The feature is an edge, and we're showing it in different orientations. Basically, you wouldn't need to learn each one, but how do I apply that to the logo child object? The behavior model would just include translations and orientation, and you can apply those to any feature—an edge, a curve, etc. Let's say the behavior model applies that. At the moment, my mental model is a set of cells representing location in space and another set, layer 6B, representing orientation. So I have two different reference frames. I didn't show the other one here—6A and 6B. The idea is that one would be locations and the other directions or orientation.

Both are path-integratable. Motion can change your orientation or your location. I don't know what to do with that, but let's continue. This is my static reference frame: 6A and 6B. This is location on the object, and this is orientation of the sensor to the object. I have these two layers of cells. I'm still confused how these would be the translation and orientation of features on the object, and this is of the entire object.

This is different—no, sorry, this is of the sensor. This is the location on the entire object, and this is the location of the sensor to the entire object, or maybe at this point in time. Those are very local things being applied to one of the features on the object. Can you make a picture that makes it all work?

I don't know. It may not be hard; I just haven't figured it out yet. I was just working on it, so I haven't thought it through. That's why I'm stuck. It's not that it can't be done; maybe we just haven't drawn it yet. I haven't thought about it clearly.

I'm trying to come up with a picture that captures everything and makes it clear.

Are these both layer four now? I didn't specify. In the classic view, the static one is layer four and the changing one is layer three. Layer four represents edges not moving, and layer three represents edges moving.

That's the classic view. We'll say that's a delta, the changing one. Keep in mind, when we say layer three and layer four, there could be multiple types of cells. In layer three, there could be temporal pooling cells and motion detection cells. We can't say all of layer three does one thing; it could still be temporal pooling and passing up to the next region. These aren't definitive answers about where these things are. We have the feature input going to both. We get the change in input from the sensor; some of that input indicates something is changing, and some indicates it's not.

We have the marking cell pathway, which would detect motion changes. If you look at the mag side of the cells, they are center-surround cells that detect change, not motion. Only when you get to the cortex does it look like they're detecting motion. That's exactly what we want, right? The inputs are center-surround, non-directional, but they detect change. There's a universal rule: there are two types of inputs, static and change detectors. According to the literature, the idea of motion only appears once you get inside the cortical column, like motion to space. Most neuroscientists believe it's hard-coded or genetic, but the more generic case is you get a bunch of change bits, and then the cortex figures out what they represent, does a spatial pool, and decides it's likely a moving edge.

We would have the mark pathway going into layer three, telling it what changes it's detecting. Features go into layer four. We have movement of the sensor relative to the object, which is also the mag pathway but over a broader area. That would be total movement. Let's leave it as you said.

That would be transformed into movement relative to the object by the thumbs using projections. That's pretty much what we had so far, except now we have the delta change going in here and a separate behavior model.

With the separate reference frames, did you say anything about whether they would both also be in layer six? I have no idea. If I had to guess, I'd say they're both in layer six because you want them both driven by the same motion vectors. Here's how I imagine that would look:

In this case, I view the minicolumns as representing the things required by grid cells. They have a series of cells that all represent motion in some direction, and their voltage-controlled oscillators fire at a frequency depending on how fast you're moving in that direction. There's a background frequency that does not change. Which of these cells lines up with the background frequency is your hot spot, and as you move, that cell moves along the trajectory. That's the basic theory about how grid cells might come about: this one's in phase, then this one's in phase, and the faster you move, the faster it moves. This is like a repeating one-dimensional space vector; as you move along, it repeats.

You have a whole bunch of these; it's unique space. What I imagine now is you have two sets of these, both driven by the same motion vector, but each set anchors differently.

One anchors to the object, and the other to the behavior. They look very similar, but the moving cells don't continue down here; they loop around. These would have different anchorings so they can have associative connections to layer four and layer three. I have two reference frames I can connect to as needed. The behavior one would be driven by changes.

I imagine those are both at the bottom.

Reference behavior frame and object reference frame.

Okay. So the base stapler object that we apply changes to—is that what you're referring to? The object reference frame, the base stapler, would be in the associative connections between the reference frame for the object and the static features. Imagine you've learned a static stapler; that's just the object reference frame and layer four pattern. Now, something starts changing on the stapler that re-anchors the behavioral reference frame but doesn't touch the object reference frame. Now the behavioral reference frame is saying we're starting a new behavior. That behavior, although it's located on the stapler, occurs in one place, but because it has its own reference frame, we could apply it to other objects at other times. That's the key. There's also object ID somewhere in there—layer three or layer two. We're not worrying about that right now, but it's somewhere. The application of behavior to the object model creates predictions in layer four. We haven't said how we are going to do that yet. If I recognize an object or a behavior at some location on some object, I will now apply that behavior to those locations on the object, whatever the behavior model says. We have not yet worked out exactly how it leads to a prediction on that floor. That's where we got in trouble two days ago. Zooming in on that, that's where we stopped. So we zoom in on this layer—there are four layers, three, both of them. What you drew there: at the top, we have the delta changes, and here we have absolute values. For the delta changes, we would have translations in space and rotations in space. This is a hypothesis because I'm not sure if the rotations make sense, but keep going.

That's a question. This is a hypothesis now, saying maybe we'll have rotations there. It seems like the mag cell would be able to distinguish the local flow from whether the object is just moving like this or whether it's moving like that. There's no question that the cortex calculates changes in orientation from the large magnocellular pathway. I didn't even get to applying the orientation yet; I'm just saying those should be able to detect orientation changes of local features. This is the current hypothesis because I have no evidence in the biology that you see that, but I'm good with it.

Other edges moving—we see that's well known. Have they ever tested having an edge under—I'm not sure. Maybe they have; I just don't remember. What I do know is it's very well known that there are edge movements. It seems like it's the exact same mechanism. The point is, at the moment, it's a hypothesis because we don't have any evidence for it. I'm good with it. All neuroscientists would say, "Oh yeah, there's this edge moving in minicolumns," because they've tested moving edges. But it seems like the exact same thing: you can detect that there's local flow like this, but with the same mechanism, you could detect a flow field that rotates. What's interesting is that somehow we believe they differentiate down in layer six—somehow layer six teases those two apart and puts them in two different representations. In some sense, you could argue it's just taking a bunch of change bits and mapping them into a set, putting them into a spatial pool, and some of the spatial pool would be rotations and some would be movement.

Exactly. The weird thing is we're hypothesizing that they're separate down here, so how do they get separate down here and they're not separate up here? I guess down here they're really different; they're not just a difference in whatever local flow you observe. Here, we're actually moving through the whole reference. But remember, it's just center-surround bits coming in, so the system has to figure out what those center-surround bits mean.

Let's keep going. I actually went as far as saying maybe we're wrong—maybe the location and the orientation are combined together down there. So, if we have movement of the sensor relative to the body, that movement would include detecting rotation of the sensor. In the grid cells and the entorhinal cortex, there are pure grid cells, but there are far more cells that are orientation and grid sensitive—they're gritty in one direction. That is consistent with this hypothesis.

I'm just keeping all the options open. If I accept that movement and orientation changes are universally the same—whatever motions are detected and put into a space pool—then I have to ask, why didn't that happen here? I have the same kind of input coming in, yet I'm assuming they're separated into location and orientation. Maybe that's incorrect. Maybe there's a spectrum between location and orientation, which seems more consistent with the biology.

Let's keep going. Let's say there's orientation, just like we've drawn it up there. I agree, it's an interesting idea whether movement of the sensor relative to the body also includes rotation of the sensor, but here it's actually the inferred location on the object and the inferred orientation of the entire object. That seems quite different. The input would be turned into, for example, movement in some direction and rotation in orientation one way or the other. The grid cell mechanism would turn into a specific orientation of this object or something similar. So this could be a location/orientation input, but that would still tell you how to move through that space.

Let's put it aside for now. This is the hypothesis that maybe we're representing rotations in orientation as just another type of change, like any other. All the behavioral changes we could have—color as well—would be a different type of change. Color change could be temperature change. I don't know what else—maybe child object ID change in the future. It could be any kind of change. But child objects don't work there because there are too many. You can't represent child object IDs with many comps. That's the problem.

We have a bunch of changes, all treated the same way. Just to focus on translation and orientation right now, we need to be able to—or it would be useful if we could—apply those to the static features coming in. If I'm sensing an edge, curve, spike, or peak, it would be useful to apply these local behaviors to these local features. That's the crux of the matter. If we have the ths and it can rotate things, we could have a connection here to tell it how to rotate the static feature, so when it comes in, it's already in the expected orientation.

Maybe you don't need that. I don't think you need the ous for this. At this point, we're just talking about edges of the object or on and off things. There has to be a blank field; these wouldn't get activated, but only when I see some bits moving.

Think of it this way: this is an edge moving in a direction. This would be an edge that's rotating at the same point. But does it always have to be an edge? Could it be any local feature that's rotating? Not with the mechanism we've specified right now, because I have no way of representing that. The idea is to take the simple representation and form something unique in the context of the behavior. I can say there's a rotating edge here, but all I can do is say it's unique in the context of the behavior. I can predict that in the next moment it'll be in a position like this, or in the next moment, I can predict the edge at a different orientation or location. It seems that way you are tying the behavior to the feature.

If I now have a ball-shaped thing that's moving, I don't want to have to relearn all the possible translations that this ball-shaped object could go through. I want to be able to apply translation and orientation, as I learned for other features, to this. In its core form, this would work because I've never seen this before. It's just a bunch of little edges that this thing is seeing. Different columns are seeing different things. At this point, there's a slow movement of this edge. I was assuming that's the smallest possible feature. All you can do is say, at time t it looks like this, and at time t+1 it looks like this. All these have some movement vector. I could learn that this set of movement vectors is followed by another set, and so on. I could represent this curving C without knowing anything about a C. It's a bunch of lines that, at this point, are moving in these directions. The next point I observe, they're moving in another direction, and so on. It doesn't necessarily allow me to fill in over time.

Maybe when I said "ball," it sounded like a big object, but I just meant that instead of a flat surface, it has a round surface. I'm not sure if we see representations like that. You would have to decompose everything into tiny edges, even round surfaces.

This is where yesterday I started thinking that maybe to solve this efficiently, I need multiple columns. I can solve this problem inefficiently—the model could just say there's no concept of C-shaped structures, it's just a bunch of bits moving, another bunch of bits moving, and changes. I can learn a very complex behavioral model of how all those changes occur over time, but it's very inefficient because I have no way of moving the whole thing; I just have to learn every point at every point in time. Does that make sense?

So, more like composing—making a C shape out of several parts, right? In reality, to do this with a single column, it would have to sample all these points, then all the next points, and so on. It could take forever to learn because I can't do it in one pass. But if I have a bunch of columns observing the same thing, and these columns somehow know where they are relative to each other, someone is observing every part at once. That's as far as I got, because it seems like they could coordinate with each other to move everything at once, without having to learn all the points in between.

I don't know how that would work, but it seems like they could coordinate to essentially move everything at once. This is based on the idea that people don't only observe moving edges in the brain, but it just doesn't seem possible. At a higher level in the hierarchy—imagine the stapler, where you have the logo on it that rotates in the opposite direction when you open the stapler. You can learn that behavior and apply that rotation to the child object, the logo.

That's a separate problem. There are two problems we haven't addressed yet: how to deal with the fact that child objects can be changing, and that there are many child objects, so I can't represent all child objects with many columns. That's one problem. The other problem is moving a bunch of stuff through space that isn't changing except for its movement and orientation. This is the top of the stapler—how do I represent that efficiently? That's not quite the same as the child object problem.

I'm bringing up the child object because you can start imagining learning the movements of edges and decomposing things into edges, but it falls apart when you need to apply movement to more complex child objects. I agree. Then we said, okay, edges are at orientations, like an object at orientations. One kind of child object is an edge; then I could have a logo at orientations, and so on. I can represent an edge at all its orientations, but I can't represent the infinite number of possible child objects, which are all high-dimensional vectors. I can't take a child object ID and say an edge is one thing—oh, I have an edge, that's one object, I can rotate it and represent it in different orientations. I can't do the same with a child object because there are so many. I can't have many columns dedicated to each child object.

The answer obviously has to be some kind of projection from the parent to the child.

For example, let's brainstorm about this. Imagine I have two hierarchical regions, both recognizing the stapler, and then I see the staple top moving. Maybe that means there's a region that's moving, and I project down to that same region, only paying attention to that region, and rotate the stapler. Sometimes I'm rotating the whole stapler down here, but I'm saying attend to only that part. I have to pass the ID back, which isn't hard, and just say rotate, and also only pay attention to the part included in the behavior.

I picture a behavioral object as occupying a bunch of points in space, and those are the only points that matter. In the physical object, the only points that matter in the topology or morphology are where there's an edge; everything else is nothing. Here, the only points that matter are where there's a change; nothing else matters. So I could project down my change locations onto this region and say, okay, you're rotating this, but only pay attention to the attended part that's actually moving—ignore everything else.

I could use the model of the stapler, and all I'm doing is rotating this part of it. Now I have this parent-child relationship where I can use this to generate all the predictions I need. Does that make sense? The behavior of the mask should be possible because at the top level, we're learning behavior on the location. If I'm looking at the behavioral object, there are points in space where changes are occurring. I then go down to the child object, which is co-located—there are columns co-locating on those columns. Those are the ones we care about, somehow moving that subset. I don't know how it would work.

I'm trying to solve the stapler problem by saying I have another model of the staple here, and if I could just rotate this model and only pay attention to that part, then I can make predictions. Even if this is a novel position I've never seen before, I could predict what it would look like if I opened it all the way up. I could just rotate this part, and now I have this object at a different orientation. I could make predictions based on using the model here—I'm rotating this whole space. I can't rotate this space, but I can rotate this space, and therefore I can make predictions on it.

That would definitely make it easier to rotate all features. It's got to be something like that. I don't know what else it could be.

It's like when I open the stapler—you can just imagine opening the stapler. Nothing changes; it's just the top of the stapler. You can predict any kind of outcome. It's as if the whole stapler rotated and you're looking at the top. No problem.

It's not like I'm doing translations or calculations. It just feels like I'm looking at the top of the stapler. It's the whole stapler. It has to be that way because, with this idea, those behaviors would rotate the features, but that doesn't work anymore. Once I move to different locations on that part of the stapler, after it has moved, I'm not really in a movement part of the sequence anymore, but I still want to make predictions about the correct orientations of features. In this case, it works because we are rotating the entire stapler or the top part. We can make predictions about any point, just like we normally do. But here, we've passed that point in the sequence.

I'm agreeing with what you're saying without getting into details. This gets back to the problem we have with breaking an object into multiple, separate pieces. We talk about that a lot. It always bothers me.

This tells you if I see something move, like the top of the stapler, it might automatically treat the top as a separate object. You could say, "I'm going to create a child object as a subset of the object I had before, and now this is its own child object, and now I'm just rotating the child object."

So, if you start changing independently of everything else, I'm going to split you off and you'll become your own thing. That's nice because it works without doing that too. In the long run, it tells you how to split out the right objects, but it doesn't require it from the start. The first time you see it, you can still make inferences. I could see a static stapler, just learn it as a static stapler, and be happy. Then, as soon as something starts moving, it tells me right away, "This part's separate from that part. Maybe you should model that part separately." At some point, maybe you forget, but at least it gives you clues on what to do. This is really hard. Everyone's being pretty quiet—maybe because we're talking too much. I'm talking too much because it's really hard.

This is part of the solution, not a complete solution. When things start moving, we try to break off the part that's moving, instantiate it in a full model, and isolate the section to turn it into its own model.

Now I'm assigning that model to this thing. That's the basic idea. It doesn't solve everything, but it's part of the solution. Making it its own model is a step that happens over time. It's not like the first time you see it, you can still do it with the stapler as a complete model. You start with a complete model. It's not even clear that it becomes its own model. The mechanism we described doesn't require it being its own model; it just requires that I'm able to isolate a subset and treat it as a child. In fact, it would be better if I didn't create my own model, because if I do, I have to relearn it and a new reference frame. Let's back away from that. Basically, I have two models of the stapler. That's not too hard. We already have that.

When it starts moving, the upper level says, "Okay, I'm going to tell whoever is below me to isolate this section that's moving and make predictions based on that movement." It could even be a dynamically determined child. That's how I see it. I don't actually ever think of the top of the stapler as separate from the stapler. I don't expect to see it over on the counter. It's always attached. It's not really its own object if you never really separate it. It's just, when it's moving, I treat it like its own object. I don't have to learn it as a new object. I can dynamically, on the fly, create a child and say, "Okay, this is the part that's changing. Ignore the rest. Rotate it, and now we can make predictions based on this rotated section relative to me." That's nice. It's a bit like the bent logo—if you have a hinge behavior, you can apply it at any part of the stapler or break any objects. You can imagine the hinge being here or there, so you can apply the behavior to arbitrary subsets very easily without ever seeing it like that before. I even imagine, on the top of the stapler, instead of going up, what if it opened like a set of doors? As it goes up, you can imagine that. I could learn that quickly.

This part doesn't really care. If something is moving, I isolate a subset. That gets into another problem. I'm saying a behavior that looks like a stapler, and as you open it, it becomes like this. These little things can flip up.

That would basically mean, just imagine it opens up like a set of doors. There's a base plate here, a hinge here, and a hinge here. And there's a big H here.

I have to make separate predictions about this, and about this, and about this part, because the original model didn't know about this movement. To make predictions here, I have to assume this is all one piece. To make predictions there, I have to assume it's one piece. So I'd have to say, I'm breaking this into three children that are behaving as their own, or it's three child objects now. There's this one and this one, and I'm learning how those child objects are moving.

how they're moving. I'm basically saying, take this part, isolate it, imagine it rotated like this, and make predictions about it. Take this part, rotate it, imagine it like that, and make predictions about it. I don't have to learn these as separate components or objects; I just temporarily rotate the reference frame of the child object. We're rotating the reference frame and attending to a subset of it. From a child object point of view, I can't do all three of these at once. I'd have to rotate the child object in one position, in one area, then to make predictions over here, I'd have to imagine this part being a child object and rotate it in a position. Then I'd have to imagine this part being a child object, rotate it into position, and mentally switch between these three things.

I think that's what you would actually do. You'd have to attend to this and anchor onto it, attend to this, anchor onto it, attend to this, anchor onto that section, just like you can attend to this and anchor onto it. It's a way of saying, what do you mean by anchor on? To anchor the orientation and location. Imagine I was observing a stapler and it was largely occluded, so all I saw was a piece of it. I could still infer that's a stapler and say, where am I on the stapler and what orientation is it? That's what I mean by anchor on. It's like saying, I'm only seeing a part of the stapler here. Where's the rest? Where's the stapler? I've only seen this piece. I imagine snapping my internal model of it onto the observed part. Imagine I only have a model of the closed stapler, and that exists in the lower regions. Then, all of a sudden, I see the stapler, it's occluded, and it's at a certain location and orientation. I can only see that section, but I still know what it is. Then, here's another part of the stapler, at another orientation. We only see that part; the rest is occluded. I can do this with a static model of the stapler and just say, I can see an occluded part, I recognize what it is, and I see the orientation. That's what I meant by anchor onto it. I have to anchor the location and orientation of this section, the part I'm attending to. It's like you said, snap the full model over it, like augmented reality. You try to do object detection, and when you recognize the object, even though you see only part of the fork, you project the entire model of the fork over it with the recognized orientation. That lets you predict what would be there if it wasn't occluded. With this mask, it's like augmented reality, but you only project part of the object model. We don't want to fill in the rest; we're basically saying it's the opposite of augmented reality. It's segmented reality. Only pay attention to this part, forget the rest of the world. It's just this part. What do you see there? You see a stapler part—good, ignore the rest.

Would this solve all of our problems? What if I see the stapler opening and I want to predict? The behavioral model is like a point cloud where changes occur. As the stapler opens, that point cloud reflects the movement. It just reflects the movement of the stapler. Remember someone showed the random bit pattern, then you could show movements through the random bit pattern. I coded that up when I was young.

My point is that's a little bit like this behavioral cloud. It's basically saying it's all nothing until I see a change; that's all you're going to get, just points and change. As the staple moves, those little points where the changes are occurring form a line of changes, and those points of change can define what we want to see below and tell it what you know. Then, the thing below says, okay, here are my specific predictions. It's very much a hierarchy. It's the same as a hierarchy, the same as composition. I have a question about hierarchy and behavioral hierarchy. If I had a higher-level behavior and, let's say, I want to project that behavior down to child objects—let's say this is my parent and it has four children—and the behavior in this parent object is that it rotates by 90 degrees, the whole thing. Is that really a behavior of that object? I wouldn't define that as a behavior of the object because it's not changing the model of the object. Be careful. I have to define behavior in a very specific way, which may not be sufficient, but what we're considering right now is that something in the model has changed, not that the whole model has rotated relative to the sensor. That's not changing. You could rotate the section, so the parts themselves have to change; something in the model has to change. The model itself has to change, not my orientation to the model, but the model itself. So attach that square to another object, like a propeller on an airplane—the propeller is rotating relative to the airplane model, or behavior. I can probably redo this example. You could just draw another big square around, or you could just rotate the B element. Rotate. That's how I would have to do it.

So this becomes a B. Something like that. That would qualify as a behavior of the object.

And Vivian's right. If you took that whole object and put it as part of another composite object, then moving—if that was the dinner plate and it's now on the table setting and you move the dinner plate—that would be changing the dinner plate, changing the place setting. But on its own, the dinner plate doesn't change when you're just moving it to a different position. So maybe this is like the B is the dinner plate and you're rotating it or something like that on the place setting.

I'm not sure if this is going to work. What were you going to say? I could probably try to break this out even further. The original thought is, if I want to take this rotation and apply it to the child objects, how do I make sure that it does?

That's A, that's B, that's C, that's D. Actually, I may have misplaced these, but I think I got it right. That's C. That's supposed to be C. How do I make sure it does that instead of doing A? I think we have that. I don't individually rotate, but the entire thing, because you're rotating the entire reference frame of the whole object. They all come with it. There's nothing different.

You're not rotating each of the features on its own. Everything that comes in here is the same. You're just rotating the grid you have for the object—the reference frame. The way we did this is that the parent reference frame represents the whole thing, and the child object is attending to these features at different points in time. If you rotated the whole thing, the child would also be seeing them rotated. It would be attending to the B and the A and the C and the D in a rotated position. Nothing's really changed. They're both rotated, and the mechanism calculates the relative orientation of the child and the parent. That was this complicated thing we worked out. The model says, I don't care if everything rotates together as long as the A and the parent have the correct relative orientation. It's the same. Nothing's really changed. If I know the parent object like this one, and I know its orientation, I can then calculate what the orientation of the child object is. That's part of the compositional model. That's already in there.

I have a question. The A, B, C example—the one Scott gave where he rotated the whole thing—how would we learn that change in orientation in the brain? Which change? The one that is being rotated. If it's not a behavior of that object, if it's rotating the whole thing, what am I learning? That's the whole object rotated. It's part of the model. I don't have to learn it. I just apply a new orientation to the whole thing. You just send on a different orientation. So all of everything is getting rotated before it even goes into the column. You just keep making the correct predictions in the object's reference frame, but it's not changing in the object frame. No, the object reference frame is the same. It's just changing the reference frame of the sensor. It doesn't have to be compositional or part of something else. It is compositional—the object is compositional—but everything rotates. It doesn't have to be a child of something else to represent this. If it's a child of something else, then we're learning a behavior, which is what we were just talking about. If it's not, I don't have to learn this in the different orientations. I don't change my model of anything.

If the model is a table with a cup on it, and every time I come into this room there's this cup on the table, this is a child object of the table. Now, if I say, by pushing a button, this rotates like this and goes back, that's now a behavior of the table. The point is, I don't want to learn a new object. I don't want to learn a new model for the table. I don't want to learn a new model for the cup. I want to somehow say the cup has now changed relative to the table and it's back again. We would learn this in six B. Then, I guess, the orientation with respect to the sensor—if it's not part of the table behavior, what I'm suggesting is, the thing we just worked on, the board is saying, okay, two objects: table and cup. They're in this relationship, and we were working through the idea, what if I didn't even know this was a cup? It just looked like part of the table. Then one day I say, look at that, part of the table changed. There would be all these change detectors here relative to the table. That's what we're dealing with.

Now we're saying that when you do this, maybe it breaks out this part of the table model. It's saying, the table's still here, but part of the table has rotated—just append to this part of it, the part that rotated, and ignore the rest. But I can still use my table model, which included the cup. This is a weird example. It's as if I already know what this looks like. It's a good part of the table. It's part of the table now. It's like, I'm going to invoke the model I had before of this thing. It's as if the whole table rotated, but now I have two objects: the current object, which is the table, and now I've broken out a part of the table object as a separate little temporary thing and said, just imagine that part of it rotated now relative to the original thing. We have to be able to make all the correct predictions here. I can't turn this and go, that's something totally new, I've never seen it before. It's part of this table, and now it rotated. I still know what it is, but it's just separated from the table. It's instantiating itself independently of the table, even though it was learned as part of the table.

At this point, the reference frame for the cup and the reference frame for the table are essentially—one's in the parent column and one's in the child column. In the child column, it's the same reference frame as the original table. In the parent column, it's the table. But in the child column, I've rotated the table and only attended to part of it. I know it's weird, but that's what it is. We're walking ourselves into deductive logic, saying it must do this, as strange as it may seem. After a while, you realize it's not so bad. It's not that we create a separate morphology model of the cup—not right away. Is it possible that we are copying? Can we copy the whole model into a lower level? You can't. In the brain, there's no copying.

It's just that it hasn't been a long time yet. But there is no copying. You can associate two things, but there's no way to take knowledge of what a cup looks like in one column and transfer it to another column. They can learn simultaneously through association learning, and there are various things you can do, but you can't just take all this knowledge and copy or transfer it somewhere else. Neurons don't do that.

This is good because it's quick and efficient. The brain figured out how to do this efficiently, and this seems like a good start.

What are we going to call this? Maybe forking? It requires having two models of the object to begin with—a parent object model and a trial object model of the same thing.

It's almost like the lower-level model doesn't even need to learn the behavior. No, it doesn't. It's basically selectively applying a behavior to the ontology of an option. I wonder if that's the way to think about it.

Behaviors like composition require two hierarchical levels.

This would even work in the primary sensory region, like in the visual cortex, where you could think of features coming from the retina. It's a very simple child. This wouldn't apply to color changes; it really only applies to morphology changes, like a stapler opening and closing. It definitely applies to all situations like that.

Does it apply to all being changes, like a traffic light changing from red to yellow to green?

If we assume the whole traffic light changes color and you're just attending to it—do I have to have a child object? There's no new orientation; it's just some feature at the same location that has changed.

This two-level approach would be necessary when one part of an object moves relative to another part, whether it's moving in one direction or changing orientation. If one part moves relative to the whole, then I need this two-tier solution. If I'm just changing the color of something, it's not clear that I need to do that. I don't think I need a separate child object for that. One other thing: I don't think you would actually need to send up a behavior ID. I didn't write that; you wrote that. I'm just questioning what I wrote.

For these predictions, whatever model recognizes the behavior of the stapler opening needs to send down the orientation of that model at a specific location. We have the stapler model as local changes.

Wouldn't this model, instead of having to tell the other one, "I'm seeing the stapler behavior," just need to say, "You should expect the stapler to be in a new location, displaced in this way"? Say it again.

This model recognizes the stapler behavior, I assume. But recognizing the stapler behavior is really only for the purpose of this module itself, so it knows what changes to expect at the next time step. It would only need to send the changes it expects down to the lower model to tell it in which orientation it should expect the stapler to be. I think it has to be more than just changes, because if the stapler is opening and goes through some occlusions and then reappears, this model can handle all of that. I'm just wondering if the lower model actually needs to recognize the behavior of the stapler. No, it doesn't. All it needs to know is how it should orient the stapler model. It just needs to know, "At this location in your object model, you should rotate and expect it to be in this orientation." So this one never needs to communicate the behavior ID. It doesn't have to send the behavior ID down. I thought you meant up. I don't know if it needs to communicate up either. The lower model, at the moment, just the first time we do this, doesn't know the behavior is occurring. It has a signal that says, "Pay attention to this subset of the model," and it should be a dislocation or disorientation. It just says, "At this location in your object model, you should rotate and expect it to be in this orientation." 

You're rotated, you're bounded—don't go out of your bounds.

Here's where you're observing it, so go at it. As the sensor moves, it will automatically change into the local orientation. As I move my sensor up here, it will move; it will just automatically move in the correct direction down here.

Do you follow what I'm saying? I was just having another thought. Basically, the only thing it needs to know is, "Here's where you should be observing this object at this orientation to the sensor." There's a bound—don't go outside of the bound. On the point of the bound, I wonder if it has to define an area or if it's more like location-by-location associations. If you have a behavior that's not just a solid part of the object moving, but more like a string that swings, it would expect different orientations of the object at different locations. Maybe that's too complex of an example. You lost me on it, but it's a good one.

Maybe it doesn't have to convey the bounds. What I do know in the upper model is where changes are occurring at this moment in time. At another moment, I know the other changes, and at another moment, other changes. That's what I know.

Does that have to be transmitted down below? Not necessarily. All I need to do is say, if the upper column determines, "Right now I need you to break off a piece at some orientation," and a moment later says, "No, I don't want you to do that. Pay no attention to that." It doesn't have to convey the whole bounds. It just has to tell this one, "You're here and you should expect this to be on the stapler at this orientation." It basically says, "I'm telling you the location and the orientation to the center." It doesn't actually have to tell anything about other points. But somebody has to know not to go beyond this. If I went beyond that up here, I would have to stop telling this to do that, which this one would know because it has the model of the behavior. For example, if I slip back onto the base of the stapler, then there's no change—basically, go back to the original orientation and that location. Then we're in sync again with the original model.

So, I guess at least we don't have to communicate—no, I need to communicate. That was a separate question. Now that I have a model where it first exists in two bubbles, this can answer something too. We say that every column gets sensory input, every column gets motor input. It's not clear, and I don't think every column gets sensory input. Not every column gets input from V1 or from the retina, right? So it may be that the final predictions might always occur at the lowest level, but we still get input from the retina because there's a motion input. Maybe only the par is projected to V1 and predicted V1 too. My point is, maybe when I learn an object, I'm always making my sensory predictions at the lowest level and making more conceptual predictions at the next level. You would learn optic behaviors higher up in the hierarchy, maybe even in places where we only get movement info input, and we would then apply them to lower-level models that have learned the morphology model.

Maybe what we're saying—let's start in a very small thing. Maybe this is what you just said. I have to assume that if I want to learn some behaviors that change the morphology to two levels, these two odd things have to learn the same object, or they have to form models of the same object. They don't have to be identical, but they have to at least be modeling the same thing in some ways. At the higher level, the predictions always occur at the first one—that's where the sensory predictions occur. The second one makes predictions of what the first models. V2 predicts what V1's going to tell it. But if I want to say what the actual sensory input's going to be, it has to come from V2 to V1 to layer four. This makes sense. The upper level can't make a sensory prediction. The upper level doesn't get sensory predictions. That was the second point. We do know it gets input from the retina, so that's confusing. But what if it was only the magnocellular input, not the parvocellular output?

If it was, it would still work, but all the predictions would occur down below. That's going to be necessary for behaviors, because if the object is changing its morphology, we need a lower level to make the correct predictions based on the behavior up here. The behaviors are wanted here, and then the actual predictions occur down here. That's really nice. Now that we've teased it apart into two learning modules, can we fit it back into one? The fact that the lower one doesn't need the behavior model—it just seems like it does.

I think the way I've come to think of it, V1 doesn't get input from other learning modules. It just gets it from the sensor. When I think about child objects, this is how I think about it: What are the child objects? The V1—those are the primitives that can come from the retina, whatever the retina can represent, which is not very much, but something. We know what it gets: it gets a bunch of center-surround bits, and it learns what the common patterns of those center-surround bits are, whatever that is. It could be little wiggles and curves. This is Bruehlhausen's work, where they showed that cells in V1 aren't just edge detectors. There are ones that respond to grids and checkerboards. It's basically, what are the different primitive patterns that can be extracted from the retinal input? Those are the child objects, and I can't do this mechanism we just talked about. I can't change the orientation of the thing coming from the retina other than just coordinating Janet.

The bottom one can be thought of as doing all the same things as the upper ones; it's just got an impoverished child, if that makes sense. I'm just wondering if this projection, like from the behavior model of this one and where it would expect—how it would need to rotate that—if this projection could be within one column. We have the behavior model that tells here how to rotate the input. So what would that require? Oh, I see what you're saying.

Since we're not re-anchoring, we would be dynamically changing the orientation of a set of the model here. I like that.

That makes more sense. It would be cleaner.

Although the idea that this is a mechanism for breaking out child objects is still a good idea. I still think this is a great mechanism. Seems like I still need that somehow, but maybe not. Maybe I can do all the mechanism you just said. I like this mechanism, but maybe we don't have to—maybe we can do it just as you said. Maybe we can do it all in one column. What this means is that as I move through the original stapler space, I have to know my orientation of the sensor to the big stapler. Now the top of the stapler starts moving, so now my orientation to the stapler is changed, but only for that section.

And then if I go back to another section of the stapler, I have to change the orientation back again. It's not a problem; it just means I have to be constantly switching back and forth between these. We have to do the same thing there anyway. You're right, you have to do the same thing either way. But now we're asking one column to do that. That's pretty interesting. I like it. If we can make that work, I like it better.

It's a little confusing to me. Imagine I have my base stapler here, and now I'm going to try to make some predictions about it here, and here, and here.

This is my original object model, so I have to go from here to there. This location is—oh, there's another problem too.

It's hard to imagine how you do all that. It may be hard to do here too. Having a model of the object behavior, like how little movements or changes occur over time, doesn't directly tell us at what orientation you should expect the stapler top to be in. Just because I am at this point in my change sequence doesn't directly provide me with a location or orientation for the stapler. We have a problem going from points of change to projecting orientation. But it seems like there has to be a solution to that.

I'm not sure we know what the right representation for the change is. We just know the model's going to contain points of change, but what it's actually representing at those points of change, I don't know. We're imagining it's some movement factor, which makes sense, but maybe it isn't that. If it is that, how do we go from that to knowing the new orientation for the whole stapler or something like that? Do you have an idea? No. Just trying to—actually, I have an idea. Go ahead. We have to solve the orientation problem anyway. Imagine you're looking at the whole retina and you rotate your head. We've hypothesized that there are flow bits out there that can detect that pattern, different from eyes moving left and right. It's a sort of circular motion.

Our theory says if I rotate my head, I know my orientation has changed via the movement, and therefore I can compensate for it. We've already hypothesized that there are a bunch of flow bits in the retina that are translated into changing the orientation of the sensors to the object. But it's changing orientation between two things.

Whatever that mechanism is, wouldn't it also apply to the top of the stapler? If I was just looking at the top of the stapler, wouldn't those same flow bits be able to determine the orientation of the stapler is changing? It's the same problem. It's not as global of a flow, but it's a local flow. So you're saying we have the retina, which detects global flow—movement of the sensor. We would have more local flow, which would detect movement of the stapler top. And then we would have very local flow. Imagine this: if I had a little feature, like an emblem, and it started rotating, the flow bits would be very strong right there—everything moving. If I'm looking at an edge of the stapler going through here and it's moving this way, my flow bits would be very marginal, because this flow here and this flow here would be almost identical. This would be slightly smaller, but that would be the motion. Locally, I can't tell this, but I still could have all the flow bits relative to that stapler top. If all the flow bits relative to the stapler top were not sufficient to detect a change in orientation, I'd probably have trouble recognizing it initially.

Maybe I'm just saying, can we use the same mechanism we assume exists for determining change in orientation of the whole world to the sensor—can that be used here, the exact same mechanism? I can imagine some situations where it would be very difficult, other situations where it would be very clear that it would work. When you say the same mechanism, do you mean the same mechanism to detect the change in orientation or to compensate for it? Detect it.

I think it has to be the same mechanism. The question is, if this column models the stapler, how local is the flow that it's detecting? Is it actually detecting quite large objects moving, like the whole stapler top, or is it detecting very small parts of the stapler? Imagine you have a field of flow bits at the bottom of the stapler—those flow bits would be much more clear, changing in orientation because they'd be real circular. Up at the end here, locally, they wouldn't be, because locally the end's moving at almost the same rate. Locally here, it would look like it's just moving. It would be obvious that it's—I'm just—that would be true if I'm looking at the whole visual field. How do I know the whole visual field is rotating? Probably there's some point in the visual field where the flow bits are pretty close together and I can detect it locally, but from a distance it might be hard to detect. So it's a question of how big of an area you pool the flow over to get it. In the case of the stapler, we can't pool over anything larger than the stapler top. The sensor could, but there's nothing else moving relative. Those are the only things that are moving. So if it's way too large, it just wouldn't say that something's moving.

When we first extracted these movement vectors, I at least assumed those are extracted from very local flow in the patch. Extracting that there's an orientation is because I'm sensing something like this, but that works in some places and not in others. If it's really far out on the staple arm, local information is really not enough. Again, the question is where do we have that large patch that pools, and how big is that patch? I would assume it depends on where this column gets input from. If it gets input from a large low-resolution patch or a small high-resolution one, or maybe from a bunch of columns looking together. But I would say it's all the same mechanism. It just really depends on where the column gets its input from.

Imagine a whole bunch of columns together, some looking at one end and some at the other. Together, they have enough information to determine rotation.

If we have just one column looking at the end, or a couple of columns at the end, I won't be able to determine that it's not movement or just translation, but actually rotation. Maybe the solution requires that as I raise the staple top, I look at the hinge area and see that it's rotating. I apply that rotation to every part of the object somehow. It would be very hard to have that as a necessary condition for each column, that it gets exactly the right size input to determine this. Multiple columns would need to collaborate somehow.

It might be possible with a single column by bringing it down to the hinge and observing rotation there, but learning it that way would be extremely difficult. From a practical point of view, you might want multiple columns looking at this. I can always get it to work with a single column, but when these movement behaviors are involved, I don't have time to sample everything as the behavior unfolds. I can only sample different points in space and time, which makes it difficult to build up a behavior. It might be very hard to learn it. If I could look at all the pieces at once, and these columns coordinate, they could help train each other, but I don't see how that solves the problem. It helps with practical inference.

The problem is knowing how to rotate the model of the staple top based on flow. If I already have a mechanism to detect rotation from flow, the whole basic model requires that. If I can do it over the whole retina, why can't I do it over a portion of the retina? In general, the information is there. We didn't explain how we did it in the whole retina; we just assumed it was done somehow.

What's going into the cortex is, as far as we know, purely center-surround changes. None of those are sufficient to detect anything else at this point, other than an edge moving in a certain direction. Somehow, the cortex or thalamus takes all that information and understands that the whole sensor or a patch is rotating, even if a small section doesn't look like it's rotating. There's already a collective mechanism determining orientation change, even if it can't be observed locally. I'm assuming the same mechanism here without specifying what it is, but now it's looking at just a portion.

I don't know how you would coordinate that if every column just gets input from a patch and can't pick which patch. Let's be careful. What do we know? This is general neuroscience dogma: you have a column, and the cells in the lower parts of the column are directly sensitive and prefer very large receptive fields. These cells respond best when there's a huge thing moving, even to random patterns. I'm almost certain I read that, though Neil couldn't find the paper. It's basically saying that somehow, already in a local column, we have cells detecting large moving objects.

They respond less as you get closer, but still prefer bigger stimuli. Up higher, they prefer shorter ones. If they can detect that, I don't know how. They must be getting converging inputs from magnocellular cells, a whole bunch coming in, and somehow figure out that the orientation is changing. Maybe something subcortical is doing that and sending an orientation signal. That could be true. As far as we know from the retina, it's all just center-surround cells, so maybe someone else is processing that. The only other thing that could process it would be the thalamus, but my assumption has always been the cortex. Somehow, it takes that same field and figures it out.

Now we're just asking it to do the same thing over a smaller part of the visual field, limited to whatever the staple top is. It's hard to imagine how it would always adjust the size to the object or behavior it's recognizing. How would it pick the bounds? Otherwise, it could only learn objects of a certain size.

At any point in time, the model predicts changes at multiple points. It's rarely just one point; there's a big cloud of points outlining the morphology of the upper part of the staple.

What if, instead, I only look at the movement vectors and velocity at those points? Is that sufficient? What if I feed only those points into the mechanism for determining motion? Would it get the right answer? Instead of the whole world rotating, I'm just looking at some sample points and seeing if they're rotating. It's the same mechanism, just subsampling. That seems necessary.

A single column would have trouble with this because it can only look at one point at a time.

I don't know how that works. Can you repeat where that would be happening—the subsampling? Imagine the retina with mag cells all around it, each detecting change. I take that mass of cells and bring them into a system where I can look at those cells and determine if the system's orientation is changing.

I'm still thinking abstractly. We don't want to look at the entire retina; we want to focus on the subset where the change bits are happening in the behavioral model. At the moment, I have a bunch of change bits in my behavioral model. I look at the changes occurring on those bits—it's a pattern. I'm not looking at all the bits; just those aligned with the change bits on the stapler top. The stapler top is moving, so I have change bits there, and I want to calculate using only that subset. I'm subsampling the entire space, only looking at those bits to determine orientation change or movement by examining a subsample—only the ones aligned with the stapler, wherever change was occurring. Who would communicate that subset? Where is that information? The problem is a single column would have trouble doing that.

You could imagine all those ganglion cells projecting onto a single column to do global orientation change or global movement. I can imagine that, but I can't imagine a single column selectively looking at different bits based on parts of the model it isn't even observing. The model tells me, if I go to this point, I'll see this change; if I go to that point, I'll see another change, but I'm only looking at one point. All I know is the change occurring right here, because that's all I'm observing. Other columns nearby would be looking at other points. There's a constellation of columns looking at the whole retina, and collectively they could say, "The parts we're attending to..." It almost seems like we're moving the behavior model into this pooling mechanism—not pooling, but selective multiple columns. At that point, we don't even need the behavior—actually, we do.

How do we do this pooling without a reference? I didn't use the word pooling. Why are you saying pooling? Where's the pooling? Maybe I sloppily said pooling.

At least it's like applying an arbitrarily shaped mask onto a larger field of view. Imagine we have a retina with all these bits. At any point, any translation of the eye to the world would result in every one of these bits moving—either all moving the same direction (eye movement) or showing a rotational pattern (eye or head rotation). The eye is rotating relative to the world. That's what I meant—head movement, not the eyes.

It's clear the brain can figure this out and bring it to the cortex. That's part of the theory.

A single column attending to one spot isn't sufficient. But it could be that the column is also getting input from a much larger area via the magnocellular cells. That would be evidence that, while I'm only detecting a feature here, I'm detecting motion over a much larger area, so I can know how my patch is moving relative to the world. I bring it into a big spatial pooler. These receptor fields are quite large—a single magnocellular cell represents a large area, while a single parvocellular cell covers a small area. I don't need a million axons coming in; maybe a thousand or two thousand are sufficient. Given that wide input, the column can determine how things are changing.

Now, we're trying to do the same thing, but focusing on the stapler top. Only look at the magnocellular cells in the stapler top, because the others aren't changing. The only ones changing are here.

Isn't the column sending signals back to the TRN, which inhibits parts of the column? We don't know what the TRN does, but it's a good hypothesis that the TRN, presumed to be related to attentional mechanisms, could play a role.

In my mind, attention has always been about restricting your area of input and trying to recognize only what's in that area. It's like anchoring to whatever feature or object is there. If it's coming from layer 6A, or if it knows what the object is and what poses it can take, it could theoretically restrict what the thalamus senses. But in our current model, how would I tell it to restrict to the stapler top? It knows what the object is and what it's supposed to look like, but the column wouldn't be able to take that associative model between features and locations and know how that would project onto the retina. 

What could happen is you have all these columns looking at the stapler top. Each one independently looks at the stapler top, knows where it is, and knows what change to expect. If something is happening as a whole, they could project—now this is a different picture, involving the thalamus.

Forget about the retina for a moment.

There's the TRN. Each column says, "I'm on an object at some location." I don't know why it's telling you that, but it's possible. These connections are generally topographically mapped, I believe.

Could that—what's it telling us? Just restrict where it might say, cut out the input from anywhere else. Basically, because it's a loop, it says from 6A it goes to the TRN, which inhibits the LGN from sending it back. So it says, "Pass information from these guys, but don't pass any other information." It's like saying the world is occluded right now; all you can see is the top of the stapler. The attention is on this. 

I did this thought experiment: I had a picture of a street scene, and I had everyone stare at it. They couldn't see the screen. I flashed it in front of them, just like a flashed inference. "Oh wait, that was the street." Same thing. Then I asked, "What was at this location down here?" They had no idea. So I put a little rectangle and said, "Fixate on the center dot." Everyone was staring at the center point. Then there's a little rectangle, and I said, "Don't take your eyes off the center point, but attend to that spot." 

I did this from an eye test. They make you stare at a red dot in the middle and click whenever you see something in the periphery. Usually, things have to be changing. This is different. I said, "Attend to this rectangle, but don't look at it—mentally attend to it." Then I flashed the exact same image at the exact same time, and they said, "Oh, there was a dog lying down there." They had no idea there was a dog in the first image because they were attending here and saw the whole scene but didn't get any of the details. Then I had them attend to another spot, and then back again to the street sign. They couldn't see it; you wouldn't see any of that until you attend to it.

My point is, if you have this ability to, even while you're looking here, isolate a subset, and if you attend to that subset, you don't see the rest of it—you just see what's in there. It was like, "Oh, there's a dog there." Everyone saw the dog, but you didn't see the dog in the beginning. Normally, you have to attend to different spots, moving your eyes around. You can just attend to one thing at a time, and then you see that thing. That's basically what you're saying here. In some sense, I was telling the TRN, "Don't pass information from anywhere else, just from those locations," and then we'll do inference.

The difference here is what object you want to separate out, because that signal is coming from the column, so it knows, "I want to be on that stapler." In the experiment, you're fixating on the angle; you don't know. Here, the dog is here. What the system might actually have is changes at these locations, which define a boundary of something. I'm not actually telling the TRN what it is. I don't think the TRN can know; it doesn't have enough neurons. The bounds for the column are known because the column knows the object. Right now, the bounds are being determined—we're trying to determine a subset of the object. All we know is that these dots are changing; that's the information we have. You and I can say, "Oh, that's the top, that's the stapler," but at that point, the signal for change is not "stapler." The signal for change is "edge," not "stapler." A set of edges is a stapler. The change is not behavior; the set of changes is a behavior. The set of changes would dictate a morphological region, in some sense.

So, this is what I did with the dog: it's saying, "Attend to this region here, which is bounded by changes." Then, what you would infer is, "Oh, there's a stapler there at some orientation," or something like that.

I'm going to have to go on the first line. That's fine, I can still talk, but do you want to do one more thing before I go? Do you have something? No, it just goes on forever. Chance doesn't stop, have you noticed that? I was thinking this morning, thinking through that weekly wrap-up, all this within a year, and that may not seem fast to you, but it seems super fast to me. That's unbelievable. We could actually do this. I think things are accelerating. I actually took a huge step forward this last week. If a third of the cortex is doing modeling of morphology, we just added another third, and then there are a few more things we have to do. We have to do all kinds of behaviors. That was big. We're going to do more. I want to accelerate because we're getting close to the end. You don't want to stop; you want to keep going. Go ahead.

Help me. I don't have any ideas. I'm just trying to—I'm not sure why. You can text it like I do. I'm just kidding. Just think about things in a different way. I just need some help translating.

In this simple column, we have some change section bound to location and orientation. For whatever attribute, there is some change dependent on location. It's observed at that location.

We're inferring an object. I'm assuming we've already inferred the object, like you're looking at the stapler and it starts to move. Is that right? Is that what you're saying?

There's a change that's dependent on location and orientation. It makes a big difference if we know what we're looking at or if it's just random. That adds some location, orientation, and also time. Part of the change could be orientation. We'll start by saying there's a change at some location. It may not have an orientation; it could be color or something else. Orientation is one possible change. The orientation of the sensor to the object is not always relevant. In the case of color, the orientation to the sensor doesn't matter because it's just color.

Is the location in object space? Yes. Then you don't have to mention orientation because we've already taken care of that. The orientation was typically referred to as the orientation of the sensor to the object. If we are locked onto the object, we don't think about orientation.

The problem we're trying to predict is: if this change occurs, how do we know that the color doesn't change? This orientation, in this X, Y, Z, the change—basically, we want to know the set of points that is a subset of the object that is changing and that we know was observed. That's an observation. We want to know that this is changing. Does that affect other attributes like color? We know that some changes occurred at some locations, and we will note what those changes are. If it was a change in color, it would be in the color minicolumns. If it was a change in something else, it would be in other minicolumns. We've observed either a color change or not, or an edge moving or not.

The problem we want to find is: how do we know what this subregion is? By the very nature of our definition, the system is detecting changes, whether in color or something else, and those changes all occur at some location. By definition, the points in this behavioral space—the points of all changes occurring at this point in time—would define a morphology. Even if the whole object changed color, at least the color change is occurring everywhere.

It is the very nature of detecting a change that determines the subset of the object we need to attend to or concern ourselves with.

Does that make sense? I'm just trying to think if that can be done. The thing we're struggling with now is that a single column can't imagine all those changes at once. Its model has all that information, but it can't process it in real time fast enough to keep up with things. It could do it when we're static, taking all the time in the world, but if things are changing, you don't have time to sample everything. You have time to make predictions about everything.

I don't have a solution. The translation you described is correct. That's the problem. I feel like we're close to a solution to object behaviors. What is the part that you would want to know more about right now?

I feel like I haven't been able to really follow this idea, but that's maybe also because I haven't had enough coffee. My biggest question is: how do we figure it out, since a single column cannot see everything? How do we know the subregion of the project? Let me just pop up to a higher level. We've always focused on a single column because I believe single columns have to have incomplete models of objects. We have to understand that a single column can't understand everything. That doesn't mean we can make practical systems with a single column. Even when we're learning the morphology model, this idea that we're going to trace over the whole thing works, but it's slow. Maybe you already have this working in mind, so you can look at it all at once with a bunch of columns, and it seems like the columns have to inform each other. Somehow I have to be able to learn things about a model that I've never observed, because it feels like the column can't observe everything, but it needs to learn models even though someone else observed something. There has to be some communication. This becomes even more important when we're dealing with behaviors, because I don't have as much time to do observations, and there's a larger space to observe. It's four-dimensional now, with time added to three-dimensional space. It's really hard to build up a concrete model when everything's moving. I can't see all the different changes that are occurring, and I'm going to miss things.

It seems clear that multiple columns are going to get involved in both learning, inferring, and predicting, and I don't know how that happens. I can imagine voting on behaviors and inferring quickly by doing that, but I need more than voting. How do I train a column? How do I learn something I haven't observed?

Why would you not? You mean applying where in a situation? I can't imagine every column—columns can vote, but how do I learn a model? I have trouble imagining every column can observe all parts of a model. Maybe it involves hierarchy. I don't know. Somehow models have to inform each other. I don't know why we assume that for the morphology models. What do we assume during learning—that the column has even all? Yeah, but it has time to do it. We can take all the time in the world. It works. You said it can work, and I think it can work sporadically in the behavioral models too. If I had a really fast sampling rate and could sample a hundred points a second and move around, I'd catch everything. But it doesn't work like that. So theoretically, yes. Or, as you were saying, if we keep looping back the behavior, then we do it again, which I think we even have to do, even with multiple columns. The first time a child sees all this thing move, it doesn't get it, then it does it again, and again, looking at different parts.

So maybe I don't, but I still feel like on the behavioral part, I can't sample fast enough to accurately build that model. In theory, yes, I could do it quickly or over and over again. If I had a perfect time signal and a perfect space signal, and knew exactly where I was at every point in time, I could just sample over and over and fill in all the pieces of my model. I get it—it might take days or hours to sample over a complex behavior to get all the points. It doesn't seem like that. I'm not sure. It's not like we don't have a lot of time to learn the behaviors of the world. There are not that many complex behaviors we can actually model accurately, or at least we are not. When have you last learned a totally new behavior that you couldn't take an existing behavioral model and just apply it? Every time we work on neuroscience, trying to learn things we've never done before, that's why it's so hard. But that's still—you don't have to integrate over a large area of many different things moving together. In some sense, it's like that. I can imagine learning a simple behavior, like color changing on an app. That's easy. Even conceptual behaviors seem easier to learn. If you already have a lot of the parts you need, we need to explain that if we've already done a lot of the components, we're just reassembling them in a slightly different way.

Yes, but the problem we're dealing with here is trying to learn concepts about the brain, and we don't even have the foundational concepts. It's like we're starting as a child; we don't even understand what neurons do and how this works and that works. We're trying to figure out these little basic components and rearrange them a bit, try to get them, rearrange them this way. Every time we read a paper, we're looking at pieces that are in some sub-sample of the bigger space. You say, can I recognize that from this subsample? No. Then I look at another one—can I recognize what's going on from this subsample? No. Can I recognize—no. It's like you bang your head against the wall over and over again, and eventually it works.

I love those analogies. Have you ever seen this little trick? There are a bunch of circles on the screen and some lines through the circles. You look at it and say, what is that? Nothing. Then a moment later, the lines move—all of them move in different ways, and immediately you know what it is. Oh, that's a person walking behind it. The orientations and the lines, the orientations and the movements—the real trick is when you can see the lines and they're not moving, you don't know what's going on. But as soon as they move, you realize, oh, that's a person back there. Before that, you're just looking at a bunch of circles and lines. You don't see anything, and then it stops moving again, and you can't see it.

This is, in some sense, why I bring this up. This is a subsampling of the behavioral mask for that behavior. You're seeing some pieces of the behavior part, some pieces of the motion—not all of them, but enough that when you actually see how they're moving, it pops into place. It's not enough from a feature to recognize morphology, but the subsampling of the motion space is sufficient to recognize the behavior and the object that's creating that behavior.

So this would be re-anchoring in the behavior reference space. All of a sudden, you have enough. This is essentially what I'm getting at. Somehow all this happens instantaneously, and obviously different columns are sampling these things. Maybe they're just voting. Maybe that's all it is—they're voting anytime they look at this kind of stuff.

With fMRI—it's not my favorite measurement technique—but they always point to a higher cortical region for structure, for motion kind of stuff. We do have multiple tiers of higher regions, because we don't have just one. But the problem is fMRI would not show this in a single column and lower it down. You don't see much. If you see something special pop out, you can't say. First, fMRI is a terrible measurement, as you point out. Second, even the null result from V1 or something doesn't tell you it's not happening there. It just says fMRI can't see it.

Now, we do have all these visual regions. What's the purpose? We talked about this, Scott. Clearly, in the brain, you learn things top down, top first, and then you can relearn lower down. But that doesn't mean that knowledge isn't eventually down lower, and it doesn't mean we couldn't learn it lower down to begin with.

Those are not incompatible hypotheses. Our hypotheses here are not incompatible with the idea that you're going at the top and moving down. We just don't want to focus on that because it's going to be learned in columns everywhere, so it doesn't matter. It's easier to think about the lower ones first. I'm just going to lodge my prediction that certain things like this are going to require hierarchy. I'm not going to put money on it, but I disagree. For certain higher-level features, when we get to multimodal knowledge built on a lot of previous knowledge, you're right. But the mechanisms themselves, I don't think require a big hierarchy to understand the basics. Just two levels in a hierarchy. 

But this example itself, I thought that's what the disks and the lines were about—that doesn't require hierarchy, that's just multiple columns. Vivian's not even off of voting yet; she says multiple columns are going to vote. I thought you were suggesting hierarchy. No, I said multiple columns. When you say not voting, when I think voting, I'm thinking flat. The other suggestion is that there's a new mechanism that's not even political. I'm not arguing strongly for this, but it feels like learning has to be spread. There has to be another horizontal spread besides voting. That's what I'm arguing about. It just seems that it doesn't necessarily have to be hierarchy. I wasn't thinking hierarchy. I was thinking purely—there's this paper I can show you, it's on my laptop. They show these motions, these activities that are occurring. You activate a single column, and I mentioned this the other day. They activate this sensory input to a particular column, and you can see the information flows up and down, and then it starts spreading horizontally. Why is it voting? These other columns didn't get any input, and there's a certain pattern in which the spread happens. It could be just voting, but there's nothing to vote on in that case because the other columns aren't getting any input. So, are they sharing? What are they sharing? Is an adjacent column saying, "I have no input," and this one says, "I'm figuring out something, I'll just tell you what I learned. I'll activate the same thing on your column so you can learn the same thing because you're not getting input, so I'll train you on what I learned." Then you might learn it eight times faster or something like that. Once you have a mechanism like that, maybe other stuff is happening. I'm just throwing it out. It seems really complicated to do this with one column in a practical way. Some people call this voting, I think I called it propagating, and now it's a sharing of some information that's not voting. Voting was always about inference. I'm not even arguing, I'm just talking about something that's always bothered me that may or may not be a real problem, which is not about inference but about learning. That's why it's propagating. But it is all about learning. It's not just propagating something; it's about training other columns on things they didn't observe themselves, which would make things much faster if you could do it.

There is an issue of hierarchy too. For example, if I have two columns, hierarchical, and they have different receptive field areas—one's a subset of the other—it would be like two receptive field areas. Maybe there's something going on there too, where even though there's only input here, this one looks at the whole thing, and maybe it's going to tell these others that they should be inferring the same thing. It could be like two levels of a hierarchy. Maybe it's not even necessarily hierarchy. Vivian's playing my role; I wouldn't normally argue that you don't need anything else, just one column is sufficient.

Maybe it is just one column. If it's just one column, maybe we just don't know because we have multiple columns to begin with and just don't know how it is. Let's say it's one column—what that means is somehow we have to be able to explain how a single column, once it learns this model, would be able to make predictions about where the stapler head is at different points in time. Even though it's not tracking that end of the stapler, it would be able to say, "At this point in time, the top of the stapler should have moved up to here. I'm going to make that prediction." That was my point about using the occlusion thing, because I can't track it through that. There's no way anybody can track it through there; it's like I've lost track of it. But somehow I still know where it's going to be, even if I wasn't observing it. They're either voting on state ID, or a single column that's looking at something stationary, while other columns are looking at the top of the stapler that's moving—even that column that's looking at the stationary part would know that the top of the stapler is moving because they're voting on state ID. Wait, are we still voting on states, or just voting on object ID? You could argue that you could vote on an instantaneous moment in a state trajectory. Remember, states are now four-dimensional vectors, so at any point in time it has some sort of morphology to that state, and you might be able to vote on that, but I don't know about that.

All right, I have to go. You have the video. Is it—yeah, I'm talking about this. The points, like if it's static, then we don't—here, I think it's an easy example. This is not what I was talking about. Someone was showing this the other day. Will was pulling that up; he had that in there. No, this is a thing where you've got—should get it posted though. I know what you're talking about. Here we go. I'm just going to find it. What would I see if I go to YouTube and say—

Figure motion. I don't know how many hours that was, but that happened.

You'll get to start the next session with all the questions and objections you have.