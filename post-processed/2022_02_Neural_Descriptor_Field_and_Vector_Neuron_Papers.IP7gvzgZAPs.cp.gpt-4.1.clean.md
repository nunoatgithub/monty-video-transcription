These are papers that Heco originally posted to Slack. He posted the neural descriptor fields one, which uses something called vector neurons. That's a component. We discussed this in detail on Friday at the research meeting.

It's fairly relevant to the problems we're tackling. At the end, there's a question of whether we want to look into these in more detail. I'll go through it at a much higher level of detail than we did on Friday. The discussion on Friday was helpful for me to understand it better. I'll go in the reverse order of what we did on Friday, starting with vector neurons. I have two or three slides on that, and then two or three slides on the other topic, describing it at a high level. I'm not going into any of the math or techniques.

Vector neurons: the title is "A General Framework for SO(3) Equivariant Networks." I had to look up what SO(3) meant—it's basically dealing with full 3D rotations. I'll explain equivariance in a second. They're trying to create neurons that work inherently with 3D data, rather than 1D vectors of numbers, which is what traditional neural networks use. These are not biological vectors.

No, it's nothing to do with biology. It's very confusing.

A classical neural network has a set of numbers—a vector—going to another vector, with matrix multiplication. Instead, they have neurons that are 3D vectors. At each point, you have three numbers instead of one, projecting to a layer of vector neurons, again with three numbers. These are X, Y, Z initially, and they can be whatever later. X, Y, Z numbers come in, representing the shape of your object, and it tries to work inherently with this 3D data. It's like groups of three neurons. The paper creates a toolbox for building deep learning networks that are equivariant to 3D pose and invariant to identity. The point is to create a general framework so you can build all the standard deep learning networks, but now with 3D closure. That was a key focus.

What does invariance and equivariance mean? On the left is a 3D point cloud of an airplane; on the right is a representation of what you think it is. Invariance means the global object pose shouldn't affect the query result. As you move the pose of the object, its identity should remain the same. This property is desired in point cloud processing tasks like classification, segmentation, and surface reconstruction. Here they're showing classification—that's the invariance piece. The equivariance piece is shown here: as you rotate or change the pose, there's a corresponding change in the representation. For example, with a rotation matrix, as you change the 3D angle, the representation changes in a predictable way. The pose of the object is one example, but it could be anything that changes predictably, like morphological changes or behaviors.

For example, segmentation of the object into wing, body, etc.—if you rotate it, the coloring should rotate accordingly. If you want the exact shape of the object in the scene, segmenting it out will change predictably as you rotate. It seems similar to pose, but it's not just pose. Pose is an example; these are both examples that pose would qualify for. They're all tied to pose, but not exclusively. If I know the pose, I can generate these. The pose is the underlying thing that's changing, and you could generate these examples from pose. If I have a model of the object and know its pose, I can generate these. In a more general case, one could argue that behaviors or other changes could also be equivariant, like a stapler opening and closing. That might be considered equivariant to parts of the state because of the movement. Another example of equivariance could be changes in the state. Did they mean it that way? No, they're just talking about this kind of thing—rotation. Equivariance is a generic mathematical concept. Pose is an example, rotation is an example, but the exact pixels belonging to the wing would also change predictably with 3D rotation. The set of pixels corresponding to the left wing changes equivariantly with the rotation.

One idea I'm working on is that a model of an object in different poses is just a way of representing its different states. The question is whether this approach deals with all the states of an object, or just these kinds of things, which in their experiments are based on color. It's all rigid—no behaviors or changing objects. They do have a robot grasping example, but here it's just rigid objects. There's no reason it couldn't be generalized to behaviors.

This notion is very generic.

Depending on how they do it, one approach could focus on pose, while another could be a basic algorithm capable of handling any transformation, showing how objects change. It's more fundamental than just pose because they're creating layers in a network that can vary smoothly with pose. Depending on the network you create, you can apply it to different scenarios. It's a toolbox, not a single-purpose method. It can be used in any situation where you want to treat pose intelligently. For example, imagine an airplane with ailerons on the wing that move up and down. It's still an airplane in the same pose, but in a different state. The question is whether this technique encompasses that. It should. They don't show it in an experiment, but there's no reason it wouldn't work. I wanted to clarify because it sounded like you were saying if I take the airplane and apply a transformation, like lifting or tilting the wing, it shouldn't be equivariant to that. That would be a change in state you can't describe as just rotation. It's equivariant to the angle of the aileron, but that's a specific part of the object, not the whole object. This isn't specific to rotating the entire object. The example and experiments focus on that, but the fundamental technique is more general. It's about generically updating a layer that's equivariant to any angle that's part of the representation.

How would I remember the word "equivariant"? It's an important distinction. When you say "equivariant," I think of something invariant to code or scale. What does "EQU" mean? It means it varies in a predictable way with respect to what you're varying—it's a mathematical definition. It varies equally. That's good enough; let's continue. Much of what we've discussed falls under equivariance.

Their fundamental trick is that each neuron represents a 3D feature, like a shaped feature. It also learns a 3D direction specific to that feature. How do three numbers represent a feature? A neuron represents a feature, and these are its three attributes. It's whatever the network learns—just a 3D point, X, Y, Z. X, Y, Z come from many different points, and the network creates a three-dimensional feature vector. For example, it could represent an edge at a certain orientation. In classic neural networks, you might have features, but now it's in 3D. This could be like a minicolumn feature, similar to the directions of minicolumns we've discussed. The output of the neuron is how close the input is to that feature in that direction. Each vector neuron learns a different direction, resulting in a layer of vector neurons, each representing a different direction. You end up with a variety of directions and features, and the network learns both.

The choice of three dimensions is arbitrary. At some point, they no longer represent X, Y, Z coordinates. I don't know why it has to be three specifically. In this case, they're dealing with point clouds, so the directions are probably three-dimensional. That's a question I had as well—why three? Is it about feature design? I think it's because that's how they check for equivariance: they apply a rotation to the input, and the same rotation should apply to the feature. But that's just in the proof; they don't actually use it elsewhere. It's just to prove equivariance to rotation. If it were four instead of three, it would probably still be equivariant. It's just in the proof.

They create a layer, and as Carl was saying, to prove these things are equivariant, there are other components like normalization layers, but this is the fundamental operation that's different. Is it correct to say that a classic neural network operates over a two-dimensional field, and this approach tries to use the same techniques in three-dimensional spaces? In the past, neural networks worked in two dimensions, but here they're inherently three-dimensional. That's their claim and goal. It's an easy way to explain it, because even in two-dimensional networks, it's often hard to interpret what the neurons represent. The same applies here. You wouldn't necessarily know what they mean. You can use 3D inputs with traditional networks, but you don't get this equivariance. The main contribution here is showing equivariance.

If you want to do anything with pose or recover pose, you need equivariance. The Monty architecture is also trying to achieve pose equivariance; it fits the mathematical framework because we're making predictions based on pose.

If you use 2D images and just two neurons, would that be equivariant to translation, not rotation? No, just rotation. The way they handle translation is by taking the center of mass of everything and shifting it, so the translation part is a workaround. Everything is centered by mass. Is there a convolutional equivalent that would avoid this workaround?

Maybe that would be the way. Traditional neural networks don't take the center of mass; they are sensitive to translation. The whole point of the convolution layer is to achieve translation equivariance by design, but it's limited.

It's a complicated idea. For example, if you show a picture of a dog in different positions, the network will still recognize it as a dog, but it has to be trained on translations. Usually, it's only a few pixels of translation that are equivariant. I didn't know that. I'm not totally convinced by the actual operation of convolution. You have the same parameters applied at different places, but there are a lot of edge effects and other issues. Max pooling helps a little, but once you get overlapping regions and edge effects, it doesn't work as well. As you move up the hierarchy, it only works if each filter in your max pooling has some overlap. If that overlap is exceeded, you lose track of translation invariance. If you look at a single max pool unit and one set of filter outputs, moving it a little within that receptive field is invariant, but that's just a single unit. Once you get to the next layer, it doesn't hold. We've done these experiments before; you don't actually get translation invariance.

That seems surprising, since image networks work so well. You train on 1.2 million images, and for each image, you have to show lots of different positions. They actually do that—manually translating and rotating images. If you don't do that, it doesn't work. That's an example of the same problem. You have to pick the center, the mask, to pick the center. It's a little less of an issue here because they're just dealing with positions. Everything is position-based, not color-based. It's all shape, which makes it a bit easier. You don't need to segment the object from the background, which also helps. You can just subtract the center of mass.

They create these things that vary smoothly with pose, so you can do a lot of things that vary smoothly with pose and don't get confused. For recognition, the invariant layers essentially learn to undo the equivariance and create a more stable representation. That's like a pooling layer in our case. It undoes the equivariance. I'm not going into the details, but they have an invariant version and an equivariant version, so there are two outputs. This is similar to what we had in the cones model, with a pooling layer and a location layer for translation. That was never ideal, because when we perceive something, we perceive it as a pose, not just as an object. We see the details as well. Here, you can read out either one, but they have a separate undo mechanism.

The thing I'm working on now potentially solves this differently. The model of the object is learned through different poses, so you're only recognizing a portion of the model at any time, and that portion dictates the pose. For example, you take a 3D object like a cup and lay it out in a sheet; you're only recognizing a part of it, and the part you're recognizing tells you the pose. It's a different way of solving the same problem.

There are downsides to this technique too. Here's an example of the results. Looking at the top section, these are vector neurons (VN). They apply this to two traditional 3D networks. The difference is that these have equivariance. The dataset is ModelNet40, which has 40 categories. These are inherently 3D networks, but they don't have natural equivariance properties. In the pink section, the first part shows all the rotations they were trained with, and the second part shows all the rotations they were tested with. They were trained with rotations about the Z axis and tested with all 3D rotations. This tests how well the model generalizes to other rotations. The other networks' performance drops significantly, while the vector neuron network's performance is not affected by the additional rotations. The first two networks do well with Z-axis rotations, but fall apart otherwise, unless you train them on all sorts of rotations. If you do that, they get better, but still not as good as the vector neuron network, and it requires a massive amount of training.

Is there any indication that this method requires fewer rotations for training? Yes, it requires much fewer rotations. That's what the middle column shows. You only train it on Z-axis rotations, but you still need a lot of training. That's one downside—it's still a deep learning technique.

I'm not sure what these other methods are. There's a long list, maybe another category, but I'm not sure. They also show reconstruction or prediction results. For example, a network trained on airplanes can reconstruct poses it was trained on, but as soon as you give it other rotations, performance drops sharply. Those are the old techniques. The orange ones, which use vector neurons, work fine and are not affected by rotations.

Generally, they can predict quite well on rotation.

That was the first paper. Let me move to the other one: Neural Descriptor Fields. They create vector neurons that are now equivariant to pose. They actually create object-centric reference frames around objects. The second major contribution is that they generalize to reference frames of completely novel objects within the same category. I'll show a couple of examples of that. What does it mean to create an object-centric reference? Wasn't there object referencing in the previous paper, where you had to pick these dimensions? No, it just did classification, but what were the dimensions of space there? X, Y, Z. But X, Y, Z relative to what? Nothing—it was just world coordinates, not object coordinates. Now they're going to try using object sensorimotor coordinates, creating a reference frame around the object. It's the same idea, but now with an object-centric reference frame for X, Y, and Z. And one that works for different objects from the same category, like different-shaped coffee cups. I'll show that.

One of the big problems here is figuring out the right reference frame when you're trying to infer it—anchoring the reference frame. For example, you have a model of a plane and its own reference frames, but when you infer, you have to figure out what that is. Do they solve that problem? Yes. It's just: given a target pose, find the pose.

One consequence is that you can have poses relative to the object, and even if you move the object around, you can still find that pose independent of how the object is related to you. You use that for gripping—that was the task. The core thing is these two generalize.

Here's an example: coffee cup. Here's a point that's relative to this coffee cup off the handle, and it gives you some representation. At the end, they say: here's another coffee cup that's smaller and slightly different in shape. You take the same point relative to the handle and you end up with the same representation.

Is it just scale differences? There's some deformation—it's unclear. That's one of our questions: how deformable can you be? Here's the gripper—a hand. It's a gripper, a 2D gripper. Let's first assume the demonstration and test are identical.

We can attach a body frame to the object and record the grasp pose in this frame. Here, they're going to grip it. Now they have the pose of the gripper relative to the object in the object's reference frame, estimate the pose of the mug in its new configuration, and execute by moving to the recorded grasp pose expressed in the new frame.

But when the mug has a different shape, this procedure fails, since the gripper must align to a local geometric feature like the rim, and this feature changes location on the new shape. Instead, we must attach a local reference frame that moves with the task-relevant feature. Moving to the demonstrated grasp pose, expressed in the corresponding local frame on the new shape, can then align the gripper to the rim. That's part of their technique: they figure out key features and associate a reference point in the object's reference frame relative to that feature. All they have to do is locate where that feature is. In some sense, that's what we do too—if you're going to grab the handle, it doesn't matter where the rest of the cup is; you have to figure out where the handle is, and then for that. The cool thing is they can do this all relative to the object's reference frame, even if the object is distorted up to a certain point.

Here's an example: an object they start with, and they pick a point on the handle. Now they rotate it and can find that same point on another object that's rotated differently. These colors show how the reference frame moves with the object. They can do this pose and a larger mug pose, and a similar thing with a bowl. They can basically create these reference frame fields around the object.

It's hard to interpret exactly, but they're showing something that's in there, into that rotation, facing the handle. This is the same invariance as before. Would this whole field rotate with the object? In the example of the cup on the left, it looks like it's rotating in-plane, but assume that's just for visualization—through the plane as well.

One big disadvantage is they don't just directly compute things. For every new object and new pose, they have to go through a minimization process, trying to fit the reference frame to the object or trying many variations to find the one with the minimum energy. That sounds like techniques from decades ago, where you did stretchy fitting of reference frames, but now they're doing it in a deep learning setting. Is it slow? Yes, it's slow. The older template-based ones never worked super well. Here, the expectation is that with deep learning, it will work much better, but yes, it's slow. Inference involves optimizing to find the right six degrees of freedom reference frame—rotation and translation. That itself is relatively fast, but the challenge is... I'm not sure how fast. The key part here is morphing the shape, right? No, they don't need to morph the shape. It's a different cup, but all they need is to find the cup shape already represented in their deep learning model. The new, novel cup shape is slow to recognize the pose. The object-centric part is slow. I thought it was modifying to the different shape that was slow, but you're saying it's the pose that's slow. The pose piece is slow.

There are two key aspects here: finding the key points, which is the whole encoding trend, and handling rotation. What about translation? They still use the center of mass. They train on 3D point clouds, taking the center of mass of the point cloud, but they train with partial point clouds. The system learns various subsets of the object.

It's a toolbox for 3D invariant and equivariant network layers—that's the vector neurons concept. You can represent the 3D location and full 3D pose anywhere in the object's reference frame, capturing the full morphology of complex objects. They show examples like rabbits, including local curvatures; it just learns all that.

It can handle some deformation of 3D shape, though it's unclear how much deformation or difference it can tolerate. It works with partial 3D data because it's trained on partial 3D data, and it generalizes to novel instances, but the extent of novelty it can handle is unclear. It's a general technique for 3D data, not specific to vision. It just requires 3D points—no color or other features. Presumably, it could be augmented with color, but the core method isn't vision-specific.

It's unclear how it scales to large numbers of objects. In the last example, they only tried three different objects. How well it deals with noise and ambiguity is also not clear. It's not certain if the exact pose can be recovered; you get a representation that's equivalent, but it's not clear if you can read out precise angles. Training is probably quite slow, and inference may be slow as well.

It's not clear how we might use it in Monty. Maybe individual modules could use these tools—perhaps each module could have something like these vector neurons or something equivariant as a base. We might be able to remove some limitations. The question is whether we should investigate this further.

As I was leaving this morning, I thought about the Columns paper. We could do something similar—just replace the location signal with their signal, since we never really had an object-centric location signal before. That's one possible approach. This is related to the Monty architecture, but a simple way to think about it is to change the location signal to this representation. I don't know if any of this would work in our context, but that's what occurred to me. Another thing is that the location could be a sensorimotor input. This gives a way to describe local curves and shapes. It might be a replacement for this as well, but it doesn't have color or similar features.

Is this field new and hot, or has it been around for a while? This paper is from December last year on arXiv. The previous paper is from June or earlier last year. Is this generating a lot of buzz? Are many people working on this? Did it get a best paper at a conference?

The reason I ask is to gauge how many people are working on this—20, 200, 2000? If a lot of people are working on it, there will be more progress. Or it could be an idea that's cool but doesn't catch on, like some SLAM architectures that were hot and then disappeared. There's another paper from a year before called Geometric Deep Learning, which introduced this approach to constructing architectures with different types of invariances and equivariances. This seems like a special case of that program. The geometric deep learning work made a big impact, so my guess is this is an example of that trend. It's a special instance of creating networks with certain types of invariances or equivariances, and we'll see more of them in the next decade.

One could say maybe all neural networks will go in this direction, since the field of AI will have to be built on these concepts. It's hard to say—this could be infused throughout everything, since the world is moving toward 3D sensorimotor modeling, and this could be the basis for artificial neural networks, or it could remain a subfield. We don't have to know the answer now.

I've been talking to Scott Purdy at Zoox about self-driving. They deal with point clouds constantly and use a precursor to this. In the self-driving world, they could gravitate toward these kinds of networks. There is slow interest.

It needs to be sped up. But no one would use exactly this technique; it's going to evolve. Deep learning is evolving so fast. The idea of infusing 3D into the networks in an inherent way is a big deal, because that's already being done in self-driving cars. They use a precursor to this called pointnets, which deal with 3D points. The new thing here is the variance piece. In some sense, sensorimotor learning is all about 3D space, modeling, 3D structure, and how we'll structure and model more. That really hasn't been the center of networks for a long time, so it should be moving in this direction.

One interesting question: if we could define what a learning module has to do in Monty—what the inputs and outputs have to look like, and what it has to accomplish—then that might help answer whether this approach is useful. Could a team go off and use these techniques? We can only specify it at a very generic level; we can't say it has to be exactly this or that, because these systems will just learn representations and it's hard to specify ahead of time.

It may be that we can recover the exact pose, for example. Right now, we'd have to do some work to get it to output the correct pose, but we could say representations of the pose have to be consistent for consistent poses. We could specify it at that level. Maybe that's too simplistic, but in my mind, there should be a specification for what a learning module has to do in Monty. In some ways, we're indifferent to how you go about doing that. Some methods may be better than others, but if you can define the interfaces, you can have flexibility in between. I don't know if we could do it at the level of Monty modules; it might be the entire hierarchy. We could define a single learning module and say, for example, you want to recognize a certain object and a certain pose, and expect each module or column structure to do that.

There's still the issue of training scenes where you have to manipulate objects or use a rotator. You don't have to rotate them too much because it's equivariant rotation; you just have to show lots of subsets or deformations. If you can't see the bottom of the airplane, you won't be able to recognize it. You have to move things around, and that's true for us and for this approach. You wouldn't have to do 2D rotations; you would only have to show the other side, or maybe three or four angles. That's okay too.

This relates to another question: why do you say training is slow? Do you have specific numbers? It should be fast based on what you just said. It gets back to what Lucas was saying. I should verify this, but my intuition is similar to his: you have to show a lot of partial subsets of these point clouds, and a single image presentation could be training.

I read this paper, and they were vague about having code trained. It seems that a lot of training is just interesting, and these are complex networks. The second paper would be much more data- and time-intensive for training. Even for three objects, they need a hundred thousand examples to train. The first piece is relatively simple, but the second one doesn't even discuss training code; they just assume you have it. They need an encoder plus training on top of that.

We could try what you're suggesting: each module will recognize full objects, and maybe we could try small modules looking at small patches to get local shift features. That's one representation. All the stuff we've discussed about curvatures, smooth surfaces, and displacement is already captured here. We wouldn't need to build a graph; it incorporates all that, but it's an alternative. We could still go through parallel approaches. It's a very different mentality than what we've been talking about.

So, should we spend time on this or not? What's your intuition?

I think we should spend a little time. It's so close to what we're trying to do, and there are so many unknowns. Maybe it fits very well; we could at least see if it fits and if we can specify some sort of interface, possibly with some variation. Instead of having an encoder network that maps the whole point cloud into a major representation, we could have multiple local descriptors around the object, multiple local curvatures, each with a reference frame, and then build a graph from that.

The key thing about Monty is that it's a platform for sensorimotor learning. There are capabilities we want to achieve with sensorimotor learning and interaction with objects in the world. It's hard to know if you could plug in a completely different learning system and still achieve the sensorimotor learning aspects you want. The trick is, if you're going to investigate this, it could become the tail that wags the dog. You don't want to just run with it and lose sight of the bigger picture.

That's the issue. I think that's the problem; it's always the risk of working on something like this. We might just say, "Let's investigate this," instead of considering what we're trying to achieve overall. Does it help us do that? Still, it seems you don't want to discount it. It seems pretty important—potentially significant additions in normal networks. We could try feature learning for local curvatures and see how this would go. It would be learning invariant curvature features. Maybe that's the start. That's a good idea.