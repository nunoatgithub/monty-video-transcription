Encoding object similarities in SDRs is the work I did for my internship. I'll go through motivations, goals, and limitations, then present toy examples of how I compute similarity from evidences and encode those similarities into SDRs. Towards the end, we'll cover schema and review synthetic and Monty results.

In Monty, we have sensor modules that make observations of objects, like a coffee cup. Regardless of whether these are distance or surface sensors, they extract features, which are later converted into observation space—such as patches, RGB images, or other forms. These are then transformed into features like hue, point normals, or curvatures by the sensor module. This allows the learning module to take those features and apply similarity metrics, which is crucial for generalization.

Without this, recognition would be brittle, and we couldn't apply tolerances to features. This is important for creating a modular structure of LLMs stacked on top of each other. The higher-level LLM needs to process whatever is sent from the lower level and apply similarity measures so it can generalize to different object or feature variations. This enables Monty to generalize features and recognize, for example, a cup, even with different lighting, features, morphology, or point normals, by applying tolerances and thresholds.

The problem in compositionality arises when we add another layer on top. What do we send to this higher level? The higher level looks at objects in a scene and knows the relations between them, such as plates, knives, and utensils. It recognizes objects, but what is an object in a higher-level scene, and what does it expect to see?

We have different options for what to send up. We could send the full graph, but then the higher level would have to figure out how to apply similarity metrics between the learned graph and the one being sent, to apply tolerances. Otherwise, it couldn't generalize to different arrangements. Sending the full graph—all location-feature pairs—has never been seriously considered, as it defeats the purpose of the learning modules. That would be a naive and inefficient approach, not forming representations that are constant over variable inputs. If the higher level could compare these graphs, it might functionally work, but it wouldn't summarize objects into a single representation.

We can't just send the object ID, as that assumes all objects are equally similar or distant, so we can't apply similarity metrics to those IDs. Sending 1, 2, 3 doesn't tell us how 1 and 2 are related.

I'm going to push back on that a little.

You're showing several things here in ways I don't usually consider. You say you can't pass an object ID, but it's not clear why. The point of this exercise is to ask if object IDs can be represented in a way that encodes similarity. When I say object ID, I mean an SDR, not an integer. In our theory, object IDs are always SDRs, never just integers. In some versions of Monty, it was integers, but that's the baseline we're trying to improve. If we use a PID that's an integer, it wouldn't be helpful, as it can't determine similarity. For object IDs as integers, there's no similarity embedded—just separation—so we don't know if object ID 1 is closer to 2, and so on.

What we want is a representation that encodes similarities, so that two cups are closer together than they are to the cereal box.

If we replace those cups, for example, and it senses cup two versus cup one, the scene has some encoded tolerances so that it doesn't really matter—it's close enough to what it has learned. It might not change the scene, but if we change it to a cereal box or something like that, it would need to update. These things have to be co-similar, but it's not clear that's always the case. We've discussed various ways of doing this. A model can consist of the morphology of an object, which is independent of the specific features of that morphology. I was talking about orientation, which is part of morphology. Just the orientation of features can be enough to recognize and see similarity between objects. What you're recognizing could be both morphology and specific features—there could be two different things being represented. It's not necessary to have a representation that encodes both.

That's an assumption I'm not sure I agree with. When we get to the limitations, I'll explicitly say this inherits the limitations of EvidenceLM. When we do evidence matching, whatever extra evidence we accumulate goes into the similarity. When Vivian did this, she said we have features we can use to add evidence, or we could just use morphology or whatever is being used to add evidence. This is what we use to create the similarity scores, so it inherits the strengths and weaknesses of these evidences. Here, we're only sending one SDR representation, which includes whatever is being encoded in the evidences.

Even if we had something to merge these two representations into a generic mug morphology, which we probably do want to add, it's still useful to capture that a mug is more similar to a coffee cup or a soup can than to something totally different. Pooling together morphology models into a single one doesn't solve the entire problem. Wouldn't the morphology model capture the similarity between those different mugs? The graph does. Maybe I understand what you mean by morphology model in this case, but that's essentially what we're trying to encode here. Morphology is basically the information that indicates the object exists at this location in this orientation. We also discussed that the sensor module would usually send a skip connection to the higher-level module, so we would get some point normal information there, which would be similar between the two cups but not to other objects. There has to be multiple types of information sent up. We don't understand all of it, but we've framed some of the constraints on this problem. So far, the presentation doesn't adhere to those constraints, but that's okay.

Some clarifications: we do not assume that when we encode similarity in the SDR, we are only encoding one type of information. What we send up is an SDR that combines morphology and, depending on how we accumulate evidence, can also combine features. You send up one SDR and talk about how to encode things in that one. If we change the way evidences are set up, could we change what gets encoded? If we had different types of evidence or information, could you apply this technique? Yes.

In EvidenceLM, sometimes we can ignore features or decide not to use features like colors to add evidence. If we don't use those, the similarities we encode will only consider morphology. It would be easy to accumulate those separately and have two different confidence scores: one for morphology only, and one for things like color and lighting. Whatever we accumulate as evidence gets translated into similarity in the SDRs. In the current setup, Monty tends to predominantly rely on morphology. If we look at how it weighs different objects, it already assigns a lot of importance to morphology, so we just take those evidence-based similarity scores and see how we can turn them into an SDR. EvidenceLM only adds evidence when there are good features, but it does not subtract evidence. Morphology is treated differently than features, and it can recognize an object just based on morphology without the features matching.

In the SDR that we send up, we do not assume that a specific bit has a specific meaning. We don't say that bit five represents a cylinder-like feature, but similarity is represented as a distributed code across multiple bits.

The similarity itself is distributed across the bits, but there's not a specific feature for every bit. Hopefully, that somewhat addresses the capacity issues, including similarity.

In what follows, it might seem like I'm doing some sort of deep learning, but I'm only using optimization algorithms like gradient descent. I'm not actually doing any deep learning to encode similarities into SDRs. I'm not even using a single perceptron in this model, so there's really no learning, and it's very quick because there are no weights to be learned.

The goal is to extract similarity scores from evidence. We want to look at the evidence as we examine a cup during a sensory motor experiment and extract similarity scores from that. We also want to precisely define evidence scores. When we look at an object and have other graphs in memory, we can accumulate evidence on the objects in memory and the current object as we go through it. It's basically your confidence in the set of objects you might be looking at. Evidence scores are scores for the different objects that might be hypotheses.

We're getting SDR representations from the scores accumulated from the similarity we found. These are two parts, and I'll go through them in detail. We want to continuously adapt the SDRs to new similarity targets. In Monty, when learning from scratch, we keep adding more objects to the graphs, so we want those SDRs to continuously adapt as the similarity matrix grows larger. The object SDRs should continuously adapt.

We also want to maintain some stability between the old objects and SDRs saved in memory and the new objects we receive, so that higher levels expecting object representations don't see them change too much and can still represent the hierarchy.

We want to maintain useful SDR properties. We did some testing on that, but more is needed. There are limitations: if evidence LM is sensitive to scale variations, for example, two objects with the same morphology but different sizes—a small cup and a large cup—will not accumulate evidence for the large cup if the small cup is a different scale, and that carries over to the SDRs. The similarity between them will be small in the SDRs.

If only morphological features are considered, then color or non-morphological features will not be represented in the SDRs. We still need to store these graphs in memory; this is not a way to get rid of storing graphs in memory. We will still have those stored. What we're changing is what we pass on to the next level. That seems obvious, so I'm surprised it wasn't mentioned. How could we not store the graphs in memory? That would mean not knowing anything. We're not using SDRs to store the graphs; we're keeping whatever was there before. After we create object SDRs for the graphs, we still have to keep the graphs and only use the SDRs to send to the higher level. But isn't the graph the memory, not the object? We're not replacing the graph with this. I can't imagine ever replacing the graph. The graph is the definition of the object, so how could you not store it? I think you would always need to store it, but you're not storing the graphs as SDRs. In the brain, the graphs would be stored as pairings of multiple layers of cells associated with each other over time, and there will be SDRs. But what I meant is that one way of moving toward neural representations is that if we can figure out how these graphs can be represented as SDRs, later on we could delete those. I think that's absolutely impossible. The graph is not a singular thing; it's a set of observations at locations. You can't get rid of that. That's the whole thing. Even if we went to grid cells, at any given time, grid cell activity could be viewed as an SDR, but we would still need multiple sets of active grid cells bound to multiple sets of active feature cells.

There's no way to summarize an entire graph with an SDR. It's more accurate to say we're not changing the representation of the graph. By the way, at least to me and I think to Jeff also, the dendritic synapses stored on active dendrites are also SDRs. The permanent storage is also SDRs; it's not always just dynamics. If Subutai is suggesting we don't have to change the nature of our graphs, you're not doing that right now; you're just talking about creating SDRs. I'm not suggesting we need to change that. This statement on its own makes no sense to me. It's like saying we don't need to memorize anything. Of course we need to memorize things. Maybe just say this is not affecting how we store graphs in memory. You're only changing what we're transmitting from the learning module. This is changing the communication protocol, particularly in the case of hierarchy. I'm not trying to change how we represent graphs. Maybe that's exactly it. That's not what it says, but that's what I meant.

I'm still not quite sure I understand evidence LM, but maybe we can clarify.

Oh, evidence LM is not the part I worked on. I understand, but yes. OK, so how do we compute similarities from different objects? Let's say we have these three objects and we're using evidence LM to do the matching.

We do this matching in a sensorimotor experiment. In each episode, we look at a different object. Let's say the sensor module is looking at the fork right now. Before starting the episode, we have accumulated zero evidence on all objects. This is a toy example, not a real experiment.

After moving through a sequence of observations with the action policy, we accumulate evidence on all these objects. Similar objects will have a higher evidence score than dissimilar ones. For example, we accumulate more evidence on the knife than on the cup because the sequence of observations for the fork doesn't match the cup. In theory, you would do this for all objects the system knows—all objects from all hypotheses—which is biologically impossible. This is just the engineering way of doing it.

Is it a single number per object? Yes, this is the maximum evidence. Every hypothesis accumulates some evidence for every pose and location on the object. What I'm showing here is the maximum—the best hypothesis from every object. At this point, it's a single number per object. It can keep going up, and we will normalize it, but not yet.

We're going to calculate the relative evidence with respect to the object we're looking at, then linearly map it to the range of overlap bits we want. In this example, we're using SDRs of size 2048, with a sparsity of 41. We linearly map the range from negative eight to zero to zero to four, representing the number of active bits in the SDR out of 2048. This is the target overlap.

Wait, not the total number of active bits—it's the overlap. The overlap between, yes. For example, between the fork and itself, the overlap is 41. We don't have to aim for that because the representation will set it. The desired overlap score is the target. The knife would still have 41 bits in its SDR, but it's expected to have 36 overlap with the fork.

That 36 comes from the evidence score. It's not a target; it's what it is. When we encoded, we still haven't created SDRs yet, so that's going to be the target. If I have infinite knowledge, I would know the fork is supposed to have 36 bits overlapping, and then we encode those. The processing determines that. That's what we're getting from the evidence scores.

We can do the same thing for episode two with a different object, filling in the second row. If you're just comparing those numbers, could we convert each number using a scalar encoder and then do overlaps between them?

Convert which numbers? The max evidence score. What we want to create is object representation, not just similarities. The scalar encoder takes a number and gives a similarity score, but we want object representations. The scalar encoder would give you an SDR for a number, but another object could also have 10 evidence and have nothing to do with forks. The scalar value itself doesn't tell us what we want; it's the relative value.

We don't want to encode the similarities; we want to encode the object information. The similarity will be calculated by the higher-level LM. The same applies for the last object.

I noticed these are not symmetric, but they're supposed to be symmetric. Because of how we're doing this, we have to average them to create symmetric matrices. I'm just going to average these numbers. Why is it supposed to be symmetric? That's not obvious to me. In a sensorimotor experiment, it doesn't have to be symmetric because we can take different paths. Inherently, it shouldn't be symmetric. When we encode the SDRs, it's a distance metric, and that needs to be symmetric. When exploring, they are asymmetric, which makes sense. But for the symmetric matrix representation, that's needed for this technique to work. The similarity between A and B should be the same as from B to A. Why do you think they should not be symmetric?

The lack of symmetry is just noise. We're essentially averaging over noise. If you visit the exact same points on the object, you would assume they're the same, but in practice, you almost never observe the same points. Logically, similarity should be symmetric, but not practically. Isn't that what you want to convey? I don't know, but it just wouldn't be. Where does that come from? It doesn't come from observations, because observations wouldn't result in that. This is the ideal, but a real system wouldn't know the ideal.

What neurons are going to look at this and say, "Oh, these should be symmetric"? I don't know. At a certain level, neurons could average over experience to factor out the importance of directionality. If you observe the fork many times and the knife many times, sometimes you'll think of one more than the other, and you'll have this variation on both sides. But there shouldn't be any directional bias. I don't think "noise" is the right word for it; it's just what you sense so far. If you only sense the handle of the fork and think it could also be a knife, that's not noise—it's just what you sense. I guess I just feel like any sense of a one-directional bias is noise, but I don't know. In the real world, this wouldn't be true. In the real world, you would see many forks and many knives, and you could add up these scores over many times that you see these objects. Over time, it will become more symmetric without explicit effort. I think what actually happens is that you learn behavioral patterns that seek out the answer, and you don't come up with these general overlap scores. If I feel something like the base of a fork, I would immediately jump to the top, or it just wouldn't be—it's not like we get all these points and see them all over time. The brain figures out action plans that are not symmetric, but I don't like it. For what it's worth, we are using that policy when we're getting these observations.

We don't really have to do this in a sensorimotor experiment. We can do it on the graphs saved in memory, for example during sleep time for the Monty system. We can parallelize it as well. If you use these similarity scores, you can do some sort of hierarchical clustering. You can see that cups are clustered together, Lego blocks, and so on. I think Niels has shown something like this before. I think it was Vivian who first made this up.

Is this based purely on morphology? It depends on what you accumulate and the evidence. I think it has more weight on morphology than on features because we don't subtract. I'm guessing this is really going to represent the similarity between objects. Often, a small difference in two objects makes me perceive them as completely different. I know they're completely different objects because of that one small difference, and that's the problem I always have with this approach. I have this one bit difference here, this two bits difference, but that's the difference that matters, and I don't group everything together. I sometimes completely separate them out. I feel like that's an orthogonal problem we could solve by outputting in the CCP two SDRs: the general SDR for generalization, and a more pattern-separation SDR that's completely random for every object. That would be like the Numenta logo or Numenta cup. That wasn't stated up front, but that was a possibility. The problem Ramy opened with the datasets is where we're really hoping this will help. For example, we learn a dinner table setup with normal color, and now it's medieval stuff—how do we generalize to that? To your point, we would also have an SDR for "medieval cup," and that way we know we're on that scene. From a presentation point of view, it would be helpful to state that problem up front. Otherwise, I don't know that, and then I look and say it's not going to work. I think it's also hard for Ramy to come in and have ten years' worth of context. I understand that, but I also don't want to sit here and be silent if I don't understand what you're talking about—that wouldn't be useful for anybody. The nature of his work is to focus on one very specific thing. I do understand that. We talk about passing up one thing from one region to another, but that's not what we're talking about, because that's not going to work. You're saying this is an overlap similarity SDR, but it's not going to be sufficient. We'll have to have something else in addition. That helps me a great deal. I also have a slightly different view on the problem—there might be one feature that gives you an entirely different similarity. That is, if you are moving, it still depends on how you accumulate evidence, and that also depends on what motions you took on the objects.

For example, if you're looking at an object and you know it has a handle, and your action policy tells you to check the handle, then you're not going to accumulate any evidence on the other objects. That will be reflected in the similarity scores. The action policy, knowing that this graph is different, will go to the features it knows are different or discriminative about this object. In a sensorimotor experiment, you won't accumulate any evidence on the other objects.

Are you trying to say that the objects will be more separated? Yes, you'll go to the discriminative features because the action policy tells you to. For example, if we're looking at a fork, the action policy should be smart enough to say, "I think I'm on a fork. Let me go to the tip of the fork and test what's there." If we do that, we won't accumulate evidence on the knife, because the difference in evidence will be large. When we encode those into similarities, it will show that they are different, or at least there's not as much evidence. If we want it to be robust and say, "this object is very different from this object," it's still useful to have that as a separate representation. That's just the way SDR bits and SDR representations work—it would be useful to have one that's robust to corruption, but still unique for a specific object, whereas others may have some overlap.

I think you two are talking about different things. For example, you might have a fork where the right tip of the four prongs is slightly dented, and that happens to be my personal favorite. I grew up with it as a baby, so I really like it. No amount of similarity will change that. In my kitchen cabinet, I have a set of mugs I got for my wife's birthday. They're clever mugs, all identical, like camp mugs. I like them, but one has a little chip in the corner—just a slight discoloration. They all have the same design and function, and I use the same action policies on them, but I know that one is unique. That's how I picked that one today. I know it exists, I can look for it, but it's also identical in all other features. That's what I was thinking, just like what Subutai said. It's relatively more identical, but you can still figure out that it's different. In our case, we have this metal thing that fell off during the '89 earthquake and dented itself, so we remember it. It's a memory of the earthquake. My mug example: they're identical in every way except for a slight chip discoloration in one part of the lip. It's a separate object, but it's the same as all the others.

We can have these hierarchical clustering figures. Now that we have this symmetric target overlap, we can encode it into SDRs. The goal is to generate three SDRs with pairwise similar overlap bits between them.

We start with the target overlaps. I'll assume we have these target overlaps. We're going to create toy SDRs—SDRs with a size of 10 bits and overlaps between them. The number of active bits is 3. This is just a toy example.

Along the diagonal, we have 3 and 0, 1, 2. Just a toy example.

We'll start with a dense representation of three objects and optimize these dense representations to give us the SDRs we want. These dense representations are just float values, and the size of each dense representation matches the size of the SDR we want. They're randomly initialized. We'll use gradient descent to optimize them. We take the top k of them to binarize into SDRs, then calculate the current overlap between those SDRs. This is the pairwise overlap. We calculate the error between the current overlap from these randomly initialized representations and the target overlap. It gives us an error. We cannot calculate the gradients or use this error to modify the SDR or dense representations directly, because the top k is not a differentiable function. As I mentioned before, we have a way of handling that, but that's not what I did here.

What I did is move the dense representations further apart or closer together, depending on the error. That error controls the pairwise distance, which is in Euclidean space. If I want the overlap to be higher, I minimize the distance between the dense representations, and so on. The gradient goes through this path and does not go through the top k. Is this clear? Is the loss just one number, or is it a loss for each cell and overlap? This is what it looks like: the dense representations copy the pairwise, and the loss is, for example, making them closer to minimize the distance for the desired overlap. Error is the target minus the current. I multiply these values by the pairwise distance, take the mean, and then backpropagate. You have a matrix over all the pairwise values, and you take the mean because that's how we backpropagate.

It becomes a summary value at the end, but it's based on element-wise values. To reduce the overall loss, we improve each element-wise value. These are proportional to how much error we have. 

We did some results on synthetic data and some on Monty. The synthetic results are useful because we can do things we can't do in Monty, testing the limits of the system.

This is the baseline: we have 10 synthetic objects, these are the target overlaps we want, and this is the predicted. We start from here and are able to match the target overlap by minimizing this loss. It's very nice that we're able to minimize advanced representations, which gives us the SDRs we want with the features.

We examined the relationship between SDR size, sparsity, and the number of objects we can encode. These are the results.

If I fix the sparsity at 2—the number of active bits in these representations—and change the SDR size, we get the overlap error, the average overlap error for all of these, specifically the lower triangle here. What is the average overlap error between this lower triangle and this one? Error here means the error in matching the target overlap, the pairwise error. It's not error in data recognition, just error in the target overlap. In this part of the presentation, it's about encoding that target overlap into the SDR.

Could you go back to the previous slide? What's the input to the system? The input is the target overlap, which we get from the similarities, after it's been normalized to be the SDR bits we want. In the toy example, it's three, but here it's forty-one out of two thousand forty-eight.

That's the only input. It's just trying to find a representation with these overlaps, but does it have to know what object it is? No, this is a very under-constrained optimization problem. We're telling it to find three representations in this space that have these similarities between them. It doesn't need to know anything about the objects; we just want to encode the similarity in those SDRs.

It doesn't look at the actual objects. We only look at the objects when calculating how much similarity should be between them. The representation is based on that.

When you're done, you map the object ID to each of those representations. We're saving every graph with an SDR representation, always refining it with more evidence. So there will be an integer associated with each SDR—a graph, a unique ID, or just the graph itself. For example, object 23 will have this SDR, object 55 will have this SDR. At the end, you'll have these relative overlap scores, and then just send them up. You don't need to say object 23 anymore; you just say, this is the graph. When I recognize this, I will pass up this SDR, which is basically the name of this object. There's a discrete number of SDRs. It doesn't know anything about the properties of the graph. It's not like two similar graphs will have similar representations—only if they happen to have similar target overlaps will they have similar representations. It turns out that similar evidence scores correlate well with similar graphs, but it's not actually looking at the graph, just the evidence.

Some totally different way of doing this might be to try and put the graphs on top of each other, like techniques Ben was exploring at the start of Monty. But this is nice because when we're recognizing objects, the learning module is naturally rotating objects in its mind based on the most likely hypothesis. That's why we get these nice alignments between objects that are not just morphologically similar, but similar when appropriately rotated. If you're learning a completely new object with its own graph, you have to do the comparisons with everything else. This is an interesting follow-up.

At the end of the day, it's just finding ten SDRs that have these target overlaps. That's what we're encoding in the SDRs. You can't do that with some form of scalar; there's a quadratic number of comparisons.

In my mind, what evidence LM is doing is aligning them and testing all these hypotheses, since the hypothesis space includes all rotations. This is what it's doing. It's already been done for evidence matching, so why not just use those scores instead of doing it again? We talked a lot about the zeros last week. I don't know how that's handled here. I have a few active bits. We're going to talk about the receiving end of these SDRs and how that's processed.

Not in this presentation, but I'm working on it. That has always been the problem with overlapping SDRs like this. It's not that you can't come up with a way of encoding it, but the way neurons work in recognizing patterns is problematic. You have to think about, in real neurons, now I have these overlapping scores. We're not going to try to recognize these things uniquely because it doesn't work for that. What does it work for?

You can't have it both ways. You can't say these are all similar but still be able to tell the difference, because if there are 80 or 100 bits on and only two bits are different, the receiving end can't really tease that apart. You can vary the threshold for matching, but if you have a separate SDR, for generalization, you would have some threshold you could adjust. For example, I don't care what kind of dinner table I'm eating at, but generalization means I don't want to tease these apart—I want to say these are all the same. It's the same solution we use for features from the sensor module. The receiving end uses the same mechanism for features like color or curvature, where we say these two shades of red are very similar, so we don't encode them uniquely. We look at the similarity, and if they're similar enough, both add evidence for the model. We would do the same with object SDRs, calculating the union of two SDRs to get a distance, and if the distance is close enough, we add evidence; if it's too far, we don't.

At a higher level, we don't need to distinguish these objects from each other, but that's only at the higher level. If we want to attend to the difference—like this cup versus that cup—we need to go to the lower level. The higher level should only deal with recognizing scenes. In my model, which could be wrong, there are two representations coming in: a mini column representation, which is not very sparse or large, and that's what we call the spatial pooler. It takes many patterns and maps them into a much smaller set, so you get similarities but don't tease things apart. Then you run it through the sparsification process, which, under different contexts, creates something unique. So you need both a unique and a non-unique representation.

Here, you're trying to do the non-unique representation, but with more objects, is that correct?

I'm trying to understand that, but it's done in a way to enable it—it's the morphological, the graph similarities, as dictated by the evidence. I'm imagining region 1 and region 2, or column and region 1 and column and region 2, passing information up. We've already said we need a unique version, which this is not, and a non-unique version. I'm trying to make it all work. The problem with spatial pooler representations is they only include feature similarities, not graph similarities. This will have graph similarities as computed by the evidence algorithm. So, with whatever limitations the evidence algorithm has, this will be able to encode that. In that sense, it's better than the spatial pooler representation. Maybe we can have a reading sometime to discuss where we want to encode similarities, where we want unique encodings, and how best to do that with SDRs, and have Ramy present his slides. I feel like he came up with a really nice algorithm. I'm trying to make it work in my head—how to get the whole system to work, and this is a piece of it. But I can let it go and say I don't know how the whole system will work yet. I think the core is, once you have a set of pairwise similarities, how do you create SDRs from them? That's not an easy problem, but that's what he solved. We can use that technique wherever we want, but I have to be convinced it's usable and understand how neurons will take advantage of it, whether it's practical. Just because it's done doesn't mean we're finished. It's a good, interesting result. It's another way of creating similar spatial pooler representations, but here it's based on a graph rather than just static features. I'm trying to map this onto neuroscience—where do these representations exist, how do you pass them off, and how do neurons tease them apart? The algorithm Ramy implemented is extremely neurologically plausible. I'm not objecting to that. We'll keep talking about this, but maybe that's not so related to Ramy's project. I'm trying to understand the whole thing.

These are just different ablations. I wanted to make sure it works for different ranges of SDRs at different sparsities. At 2 percent, I'll be at 21 bits, and so on. That's the overlap error in matching the target overlaps. If we create these representations and compute their pairwise overlaps, this is the kind of error we see. On average, it's about one bit for this size of SDR. I tried to normalize the overlap error by the number of active bits, because it needs to be fair. For example, a representation of 1496 with 81 active bits—one bit of overlap error is very little in comparison. When you normalize it, that's what you get. For 1024, it's a bit of a high normalized error, and as you go up, it seems to stay the same.

no matter what, these are tiny numbers. This is half a bit in error—41 to 81. It's pretty small, even at the max. It doesn't matter. I was just hoping for this, basically those two here. But at this scale, I'm not sure it matters. After one bit, I changed and fixed the SDRs at 2048 and changed the sparsity. This is what it looks like with changing sparsity. Number of bits, number—yeah, that number could be five or ten, and it would still be fine. Varying number of objects, this is the trend we're seeing with more objects: 10, 50, 100. Again, this overlap error on the side here. On your point, Subutai, if we make it too sparse—if it's five—then we have less chance of encoding a lot of different objects. Not at all. Perhaps you were saying we could have five bits of error and it wouldn't matter, right? Not that you were saying we would encode it. Oh, you mean the bits of error? He's talking about overlap error here. So the error of a few bits is nothing out of 41 bits on, or 81 bits on. That's what I'm saying. Okay.

This is what it looks like in Monty. It's different from this, because here for synthetic we have the full target overlap. In Monty, we are basically filling those rows one by one. These are the kinds of results that hang out. I'm not sure—I think you'll have to explain what's going on here. Basically, it's the same: target overlap scores for 77 episodes. We're looking at these objects in the YCB dataset, going through them one by one, and every time we finish an object, we add that row to the target overlap. The challenge is that it needs to adapt to adding more and more objects every time we finish an episode, because we don't see all of them at the same time. I still don't know what's going on here, but maybe it's obvious to everyone else. I think you're learning objects in an incremental fashion here. Yes, one at a time, so if you've only learned three objects, you can only have a three by three matrix of similarities. The rest are random. What is prediction and target, though? Predictions is the wrong word, but this is the current overlap. This is similar to that—just use the words predicted overlaps. I'm sorry.

The one on the left is showing the SDRs that the algorithm has made and how good they are at clustering. The one on the right is what we want them to look like. The good thing about this graph is they look very similar. You can only do it up to this. There's only a square in here. I don't know if you can see my mouse, but there's only a square in here that matters. The stuff to the right and the bottom don't matter. Is that right? As we see more, like I showed in the previous evidence.

I don't know if it's the correct word for the left—prediction. Actual? Current overlaps, maybe. Current, and this is the target, or calculated overlaps, maybe, or trained overlaps, or object SDR overlaps, algorithm result. So why'd you call it prediction? Deep learning terminology—oh, is that right? Whatever the network outputs, it's usually prediction.

But in our context, prediction is something completely different—predictors are.

Okay, I got it. No problem. We keep adding to it because every episode—yeah, I got it. When it goes from 22 to 23, it's gone through a bunch of iterations and settled, and now you're showing the actual error after the bunch of iterations. It's supposed to stabilize like this after every episode. This is the error—what's an episode, basically? After we add each row, that's an episode. Doing a sensory motor experiment on one object, on a new object. Every episode is a new object. So this is the error after training the encoding algorithm, to see that, to encode this. Error after each new object. Call it that. But that's not right, because the scales don't match between the left and the right. The scale is now at 260, 270 objects, and then—oh no, what is the number on the right there? The 409, 425, 435? This is just—I have 77 objects, but I'm showing it, 10—oh, okay, so divide by 10 is the number of episodes. I'm showing 10 variations per—okay.

I have more detailed slides when I presented them to the team. I just copied those visualizations.

This is what it looks like on a t-SNE. I took the trained object SDRs and plotted the t-SNE, which is a dimensionality reduction algorithm. It tries to map the representations into a lower dimension while maintaining similarities. What does t-SNE stand for? Is that a standard term? It's standard. It's just doing similarities, trying to project high-dimensional similarities to two dimensions while maintaining the similarities. It's pretty good—stochastic neighbor embedding. The T, I think, is for the T schedule. No, it's basically dimensionality reduction in a way that you want to keep similarity. So it's not linear, as opposed to PCA.

Basically, what it's doing is showing that similar objects are clustered together.

And to your earlier point, things like scale, the use of color, and evidence accumulation could explain why the clustering isn't perfect. Also, trying to cluster an abstract space into just two dimensions is challenging. Overall, this looks pretty positive. I see a lot of cylinder objects that are not all in the upper right. On the left there's a chef can, and on the right there's a tomato soup can—they're both cans. If you added a third or fourth dimension, it might look totally different, and they might be similar on one dimension but different on another, like color. This is done just for visualization. This is not an actual result; it's just visualizing something that may be perfectly correct, but the visualization still struggles because of 2D.

You can't collapse everything; it's a nice sanity check that it's working well. If it looked random, then that would be a problem. Ultimately, as Ramy pointed out, the limitations aren't with the algorithm. It's more about getting Monty to represent similar things better and being able to address scale. When you do the projection, is there significance in trying to orthogonalize or find the most significant dimensions, or do you just map it to the lowest two or three dimensions? No, I just map it to a 2D dimension and define the distance function as the overlaps. The visualization is just to see if it looks like it's doing the right job, but the actual overlap is what matters.

I could take the chef can on the left and look at the bits in that, and it might overlap with the others up there. It should. This is also an iterative procedure; sometimes clusters just snap together, for example, with the tennis balls. Sometimes they snap to the lemon, and sometimes they separate. It uses gradient descent and sometimes struggles to map them together. But you're not changing the underlying SDRs when doing this; it's just a flawed visualization technique. The ultimate measure of the SDR encoding is the loss between the two matrices that Ramy showed, but that's hard to see. This is just nice to look at.

Actually, this distracted me because the other stuff is a real result. This is just a visualization, so we don't really know if we extracted the right evidences. We can look at a heat map and see that one object is similar to another, but we don't really know if the encoding algorithm works and if the evidences we extracted work, except that similar objects are closer together than random representations. The first time you made this, it didn't look like this, and it helped.

Was that because it wasn't encoding correctly or because you weren't using the TSNR thing correctly? I had a bug in the code, and that showed how if it's random, it looks completely different—everything is all over the place. Some of these outliers look like mistakes, but they may not be. One way to check is to look at the tennis ball object SDR and the lemon, for example. You can look at the SDR for the chef can and the other cylinders and see if they're overlapping properly. Another helpful thing is to generate a completely random one and show it as a reference frame. That way, we can compare and have context. I have a figure later on Slack. You could show this before and after learning.

There are some obvious examples, like the Phillips screwdriver, of objects that look similar or are clustered together. For example, the knife and these utensils look similar. Toy airplanes—well, these are not toy airplanes; they're parts of a toy airplane, I guess, from the YCB world. Here you have a medium clamp and a large clamp, but they're not near each other. We can spend too much time looking at this, but it's just a quick visualization. It doesn't really capture scale. The medium clamp and large clamp are the same size in the evidence, so they'd probably be closer together if scale were captured. I don't know how much more time we have.

I guess 15 minutes. It would be good to have a five-minute break in between. What's next? We have our models training. I did some experiments on streaming objects—just adding more objects. I start with 10 objects and keep adding 10 at a time, then look at what happens to the error as I train more. This experiment tests if the SDRs adapt to more objects. The error goes up as we add more objects to the target similarity, then optimization brings it back down. As expected. Is this with the first 10 SDRs frozen, or do they keep changing? In this example, they keep changing. These are the results I showed before. I haven't done experiments with stability yet to see how the error would change.

Did you see how much these original SDRs changed after you added more objects?

No, not really. I know they change a lot, but the stability PR should help fix that. The thing is, it would be important to look at the error and see if it's still manageable, because some of these changes need to happen. Some of these SDRs need to move because we're adding evidence to either bring them together or apart, so there's some distribution shift.

Some extra experiments are needed. I'm not sure if we should go through these, but basically, not giving them enough time to train. This is what it looks like over time. Was it 2 percent sparsity here? What if you picked a much higher number, like 10 percent? What would it do for the streaming? Take the entire argument you just made, and instead of making it that small, make it less sparse. I used 5 percent here. What about 10 percent?

I'm jumping ahead to the problem of forming unique representations. The reason I always thought these overlaps would not work is because they do not allow you to determine unique representations. The neurons won't be able to tease apart these different objects. They group them all together, and you can't get around that. You need to be able to form a unique representation. If I understood from the beginning, the exercise was to form two types of representations: one that's unique and one that's not unique. I would have been much more amenable to this approach. I feel bad because I've been arguing with Niels that this isn't going to work, and the reason is you can't tease apart these overlap representations. But now we're saying we're not going to try to tease them apart; we're going to have a separate representation that's very unique. One way to get between the overlapping SDRs and unique ones is to go sparser. You can start with these, and that's what I'm asking. What if I had a 10 percent sparsity for these overlapping SDRs you've just described? But I want to form unique ones by self-sampling from those, randomly.

You can do that to a degree, but I still think you would want to do that alongside having completely random unique SDRs, because that won't be as robust. The way I see it, if you had a higher-level learning module, it would have two matching filters. For each point, as you move to a point, what does it learn there? It learns a unique SDR from the lower layer, and that's what the very specific object was—only for that thing, like the Numenta mug. Then it learns the more generic SDR, maybe from a different layer. I don't know if that's layer two versus three or whatever, but that one, I think it's important that you can vary that threshold depending on the context. If you say, "I don't really care what dinner table I'm at, I just want to be at a dinner table," you're going to lower that threshold. As long as you see, "Okay, this is a cup," that's enough. The reason I didn't want to go that way is because I don't have any evidence that there are two sets of neurons projecting up the hierarchy. You really only have evidence of one, at least from layer three. For example, the paper I just reviewed said layer two didn't project much at all. I don't believe them, but one possibility is you could have the same population representing both the highly sparse and the less sparse one. That's easier to do in a time sequence—maybe in one part of a phase you get the denser one, next part you get the less dense one. But I think your point, Niels, was that if you did it that way, by subsampling the denser representations, you'd have more errors. It's a trade-off. Imagine I had 4,000 neurons and 10 percent active, so 400 cells are active at one point in time. Then, at a later moment, I have 2 percent, so 80 cells active. There are many ways you could pick that 80 out of 400 and still uniquely capture it. I think there would be; I'd have to work on the numbers. It's not clear to me it wouldn't work. We have that dichotomy: is there one set of cells doing all this, or are there two populations? At the moment, I don't have much evidence for two populations. I'm trying to think of a way to make it work with one population. Maybe layer two does project up, and people just didn't see that. I don't know.

That's why I was asking if you would get similar results with less sparsity. It's not clear how sparse this is. Did you check how often each bit is active? Maybe we should try to wrap up. In general, I think the higher dimensional you make it, and it's still sparse, the easier it gets. I don't think that's going to create a challenge for this approach, at least that's my intuition. I think that's consistent with your results with the overlap error and so on. I'm not sure yet. I had a question last time: how many bits are actually on during the entire cycle, what is the duty cycle of each? That's a good point, but we could add some regularization to improve that. We have to do that with a spatial pooler, by the way—add a regularization factor to make sure all the bits are used.

Because we are forcing specific overlaps, there will be large populations of dead neurons. That's just a matter of design. You don't want that, and it's not really an SDR. The spatial pooler had the same problem: we had dead bits and didn't like it, so we added a factor that boosts the activity of dead bits. This is just a function of how many objects you have. If you only have 10 objects but 2,000 bits, and you want the 10 objects to have some overlaps, you need more objects for better utilization.

That's part of it, but the other part is whether we want to do unions or not. If we want to do unions, I don't think you'd want to. I don't think you could do unions on this type of thing.

I think you could, because you could have situations where unions are possible. When the representation becomes less sparse, unions don't work as well. The denser you make it, the less effective unions become. My question is whether this could be done in a single population of cells. These representations would be less dense, so you wouldn't form unions, but with sparser ones, if you're subsampling from a dense, overlapping population, that population is essentially a union of all the unique objects. You take all the unique objects, form a union, and get this representation. The challenge is how to encode both uniqueness and overlaps in one population, or whether you need two populations. For now, it's one population, but I still think you could do unions in the sense that, early in inference, if you don't know if it's a cup or a ball, nothing stops you from representing the union of those two SDRs in one SDR and passing that up. The more similar the representations, the more you converge to the sparse union, which is just the most likely hypothesis. This needs to be balanced with what Vivian mentioned: if you have objects with similarities between all of them, that forces a denser representation.

One way this might work in the brain is through very fast inhibition, which enforces sparsity. The inhibition is fast but follows the initial activation, so you could quickly activate a dense representation, but it would soon become sparse. It's a time-based process: you might have a union briefly, but then quickly transition to sparsity. That's how temporal memory works, and that's what happens in neurons—they can all become active at once, but inhibition quickly shuts most of them down.

Now, comparing synthetic and Monty: in the synthetic case, I suspected Monty would perform slightly better, with fewer dead neurons. Looking at the charts, the x-axis is the number of objects—ten objects. The left chart is before the learning algorithm, and the right is after. The blue is random, and the other is after learning. The same applies to the synthetic experiments, which use random target similarity. In Monty, the target similarity is actual, and we see a lot of low target overlap and only a few bits of high overlap. In Monty, only a few objects are highly similar, and many are low similarity. This relates to zeros in the matrix—not actual zeros, but low overlap. Low overlap leads to fewer dead neurons because it doesn't force high overlap between too many objects. If every object had to have some overlap with every other object, it couldn't be more than 80 bits apart.

If there's a threshold for overlap, we could enforce no overlap below a certain value, which would significantly reduce the issue, but we haven't implemented that yet. I'm still confused: if every object has to have at least one bit of overlap with every other object, the representations can't use more than 80 bits. Why must they overlap? The claim was that there are no zeros in the matrix, but that's not necessary. High overlap uses more neurons, and low overlap uses others, but the furthest two objects still need some overlap.

So you can't use more than 80 bits. Why are there no zeros in the matrix? You can have negative evidence for many objects. The experiment doesn't have zeros, but there could be. I don't understand how there could be so many active neurons.

If there are no zeros, and every representation has at least one bit of overlap with every other, the total number of neurons used can't be more than about 80.

Am I wrong in my logic? You're using 41 at a time, so the furthest something could be is another 40. There are a hundred objects, but every object must have at least one bit of overlap with every other.

Maybe my logic is wrong. I need to think about that. But Ramy, why are there no zeros in the matrix? Wouldn't we have zeros, especially after normalization? There could be zeros, but my point is that with many low values in the overlaps, this spreads to occupy more of the dead neurons. There could be zeros when we normalize; there will be at least one zero in a row.

We can wrap up here. Go ahead.