Tristan Slominski: I'm Tristan, here to talk to you about our path toward an easy-to-use platform and how you can help us get there.

What is the platform? The platform is the experiment and runtime framework that runs our Thousand Brain System Monty. Before outlining our current criteria for version 1.0 of the platform, I want to highlight that the current platform version 0 works. Our code is available, and you can use it with enough effort and motivation. Our goal for Platform 1.0 is to reduce the currently rather large effort and motivation required for adoption. The main internal challenge to building the platform is the conflicting goals between research and platform development. There are multiple characteristics of research and a stable platform that differ. For example, take tolerance for failure. Because of the nature of research, we expect experiments not to work, at least initially, and our tolerance for failure is high. For a platform to be useful, it cannot fail at the same rate as research. While users may initially be willing to tolerate some failure due to our growing pains, ultimately, the goal is for failure to become surprising and not generally tolerated.

How we organize our activity can significantly impact our success and execution. We want to minimize the cost of change in research, as ideas can change rapidly. However, there's only so much change that platform users can absorb. In general, platform users are focused on conducting their own research or solving their own business problems, and they want to minimize the cost of using our platform in terms of work, time, and money. We should keep in mind that research is the source of valuable new capabilities, while the platform on which solutions are built must be both usable and stable. Our current strategy for navigating this challenge is to use prototypes and implementation projects, as described in our RFC 14. Researchers start with prototypes on their own fork of the main Monty project and should be able to iterate quickly without concern for platform stability and requirements. Once the prototype capability is proven, we will dedicate additional effort to integrate the prototype into the platform.

Here's how we define 1.0, our goal for an easy-to-use platform. Ideally, we want to address all improvements that require breaking changes before announcing 1.0. This would communicate stability and garner trust. Additionally, even with all breaking changes out of the way, we want a history of validated use by external parties. I do not think we gain anything by declaring 1.0 only to iterate through major versions rapidly, especially since we already have a version 0 release cadence that communicates our changes may break existing code.

Our goal for Platform 1.0 is to reduce the effort and motivation required for adoption. We do this by tracking and eliminating any current undesirable effects we observe. Here are 14 undesirable effects likely to require breaking changes: Platform logging is not sufficiently configurable. Platform internal representations are difficult to visualize. Configuring the platform is complex. Using the platform requires configuration and run boilerplate. Experimental framing is embedded in the platform. Platform is distributed via Conda. The platform supports a single actuator. Main execution loop contains incidental complexity. Reference frames and coordinate transforms are challenging to use. Experiment, platform, and environment are tightly coupled. Hard-coded naming conventions. Manual benchmarking. Under specified cortical messaging protocol. As we improve these, they will all lead to unstable API.

There are also undesirable effects that we're likely to address continuously and gradually, for example, dependency updates and architecture support. Lastly, there are non-breaking changes that we would also like to address, but do not block version 1.0: insufficient and coupled platform unit and integration tests, benchmarks being used as integration tests, exploiting hardware acceleration, and integrating standard and legible performance and profiling metrics. Here are 15 items that we want to accomplish before declaring version 1.0: improved logging, telemetry data emission and collection, improved configuration, reduced boilerplate, distribute via PyPi, multiple actuator support, simplified main execution loop, improved reference frame and transformation experience, decoupled experiment, platform, and environment, remove hard-coded naming, automated benchmarking, cortical messaging protocol V1 published, stable public API, a modern Python version, and ARM64 architecture support. We have worked out a lot more details behind those items. With so much to accomplish, we want to ensure that all the work we do has the desired effect. For this purpose, we use tools called reality trees. The Platform Current Reality Tree is a tool where we list all of the known undesirable effects and then figure out what is causing them. The purpose of this reality tree is to stay in the problem space. There are no solutions here, but it should provide enough context and detail as to why an undesirable effect is occurring. To think through solutions, we have the platform Future Reality Tree. Whereas the current reality tree lists all undesirable effects, the future reality tree lists all the desired effects. For each desired effect, we work out the details of what needs to be done to achieve the desired effect. The things that need to be done we call injections. These correspond to suggested work that will result in the desired effect. Some of these injections may be as simple as a single pull request, while others can be multi-month efforts. You can learn more about this approach and get into all the details starting on discourse.

Additionally, as mentioned earlier, the project roadmap will eventually include all injections, making them easier to find, especially on the future task widget.

That is our initial plan. However, that's only our plan today. You probably have good ideas for improvement or problems to solve that are not listed in our reality trees. Ultimately, it will be your contributions that will propel this platform forward. Also, we are hiring. Join us.

Up next, Jeff will wrap up today's presentations with a big picture view of what we are working on and our vision for the project.