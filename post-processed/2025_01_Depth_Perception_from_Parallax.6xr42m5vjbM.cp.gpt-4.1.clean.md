We thought we could talk a bit about depth. I have a few slides I can go through. Hojae also put some material together on structure from motion. I thought we could start by discussing parallax, how it works, and how it relates to Monty, thinking through it in concrete terms. Eventually, this discussion could get into where this happens in the brain and related topics. Unless there are any objections, I'll get right into it.

I posted some notes in the group about various things I've been reading about depth perception, covering everything from monocular to binocular cues. One of the most important aspects for fine-grained depth perception, and one of the less clear computationally, is the role of parallax. I put together some slides discussing that and how it relates to active perception and inference of depth.

Definitely stop me if anything's unclear. 

Parallax refers to having two different views—here, in space, but it could also be in time with motion—two different viewpoints viewing two different images of approximately the same thing. Any displacement or change in that image is generally referred to as parallax. For example, imagine a telescope or camera looking out from two different locations. If objects are very far away, at infinity, the images will be perceived in parallel with no difference. For a much closer object, it will be perceived on the right-hand side for one viewpoint and on the left-hand side for the other, creating a shift.

That's the basic idea. If you had a bunch of objects at different depths, they would shift by different amounts, sorting themselves by distance. Because we have mobile eyes, we tend to fixate on the object of interest. When we view the world, we fixate on something—say, an apple—and the center of the apple falls on the fovea of both eyes, so there is no disparity for the apple. This is different from fixating at infinity. Disparity, a term often used, is best defined as a computed estimate of parallax. Parallax is a property of the world and optics; disparity is what the retina picks up in terms of the relative difference between where an object falls in the two images.

Because we can move our eyes, there's no disparity at the fovea for the item we're fixating on, but there is disparity for other objects, like an orange, which tells us something about its relative depth to the apple.

However, that's not always sufficient. This gives a sense of relative depth, but in different instances—such as both objects being far away or both being closer—the relative disparity sensed for the orange is the same. In general, this is combined with additional depth cues, such as how turned in your eyes are. If your eyes are turned in more, your point of fixation is closer; if they're more parallel, the fixation point is farther. This only matters if you're trying to figure out the actual distance. It still accurately conveys relative distance, which is sufficient for many tasks.

If you're not reaching to grab something and just trying to decide where the boundaries of an object are, relative distance is often enough.

For this to work, the retina, eye, or brain—at whatever level it's happening—needs to know the commonality between what's being perceived in the two eyes. To sense that the orange has shifted across the images, you need to understand it's the same object. This is known as the correspondence problem: how do features present in one image correspond to features in another?

The brain is believed to solve this at fine levels of detail using disparity-sensitive neurons. At the fixation point—say, on the apple—there will still be disparity in the images in terms of which parts of the apple fall where on the retina.

Some neurons have binocular receptive fields, receiving input from both eyes, and look for the same feature with some displacement. To make this more concrete, imagine simple stimuli going into both eyes. Most of these stimuli are the same for both eyes, as shown outside the red box. Within the red box, the stimulus has been shifted. Everything outside the red box can be easily fused and perceived as a single stimulus because both eyes perceive the exact same thing.

But within this box, the stimulus being shown is simulating some disparity. All these black points have been shifted over by a small amount. A disparity neuron would have a receptive field that responds to this stimulus—this is just a toy example—a white box with a black box next to it, but in this kind of retinal space, it responds at this location on one retina and at another location on the other retina. It will be sensitive to this precise kind of disparity of that feature.

These neurons are known to exist, definitely at the stage of V1 and later. Whether they exist even earlier is less clear. In this example, what really matters is that if you're fixating on something at a distance and there's something closer to you that you're not paying attention to, it exists at two different locations on the two retinas, but that's not really the thing you want to attend to. The thing in the little rectangle is not the thing you want to attend to; it's just something in the way, and you don't want to attend to it. A more realistic portrayal is that there are some things that would be exactly the same within the box—that's the point of fixation—but there will be stuff nearby that shifts if there's any depth.

Wherever you're fixated, there should be commonality, and where you're not fixated, it should be displaced or have disparity. This example was a bit odd because the thing in the center was not the thing you were fixating on, since it was moving. Let's assume that this is what you're fixating on, and so that's the same for everything else.

You can do this experiment: look at a wall away from you and attend to the wall, then hold your finger in front of your face. You'll see two fingers unless you focus on the fingers. That's an example of what's happening: you're attending to the wall, and therefore the fingers appear doubled. I'm trying to show a more fine-grained example where this is happening at a small enough level that you have neurons selective to this disparity. I'll talk about the finger later—the finger is where the disparity is so large that your eyes can't fuse it, so you see double.

But why would I want to fuse it if it's at a different distance? If you have a small object and you're focusing on a point, say this corner, there's going to be zero disparity there. The red rectangle is the object; within the red rectangle is an object, and there's a point of fixation. Let's say that's point E, so that's going to be the exact same across the images, but everything else is shifting by a small amount.

Because the object itself has depth, that's why—like looking at the curved surface of an apple. One point on the apple is at one depth, another point is at another. Do we know that the brain actually wants to fuse those other parts? It does. That's how you perceive fine-grained depth detail, like the curvature of a surface.

Thinking like a single column looking at this, that column is a patch. Nobody's looking at the entire apple; that column is looking at a patch at a time. Different columns could be looking at different patches of the apple at different depths. For some, maybe the one right in the center of fixation, you want that, but maybe the ones off to the side are looking at the side of the apple, which is a little further away. For that column, you want to fuse the two together. I'm not 100 percent certain of that, but I'll go with it for now. That provides the signal for relative depth.

With these kinds of fine shifts, you perceive depth. The classic way to test this is with a random dot stereogram, which is something like what's being shown here but at much higher resolution. Each eye sees a totally random stimulus, but because there are correlations across the images that activate these disparity-detecting neurons, you end up perceiving a consistent stimulus like a banana, a star, or an elephant. I programmed some of these when I was young.

This also works with one eye. You don't need two eyes to see this effect. That's across time—I'm showing it here with time so we can actually perceive it, since I can't show you two images. I'm testing this because, across time, it makes sense. I trust you, but it wasn't obvious to me that this would actually occur without time—how a column looking to the side would somehow fuse these two together, route them. In general, that's what we're asking a column to do, but imagine a column that's looking at the...

And we're asking it to say, I'm looking at a patch of retina, but there are two retinas. Somehow I have to take the input from those two retinas, which aren't the same, and shift it to make them the same. I don't know how that would happen. There seem to be, or at least one way the brain seems to solve it—probably with multiple solutions—is to have neurons that are essentially hard coded for a particular kind of subtle feature at a particular disparity. This green neuron here receives inputs from both the fovea and a point displaced from the fovea, detecting the same feature but with that offset. That's what I was trying to show here. These cells are known to exist in the cortex. You can measure them.

I have a question. Any idea how, in the case where there are static images, you can imagine there's some kind of difference being taken locally between input from both eyes? There's some local difference being computed to create these disparity cells. Any idea how you're using motion to artificially drive those?

Yeah, as in what's happening here? I haven't thought about it too much, but I think it's something where the same complication is happening, but there's a buffer so that comparison can happen as long as these stimuli are shown close enough together in time. A similar comparison is done. Maybe I could speculate on this a bit. The only thing that could possibly fire here—there are no features—the only thing that fires is motion detection. There are a series of points in this image where motion is being detected, typically by magnocellular cells in the retina that detect motion. Parvicellular cells do not detect motion; they do not fire when things are moving. So you have a set of magnocellular cells firing at the borders of this thing. For the columns receiving input from those magnocellular cells, they would say, there's a feature here, something is going on. The feature is just that there's a set of columns active that surround these objects. It's literally like saying there's some feature at this location. There is no feature, but there's something at this location, and that is apparently sufficient to recognize some of these things.

Now, I don't know what the image on the right is—I can't even tell what that is—but one of the things I learned when I did this earlier is that your limit to recognize a particular object is very limited in this regard. If you start making something a little more complicated, you just can't tell what they are at all, and I was surprised when I did that. Here I could see, oh, it looks like a pepper and a star, and a star is a different thing, but the elephant—I don't know what that other thing is on the right. I think it's meant to be like a jeep, like a car. Oh, I didn't get it. I couldn't tell that. There's a very limited ability to recognize things in this regard. I would argue the mechanism is magnocellular cells are firing because they detect change. So a bunch of columns are being activated at those points, and that's it. All you have is the idea that there's something here, and maybe you can get the orientation of that thing.

I don't think this is necessarily contradictory, but Scott, just in terms of this view where it's not necessarily dependent on motion, it's more that at any given point, these cells are going to be getting a stimulus. One of the stimuli, the first one, is going to come here, and then the one that's shown a quarter of a second later is firing at that point. At the next time point, you get another shift. That's why it gets fused together so that it feels like it's both 3D and wiggling. It doesn't feel like a 3D image that's static in space.

Isn't it true that there really is no disparity in those images? There's no disparities. But that's the thing—I think the brain perceives disparity because of how closely in time they're presented. I disagree. I think it's a change. The brain—this only works because the cells are detecting change. There is never a point where there's any possible thing to look at to say there's a disparity between these two things. It's just edges of change. I'm not saying disparity cells don't exist, but I don't think they're playing a role. I don't think they play a role, maybe in a special case.

I guess the question is, is this triggering a depth percept for you or not? It's not a very strong depth percept for me. I get more of a surface texture segmentation, slight depth, like it may be driving the depth a little bit. I don't think this is a depth perception example. This is just a figure of separation example. This is a more pronounced example where there are more cues. But I would argue I perceive this as being almost like a 3D paper world, like paper cutouts inserted into a 3D scene. Any individual person looks kind of 2D, but the arrangement of them within this street looks 3D to me. I think it's an artifact of the way this is presented.

First of all, all these perceptions—I'm not sure about the disparity neurons—but almost all these effects can occur with one eye. Typically, when you're walking down the street, your head is constantly jiggling a little bit one way or another. You don't have to try to do it, your eyes don't have to move; that's just natural. The effect you illustrated in that street scene will occur in your head. The fact that it feels like two sheets of paper is probably because it's a very unnatural way of presenting it to us. The world doesn't jiggle like that without our motion. We typically move our bodies and sense it, but it's really no different than just moving your head back and forth a little bit. Technically, there's no difference between those two things. I don't think you should read too much into it, other than that it feels like sheets of paper. That effect is what's happening, and it doesn't take two eyes to do it. It could be disparity over time, but I think it's the same idea ultimately. Disparity over time—whether it's over time, and I guess Hojae's presentation might go into this more—you still need to solve this correspondence problem. The way the brain seems to solve it, at least for static inputs, is to have neurons selectively tuned for these features.

It's interesting because if you think about the street scene, you don't have to solve a disparity problem. You fixate at some point, and everything that's not at that depth is changing, so you can just ignore it. It becomes blurry; you're not trying to solve a desired problem. I was surprised you mentioned the disparity neurons because the only way I can imagine that is if you're trying to focus on multiple depths at the same time, like the example of the apple curving away from you. That's a different problem—actually trying to perceive multiple things at different depths at the same time. Normally, I've been thinking about this problem as just wanting to ignore all the stuff that's not there. If I'm looking at the woman and the child, everything that moves relative to them is not part of the woman and the child; it's something else, and I can tell how far away they are. The disparity neurons, if they exist, would say, "Oh, this is this far away," or "That's that far away," as opposed to fusing them. That's the part that's weird to me—trying to fuse those.

The way I'm thinking about it is, say the receptive field of a V1 neuron is something like your two thumbs held out at arm's length, so it's a reasonably decent amount of information. Within that receptive field, you have a perception of depth all at once. It's not like you always perceive everything you attend to as just being a single plane relative to everything else. So, within that kind of local area—let's say I have two eyes, and they're trying to focus on something in the receptive field of that neuron, but maybe the thing they're focusing on, or the things they're trying to attend to, are smaller than the receptive field of that neuron. Normally, I wouldn't want to pay attention to things that are further away. I just want to say, "Yeah, the only features I'm really interested in are the ones that are in focus, the ones that come from both eyes at once, that are aligned, and anything that's not aligned would not be a feature that I'm attending to." That would be a quick way of saying, "Yeah, there's a bunch of input coming from the retina over this area, some size of a little circle or something, but actually the thing that's of interest to me is smaller than that." Only the axons from both eyes that are co-aligned will be recognized, and the axons that are non-co-aligned won't—they'll just be ignored because I don't really want to attend to them. I don't want to see more; I just don't want to see the stuff that's beyond it or closer.

It's funny because you're trying to unite them, and I'm trying to separate them. I'm trying to say, "Yeah, the things that are further or closer away, I don't want to see." I'm agreeing with that for larger differences. I'm just trying to say that within a small, narrow field, there are often cases where we want to perceive true depth there. That is how primates are able to break camouflage, for example—they can see even if something in a 2D input looks the exact same. It's that disparity at that fine grain that makes stuff pop out.

I'm questioning whether that's true. The hidden animal pops out because it moves. If the animal doesn't move, you don't see it. You can, though—that's one of the innovations of depth perception. An animal that doesn't have depth perception will be unable to see, against the background, something like a primate can, because of the 3D depth popping out, because there's this disparity. I'd like to see examples of that, because everything I've ever noticed and thought about in this regard is just the opposite. You can't see these things until they move.

I guess all I'm going to say is there are different ways to think about these problems, and I've been thinking about it differently than you. I've been thinking about how I eliminate information that isn't at the right depth, and you're trying to somehow fuse information at different depths, which is the exact opposite of what I would normally want to do. Maybe I'll move on, because then I'll talk more about what you're describing, which is just within a particular area of focus. But then your eyes are constantly moving, constantly looking at different things, so most things that you're not attending to are not in focus. I'm still lodging my concern about this idea that a single column's retinal patches are trying to unite different depths on it. I'm not aligned with that yet—maybe I'm wrong, but then we can go on.

So in theory, having these disparity neurons can help you have a fused percept of some depth in a small area, but this depends on having neurons tuned to particular offsets across the two retinas and to particular features, whatever those very simple low-level features are. Of course, you can't have this for all possible disparities and all possible features, so the reality is different.

If I read about this, I would assume the disparity neurons are doing something completely different, something that I need to do, and I would think, "Wow, they could solve that problem." It's very difficult for a single eye to determine absolute depth, but two eyes can. One of the problems we have with vision is knowing where we are observing things in the reference frame of an object. If I had a column with some disparity neurons in it, I could use these neurons to determine the absolute depth or distance to the object, which would be very useful—not to recognize a feature or unite things, but just to know the actual distance. For example, this feature I'm observing is at a certain depth for me, and the disparity wouldn't be to unite or fuse images, but simply to determine the distance. I wonder if that idea has been discussed. That's at least how I understood it from this picture: the purpose is to detect absolute depth, but to do that, it has to recognize which features correspond to each other. When it knows that a feature on the left retina corresponds to a feature on the right retina, the disparity between them tells it the absolute depth. In this case, all we need is the absolute minimal feature that will satisfy that requirement. It doesn't have to be a feature relative to the object itself; it could literally be a progression of dot differences or something similar. It's not a feature that's part of the object representation—just the minimal thing that achieves the disparity. It doesn't require any models; it's just to figure out correspondence between the two images. That could be all hardwired, or not, but it could be hardwired, and that makes it not useful for being part of a model that says, "This is a feature of the object." It's just a feature used to determine depth.

I'm not trying to imply that this is going to be a feature that then becomes part of the object, if that's what it sounded like I was saying. When I was talking about the apple and seeing around the side of the apple and the depth, it's more just to get the sort of features we're getting right now. Basically, a feature at a location—but it's not even a feature at a location. All we want to do is calculate distance. That's all we're trying to do: calculate distance. That's what I mean by location, but there's no feature; it's just distance.

In the context of Monty, it seems that what we know about these neurons and vision is that there is a way to extract depth from binocular input very early in visual processing. Maybe in the first input into V1, you can extract depth by taking these disparity neurons, seeing what the difference is, and getting the absolute depth from that. This depth is not a feature; Monty gets depth input as sensory input features, and it's needed to determine the locations where the actual feature on the model is.

This is what we are doing in modeling to extract that. I didn't follow all that. Sorry. Monty assumes a camera with depth. We assume a depth camera, and now we've been going back into the literature to figure out whether it's reasonable that depth information is available and can be extracted without too much knowledge of a model or too much preprocessing. For me, this is the context for why we're talking about this.

And on the point of it being hardwired, I posted a summary earlier today about how babies learn depth perception. At birth, they don't have binocular depth perception, but it is learned very quickly in the first months. They first have to separate the input from the right and left eye in V1, where you can actually see these stripes if you put in some dye. After they have learned that separation, they gain binocular depth perception within a couple of weeks. After that, it's quite fixed, because if a child or a monkey is deprived of sensory input during that formative period, they have lifelong impairments. If you are cross-eyed during that period, or if a cat has an eye patch during the first few months, that eye is functionally blind for the rest of its life.

I slightly misspoke when I said it could be fixed or not. What I meant is that it's not something you would learn throughout life. We learn new things throughout life, but this is something that, once set up, doesn't have to change. I can see that it could be part of this developmental period. In some sense, it's learned, but when I say learned, I mean seeing something new and learning it. No, it's not like that. This is part of development: you set it up, and then it goes.

What's interesting is that in primates, V1 has these extra layers, and only V1, and I think it's only in primates. I've always wondered what is going on there. It always felt like the difference is that we're trying to perceive at a distance, while their fingers are right on the object. You have other ways of knowing where your fingers are, but we're seeing at a distance, which is tricky. I always felt maybe those extra layers in V1 are calculating depth. There are a bunch of calculations that have to be accomplished, because it's not only the distance, but when your eyes move, how far they're moving on the object depends on the distance. You have to know the distance, and then use it to calculate the displacement on the object. I thought that's something that only applies to eyes, and maybe that's what's going on in those extra input layers. I still think it's a good hypothesis. I don't know whether the disparity neurons exist in V1.

I think there's evidence that could correspond to processing in layer 4. There's at least evidence that this is where the first binocular inputs really come together. The way I've ignored that for years, because we have this common cortical algorithm, is that we could assume we have a camera with depth, or we can say an animal with one eye is pretty good at vision.

You can do 95 percent of what you can do with two eyes with one eye. Let's not focus on that too much; that was my excuse. The core principles of parallax apply whether the two images are across space or across time. There's a big difference between fusing features and determining depth. I was reacting to the suggestion that they're trying to fuse features together. Fusing features is a way of perceiving depth, as long as it's not the features we apply to an object—it's not the input to an object, like "what is the feature at this location?" That will definitely be learned throughout life. You could change what features a column can assign to it, and you could do that in this case; whatever features are being detected could change.

I'm trying to go the opposite direction. We're using features for two different things: there's a retinal feature, which is the minimum required to figure out depth, and then there are features of our object models, which is something completely different. It's a common use of that word. I'm pushing back on the idea of fusing features together and feeding them into our model, as opposed to fusing features and using disparity neurons just to determine the depth of the neurons that are not disparate. The ones I want to pay attention to are the ones actually in plane; anything not in plane is what I want to ignore.

A good way of emphasizing this is that what you are fixating on is the most important. This gives you a bit of depth information around that, which makes sense to me. The language can be confusing, but this is basically saying it would be reasonable to assume we have something like a depth map near the point of fixation, from these disparity neurons. But they can't cover the full space.

What happens when disparity neurons aren't doing what they're meant to do? This is what we were talking about earlier, Jeff: if you hold your finger at arm's length and fixate on it, you perceive it as a single object, but if you fixate in the distance, you perceive your finger as double. It's not confused. I don't think the purpose of the system is to see the two fingers. My point is that this shows it only works in a very narrow depth of field, around where you're focusing. That's where that part of depth estimation works.

In addition, you have all this active perception of depth. In particular, you need to figure out where to fixate, where your eyes should converge, and how much the lenses should accommodate, so you have the stimulus of interest in your fixation. If you're fixating at infinity, any objects near you will be doubled. You need to estimate where your eyes should converge to focus on something like a tree trunk—how far away is it so you can focus on it? On that tree trunk, let's say there's a beetle with camouflage; you could use those disparity neurons to perceive the beetle standing out from the bark.

There are different ways to do this. The paper talked about contour stereopsis. You can do the correspondence problem for big, obvious things like the tree trunk, so you don't need hardwired disparity neurons for these, but it's obvious to each eye that it's seeing something large. Using that information, I had this basic idea—I'm thinking about the beetle on the bark. If you see something and you're not sure if it's different, my first reaction is to move my head to the side to look at the edge and see if the beetle is protruding from the surface or if it moves separately from the bark. My personal observation is that my ability to determine depth using these disparity neurons is very limited. It's hard to determine if it's a well-camouflaged beetle. The whole point is that camouflage means there are no local features for the disparity neurons to work with.

The way I would really solve the problem is by moving my head to the side and looking at it from a different angle. That improves parallax. In the literature, one of the main reasons for fine-grained depth perception is how it breaks camouflage. The person who discovered this in the 1950s or 60s was a military technician who would look at tanks. In the military, they knew that to spot camouflaged tanks, you need to give two images, one to each eye, because using depth, the object will pop out. At the time, this wasn't fully appreciated perceptually. The whole point of camouflage painting is to make it very hard to find edges where you can do disparity. There are lots of little boundaries matching the background, making it hard to pick out edges that are moving relative to each other.

I'm thinking about what we need to do for Monty and which of these things are appropriate. In my mindset, we don't need to have two eyes or do this converging thing. We could, but it isn't absolutely necessary. Maybe the bigger thing is to have two cameras and just use them for parallax, bouncing back and forth between the two.

Are these disparity neurons also useful for calculating how to fixate on an object? We basically want to minimize the disparity on some objects. I think so. For fine perception, it makes sense to use them. You coarsely fixate on the tree trunk, but if something in the middle catches your eye, you fixate on the beetle or the tree branch. Other than depth, it could also help guide your eyes to fixate on an object by minimizing disparity. I don't know if it guides your eyes or just tells you when you've fixated. When reading small text, you want to keep the letters in focus. I'm not sure these disparity neurons would tell you that, but thinking of them as part of active perception feels a little odd to me. It's more like when my two eyes are converging, there will be some point where things co-align at a certain depth. It could be tweaking the eyes slightly—if you're a little out of focus, move this way, a little out of focus, move that way. It's not "move over here," but more about fine-level control to keep things in focus. If that's what you meant, then I agree. The "move over here" is probably a higher-level process.

If that's going to be model-based or motion-based, or based on unexpected motion, I was reacting to the word "guide." It's a limited word for what happens—it's more like a local feedback loop to keep things in focus.

On the model-based point, one of the nice things about how we build Monty is that it has internal models. That's a natural cue for knowing where to fixate. For example, if I'm at my desk, I know where my computer is relative to me, and that already gives my visual system some sense of where the eyes should be converging. Based on what they focus on, you get a sense of whether what you thought you were going to focus on is in front of that, and you can have an iterative process improving the actual accommodation. There's a two-stage process: an initial coarse sense of where the object is so you can fixate, and then the disparity neurons do small adjustments around that point of fixation.

If disparity is just for fine-tuning, then I get it. Using the word "features" and all that is confusing—it's a fine-tuning mechanism. On a high level, this fits nicely with what we're doing in Monty and validates that it's reasonable for us to assume patches can estimate depth at some level. I'm sure someone's done the math on these things. If you know the granularity of retinal ganglion cells and the physics, you can determine the accuracy and range at which these two mechanisms can work.

How much can these disparity neurons actually correct for? There would be a small amount; they could probably calculate that. What accuracy do you get just from parallax and so on? These are all things I think are estimable mathematically. I'm sure they've done that. I didn't come across any specific figures, but in general, it seemed people were consistent with our intuition, talking about very small differences for this. It can only work for small differentials. You can test it yourself with the finger example—it clearly doesn't work for larger differentials, and that's not a particularly hard stimulus. Hojae will talk more about motion parallax, but I thought it was useful at a high level to touch on the concept that essentially the same mathematics of parallax apply to images gathered over time as they do to binocular vision, or even for animals or systems with more eyes. It's really the same ideas about perception from different points in space. Something like smooth pursuit makes sense in terms of foveating at a point in space associated with zero disparity, and everything else is relative to that. You have zero disparity, but also some absolute sense of depth at that location.

I was confused when looking through the literature by the sheer variety of terminology used. In general, you can think of motion parallax as broadly equivalent to structure from motion. Sometimes people refer to structure from motion as when the eye is static and other things are moving, whereas if the eye is moving, that's motion parallax.

There's a computer vision term, stereophotogrammetry, which is essentially the same disparity matching correspondence problem happening in stereopsis, but done through a computer vision process. As mentioned, whether it's motion parallax or stereoscopic vision, it's all the same type of thing. Structure from motion and stereophotogrammetry use similar algorithms. These things aren't as distinct as they might seem on the surface.

A last note about how this ties in with Monty: when solving the correspondence problem in computer vision, it's computationally expensive and challenging because they try to do it over the entire image. For example, with two images of Notre Dame, you try to find every single point in one image that corresponds to a point in the other image, which gives you a depth map of the entire image. But we are concerned with a fovea or a narrow point of perception, which makes the problem much easier and less computationally expensive. In general, I couldn't find much active research on this—maybe there's something in robotics I missed—but it makes sense because there aren't Monty-like systems perceiving the world through narrow apertures; everything tends to try to view the whole world at once. Showing the picture of Notre Dame reminds us that this is why disparity neurons can only work locally: the computational requirement to do this over the whole image would require disparity detection over very long distances. Disparity neurons can't do that; they have to do something very local. That's just a different way of looking at the same thing: disparity neurons have to be simple and small. Also, integrating over neurons from the entire visual field into V1 would be unmanageable.

I found this image very helpful for myself. Stripes that form in V1 after the first learning period correspond to input from the left eye versus the right eye—these are ocular dominance columns. If we have a small cortical column, it can integrate over a small difference in left and right field between these two inputs. But if we looked at the entire picture and tried to find correspondence, we'd have to look from one side to the other, which becomes a combinatorial problem. You can imagine a neuron doing maybe one or two stripes, but beyond that, it gets too complex.

To summarize, in general, extracting depth from either binocular or moving images is not easy, even for biology. It's a hard problem in computer vision, but fortunately, people have worked out the algorithms needed to solve the correspondence problem given two 2D images. It's much more tractable if you have something like Monty, where you're moving in the world, so you can get crude estimates of depth, fixate, and then get a fine estimate over a narrow view of the world. These are the tenets of what we're doing, which I thought was a happy accident in terms of the kind of depth information we need.

Very nice. That was a great summary. I'm glad we discussed and debated it. I have a better understanding now. Thanks. It definitely helped me because I had read snippets about parallax, but it never really made sense until I saw how it fit with what we want to do. I've concluded, perhaps erroneously, that most parallax occurs because of motion. Maybe Hojae is going to talk about that. Is that what you said? I didn't really touch on that much, but one of the interesting papers I came across showed that mice detected a kind of depth map already in V1, even when the mice weren't moving around. It's just in the field of one of their eyes, but through motion parallax. There seems to be better biological evidence that depth from motion parallax emerges earlier than depth from binocular fusion. Of course, rats and rodents don't really have binocular vision. I think they have a little bit just in front of them, a small overlap, but I wouldn't call it binocular vision. My perception is that they have almost no binocular vision because the centers of their visual fields can't align. There's a slight overlap at the edges, but maybe that works. My assumption has always been that their vision is more for detecting predators around them rather than discerning prey in front of them.

I have some thoughts about this, but I'll wait for Hojae's turn. We have time. Mine is pretty short. In computer vision, there are ways to estimate depth that parallel what Niels talked about. There are many cues that allow us to perceive depth. Niels mentioned convergence and accommodation. I'm still learning these terms, but there are many ways to perceive depth. The focus of this presentation was parallax. Parallax can happen in several ways. With binocular vision, because of interpupillary distance, that's one way parallax can occur. Another way is through motion parallax, which can be monocular. With one eye, we can still estimate depth. I'm not sure how accurate it is, but I agree with Jeff that with some physics and math, there's probably a way to estimate how accurate it can be, though that's not the focus here. Structure from motion is a way to perceive depth using one eye and movement, which is the setting Monty's distant agent already has. Is this movement of the object or movement of the eye? Movement of the eye, or movement of the camera or eye. Is that different from motion parallax? I don't think so. You had motion parallax and then a separate bullet for structure from motion, so I didn't understand. It's the biological term versus the computer vision term.

We can come back to a six-second video later.

I think the main argument is that in Monty, with the current settings and without extra cameras or changes, we can remove information coming from depth and still estimate depth with what we have. The caveat is that I'm not sure if that depth map will be as good as the ground truth one.

This is the classical output from structure from motion. Is this the cool video? No, it's not, but this is another interesting video. There was research where people took a bunch of images from Flickr. All these black triangular cone shapes represent cameras. When you upload pictures to Flickr, some phone and location information is stored along with the corresponding picture, and people tried to use those pictures to reconstruct the coliseum in 3D. There are other examples on a website called Building Rome in a Day. What you saw was the coliseum, but there's also the Trevi Fountain. This is interesting because many people take pictures of these landmarks. Nowadays, structure from motion is used a lot for mapping terrains. Drones are sent out to take pictures and map out areas. In these videos, they're actually creating a 3D model, not just linking together images. They're creating point clouds—the output is points in 3D space, and then they figure out what it looks like from different viewpoints, allowing the system to be viewed from a novel direction.

To highlight the parallels between stereopsis and structure from motion: in this instance, you could argue this is stereopsis, but instead of binocular vision, you have a thousand cameras all viewing the same thing at the same time. The calculations needed to determine depth are basically the same. This kind of point cloud reminds me of Monty because we also make graphs or point clouds of objects. These are very detailed because there are thousands of cameras, but the minimum requirement is just two images. Theoretically, it's possible to reconstruct structure from as few as two images, and then refine it as more images are taken or as more steps are performed in Monty.

Structure from Motion (SfM) is a technique that uses a minimum of two 2D images to reconstruct 3D structure. The final output is a point cloud, similar to 3D models produced by LiDAR sensors, which are more expensive. The minimum requirement is that the two pictures have corresponding features visible across them—more images are better. The mathematical idea is that in the 3D world, a camera captures light from different parts of the scene. When taking an image, point 1 ends up here in the first image, point 2 ends up a little to the right in this image. If the camera moves, the 3D point is projected to a new location in 2D, and so on.

Does the camera have to be in motion, or can it just use different points of view? You can have two people with two different cameras taking pictures; it doesn't have to be one camera moving. Often, this is also extracted from video where one person moves the camera over time. In the Flickr example, those are probably pictures taken by different people at different times. Most cameras have zero depth ability, but if we're doing this with eyes, there are many ways to determine depth. In Monty and in eyes, you don't have to move; you can just be in one spot as long as you can see the points.

The overall algorithm is fairly simple and corresponds to what was discussed earlier in Niels' presentation. To go through each step: first, extract features—not location features, but retinal features as Jeff mentioned, features from the image that the sensor gets. If you're going to do it this way, do the whole image at once. Then you need more than just a retinal feature. If the retinal feature only works at a very small distance, you can say there's an edge that's moved over a little bit, but there are many edges it could correspond to. You have to identify that this particular corner here is the same as that particular corner over there, so the features have to be much more complex.

In Monty, the example is a picture, because not many people upload close-up pictures of a mug, for example—usually it's the whole building. I'm not sure how well this would work with the 64 by 64 small images we see in patches. For Monty, you can imagine those instead of these two pictures. The point is that this step can happen in the sensor modules without involving the learning module, so we haven't learned those features yet.

It can't be done on a large scale like this; it can only be done very locally. This doesn't tell you where that feature is in the overall object, just that it exists.

Our pictures, or the Monty pictures, are usually very small, 64 by 64. It's typically a very zoomed-in picture of a mug, like the kind of images Scott has shown me.

But still, whatever the underlying images are, let's say there is a way to extract some kind of feature, like a feature from computer vision such as SIFT or something. Deep learning is another approach, but we don't need to use that. Anyway, the first step involves identifying some features, and the next step is the correspondence problem. When we have features from one step, we move a little bit, get a new image, and need to match those features. This is the correspondence problem that Niels talked about. Mathematically or computationally, this comes down to representing features in computers as vectors for particular locations. There will be some vectors, and we're trying to find similar vector values. We can use Euclidean distance between these vector values to see how close they are. I'm just trying to make this more concrete, how this might be done on Monty.

But, Hojae, didn't we agree that this is exactly not the thing we want to do in Monty? This is the computer vision way of doing things, but Monty doesn't look at images like this and try to figure out the different parts.

I just want to make sure I'm understanding this. I think it's useful to see how it's done in computer vision, and maybe we would take approximations of this. To what you were saying earlier, Jeff, we can use much simpler features, just in a local area. This is exactly what we want to avoid in Monty. Monty, or Thousand Brains, basically processes a patch at a time and integrates them in space, as opposed to trying to process whole images at a time. The only important thing is to understand, for that patch, where is it in space? And the next patch, where is it in space? And the next patch, where is it in space?

I guess in Monty it would be maybe slightly useful in the case where we have a small patch moving but it's not being moved by the agent itself, or we have an object moving in the patch. It could be an alternative to using optical flow or something like that—a similar approach to track how the sensor is moving. I don't understand that. There's no place in Monty where you should ever be looking at the entire image and trying to correlate across the entire image. I'm thinking of this more as, what does the approach do? So, replacing the image of the cathedral with a small patch on the object. The idea is, we have a small patch, and now that small patch moved over a little bit. How do we find out how much it has moved? We do it by finding specific low-level features, so doing it very locally again.

It's complex because how much you move depends on how far away it is and the orientation. Imagine you're looking at the facade of Notre Dame from the side, from a skewed angle. Now you have even more difficulty because you're moving in depth as well as laterally. It's really complicated.

For example, there's this nice cathedral. For practical purposes, think of these images as patches that Monty sees—the 64 by 64 patches, the tiny little dots. Between the two patches, if we go this route, we can extract some features and try to match features between two patches. I'm skipping a lot of the math here, but basically, if we have the corresponding features, there is some epipolar geometry we can use to put these images in 3D space in the same coordinate system. I used "common reference frame speculation" a bit loosely, but basically, this process overall is called triangulation. Once we have these two images with corresponding points, we can estimate what the 3D point is, and that's the whole point of knowing this. For the algorithm, the final bundle adjustment is when you have—so we do this starting with two images, and this algorithm is iterative in the sense that once we're done with the two images, we'll try to add another one, a fourth one, and so on. That's the final point. I don't have a better visualization of this. I think that idea starts falling apart when we're talking about patches.

Typically, what we'll do is fixate someplace, move some distance, fixate again, move some distance, and fixate again. We don't move our eyes smoothly over an object doing this integration smoothly. It doesn't happen, and we don't want it to happen that way. Maybe if something else is moving, like a car driving past or something, that's a different task. Or if an object, even if it's not moving, is behaving like a bird flapping its wings or bobbing its head in front of you, that's a separate problem.

That's the end of my structure of what structure from motion is. It's one of the ways to estimate depth, with probably the least minimal requirements—one eyeball is fine, one movement is fine, and we don't need two eyeballs like changes in Monty. I think we're agreeing, Jeff, I don't know how well this will work in patches. In computer vision terms, it's because I think the patches are quite featureless. There aren't a lot of interesting edges or features, like in the cathedral example. So, one, there aren't that many features, and two, when we see one patch, there are a lot of edges there, a lot of edges on the cathedral. In some sense, it's almost defined by a bunch of edges. But it would only work locally, right? We've agreed we're not going to try to do this the way they did it in these computer vision systems. It's something that can be done locally.

In the bottom line, just to jump to the conclusion, it's really interesting how biology does this. But if we had a perfect depth camera, we wouldn't need any of it, right? Is that correct? It's just interesting to know that we can make that assumption, that we can have depth. And if we needed something else, we could ask ourselves, maybe we'll use LiDAR, which is a different depth camera. That's another depth camera.

So it's also useful to ask why this extra machinery exists in the neocortex with V1—maybe it has to do these computations, but we don't have to. One of the reasons we discussed this was the application of Monty: depth cameras are expensive, and many people don't have them. It's interesting to consider what we can do with 2D cameras, since that seems sufficient for people. To Viviane's point about biological matching, we're currently assuming depth information is available in the first cortical column. Does that mean we're making incorrect assumptions about what cortical columns can do? What if the first cortical column wasn't, and V1 was entirely responsible for getting depth information? Then we'd have to rethink what the columns are doing. That doesn't seem to be the case, though. Those were the main points that led us down this line of thought.

If we wanted to reduce costs or use a different type of camera, we could just use two cameras, like two eyes, separated by some space—the larger the distance, the better parallax you'll get—and build a system that works like that, alternating back and forth to determine depth. But that would all be outside the cortical column, handled in the sensor module, which would get input from both cameras and process it. That's one possible approach.

Another thing to remember is that depth cameras are generally limited. They tend to work by projecting some form of light, using time-of-flight algorithms or structured light. In both cases, they measure the light when it returns, but this fails quickly in daylight. Depth cameras are difficult to use except for LiDAR, which is popular but also expensive. Still, it's nice that what the brain does is robust in many different settings.

I don't know the current state of the art for depth cameras. I thought they were pretty cheap now since they're in game consoles, but maybe they're not very good. Decent ones like the Intel RealSense can cost nearly $500. There's an infrared depth camera in my phone, right? Phones use infrared depth information for face recognition.

That's why we did the Monty Meets World with the iPhone, because it has this camera. But getting a standalone one isn't easy. Buying a gaming console might be the cheapest option. We don't have to worry about the cost to get or build one. They're not very expensive—they're in every phone, so they're going to be really cheap, maybe pennies or a few dollars per phone. They can be built cheaply, so we don't have to worry about that unless we need to buy one.

In terms of the limitations of Monty or sensorimotor learning, it's not really a concern. There are many ways to work around it. A takeaway from all our reading is that if we need to revisit this and build depth perception from 2D images, there are many ways to do it that fit well with Monty. For now, we can probably just keep using the depth camera as we are.

Think about the ultrasound system we discussed with the Gage Foundation. We talked a long time ago—I'm not sure if you've talked since. It's interesting: there's a camera, and I assume they get depth perception from it, but you can't interpret those images until you start moving the sensor. That movement isn't like a saccade; it's continuous motion. That's something to consider—a practical system where the camera moves, but not like eyes.

That reminds me of a third point: after a lifetime of exposure to the 3D world, how do we recognize images in a 2D picture or painting? What kind of information from depth perception and how it emerges could inform that? There are many monocular cues for depth perception that don't involve movement, like shadow and lighting, or relative size. For example, if I have a set of features—say, letters in a word—and the letters on one side are smaller and the others are larger, with the right skew, you'd say the word is facing away, moving in depth, or rotated in depth and plane. So size cues are important, and this one actually relies on having learned models.

We only have a couple of minutes here. Can I just throw out something to think about? Something I've always found puzzling and haven't really thought about too much, but it's related to this. Think about an animal learning a model of its environment and the grid cells in the entorhinal cortex. The reference frame is a plane, in the plane of the environment—like a map on the surface of the environment. But we never see that view; we don't look down upon our environment from above like we're looking at a map. We're seeing it skewed from the side, and the views we see are never the model we end up with. We observe these different skewed things, yet we end up with a model that is like a map.

It's very easy to draw a map view—if I asked you to draw a map of your house, you could do it. But if I said, draw what your living room looks like looking through the door to the kitchen, which is something you see all day long, it's really hard to draw that unless you're a practiced artist and have learned to work on depth perception issues. There's a gap between what we actually observe and the model we present, which is not the same. The same thing is true for the models in Monty. If you assume you have a finger touch sensor, it's not a problem, but visually it would be. If I'm looking at an object visually, like a face, and I'm seeing faces at skewed angles all the time, the model I have of a face is more 2D—there's a nose in the middle, two eyes, and a mouth. But that's not usually what I see; usually, I see these very odd-looking things.

The same problem exists in cortex: how do we convert from what we see to what we actually map? Maybe part of this is a confounding factor, like the decoding problem—the fact that we're not very good at drawing in 3D. If you could actually walk into a room and see it from a different perspective, you would be surprised, but you would make the correct predictions for how the 3D face or the room would look. So the model must be 3D and have that information. Even mental imagery—if I imagine my kitchen or someone's face from the side—I can have a pretty good image of what it looks like, certainly way better than I can draw.

It's interesting because, first of all, how do you learn that model from your observations? Clearly, when I see a face from an angle, I will recognize if something's wrong about it immediately. But it's not a generative model—I can't just draw that. It's extremely hard to draw a face that's even slightly realistic. If you've tried it, it's really hard. I just can't do it. It's like you have a model of the environment whose reference frame is anchored on the floor, like a map of the floor, and yet somehow we're able to generate 3D images on the fly—not that we can create them, but we would know if they're wrong, and we can infer from those skewed views and angles with no problem whatsoever. In fact, that's almost all we do, and yet the model itself is not that view.

I have another meeting in a couple of minutes, so I have to run. I'm in the office today, by the way—I haven't been here in months. We have the group because we've talked about the bottleneck of goal states, which I think is what Viviane was also getting into earlier. The model is in the column, but when we actually want to produce actions to draw it, we can't just take the model and put it on paper. We have to send action commands to our hand to draw it, and because we can't communicate the entire model, we recognize mistakes when features come in, but we can't send it out.

Think of it this way: there's a huge amount of evidence in the entorhinal cortex that the models are basically two dimensional, and somehow we're able to tack on three dimensional objects onto that two dimensional model and then generate views of it.

I'll leave it as a puzzle. We're not doing this in Monty; we're just doing direct X, Y, Z location and writing it down. Our models are three dimensional and we could, in theory, create them. This goes back to the question: in the brain, are all these models really two dimensional with 3D appendages, or are they 3D? I'm not so certain. Grid cells are basically two dimensional, yet somehow they work to do this three dimensional modeling. I just wanted to throw that out as we're talking about these issues in vision. This problem really surfaces in vision, and that's all I want to talk about—it doesn't surface with touch. That's another mystery of vision, in my mind: how does that work?