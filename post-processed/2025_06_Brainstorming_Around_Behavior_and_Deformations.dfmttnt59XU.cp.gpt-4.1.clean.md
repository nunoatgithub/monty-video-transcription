Today we'll spend five or ten minutes going over the repeat feedback. Vivin, did you get a chance to go over this during the code meeting? No? Okay, we'll start with that.

Last week, we brought up a potential issue and a solution to object distorting behaviors. I put the link to her writeup in the VRA research channel. We can join in on that when it happens. I think the presentation will be short, but I already have questions in mind, so depending on how that goes, that may be the main topic of today's research meeting. As time permits, we can also continue on to the Google topic. I can't imagine that topic will be short; it's very confusing. I don't think we understand it very well—at least I don't. If you could also go over what problem is being solved and the distortion model, just introducing that concept again, that might be helpful to Tristan, who might watch this later, and also everyone watching on YouTube. I updated the X College robot this morning to add a bit of context, to recap the problem, but mostly to present what I discussed on Friday of the brainstorming week.

This is a bit of a recap, but I thought it might be useful to give some context. Basically, we started out with a solution to modeling object behaviors. We also had a nice idea for how to use an object behavior and apply it to make predictions about morphology. That idea involved using the movement stored in the behavior model and sending it to the child column to move through that reference frame, the same way you use the sensorimotor movement to move through the reference frame. I still think that's a good idea, but one issue was that it didn't work with object distortions or behaviors where many parts of the object move in different directions—not the entire child object moving together, but all the parts moving separately. I call that object distortions. When I revisited the idea, I thought about object distortions in the static case first—no behaviors, just considering all the ways we can recognize a cup and how it can have different distortions. We might have a model for a coffee mug and recognize it in a different orientation, location, or scale, all of which are already accounted for by our theory. We can also imagine scaling in one direction, where you scale the movement vector only in the X direction. But you can also have more complex distortions, like different scales or orientations at different locations on the model—different transformations applied to the movement vector depending on where you are on the object.

For this object deformation, we have a proposed solution: the idea of the bent logo on the cup, where the logo turns its orientation and bends at some point. We put that on the cup, and the way we learn that is by using hierarchy and learning the child object's location, orientation, and scale on a location-by-location basis. The child object doesn't have just one orientation, location, and scale; at different locations on the parent object, it can have different orientations or scales. This is just a sketch, but hopefully it's simple enough to follow. We would have the Menta logo or TBP logo modeled, and then send its object ID up to the parent column. The parent column could model the cup, and at different locations on the cup, we would assign the logo ID and also calculate and store the relative orientation of the logo to the cup in the parent model. That relative orientation can be different at different locations on the parent model.

Does that make sense so far?

I then extrapolated a bit on that idea. One thought was that the map of translations, orientations, and scale could be independent of the specific object. Instead of saying we have a relative orientation of the logo to the parent, and that relative orientation is different at different locations, we could have that map independent of the logo on the cup parent object. It could be a general deformation model that models relative orientations at locations, and you could apply that same deformation to different objects. For example, you could have a different logo that bends, and you could imagine that immediately without ever having learned that different logo in the bent variation.

and then instead of associating object-specific locations with each other in the backward connections, we would apply a movement command to the morphology's reference frame. That's a requirement if we want to learn these general deformation models. We couldn't use the feedback connections that associate a location in the parent model with a location in the child object, because those locations are unique to specific objects. We would need a more general solution that we can apply to any object. We could use the mechanism we discussed a couple of weeks ago, where we just apply a movement command to the child object's reference frame instead. That is general and independent of the specific location.

SDRs—do you want to start the discussion? Sure. On this last point, I was thinking about the bent logo. You said, "I can apply it to another object," and I was trying to imagine that. I could show you a cup with the bent Thousand Brains logo and say, "We're going to put another logo on there." I can imagine that, but I wouldn't necessarily be able to make specific predictions about exactly where the bend occurred. I wouldn't be certain; I would have to think a lot to say, "Maybe the bend occurred this far across the Thousand Brains logo," and then try to imagine how far across the other logo it would be. It wouldn't be something I could easily do.

When I learn it the first time on a specific logo, I actually notice exactly where it occurs—"Oh, it occurred right here at the T, U, and the S," or something like that. I learn that. My thinking is that, instead of throwing away the particular solution we already have just because it's hard to make a specific prediction for a novel logo, I'm not sure that's a sufficient reason to look for an alternate solution in this case. I get the general idea of bending the image here, but I won't be able to make specific predictions about it. When I learn the original bent logo, I'm not imagining movements or behavior; I just look at where exactly on the logo the bend is, and that's where I learn those points. I'm not convinced this particular example requires an alternate solution.

Just to clarify, I wasn't thinking of this as throwing away the previous idea. It was more about generalizing the current mechanism. We would still calculate the relative orientation of the parent to the child and still send that information up, still invoke minicolumns, and learn that. The difference would be that those relative orientations would be in a different reference frame than the reference frame of the TVP cup with logo model. It would be like a separate map, like the behavior models.

But the idea that we might apply movement commands is strange to me. That's the part that struck me as odd. Maybe I didn't understand it correctly, but I don't see why I need to think about movement commands in this example. It seems like a big ask to always have to learn the backward connection between the specific location of every object. Obviously, I couldn't do that. I could learn the backward connection, tying it to a specific location on a specific child object, but let's say I can't learn that. Maybe I just don't have that. I could still calculate the relative orientation at those points, so whatever is going on there, it's going to change orientation.

Can I do that? Let me just put the backward arrow on here so we know what we're talking about. It's that green one there. We just add that. So basically, oops, this is a very thick line.

This would be the backward connection that goes up here and then threads. We have the activation here in layer 6A of the location of the child object and the location in the parent object.

I'm saying we can't rely on that because we haven't learned it. Exactly. But we can still calculate, or could we still calculate, the relative orientation change at that point? That's not object-specific. We know the relative orientation in the parent column, and one thing we talked about is using the layer 6 cortico-cortical connections—this dark green arrow—to communicate that to the child column. I think that's still a valid general mechanism that would still apply.

The solution I propose, the thing that would require applying a movement command, is if the location of the child object relative to the parent is changing. If the logo bends, the features will also be at different locations.

Imagine we just didn't learn the purple backward connection. We learned it in a specific case, with one logo on one object—the logo on the cup—and now we're going to see something else on the cup.

We would say, "Okay, this cup, I'm expecting the change in orientation at these locations, so that's going to be stored." I just won't be able to predict a specific object or a specific location of that child object.

It feels like I can't do that. I really don't have the ability to do that. I could take a wild guess, but I don't have the ability to say, "This is where it bends," or "This is what is on the left and right of the bend." I have a general impression that the new logo bends somewhere and occupies the same space.

I could say, "There's another logo about the same overall size here."

It just feels like I don't have enough information to make a very specific prediction about where it bends.

But I do know it bends, and that's all I can say about it. I don't know specifically.

I think you can make pretty specific predictions. The example of the bend is more about not knowing where exactly the bend is because you don't know how to anchor the distortion yet—you don't know the location in the distortion reference frame before you actually see the bend in the logo. If you think of a distortion that is more uniform, like an arc-shaped logo, you can imagine applying an arc to any kind of logo without having to learn these associations. When you think about the arc, it feels like you wouldn't know exactly which letters in the logo are at different points in the arc.

You wouldn't know exactly, but you could make pretty good predictions. For example, with the old Intel logo that had the dropped E, once you see it, you know exactly where the logo is supposed to change—where the E drops. Here, I might say, "You just drew it between the U and the M," but I wouldn't know that specifically for another logo. I could try to imagine it, but I wouldn't have learned that specific point. That's what I think is about the anchoring—once you anchor it in the distortion reference frame, once you see the bend, you know how to predict all of the letters after the bend.

I'm going to make a point—not quite an objection, but something that bothers me slightly. The idea that we're going to use these movement commands, which we came up with for object behaviors, and now apply that same mechanism to a non-behaving deformation makes me uncomfortable. I can think of reasons why I don't like it, but we can keep going. I'm not convinced it's a great solution. When I get to the proposed solution, there's an issue with it, and dealing with that issue actually moves us further away from applying the movement command again. I was reading your notes this morning—it's like you have to do a cumulative movement vector, right? Is that what you're referring to? No? Alright, I'll let you keep going.

Hojae, do you want to say something?

I agree with both of you. For example, if we have a new logo and I want to predict the tip, it could be non-overlapping. I don't know why it's not showing on my screen, but I can see it just fine on my side. My T is gone, so I can just look at your T.

This might be what we want to predict, but again, we don't know where we anchor, so it might be slightly overlapping or rotated at a different point. As you said, that's because we don't know where we're anchoring this kind of change. If it's overlapping like that, it wouldn't be the same distortion. I would say this is a different type of distortion, where the bend also includes a translation of the bent part toward the non-bent part.

For example, if I'm bending at a certain point, the general deformation is more related to whether we can make predictions. I'm not sure if we can make specific predictions about where a letter might be, but I still want this general deformation model to be learned so we can learn some general behaviors. If we see a stapler hinge, like a stapler opening, we want to recognize this as a hinge behavior so we can potentially apply this to a laptop and realize that the laptop opening is also a hinge behavior. I'm not sure if it's related to that or not.

Sorry, I think my point is lost. I don't want to distract everyone, but this is what's going on in my life these few weeks. I'm going to dismiss them now. Do you want to learn about object behaviors? They're learning. Lizzie's pointing at the screen, going, "Wow, look at all that stuff." It's full grandparent mode—now half grandparent mode, since there are only two of the four.

It's a lot of people.

Sorry, Hojae, do you want to briefly say that again? I guess the more interesting question is whether we can recognize similar behaviors across different objects. That stapler opening and the laptop opening are essentially the same behavior, and we want to recognize that as the same behavior.

and it's related to this in the sense that if we learn the stapler behavior or deformation, we can theoretically apply this to laptops. It implies that we recognize the same behavior. The way I was thinking about this is that object distortions seem similar to object behaviors in that they're independent of the specific object you're applying them to. You can imagine bending, breaking, or stretching all kinds of objects. It's not dependent on, for example, a coffee cup—maybe that's a bad example because it's not bendable—but if you have a balloon, you can imagine it inflating with all kinds of prints on it. You can imagine a logo inflating with the balloon. That kind of distortion is similar to object behaviors, just as features can change their location, orientation, and scale, but it's independent of the specific features that are changing.

What it feels like to me is that in object behavior, we definitely have this time flow, but when I'm looking at these distorted coffee mugs, it's more like a key frame or a learned state of a behavior. Therefore, I don't need to do the path integration or time integration over the behavior. If I look at the bent coffee cup, or any of those, I imagine them as different. The one on the right is definitely not normal—I'm applying some kind of behavior or distortion I've learned elsewhere. There might be behavior related, and I'm applying it here. The middle one is just a form of cup; I don't see that as a distorted cup. That's a standard cup—I've seen those. Same with the one on the left. The one on the right feels more like a key frame of a behavior. The other two don't, unless I see the logo distort with the cup. The cup gets broader and the logo gets broader as well, but I would still think of that as our standard compositional model. If you put a different logo on here, you would know how it would look, with the scale getting larger towards the top of the logo.

It's all very confusing. Some of these feel like a deformation, and others are not. The cup in the middle just looks like a flavor of cup—I've seen lots of them like that, so I just accept it. If I saw a lot of cups that were bent like the one on the right, I would probably look at that the same way. The first time I see it, I might think it's a bend, that somebody bent it somehow.

I'm bringing up my lack of clarity in thinking about these things. I don't think they're all in the same class or at the same point in learning. I don't think it's as simple as these all being flavors of the same thing. I could explain growing scales—I was trying to make it look like the cup is changing scale towards the top and everything on the cup is changing scale with it. In this case, the only thing that would really indicate that is the logo, and that's just putting a distorted logo on a standard cup.

If I saw cups behave like this—starting cylindrical and then becoming conical—that would be a different thing. I'm still going back to my basic objection or discomfort with saying these problems have to do with behaviors.

The one on the right feels more like a behavioral type of thing. I've seen things bend before, so this looks like a bent object; it's a result of a behavior. But even then, it looks more like a key frame. I have to imagine my movement commands getting bent to follow this. It doesn't feel like that to me. I would describe the model more like a key frame as well.

Can I just throw something else in? This is a really confusing topic. I've often thought about how we can take a two-dimensional object and wrap it around a three-dimensional object. I can imagine our logo going across the top of a car from the left side to the right side, or a van. I have the morphology of the van, and I can take a very big logo and have it wrap around the top of it. I can do that with a face as well. It's a general property of applying one model onto the image of another model, in the same way we talked about the logo wrapping around the cylinder. I'm just expressing my confusion. I don't know if anyone else is finding this confusing, but I'm not understanding what you're saying. I think I'm saying, Viviane, that my confusion is that I'm not happy with the description of it so far. The example of wrapping a logo around the car or the cup is a good example of why we would need a general distortion model, instead of having to learn those specific associations for a specific logo and shape combination. I don't know if it's a distortion model. I need the morphology of one object, and that's not a distortion. In some sense, I can take a two-dimensional object and wrap it around the morphology of a three-dimensional object, which is not a distortion model—I'm just applying it.

The problem with that is, to make predictions about the two-dimensional model, we have to learn explicit feedback connections between the locations on the morphology of the cup and the locations on the 2D logo. If we take a new logo where we haven't learned these feedback connections, it wouldn't work well. Again, I'm not sure it wouldn't work.

What if R1 is some long skinny logo and R2 is the top of the minivan or the van? Or we could just stick to the cup. First of all, R1 knows it's looking at the new logo, the new image, so it's going to keep assuming that's the case. It says, "I know that, so all I need to know is what." It doesn't have to be told that over and over again. Once you've recognized it, it says, "Hey, I'm looking at the new logo." We don't need to pass the ID back. The only thing we have to pass back is any kind of scale, orientation, and specific location. I argued earlier you might not be able to know the specific location; you could guess, but you wouldn't know specifically.

Based on just the movements of the sensorimotor, you would move in the 2D reference, in the 2D model of the logo. Those would be getting more and more off with the bend of the cup, so you make wrong predictions about the features you should be sensing at locations. I think it's very difficult to make an accurate prediction. That's my point.

If I saw the Thousand Brains logo wrapped over the top of a car, and in one image I'm looking at the side of the car, in another image it bends between the O and the J in "project," and another time you show me bending between the N and the D in "thousand," I wouldn't know either one of those is wrong. I would say, "Oh, that's wrong." I wouldn't be able to predict that. I would say, "Oh, the logo is going around the top and that's where it breaks." I wouldn't make a prediction where it's going to break. Unlike if I really had learned the object and learned a specific thing, I might learn where the prediction is, where—if I would then—I guess I'm just saying I don't know if we can predict that. I think that's valid, just so that I'm understanding this correctly as well.

The problem we're trying to solve is we have this canonical cup, and then let me copy over the scaled one. If we observe the behavior of the logo rotating, and we saw a deformed version of the cup, without having to learn the rotation again, we'll be able to predict—if we have observed this behavior in the standard—that we can predict what's going to happen if we apply that behavior to a different morphology. I thought the problem goes back prior to this. Maybe I'm wrong, or maybe my memory's bad; I haven't been thinking about this for a couple weeks. I thought it was more like we're trying to deal with basic deformation of objects and extreme versions, like a t-shirt, in-between versions, maybe a flag waving, and even the very simple one. I kept saying, "What's an egg versus a circle?" It was something along those lines. Is my memory incorrect about that? We were trying to say, "Hey, I have an object and now it's deformed somehow."

Like an egg is a deformed circle or something like that. I guess there are two parts to that. One is being able to recognize the object in its deformed state, which is the static version of the problem. The one we started with was the deformation happening as part of a behavior that you learned, like a ball bouncing and squishing, the circle turning into an egg, or the balloon deflating, and you're learning how you go through the deformation or how the object is deforming over time.

Those seem to be separate things. One is recognizing deformation of a non-moving deformed object. The other is being able to know the behavior of the object between those key states, those key frames. I can imagine a ball and a squished ball, and I can imagine the ball bouncing—that's a behavior I've learned. But I can also imagine a ball instantaneously turning into a squished ball or a ball popping out like a cartoon into a cube or something. I can imagine different behavior. Maybe I could imagine applying a lot of pressure to a sphere or a ball, and it doesn't collapse right away; it just moves a little bit and then all of a sudden jumps down. The actual behavior between these two key frames is not dictated. I'm just trying to separate out the idea that I have two images of the same thing, and I don't always have to have the behavioral sequence between them.

I think that's consistent with my proposal, that you'd have key frames of distortions that let you make these predictions about what to sense where. For the behavior model, you still have everything as we discussed before: it basically stores changes over time at locations, and you can recognize the behavior and predict. I'm saying I feel uncomfortable with the idea that if we're not observing a behavior and we're just observing the distorted result, we're going to use the mechanism for behaviors to help us out. They don't—I'm not proposing that. That's using the movement command. The movement command is kind of—yeah. I was thinking of that more as a general mechanism we could use for different things, but not necessarily behavior-specific. If you mean that, then yes, I did steal that mechanism to use it, and maybe I said I thought it was a great idea when you first mentioned it.

Maybe I'll go into what I'm proposing, and then we can talk. Keep going. Sorry, go ahead. I'll try to keep that brief, if that's possible.

How this relates to object behaviors: object behaviors can distort the object, like a balloon being distorted by behavior. We want to continue making correct predictions about the morphology, so we want to be able to move somewhere else on the balloon and know what to expect at that location.

Could we learn both the stepwise and the distortion map of an object? The stepwise model is the basic behavior model we've discussed for the past months, storing changes at locations over time. It's used to recognize a behavior and to predict changes to observe. The commutative one would be separate—what I'm calling the distortion model. It would store the difference between the original morphology of the object and the current time step, or generally between the original morphology and a distorted version of the object. This one doesn't actually represent anything in time, but you could learn an association between a point in time in a behavior sequence and such a model.

I would call this a deformation model, which can be used to make correct predictions about morphology after the deforming behavior has happened. It can also be used to recognize deformed objects and make predictions about them without learning a new model. It wouldn't represent anything with time and wouldn't be the same as the behavior model, but we could associate it with a point in the sequence. For example, if we went through this behavioral sequence, now we expect whatever object we have to be in this distorted state. To keep making correct predictions about the features of this object, we have to apply these distortions to our existing model.

What we would store in this distortion map is: if we are on a location in the object's reference frame, we need to apply the inverse of what's stored here to the incoming movement vector. Given any morphology model, what kind of transformation do I need to apply to get from the current location in that object's reference frame, based on how my sensorimotor moved, to the correct location where the feature is stored that I want to invoke?

Let's say I have a model of the balloon with all these features stored, like a logo. Now, since it inflated, those features are in new locations, and the deformation map would tell us how these new locations map onto the original locations in our learned model. For example, if we have the Thousand Brains Project logo and it distorted—the logo mark moved to the left, "Thousand" moved up, and "Brains Project" rotated and fell down a bit—this would be the distortion map. It would tell you, if you are here on the logo, the feature you expect is actually down here in your model, assuming this is the model you learned.

If I'm moving my sensorimotor from the "T" down to the "P," we'd apply that movement in the distortion map's reference frame. We would have a compensating transformation stored there and apply that to the movement, which would then tell us where in the object's reference frame we would expect to be. That would be the "P," so now we expect to be here on the logo, and that means we expect to sense the "P." This works independent of where you're coming from or how you're moving.

Another example: if we have a bending cup, let's say first the red movement. We are moving from here up to here. We apply exactly that movement of the sensorimotor to the reference frame of the distortion model at the location. In the distortion model, we have stored a compensating movement. We add that in, and this resulting vector tells us where we would expect to be on our original model of the cup.

As another example, if we do the screen movement, we are at the bottom of the handle and moving again to the edge of the cup. If we are here, we would also apply that to the reference frame of the distortion model. Add this orange arrow to it that's stored there, and that would tell us the correct movement in our model's reference frame.

In summary, this idea would use movement transformations in addition to the incoming movement vector from the sensorimotor system, applying translation and rotation so that the feature location under the deformed model corresponds to the location of the same feature on the non-deformed model and the stored post-transformation. What is stored here would have to be absolute differences between the current time step or the current deformed state and the default model that we have learned. If this is within a behavior, it would probably be too much to calculate on the fly. I was thinking of it more as key frames that we store, independent of a specific object. These distortion key frames could also be used to recognize distorted objects, independent of what behavioral sequence led up to it or if there was any behavior that led up to it that we observed. They would reduce the need for storing morphological states specific to objects. With this solution, we wouldn't have to learn key frames of specific objects anymore, as we discussed before.

I have a question. When thinking of a single column, how does a single column know how to integrate distortions in other places, at different locations? If you can only observe one location on that behavior, it would need to know the full behavior to know how to come up with this cumulative map of distortions everywhere. We were only thinking of a single column. This map would take a long time to learn. You wouldn't observe everything at once. This would be like learning a cup; you have to observe all of these locations as they're distorting. One way I imagine this could be learned is that if you are doing smooth pursuit and tracking a feature as it moves, you try to have a constancy that this feature keeps getting mapped to the same location in the object's reference frame, similar to how we discussed the thalamus learning that as an object rotates, features should still be mapped to the same location as in the original object model, basically preserving this feature constancy. You would have to learn this over time, the same as the behavior model where you can't observe all the moving parts at once, but you have to see it many times.

Would it maybe have template behaviors? My problem is, I'm thinking of this as observing a few features on the behavior and being able to extrapolate what's going to happen on different parts by just predicting or assuming, "Oh, I see this is the scale behavior," or "This is that behavior." Then you can say, "Okay, this location, assuming my behavior is correct, this is my prediction, this is what's going to happen." Are these cumulative distortion maps maybe leading into, if we cluster them somehow or come up with a way to categorize them, template behaviors that we can apply and easily infer and use? Or are they just going to be a different distortion map for every behavior, and we just have to learn every single distortion on a map?

Those don't tell you anything about a behavior themselves. They just say how the features of a model remap to new locations and orientations. There's no temporal aspect to this, no sequence, so I don't think it could replace or help with modeling object behaviors. It would replace storing key frames for specific objects, but if I understand your question, I don't think it could help with learning behaviors. In my mind, they're still describing behaviors.

These distortions are basically another way of describing what behavior has happened, or what this is about. That was the point I was trying to make. Roman, it doesn't feel like that to me. When I look at this, I don't feel like that's an object that's undergone some behavior necessarily. I do with the bent cup like this, but I don't with the conical cup.

Maybe you do get the feeling if you see this, that you imagine it bending, at least I do. That might be because you associate this distortion with a behavioral sequence. I've seen bent things, but I've never seen a bent cup like that. In this case, I just don't see cups like this, but I do see things that look bent like this. With the conical cup, if I've never seen a conical cup before and they're all cylindrical, but I've seen things made conical by some behavioral sequence, I might feel the same way about it.

I'm going to come back and lodge my general overall thoughts about this. This obviously seems like it would work when you think through it, but I don't think it's quite capturing what's going on. I'm confused. I don't think this is quite right. It's hard for me to put my finger on it, but it just feels off. There are too many odd things, too many corner cases. I feel like there's a simpler solution, or that's what it feels like. There's a simplest thing that goes on somehow that I don't understand yet. That's not a very strong objection—it's strong in my mind, but it's not a very good argument. I don't like it; it doesn't feel right.

Maybe it feels a bit complex because it solves more than the original problem we set out to solve. Maybe it's also that I have to learn these—what are we calling these—morph distortion maps. Distortion maps. First, I'd have to have a lot of them.

There could be a lot of different distortions. Isn't that right? Yes, that's right. Then, how many distortion maps do I have? It would cut down on having to learn key frames of objects, but it seems like I do learn key frames of objects, at least many of them. If I've never seen a cup like this, I would agree with you—I have to somehow infer that this is not something I recognize right away. But it seems like the general case is if I started seeing a bunch of cups like this, then I would say, "Oh yeah, that's my new model for that." It's the general case. We call them key frames or just static models of some sort.

I don't know if we would need that many distortion models. It seems like we have models of very general distortions, but once you get to more complex things, like a crumpled t-shirt, we wouldn't have a specific model for that. You wouldn't have a model for every possible complex distortion; you would have models for bending and scaling in different ways. But even for bending, I'd have to have different ones at different points, at different amounts of bending, wouldn't I? I feel like that wouldn't be too many models compared to how many there could be. It just doesn't feel quite right to me. I can't put my finger on it.

We made this transition once from thinking about object models as existing features at locations, and then we made this important but subtle extension, saying it's really orientations at locations. We can assign specific features at those locations, but there's a morphological model that's independent of the specific feature. That was a huge conceptual leap, and I think it's held up pretty well, even if we haven't worked out all the details. It didn't add any real complexity to our biological circuit. We had minicolumns represent orientation; we've always had minicolumns, so it fit. There was no real additional machinery needed.

Here, it feels like a lot of additional stuff, and I feel there's going to be a simpler answer. It's a gut feeling. I'll have to keep thinking about it until I become comfortable one way or the other. If we have a morphological model that's just orientations at locations for a logo on a cup, that doesn't tell us anything about the identity of the logo. If we're storing the child logo at a different location than orientations on the parent cup, and the idea of the logo changes, that's not really changing which minicolumns would get activated. In that case, it would actually work quite well already.

The thing that bothers me is the backward connections—how we inform where we would expect to be on the logo. Maybe we just need to work through the details of that part. Maybe we don't need to add anything like distortion models, and this already works, but making predictions about child objects, if we've learned the parent on a location-by-location basis, is still not super clear. There are a lot of things that aren't clear to me. I'm still struggling. I keep going back to my egg and circle problem, trying to see if this informs me about that. How do I even learn that basic morphology model? Is an egg a distorted circle or not? It feels like it isn't, but somehow, I don't know.

It's messy and confusing, and there are enough missing pieces that my intuition strongly says we're not there yet. There's something else, some conceptual idea that we're missing. That's just how it feels to me.

It doesn't feel like it's a hard conceptual idea, just that we haven't thought of it yet. Once you think of it, it seems easy. I have a question about how big these behavior models would have to be. This is something I've been thinking about, but I've never brought it up. Let's see if I can get to where you're at. If we had a simple situation like a stapler—it's really simple. Six months trying to understand the stapler. Simple in the sense that it's just a rotating thing; it isn't inflating. Would we really need a key frame for every position? No, because it seems like you could have something like a vector field that can apply anywhere throughout that sequence. That is the behavioral sequence, right? Isn't that the behavioral model? The behavioral model is that vector field, and it only exists once.

But then, if the stapler stops, the basic theory we have right now is that when the stapler stops, you immediately start learning it as a key frame—"key frame" is a poor term, but as long as we all understand what we mean by it. We learn this is a different morphological state of the stapler.

This touches a bit on what we were just talking about with Vivian's idea. When I see the open stapler, I don't have to imagine the flow through space to predict what I'm going to see. I just know what an open stapler looks like, and that's what it looks like. You do learn this flow field, but then if it stops, you start learning a morphological key frame. Just reviewing what we've said doesn't mean it's right, but is your point that you think it's too much to learn all that? No, not at all. Sometimes we draw these behavior maps, like they exist at each time point, and they only relate to the locations currently in play on the state. In some sense, you draw all these vectors in one plane, but they wouldn't all be in one plane; they'd be in different slices of time. Maybe I jumped ahead there. This isn't how we've been talking about it. We've been talking about each row of those arrows being at a different point in time. It's the same model, though. The way I've been thinking about it is, if we have the whole flow field, it can apply to any point in the sequence, as long as we know which arrows to use based on where there is matter at this location. 

Let me throw a counterexample, which might complicate that. Imagine the stapler goes halfway up, a quarter way down, and then all the way up again.

It doesn't make a smooth transition. If you can see, my arm moves, and that's just behavior you can easily learn. But now there are two points in time where the stapler is moving up in the same space, but at different points in the sequence. At one point, I predict it's going to come down again, and at another point, I pick it up again, but the flow field at that point is the same—the arm is moving through space at this location, at this speed. I have to have a unique representation; otherwise, I wouldn't be able to predict if it's going to go all the way up or come back down again. That's why you can't have this flow field in one space like that. It has to go through time because the representation for each of those points is unique. It's like a melody sequence where you have repeating notes. Words are conjoined in some way because, let's say, I'm just playing with a stapler, not passively observing. In this example, it goes like this, and there's a hysteresis problem. You need to remember it's not a history issue; it's a unique point in time problem. Let's say you're playing with the stapler, learning the stapler. I was describing a stapler that had a behavior on its own. It wasn't like I'm doing it—the stapler has this behavior on its own. But now we're not doing that; now we're playing with the stapler. I'm thinking about learning the basic morphological rules of this hinge. At any point in time, the hinge could go up or down. You don't know which way it's going to go. I don't, but let's say this is reversible, like the arrows. If we want to move the stapler down, we just flip the arrow or the arrows. Either way, I can learn this flow field by just two successive neighboring observations. If it's here and then here in the next minute, I've just learned a little part of that flow field, so I can build this vector field of possible motions, and that's a compact representation of the whole.

I just poked a few holes in it. Let's say that's the base model. We could apply a second layer on top to add particular cases of how you might move through those trajectories—something more compact than having to store a behavioral key frame unique to every point in the sequence.

He might even learn a sequence—not to sidetrack too much, but it might tie into how dense our behavior models have to be. We can be too dense, obviously, or we can't have too many points.

This could be the base behavioral model—very parsimonious, not requiring much, and learned through interaction. But I think we're mixing two things here, and we have to be careful not to mix them. When I've been talking about the behavior of the stapler, in my mind, it's the stapler acting on its own, going through some behavioral sequence. That's quite different from an object that can move but doesn't do anything on its own, and I move it up and down. Those are very different things, and we have to be careful. We have to solve both.

Up to now, even though when we actually operate a stapler, we have to use our hands, that's not how we've been thinking about it as an example. I've been thinking about it as if it automatically does its thing and we're just observing.

If I have to interact with it, then we're getting into the sort of problem where we should start thinking about that, because most objects don't move on their own. That's a different problem, or maybe a related one. I think that goes under actions and maybe causality and model logic behaviors.

That's another big topic we need to come back to, either in a future research meeting or an open focus session. I'm curious if Viviane can go over the issues she saw. I think I understood the model object behavior and how this could work, but now I'm curious about the issue she thought she saw. I'd be happy to go over that, unless you think it's already too complex. Maybe you'll make my case for me. I think the mechanism itself is very clear. Where it's unclear is whether we need this kind of thing. I can understand the composition of movement vectors to predict what feature to expect.

Good suggestion, Jose. Let's hear Vivian's issue. I'll try to be brief, and then we can see if it's something we need to go deeper into.

As I was thinking through this, I realized that all the examples I gave involved only one movement, and the whole thing doesn't work anymore if you do a second, third, or fourth movement because the distort sheets—what do you mean? Like the exploding logo? Or do you mean the sensors moving several times? Let me show you. Here's the example: we're on the T and moving down to the P. We go to the distortion map, move here, retrieve the distortion, and apply it. We correctly move in the object's reference frame and predict the P. Now we do a second movement: we're at the P and move to the N. In the behavior's reference frame, we're moving up, we retrieve this vector, and now you just look at the red one. If we add that vector, we're overshooting—we're up here now because the location in our object's reference frame has already been compensated by the previous distortion, and now we're adding up distortions over time. What we would need to do is still move with the actual movement vector that our sensor is doing in the object's reference frame—the green one. We are at this location, and then we apply the transform that's stored.

to retrieve the correct feature, which is n. This is the vector we apply, the green one, but we apply it from this location instead of that location. That's a little odd. Basically, when we move through the object's reference frame, we still use the correct movement of the sensorimotor. We don't apply anything directly to the sensorimotor movement. Instead, at every time step, we take the location we are at, given the sensorimotor movement, and temporarily retrieve a feature at a different location. We are not actually updating our location in the reference frame; we are just retrieving a feature at a different location when making a prediction.

Let me read this as well. We keep track of the story location in the object reference frame and apply the movement to that every time. We use the actual movement vectors in the object's reference frame, which is the green part, but invoke features stored at the offset locations.

I'm not sure how practical this is. The compensating movement would not be treated the same way as the sense movement. It would just be applied temporarily to retrieve the correct features, but not used for path integration over time.

Would another solution be to reset the cumulative distortion map every time we make a movement and then start building a new one? That would introduce a lot of other problems, and the distortions would be different at different locations. We'd have to learn a bunch of different distortion maps depending on how we move through the space. I think just temporarily using the distortions to retrieve a different feature would be the easiest way to solve it. We only store one distortion map, and we can use that and move arbitrarily through that space, but we still retrieve the correct features. If I understand, if we need to reset the cumulative thing, then just going through the sequence of behaviors through memory replay is easier. We don't have to reset this at all. We move through the object's reference frame, just using the movement of the sensorimotor the same way we did before. Nothing changes. We are not applying any extra movement commands from the distortion map. We are just moving through the object's reference frame, but when it comes to predicting which feature we're going to sense at that location, we need to take—so we're moving through the object's reference frame and through the distortion reference frame at the same time. Both the green and the movements through both those reference frames are in sync. The same thing is happening here and here. In both of them, we are at this location. Now we retrieve the compensating thing, but that's not applied to change our location in the object's reference frame. It's just used to tell us which feature we should expect. It just retrieves the feature at a different location.

So it seems like this one—is this one combining? When we do two steps in the sensorimotor, one movement and another movement, instead of doing movement compensate, which would lead to an incorrect prediction of the features, we add up the movements of the sensors first, and then wherever it ends up, we apply that one offset at that location. If I see this move as A and this movement as B—I'm just going to label everything—so, C, D. Before, we were doing A plus C and then D, but now we're just adding up the movements first and then doing an offset. That would work too, but I feel like that's more complicated than what I'm proposing. You would have to keep track of the movements and replay them after every next step. That would involve a lot more than just temporarily applying these. That would work as well and make the correct predictions, but to me it just seems a bit more difficult.

I thought that was your solution, so I'm not understanding your solution. Hopefully, I'm going to try to read again, but you can continue explaining. I thought this was your solution, which I thought was the same as replay. So, say it again. My solution comes out to whatever calculation this would come out to, but we are not replaying; we are just keeping track of the location in the object's reference frame, same as we always do. So A plus B basically moves us from here to here. As we make the second movement, we don't have to repeat the first movement. We're already here in the object's reference frame, so we just apply the next movement to know the next location.

But does it require looking up a second location for the features stored at the offset locations? I'm confused.

I'm just trying to understand. What you wrote out here, Hojae, is very helpful to explain this better. What I'm describing as temporarily applying the offset is basically not applying plus C in between in the sequence, but instead, as we do the first movement, we do A plus C and make a prediction, but only the A is persistent in our location in the object's reference frame. We stay at this location, which is off the object in the reference frame. I'm just trying to understand, do you have to look up two things? Do you have to keep track of two points in the reference frame to make this work—where you currently are and some other object, the storage location or something like that? Yes. You have to keep track of your location in the object's reference frame, then you have to apply the offset to make a prediction about the feature. But you don't have to keep track of the offset; it's just at this time step. You don't have to keep track, but then there are two reference frames: one for the displacement or something like that.

These two reference frames—that's the reference frame. But that's a problem in its own right, isn't it?

We're asking columns to have a lot of reference frames. That's actually my main concern with this—now we have three reference frames. I was already worried about having two, and now three seems too much to ask. It seems unlikely, not impossible, but unlikely. 

To wrap it up, mapping this onto a cortical column would involve two reference frames plus the behavior reference frame, which I'm not showing because it's independent. We would have the distortion model and the object model as separate reference frames. The distortion model would store transformations that need to be applied, such as orientation or location changes. A location in that reference frame would be associated with a specific transformation. We would then apply that transformation to the location in the optics reference frame to retrieve a different feature. In our example, the location in that reference frame would be here, and that's what we would keep for the next time step. The feature stored there, in this case, is nothing because the logo doesn't exist there. We apply this kind of offset stored in this model temporarily to retrieve whatever is stored at this location, which is the "p," and then we move again. The next movement would be applied starting from this location.

In theory, neurons could do things like this. It's asking a lot of them, but it could potentially work—looking up an offset in one set of minicolumns and then applying that offset to the location in another. But when I think about grid cells, I have no idea how you would do that. It seems complicated. We could implement it in software; I'm sure you could make this work in software. In software, it would be pretty straightforward. But I agree, I don't know how this would be done with grid cells. 

Maybe some problems would be simplified if we do this hierarchically, where the distortion model is in the parent column and we can do some kind of transforms in the thalamus. But I'm not sure if this would solve any problems—just throwing out that possibility. I'm always worried that we could implement it, but it might have consequences down the road, like digging a hole we'll have trouble getting out of in the future. We try to make it work for the next thing and the next thing. 

I think this is a great proposal and great for making us think about the problem, but I'm not on board with the solution yet. I feel like we're missing something or not thinking about the problem correctly. It's not that the solution is incorrect, but we're not quite grasping the entirety of the problem yet. This problem is really broad. We started off with object models, then split into morphological components and specific features, then did compositions, then distorted compositions, and now we have behaviors and deformations. All these things have to be solved. I just feel like something is missing. It doesn't feel like this wraps it all up for me. It feels like hammering and patching things.

If this was the solution, it would solve a lot of problems, even ones we didn't try to solve, like recognizing distorted objects. But the solution still seems too complex, especially to be implemented in grid cells or cortex. I thought it was useful to zoom out and think about distorted objects in general, since behaviors seem to be related, so maybe there's a general mechanism we can use for both. But I agree, we need to zoom out further. I have trouble relating this particular problem to the broader context of all our problems.

It's not clear to me—when is an object distorted, and when is it just another object?

You brought up the issue of the staple having two things: a behavior when we interact with it, but it could also have a behavior on its own. Those are separate things. We're mixing these together in a way that makes me feel confused. I don't think we've got it quite right.

I'm still trying to understand the problem, so I'm going to try to share the other brainstorming document.

Is this sharing? No. Is this the one from brainstorming week? I looked at that. Viviane, you took a lot out of that. One second, it's looking into a new one, right? So this is a different document. The link is also still on Slack—there are two links for Excel, and it's the second one.

The object distortion—the most direct question I was trying to answer was: if we have a behavior model and an initial morphology model, what features should we expect in the morphology model after the behavior is finished? Implementation-wise, in software, we can implement this dissolution map or amp replay. I think both are doable in software.

I think the more difficult question is whether we can use this independently of the object it was learned on and apply it to another object. For example, if we've seen something change from cylindrical to conical in a mug, and then see a completely different object that goes from, say, a cylindrical to a conical form, can we predict the features of that new object? How do we generalize behaviors without having to learn them again? I'm not sure if we can; that's my current position right now.

The gamma map I drew would be independent of behaviors—it would just be a distortion. The general question is whether we can apply these distortions to new objects if we've learned them on one object, just as we can learn behaviors on one object and apply them to another. The question I started with was: how do we take a behavioral model and an initial morphology model, go through the behavior sequence, and then make predictions about morphology? In particular, how does that work if there was an object distortion in the behavior?

We talked about hippocampal replay and using that for this, and the proposal of distortion maps, both of which are doable in software but might be difficult in biology. The scale map takes it a step further, addressing a separate problem: it doesn't solve the initial question but extrapolates, suggesting we can use this for static object distortions where no behavior happens, but we can still recognize the object.

I'm not following all of this, but I'll try to play back some of it. It seems like you had this idea back in the research offsite: we can apply these.

Let me start over. The idea is that on a location basis, we can apply scale and orientation in a hierarchical fashion. That seems promising and could explain some of these scale differences, like the skinny cup versus the non-skinny cup.

I think that's a good idea. We can do this on a point. Regarding Scott's concern about the models being too big, we don't have to store every point because the system can extrapolate between points. This general mechanism allows us to memorize, at certain points, the orientation and scale change for the current scenario.

I think that's what we're talking about. Was that what we were just discussing, Hojae? Yes. The question under that map is about learning things on a location-by-location basis for a specific child-parent object relationship, but in a general way. The specific object ID passed in layer four or three is always optional. It was optional in our morphology model; you don't need it for the morphology model. So, just like we have a morphology model not specific to feature IDs, these features—orientation and scale—can also be independent of particular child object IDs, and the system should work. It's just an extension of that idea.

I'm not sure why that doesn't work in all scenarios. It seems like it should. It's not too much of a learning problem because we don't have to store every point, just enough to extrapolate between them.

Sorry I wasn't following everything you were saying, Hojae, but that seems like the essence of how we're going to solve a lot of problems. I'm not sure where it fails. Maybe Viviane or someone else can explain what I'm missing.

I'll give an example. Is there a stapler picture? Okay, there we go.

Coming up—it's got the stapler below. Let's say we learn this opening. If I see a laptop, how do I apply this opening to the laptop? That's a behavioral model now, right?

I thought we had that worked out. The behavioral model itself, as we've described it, makes sense. Having two models—one for morphology and one for behavior—makes sense, and I think we can make that work.

The behavioral model is independent of any specific object. Our behavioral model should work with multiple things because it doesn't care about the specific object. We spent a lot of time trying to predict what would happen in a novel situation. I thought we had worked that out, but maybe I don't recall. For example, with the stapler model, we have to map the closed position to the laptop's closed position and another position to the end of the laptop. This is a location basis for the stapler, so how do we stretch it out for the laptop? That's a separate question about scale.

We're going to run out of time soon. Here's a parting thought: before the Thousand Brains Project, when we first came up with the idea of using reference frames, we didn't understand the importance of orientation. We started by putting features at locations, always asking what features are at this or that location. Maybe the way to think about this is that the actual feature IDs are secondary. The entire system might be understood by modeling morphology, orientation changes, and scale changes independently of specific features. Our models of static objects and behavior can be understood as things not tied to specific IDs.

Often, in the world, we don't see color. If it's dark, we can use different modalities. Maybe the world is really structured as edges, orientations, movements of edges, orientations, and scale. As an afterthought, we can apply specific ideas to things, which helps us recognize specific objects or flavors of these things. I've always started with specific objects and added more generic features to them. Maybe the way to think about the whole thing is to start with the generic features—understand behaviors and morphology independent of specific features—and then apply that to specific features afterwards. It's a bit of a mind shift, and we're already doing that, but I don't think I've ever stated or recognized it before. I don't think anyone has said that's what we're doing.

Does that make sense to anybody? I think that's a good approach, and it might solve some of our additional problems. The first thing that comes to mind is that we still want to be able to make predictions about features. If I apply a behavior to a stapler, I want to predict its color or the color I'll sense at different locations correctly. We still have to do that. We're not ignoring feature IDs; we're just saying the base model is independent of feature IDs, just like we said the base model of a face is independent of specific feature IDs. I can see a face drawn with fruit, with dots, or made of anything, because it's about orientations, positions, and scale. In the end, the noses, ears, and eyes really do matter, but we can add that later. It's just a flipping of priorities. I'm going to think about how to solve all the problems we've discussed as abstract ideas of morphology and changing morphology. We've already done that for behaviors and added the morphology model. Maybe I'll start thinking about everything in that regard and make sure we can solve everything using that mindset. Then we can come back later and ask, how do I predict the actual feature I'm going to see? How do I make predictions when I can? I can't always make predictions, but when I can, what would it be? 

To add more to the stapler and laptop example and how to frame the open question: I thought we already had a full solution for the stapler and the laptop and making predictions about them. The last missing piece was applying the movement vector to the child object's reference frame, and that solved everything—applying the behavior, recognizing it on a new object, and making predictions on the new object. The main missing piece was how that mechanism works on an object that's distorting. With the stapler, we don't have that problem because the whole child object, the top piece, moves together, and we can just change its location and orientation. But with something like the balloon, we can't do that. That, in my mind, was the main remaining open question. 

You spent a lot of time today talking about how the logo distorts as the object distorts. I thought that might be related to general object distortion. The change in attitude I'm suggesting is not to worry about that yet. In fact, I even pushed back a bit—can I really make a prediction about that? I'm not sure I can predict what that thing will look like as it's distorting. Even if we don't make predictions about features, we still need to be able to deal with distortions in morphology. I agree with that, and that brings up the egg and circle problem, and all your cup examples. What I'm proposing is not a solution, just a mindset. Let's deal with the balloon problem, but not worry about predicting what the logo looks like. Maybe I need to predict the distortion of the surface in general.

The reason the logo was brought in is because, in past meetings, an argument for the balloon was that you could solve it with rough scale, or by putting the logo on the balloon to show the issue that you can generalize that deformation to new objects. You can apply the same deformation to the logo or any new logo. That's why I brought it in—to show the generalization. It was a good exercise, but now I'm arguing that we don't need to go through that right now. If we all agree these properties need to be solved, then it would be much easier not to think about the logo. The balloon is just a weirder case of the egg and the oval—it's a more extreme case. It was a great exercise, and everything we talked about today was great. If today's exercise gives us a new way to think about these problems, then that's good. I'm definitely going to think more about this. It's a weird thing to reframe all these problems as morphology problems in a broad sense, but it's very liberating. Maybe next week we can talk about eggs and ovals.

Yesterday, I was trying to write a little about the nature and structure of the world, trying to get down to the base of what's actually going on and how we're modeling it. This idea is really interesting and helpful to me—thinking about the world in terms of its basic morphology and changes in morphology.

So much of the world is like that. So many things we see are like that. Specific details, like color, are not persistent. A green isn't always green, so I can't say if it's this color, it's a mean apple, because in different scenarios it isn't. But the shape is always the same.

I'm thinking out loud. I think it's a liberating way of thinking about it.

Maybe we can try to think about behaviors again, but without any features for location—just in terms of a black and white world with edges. As Viviane just said, Hojae, we had a pretty good solution for behaviors. The challenge that Vivian's bringing up is distortions, and I don't think we have a good definition of distortions. I was arguing earlier, but what we really want to do is think about the entire collection of things we've come up with in the Thousand Brains Project, starting with this morphology-based view of the world, and then solve things like distortions and behaviors, which many of us have already addressed. Then we can ask how to fold back in specific behavioral IDs as we go.

I think we have behaviors pretty well, pretty close. Maybe it was too much to bring in distorting behaviors when we don't even have a solution for distorted static objects. It might actually be a separate problem.

One area I'm particularly interested in with this behavior model is the distinction between the cumulative version and the totally differential version. I'm not a fan of the cumulative version. That was a new idea Viviane brought up.

I think it came up last week as well. I'm also a fan of the differential version. It makes more sense to me in terms of how you'd learn it, and it doesn't necessarily require a supervised starting point. I like the idea of it. It also seems to fit with the idea that if you asked me to sing a melody, I couldn't just start anywhere; if you asked me to start halfway through, I'd probably have to go back to the beginning. I like the idea of these differential models. Maybe that's an area for future discussion. When you say differential models, do you mean how we've been talking about behaviors so far—change, step by step? Step by step application, having to integrate through the whole model. I'm a big fan of that. I don't want anyone to think I'm saying we shouldn't do that anymore. This is still how I'm thinking. We are learning and recognizing behaviors. In my mind, that's totally independent of the distortion maps, which are something on top. They wouldn't replace a behavior model; they would just be there to remap locations in the morphology model to predict the correct features as it is distorting.

What would be helpful for me, or in general next week, is if we can end with either a question to think about for next week, or if we can't say that concretely now, at least post on Slack earlier so we can get a chance to think about it. I think I'm going to think about the egg and oval example, just focusing on morphology, and see if there's a simple solution to this kind of distortion. The whole idea of distortion is both unanswered and poorly defined. My question is, what about the egg and oval? The egg, the balloon, the bending coffee cup, the t-shirt—these are all variations of distortion, and I don't think we have a crisp definition of when something is distorted and when it isn't. The flavors of it. The problem Vivian's been working on is still a good problem, but I don't think we have clarity on what the problem is. We have some examples, but it's confusing if they're all the same, if they're different, or if they're on a scale. I don't know.

If I had to define it, in my mind it's when the same feature now appears at different locations and orientations in the object's reference frame. If we're just thinking about morphology models, we could forget about features and say there are changes in orientation at different points in the reference. In little cases, parts of the object will now exist at different locations and orientations. But that also applies to behaviors; behaviors do the same thing. I think we're talking about deformations as something different—behavior, flavor, or something different than behaviors. I don't think that's a good enough definition yet. Maybe it is. I need a definition that clarifies all these things and shows how they're related. This is A, B, and C; they all fit into some scheme.

I think, Hojae, we need to spend more time thinking about deformation. We need a better definition. I think we got bigger just through our proposal definition. I'm going to see if I can improve upon that. Other people can do that too. We can come up with more examples of deformation so we have more of a spectrum. Maybe we're missing some good examples that would help clarify things.

To me, the most important thing is to have a better definition of the set of problems that deformations relate to. Behaviors could be in that spectrum—behaviors could be deformations that occur over time versus deformations that don't occur over time. I don't know. That clarifies at least for me what to focus on for the week as much as we can. Sometimes it's really hard to do this.

Are we done? I think we're at a good point.