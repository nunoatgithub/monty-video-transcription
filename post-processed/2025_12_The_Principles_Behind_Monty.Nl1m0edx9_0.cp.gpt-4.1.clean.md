Scott Knudstrup: What I'd like to start with here is this question about cortical columns. If you've been following the Thousand Brains Project, you've certainly heard some talk about cortical columns. Because it might not be obvious why they're so fundamental to Thousand Brains Theory, I'd like to spend a few minutes talking about what they are, why we care about them, and how they inspire Monty.

Cortical columns are found in the neocortex, which is a thin sheet of tissue wrapped around the old brain. The neocortex taps into the more ancient structures below, sometimes augmenting the old brain's sensory systems, and other times granting entirely new capabilities, such as language. Structures in the old brain come hardwired, custom-made to serve some narrow but system-critical purpose. The neocortex, on the other hand, shows up with few concrete skills, but it's eager to learn—a flexible, general-purpose substrate that learns on the fly. It sounds like a good idea, but how do we know it works?

Nature's go-to strategy has been to keep printing it, and it seems to be working. Even as our brains and our ancestry have been getting bigger as a whole, the neocortex takes up a greater and greater portion as time goes on. We know that it works, but how? One important clue came in the 1950s from legendary neuroscientist Vernon Mountcastle, patron saint of the Thousand Brains Project. While cataloging responses in the somatosensory cortex, he noticed that cells in the same narrow, vertical band always respond to the same kind of stimulus. For example, if you were to perform this experiment on me, you might stick an electrode in a region of my brain that responds specifically to sensations of the fingertip, labeled in blue. While applying various kinds of stimuli to my finger, you might find a narrow, vertical region whose cells respond only to light pressure against the skin. But if your electrode crosses into the boundary of a nearby column, all the cells there might respond only to flexing of the nearby knuckle, or to heavier pressure, like deep tissue pressure. This was a remarkable discovery, and it didn't take long for others to look for the same kinds of themes in other brain areas.

Within a few years, David Hubel and Torsten Wiesel famously discovered the same pattern in the primary visual cortex. This was good enough for the Nobel Prize. There, columns identified visual features, such as a specific orientation of an edge. Here, I'm showing a handful of orientations.

In the visual cortex, they also found that columns were bundled into bigger columns—hypercolumns—which form a larger structure that binds together features belonging to the same region of the visual field. Columns, or mini-columns as we might say to distinguish them from hypercolumns in primary visual cortex, turned out to have real meaning for the brain. Their discovery suggested a new kind of building block for modular design, causing a lasting shift in focus toward understanding the form and function of cortical columns.

In the words of Mountcastle himself, columns are the basic functional unit in the neocortex, each performing the same essential computation. But what computation is that, exactly? You're probably aware that the Thousand Brains Project is rooted in the notion that cortical columns are fundamentally spatial creatures, placing their knowledge of the world into reference frames, which they also use to guide decision-making behavior in a continuous sensorimotor loop. While column-to-column interactions are enormously important, we begin by taking a more maximalist view of what columns are capable of on their own. As a motivating example, consider the following situation: imagine you've been blindfolded and instructed to identify an object placed in front of you using only your fingertip, which is something we do for fun at the Thousand Brains Project. Since your fingertip projects only to a small region of the somatosensory cortex—a column or so wide—this task rests on the intelligence of just one column.

We take our first observation, which feels like a rim of some sort, and then another. You're starting to think maybe it's a cup or a ball, something with a rim. Moving over, you feel some kind of protrusion, then move back to the body and feel a cylindrical area. You start to think, could this be a mug? Maybe it is a mug. That seems fitting, consistent with all the observations so far.

Now, think for a moment about what needs to happen for the column to arrive at this conclusion. First, you need to know where in space its sensations came from. Second, it needs to assemble all these locations into a coherent 3D model. Finally, it needs to compare that model with all the objects in its memory.

Let's run through a similar example, now pivoting to Monty and its architecture, and how it's inspired by this example.

We'll substitute in Monty's components here. Instead of a finger, we'll have a camera, which captures a small field of view image. It's analogous to a small patch of retina, only observing a small portion of the object at a time, like our finger did in the previous example. Then we'll have a sensor module, which performs location and feature extraction on the image patch. You can think of the camera and sensor module like the retina and the thalamus. We'll also have one learning module, which stands in for the cortical column. Finally, we'll have a motor system, which drives the camera and controls its movements.

At T equals 0, the camera takes in a small part of the object's surface, which it passes directly to the sensor module.

The sensor module determines where the patch is located in space and extracts features from the image, such as curvature or color, sending this information down to the learning module.

The learning module compares the observation with points it's seen before, looking for potential matches. At this point, the curvy red patch strongly reminds Monty of the location seen on the mug and the cup, and less so of points on other objects, like the blue bowl. Right now, the mug and the cup hypotheses are top contenders, but the learning module doesn't yet have enough data to decide between the two. Where should it look next?

The smartest thing to do would be to look for what distinguishes them—in this case, a handle.

And so this is exactly what Monty does. It sends this location down to the motor system, whose job is to translate the target location into a sequence of commands that will get the camera pointed at the target, such as turning right by 10 degrees and up by 5.

After moving the camera, we repeat the process. The small patch is processed by the sensor module, which extracts the location and a set of features, then sends that information back to the learning module. The learning module's hypotheses are updated in light of the new information. For each hypothesis, the learning module considers: if I started out looking at this specific point on this specific object and then looked over to the right, I would expect to see X. If what I'm actually seeing matches that expectation, as it does for the mug, I'll increase my confidence in that hypothesis. Otherwise, if it doesn't match, as for the cup, I adjust my confidence in that hypothesis downward.

When one hypothesis stands well above the rest, the learning module reports its final verdict.

A quick clarifying point: the illustration on the left might suggest there's one hypothesis per object, but in reality, there are many. On the right, you can see that each point is a different hypothesis—many associated with the mug, many with the bowl. This allows Monty to perform accurate pose estimation in addition to object identification. There are many learning modules that can work together, just as there are many cortical columns in the brain. They can communicate with each other to perform faster inference, communicating laterally, or be arranged in a hierarchy to learn and recognize compositional objects.

To recap, Brain and Monty share many features by design: modularity, modality-agnostic functions, and learning modules that aren't specifically designed for vision or touch. They have a narrow input locus—cortical columns and learning modules—with precise spatial mapping capabilities. They function alone or in an ensemble, and also provide motor output. Cortical columns contribute to the motor side, enabling continuous sensorimotor loops.

Thank you for your time. I'll pass this over to my colleague Hojae, who will talk about how to get started with Monty and run your own experiments.