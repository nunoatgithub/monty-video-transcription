Okay, so this is a very cut and dry way of looking at things, and I don't know if it's the right way, but I split it into three categories. The first is figuring out what information is transmitted across the AI bus and what that information is. The second is that each module has to operate on that information, and there are various things that have to happen. Some modules could do different things, but the AI bus operations would be common. You can look at it two ways: how they work with that information, and also that they could do different things with it. For example, a vision module might do something very different, and an episodic memory module is different from the standard. I don't even have that fully defined.

The third category is about things inside the modules that we've discussed, which are common but unclear whether episodic memory would do them, such as updating based on motion and similar operations. I can go through each of these in more detail, but this is how I split it up. The first question is about things we discussed last time when Marcus presented: how are object ID and location in the external world, relative to the body and orientation, represented? There are many options that have been mentioned, like SDRs, lists, vectors, and particle filters.

There's also DDC, which I think stands for distributional distributed coding. It's similar to SDRs but with probabilities. It's a small community working on things similar to SDRs, but they don't really discuss high dimensionality and sparsity, though there are relationships. I think there's a nice match between SDRs and DDC, and you could see combining those two.

Then there's the question of how much of the representation is learned. Marcus presented an extreme view last week where the entire thing is learned by backpropagation. Another issue is how much information is represented jointly—are these three completely separated, or is everything joined into one representation? Those are the questions to go through.

A separate question is how uncertainty is represented. We've talked about a couple of different ways to do that. It's tied into the representation question, though there might be different answers for location and orientation than for object. That's why I've separated them. If we do them all jointly, they would be the same, but it might be slightly different. How are location, orientation, and uncertainty represented, and how does voting and ambiguity resolution happen?

You could imagine that this might belong in the second category, but perhaps there's something in the AI bus that handles it. The representation scheme might dictate how that goes. These are all tightly related. Is object similarity represented? This is a new question added after last week's discussion. Are all objects distinct, or do we represent similarity? We all like the idea of having similarity represented.

Another question came up last Tuesday: do we need to represent and communicate the current common location? For example, if it's represented in relation to my body, what happens when the body moves, or if it's related to something else? We need to communicate that common point. This is an area where you could deviate from biology if you wanted to. The important point is that at any time, every participant on the bus knows how to generate their object-to-body information, and everyone needs to know where they are relative to that common point.

That's something that could change. In humans, our body moves steadily through the world, but in a system like this, you could abruptly change everything. The question is, would you need to communicate it? Yes, you'd have to, because everyone needs to know their new relationship to the common point. In a physical body like a human, you could imagine scenarios where this is done abruptly, and yes, you'd have to communicate it. The question is, do you communicate? Everyone has to know where they are relative to this common point.

I'm considering the possibility that the common point changes with every attention shift, and that's an interesting idea to keep in mind. Introspectively, it doesn't seem that way—when I attend to things in the world, their positions feel relative to my body. If I turn, I don't feel like the cup has moved relative to my body; my sense of the cup's location remains in relation to myself. It doesn't feel like that's changing every time, but as long as everyone knows, it has to be communicated somehow.

I was thinking about how our senses and body move continuously, but you could imagine a system with multiple sensors, like on a factory floor, that learn to model movements. If you rearrange those sensors, perhaps in another room with new cameras, you could switch to those cameras as if they had simply moved, even though their positions relative to each other are different. The system would continue to work, just with the sensors in new positions, resulting in a discontinuous flow. That information would need to be communicated to all participants, possibly as a preset configuration of sensor positions.

For example, with the location of my sensors and fingertips, I can only move so far from my body, but in a system with robots moving independently, there's no longer a fixed reference point. If a robot's finger is not fixed to its body, or if you have a distributed robotics system with multiple robots, the bus becomes more dynamic. This is similar to the camera scenario, but now the cameras or robotic arms can move and take over tasks from one another, such as on a conveyor belt where one arm picks up where another left off.

This raises the challenge of allowing many robots to use the same bus for communication, relaxing the constraint of a shared spatial location but introducing the challenge of mapping their relative positions. One restriction is that the system is only useful if multiple sensors can sense the same thing at the same time, since communication is about observing things relative to a point. If sensors are in different rooms, they can't observe the same things simultaneously, so while you could switch between them, they can't communicate about shared observations in real time. At any moment, sensors in different locations can't interact usefully because they can't observe the same things.

They can't be observed in the same logic. My point is, it's only useful if two sensors, like my eyes and ears or fingers, can at some point observe or attend to the same thing. Then they can communicate with each other. If they can't, there are other ways to communicate, but not directly on the bus—it's like language. The major benefit is that these things can work together because they can communicate, sharing observations of the same thing from different positions or orientations. That's part of the benefit.

I have a question about object similarity and object identity. How does that work? Looking at the whiteboard, there are different E's; they're all different. There's the issue of how to take E's of different shapes and classify them as an E, but then there's the question of E and F—similar representations, but different objects. Different scales don't matter. There's an issue we haven't addressed: the state of objects. We've talked about states like laptops opening and closing, or smart home devices. For example, an E, an italicized E, and a bold E could be different states of that object. Not all differences are understood as states, but many objects don't have rigid morphology, and we see them as the same even though we know they're different. I'm including the issue of state here, which might have the same solution as similarity, but that's not exactly similarity. A stapler is a stapler; they're similar, different instantiations of the same thing, just like letters.

My handwriting might be different from yours, but you might recognize it as mine. There are distinctions among different things in the world, and some haven't been addressed. It might be a mistake to ignore them, so I included that as point G. We can do useful things without solving that initially, but we do have to solve it eventually. Both of these may be things we can figure out later. There are many advantages to the plug-and-play architecture of the bus, and the number of applications is large, but it could still be quite good even without solving everything up front.

The point is, this could help structure our meetings. We would focus on these things in terms of what operations modules perform. For example, Abhi asked how we do associative learning of object ID, location, and orientation—how would different modules learn? A related question is how to implement many-to-many mappings between modules, including one-to-many and many-to-one. If you have an audio module, it might associate a sound with many different visually distinct objects. From the audio standpoint, it's one thing, but you'd want to associate it with many physical objects.

There's also the idea of plugging in a module and having it work. What does it mean when a module inserts itself into the bus? In a system using sparse representations with associative memory, I can see how that could work. In the real brain, if you added new columns to a cortex, they'd have to learn, with new connections. In an engineered solution, you could design it so you could instantly plug in another camera and it would be useful. I can explain that from the model in the columns paper, but I don't know how to do it in other systems. It would be a nice attribute to have, even if brains don't do this.

A big question is how to transfer knowledge from one module to another. How does that happen? I have some thoughts to share, and each of these could be a topic to dive into.

Within a module, it needs to be able to do certain things to participate in the AI bus: how are internal locations relative to the sensor and orientation represented? How is the pose calculated for transmission on the bus? How is the graph represented and constructed? How do you query the graph? When you move, how do you update the external sense of pose? How are internal reference frames represented and updated? These are overlapping questions, but I split them out for discussion.

One core area needing research is unsupervised learning of reusable parts. Would you say that falls under any of these categories? How do you learn a good set of parts or objects to construct others from? I didn't include that here, so it would be a separate question. One advantage of the bus structure is that you have flexibility in how you implement a module, as long as it works with the protocol. That's why I don't care how you do it initially; as long as it works with the protocol, the internals don't matter.

That way, we wouldn't get stuck on not knowing exactly how neurons do something. You could develop partially engineered solutions that may not learn as well or have all the representations you want inside the modules, but as long as they communicate properly and adhere to the protocol, they're still useful.

From a biological perspective, there are many kinds of eyes—some primitive, some with color, some without, some with multiple lenses. Some are better than others, but all are useful. You could think of it that way: there might be an ideal way to model with a particular sensor, but you could start with something less than ideal. Teams could work independently on their own modules, upgrading them as needed. One question is whether there's a difference between object IDs, how objects are represented, and how object parts are transmitted, or if it's just different objects. This is about the generic operations within modules that every module would have to do. If it impacts the bus, it would be included; otherwise, it might not matter. You could have a module with no learning that still works on the bus. Modules could represent graphs in different ways, so maybe these aren't generic. We'll have to decide how to build some of them, but the representation could be completely different inside each module.

That's why I don't mind if people use standard CNNs or ANNs inside the module. It may not be how the brain does it, but it doesn't matter right now. You want to build something that works, as long as the module performs the right functions. To communicate from module A to module B, you have to adhere to the bus protocol and be able to query something in the graph. The query has to be global—location-based or object-based queries. Not all modules can answer object-based queries; for example, the somatosensory system can't identify the red cup. Not every module can answer every question, but as long as it outputs the right information, that's fine.

You need to be able to do some useful communication on the bus.

Imagine this as an established platform with many companies and people building different modules. Someone might be an expert in infrared sensing, another in a different type of sensor manipulator. They can build their modules any way they want, as long as they communicate properly. In enterprise systems, there's often a bridge interface that translates messages between modules. The module translates from its own object to the other module's object. There's no internal knowledge shared. For example, the auditory module doesn't know what the visual module knows; they've just learned to associate with each other. The auditory module senses something at a location, and the visual module recognizes it at that location. They don't need to know each other's internal structure. The query is in the format of object ID, location, orientation. There might be other things transmitted, like state, but at a minimum, this is the language. All queries must use this language. The goal is to avoid any internal knowledge of other modules' workings.

What binds the associations together? What learning process creates those associations? We haven't decided that yet. That's the obvious question we'll get to—it's the bullet point on the upper right. I can see how it works in a neural system; we used a particular method in the columns paper. It's a starting point: an association of sparse representations, where yours and mine are pretty similar.

Before we continue, Marcus proposed something different last week, but we should all at least understand what we did in the columns paper. That's a very simple version of how it works.

We walked through the voting presentation. Not everyone may have seen it, so there's no common language to be used. The common language is orientations, but it's not even a language or a common representation. There's no specific format passed over the bus; everyone has their own format, and then you associate and link them.

In the color scheme, which I haven't seen in most computer systems, that seems to be what it might do. It's powerful, whether you're doing a playmark as a pretender or a play that did it. Do you want to dive into one of these now? You had something you wanted to talk about. I think we should start tackling these things.

To me, this is still the first place to start.

As for this question, we have these three variables. At the moment, I think they're sufficient to get started, though not sufficient ultimately. We should debate that. Does anyone see an obvious hole? It doesn't represent state, commonality, or actions. Actions could be defined by a series of these over time, but they're not directly represented. This would let you do inference of static things.

Are sequences a problem for later? Sequences could be handled in two ways. Each module can learn sequences and predict what's going to happen in its world over time. The question is whether that knowledge is shared over the bus, or if the bus just represents the current state. For example, if I model how a stapler opens, I could output a series of objects and orientations over time, resulting in a constantly changing representation. There's no explicit representation of the sequence, just encoded patterns over time. Alternatively, there could be a representational concept of state, like "open stapler" or "stapler closing," and you would pass those over time.

Imagine a language module looking at the bus and describing what's on it. It could say, "There's this thing over here with this orientation," or "There's a cup over here facing this way." Language also includes actions, like "the water is flowing" or "the car is going down the street." How does it get that from the visual system? Is the visual system just reporting a series of locations, or is it saying, "This is an object moving in this direction relative to the body"? It could pass trajectory information instead of just location. So, both recognizing external sequences and generating internal ones are necessary. The modules have to do that, but the question is what goes over the bus. Does the language area see a sequence and describe it as moving, or is there something else translated on the bus that associates the object with the verb "driving"?

I don't know the answer to that.

I'm willing to proceed with these three things, but the real limit of the system will become clear as we build and learn more. One thing I see missing is a coordination mechanism—something abstracted away to ensure things are synchronized. Are you talking about time? Yes, in time, but also being able to synchronize the bus. That's interesting.

That's more of a substrate question. For example, in one substrate, you have neurons that are always active, maybe cycling at some frequency. Maybe they're all cycling at the same time because they're attending to the same thing.

You can imagine the transmission times have to be short enough so that they arrive within the same timeframes. If you're building this in software, what would be the substrate? Typically, you think of either event-based or time-based systems. In this case, it's interesting because waves propagate across the brain—not just point-to-point communication delays, but a purposeful rolling wave of synchronization. That could be abstracted away. My first guess would be to abstract that, since we don't want to model all the biophysical properties. You could use a clock-based system where everyone runs on the same clock. It's a non-biological solution, but it could work. This is similar to how PyTorch works in distributed systems—just discrete steps. Are there downsides? If the brain has adapted to finite propagation delays, it might treat long connections differently than local ones, but that could be abstracted away within each module.

For example, if I have an auditory module doing speech recognition, it could post recognized sounds to the bus, reporting its current state at any moment. The distinction is whether you see coordination as a defect to overcome or as a source of emergent properties. For instance, with exponential signal decay, you naturally get locality. If you get a stimulus here, and myelin only goes so far, the brain has to work around that with various tricks. For the neocortex communications layer, not within a column but across space, I think it's more of a problem than a benefit. My stance is to always aim for the simplest possible solution and see if that's too simple. Each module can handle distance and timing on its own.

I don't think there's an advantage to the brain having the area representing my left hand far from the right, except for doing my right hand. If everything could be close together, it probably would be. My first stance is to have a bus structure that acts as if there are no delays. For the goals here, that's a good assumption. In the back of my mind is how the brain constructs itself during growth, but that's not relevant to the current problem. We're trying to capture the essence of information processing, not recreate a brain. The key question is what can be abstracted away. I would abstract away timing. I don't know enough about PyTorch or other frameworks to say if that's important, but it seems like a basic, soft synchronization. All the basic substrate communication is easy to handle.

These questions are only important if they fundamentally impact algorithms. The purpose of this meeting is to discuss assumptions and determine which are correct. The underlying communications protocols should be simple.

Since I missed the first meetings, I want to understand something fundamental. With the AI bus idea, are we trying to strike a boundary between neuromorphic and machine learning architecture, or something in between? The goal is to build something today, not neuromorphic hardware. The AI bus is meant to be useful now, not just to explain the brain. This is about creating an architecture for AI that is viable today. It's a technological platform aligned with the company's goals.

Just a question: when two people in the back corners of the room are talking at that volume, can people online hear them well?

Someone: Yep.

What I found when I'm online is that people in the corners can't hear. So, we can put that aside. We have these three ideas, these three variables. I'm nervous that we might be missing something important, but we can start with just those three, like the solution Marcus presented. There was another space, a placeholder, letting everything fall into one thing—object, state ID, location, orientation—letting each choose how to represent it. We don't know how to do object state at all; we have no theories about that. I don't care about it right now, and we haven't discussed it here, so let's not deal with it yet.

Marcus is proposing you can just add things and they'll figure it out. That might be true no matter how you represent these things. If I'm not mistaken, you tried to represent a stapler opening versus closing with grid cells. We were representing how a module could learn what a stapler does—how it learns the stapler opening and closing as a sequence of displacements. These modules can still do that. The problem is dealing with what is on the front end. Everything goes back to the bus, and the bus is retained here. Internally, a system can handle state transitions, like opening and closing, using displacement cells. The question is how to convey that information. We had one idea of how displacement cells work in the columns. Marcus proposed that displacement should be viewed in terms of vector cells instead of grid cells—same idea, different representational space. That was clever, but can that kind of displacement be shared on the bus in a meaningful way? It has to be something that can be shared, even if others don't know what it means, and they have to learn to associate with it. We need to think about what could be shared on the bus that's useful.

For example, if displacement is all in the language of local knowledge that doesn't translate to others, maybe it could be, maybe not. The system will have to deal with this. Either that kind of state information is transferred on the bus, or movement information is transferred, or it has to be recreated on the other end through a temporal sequence of static things.

As you said earlier, you have different sparse representations and learn to associate them. With state, using displacement cells, you get a sparse representation. Whether you're seeing the stapler open or moving your finger along it, you get associations between different sparse representations. Imagine a series of static impressions—something is moving, but at any moment you have a representation of its current position. This is a sequence of states, and then you have a temporal memory that learns the sequence. You can play it back: if you're in one of these sequences, like a melody, you can predict the next state. That's how we thought about this working, and the sequence can predict what's next.

Now the question is, what do you put on the bus?

Option A: you put these things on the bus, and there's no information about the sequence. Anyone extracting information from the bus would have to recreate the temporal sequence. This would go into another module, which would learn the sequence from the series of states. The other option is to send something about the sequence on the bus, so another module can use that directly. I don't know how to do that, but maybe it's possible. The bus itself could know something about this information, or it might not.

Sometimes I'm aware of all this data, for example, "What's burning in the kitchen?" Is that a state? That's less about querying the model.

But that's the query—would you query something like that on the bus?

I think in some sense you could. You can imagine two models on the bus. For example, the burning is an exception state—you don't normally have burning in the kitchen. But what's making the noise? To address the question, not the unexpected noise, let's focus on the normal case. If I hear a sound, like a train, the auditory column recognizes it and says, "Here's my ID, here's where I think it is." Another column might say, "I know that sound; I've associated it with seeing a train, so I should expect to see the train over there." Or, the visual column sees a train and the auditory column predicts it will hear the sound. That's a kind of association. You could ask, "What is at this location?" or "Where would I find this thing?" but you're limited to questions about locations and objects, not state, since we don't have a state representation. Those three objects are enough for certain applications. We want to eventually expand, but for now, we need a minimum set of capabilities to demonstrate when the system is complete. It won't be everything; for example, it won't immediately tell you what food is burning on the stove.

I have ideas for this, but we can't build everything initially. This is a usable subset, but we won't know for sure until we lay out the capabilities we want. If anyone has a proposed addition to the list of three, please say so. Other questions, like how to represent uncertainty, location, and orientation, might resolve themselves as we go. I can approach it from a biological perspective, but I don't know how we resolve that yet.

One thing I'll add: last week I used deep learning terminology and compared it to autoencoders. I should have also translated it into our language. In some ways, what I proposed was like a spatial pooler for space—describing what's out there relative to the body, but using a spatial pooler for location. The traditional spatial pooler takes the sensory array and describes it with overlap properties. What confused me was not mixing orientation or location, but mixing object ID. That seemed like it would introduce real difficulty and a combinatorial explosion of possible things. Many queries are either "I'm looking for this, where is it?" or "What's at this location?" Mixing those up creates a combinatorial problem.

If we could separate those two basic ideas, I might understand it better. The combinatorial explosion part I feel I can solve, but queries are a challenge. Does that break the idea of queries? That's a good question. I was assuming it would work, but maybe it won't. It's important for the system to be able to ask itself queries and for external users to do so as well.

For example, with episodic memory, you want to query what happened and where things were observed over time. Imagine robots running around a factory; you might want to know where one went and what it did. Many queries, both internal and external, would be "Where was this?" or "What was that?" or "What sequence of events led to this?" Building up episodic memory is an important capability, and you have to be able to query it. Playback should show the sequence of events in an understandable way. I'll have to think more about that.

This brings up another idea, a bit tangential but related. We previously discussed graphs in modules, where there's a graph with inputs from sensor and motor, and it attends to some component. That component is added to the graph, and more layers build on that.

He talks about how, when you learn new graphs, they become components—base units. In the brain, the classic story is that fast learning only occurs in the hippocampus. The famous case of H.M., who had his hippocampus removed, showed he couldn't learn anything new. It's well documented that model building in the cortex is much slower, taking months. We often talk about rapid learning, but in reality, it's not as fast as we'd like, and without the hippocampus, it's extremely impaired. I never understood how the hippocampus could transfer memory to the cortex and always dismissed that idea, but with the bus concept, I see it's possible.

Imagine you have the bus and a set of modules—different sensors and actuators—all modeling something, like cortical column equivalents. Over here, you have episodic memory, which in the brain would be the hippocampal complex. This is the fast, episodic memory: a sequence of events in time, remembering exactly what occurred, in first-person, body-centric coordinates. Most of the memory we store during the day is in the hippocampus. The other modules learn too, but much more slowly. In the brain, these are slower, limited by neurons and synapses. If I adopt this architecture, I can have a system where these modules are relatively static. They need to be trained, but once they're working, it's like going about your day—almost all memory acquisition happens in the episodic memory module.

Aren't those also learning during the day, slowly? We don't know. Some evidence suggests almost nothing is learned over time without the hippocampus. Some theories say only these get updated during sleep, as many memories seem to be formed then. There's a lot of research about hippocampus-cortex interaction during sleep. I used to think each module could learn on its own, without the hippocampus, but for a practical system, if I want to deploy something useful, like a factory robot, it needs this component. Otherwise, it's like living the same day over and over, with memories lasting only about a month.

When you forget things, you forget what you did earlier, but some things last up to a month. Somehow, memories have to be transferred into the slower-learning modules. The point is, internal modules have to learn and build their graphs. There's a question about how a learned graph becomes a basic element, and I'm working on that, with issues of attention and more. But from a practical perspective, you don't have to solve that immediately. You can have a system where model building is in one location, and the graph is constantly updated there.

The other modules would be pre-trained and don't need to continue training for a practical system. If you want an AI Einstein, those modules would be updated all the time, but for a practical system, they can be static. The graph is now global—not in the sense of one graph, but each module might have its own graph, which isn't being updated. You could fix these modules so they're not learning in a particular application, but they still have a structured graph. The rapidly updating module keeps track of things like where the food is, where the chair is, and remembers to avoid obstacles. That's all happening in the episodic memory module, not in the others.

The learning problem of when a graph becomes an element or node of another graph doesn't have to be solved immediately. The system can work with fixed modules, and graph building happens in the episodic memory. The graph itself doesn't get transmitted; only node information is passed around, not the entire graph.

This simplification allows the system to be much simpler and avoids some of the learning problems you ultimately have to deal with, while still being quite useful. It brings us back to a plug-and-play architecture, where you could imagine a set of pre-trained modules—maybe built on CNNs with some glue logic—that output the components of what they're seeing. You could just plug these in, and they'd all work the same, even if they're different modalities. All the learning would occur during the operation of the system, making development much easier.

In this model, if something happens and another module appears, does that become a new static module for learning additional tasks? The static modules are still needed, but they don't have to be continually learning. All behaviors come from putting things on the bus. For example, if you're looking at objects on a table and the goal is to pick up object A, the system knows where object A is and tells someone to pick it up. The episodic memory doesn't have its own actuator; it just stores the activity on the bus—what actually happened, in sequence. This is well-researched in the hippocampal episodic memory literature: there's a time sequence, and you can recall episodes in order.

You're describing a recording mechanism. What triggers it to output something on the bus? That's a good question. It could happen every iteration, but maybe the question is, how does the system decide to act? If this is a pure recording mechanism and the other modules are pure action mechanisms, it actually goes both ways. The episodic memory records but can also put information on the bus. What triggers it to do so is an open question.

This is a modeling system with sensors and actuators. What's its purpose or goal? That's not shown in this picture. You could imagine this as the temporary model of the world as it is now—everything you know about today so far. You could query the system: "Where is something you saw today?" It should be able to answer, for example, "I last saw the mashed potatoes at this corner of the table." An external query, like from a language model, could ask, "Where is X?" or "What is at this location?" or "When did this happen?"

If you tell the system to pick up the coffee cup, and the cup was just temporarily placed, only the episodic memory would know where it is. The static modules know what a coffee cup is, but not its current location. The episodic memory can output where the coffee cup is, and then the system can act on that information. The information about the cup's location could be communicated over the same bus or through a separate mechanism.

This isn't really an issue of what capabilities we want to demonstrate. What I see here is a system that can do inference and prediction using both static and dynamic knowledge of the world. The static knowledge is here, the dynamic knowledge is up there. It can do inference and prediction from different positions, keep track of things, and maintain situational awareness even when it can't sense things directly. It has a sense of time, so it can use history to make predictions. For example, if the episodic memory graph records the last time I saw something, I can recall where the food was on the table even if I can't see it now.

We're sliding into a capabilities discussion. Imagine we have this system—what do we want to do with it? I keep thinking about surveillance, but I'd rather imagine applications like monitoring crops instead of people. In agriculture, the data changes slowly, so there's not much of a time component. This system would be better for dynamic environments.

It might be too much to figure out right now, but Kevin's question gets to the heart of what we want to demonstrate that's valuable and surprising. Inference is central—what is this?—but now you can do it in a multisensory way, with dynamic and changing things, not just static categories. You can handle new arrangements constantly.

Another question is whether to introduce robotics or physical actions, which opens up complex applications. The architecture should support it, but it's a big challenge.

Subutai had a good list of technical questions, and it would be helpful to present them. The next big decision is what we want to demonstrate—what will make people say, "That's amazing." We may not solve that today.

I think we should start implementing, even if we don't have all the answers yet. We need to resolve some things, but we can try a few ideas and get things going. Maybe make small projects, see how they evolve, and come back with feedback. It's good to have a goal or a low bar to demonstrate something and see which things happen.

I've been through this before. When doing big things, you can't resolve all issues up front. You have to pick something, start, and learn as you go. That's why I'm partial to Lucas's suggestion. Until Marcus brought up his autoencoder ideas, I thought we could build this using the mechanism from the Columns paper. We can get started on that today.

We can walk through that, but now we need to seriously consider other options. We could try multiple approaches, but we shouldn't let this drag on for months. We need to pick something and get going. I know how to do the neural one, the positive one. Does anyone else feel comfortable with that? I assume you do. So we know how to get started on that. I just don't know how to get started on Marcus's idea. Marcus, you'd have to answer that.

I've essentially known how to get started on it. Jeff, the idea I want to get started on is just the mechanism that exists in the voting mechanism—associative learning between representations in each column, with the union used primarily for representing uncertainty. That mechanism is easy to implement. The hard part is calculating location relative to the body and orientation; that seems like it would be easy, but it's actually about reference frame transformation, going from SB to OB. What if you represent them in polar coordinates? I thought the hard part was computing the SB, the position relative to the body. That's just a forward calculation. I was thinking about using a convolutional neural network to output both ID and pose for an object. It's doable, but we haven't done it yet.

Do you even have to convert it? What if you just have different representations based on perception, and different sensors link up based on what's firing? No, I don't think that's possible, because then it would be changing constantly. The whole system works because there is a common reference frame. Without it, you can't figure things out simply. Your fingers are constantly moving, but the brain always knows exactly where the finger is relative to the body. The question is how individual modules know the pose and orientation of the object. That's the hard part. Doing the voting from the columns paper is easy.

As I said before, the first thing we have to do is define the bus parameters and architecture. It doesn't have to be hard. We just have to pick one and go with it. If that's trivial, great—let's do it. Then we start putting things on the bus, and they have to output the correct information, which is harder. We know how a convolutional neural network can identify a pair of glasses, but how do we get the orientation and distance? One is a quick implementation; the other is an open research problem.

I'm under the impression, and Marcus has hinted at it, that we could solve this problem in many ways. Maybe we don't know the best way yet, but we can try different approaches. For example, distance could be determined with multiple sensors, or a camera that detects depth. For orientation, maybe we train the system on many different orientations and represent them that way. We could use simple methods to get started. We talked about unit tests and listing out properties; that's something I've discussed with Lucas. The first thing I would do is define the bus, even if it's trivial. One set of unit tests could hardcode the output for a module to test queries. How do I send information back and forth? Do I need an attention mechanism? You can build unit tests for the bus, then start building modules, like a visual module, which is a separate development exercise and may have its own unit tests.

One question at the beginning was about the relationship between modules and cortical columns. That's not clear to me yet. We thought a cortical column might really be a what column and a where column, with all the functions performed in the bus as what and where column functions. Is a module all of vision, or do we need thousands of modules like cortical columns, each looking at a small patch? That has big implications.

To me, the output and input of the bus matter most. Using a convolutional neural network for vision doesn't rely on the many columns' global voting, so that could be like one eyeball. You could have many, and in the brain that might be a thousand columns, but we don't need to do that.

But we might in the future decide to implement this in a more brain-like way, dividing it into subcomponents or columns.

If we can define what the input and output need to be, we can accept the implementation problem and choose the best way to build it. Right now, we could build some of these subdivisions with a CNN, some glue logic, and other components to get it working. At the moment, I don't care if it's not biological, as long as it communicates properly.

If we define the bus architecture, we can specify what the bus must do, and then think about how to build the modules. One extreme is to do everything with SDRs—maybe binary or O-valued SDRs—which I know how to do. The other extreme is to use deep learning, and then the output of the bus is what matters. For location and orientation, you need some representation, but it shouldn't matter how it's implemented internally.

Are there alternatives that don't require this approach? It's an unsolved problem, so it's a matter of literature search. If the system is going to learn, you have a classifier output, and you can represent those outputs with SDRs. You also need to go the other way—take an SDR and turn it into another representation. We could create encoders of arbitrary resolution.

I'm neutral on whether the bus should be binary. If we use non-binary SDRs, we have to question whether the associative memory system still works. We could do binary SDRs and create encoders for polar coordinates, location, orientation, and so on. The columns paper didn't have a concept of similarity, but for polar coordinates, you need similarity. For objects, we don't have similarity yet, but that's a natural output to work toward.

Brains use vector cells or head direction cells, which aren't big SDRs but small, not very sparse vectors—a bump of activity moves around. We have encoders for that and can do any dimension, even encode infinite spaces. If we want binary SDRs, we have that available at arbitrary resolution, which is an important property and something we can do better than a brain.

If I have a precise location, I can tell you exactly where it is, but I also need to represent general areas for uncertainty. In the brain, it's a bulk of activity—cells around the bump have similar representations. I need to represent both high precision and ambiguity in location, which is different from ambiguity in object ID. For location, I need to say it's in a rough area or narrowing down. Maybe that could be a union, but I'm not sure. Ambiguity in location seems to be along the same lines, not scattered. As long as it works, we can use all the SDRs.

Imagine you have a very high-resolution vector or polar representation of space in this room. From my position in the corner, I can create a detailed polar map of all points in the room. That's great, but I also need to use the same bits to represent much fuzzier locations. I can't have a million bits just for a small area; that's too much. This relates to the challenge of attraction—assembling something from parts requires knowing where each part is connected. Convolutional networks don't do this well; they learn many relationships, but the connections are implicit.

When you say "connected," do you mean physically or visually adjacent? We need to know where things are relative to each other—topology rather than geometry. You might infer closeness from polar coordinates, but is there a way to explicitly parse and state, "this is connected to that"? It could be a squashed representation. I'm just talking about representing locations in space, not what's there. How the system knows what things are combined is a separate question.

You only need to know the position space of the recognized object, which might be a subset of a larger object. The challenge is how to compose these parts. Disambiguation doesn't always require pose if you recognize an ensemble of parts and represent their invariants as a graph, regardless of position. You're talking about joint angles constrained by one another.

If I have a partial interpretation of a bicycle, the critical notion is that parts are in relative positions, not just connected. Even if joint angles differ, you'd still recognize it as a bicycle. You want to abstract away geometry and focus on relative positions. The key is learning the relative positions of parts. Position implies a metric, but you can still recognize things even if they're distorted. You have a graph of relative positions, and as the graph distorts, you can still recognize similarity until it breaks down.

Connectivity isn't the main issue; it's the role of positions. If part of a bicycle is obscured, you'd still recognize it as a bicycle—a subgraph would match, even if some components are missing. It's not about determining connectivity, but about relative positions and the assumption that they hold.

This reminds me of the Stretchy Bird experiment. As long as parts are in the correct relative positions, even if not physically connected, you can recognize the object. If a bicycle is missing its frame, you'd still see it as a bicycle but missing a piece. The frame is an object itself, not just an edge in a graph.

I'm trying to get at the type of similarity between objects that are topologically the same. They don't have to be connected, just in similar relative positions. The underlying topology remains, even if the positions differ. I'm hypothesizing that associating things by connecting their representations will fit in, building topological maps or graphs of objects, and recognizing examples like a cat always having an arm connected to a body.

Cats in different poses still share some similarity—it's the topology. I've been talking about the morphology problem, and we think it's related. There's a cluster of important ideas here, and it's hard for a CNN to recognize sameness unless it's trained for it. In this case, the CNN would attend to different components, not recognizing the object as a whole, but by focusing on parts, it could temporarily build a graph of those components. Marcus and I agree there's a missing component, which I've called morphology and he might call topology. The current approach doesn't include that, but this is just version alpha. The question is whether we can make a version alpha that's cool and useful, even if it doesn't solve all problems.

This is another problem—learning behaviors. I'm coming back to the issue Subutai mentioned about discerning pose. I'm trying to abstract it to something easier to solve, rather than brute-forcing a CNN to learn all object IDs and orientations. If we could break it down to smaller parts—identifying connections or adjacency—we wouldn't have to look at the whole thing at once. You can attend to parts, and if they're visually connected, you can start building up from there. Visually, things may appear connected, but only by touch do you know for sure. Still, associating visually connected parts works regardless of scale or orientation.

I agree with Kevin that brute-force training of vision models is problematic, and this is a widely accepted criticism. The abstraction and composition steps—breaking into parts and combining them—are closely related to narrow symbolic approaches. In V2 or V3, there's the notion of end-stopped line segments, which may relate to this, but that's another theory.

What goes on the bus will be tricky. We want to build something that works today, even if it's brute force, and improve it over time. The architecture allows us to start with something useful and not wait to solve every problem. We can build a system that works on a subset of tasks and is still very useful, not just a toy. Over time, others can improve it. I'm concerned about the bus contents being fragile, but with many similar sensors, their combined output is robust. Looking at things from multiple sensors leads to the right answer.

A maxim from working with Subutai: never put computation where a sensor can give a direct answer. This highlights the advantage of the bus—you might be better off with many trivial sensors, like a fly's eye, each fragile but together robust. Adding more sensors or modalities makes the problem easier if you can integrate them, and the bus enables that. We should consider building really simple versions of this system, even if the problem seems hard.

Not so simple, right? We know these systems are quite sophisticated. But what if you have a hundred trivial sensors versus one or ten sophisticated sensors, or a thousand trivial sensors versus ten sophisticated ones? What are the implications for learning and modeling? In some sense, you have to build up a graph in episodic memory, and nobody knows more about the literature of real things than your graph can do. You're restarting a world of concepts. That's an interesting dimension—multiple and small versus fewer and complex.

I was thinking about how to build one of these using multiple columns, and what kind of local voting you would have. You might have a system bus, but also local buses for different sensor modalities, with different columns playing to these, and then these playing to others. That may be how the cortex is built. I was trying to avoid that by putting more intelligence in a module, to avoid hierarchical construction. But it's more than that—these would always vote together before voting up, that kind of thing.

I wouldn't route this low-level column to another low-level column, though I might. The brain has this tiered hierarchical structure. When thinking about attention, I wondered how to attend to something—narrowing the area of input, but still getting multiple columns. At one point, a set of columns represents a small feature, at another, a bigger feature. How does that work? I went down a rabbit hole with that. So, can we build a system with maybe fragile but sophisticated modules that recognize all the things the application cares about? For example, in a bin picking problem, with parts scattered and sensors looking in different directions, together they could determine the correct orientation, which a single camera couldn't do. That could be a good solution.

It's a bifurcation in architecture: a few sophisticated modules versus many small ones. If we had enough resources, we could send one team down each path and synthesize the best of both. Ultimately, I believe this will become a huge industry, with many people building components and deploying systems. There will be many specialized sensor technologies and sensor-motor systems. We don't have to do all of it—it's impossible. In the end, our job is to define and maintain the bus, while others build the components. We just need to get the system started, show how it can be done, and then open it up for others to innovate in their domains and applications.