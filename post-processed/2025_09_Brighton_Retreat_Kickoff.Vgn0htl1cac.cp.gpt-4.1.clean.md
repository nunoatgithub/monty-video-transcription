In terms of the format, everyone can pitch a project or two if they want. The pitch should be no more than five minutes, ideally one to two minutes, covering what the project is, what you plan to do that week, and why it is important for Monty. People can ask short follow-up questions, but we should avoid deep discussion after each pitch. After all the pitches, we'll form groups based on interest. We shouldn't have a project with just one person; you should always find at least one other person to help.

We can have groups of three, and you can also dabble in another project, but your main focus should be on one. You can help out on another project, but only in a consulting capacity, not by writing code or similar tasks. On Friday, as usual with focus weeks or hackathons, every team does a short presentation showcasing what they did, why it was important, and the progress made during the week. Jeff, Celeste, and Terry will be the judges who decide which team wins the trophies. I added a second page to the spreadsheet, "How to win the prize," with some basic categories similar to last time: best in show, most entertaining presentation, most ambitious, best execution, most exciting results, most impactful, makes the community happiest, makes the team happiest, and most creative solution to problems. The prize is a small trophy, so don't take it too seriously, but it should be a fun little competition. Hopefully, every team gets some final results.

There aren't really any ground rules on when or how you're allowed to work on it, but don't sabotage other teams. I think that's obvious.

Tristan, the TV is not going to work out, right? I couldn't get it to work. Whatever team gets the TV to work gets a bonus. That's fine.

Should we just start? We can start at the top of the list, and whenever someone wants to pitch a project, we pitch it. If it's not relevant anymore, we skip it. Should we put the laptop between Jeremy and Tristan so people can stand at the front and present progress, both for the recording and the room? I was thinking so Ramy can see us, or you could just join the meeting from your laptop so I could hear you and mute this one that's seeing the whole room.

I just didn't know how important it is that Ramy sees everyone when people are pitching. Maybe it's easier if, like in the Menta office, the camera is at the end of the room. Whoever pitches can also just stand over here. That's a good idea. I can see them. Sounds good, right? Can you hear me well, Ramy? I hear you. You're a little quieter, but I hear you. We just need to talk louder.

First up is "implement object behavior, prototype start." I thought this would be a fun project for this focus week because it's about building a prototype quickly and not getting too deep into the weeds. It's not about building anything perfect, but about getting an idea of the issues with object behaviors. In past research meetings, we've gotten more concrete about how we want to implement object behaviors, and I feel like I have a pretty clear idea now of where to start and how to try to implement it. This is a great opportunity to test that idea, see if there are any holes in the logic, and get a first prototype of object behaviors working. The main issue might be that the scope is a bit big for a week, since it would also involve setting up a test bed, but it could be a good forcing function to set up the most minimal test bed and do a minimal first prototype. That's my pitch.

Being able to model object behaviors will unlock many new applications. It's one of the three big milestones in the project: composition objects, object behaviors, and manipulating the world. The closer we get to object behaviors and modeling movement, the more people can do with Monty. We won't add that capability to Monty this week, but it will get us closer to knowing how we would add it. The scope is big, but with the way this week is structured, we'll probably have at least one team with three people, and that could be one of them. It's a moonshot project, not something that can be added to Monty quickly—it would take months and multiple people to get it right. This would be a quick prototype to help scope that effort. Cool.

Next up, draft a spec for the CMP, suggested by Jeremy. My thinking here was, when you talk about a messaging protocol or any protocol, I think of something that has a spec describing the system's behaviors, the message format, and how it behaves. I think it would be useful to have something more concrete to make implementing modules easier for us and the community if we're only matching a protocol. I thought it would be useful to do that here, where we're all in person and can talk about it and get on the same page about what we mean when we say we have a protocol and what that looks like. 

What would be the outcome at the end of the week—coming up with a spec or at least a draft, like a written document? I know that Tristan started one, but we could fill in more details about what actually happens when doing voting or setting goals, nailing down the specifics of the protocol. That could then become part of the documentation or the protocol in the code base.

One thing to consider is making the most of being here in person and collaborating in teams. Maybe we could have teams where one person does the main work with one or two collaborators providing input. That way, we still use the fact that we're all here, but not every idea needs multiple people working on it. For example, if Jeremy worked on that, he could still get input from a few people, so he wouldn't be on his own. It's just a thought as we go through the projects. A project that naturally involves interaction with everyone doesn't necessarily need multiple people working on it full-time. I remember in a previous hackathon, Charmaine did interviews with everyone, so it was everyone and also just Charmaine. That might make it easier in terms of team sizes. Otherwise, we have three projects, two three-person teams, and one two-person team. If a three-person team wins, we can just order another trophy. They're pretty heavy, so it's a bit much to get three trophies here. And what do we do with the third one if the two-person team wins?

Prototype emitting telemetry data: Right now, every time you want to do a visualization, you have to make custom changes to the entire Monty framework to get the data you want to visualize, put it in the right place, and then build a visualization around it. The problem is, if you want to change it, you have to go back and change a lot of Monty code. For example, Rami recently wanted to visualize some data, so we had to punch a hole through the entire framework so the data could come out of where it was being collected.

We have the technology to avoid that. If you think of how logging works, you get a logger and then use logging.info, and out of band, all those logs are collected into a spot. The idea is to make collecting telemetry data and snapshots—like a screen buffer you want to visualize—use the same pipeline as logging. The benefit for the community is a standard mechanism for selecting what data you want to visualize in your telemetry stream. Just like in logging, where you can turn on logging per module, you could turn on snapshots per module. If you have my_favorite_learning_module.py and want snapshots just from that module, you would configure Monty to have trace level four snapshots for that module. Those screen snapshots would show up in your data stream, and we would have handlers to grab this data and put it in whatever format is needed, like saving it as binary onto disk.

Another benefit is that this would all be streaming, so none of the data would be collected in RAM and then dumped to disk. You would not be collecting this data in RAM, which is important as we increase the number of learning modules. It would be a standard way of collecting any visualization data you need. Every time you write a new module and want to collect a snapshot, you would write it out like you do with logging. This also reduces the amount of code to modify when adding new telemetry code, because you don't have to invent a new mechanism every time. You just get the telemetry data object and emit telemetry, just like logging. That's the benefit for the community. The existing buffer class could probably be simplified after this is implemented. The previous location still needs to store off the observations so you can update the model at the end of an episode, but you could remove the stats.

If everything goes well, the goal would be to have that integrated in Monty, or it could be follow-up work after this week. This is a prototype—a fork of Monty demonstrating how this works. We would probably grab one visualization and rewrite it, since all visualizations would have to be rewritten to accept a data stream instead of the current object everyone expects.

It's also interesting you mentioned that it doesn't go into RAM; it writes directly to disk every time it stores something, just like logs. It just gets dumped. I'm wondering how much of a performance hit we're getting when we do detailed logging, just from accumulating image data. As long as you have RAM, you don't get anything, but you can't run anything with detailed logging that's more than 10 learning modules. Part of that is the JSON file gets stored, so maybe part of this would be thinking about how we store things like imaging data as well. I wonder if you could add filters, in the same way you have logging levels. You get filters, so by default all the snapshot would be trace level, and you'd have to turn off trace level for that module. You can also specify the name of the module, so you could just specify, "I want these learning modules," and even if you turn it on, you could add a filter so you only want it to come from the first one and ignore the others. There's always a performance penalty for storing all the screen buffers that everybody ever sees, but it would be just like the standard way of dealing with it, like we already do with logs and metrics.

Next one: reference Brain transformations library. This is the thing Scott talked about, where all the data has metadata of what reference. I'm trying to remember the current reality problem with it. Reference frames are detached; the variables that store a location or pose are detached from the reference frame they're in. To implement any transforms, you have to do two things: the appropriate combination of low-level operations, which are stored in transform tools or whatever that Python class is called, and you have to remember which reference frame the variable represents. You have to pull that out of your knowledge and assemble it, and then you have a transform. The alternative could be that you have a variable and you just say "to body coordinates," and it just does it, or "to disc sensors coordinates," and it just does it. The API is very different. Right now, we have to specify how to do transformations, and we don't have all the information in scope. The idea here is to have a reference frame transformations library that has all the scope attached to the variables you're dealing with. You stop specifying how and just say, "I want to go from this sensor's reference frame to the body reference frame because I'm voting," and then in the sign of the learning module, "Okay, I have this in a body reference frame, I want it in my reference frame," and you just say what you want and it just does it, because all the data and metadata is attached with the variable. Now, every time you're trying to do that, you have to figure out the how.

What do you mean by attached to the variable? Imagine you have a unit library, so you have meters. You're never handling an integer seven; you have a variable called meters with a value of seven, and you can do a .to_millimeters on it. You'll get another variable that represents millimeters. Two millimeters would probably be serializing something into "two millimeters," a string, and then you'd get 700 or something like that. You never have to worry about the units because it's always the correct thing. You have a function where you expect millimeters, but you'll get this generic type of measurement, and in your function you just say, "Whatever, you get to millimeters," to make sure you're in the right units. You don't care what number is in there; it'll give you the millimeters when you ask for millimeters. Because it knows it's represented in meters and you need to multiply by a thousand or something like that, you couldn't accidentally apply the wrong transform because you assume it's in a different reference frame than it is. It's Python, so you absolutely can, but it's easier to make that mistake. We could talk about this more if we do it, but I feel like there are a few approaches you could take. Scott and I were talking about it recently because I feel like this is the most advanced and powerful approach, but probably requires some sort of graph representation of all the different spaces and how those are linked. One thing we could also do is just attach metadata to each variable, and it would basically check for this and make it easier to see what reference frame it's in and with respect to what. It wouldn't solve the problem of "I just want it in body," but it would make it much easier to know the series of transformations you need to do to get that.

and that would be just a case of adding a bit of metadata and then changing the transformation functions. Especially because you can't always do each transformation if you don't have extra data. What are the hypotheses? It's not that you can't transform. That's the potential danger—in some ways, it's nice that we have to think about how you actually transform between these spaces because it forces us to consider if this is reasonable. The problem is, it's the other extreme right now where we don't even know what reference frame this is in. But if you basically had, in the same way as robotics notation, this rep coordinate system with respect to another coordinate system, that could just be two pieces of metadata. You could always print that out if you wanted to know that about a variable. Then, if you're transforming them, any transformation function will check: does the bottom prescript match the top prescript of the other matrix? If so, it's a valid transformation, and the output will be this prescript and this prescript. If they don't match, then it's invalid. That would be simpler and already a lot better than what we have. 

The advantage of this full system is that when we start having multi-joint agents or something, it might get really complicated to say, "How do we go from finger to shoulder?" Maybe having an automated system to do that could help. We can talk about it in more detail.

Next project: wrapper for customizable interactive visualizations from Rami. I could pitch this one. It's mostly that users would be able to mix and match visualizations now because I have a pending PR for having the widgets separated and talking to each other through pubs or topics. They're not intertwined in the code itself. We could have a wrapper that goes over these visualizations, and you could bring in some visualization, like J's visualization with mine, and put them in separate renders. You wouldn't have duplicates in widgets required for these visualizations. For example, if you like J's visualization and mine, they both need a slider for the episode number. The wrapper would be smart enough to know to just put one of them, put all the controls somewhere, and put all these visualizations together. When you change one control, that control will publish to the topics required, and all of them will change at the same time. You can design the UI of whatever larger visualization you want, and it will bring in parts of visualizations from other self-visualizations. That would be the idea. We could use this in presentations or whatever. You could quickly define a little piece of code that says, "I want this part from here, I want this part from here," and it will make one visualization where you can change them all with the same widget.

Nice. Cool. That would be cool.

Next: make Lego robot reproducible from Will.

This project takes the Everything Is Awesome team's Lego robot and replaces all the Lego parts with 3D printed parts. We could have a link to buy a bundle from a reputable 3D printing company to reproduce that robot exactly every time and not have to worry about the bits that are difficult to get. The project is essentially about designing the 3D CAD pieces and doing as much pre-work as possible to get everything ready so we can start doing cycles with 3D printed parts after the hackathon week. The benefit to the community is that it's the first robot that actually moves in the world autonomously and measures things. It would be exciting for the community to get that, play around with it, reproduce it, and make it. Would you need experience with making CAD models to work on this project? No, I don't have any, so we'd all be learning as we go. It's just reference frames, right? We all have reference frames.

Are you thinking of making the parts generic so people can configure them in different ways? Making it more modular might be a good goal, like getting the actual robot bits so the original one works, but having it modular would be useful. You could add multiple sensors and move it in different ways, maybe have more axes of freedom.

Nice. I'm trying to remember the little robot or whatever it's called. That's an open source one from Hugging Face, I think. The new photo guy—it's like an arm desk, or it's called Arm 100. But it's 300 or something like that, or that's for two arms. I think the main issue, if I remember correctly, is I don't know if the camera is actually mounted on the arm. I think it's more at the base or something like that. It could be interesting. It's probably beyond the scope of a hackathon, but maybe there's a way for someone to implement MON using this, and that opens up other possibilities. I thought about that. These companies seem to change their arm really fast or go out of business. I want to pick one where, in five years, you could still do this, and the CAD diagram is still available. 

Do you have an idea how much it would cost? Two, three—it's pretty cheap. I was looking into 3D printing, where you send them the CAD file and weekly get back the 3D printed objects, and it's super cheap. It wouldn't be a huge thing for people, especially with the simple one. It's just a moving whatever. It wouldn't be too expensive. I think it would be cheaper than the Lego version.

All right, next one. That's it. If you crossed out "extract habitat sim wrappers" as a separate project, having some dependencies and Python requirements are holding us back from upgrading the rest of the Monty stuff. The work here would be to create a separate TDP simulator_habitat project that would compartmentalize our Habitat Sim interactions, taking advantage of Jeremy's work that already isolated the simulator protocol. This would involve putting the simulator protocol into interprocess communication or some kind of network communication, and developing a mechanism for controlling and interacting with a simulator that comes from a different project, TDP simulator_habitat.

The benefits to the community: Monty could be installed from PyPI, so pip install Monty, TDP Monty. Monty could use the latest Python, the latest dependencies, and could run on ARM64 natively. It would be much easier to measure Monty performance because Habitat Simulator would be in a separate process, completely separate. Whatever you're measuring on Monty, you're just measuring Monty. All the installation problems would be associated with the simulator package, not the Monty package.

Wait, is that because the capabilities paper, FLOPS there, were they including the simulator in the numbers? No. No.

No, but it took months of work to do that because Simulator and Monty were in the same spot. We cannot do bespoke instrumentation of all of Monty versus just running perf from the Monty Sim process. Installation problems would be associated with Habitat Sim versus Monty. Someone else could write a simulator that works in the same way, but it's a different simulator. It also specifies an integration point for others: bring your own simulator to Monty as long as you're meeting the simulator protocol, and you have an example of how a different simulator works. We also want to do this as part of our platform 1.0 roadmap.

Anyway, sold me. Sounds pretty good. It's that darn back everywhere. To the discussion this morning, is there any reason it's a security concern when Python's support lapses for people to install? TDP Monty, I'm trying to figure out how to not have the wrong conversation about this. I don't want to have to justify upgrading because there's a problem. When we are not upgrading, we are missing out. The Python we have is from 2009. Python 3.8 has a pretty long lifetime, mostly stable. It's just embarrassing anymore. Here we go. People start laughing at Python 3.8. It was created in 2019, so that's not that bad, but we've missed out on six years of new features. It's not that it's going to become insecure; we're just running six years behind. We're building the latest next thing in AI, and we're using technologies from 2019, before ChatGPT-5 existed.

It's a hard sell to me. The other thing is, because we're using an old version of Python, all the libraries we're using are now going obsolete. We are on MPI version one. The reason we are on MPI version one is because MPI version two doesn't support Python 3.8, so we can't upgrade to MPI two. At a certain point, people will not be able to install Monty because it will be old software that nobody knows how to run anymore. This is the life of software—we always have to update it. To be clear, I'm not arguing against upgrading; I just thought maybe it's another benefit. For this part, it's less a security issue because you're not running anything in production that someone can attack constantly. It's more about whether it's supported and what the user experience is like. Especially as you get further along and distributions of Linux start removing older versions of Python, you can't even install it without building it from source. As soon as you go obsolete, any new platform that comes after that feels no obligation to include it, and you rapidly become uninstallable by anybody with anything modern. Lots of excellent reasons.

Okay, implement test bed for computational objects. Viviane added this one, but I can pitch it. We all know compositional objects are something we care a lot about—it's what we've been aiming for a while. We actually need a way to evaluate it and see how we perform. We talked about this last Wednesday. There are a few ways we could approach this, but Omniglot seems like a really good candidate for a variety of reasons. It would allow us to test that we can have stacked learning modules representing different parts of objects as well as these compositional objects. It would also enable us to see if we can improve Monty's performance on Omniglot. That's a very challenging dataset in machine learning because you have an extremely small amount of data to learn from. People in the community are interested in how we do on these more traditional machine learning benchmarks. We have lots of good reasons for not evaluating on some of them, but actually Omniglot is a pretty good match for Monty because of the size of the dataset and because it has actual movement information about how people were moving their cursor as they were writing these letters. It would be a great dataset to evaluate this. After the hackathon, we could go into September armed with this benchmark and tick off some of these really nice things that Scott and Rami have been working on, like hypothesis resetting and the policies, and actually see how those perform.

It's also a cool project because the scope could expand further. One of the other projects you'll hear about is about having these 2D sensorimotor modules specialized for processing textures and logos. If someone works on that and makes progress, there's scope at the end of the week to try and combine them and see if we can do the logo-on-mugs dataset and similar things. There's a lot we could do with it. Part of it would be to also set up the logo-on-mug dataset. I don't think that should be a starting goal, but it could be something to try. Another thing would be coming up with evaluation metrics and implementing those in Monty. We talked about how prediction error is a really good way to look at how well Monty is modeling the world without supervised labels. As we go into the future, we'll have less access to supervised information about the world and what Monty is seeing, so one of the only ways we'll have to measure its performance is how accurately it's predicting the world. This would be our first measure, looking directly at that.

If it becomes a two- or three-person team, one person could work on the logos and marks dataset, another on GL, and another on performance measures. Omniglot will not be a good dataset to test policies, for example, but since Omniglot is already pretty much set up, it mostly requires evaluation metrics. It sounds like it could be reasonably within scope to try both. To give a bit of context with the composition of being able to model compositional objects, this is one of those huge milestones that will unlock many more capabilities or applications for Monty. Before it, it can model objects and void, but even if it's a compositional object, it models it as one large thing. With it, we can deal with pretty much every application that has some background and different objects within a scene. We can then deal with that and more efficiently model things by decomposing them into different parts. It's a huge capability to add to Monty—a huge benefit.

Cool. What was the next one? Bounded evidence. EMA Neurons. Ramy, I could pitch this one.

The reason for this one only came about last week when I faced some problems with just using the raw evidence, accumulated evidence scores, which are unbounded. They could keep increasing—we've seen evidence go to 50, 100, or more. There are a lot of problems, like when you're trying to find formulas or mathematically understand what's going on. It's very difficult to work with these unbounded evidence scores.

One approach I took was to divide the evidence score by the number of steps the hypothesis has taken, or the age of the hypothesis. While this seems to work, it's not very biologically plausible and probably not the way we want to go. Niels has suggested some different approaches, more like a neuron's membrane potential, where it would decay if it didn't get enough evidence. I think this project would be more of an investigation into how to make the evidence more biologically plausible, similar to how neurons do it, so that it would decay and increase exponentially. It would also plateau when we increase it, so it would be bounded between zero and one, or negative one and one. We would basically use something like an exponential moving average so that we could get the same effect of bounded evidence.

What we're thinking here is that neurons will be supported by evidence, so as they plateau towards one, if they're getting evidence, they'll get closer and closer to one but taper off at one. If they don't get evidence, they will start to decay, just as the membrane potential will decay if it doesn't get spikes. The difference here is that in the integrate-and-fire model, all of the integrate-and-fire models just keep integrating the charges as spikes come in, so they keep adding them. They're not really bounded—they're bounded by the threshold, so when they fire, they just go back down. We have to think about how much biological plausibility we want here, how to compare it to existing neuron models, and what benefit we'd get from that.

If you, Niels, have anything else to add?

Yeah, I was thinking there are a few cool things about this. Viviane had already added some parameters so that if you select the right parameters, it is an exponential moving average. We've already experimented with it a bit, so we were already thinking we should probably go back towards that. It seems better, not really thinking about the biology that much, and then thinking about it—okay, when implementing—yeah, sorry. No, that's true, as in that it was bounded and stuff, but I was thinking about it, implemented it, and it didn't work well yet because we don't have efficient policies. After we have the efficient policies, it should work much better.

At the same time, Rami, you implemented this slope tracker, which was an independent measure of evidence that we had. What's interesting is neurons have different compartments. This is the membrane potential of the neuron. As you get incoming spikes, it goes up and accumulates. If you stop giving spikes, it'll decay down, and if it hits the threshold, it has its own kind of discharge. This kind of behavior isn't exactly modeled by an exponential moving average, but it's very close. Exponential moving average is nice because having networks that use integrate-and-fire neurons is notoriously difficult to train and learn with, but we know already that exponential moving average would work pretty well. We just need good policies so that we move around enough.

In addition, when people model integrate-and-fire neurons, they tend to model neurons as point neurons, treating the entire complex structure as a single dot getting inputs and having an output. The dendritic compartments are totally ignored, and that's where Numenta was unique for coming up with multi-compartment neurons where you simulate all of this. Those are even more complicated and really hard to train to do anything interesting. Numenta was unique because HTM neurons approximate a bit of a point neuron and a multi-compartment neuron. If we did an EMA neuron and had the slope tracker, that would potentially model the soma, and our old evidence count with the EMA would model the dendritic compartments. When they spike, they have a much longer time constant, so even if you don't get further input, it stays up for much longer. The soma depends more on what's coming in at that moment, and neurons integrate those two. It's interesting that all these different things are converging on what it seems the brain or neurons are doing.

Part of this week would be implementing this and thinking through some of those consequences. As Rammi said, we don't currently have spiking, but as we move towards unsupervised learning, we want to have some sense of which columns co-occur a lot so we can associate them. If we start having spikes and measuring that, it would be a natural way of counting that kind of co-occurrence. It could have some interesting implications. I think it would be exciting for the community—they would be really interested to see us having a neuron-like model. We already had something similar, but I think it turns some people off when we say it's evidence count, and it's not clear that there's a connection to the biology. I think we can make that much clearer here while still not restricting ourselves by trying to simulate the brain.

For the implementation, you would take the existing implementation of EMA, add EMA for the slope tracker, and then test parameters for hypothesis adding and deletion. Then see how well it works.Then there might be some theoretical work, such as thinking through other implications like the multicomparisons, spiking, and anything else that isn't necessary to implement this week but should be on the roadmap, such as unsupervised learning. Now we have our EMA neurons, they're bounded, so let's add in spiking, start counting these, and see how that works.

My main concern is that if we already had the efficient policies, it would be great to proceed now, but we'll run into the same issues I encountered three years ago, and it will not work well. That would be discouraging, and we wouldn't really be able to test the new mechanisms. In theory, you still have that issue if you set the time constant for the background evidence long enough; at the limit, it becomes equivalent to the current one.

I don't think so. You'll have very low resolution if you set the pathway very high compared to the current weight. You have to work with the 0.01 threshold, and you have to adjust that as the episode goes on. All of them would be struggling with that, so everything would be normalized the same. It might be an issue, but that's the project.

Next up is Monty Flame Graphs. This is about building a quick prototype to ensure we can use flame graphs with Monty easily. Every time you run an experiment, you can get a flame graph to see where Monty spends its time. Perf is standard tooling in the industry, and flame graphs are standard as well. There's no invention here—it's just figuring out the happy path for all of us to get those visualizations out of Monty. The benefit for the community is that it becomes trivial to rapidly identify bottlenecks where Monty spends most of its time. If anyone cares about performance optimization, they can look at a flame graph, see where the most time is spent, and decide to fix that. This is especially useful for robotics in the real world, where you need to keep up with the world and want to know where Monty is spending its time.

Having a tutorial on how to get flame graphs is helpful. That feedback loop is super helpful for making effective changes without just guessing. You can rapidly hill climb to really good performance using this type of tool.

With the existing profiler, it's just easier to use perf. Anytime you add new code, you have to update your profiler, but perf just works with any Python code. We're not integrating it specifically; it's a mixin for the experiment class, with wrappers around all those methods. If we add methods, you have to remember to add those wrappers, but with perf, it just picks it up for the interpreter. Perf works at the Python level, so you don't have to change anything or remember to turn it on. You just run it with the perf tool, and it instruments the language and function calls inside the language.

That's what the profiler does too. I never had to change anything when the codebase changed; it just profiles how long you spend on each function. Maybe I'm missing something, but whenever I used the profiler, I just added it. Why don't we use that for floppy? Why did we write floppy instead of using the profiler? I thought the profiler measured the seconds you spent on each function, not for counting flops. Seconds or milliseconds—the time, so that's work time. Flame graphs don't do work clock time; they just do flops or time on CPU instructions, like how much it takes.

I'm not sure which one the profiler currently measures, but it was definitely time spent on each function. That's generally what you do with profiling. Flops are very specific to mathematical operations, and in deep learning, it's become a focus. Generally, you care about these other things, not flops, because it's dependent on hardware, implementation, and other factors. The profiler is probably more tedious because you get a large Excel spreadsheet and have to analyze it, whereas this gives you a nice visual representation. There are other benefits: you're using standard tooling versus bespoke tooling, and you can use tools built for perf data and flame graphs. The profiler is just a library we import; you don't have to rewrite anything to use it. It's a standard Python profiler library, imported and wrapped around the whole experiment. You use the profile experiment mixin in your experiment config, and at the end, you get a spreadsheet with how long you spent on each function, but it takes much longer to run your experiment.

With perf, nothing takes longer. The data is collected out of band by the operating system. If you run perf, there's no performance overhead; all the data is collected out of band by the Linux kernel. The difference is between adding more Python to measure Python versus the kernel intercepting calls at a kernel level. It's incredibly simple to set up—even the first time, you just put "perf" in front of your command with some flags. In AWS, to compare with floppy, we had to set some security level, some paranoid level, but besides that, it was straightforward.

We are using the cProfile library for profiling. That's another advantage: perf can also look at flops if we ever want to look at that again.

Okay. Next project: implementing the sensorimotor module. Implementing a 2D sensorimotor module has several benefits. The idea is to have a sensorimotor module specifically designed to output movement in 2D space—how you're moving on the surface of an object—versus 3D space, plus detecting 2D features on that surface. Instead of the sensorimotor telling you you're moving in 3D space, it will indicate movement on the surface of the object—up, down, left, right—and detect features like edges. Instead of detecting a point normal and curvature direction, which might not exist on a flat surface, you define the orientation by the edge present, for example. This is important when dealing with 2D datasets. I implemented something basic for this during the ultrasound project, since we had 2D images. You could probably use some code from that for edge detection and curvature extraction. In the context of compositional objects, this would help model the logo, which is inherently a 2D model wrapped around various 3D structures. If you look at movement on the surface of those structures, it will be consistent with the logo model learned in 2D. Even if your sensor moves around a curve, the 2D sensorimotor module would represent it the same way as on a flat surface. The outcome of the project would be a new sensorimotor module class that outputs 2D movement on the surface and detected features on that 2D image.

Salient Showdown, Scott. I think everyone is familiar from research meetings with this project I've been working on. I'm trying to recruit someone to help with it. The basic idea is a vision-oriented project: if I pick up this bottle and try to recognize it, I don't randomly move my eyes across any point, which is what Monty does now. Instead, it tries to find the most significant or salient points and immediately attend to those. After a few of these, I should be able to identify the object more quickly. It doesn't require any memory of previously seen objects, so it's not a model-based approach. This is a model-free strategy. I have a pretty good fork ready to go. Anyone interested should be able to, after maybe a day's work, drop in any function that takes an image and outputs a scalar for each pixel, indicating where to look. In principle, you could spend a week finding the most efficient functions, and make a test bed—it already works for the YCB dataset, but we could use existing or smaller benchmarks for faster testing. You should be able to drop your function in, run the benchmark, and see the performance improvement in terms of steps, since that's what we're trying to reduce. That's the basic idea of the project. The preliminary work is done, so it should be easy to drop in. This project could work asynchronously for most of the week, which is good for remote participants. There would be some early setup to get my fork running and set up the benchmark, which would be good to do together. Beyond that, it's just dropping in your function, trying it, and checking in to compare results. For evaluation, just use the standard YCB objects. The competition is to find the best algorithm. If you want to make it personal, go ahead. 

Another addition: Viviane, if you're talking about object behavior, sense traditionally involves motion in animals. Experience with Salient could help with that, especially if we want to attend to moving parts of a scene for object behavior. If they're moving, that's important. There are a lot of little bits like in other projects where it would be nice to have development, even if we don't build out the full implementation for motion right now. Getting some development on that project would be good.

Was that the last one? I think Will added one. I added one more, which is just an excuse to show a video of Tristan.

Can you guys on the call still hear me? Yep. All right. This one's called Interpretive Dance of the Neuron. The output of this project is each member of the TPP staff using neuron and doing an interpretive dance of that neuron. This probably won't be chosen except for entertainment. Let me give you a quick example of what that might look like, if I can figure out how to share sound. Here we go. Share.

And where did you benefit?

No, actually, but there we go. I'm sold. I did get permission. Brain AI mouth, make braining and we can go viral.

TBP team officially lose their minds. Brighton.