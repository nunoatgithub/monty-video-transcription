Hi Colin. Welcome. We're here to talk about the hypothesis updater prototype.

What should be the next steps? Reading your summary, you mentioned that phase one and two make sense. For phase three, what are your questions? We've only had one or two exchanges about it. Is there anything else we should synchronize on? In terms of the overall workflow plan, I think that makes sense. Start on a different branch or fork, develop a GPU backend and CPU backend, and get it all integrated.

Specifically, on your TBP monthly fork, if you create a branch and then create a draft pull request with that branch, as a maintainer, that allows me to push to it. That can facilitate collaboration, so we're actually working together instead of just me working through you. I could say, "How about this commit?" and you could respond. That might be helpful. So, create a new branch on my fork, start a pull request, and we'll work through that pull request flow as a draft.

If we look at a pull request, for external pull requests, maintainers are allowed to edit the pull request. I think it gives me branch permissions. Let's try that—make a branch, make a pull request, and see if I can push to it. That would be a good experiment.

Do you want me to do that now or follow up after? Is it a good use of time? I have some things I want to talk about. Let's talk about that and prioritize. Let me share my page.

I just started making a document to discuss this.

Getting used to the Google workflow.

I can see your ID.

The key thing I wanted to talk about is that when I first proposed this, I was looking at trying to batch operations across LMs. In the initial post and comments with Viviane, she suggested that keeping the LMs more separate is conceptually better. The runtime is not easy to predict—it's not clear which LMs will get what inputs at different times. It might be more complicated to batch across LMs, so that's what led me to consider a different CUDA streaming approach.

I started outlining that in the comments on the separate posts you created. I think that's where I would start to change the interface between the Monty experiment and the hypothesis updaters.

Do you feel like it would be helpful for me to explain what a CUDA stream is, or do you get that from the post I made? I think I get it from context. If you want to, you're welcome to. It's like a CUDA thread that runs in parallel, and you can put kernel operations per stream. It would benefit us because we can keep the LMs more separate. They could launch the same kernels with different inputs or slightly different kernels, all on their own stream. The GPU can schedule work from all the different kernels across all streams. GPUs are designed to have complex scheduling to maximize throughput. If one warp in one kernel starts a memory access that takes hundreds of cycles, it switches to a different warp that has compute work to do. GPUs are designed to process all the work they have stacked up. We would still benefit from parallelism across the LMs, but keep them defined separately and allow for more variation in behavior.

You mentioned that, based on the data flow diagram you outlined, we want LMs to all be doing the same operations at the same time. We don't want different flows for the different LMs. I may have misspoken about the same operations, because that presumes all LMs are the same implementation. They could be different implementations. The internals of an LM could be different—one could be the current feature graph LM, another could use grid cells, and we're trying to compare them, or have a mix. I over-constrained and misspoke. It's not that they do the same operations, but that they're all going through the same phase. Each one could have different operations.

By phase, I mean they get inputs and need to produce outputs before moving on to the next phase. That makes sense. Even in my original ideation of the Stacks implementation, we would group LMs based on their type or input. I didn't figure out the details about whether they all need the same input for the kernels, but that would add complexity to group them into the proper buckets. CUDA streams avoid that complexity. We can have different types of LMs processing different inputs, or some could be doing no-ops or other kernel operations, and we could still benefit from some parallelization. It will be less than if we stacked it, which is the most efficient way, but the flexibility is better. I think it's a better approach for Monty and maps to the existing architecture better.

but what comes with that is we need to switch from the sequential, blocking loop we have now for processing hypothesis updates to a non-blocking model. Right now, the Monty experiment loops through each LM, calls the updates, waits for that to finish, and then goes to the next LM. We wouldn't benefit from any parallelism, even if we use CUDA streams, because one stream finishes before the next begins. We need to switch to something with a dispatch phase. We can work out the details more, but this is just a quick pass at the basic structure. We want to launch the kernel update, or the pipeline of kernels, for each LM and then return immediately. It should go through all the LMs, dispatch the kernels, and then have a synchronization or waiting phase while the work completes. The Monty Experiment should not do anything until that's done, but once it gets the update that it's done, it can read the results and move on to the other steps in the experiment. It's a common pattern and pretty straightforward, but it will require some refactoring of the code. I started looking at the different calls here, like step learning modules. Right now, we just call this LM step method. Instead, we'd need to do a dispatch step and maybe rework some of the logging, then do a wait. One thing that pops up is that in the dispatch step, we're still getting sensory inputs for every LM—line 81, sensory inputs collect. But that's still done in series; we collect those inputs.

We'd have to figure out exactly what operations are worth parallelizing. I didn't look into the details of some of these other pre-processing steps. If they're quick, it might not be worth the time, but we could look into whether we can parallelize more of the pre-processing and do the full pipeline. I'm wondering if the parallel switch should happen at the step learning modules level, as an alternate step learning modules implementation, instead of reusing it and swapping out calls in parallel. That's my question. So, instead of having those learning modules in step learning modules, do we do this a level up from this function? No, what I mean is—okay, I see what you're saying. This is why you're talking about non-blocking, because the dispatch step will add an unblocking call. If we have a CPU backend, you can still do the same thing, just collecting it later. Where do you collect the gathering set? At a high level, I kept the same code to see where it would insert, but after that, you'd want these calls to all return. If the loop finishes before any of the work is done, all that happened is a dispatch, then you have a blocking call after that. Once all the LMs have dispatched, we just wait until they're all done. There are more complex ways, like starting work as soon as one LM finishes, but that could come later. It's simpler to wait for the backend to say it's done processing all the work, then continue with the rest of the code as normal. You'd need to add a call to read the results back from the backend, which will need to read whatever results you want—hypothesis values or everything—from GPU memory back to CPU. You want an explicit read call, then you can do the rest of the code. So, scatter, block, gather, and then block and gather. That makes sense.

This is step learning module. That allows us to—okay, got it. That's helpful. You know the Monty code better than me. I was just following the functions, and for the experiments I was running, it seems like this is where the looping over the learning modules happens, where we would start adjusting how we dispatch the work. Would it be helpful to you—I'm thinking about more generic processing, in case you want to stack things together. With this approach, dispatch step is pretty much, if I understand correctly, a dispatch step will create a GPU stream. Is that what the outcome of a dispatch step is? I think, correct me if I'm wrong, but my understanding is it's a set pipeline of operations for the hypothesis update steps. I guess what I'm asking is, you said stacking things can be more efficient than adding streams. If we want to stack things, we'd want to gather all the sensory inputs at once, have a set of sensory inputs and LMs, stack them together, and then dispatch that stream. In my original proof-of-concept code, we did one kernel dispatch for each step in the pipeline for all the data across all LMs. In that case, we wouldn't need multiple streams—just one CUDA stream, one sequence of kernels, and one kernel dispatch for the entire operation. That would be more complex because we'd have to accumulate all the data into one continuous buffer to pass into the kernel, so there's some overhead. With the CUDA stream approach, each stream is handled separately, with different kernel dispatches and references to memory objects. It's simpler to implement but might be less efficient because you're dispatching multiple kernels, which might have their work parallelized by the GPU scheduler, but you're not defining thread-level parallelism by doing one giant kernel dispatch for everything at once.

Does that make sense with the difference? That makes sense. The reason I'm asking is because if we're going to change the API, I'd rather change it once than change it now and then have to change it again when we add stacking. Even if in this initial step we're not doing any stacking, I want the swappable backend component of the API to support stacking if we want to add it later.

So, in this implementation, we wouldn't be able to do stacking because we're dispatching a kernel on a new stream with every dispatch step, right? Sensorimotor inputs are just the inputs for that learning module, so we're essentially dispatching a new kernel stream per each LM with just its inputs. There may be a feature optimization where, if we were to stack learning modules that are doing the same thing, the kernel just needs to be the same and the inputs would also get stacked. Is that true?

Yes, it would be a step where we put all of the inputs together, stack them into one input buffer, and then use one kernel. That API would require multiple sensory inputs, like a vector of sensory inputs, not just sensory inputs for the cell. We would have a vector of sensory inputs and a vector of LMs. In the current implementation, it could always be just a vector of one, but it's hard to introduce that list late after the fact. That's what I'm thinking about with this API—where to make the breaking point.

If we're actually stacking all of the operations across all LMs, we wouldn't have to do the three-phase approach where you dispatch, block and wait, and then read, because we'd just have one pipeline. You dispatch a kernel and then, once it's done, read the results. But we might have to do different subgroups, like batch LMs of a certain type together or ones processing different inputs. In that case, maybe we would still need some level of a dispatch step, or maybe each group of LMs that are processing together run on one stream. It would introduce a level where we separate LMs into different groups and then follow the same dispatch API.

This is why I was thinking about where to make the interface, because it could just be: here's all the learning modules, here's all the sensorimotor inputs. Even the initial GPU implementation could just be a for loop of single dispatchers, but grouping would have to happen inside that API, not before it. Do you think the grouping should happen before? Do you have enough information to do meaningful grouping before handing it to the backend, or should the backend do the grouping? That's the question I'm trying to pose.

I don't know, because we could group LMs as long as they're doing the same operations, and we could figure out how to do it with different inputs. Even if they're processing different sensorimotor patches, as long as it's the same type of LM doing the same sequence of kernels, we could stack them. But, as Vivian mentioned, it's hard at runtime to know which kernels are going to be processing which inputs—some might not be processing the same input, in which case you might not want to include them in the batch. It seems like, per step, the grouping could change. It would probably be more efficient if we could do the grouping at the beginning and have that be consistent throughout all the steps, versus regrouping LMs at every step, which would get complex.

This is a naive question, but if an LM doesn't have inputs for a new update, would you just zero out the sensory inputs, or would you just ignore the outputs? Would that be more efficient than trying to regroup things? I think zeroing out would be better. You could have a simple case in the kernel itself, like a flag, or if it sees the input is just zeros, it skips doing the work. That would probably be better than doing a whole new grouping just to skip a no-op for one LM.

That handles the case where not everything will be processing. The only other case is, for LMs—these are sensory inputs, but in the future, an LM could get other types of inputs.

As I'm building the CPV1, what can happen is a learning module's inputs—let me pull this up so I'm not doing it from memory. Do you want to share? Yes, I'll share the screen.

So, learning modules receive inputs that are the outputs from the sensorimotor modules. These are stable—once your sensors are hooked up to your learning modules, it's always the same. The difference is the inputs from the previous step's learning modules. This encodes a hierarchy of learning modules, so learning module outputs can be zero, one, or two cortical messages, or no messages.

It can output a goal or a preset percept, like a perception. A goal would go to a lower-level learning module, and a percept would go to a higher-level learning module. It's top-down, goal-driven, and bottom-up, assembling a hierarchy of inputs. The inputs to the learning module might be different widths because it can get a sensory precept and a goal from a higher-level learning module. Sometimes it might get no sensation, zero inputs, a percept from a sensorimotor module that's stable, or additional inputs.

There's a bounded count of how many messages, depending on connectivity. The inputs will be variable—no input, one, or more, up to some finite amount. At any step, that width can change. At a high level, that should be fine because we're already envisioning an input processing phase where we stack the appropriate inputs, assuming these would be variable sizes. In practice, at lower levels, there could be overhead if you're recreating a new buffer and transferring it every time. With fixed-size inputs, you could more easily pin the memory between the CPU and GPU and just push to the right place. We could get around that by over-allocating—if it's a certain max input size and it's not an unreasonable amount of memory, we would know that for each input because that is a configured connectivity. That is defined at the start of the program, and you can only get inputs from the things you connected to, so we will know that.

That's probably pretty straightforward to get around. It's really just figuring out the LM grouping and having that consistent, even if the individual input data is changing step to step. I don't think that's a big hurdle.

Right now, LM grouping is by type—if it's the same class, it will have the same implementation and do the same computations. It should be trivial to group these before the backend, and the backend wouldn't have to worry about it. The API could just get the grouping. In simpler cases, it's just one level of hierarchy of LMs, but you are envisioning a more complex graph. We probably have to do the grouping based on when the LM would be processing data, so the first level would get grouped together, and if those are passing to a second level, that's not how it's going to work. Any hierarchy messages are across steps. They're all one level and get input from previous steps. There's still a setup—let me show a graphic.

I'll download it and share it real quick.

Take a screenshot.

Put this in here. It works like this: when it's getting passed between the levels, it only ever travels at a specific time, at frame one, which is one step. Even if it goes up the hierarchy, there's travel time, so at step two, one LM will receive the previous step's output from a lower level, and the same thing happens from top down. That level L will send down to L-1, but in the next time step. Every step, all LMs compute. There's not a hierarchy breakdown inside a step. Every step, every LM gets inputs, outputs, and then moves to the next step. There's not a process where, in a step, we do level one, then level two, then level three, and pass messages between them. We don't do that in a step.

Does that make sense? Yes, that makes total sense. That would be even easier for the grouping because you can just group all LMs together regardless of the level. Exactly. That's why the framework can just give you the groupings of similar LMs, and they'll compute the same.

It seems like stacking could be feasible with the system and wouldn't deviate too much from the principles or require too many changes. I do think it would be more complex. The CUDA streaming approach is simpler, and I don't know what the benefits would be without profiling data. The alternative is the CUDA streaming approach versus trying to stack and do a single kernel dispatch for all. I'm not advocating that we try stacking now. I just want to make sure we can support that and that we don't have to change the API twice.

When you get a group, that's a consequence—inside that call, even if you get groups, do you now dispatch block and scatter/gather in that call, or do you do it across the group calls? I feel like we should try to have the API support the idea of an LM group, and for the initial implementation, every LM is in its own group. If we don't want to support stacking, the API would still be consistent in the future. If we start to group LMs, then that group becomes part of the dispatch block, and the results flow accordingly.

Would you want to, if we do the grouping thing, wait for that group to finish before starting the next group? For that group, the implementation would be to dispatch one GPU stream.

Dispatch one GPU stream and then wait for that to finish for that group. Dispatch that one stream, then grab another group, dispatch that stream, and then wait for all streams to finish. The second approach is to keep the same idea we're doing now for each LM, and use that same dispatching scheme—multiple streams—but have each stream be a group.

The API is a list of groups. Initially, I can give you either a list with one group or a list of single-LM groups, but it doesn't matter. The API gives you a list of groups. At the experiment level, we can design the flow around that for a consistent interface, so the backend can change depending on whether we do stacking groups or not. This allows the backend to use the group if it wants—basically, a list of lists. It uses the groups if it wants, or flattens it if it doesn't know how. Except for the CPU version, we would just flatten it; there's no distinction between groups. It would just become a for loop, as it currently is. We might want to do some multithreading at that level for the CPU. Some optimization could be done, but the naive, current approach is to flatten into a for loop, which would be equivalent to what we have now.

If you recall, I was saying during the screen share that for the Q and CPU backend, whenever we refactor, we want the default implementation to be the Q implementation. The default implementation of the SAP will just flatten that list of lists into a for loop and be exactly the same code. That sounds plausible. I'm sure we missed something that will become apparent as soon as we start writing code and realize it won't work. It's a pretty reasonable approach.

What else should we talk about? That doesn't seem impossible, and it seems like we can figure out a way.

The rest of it is starting to get more into the weeds about what we need to change. I was looking at the stack of functions before you reach the hypothesis updater, like step learning modules and the matching step. Compute possible matches, update—we'd have to figure out how to do all of that in a non-blocking dispatch way. That's what I wanted to ask.

Which kernels work? What is the minimum set of operations? For the prototype, we should just use the minimum set to make sure it can run end to end, without necessarily making all the kernels work—just make the machinery work. Pick the kernels that are functioning. Of the ones I shared in that post, everything is working except for KNN, with the caveat that a couple are not quite as consistent. The custom distance is accurate to a tolerance of e-2, whereas the others are all to e-5, and I'm not sure why custom distance isn't as consistent. There was a bug with the pose evidence kernel, but now it just has a slightly lower tolerance for it to pass.

The kernels seem to be functional and give pretty much the right answer. Everything except KNN.

That gives us a target for what the GPU hypothesis updater can swap. That gives a target for the initial implementation of the GPU hypothesis update. The thing that's missing in my understanding is how well we could drop and replace. The proof of concept took the core computation, made that work, and made it work for the trace data I was saving through the profile hypothesis update. But there are still edge cases and control flow linking these computations. I haven't worked through how that would all map to a GPU implementation. Once I got to the level of breaking into these different operators, I implemented the kernels, but more work is needed to think about doing the full end-to-end process: you give an input, sensorimotor reading, and then final hypothesis update outputs. We probably need to do some work to chain those together. Is that something you want to do now, or offline?

We can go through it and talk about it. Is there anything else you'd rather discuss?

I agree, we have an idea for the high-level API, but that's still not the thing you need. The thing you need is the detail of swapping out—what object can we swap out, or what do we have to change to make that possible.

This is just the multi code base.

I don't have a specific question. I just need to spend more time going through this and thinking about how everything chains together.

We can do that now. The other thing I'm wondering is, with the hypothesis update, are all the operations there? I remember there's some in the displacer and some in the update.

It sounds like we're not even talking about an alternate hypothesis update. We're talking about...

Actually, inside the hypothesis update, there is a different implementation. I think this could just be a different hypothesis update. It's up to us to figure out what makes more sense. The reason I'm thinking about this is because if you look at the resampling hypothesis update, that's already an alternate implementation. You probably don't want to re-implement a GPU or hypothesis updater and a GPU default one. There is some overlap. Did you actually have a chance to look at this and see if it has other differences? Okay, got it. That's what I wanted to highlight. I saw that it was updated, but I haven't really looked through it. So this does different things.

It would probably make sense to have a different backend for the hypothesis updater. The current level API could stay, and when it calls into update hypotheses, that's where the backend takes over and does the computation. I can ask the team, because I don't know off the top of my head whether the resampling one is the feature and we can just ignore the default if it's going away. I can double check. Let me make a note to myself. I haven't really gone through it. At a high level, what's different about the resampling? I don't remember off the top of my head. Ramy built that one. I refactored a lot of the code, but it was from a mechanical software design perspective, not looking into the details. I have to ask. Hold on, let me write down: check if resampling. It might come back to me when I look at it. The resampling hypothesis update is the feature.

Let me not bother with GPU for the default hypothesis. I'm going to look on my screen real quick to see what the difference was. Actually, I'm going to look at the pull request that introduced this because it probably has a good high-level description of the change. Was there an RFC for the resampling? Oh yeah, probably a good call. Let's see. I feel like I remember seeing that, but I didn't go through it. Resampling Dynamic Adjustments, RFC 13.

So the resampling thing is: right now, the default hypothesis update, at the beginning of the program, you set how much sampling you have to do for every iteration. It's basically fixed. What we want to do with the resampling is, when we're talking about resampling, it's about how many hypotheses we're testing. Resampling means: do we add new hypotheses to this hypothesis pool, or do we remove some from the pool? Prior to this, there was a fixed ratio of old and new hypotheses, and it was always constant. But this means hypotheses from unlikely things still get computed and tested because you have a fixed hypothesis pool. Resampling means we want to stop computing hypotheses that are very unlikely and sample more from the object we think we're actually on. For example, if I'm looking at a cup or a potato, and I'm trying to decide between them, I might have a 50/50 split of cup points and potato points that I'm sampling. As I get more evidence for the cup, I want to get a more fine-tuned sampling of different cup rotations and other things—rotation is a hypothesis as well. I want to sample more from the hypotheses about cups and less from those about potatoes. That's where the resampling vocabulary comes from.

My understanding is that you do different hypothesis update operations for each object graph in the LM. So if you're doing this resampled hypothesis updater, for whichever object you want to do more, you just set that one to use more hypotheses in its update step. In the separate potato step, you reduce the number of hypotheses you're looking at. But they're still separate calls to the hypothesis updater, right?

I think so, but I don't know the answer to that. It was just my understanding from the older version of the code when I was implementing this. It seemed like every object graph is processed separately, independently.

Even when I was shifting away from doing all LMs at once in one kernel dispatch, I was still going to try to have all objects for one LM run in one stacked kernel.

Nothing you've said so far makes me think that wouldn't work for the resampling one.

Conceptually, I want to make sure I understand whether object graphs are processed separately in the existing code.

I don't know if this answers it, but this is what I'm looking at. One thing that's new in the resampling hypothesis update is inside the update hypothesis. These are the relevant bits: existing hypothesis, informed hypothesis. These will choose hypotheses based on what's going on, versus this does not occur in the default one as far as I recall.

This is still happening per object graph. That's what the graph ID is there for. Sample informed—that is an object graph. It's called separately on each object graph. Yes, this is the answer to your question. The API here is: update hypothesis is called per object graph. At the API level, this just adds some overhead at the beginning to do some thoughtful adjustment about which hypotheses from that object graph we're actually going to consider and update.

I'd have to look into how the sampling operations work to see how easy it would be to do that on GPU as well, because we wouldn't want that to go sequentially either. We'd want to be able to do that in a batch. This is the sample informed and sample existing.

Sample specimen—I think this is just an array operation. I don't think there's much happening here. Calculate, keep, and remove IDs.

Calculate slopes. This looks like an umpire matrix thing. The main factor for identifying what is easy versus difficult for the GPU is how much one value depends on all the other values. If we have a big list where each index is processed completely separately, that's ideal for GPUs. In contrast, KN is challenging for the GPU because it requires synchronization between values.

This might be pretty straightforward. That might help. I don't know if the default hypothesis updater uses that. Tracker dot—yeah, it doesn't even use a tracker. It's not a thing.

What I'm talking about is the feature. If you're going to use the GPU, it should be the resampling hypothesis updater. This is too rigid. I would say to do this unless I provide counter evidence. I'll confirm it for you, but I think this is the one to focus on. I'll take a look and see how much that would change things.

In terms of next steps, we still need more discussion to figure out the API and how to integrate these backends. We're working on graphs, groups, and similar structures.

What's most helpful for me is to start working on the backend. I want to see what it would take to make kernels for the resample-type updater operations. I'll do that, and I'll try to change the API on the Monty side to support grouping. I'll see what the grouping API would look like, since I don't need GPU expertise for that. Jeremy or I can look at that. 

From this, as we discussed the scope, we're talking about resampling hypothesis update. There's another thing missing here—a refactor for parallelism and a refactor into a grouping call. That makes sense to me now, though I might not remember what it means tomorrow. What am I trying to say? Can you help me?

The hypothesis—the grouping call at the high level. That wasn't helpful for the GPU conversation. Refactor the hypothesis updater to support moveable backends. That's the helpful part. Resampling hypothesis updater to support moveable backends. This is what I can do because I don't need GPU expertise for the grouping. This is the refactor step: learning modules to use grouping, like grouping of LMs, whether that's one LM per group or larger groups. From the experience side, that's what it wants to see—just groups of LMs. We feel good about this, at least for now, with the simple dispatch.

That should work fine for the groups. The backend can handle whether it's a group that gets stacked into one kernel dispatch or just an L1, where we'll stack across object graphs. We already have to figure out some of the stacking. It's just whether we go across LMs or not. There's not a clear path between this and getting to the resampling hypothesis, so this is agnostic of the backend. This is an independent thing. Nothing depends on this, which is great, technically.

If you do the resampling hypothesis updater, we can still run it in a for loop. It will be inefficient, but it will still work on the GPU. To help with that, this step is the question. This is breaking. Now, I have a cold question that I know the answer to.

But we are out of time. My question is: is hypothesis update called per LM? Is there a clear linkage between the groups and hypothesis updaters, or is there another loop inside an LM hypothesis update? Right now, every LM has its own hypothesis updater, but it calls it separately on every object graph in the LM. We want to shift it to definitely put GPU back on, where it's doing all object graphs at once within an LM and then stacking the group. That would mean doing all object graphs across all LMs as one core dispatch, but that's still to be defined in the API. Is that true, or do you have a clear idea of how you want to do it? I think there are still some details. It's internal to the LM, because it's just whether it's calling a dispatch on a group of LMs or on the LM itself, but then inside the LM, how it calls into the backend and how we do the stacking—whether it's across LMs or not—there's more to figure out. Am I saying the right thing? Define how LM to object to hypothesis object graph, LM to object graph, hypothesis update call should work. This is hypothesis update. We're confident about this, but we're still missing the path through the object graph—how the object graphs are getting handled. We're missing the clear definition of an API where things get parallelized across object graphs, across LMs, across hypotheses. We want to parallelize across object graphs and across hypothesis update calls, which is the same thing.

It's important to say that for the CUDA streaming approach to work, we'd have one backend object shared between LMs because it needs to be. It's one backend handling the streams, running them across LMs, and reporting back when all streams finish. You want one call to one backend, not having to ask every LM whether it's done. Across groups, it's still one backend, since the groups also call into the dispatch the same way.

And then every group or each LM references that same backend object. Does that make sense?

It probably would, but I'm also feeling time pressure to get this done. When do you want to meet next? I'll send you a schedule and see what we can do if you're willing to meet. In the meantime, while we figure this out, I can at least attempt some progress on the refactoring. This is still a big unknown, and next meeting we'll try to figure this out. Does that sound reasonable?

We can talk over Discourse. Sometime early next week would be fine.

Thanks. It was really fun to finally get to talk about this. I've spent a lot of time reading documentation and thinking this is right, but I'm not really sure. Sounds good. I'll also see if I can get Viviane involved, since she's the expert on that, and that should help tremendously.